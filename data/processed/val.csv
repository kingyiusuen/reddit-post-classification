label,text
1,are linear regression assumptions ever satisfied and what do you do if they aren t i ve read that linear regression requires meeting a number of assumptions i think 4 or 5 depending on where you look do most researchers ever satisfy these and if not does that make their results and p values misleading because in my experience these are rarely ever completely satisfied and transformations to try and satisfy them makes the results harder to interpret for example if you are doing a log transformation what does it mean that this independent variable is significant with respect to the log of the dependent variable so what would be the best way to do this here is a link to the assumptions i m talking about
0,currently a data scientist want to increase my skillset to expand into data engineering any great resources courses etc that you guys can recommend thanks
2,what is it like being a phd student in a big research group hello i was wondering what it is like doing a phd in a big research group such as sergey levine pieter abbeel etc these groups have a lot of students so do these students regularly meet with their advisor does their advisor look and discuss at a low level about their research progress or are they mainly collaborating with other students in the group and benefiting from that i am just confused about what makes such students really prolific in terms of research output and what kinds of resources make these students stand out
1,how to create your own statistical question and how decide if you need to use a interval or test
2,choosing best parameters from an optimization hey forum i wanted to get some help and potentially collaborate hire someone to help me find come up with a solution to the below problem and the correct methodology on how to go about it here is the background x200b the use case is algorithmic trading to make a simple example i use indicators to trade forex stocks etc i perform backtests on data over 10 to 15 years and my goal is to determine the best performing indicator input parameter combination i do my backtesting using a process called walk forward analysis walk forward optimization the goal is to determine how robust the trading algo is the system when it runs on out of sample data the goal is to select the best performing parameters indicator parameters like stochastic if that s what you are using and carry that forward and use those input parameters on your out of sample data so here is what a in sample results look like x200b param1 param1 cagr avgdd 27 140 10 661 27 160 10 236 29 145 9 633 31 150 12 927 33 155 3 952 35 140 3 214 37 145 5 977 x200b cagr avgdd is my performance metric it can be anything really profit or profit factor etc basically parameters 1 2 x are inputs to the system during an in sample optimization run for each input parameter that i want to optimize i pick a start end and a step so if there are 3 parameters and each has a start value of 1 the end value of 4 and a step of 1 then you have 4 4 4 combinations passes and each pass will generate a performance metric value my in sample runs have a statistically significant amount of data for example a run over 4 years in sample period will have at least 5000 to 10000 trades for each pass parameter combination x200b so here is the problem that i want to solve how do i know which single pass parameter combination to select to run on my out of sample data set how do i choose the best performing input parameter combination i know it won t always be the one with the highest performance metric value for example my highest value can be 10 and the one right below it can be 4 3 clearly here 10 is an outlier what algo or method should be used some people have said knn is that true can this be done with more than just 2 parameter inputs what if i am optimizing 3 or 4 is there a downside to optimizing 3 4 5 etc different parameters x200b i m also looking for someone to help automate this for me looking for someone in math data science background to help me with this
0,data science field is overwhelming so i am a beginner in this field and the amount of knowledge and work being done looks very overwhelming in fact my peers too seem like years ahead of me when it comes to knowledge and implementation curious whether anyone out there also felt this way and how did you manage to get out of this confusion to feel a little confident that you know something and can do something or if you still feel this way i want to know your experience
1,reasons to discourage using linear regression on big data are there any mathematical justifications as to why obvious question we shouldn t use linear regression on big data for example is there a math formula that shows the more data points rows and columns you have the probability for these points to become non linearly sepperable increases is there a mathematical formula that explicitly shows that linear regression models can not model non linearly sepperable data is there a mathematical formula that shows when you have more data points the standard error of a linear regression model e g the beta parameter estimates are more likely to rapidly increases is there any mathematical formula that shows linear regression models for big data are unstable e g for two inputs that are slightly different the same linear regression model would produce notably different outputs thanks
1,expected value of throwing 3 heads in a row i know a solution to this problem is as follows a t b ht c hht d hhh and then you just say e x where x is 3 heads e x a p a e x d p d my issue is not understanding why we choose those events could someone please assist me with a detailed and nice explanation i know we need independent events that are exhaustive but i would never think to use those thank you
0,it s crazy how effective it s to include data scientist in your job listing example at the company i work for they had been trying to hire a analyst for quite some time it was originally called technical analyst and the response was lukewarm 20 25 applicants and some even withdrew their applications underway then hr renamed the job to data scientist included that in the tittle of the listing and slapped on some buzzwords on the new tools we use result almost 300 applications the shortlist included people with experience from big name tech and banking companies prestigious schools etc
2,suggestions for a model to determine vaccine priority hi everyone i am working on a proposal to create a model and to guide the ministry of health to determine priorities for covid vaccination i am from nepal and we have a second wave and not enough vaccines based on pcr test data covid spread demography location and age please suggest me what can i do to make this more effective
0,how accurate were the statistical models you developed on real world data when it comes to real world data how accurate were the statistical models you developed were these models able to consistently and accurately make predictions e g for supervised binary classification has anyone been able to develop a model that had high accuracy high sensitivity and high specificity
1,how is unbaised estimate s 2 of sigma 2 derived for linear regression in seber s linear regression analysis for y x beta epsilon where var epsilon i sigma 2 section 3 1 and 3 2 are about least square estimate of beta does it assume sigma 2 is known section 3 4 and 3 5 are about mle of beta and sigma 2 section 3 3 gives an unbiased estimate of sigma 2 as s 2 which is defined as rss n r how is the estimate s 2 of sigma 2 derived is it derived as ls estimate of sigma 2 thanks
1,ma in statistics job prospects i m considering doing a 4 1 degree mathematics bs statistics ma at my school binghamton university i have almost a 4 0 gpa 3 98 right now and am really far ahead in the curriculum i m taking grad level math classes in my junior year i m wondering how good my chances are of landing a job after graduating with the ma do you think a job is guaranteed for me after graduating with degree and a high gpa also i will likely do a few interships before i graduate also for reference i live in the northeast i guess my main question is how valuable is this degree and skillset is the job market saturated i know stats is predicted to grow a tooooon by the bls so that makes me think i d be guaranteed a job is this an accurate impression
0,any successful 1 man end to end stories hello all there is certainly a lot of hype still about data science and machine learning as somebody who tried sour side as well few failed projects because quality of data not really significant results i was wondering if somebody had any success stories with quick ml projects that had real business impact and were set into process to bring value also if it was worth to go down python way i have experience doing my master thesis where i used ml or much quicker way with pre defined tools like power bi or those online engines now i got promoted to business intelligence manager position only successful part of ml i did so far was with engine inside of power bi down the line there are certainly some ideas i have with speech recognition better assigning employees to customers finding right time to contact right customer but uncertainity of ml and its timing to get it done scares me for some thing i know i can bring 50 60 of value by doing visual analysis solving the big chunks of decisions in 10 of the time so generally i am advocate of ml is hype that is not worth it and it will stay like this until we have done majority of analysis visually first i can see how they are failing in other business unit where they hired niche data analysts and data scientist before even having data in db so it only enforces my thinking also if somebody has in mind employee churn project thats a no go with quality of data in hr not updating positions wrong assigned managers not standardized position names tl dr is e2e ml deliverable by single person in company as side project with bringing real value relatively quickly
0,covid 19 india related ask hello everyone a number of volunteers product software management backgrounds are trying to build a model to predict how oxygen demand will shape when supplies improve they wish to be able to help agencies distribute to the correct places logistics issue as you might imagine there is a model they are trying to build but too many unknown variables uncertainties do we have folks in data science who can help even if you can add a little it helps a lot if you know folks with relevant forecasting demand prediction experience it could be immensely valuable we can share whatever data points we have but most people in this sub probably already know what this entails thank you so much update i am so sorry that i am adding this late here is what i can say the intro document here are the issues on gh for what is becoming the large volunteer alliance one db all volunteer discussions take place in slack main channel for this is coronasafe demand prediction i will respond to each of you through dm
2,for combating ai bias for people of different skin colors i was thinking about this problem and i was wondering what do you guys think is the best approach to solve this would we need to tweak network structures or is correct dataset evaluation more important one of the most obvious solutions is to have a pre trained gan as a pre processing step for facial images that augments the colour space in such a way that all the faces in a big dataset converge on a single randomly chosen colour thus ensuring no bias for the model to exploit my take is that pre processing ensures the skin colours received by the model is the same and then reverses it if outputting storing results would counteract the bias in a naive form while the diversity of datasets is important it may not be achieved in every use case so i think such a method would be very generalizable to all datasets as well as reduce computation load what do you all think
1,has anyone ever worked on a machine learning model for queues has anyone ever worked on a machine learning model for queues suppose there is a bakery the bakery has has n people working m people in line and q orders that they are currently working on the bakery is interested in making a machine learning model that predicts how long a customer will have to wait before the customer s order is ready and how long will the next customer have to wait before they can place an order has anyone ever come across a machine learning model which can predict waiting and processing times i have seen examples online where people try fitting exponential distributions to historical waiting times and see how well they fit as well as trying different m m k combinations but has anyone ever come across an instance where machine learning algorithms e g random forest neural networks are used to predict waiting times i saw something like this but there was no python or r code for this paper can anyone recommend some source blog github website book youtube lectures etc which show and provide computer code for analyzing queues using machine learning models thanks
0,is it good practice to manually fill in missing data a date column for all but one row is complete with the one row containing only the year this is an issue for the analysis i want to undertake and i can quite easily replace it with the correct data after a quick google search my question is is this good practice or does it generally complicate things due to the data now coming from multiple sources and therefore should an alternative approach be used such as estimating the date tia
0,reporting change of percentages so you’re reporting out weekly kpis you want to report out wow change the metric went from 4 7 to 6 3 how would you show the change view poll
1,can you isolate the individual effects of an exposure in a regression model that uses dummy variables to incorporate two exposures in my regression i have two exposures of interest exa and exb i have created dummy variables to incorporate these two exposures into one variable group1 exa exb ref not included in the model group2 exa no exb group3 no exa exb group4 no exa no exb in my regression model i have included dummy2 4 as variables in the model and am using group1 as the ref i e when group2 4 are all 0 if i get a coefficient of z for group2 for example this would mean that the risk of having the outcome is z for group2 vs ref group1 since group2 is exa no exb and group 1 is exa exb can we attribute the difference in risk z due to exb status if i think that there are interaction effects between exa and exb would it even make sense to try to tease out the individual effects of exb and exb using coefficients in the model i realize that running a regression with individual components exa and exb would make more sense but i m trying to work out the dummy variable concept in my head through this example
1,defect counts in control i monitor a process that tracks the presence of defects in 10 specific places each of the locations can have the following level of defect none trace very light light moderate moderate to heavy and heavy i don t much care about the defect level just if the defect is present any level of defect is just a varying degree of bad the goal is to have none what kind of statistics can i do on this to keep track of the process to make sure it s in control the data is not normally distributed the mean is 0 89 and the median is 0 the histogram is skewed to the right trails off to very low frequencies up to a count of 10 it s uncommon to ever have 10 locations with defects but it has happened a few times i have looked a days between defects that data still doesn t look very normal any suggestions would be appreciated i want to track this process correctly it s very important to know if this process is in control where can i learn about this thank you
2,gifsplanation via latent shift a simple autoencoder approach to counterfactual generation for chest x rays to be presented at midl2021 motivation traditional image attribution methods struggle to satisfactorily explain predictions of neural networks prediction explanation is important especially in medical imaging for avoiding the unintended consequences of deploying ai systems when false positive predictions can impact patient care thus there is a pressing need to develop improved models for model explainability and introspection specific problem a new approach is to transform input images to increase or decrease features which cause the prediction however current approaches are difficult to implement as they are monolithic or rely on gans these hurdles prevent wide adoption our approach given an arbitrary classifier we propose a simple autoencoder and gradient update latent shift that can transform the latent representation of a specific input image to exaggerate or curtail the features used for prediction we use this method to study chest x ray classifiers and evaluate their performance we conduct a reader study with two radiologists assessing 240 chest x ray predictions to identify which ones are false positives half are using traditional attribution maps or our proposed method results we found low overlap with ground truth pathology masks for models with reasonably high accuracy however the results from our reader study indicate that these models are generally looking at the correct features we also found that the latent shift explanation allows a user to have more confidence in true positive predictions compared to traditional approaches 0 15±0 95 in a 5 point scale with p 0 01 with only a small increase in false positive predictions 0 04±1 06 with p 0 57 method overview demo code paper ridiculous promo video
2,i need help generating gst reference wav file in this colab hi i am trying to clone voice and i found this colab but its giving me a hard time due to the absence of reference global style token wav file of my voice i have seen section 4 of this gst tacotron but it doesn t have pretrained weights to use the eval py can anyone please help me generate a good gst reference wav file or help with voice cloning in general
2,google ai introduces ‘align’ to scale up visual and vision language representation learning with noisy text supervision excellent visual and vision language representations are crucial in solving computer vision problems such as image retrieval image classification video understanding that is why visual and vision language models rely on curated training datasets such as imagenet openimages conceptual captions which require expert knowledge and extensive labels all these datasets need non trivial data collection and cleaning steps limiting the size of datasets and hindering the trained models’ scale in comparison nlp models use large scale pre training on raw text without human labels and have achieved sota performance on glue and superglue benchmarks google researchers propose a technique to bridge this gap by using publicly available image alt text data text appearing in place of an image on a webpage when the image fails to load the team employs these image alt text data to train larger state of the art vision and vision language models summary paper google blog
1,question what statistical test should i use i have recorded the abundance of an invertebrate species in 10 quadrats for both two separate streams i am interested in seeing if there is a difference between their abundance between the streams what would be a good test for this data thank you in advance
0,why does everyone talk of deployment as something which is hard to do i keep running into people who say that you re only valuable as a data scientist if you know how to deploy your models and how anyone can learn model development but not model deployment and though i agree that model deployment is crucial i m not sure that they re right about how tough it is i ve deployed conventional machine learning models logistic regression at my job and cnn based deep learning models for my self projects i made the app using flask containerized using docker and deployed it to a linux ec2 the thing is all of that turned out to be pretty trivial to me and something which any data scientist can learn in 2 3 days what i m missing here did i find it easy because i was dealing with models of smaller scale or so many people say it s tough to deploy models as they ve never actually done it
2,improving topic modeling hi all i’m trying to implement some topic modeling for employee training survey responses i have 200 different training courses with between 10 and 400 survey comments each i want to use lda to extract topics and then i’d like to make a word cloud with the actual topic titles colored by the average sentiment is there any way to automatically infer the topic labels see idea below new training are being developed each week that have new topics like excel or providing feedback to the people you manage additionally does anyone know of any way to have a variable k number of topics auto selected by course some courses are a single topic one hour course while others are a week long with several training topics idea for auto tagging i’m thinking about extracting the nouns from my lemmatized text since i’m truly looking for topics thinking i could then create a noun list and label topics by using the most frequent noun in each topic
2,fine tune cause overfitting problem discuss i am trying to use deep learning to predict continuous emotion evoked by movies because my dataset has a similar task as the pre trained model so i only replace the fc layer in pre trained model to make it suitable for my tasks at the same time i try to ensemble different pre trained model to improve my accuracy before passing the features extracted by the different model to fc i concatenate different features extracted by different models this is the way how i fine tune the pre trained model but it only performs well in the training dataset i have already used dropout and l2 regularization tricks in case of overfitting it seems that overfitting still happens as i know the reason why overfitting happens is that the network is so complicated for datasets but my network only uses the pre trained model freeze all layers except for the last fc layer which is replaced by mine i don’t really think that it is so complicated in conclusion overfitting happens when i use the pre trained model that is the last fc layer replaced by mine what can i do to fix that update the overfitting problem happens in the first few epochs here is the graph of my loss
2,discussion risk mitigation techniques for online learning i am designing a backend service in which there is an online machine learning component it s implemented as either as reinforcement learning or a genetic algorithm not sure yet which it s impossible to train offline for various domain specific reasons we have to pick some naive heuristic solution as a starting point and train online i am exploring the idea of risk mitigation that is ways to control the total damage that bad policies can inflict we have to run the actions generated by bad policies to identify them as bad so it s impossible to screen them a priori there are two concepts i am trying to work in 1 restricting the amount of damage available to unknown policies suppose we re training an ad recommendation engine then if a policy is unknown only let it recommend ads to a small group of people 2 check similarity if we have a set of policies we trust to not be harmful ensure that new policies that are learned or that evolve are somewhat similar to those once we trust the new policies add them to the pool of trusted policies curious if anyone has run into this problem in online learning or knows about a paper or something that would point us in the right direction would love to hear opinions about my ideas as well
2,prediction using time varying covariates in survival analysis hi guys i m having trouble with making predictions using time varying covariates i already prepared a dataset with the appropriate format in order to include time varying covariates for each event or censored observation i have several intervals with the start and stop time and the values of the time varying covariates in those intervals but my issue is when i m at a certain time t and i have the values of the time varying covariates up until that moment t how can i estimate the remaining time to event because obviously i don t know the values of the covariates after moment t even if wanted to assume that the most recent values are the ones that will stand after moment t i don t know the stop time of the last interval the one that starts at time t and finishes at the time of the event because obviously i don t know the time of event that i m trying to predict so how can i make this prediction of the remaing life at time t and how is it done in r also is it possible to do this with cox ph model or with parametric models
0,admitted to the master of applied data science mads online at the university of michigan thoughts hello all i have been admitted to the master of applied data science mads online at the university of michigan to start in fall 2021 does anyone have any insights on the program if i understand correctly the program is very new and the first cohort of students must be about to graduate now i would love to read the experience of some people that have taken the program and whether they thought it was worth it as well as any objective opinions in general i have found very few first hand opinions on the mads at michigan out there and the opinions that i have read mostly here on reddit were on the negative side given that other schools such as georgia tech have tons of overwhelmingly positive opinions both on the quality of the classes as well as the price i am scared this is not the right move for me it is a big investment in terms of time and money after all my background x200b bachelor in economics 10 years ago 5 years of data analytics work in large tech company in bay area obtained 2 professional certificates at uc berkeley extension 1 in programming 1 in data science medium proficiency in python sql statistics probability i intend to take degree in 3 years while working full time i can afford the tuition 45k before employer contributions if program is worth it it won t put me into hardship or debt what i want to get from the degree applied knowledge that i can use at the workplace to move onto more technical roles also a degree that will officially open the doors to the mentioned roles as many positions state that a masters degree is the minimum required qualification if i were to decline the offer my current options are not many master of science in data science at colorado boulder everyone is admitted through their introduction courses that take place every 3 months apply to other schools for the spring 2022 semester georgia tech would be the first target any other recommendations
1,can we begin to understand possible mathematical reasons as to why algorithms like xgboost and random forest win kaggle competitions instead of neural networks could there be any mathematical reasons behind why algorithms like random forest and xgboost are known to win kaggle competitions i e perform well for medium sized tabular datasets compared to deep neural networks and linear regression models heuristically here are my general conclusions 1 glm general linear models perform best on smaller sized datasets provided certain statistical assumptions are met 2 boosting and bagging algorithms e g random forest and xgboost perform best on larger tabular datasets and do not require many statistical assumptions 3 deep neural networks perform best on very large datasets preferably on non tabular datasets e g tensors pictures audio computer vision text nlp but can there be any mathematical reasons that try to explain these general conclusions provided these conclusions are correct for instance suppose there is one response variable and one predictor variable and when graphed together they look like a sine wave it seems unlikely that a linear regression model could perform well perhaps this is because a linear model can only capture a linear trend perhaps it is too hard to understand the exact assumptions required for glm models to work on real world data or they are too prone to overfit on complex data the same way is there any math that explains why alphago self driving cars and google s bert nlp model are all based on neural networks and not using random forest and xgboost is this because there is some mathematical property of random forest and xgboost which severely hinder their performance on very big and complicated datasets perhaps it can be shown theoretically that random forests require an exponentially large amount of trees to model complex data which is just not computationally possible or would surely result in overfitting and the same way is there any math that explains why deep neural networks aren t as successful as random forest and xgboost on medium sized tabluar datasets do deep neural networks simply require too much effort to select the right combinations of hyperparameters and its just not worth it for medium sized datasets when random forests work well given significantly less effort are deep neural networks to prone to overfit medium datasets of course all of this comes to down to trial and error if a certain model fits the training and test data well then use that model but just using mathematical logic and intuition can we develop some general guidelines that tell us which conditions and types size of data are favorable for specific algorithms this could potentially save us a lot of time by directly trying better suited models for the task at hand e g not even trying to use logistic regression for alphago so in the end beyond empirical results could there be any mathematical reasons behind why random forest and xgboost are chosen in kaggle competitions compared to deep neural networks and beyond empirical results could there be any reasons why random forest and xgboost are not chosen for the imagenet competition thanks
1,what would be an appropriate statistical analysis hi all i am a bit confused about the statistical analysis i should perform on my data when i have it i have been advised to do a multi level regression model but what i am looking at doesn t make sense alternatively i have thought about doing a multiple regression the measures are made up just to give an idea my iv is the score on an anxiety test and the three levels are control at risk and clinical my dvs are iq score performance on a breathing exercise and questionnaire and support network confounding variables are stress and depression i want to look at if anxiety has a relationship with iq score breathing and support network i then want to see if iq and breathing have a relationship with support network finally i would like to see how the three groups differ on the dvs any help would be appreciated
1,concerning racism and poverty hi all i m curious if anyone has some studies related to racism and poverty that is i m trying to solve a question on whether or not those living in poverty are more likely to be racist every time i google this question the only thing that comes up is statistics concerning people that live in poverty beause of discrimination which is not what i m looking for x200b if anyone could link me to what i m looking for that d be awesome thanks for your time
1,gaussian process regression for more than one variable has anyone ever come across any source that shows you how to do gaussian process regression in r for multivariate data i found this really good link over here but it only shows you how to do this for one variable i have been trying to extend the results to multiple variables but so far i am unsuccessful can someone please recommend some source thanks
1,strict stationarity with dependent random variables strict stationarity means that the joint distribution of any subset of random variables within a time series is time invariant can you think of any practical or real life example where this applies when the random variables are dependent
1,how to use multiple variable correlation to determine midel i have been given sales data units as it relates to 7 different “factors” these factors are unnamed all i’m given is their rating out of 10 on a given sales day for example day 1 sales 105 factor 1 3 4 factor 2 9 1 factor 3 5 5 up until factor 7 there are 50 days of data like this i understand using excel’s data analysis tool how to find the correlation but how can i use those numbers to figure out the model of how sales are determined or how can i figure out for example how sales will increase if i raise factor 1 by 1 0 also does it mean it is actually a significant factor if the correlation is high or does it not necessarily mean that how does one determine which factor is “noteworthy”
1,properties of the smote algorithm has anyone ever used the smote algorithm before in predictive modelling i had 2 questions 1 does anyone know if it can be shown that smote might harm the results of a predictive model e g reduce accuracy instead of increasing accuracy can it at least be shown that smote is not likely to worsen the results of a predictive model 2 is this the general idea of smote suppose you are collecting data about two different types of birds from two different islands for the purpose of some predictive model you collect all the data from the first island but while collecting data from the second island a hurricane happens and you have to leave this results in your being imbalanced from the second island type of bird therefore to help your model you use smote to augment the available data from the second island and potentially improve your model birds from the second island are not inherently rare they are just rare in your dataset because of the hurricane that prevented you from collecting samples suppose the second type of bird is actually a very rare species is it fair to assume that smote wouldn t really help you
1,normal distribution std deviation of occurance hi i was wondering if anyone could help point me in the direction of where the expected occurrence from a std deviation perspective comes from i am specifically referencing the rule that 1 std deviation is approximately 68 of occurrences 2 std deviation is 95 and 3 std deviation is 99 i have had a google around and cant seem to find anything that specifically explains why these rules are held they are always just given as rules secondly i would like to calculate the amount of data occurrences for any given std deviation for example a 1 34 std deviation how would i do this
0,how much coding do data scientists do in a day i m planning to become a swe but have been developing carpal tunnel syndrome symptoms and i don t want to risk wasting my time learning software programming if i ll still always have recurring cts in the end and can t code for hours a day i m also interested in ds but i understand that there is a coding aspect to this job as well
2,improving bart text summarization by providing key word parameter hi all i am experimenting with hugging face s bart model pre trained by facebook on the large cnn daily mail dataset i have the below code which instantiates a model can read text and output a summary just fine from transformers import bartforconditionalgeneration barttokenizer model bartforconditionalgeneration from pretrained facebook bart large cnn force bos token to be generated true tok barttokenizer from pretrained facebook bart large cnn article text to be summarised batch tok article return tensors pt generated ids model generate batch input ids tok batch decode generated ids skip special tokens true i am now thinking about how i could insert an intermediary layer or keyword parameter which would indicate to the model to focus on particular words and words associated with the keyword for example if i insert a block of text which talks about different countries and the cars commonly found in those countries and specify the key word cars i d expect the summary to talk about which cars are found and in what quantity rather than information on the different countries i see a handful of potential ways to implement this but i am open to discussion 1 insert a topic aware step e g top2vec gensim etc whereby the encoded text is then adjusted further to reflect the importance of the word car 2 train models to be biased to certain keywords but maintaining a lot of models seems like high maintenance 3 somehow re fine the output layers of either the encoder or decoder to stress importance of the weights tensors of the vector towards words related to the key word i am a little stuck on how i would incorporate those i also have taken some inspiration from this paper who unfortunately have removed their code from their github link all suggestions on implementation papers to read or other guidance would be greatly appreciated to help me on my journey
0,conferences paying your own way what are others’ thoughts on paying your own way to attend conferences directly related to your job is it common practice are there salary ranges or other criteria you consider when making this kind of choice back story i used to work as a government contractor and we were only allowed to attend one conference per year on that salary i really couldn’t afford to pay my own way so i only went and presented at one i recently changed jobs to a small startup with a 25 pay increase but no real benefits to speak of yet very small and new startup there’s a conference that is squarely in line with my job description and potentially really important because of both the subject matter and the focus the boss is leaning towards only sending one of us and having that person take screenshots honestly i don’t think this is functional for two reasons 1 if i’m at a conference i want to focus on it and not be taking screenshots like crazy for 4 days and 2 if i’m not at the conference screenshots are not going to be useful to me really so i’m considering just paying for it myself 500 it will mean i can’t get right away the new computer i also need for my job but this conference is really important for both me and the person who signaled it to the boss and would get to go that’s only fair and i’m totally ok with it however paying my own way also sends a signal that i’m happy to do this going forward which i am not conferences are expensive and as a w2 employee they are not tax deductible what would you guys do
1,what test would i perform in the case of a continuous independent variable versus a categorical dependent i know that if this was flipped around i would either do a t test or linear regression but am unsure what to do when the independent variable is continuous rather than the dependent ack does anyone have any suggestions a humongous thank you
2,discussion how are you manage ml ai projects is it possible to comfortably use agile for it or maybe you use some specific tools processes can you describe it processes
1,what test would i use to compare means of answers from two different groups on a survey hey all i’m doing a survey for a class where i’m trying to compare answers for four questions based on whether or not people listen to a certain type of music for x amount of time what test would i use to compare the means of the answers for both of the groups it’s been a while since i took stats but i think the correct test would be a paired sample t test but i’m not sure i’m familiar with using spss thanks
1,question research can i explain 3 variables psm as predictive background clinical research chart reviews looking back at preoperative variables intraoperative interventions blood pressure drug dosages and post operative outcomes stratified by 2 groups conscious vs general anesthesia biostats analysis done by our statistician who recently left he did a 2 1 propensity score match for the 2 types of anesthesia and using preop variables results significant differences found for blood pressure and 1 drug between 2 anesthesia groups significant differences found for hospital length of stay and blood transfusions between 2 anesthesia groups question is there a way for me to connect changes in bp drugs to outcomes eg conscious anesthesia is more likely to have longer hospital length of stay and this may be explained by higher blood pressure during surgery
1,those of you who did a master s in statistics how did you fund your degree hi i am a rising junior undergrad majoring in math and statistics after deciding a ph d is not for me i ve begun looking into master s programs however they all seem exorbitantly expensive especially if you go out of state incredibly unfortunate for me because my current state has no well respected stat program i ve also been hearing that finding funding through ra s or ta s is much harder for a master s student so my question is how did you fund your education i don t think the jobs available to someone with only a bachelor would be satisfying for me at all in the long run but at the same time if the only way to fund such a degree is by taking ou at least 60k in loans i m uncertain on my options are i m at the point of considering applying to ph d programs with the intent to drop out after i obtain a master s in passing which can burn bridges and be overall unwise to add some perspective i will note that i have a 4 0 math gpa currently a gre score in the 98 ile and when i graduate i ll have coursework through some graduate level mathematics i don t know if that will make any difference in my ability to get funded for a master s i d appreciate any advice thanks
2,i trained a model to generate video game pages these past two months i ve been working on a project i ve called this game does not exist i ve always wanted to try building something with generative a i so this project scratched that itch for me here s a video with a few of my favourites read by voice actors x200b this game does not exist is an experiment in generative artificial intelligence this site contains 130 video game pages that were generated using an implementation of openai s generative pre trained transformer 2 gpt 2 to generate text and a simple implementation of generative adversarial networks gan to generate header images and screenshots to generate the names descriptions publishers and developers of the games i finetuned the huggingface implementation of gpt 2 i used the steam store games clean dataset from kaggle with slight modifications and preprocessing here is what one training sample looks like game name half life developer valve publisher valve description named game of the year by over 50 publications valve s debut title blends action and adventure with award winning technology to create a frighteningly realistic world where players must think to survive also includes an exciting multiplayer mode that allows you to play against friends and enemies around the world endoftext the model uses the tokens e g game and description to prompt each class of data while keeping context during the entire generation image generation was done by training a custom gan very similar to the architecture seen in the pytorch dcgan tutorial which was built to generate faces i created two models for this site one for generating the header images and one for generating multiple screenshots for each game to assemble the dataset i wrote a script that downloads the images from the urls in the steam store games clean dataset dataset due to my lack of resources and time to put into this project the image generation is less than ideal you may notice though that the header image model will generate artifacts in images that look like the titles of games and the screenshot image model with generate what looks like levels of a 2d platformer
1,how to calculate the average time to a place within a country for a paper on international trade i d like to calculate the average distance to a port within a country is there a way to do this ideally it would be the average distance by road and adjusted for population but that seems like it d be much harder
1,miscellaneous theorems used in machine learning i am trying to learn more about the background of machine learning and came across the following theorems 1 the universal consistency theorem i have heard of the universal aproximation theorem but not of the universal consistency theorem 2 the cover heart theorem i have heard of something called cover s effect which talks about how non linearly sepperable patterns tend to become linearly sepperable when projected into higher dimensions is the cover heart theorem related 3 stone s theorem i have heard of something called the weistrass stone theorem which talks about how polynomials can be used to aproximate almost any function is the weistrass stone theorem related to this can someone please help me better understand these thanks
1,what s the difference between a confidence level and a tolerance rate here s what i read about tolerance level but doesn t make complete sense to me the tolerance rate is the rate of exceptions that examiners would like to demonstrate is not exceeded in the population examiners should select a tolerance rate at the outset of sampling after defining the population tolerance rates are typically less than 10 percent for a given confidence level a lower tolerance rate results in larger sample sizes i don t get how tolerance rate is different from confidence level thanks in advance for any help
0,do you like being a data scientist i have the option to study two different courses one is to become a data scientist jr and the other one is the google project management course right now i work as a pm but i also know how to code so i don’t know what path to pursue now i mean i can improve my skills as a pm but in the other hand data science looks a very cool and interesting thing to study i have some experience coding with python and django so i think my profile match however i’m still wondering what to do do you guys like being data scientists what do you hate about the role is there professional growth do you consider that the demand for your profile is increasing
1,regression comparing two treatments occurring over different time spams i want to conduct an a b test to test two treatments but it s a little unusual so i want to make sure the regression i run afterwards makes sense the sample will be split into two groups control and test and recieve a different treatment both treatments occur over a different timespan control recieves the control treatment once a week for four weeks test receives the test treatment once a day for four days comparison will be by the average of a metric m over the 30 day period immediately after their last treatment so if say the start of the experiment is t 1 then all of the test treatment subjects will complete the treatment on t 4 while the control treatment subjects won t complete until t 29 however it s not so neat as that the treatment control or test can start at any point in a 30 day period it s possible maybe even likely that the effect of the treatment will vary from week to week depending on the state of world that week i was thinking to control for this maybe i d include interaction variables something like in r lm avg m treatment t where t is some representation of the time the length is this a plausible experimental design how do other people test in similar situations
2,discussion is the naacl worth it hello i am an undergraduate student with a goal of getting into ml research and academia recently i was informed of the naacl conference which is conducted virtually and it seems that i qualify for the student fee which is 25 however one must register for an acl membership for 50 so in total it s 75 im not cheap but to be honest im not financially doing very well currently but if the benefits from attending the conference out weigh the costs for an aspiring researcher then i can register the question is do the benefits out weigh the costs
2,how to tune the number of food boxes deliveries to optimize sales hi guys i d like to ask the community for a problem i am trying to solve at the workplace we have a client that owns a series of restaurants over the territory every week these restaurants receive food deliveries and our client keeps track of how much food is given to them vs how much it is sold the idea is simple he uses an in house algorithm that tunes the number of items a specific restaurant should receive for the week so that the number of unsold items doesn t hopefully go under a dangerous threshold if the restaurant consistently sells most of the food it receives the algorithm flags it as consistent and keeps on sending a similar number of food pieces every week otherwise the number of deliveries is reduced to attempt to balance the number of unsold items this has worked quite well for the client but he has asked me and my team to improve this process he has sent us data from a single restaurant that holds data from the beginning of 2021 the columns are the following restaurant id date type delivered sold quantity food description do you have any ideas on how this problem can be tackled i was thinking of linear regression and to use the number of sold items as my x to predict the y which would be the number of items to deliver i still don t know if this makes sense so i am asking around for some ideas thank you
0,what does the path to computer vision opencv for text look like hey all i m working on a project to extract machine generated text and table data from pdfs we got part of this using libraries such as pypdf2 pymupdf and extracting tables using camelot but you know pdfs can be a pain to read sometimes have weird encoding or are scanned so we need an ocr solution i ve been researching and playing with opencv but most resources involve neural networks on images i need to learn how to read text via an ocr solution can you recommend any resources or methods do they require nns any insights would be appreciated thank you
1,crossroads with my masters thesis and future career path hello all i am currently a master of applied statistics student finishing up my final year with my thesis i ve just begun the process for my topic approval and my advisor said my topic is fine but with how much similar literature there are it may be difficult to publish my methodology at the core is doing an in depth comparison of a plethora of machine learning methods to determine which one is best for a given dataset since it s quite empirical it may be easier to publish if i were to do this on a newer dataset than a well known dataset from uci or kaggle i was looking into the wisconsin breast cancer dataset now i m wondering if it s worth to switch datasets into something with less literature or if the quality of my comprehension and comparisons will be more valuable in determining whether or not my results are publishable even if there have already been quite a few pieces of lit done with that specific dataset beyond my thesis i have been at a crossroads for my career path once i graduate biostatistics seems to be a pretty ideal field for me if i can work specifically with statistical modelling and clinical data in a biotech environment i m quite novel to stuff like survival analysis only taken one course on it in graduate school and i m probably below average when it comes to stuff like experimental design my undergrad and work experience were all around regression and classification modelling so my skills are a bit more honed in that sense but i feel my coding is not nearly good enough to compete for data scientist positions i m debating then whether or not i should continue to pursue this more narrow path in biostats or maybe broaden my horizon and look into data science as well i m also wondering what field would hold my thesis topic in a higher regard in terms of work experience biostats vs data sci
1,confidence intervals for classification models the idea of creating confidence intervals in regression models is quite straightforward for example but do confidence intervals carry over to classification models 1 for example in this picture here confidence interval for an roc curve corresponding to a classification model can the lower limit of the confidence interval from this roc curve be used to gauge how well this model might generalize to new data 2 for a classification model can we make a confidence interval for the prediction of an individual observation for instance i wrote some r code you can directly copy paste the code for a particular example where a random forest i e classification model is used to predict whether if an observation will be approved or rejected see here for the code thus for each observation the classification model predicts the probability that this observation will be approved or rejected whichever probability is higher the model selects that outcome for the given observation also the higher one of these probabilities are this means the classification model is more confident with its prediction e g for two observations the ratio of approved rejected 0 9 0 1 vs 0 6 0 4 the model is more confident about the first prediction thus is there anyway to apply the notion of confidence intervals to the probabilities for individual predictions thanks
2,it is now possible to generate a model audit report with shapash many publications say that a trustworthy ai need to be auditable and that companies should build a strong governance on these topics with the new version of shapash that is now available you can document each model you release into production within a few lines of code you can include in an html report all the information about your model and its associated performance the data it uses its learning strategy … this report is designed to be easily shared with a data protection officer an internal audit department a risk control department a compliance department or anyone who wants to understand his work it’s like a “snapshot at the moment” with all the information related to the training of your model and to the upstream steps data prep scoping … do you need this feature for your models in production what kind of information would you put in your report for more details maif shapash 🔅 shapash makes machine learning models transparent and understandable by everyone github com
2,one liners in ml saw this post on r math and was wondering if y all have some interesting one line results facts to share about anything machine learning ¯ ツ ¯
2,why intel s dl based photorealistic enhancement tech is not ready for prime time gaming last week intel unveiled a dl system that enhances gta v s graphics to photorealistic level several media outlets have suggested that the technology has turned gta v to a photorealisitc game while impressive intel s image enhancement system is not ready for prime time gaming here are three key reasons memory the model requires more than a gigabyte of vram for inference while this amount of memory is available on most gaming computers we must also consider that games gobble up most of the resources of the gpu at runtime basically to free up memory for photorealistic render the users will have to make some kind of sacrifice such as playing at a lower resolution sequential processing neural nets rely on non linear computations so while you can use a gpu to process several inputs in parallel each input must go through the entire sequence of layers intel s model contains at least 100 layers of computation which is nearly impossible to run at playable framerates with current graphics cards the researchers tried it on rtx 3090 which has no shortage of vram and they still got 2 frames per second they have suggested some optimizations to integrate the model into the game engine this could give it a speed boost but not enough to get anywhere near playable framerates development and training costs the researchers needed thousands of well annotated images of urban settings to train the model lucky for them the cityscapes team had already done this for them but if a game takes place in a setting that doesn t have an open source dataset then it will be up to the game devs to curate or generate and annotate their own dataset of images which can come with huge costs also most gaming companies don t have ml talent to develop tune and train models read the full analysis here happy to hear thoughts on corrections to the take
1,is there a term for when a correlation is discovered and the discovery ruins the correlation for example i m making up this scenario let s say it s discovered and becomes well known that the team in basketball that scores the most 3 pointers has a high correlation of winning the game upon learning about this correlation teams start chucking up well defended 3 pointers in order to make more so they might make slightly more but their shooting percentage goes down so their overall scoring goes down so now that teams are doing this the correlation is no longer as strong so basically is there a phrase term for when the discovery of a correlation affects the correlation
0,any thoughts on m1 chips it s new laptop time and my it guy is of the opinion i should wait on m1 chips to come out on more powerful machines he s a huge fan thing is i do everything locally and we deal with some decent sized datasets my main machine is an imac with 128gb of ram and on some of our datasets i still have to do some fancy footwork to read them in my app memory usage is regularly over 100gb so i m concerned about these things only having 16gb he s convinced the ssd is fast enough that it won t be an issue can anyone weigh in on this i m primarily in r and do a good bit in parallel if that makes a difference
1,expectations of conditional distribution i need to work out e y where y 0 if x 1000 and y x 1000 if x 1000 i ve looked at the memo given and it largely makes sense however i m not sure how to work with conditional expectations and conditional probabilities in general we re told that y x 1000 x 1000 which makes sense however i don t understand how to go about working out e y with this x200b what i tried to do was work out e y e 0 p x 1000 e x 1000 p x 1000 however this doesn t provide the right answers why doesn t this give the correct answer and how should i go about solving this for additional info x is lognormal with mean 2640 and sd 3460
0,what is a good answer for why use l1 l2 norms but not l0 5 or l3 i see this as an example interview question all the time but i never see great answers so far as i can tell a good answer might be l2 norm is continuously differentiable l1 has closed form solution l0 5 or l3 are possible but l3 norm will tend to overregularize and bring coefficients towards each other l0 5 norm would induce some irregular sparsity not even sure how to clarify this one x200b what else would people mention i know there are answers involving hilbert spaces with the l2 norm but i don t really understand that
2,does anyone use orange in their ml workflow orange seems to be default included in anaconda and i ve always wondered how useful it it i ve gone through a couple tutorials on how to use it but i cant really find a use for it in my own work x200b has anyone here actually used it before and if so do you find it useful and do you use it regularly what tasks does it work well to streamline and finally once you ve completed your analysis testing on a ml design what do you typically do after that for full implementation whats your process x200b i feel like it may actually be a really cool tool if i can just pend enough time to actually learn it fully but am not sure if its really worth committing too as i haven t really heard anyone use it and the online resources are very limited
2,error due to colab ram depletion when implementing multi label classification with bert and pytorch background i m implementing multi label classification for tones 7 types of tones dataset shape train df 5392 8 val df 1348 8 the modelling approach remains the same as this multi label classification with bert pytorch i m getting an error in training the model where colab s ram is being depleted x200b n epochs 10 batch size 12 data module tonedatamodule train df val df tokenizer batch size batch size max token len max token count our model will use a pre trained bertmodel and a linear layer to convert the bert representation to a classification task we ll pack everything in a lightningmodule class tonedatatagger pl lightningmodule def init self n classes int n training steps none n warmup steps none super init self bert bertmodel from pretrained bert model name return dict true self classifier nn linear self bert config hidden size n classes self n training steps n training steps self n warmup steps n warmup steps self criterion nn bceloss def forward self input ids attention mask labels none output self bert input ids attention mask attention mask output self classifier output pooler output output torch sigmoid output loss 0 if labels is not none loss self criterion output labels return loss output def training step self batch batch idx input ids batch input ids attention mask batch attention mask labels batch labels loss outputs self input ids attention mask labels self log train loss loss prog bar true logger true return loss loss predictions outputs labels labels def validation step self batch batch idx input ids batch input ids attention mask batch attention mask labels batch labels loss outputs self input ids attention mask labels self log val loss loss prog bar true logger true return loss def test step self batch batch idx input ids batch input ids attention mask batch attention mask labels batch labels loss outputs self input ids attention mask labels self log test loss loss prog bar true logger true return loss def training epoch end self outputs labels predictions for output in outputs for out labels in output labels detach cpu labels append out labels for out predictions in output predictions detach cpu predictions append out predictions labels torch stack labels int predictions torch stack predictions for i name in enumerate label columns class roc auc auroc predictions i labels i self logger experiment add scalar f name roc auc train class roc auc self current epoch def configure optimizers self optimizer adamw self parameters lr 2e 5 scheduler get linear schedule with warmup optimizer num warmup steps self n warmup steps num training steps self n training steps return dict optimizer optimizer lr scheduler dict scheduler scheduler interval step x200b model tonedatatagger n classes len label columns n warmup steps warmup steps n training steps total training steps multi label classification boils down to doing binary classification for each label tag we ll use binary cross entropy to measure the error for each label pytorch has bceloss which we re going to combine with a sigmoid function as we did in the model implementation let s look at an example criterion nn bceloss predictions model sample batch input ids sample batch attention mask predictions tensor 0 2637 0 6621 0 3690 0 3400 0 5255 0 5604 0 4574 0 2863 0 6707 0 3608 0 3159 0 5064 0 5195 0 4597 0 3011 0 6806 0 3491 0 3200 0 5160 0 5008 0 4566 0 2632 0 6532 0 3484 0 3600 0 5162 0 5607 0 4416 0 2674 0 6449 0 3435 0 3526 0 5223 0 5434 0 4429 0 2981 0 6899 0 3588 0 2996 0 5088 0 4698 0 4766 0 2695 0 6288 0 3351 0 3649 0 5389 0 5892 0 4257 0 3226 0 6841 0 3424 0 3193 0 4977 0 4534 0 4946 grad fn sigmoidbackward x200b calculate the loss of the predictions criterion predictions sample batch labels tensor 0 6812 grad fn binarycrossentropybackward x200b trainer pl trainer logger logger checkpoint callback checkpoint callback callbacks early stopping callback max epochs n epochs gpus 1 progress bar refresh rate 30 gpu available true used true tpu available false using 0 tpu cores trainer fit model data module execution is stopped due to depletion of available ram in colab best guess point of failure infinite loop due to incorrect hyperparameters
1,job prospects in the private sector after a phd in mathematical statistics so i m currently finishing my master s degree and i d like to pursue a phd afterwards and i m currently in the process of reviewing applying to various phd programs i m interested in broadly speaking my interests are in mathematical statistics and learning theory it turns out that i ve found a phd offer with the following aims study of mathematical properties of bayesian algorithms in semiparametric problems and investigating the choice of priors for optimal posterior sampling needless to say i am super interested in that offer and it turns out that the university s location is ideal for me and the pay is decent as well so it s pretty much jackpot for me however i m wondering whether it d be the right choice for me to pursue such a phd if i am not absolutely sure that i want to continue in academia afterwards indeed i don t see what kind of job prospects there would be for someone who spent the last 3 4 years e g proving theorems for the choice of optimal priors in bayesian modelling all i know for now is that i want to do a phd in mathematical statistics because i like it a lot i find the field super interesting there are many problems i d be interested in solving and overall i could as of now very well see myself doing it all my life however i don t know if that will still be the case after completing my phd and i would therefore like to have decent opportunities in industry if i change my mind therefore i want to ask are there any interesting job prospects in industry for someone with a phd in mathematical statistics or should i rather only apply for more applied phds in stuff like data science and machine learning if i m not sure i want to continue in academia
0,hi i just expanded the data science cheatsheet to five pages added material on time series statistics and a b testing and landed my first full time job hey all you might remember me from the data science cheatsheet i posted a few months ago here the support from that was incredible and i thought i’d share an update since then i’ve gone through a dozen interviews ranging from fang to startups to mbb and updated the cheatsheet with topics i’ve seen covered in actual interviews improvements include added time series added statistics added a b testing improved distribution section added multi class svm added hmm miscellaneous section and a bunch of other small changes scattered throughout these topics along with the material covered previously are all condensed in a convenient five page data science cheatsheet found here i’ll be heading to a fang company as a ds after graduation and i hope this cheatsheet is helpful to those on the job hunt or just looking to brush up on machine learning concepts feel free to leave any suggestions and star save the repo for reference and future updates cheers aw github repo
1,what has a 24 and 13 chance of happening i am trying to convince someone to wear their helmet while at work no i can t fire or report them from what i read and understand there s about a 24 chance of a tbi to be inflicted upon an employee while on a work site if they wore their helmet it would reduce those odds by 52 24 52 1248 i want to highlight the dramatic reduction in probability of a traumatic brain injury by comparing those two odds also if i messed up my math i d still like to know because that s just a random conversation started i can have with friends edit ok yeah i had a lapse in logic the odds of an accident at a construction site is 10 meaning 024 052 01248 024 50 work weeks 5 days a week 6 times per year an accident occurs 01248 50 work weeks 5 days a week 3 times per year an accident occurs what else happens at about those frequencies
2,how is it possible that a researcher with an h index of 80 publishes bollocks this is a genuine question i am a master s student and i was only starting with machine learning at the start of this academic year during my long project i was trying to reproduce a paper from affective computing i tried for at least 2 weeks but i couldn t get it to work now after several months when i read that paper it is completely obvious to me that the numbers were either completely made up or they only got it because of random error very little detail is given in the paper as if they are actively trying to make the paper irreproducible in any case bullshit papers like these are not uncommon so my project turned into a debunking a particular application in affective computing but what is bogus for me is that many of these papers were published by scientists in amazon facebook ai etc one of the authors of the paper mentioned above has 80 h index i understand that he wasn t the one coding it and he probably only had an advisory role read the paper but even a 1 year master s student me would have realized how bullshit the whole paper was
1,confused between f critical and alpha hello good people i hope everyone is doing well i am learning basic statistical tests as a requirement for my university since i am enjoying holidays now and i am interested in stock anomalies and decided to run some basic statistical tests on the data from my country s stock exchange index i ran a f test two sample for variance between the week starting day and the other days of the week using the built in excel tool as far as i have learned from different sources if 1 f f critical reject the null hypothesis 2 p value alpha reject the null hypothesis now my results showed 1 f f critical so i can t reject the null hypothesis 2 p value alpha so i can reject the null hypothesis my question is which one should i use to give my judgement about rejecting or not rejecting the hypothesis is any one of f critical and alpha more meaningful than the other thanks in advance
1,comparing linear regression models using dummy variables for my research i ve explored the effects of predictors on green space visitation before and during the covid 19 pandemic the result is two linear regression models over two periods of time for the same population group i was wondering how to compare these two models and have found online that an alternative to a chow test is to run the model with dummy variables i know what dummy variables are but am confused how this method would work in practice is anyone familiar with this method any help is appreciated
1,question why the stats language is so messy i mean i feel like you have a lot of words for the same concepts but i m not sure i might be mistaken for instance recently i read the word heterogeneity and i thought wow what could this mean then it turned out it means diversity or i just came across interdependence which i think means correlation and a lot more similar examples multi level modeling hierarchical modeling mixed effects modeling apparently all have the same meaning what should i do to be less confused i m a just beginner but stats is very important to me and i want to excel at it
0,against the negativity here i just received my 200k salary offer in just 2 years even in this economy few months ago i wrote in this thread with an older account about how i think some data scientists are getting underpaid and negotiation is an important skill during interviews as much as ml frameworks it was meant to be a message to uplift all of us into better career development but when i wrote that my first job as a data scientist was making 150k a year and that we can easily make 200k with upgraded skills experience and right negotiation people here laughed at me said that i was trolling and that kind of salary was insane i told them this is the average in the bay area but they said that even seniors don t make this kind of salary well 2 years later i have just secured a 200k salary 170k in base and 30 in yearly bonus not including rsu this is for a data scientist in ml role at a company in sf not well known but a stable company i eventually settled for another company with far less salary but far better stock potential but still given that i proved my initial point i want to say few additional points of affirmation 1 don t undersell yourself know your value and worth and stick to it with confidence even in this terrible economy 2 if you can impress the hiring manager and the senior management during interviews they re more than happy to work with your professed worth if not in salary then in bonus stocks etc otherwise they will lowball you this requires a refined skill in both communication and technical chops 3 know how to play the political game during interview cycle master the negotiation tactics know how to bluff too many tech folks don t like to do this and think that they can keep their heads down and work hard and their accomplishments will be naturally rewarded by some supernatural force that s rarely the case data and software folks are not immune to necessities of nuanced and skillful communication btw i don t have fang level experience my first company 2 years ago was a mid sized startup most people haven t heard of
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
0,when can you call yourself a data scientist what are the things that a data scientist must know so that they can call themselves as such
1,mixed ancova levene test significant what to now hey guys i am running a mixed ancova i have two measurement points and i am using a categorial predictor i also have age and gender as a covariate when including gender as a blocking factor and age as my covariate the levene test turns significant however when including gender and age as covariates the levene test is not significant what can i do when the assumption is violated i heard that a box cox transformation is advised but i have no idea how to do it in spss also i am not sure which way of including gender is the best does it make the most sense to use gender as a additional categorial predictor or is it better to include it as a typial covariat best regards
2,how are text to speech models trained do they use the same data as speech recognition models i ve been fascinated with this topic for a long time but have yet to get a solid answer the best i ve come across is this paper on google s tacotron text to speech model which says end to end text to speech models typically require a sizable set of high quality text audio pairs for training which are expensive to collect so i assume you can train text to speech models with the same labeled audio data you use to train speech recognition models except in this case the audio is the label and the input is the text am i right here is the data labeled in the same way even so i expect that what constitutes good data is probably quite different when looking at text to speech vs speech to text for speech recognition i know it s beneficial for your audio data to have background noise multiple speakers and for your speakers to be talking in a conversational tone these are the kinds of conditions you want your speech recognition model to accurately perform under so something like youtube audio that s captioned or transcripted podcast audio would be ideal but is it fair to assume that the opposite is true for text to speech would you want your data to be clean single speaker and delivered in a narrative tone like audiobook data would love to hear everyone s thoughts on this and if anyone has sources where i can educate myself more on text to speech training i d love to see it
1,resources for multivariable calculus and linear algebra i’ll be starting my master in statistics this fall and an understanding of multivariable calculus and matrix algebra are a crux to many of my first semester courses although i have a background in mathematics and have studied linear algebra and differential calculus in my undergrad which was approximately two years back i am looking for a mooc or quick course that will help me with enough material walkthrough that would be required to grasp the statistical concepts the reason i’m asking for an online resource and not a textbook is because i have a full time job right now and going through an entire textbook that is not an easy read would be arduous thank you for the help
1,what are some approaches you ve been using to handle forecasting in light of covid the data i m working with is yearly enrollment figures for schools in a state going back to 1984 it follows what could be sinusoidal trend one period not enough to say that it s a seasonal pattern with a precipitous drop in 2020 due to covid there is very little noise in the dataset with exception of this one covid related drop i m not really sure what to do in this situation what i d like to do is capture is the uncertainty of creating a forecast in light of one datapoint that departs from a long term trend or at least be able to present pessimistic and optimistic forecasts i think enrollment will eventually fall back in line with that trend but without additional data it s unclear how fast that would occur any ideas
2,99 2 wrong but my maths ai brainchild is definitely learning my ai brainchild hi i was inspired by the kind and useful feedback from this group on my first post about using deep learning to mimic how young children learn to add here is my second post thoughts and feedback would be very much appreciated again x200b
2,pixlab annotate online image annotation labeling and segmentation tool annotate is a web based image annotation labeling segmentation tool for machine learning model training tasks and beyond features set rectangle polygon zoom drag labeling tool consistent json output accepted by most machine learning frameworks optimized for instance segmentation mask r cnn etc client side persistent storage no data transfer involved persistent easy label management create modify delete full screen display snapshot capture
0,should removing outlier be based on the specific data i am new in data science so excuse my limited knowledge i am studying an overview of the different stages that a ds project goes through currently i am learning more about outliers detection and removal so i have not found a resource that talks about real world application of outlier removal say you are working on a small project that has a data set that depends highly on other external factors let s assume the dataset that i will work on is economic growth in a particular state in this case economic growth depends on hundred different factors growth in business population market jobs etc given that in your project you are just limited to the economic data my question is should you perform an outlier removal get a boxplot of different attributes in the dataset and and remove all the values that fall out of the 1 5 times interquartile range or let go of the removal so as to assume the closest real world possibility in practice what would be the ideal solution to work in this scenario
1,obtaining p values from rlmer t values using satterthwaite approx df from lmer i found a technique on stack exchange for calculating p values using the approximate degrees of freedom from lmer and the t values from rlmer the paper referenced by the stackexchange post doesn t go into detail about why or how this works does anyone have a reference for this technique here s what the code looks like fit a mixed model model lmer tm power abs condition phase 1 subject data powerproslpar fit the robust equivalent robust model rlmer tm power abs condition phase 1 subject data powerproslpar get coefficients from non robust model to extract satterthwaite approximated dfs coefs data frame coef summary model get coefficients from robust model to extract t values coefs robust coef summary robust model calculate p values based on robust t values and non robust approx dfs p values 2 pt abs coefs robust 3 coefs df lower false p values
0,if you want to run sql queries on csv files from the command line without installing opening any dbms software use csvkit it happens at times that we don t need to create a database table just run some sql queries on csv files to test data use a python package csvkit install the package with pip run the command on command line csvsql query enter your sql query here file name csv you can also save the output to a separate csv file
2,discussion about no code ai development platform what do you think of no code ai model development service like darwin ai and crowd ai those services make the users build the ai models only in gui some people would say they prefer to write codes on their own so they can know how the models are built but some people would like the no code service valuing its convenience more than code customizability what is your opinion on those services
0,anyone play around with subreddit data i usually just use the site to talk about stuff i do on my free time but subs like unpopularopinion have me thinking that it would fun to tinker around and compile some dataframes noticed some examples on dataisbeautiful and datart if you have used reddit itself for making models what models did you make and how did you go about data cleansing
0,ds consulting i’ve accrued a few years of experience as a data scientist and have become quite specialized my current company has been fantastic in providing me external opportunities to present my work through various conferences and speaking to industry experts i definitely consider myself more of an applied data scientist not at all interested in developing novel algorithms much prefer the business application of existing tools and i am trying to find a way to continue working in data science but stop working in the corporate structure i’ve just found it so stifling because of politics regulations and misalignment on what data science is i would like to move into a more consultant type position and work with smaller companies which may not be able to commit to an fte ds but still have questions that ds can solve so my question is do y’all have any tips for getting into consulting or something along those lines i am working on building an online presence and building my network but any advice would be great thanks
2,augmenting a b tests with offline policy evaluation i recently read about a technique in statistics ml called offline policy evaluation the idea is that you can evaluate how new policies will perform by using historical data generated under a previous policy for example rather than testing a new fraud policy in an a b test you can use historical logs to determine if the new policy will outperform the existing one this seems like it could be a great step before a b testing new policies i whipped up some example code to test out what would be considered the hello world of offline policy evaluation if anyone is curious my question to you is have any of you have tried this or do any of your currently use ope at your companies
0,everything wrong with zindi data science competition platform warning rant coming i want to to share my unfortunate experience with zindi platform it is a data science competition platform same as kaggle but the bounty doesn t usually exceed 2000 and it is geared more toward african countries i participated in a competition there hoping that the company hosting it would hire me if i win after few weeks i snatched the second place on the leaderboard i kept slightly improving it for the span of of what s left on the competition then one week before the deadline i got my account banned i opened my email thinking it was some sort of a mistake i found an email sent by them stating that they banned me under the pretext of collaboration outside of team i responded explaining to them that i single handedly worked on the solution of my problem telling them i m ready to provide proof if they want they didn t respond then today out of sheer luck i discovered that the team that took my place on the leaderboard when i got banned work as a data scientist for zindi which is quite preposterous to say the least how can they work in the company and be allowed to participate meaning it s in his advantage to ban people who are topping the leaderboard they eliminate any competition and they get the money this explains the very empty devoid of any logic explanation provided by zindi as the reason on why they banned me the such of collaboration outside of team without the willingness to elaborate any further or give sufficient proof even if my solution out performs theirs it s just insane i would say stay out of zindi it is an unfair community chances are not equal and they are not professional zindi epitomizes everything wrong with african countries conflict of interest lack of respect to people rentier state and corruption ps i come from an african country
0,object detection from scratch resouces for fun i m playing around with image classification segmentation and now detection i ve done some basic object detection projects with transfer learning using mask r cnn and yolo which worked great however the point of this is to learn how they work and write my own terrible versions i understand the basics behind object detection cutting up the image into patches and running image classification on the patches i was just looking for a resource that walks through this without using transfer learning or pre built networks like i said i expect this to be terrible i just want to try it out for myself
0,data literacy at your company i was wondering if some companies obliges most of their employees to learn to code basic skills or for example dashboarding advanced data analysis skills with excel or through training periods one week of full training and no actual work the purpose would be to make everyone data literate so that everyone at the company can provide the data team with good datasets or so that they can handle their dashboards once they are published how to filter refresh data if so please share your experience and what are the best ways to discuss that with the managers
2,machine learning workflows to summarize translate transcribe and more x200b what if we want to extract and summarize text from documents also handle translation combine the outputs and load it into an embeddings index enter workflows the demo above takes a list of github project pages extracts text from html summarizes the text and builds a similarity search index this same concept could be applied towards a list of company pages wikipedia pages and more this is just one example of what txtai workflows can do txtai workflows are a simple yet powerful construct that takes a callable and returns elements workflows are streaming by nature and work on data in batches allowing large volumes of data to be processed efficiently the amount of functionality provided by machine learning models continues to grow rapidly txtai provides an easy way to interface with these models the following is a non comprehensive list questions extractive question answering using a text context labels apply labels to text using a zero shot classification model summary abstractive text summarization text extraction extract text from documents transcription transcribe audio to text translation machine translation workflows allows joining these models together to create powerful data transformations workflows can also be constructed in javascript go rust and java via the api see the following links for more information github workflow builder documentation article
2,deepmind s reward is enough or environment is enough deepmind s paper about reward is enough 1 only gives half the answer we need the appropriate environment e —an e that can simulate all the feedback to the agent humans already live in the real world that is the e to learn starting from the baby what will be world like e for human like a i we can achieve narrow a i because the e is easy to create such as chess like or game like but i am not sure how we can create this world like e if we can t even understand all the physics phenomena if we found a human like algorithm ready for training we need a world like environment to provide the so called reward and do we have enough computation power to run the world like e if reward is enough for human like ai or artificial general intelligence why can t be world like environment is enough let me know your comment 1
1,does it at all make sense to ask whether a factor analysis vs a network analysis better explains a dataset i ve read that these are philosophically different approaches in that a network analysis doesn t assume the existence of latent variables is it possible to submit the same dataset to both and then ask which one is a better fit i e whether latent variables are necessary to explain the data
0,is it worth taking a lower tier ds internship after mid tier swe internship if i’m interested in ds i’m a rising senior currently interning at a decent company as swe but i recently got offered a fall internship at a lower tier company in data science i’m really interested in data science and haven’t really been exposed to it but i’m worried this will look like a step down resume wise the pay for the ds role is about half as much as my current swe role too for context if i took the ds role i would have less time on school as well since it’s in the fall so would it just be more worthwhile to focus on school and maybe research there than this internship i would still apply to data science jobs in the future regardless any thoughts would be appreciated
0,weekly entering transitioning thread 23 may 2021 30 may 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
2,e2etag an end to end trainable method for generating and detecting fiducial markers presentation this work was accepted to bmvc2020 link to our paper e2etag proposes an end to end trainable method for designing detecting and enabling fiducial markers with deep learning our method is made possible by introducing back propagatable marker augmentation and superimposition into training the detector used was a modified deeplabv3 encoder which predicts marker s localization projective pose and class the images used for superimposition training were from the imagenet dataset results demonstrate that our method outperforms existing fiducial markers in ideal conditions and especially in the presence of motion blur contrast fluctuations noise and off axis viewing angles
2,machine learning security evasion competition 2021 calls for researchers and practitioners mlsec21 announced it will have a defender track for machine learning malware detection models like last year participants will be able to submit their models june 15–july 23 2021 and their submissions will subsequently be attacked by participants of the attacker challenge registration opens jun 15 at last year erwin quiring lukas pirch michael reimsbach daniel arp and konrad rieck from the technische universitat braunschweig germany won the defender challenge with this model the event is organized by hyrum anderson principal architect and ram shankar siva kumar data cowboy in azure trustworthy machine learning at microsoft zoltan balazs head of vulnerability research lab at cujo ai carsten willems ceo at vmray and chris pickard ceo at mrg effitas
0,should i use python at work if my coworkers don t have experience with it i am working on a data science project at work and my manager gave me permission to use python however he also said that my code should be clean and readable and i must explain it to my coworkers since none of them have extensive experience with python before this is because i am an intern so when i leave my coworkers must be able to understand my code and maintain it because of this do you think i should use python or use something else like excel to be safe cause everyone know excel
0,how to make sure my team receives appropriate recognition hi all i work for a fairly small to mid sized firm and we re facing some growing challenges as a data team our leadership management team currently respects teams that add value to the company so our marketing team tends to receive significant recognition other teams tend to take credit for the reports models and any other analysis we do this lends to our team apparently not adding business value to the company and makes it more difficult to do salary raises promotions hiring etc another issue is that another non technical team is currently asking me to teach them how to use sql to do their own data pulls i want to be a team player but this adds no value to my team except diminishing the workload they put on us how do i as well as my team navigate through these office politics
0,is geospatial data science a good field to get into i am a civil engineering graduate considering starting a master s degree in geomatics engineering at a canadian university i know one of the applications of geomatics has to do with geospatial data science things like spatial databases data mining and spatial statistics for those who work in this field how good is it what are some typical tasks you perform does it have good future prospects does it require as much math and stats as regular data science
1,statistic projection about huawei turnover profit in future hello does anyone has a statistic about huaweis turnover or profit estimated in future best would be for the section 5g the projection doesn t have to be that far in future some years would be totally fine thank you very much
0,can someone please help me understand these graphs these graphs come from the following article on covid 19 and statistical models these graphs show the 30 day risk probabilities of critically ill patients vs other patients in these graphs is a value of 1 supposed to indicate very high risk that a patient develops covid 19 symptoms so basically these graphs are showing the day by day risk that different groups of patients develop covid 19 symptoms after their last hospital visit thanks
1,accumulated local effects i recently came across a newer technique called accumulated local effects that attempts to explain the effect of predictor variables on the response variable has anyone tried using this method on real data did you find it useful any stories anecdotes experiences comments reviews you would be willing to share regaeding this method
0,predicting who someone may want to send a message to how would you predict who someone may want to send a snapchat or gmail to i cant really think how to approach this interview question can anyone help me get started with it
2,which is more valuable for grad study research experience or software engineer internship hi i m a third year undergrad i am planning to do grad study in ai ml right after graduating my college has a lot of co ops internship opportunities let s say i got an internship offer from a big company for doing some sw development stuff or data science stuff and the other offer from a non popular research centre for doing some ml research which one do you think is a more valuable asset for applying to graduate schools side question maybe the best opportunity is doing research or ml dev in big companies in this case
0,question when if ever is bivariate regression useful i am retooling and revisiting stata and came across my old notes on when to use each regression type x200b for when to use bivariate regression my notes read x200b don t it s trash bivariate one independent and one dependent models don’t tell you much other than confirming if there is a strong correlation x200b i don t recall ever actually using it and i started googling but now i am puzzled x200b is bivariate regression just a measure of correlation between independent variable x and dependent variable y if so when would it ever be useful
1,need help with sampling design for forest survey hey everyone long time lurker first time post love y all s work and i was hoping i could get some advice my msc thesis is partially funded by an environmental consulting group here in italy i am trying to establish cheap cost effective and simple methods for measuring and reporting ecosystem services part of this includes measuring for carbon sequestration and biodiversity and i have chosen to analyze forest structure tree height diameter at breast height the volume of deadwood etc as a proxy for biodiversity my hope is that the same set of variables can help explain both carbon storage and biodiversity forest complexity very simply put the mean values will provide average carbon storage sequestration data and the variance will provide data about forest complexity this is all backed up by the literature so with that brief intro out of the way i am hoping someone can help me come up with a sampling design that s feasible for a large group of forest smallholders distributed across thousands of kilometers and covering several different forest types i am looking to balance the cost of the surveys with reliable statistically sound data here are the basics the population 2 500 hectares of forest land spread over several italian regions and multiple forest types mountain conifer beech dominant riparian lowland plain and mediterranean the exact area of each forest unit and the forest type is already known and mapped in qgis the smallholder group will be treated as a single large dispersed forest sampling design proposal and questions i think a proportionate stratified random sampling design will help me break down the variance between forest types and require fewer sample plots overall i hope i am thinking to stratify based on two criteria 1 forest type conifer beech lowland oak hornbeam riparian coastal etc and 2 the age of the forest recently planted 10 years post intervention 10 years old and old growth 300 years the second criteria is the most important the question is can i demonstrate that management interventions have increased the complexity variance of the forest from a simplified structure recently planted toward a more complex structure old growth i am trying to decide between 1 000 m2 sample plots circular plots with a radius of 17 84m or 1 hectare 50 x 20m rectangular units the smaller sample plots will be much easier to carry out in a work day reducing travel costs and the need for multiple visits to the same site but i don t know if i m creating problems for myself by needing many more sample plots since they are smaller so here s where i m really stuck how to determine the sampling intensity and the number of plots required should i simply set a threshold 5 and then sample proportionally from each stratum even 5 would be a tremendous amount of work can someone explain why not 1 if i can estimate the variance stdev and mean from national forest inventory data should i plug in these numbers with an allowable error margin and confidence interval to determine the number of plots what should i set as the error margin ci to balance accuracy precision with feasibility i can t exactly ask for thousands of dollars to carry out the survey and i only have a couple of months to gather the data is 90 enough what about 50 i can t find clear guidance on this and i think it s more important that i can justify the design based on time budget constraints than meet certain thresholds it needs to be sound but it s also not a funded phd to be published in a peer reviewed paper i hope my questions make sense basically i need to come up with a sampling design for a large and diverse forest i am struggling to find the acceptable margin of error that reduces costs while providing sound data thank you all so much in advance
2,customers purchase intent prediction hi all i have recently begun my ml journey and although it has been challenging its also enjoyable power bi has an ml functionality that i have decided to explore to predict customers purchasing intent it would mean alot if you could spare a moment to see it here bifi any feedback or tips is greatly appreciated 😀
2,how much linear algebra how much linear algebra do i have to know before i can understand machine learning and ai
1,question statistics applied to hotels i have always been interested in statistics and recently i found out about hedonic regressions and used to to perform a 16 variable regression analysis to determine what features increases the guests’ willingness to pay for a room i am sure there are more statistical analyses that i am missing out on i am looking to learn more and if anyone has suggestions any readings please let me know it could be to perform sentiment analysis on reviews predicting occupancy rate optimising room price etc anything to do with hotels
0,eda query hey all i have a question about eda so i ve been working on this project the movie recommendation system my dataset is a pretty standard one now i want to understand how much depth should i go into to while performing eda because after a certain point the conclusions from eda no longer make the ml model better however i can go on making conclusions from the dataset finding relations between all combinations of features when do i stop
1,which variable do i need to omit hi everyone i m working on my thesis and i am running a simple ols regression with multiple dummy variables to control for industry types i e it healthcare etc i need to omit a dummy to avoid a dummy trap but which one do i omit do i just go with the one that is automatically omitted by stata or do i omit the one with almost no observations thanks in advance
1,chi squared test with limited variation i am doing a pearson’s chi squared test to test to see whether or not the upload speeds of the new router compared to my old router however the issue is that there is usually limited variation in cases like this because data speeds are also determinant on things like your internet package and etc i am getting an average of 21 5 mbps on my old router but my observed values are hovering around 25 8 mbps but the probability is like 20 is there any way to augment the test to account for situations that may have less variation than others thank you
2,dataset for spelling correction what would be a good text dataset to train a neural network for error detection task i am currently planning to use the texts from blog authorship corpus however its from 2004 and have a fear it might be a bit outdated any other good recent datasets i am currently trying to avoid wikipedia dataset as it doesn t contain many first person samples
1,what are some public datasets for proportions that would be a good project idea hello i’m doing a statistics project requiring 2 proportions and comparing them does anyone know of any datasets from trustworthy sources that have the data sorry if my english is bad it isn’t my native language
1,how do i create a zscore of variable a based on mean sd of variable b in spss
2,predicting the next day of someone s life hi this is an idea i had recently hear me out what if we registered in a spreadsheet every event of everyday of someone s life after some years with that ton of data we could finally predict the next day now is this possible
1,re running a b test i m a data scientist and we ran a few a b test and 3 of the 17 features tested came out stat sig we really want to be right that these features are good for the product so there was discussion of running a b tests again but only for the 3 stat sig features on an academic level is this a good idea i m having trouble thinking about what happens to type 1 and type 2 error in this case can anyone help clarify this for me
0,calling it now a few years from now data engineering will be just as overhyped and saturated as data science is now i see this on reddit every single day someone saying something to the effect of data science is over saturated and not that interesting i m transitioning into data engineering where there are more jobs i don t blame people for being burnt out on data science and looking to go where the fields look greener but we all see what s happening here right combine this with reddit s and the tech community in general tendency to have a massive hard on for anything engineering and i think we re seeing the beginning of a trend we ve all seen before in a few years we ll all be back here or maybe in r dataengineering saying data engineering is oversaturated that s why i m moving into quantum skunk wrangling or something
1,what test to check for correlation between variable and classification into 5 classes likert ratings apologies if the question seems basic but i am not very experienced in statistics i have collected data concerning different properties of objects e g animals that are classified into 5 classes the classes are likert ratings 1 5 let s say 1 very low weight 5 very high weight so the data looks something like this animal class property1 property2 property3 fish 1 0 5 2 2 2 cat 3 0 2 1 1 3 horse 4 1 2 4 3 each object animal belongs to a class 1 5 and the properties are all numbers how do i test whether there is a correlation between the classification into the 5 classes and a certain property for example if property 1 was volume then surely low volume would correlate to lower classes lower weight and high volume would correlate to higher classes higher weight i could obviously just get the averages for each property per class and then compare to see if there is a trend from 1 5 but that doesn t seem statistically sound if i wanted to check for correlation between any two properties i suppose i would just use a correlation test however i am not sure what to do to test for a relation between the classes and any one property
0,advice on working on personal projects while maintaining a regular 9 5 day job i ask this as i m currently in a career switch and finally coming to realisation that i m only left with few hours after work how do you create time for your personal work outside of your day work optimizing sleep to gain more hours waking early to have uninterrupted focus
2,wu dao 2 0 a new 1 75 trillion parameter multi modal mixture of experts model from china s baai lab with 10x the parameters of gpt 3 it reportedly achieves sota on a number of benchmarks across several domains this is a link to the least politicized article i could find on the topic china s beijing artificial intelligence lab released a statement this past week about their wu dao 2 0 model i haven t been able to find any demos or examples but from the handful of articles i ve seen it s apparently everything you d expect from a model that size
1,math needed to study time series forecasting analysis in grad school hello i’ve been thinking about graduate level studies mainly phd programs in statistics i enjoyed my undergraduate level time series forecasting classes and i want to pursue a research topic within this area of statistics one thing i was warned about by my professor was that while i did well in the undergrad course for time series graduate level time series is “a whole different beast” and said i’d need a ton of extra math with that being said what extra math would i need to study time series forecasting and analysis at a deeper level enough for a phd topic one day
2,what is actually the state of the art in text to speech tacotron2 was released over 3 years ago and to this day is still used as the de facto state of the art baseline against which every modern paper compares their proposed method to and while it showed human parity mos scores on very monotone and almost robotic internal google dataset it isn t able to achieve these levels of realism on more expressive non professionally recorded datasets so what is the actual state of the art on this front i m assuming it s still not tacotron2 because frankly training tacotron2 on libritts has given terrible results for me and i d also assume researchers use tacotron2 as a baseline because it s easy to compare against not because it s actually the best performing so can any tts researcher catch me up on the field what approach actually yields the most realistic speech on real world data
2,contrastive visual representation learning is more robust than you might think paper analysis title revisiting contrastive methods for unsupervised learning of visual representations arxiv code most people assume that current sota contrastive self supervised methods e g moco simclr don t work well on non curated domain specific or long tailed datasets this paper discovers interesting properties about the learned representations and disputes claims from recent works key findings 1 do we need object centric pretraining data no recent studies a b claim that object centric datasets e g imagenet are crucial to learn powerful representations with contrastive self supervised learning they argue that the standard cropping augmentation strategy is detrimental for non curated datasets e g coco openimages however this new paper disputes this claim and shows that training on coco openimages can even outperform imagenet pretraining when compared fairly thus the amount of data is much more important than whether your data is object centric or not 2 do we need priors to learn dense representations no the cropping augmentation strategy already allows the model to learn spatially structured representations this strategy is simple and often outperforms recent methods which proposed additional complex losses at a denser level in the image the representations can be used for semantic segment retrieval and video instance segmentation without any finetuning in fact it enables to find semantic segments from images without annotations so try to avoid priors and let the data speak for itself 3 how can we further boost the transfer performance w r t moco impose additional invariances by exploring different data augmentations and nearest neighbors this boosts the transfer performance on various tasks e g semantic segmentation video instance segmentation depth prediction for example this strategy outperforms mocov2 on pascal voc by 2 4 miou and 4 9 map after 800 epochs of pretraining on coco 4 is universal pretraining solved not yet models that obtain improvements for the downstream classification tasks e g imagenet are not guaranteed to outperform on other tasks as well e g semantic segmentation and vice versa so don t limit yourself to pretraining and finetuning on the same dataset often imagenet this does not paint the full picture this is in line with the findings of another recent study to wrap up it would be interesting to see if these findings hold up when training on billions of images also what is the influence of inductive biases on these findings see the limitations section in appendix g of the paper this work was done on 2 x v100 gpus so it took some time to get these numbers let s discuss further will the future of ml be general pretraining on billions of images domain specific finetuning i believe so it is known that contrastive methods can learn powerful representations on imagenet however i didn t expect it to simply work on the berkley deep drive dataset this came as a surprise to me i thought that datasets from which the classes are easy to discriminate were a must what are your thoughts x200b a purushwalkam s and gupta a demystifying contrastive self supervised learning invariances augmentations and dataset biases neurips 2020 b selvaraju r r et al casting your model learning to localize improves self supervised representation cvpr 2021
1,average probability of a type ii error for a range of values by integrating over rejection region in wackerly mathematical statistics the author states that you can only compute the probability of a type ii error for a single given specific value in the rejection region wouldn t it be possible to integrate over a range of values in the rejection region and find the average probability of a type ii error
0,scalable infrastructure for lonely data analysts with no access to raw data hello ds i work as the only data analyst of a small subscription based streaming company our app is developed by an external company and we have no access to the raw data we receive monthly csv files showing usage per customer we also receive daily updated csv file for subscription data start and end date per user this is also the only data i have access to using these through web api i ve built power bi dashboards and run some analyses on python for usage data at the start of every month i run 2 py scripts that aggregate the new month with the existing data and save as csv file in the data analytics sharepoint drive then i use this data in power bi or in further analyses in python for subscription power bi connects directly to the web to retrieve this data for analyses i have a small function in python that retrieves the most recent data from the web for my scripts and data i have a folder on our data analytics teams group where i save all my py and pbix files i thought this would work okay for the little data we have and it did for the last year but now i m still using a lot of time to run simple analysis on python because i changed some definitions e g which users count as converted i m 100 dependent on power bi because i have no other way of sharing analysis even analysis that would be much easier to run on python has to be on power bi because that s where i have all my definitions aka feature engineering categorizing users etc and it s the easiest way to share analysis with collegues i want to push to get access to the raw data but i don t know how i would implement that in my current way of working i feel like there must be a better way more automatized more structured cleaner way of working but i have no idea how i ve been asking my boss to hire a data engineer consultant temporarily to advise us on this but he hasn t prioritised it yet do you have any advice what would you do in this situation
1,a simple question about a dice game who gets to do the dishes after dinner is often decided by a game of dice where i live oftentimes we play this particular game and i ve been trying to get my head around the statistics of it let me preface by saying if anyone knows the english name of this dice game then i d appreciate the name because someone probably already worked out the statistics here are the rules single dice each player takes turn throwing the dice a maximum of three times the goal is to end the game after a full round with the highest number 1 3 4 and 5 adds to your score 2 and 6 halves your score an initial 2 or 6 is rerolled you can stop rolling at any time before the maximum of three rolls decimal places are included 3 5 for example so these are my thoughts for the initial roll the average toss would be 1 3 4 5 4 3 25 and after this i m kind of stumped because i m having trouble wrapping my head around the halving of the scores so there would be a 2 3 chance to roll an average of 3 25 and 1 3 risk to half your score so far what s the math here there are mainly two things i m interested in here what s the average outcome of one player rolling thrice when disassociated from the other players playing the game when have i rolled above the average roll and would it be advantagous for me to stop rolling what is the average increase in score for the second and the third roll of the dice thank you in advance and if this is the incorrect forum to ask this could you kindly redirect me to somewhere more appropriate
1,how can i take notes and understand statistics in a college level course next semester i will be taking an economics statistics class that is supposed to be quite difficult i did some pre reading over the break in order to familiarize myself with the concepts before the semester and i find myself rewriting 50 of the content in the textbook and partially understanding what s going on there are about 2 or 3 formulas per page explaining the derivations and one or two practice problems one of my primary concerns is that i won t be able to memorize all the formulas to apply them on the exam or understand the concept presented i was wondering if anyone could give me study tips understanding the concepts and how to take effective notes instead of just re writing half of the content in the textbook
1,parametric not sig but non parametric is sig what does that mean hello i am testing diff between means of groups from a questionnaire with likert scale questions i understand the debate between likert being ordinal vs interval which is why i’m including both tests in my thesis the anova p value is always greater than 0 05 however when i run non parametric tests like mann whitney and kruskal wallis it returns a sig lower than 0 05 i thought non parametric were harder to find sig i’m not sure what to say in my results about the conflict if there is one i appreciate your help
1,overparametrized models in statistics everyone says that machine learning models with many parameters tend to overfit that s why regularization e g dropout techniques acts a technique to bring parameters in the model towards 0 thus reducing over parametrization why are overparametrized models bad why do they result in overfitting
0,python r nah here’s a typescript kmeans implementation worked on this a while ago for a few different projects basically just a free open source typescript version of kmeans it may be of use to you if you appreciate it feel free to star the project if you don’t i guess keep scrolling lol
2,introducing distributed xgboost training with ray in the past months we ve been working on a ray based backend for distributed xgboost features include multi node multi gpu training advanced fault tolerance e g elastic training loading from distributed data sources as well as integration with hyperparameter optimization framework ray tune see here for the full blog post and the github link here happy to hear any comments
2,cybersecurity data science an overview from a machine learning perspective in a computing context cybersecurity is undergoing massive shifts in technology and its operations in recent days and data science is driving the change extracting security incident patterns or insights from cybersecurity data and building a corresponding data driven model is the key to make a security system automated and intelligent x200b by iqbal h sarker a s m kayes shahriar badsha hamed alqahtani paul watters alex ng x200b paper
1,what are the best papers books on regression to the mean there s so much mis information on the subject on the web i m doing research that requires me to understand the nuances of the descriptive statistic
2,differential privacy i have recently decided to look into this topic and it seems pretty amazing the basic idea is to anonymize the dataset and add random noise to the database queries outputs the anonymization includes generalizing the dataset with each attribute s specific taxonomy tree i think decision tree can come in handy with its information gain at each level i can find a best split and produce the tree from there the problem is i unable to produce such taxonomy the numeric attributes such as 40 45 47 will be generalized in a range like 40 50 i don t how to produce such ranges and also i don t know how to find a way to merge into the dataset as after replacing the values with the generalized one i will be left with strings of the ranges also i can t find any way to produce a taxonomy for string attributes like jobs such as engineer teacher etc if anyone knows the way around feel free to help
1,do “statisticians” still exist just wondering if that still is a job that exists given the surge of data analytics and data science
0,efficient decision tree pruning in python hi i m working on a project where i need to automatically create a large number of decision tree and to prune them the only way to do that is a function like that def do best tree xtrain ytrain xtest ytest clf decisiontreeclassifier clf fit xtrain ytrain path clf cost complexity pruning path xtrain ytrain ccp alphas path ccp alphas clfs if len ccp alphas 100 nb 2 else nb 1 for ccp alpha in tqdm ccp alphas nb clf decisiontreeclassifier ccp alpha ccp alpha clf fit xtrain ytrain clfs append clf return max clfs key lambda x x score xtest ytest so it take a huge amount of time as it fit a lot of trees see the if len ccp alphas 100 nb 2 i added in order to divide by two the number of trees but it still very slow isn t there another way to do that for example by really pruning a single tree i m not limited to scikit learn but didn t find anything that is doing that thank you in advance
1,has anyone here used the kruskal wallis test when comparing data distributions i’ve been working with 4 categories of data and trying to compute the “h” statistic to get an idea for how truly different each of the distributions categories are i have n 4 categories so 3 degrees of freedom there are several hundred data points values within each category according to what i’ve read online when there are more than 5 samples in each category you can treat h as chi squared now when i compute h i end up with a value of 68 9 according to the look up table of p values for 3 degrees of freedom this is significantly greater than the 99 9th p 0 001 value of 16 27 is my h value reasonable if so what is this telling me that there’s a much greater than 99 9 chance that the 4 categories distributions i’m looking at have significant differences between them is there any way i can take this further
1,can anyone please recommend an algorithm that can solve this kind of problem suppose you have data on bank fraud in graph network format each node is a customer and for each customer you have information corresponding to their account e g var1 var2 var3 the edges represent a family relationship e g brother sister wife husband parent son etc for simplicity sake let s say that that each customer in this data set only has 1 bank account case 1 suppose you have a dataset in which nodes are either group a fraud group b no fraud or unknown you are interested in predicting whether the nodes labelled as unknown are fraud or no fraud what kind of algorithms can be used to predict this case 2 suppose you now have point in time data you have a graph from an earlier time point t 0 and a later time point t 1 many things could have changed between these two time points for example new edges might exist between previously unconnected nodes e g two customers are now married existing edges between connected nodes might no longer be present e g two married customers are now divorced nodes that existed earlier might no longer be around e g they decided to leave the bank and switch to another bank or new nodes might have appeared e g new customers and might form connections with other nodes you have information for each of these nodes e g var1 var2 var3 from time t 0 and time t 1 you are now interested in predicting which customers with the unknown label are fraud or no fraud in this situation what kind of algorithms can you use in general are their common algorithms that can be used for node classification thanks
0,is data science turning into a catch all title for recent ph d grads it seems theres a massive influx of recent phd grads in various fields but especially stem that couldn t cut it in academia or research and claim to be experts in data science but dont necessarily have any qualifications or background in analytics phd in stem congrats you re a data scientist dont worry you dont have to know anything as long as you read blogs on data science central and took an intro stats course you can bullshit your way through the job and nobody will question you because you have a phd right wonder what implications of this fad will be long term for job market
2,clothing prediction based on weather hello all i live in the uk and i never know what to wear the problem is more complicated than what temperature is it is it raining is the sun out humidity wind i ve created a website where people can log how comfortable they are and what they ve got on and the site automatically records the current weather conditions hopefully this will give me a good dataset which i can use to train some ai model and start making predictions if you have any ideas i d appreciate the input
0,how much data should you use in a model this is a concept i always struggled with in statistics is more data always better suppose you 50 years of data about hospital visits you are interested in supervised classification you have predictors such as age height weight blood type salary etc you are interested in predicting if the hospital stay will be less than 1 day or more than 1 day this can be easily solved using random forest my dilemma is using all 50 years of data might be able to capture a wide variety of patterns but since we are interested in predicting future information maybe some of the older data is less relevant and might surpress more current trends how do you deal with this problem
2,are there ml phd programs that also let you work in industry at the same time i m wondering which ml phd programs let you build career capital while you study i think there s a lot of valuable experience industry provides while learning and exploring problems as one does is ml also a lot of the problems with a lot of impact are found through market research on users which you can only get in industry are there ml programs like this
0,is everyone using the same software tools libraries just with more sophistication while i may not be a cs ce i have studied programming for years and in the ds game today the amount of startups solutions platforms being created is insane and as i wrap up my ms of ds and read more on this sub other subs about ds and solutions i think to myself is everyone essentially using the same open source tools just in slightly nuanced ways for example with all these companies coming out with deep learning solutions tools is it fair to assume that at a certain point they are all basically using pytorch tensorflow nnet r then it s up to their development team to leverage these libraries for their applications it then becomes a game of who can use the library best vs developing something new does this make sense i am wondering if these startups majority of these companies do have some secret sauce or if it s all the same at the end of the day another example google releases kubernetes as open source very sophisticated tool for orchestration launches containerization to a whole new level so anyone that offers containerized solutions orchestration is it fair to assume that s what they all use kubernetes is there something unique in what they do or is it more integration of these open source tools what about google they must be using something more sophisticated internally beyond kubernetes or is it just lastly i understand the purpose of open source release it to the community so people pick it up learn your desired skills then you hire them it works i m just asking more on the tech stack side and what makes it unique i am always daunted by solutions in data science but i am starting to think they re all amalgamations of all the libraries i typically use just packaged in a professional way respecting development time too
0,data scientist vs senior product analyst i m in the unique and fortunate situation of having the option of choosing between a data scientist promotion in current company or senior product analyst offer from a new company position for my next role both would be significant upgrades for me but the product analyst position at a new place will pay significantly more i don t know by how much yet as i m waiting for a formal offer from my current company i have wanted to break into ds and get the title for a while but i m not sure if staying at my current company with lower pay is worth the title i know titles and actual work in our field really aren t well defined i have some indication of what the ds work if i stayed would look like some cool ml models recommendation algorithms testing as well as more typical data analyst work with sql and no modeling my preference would be to do modeling and more ds long term but i m also kind of ready for a reset and new company there s certainly some factors i m leaving out but which opportunity would you jump at a ds role with the title and maybe more interesting work but lower pay and company frustrations or a senior product analyst role at a new exciting company with significantly more pay and more typical product analytics work maybe which would you pick is too subjective but what considerations would you make any advice welcome thanks
1,is it possible to cross correlate rainfall data and flood extent basically i want to find out whether upstream rainfall affects downstream flood extent can i do this using cross correlation i wanted to use cross correlation because i think that upstream rainfall will have an impact but after a few days lag period how should i treat my data before performing cross correlation do i have to have the same number of data points in both time series for cross correlation for example i have rainfall data for 30 days but i have flood extent data for only 7 days so do i need to create a third time series comprising of the same days for which i have flood extent data or can i perform cross correlation on the 30 day time series and the 7 day time series i would also appreciate if anyone could point me toward some resources on cross correlation any help or advice will be highly appreciated kind of walking in the dark right now
2,how long of sequence to train lstm gru on wondering if there is a general rule of thumb for how long of a sequence i should train my gru lstm on also is there any research on the upper limit of how far back in time a gru and lstm can realistically look what i am trying to do is create a decoder for my vqvae audio auto encoder using raw audio i know rnns are not the best for generating raw audio due to the extremely long range dependencies in audio signals however my decoder consists of a bunch of convolutions before i expand out to the original length of the audio and use an auto regressive rnn network to finish the decoding this way i think the rnn can focus only on the higher frequency short term signals currently i have a stack of 2 grus with 512 cells each i am training it on sequences of 1024 sampled from the output of the expander portion of my decoder when it comes to actually running this in inference i plan to just carry the state of the rnn forward from one prediction to the next never resetting the state for hundreds of thousands of samples i am unsure if this is a valid thing to do since it was only trained on sequences of 1024 maybe i should run it for 1024 steps then reset the state go back 256 steps for a warm up and repeat this over the full sequence
1,need help choosing best tests to do so i have 2 questions i m doing my master s thesis which involves ct computed tomography head exams and i don t know not sure what tests to do i m studying dental disease in rabbits and i grade 1 to 5 ordered them in quadrants i also identify lesions that are present binomial variable yes or no first question i want to see if there s agreement in grades between mandible vs maxillary and left vs right the test i think i need to do is cohen s kappa but not sure if that s right second question is to relate a lesion with the grades e g one of the variables is abcesses and i want to know if as the grades increases so does increase or decreases the chance to identify them
0,how to move away from creating dashboards i currently work as a data scientist for a govt owned corporation this corporation has different business units under it and i have been working as a ds in one of those business units for an year now i am the only technical person in my business unit so no one in my company understands my role they just call me the data person anyway i have done around 8 9 data science use cases mostly regression problems and some clustering and classification problems for my business unit and the insights are actually being used by the business few months back my boss asked me to assist person x in creating dashboards person x has no idea how to use any visualization tools and is at a much senior position than me so i basically created a few dashboards for person x everyone at my company lost their minds and started praising me for leading them to a data savvy stage with these dashboards i was then asked to share my key achievements with all the business units of the corporation i had to explain 2 of the key data science uses cases and one dashboard as well after the meeting ended i was bombarded with so many requests to create dashboards for all the business units of the corporation i was a little disheartened regarding this tbh i spent so much time creating and explaining those data science use cases but what impressed them was that one dashboard ugh now the problem is that i don t really enjoy making dashboards tbh that was the first time i made a proper dashboard for a business and i didn t bother to research much into the whole dashboard process cause i am not that interested in it i prefer coding things out rather than using drag drop tools most of the work that is formally assigned to me is eda like work which i do using python scripts during my free time i would think of ways to implement data science use cases and start working on it since i will be busy with all these annoying dashboard requests i won t have time to work on data science use cases which is really annoying me how do you think i should approach this the boss of my boss has already dragged me into these dashboard requests without consulting with my boss even if i express my unwillingness to my boss i don t think he can do anything cause of the designation hierarchy right
1,how would i calculate the standard error of b1 b2 in regression model let s say i estimate the following regression model q alpha b1x b2y e i rearrange it to q b1 x alpha b1 b2y e how would i calculate the standard error of alpha b1 i want to calculate the if this value is different from a number p but of course i ll need the standard error also what do i about the distribution check the distribution of alpha b1 or assume normality
0,why do so many people think python is easier to productionize than r i hear this all the time this sub others out in the wild in real life people talking about how python is better for production than r more performant etc why everything i deploy at work is dockerized and imo it s no easier to spin up a container to run python than r python is also not a fast language by any stretch and i d argue that while pandas is faster than dplyr for data wrangling in most cases you probably aren t doing tons of wrangling in production situations where you need things to run fast i don t want to call it a myth necessarily but where does this idea come from
0,text classification for item matching best setup hi there i am building a text classification model to match the name and description of a customer s item e g name suction press nip category paper machine parts to a list of 10k basic items name steel unalloyed category metals i have some initial matched data to test and i will get more and more hopefully i ve build a sentiment analysis program in the past this is a good example of what i used spacy scikitlearn this current problem is more complex though it s 1 to 10k match and not binary or max 5 6 values the string for the item is short and absolutely at the discretion of the source client item log which reads tutorials examples would you suggest to take a look at in python please
0,how do you take notes while reading a statistics book hi it s been a year since i finished grad school and it feels like i need a refresher on the concepts i started reading islr and i felt like i need to take notes so that next time i need to refresh on the concepts i can just go through the notes instead of reading the entire book the note taking is primarily because of potential future interviews do you guys just do it old school by taking notes in a notebook or do it differently now also is there anything else i should do in order to prepare notes for interview prep any other advice is welcome thanks
1,q why calculating quartiles gives different answer depending on the application used so when calculating the third quartile the method i use is n 4 3 where n is the amount of data points if the answer is an integer i take the mean of that positions data point and the value above if its a decimal i always round up and find that value however i m getting conflicting answers my data set is 37 43 43 44 44 46 46 47 47 47 47 48 51 52 53 53 54 doing it with the method above gives 51 as the third quartile typing quartile on excel gives the answer as 51 if i type it into my graphical calculator the answer i get is 51 5 what s going wrong here if i use median and median again i get 51 5
0,disappointing interview experiences i want to commiserate hear about your terrible interview experiences here’s mine that happened a few weeks ago i saw a job posting for one of my most watched companies and it was for a job that was an exact mirror of my resume i had work experience academic research and personal projects done in the exact area and oddly specific tech stack i had worked in i applied on day x they emailed me on day x 1 to set up an interview for day x 2 i was scheduled for a 45 minute phone call to go over the usuals some technical stuff the interviewer called and started off the conversation by saying how they contacted me as soon as they read my resume on the first “tell me about yourself” kind of question i stuttered really bad i have a usually mild speech impediment but it is worse over the phone and the interviewer’s tone immediately changed i didn’t think this was going to be a problem because i briefly mentioned this in one of my emails giving them advanced notice the interviewer basically was like “if you don’t have anything else for me i think that’s fine ” and ended the call after 4 minutes it was a 4 minute phone call that was supposed to be 45 i received the rejection email a little later just very disappointed as this was a company i had built up in my mind and a place where i really could have added value i will be okay as there are lots of other opportunities for me but i was just very upset following this has anyone else had a similar experience or any advice to offer up really not looking for sympathy as this is not a new thing and i will absolutely be more than okay in my career but just wanted to hear from others about their bad interview experiences
0,what type of job would allow me to create useful tools for a data science data analyst team i am currently a data scientist analyst my current job has a lot of analyses and code that could be functionalized and used as an internal python package i am interested in working in a role that is focused on creating packages tools and other processes to help streamline and create efficiency for an entire ds team and less so a role that is actually ds itself curious if anyone knows if there is a certain job title that fits this description
0,what types of questions and coding samples are normal to expect when one is interviewing or thinking about prepping for a data science job interview at a manager or director level i m probably looking to change my job within the next few years after i finish my doctorate my current supervisor has informed me i ll get a bigger salary bump if i leave and come back with the ph d rather than stick it out and try to negotiate with my current company so i assume i ll be at a disadvantage when i go through this process as i ll be applying as a student i m a manager right now and would like to interview for a similar or higher level role i have casually interviewed with a few places in the past and have been frequently surprised at the types of things asked for data science manager or director level roles one company we ll call them swamazon for anonymities sake asked me to code a regex function at the level of write a function that splits the string another company while trying to assess technical skillset went into depth about having me code a group by function in sql neither was very interested in seeing examples of anything more advanced from me either i can t tell if they asked this because they assume someone who manages a team must be an idiot or if i just look like an idiot but curious to hear if this is standard these days what do you guys typically ask when interviewing experienced data science folks
2,working memory hackathon hi all i wanted to share an announcement about an upcoming hackathon you all may be interested in along with the whole brain architecture initiative in japan we re cerenaut hosting a working memory hackathon with a delayed match to sample challenge and an agent that possesses active vision low resolution periphery high resolution fovea that actively moves around the scene we supply a basic architecture and implementation which is based on pbwm there are cash prizes up to 100 000 jpy there will be an orientation event early june hackathon call for participation
0,is this a data science job for real hi i am a cs major i graduated in 2019 and have been working in a company since then when i joined they had just started an ai team in fact my manager was the first data science hire and i was the second one we have been working on creating an mvp and don t have any customers for it yet my manager is more inclined towards the business side of data science so he s good in storytelling in demos working with sales and product management for the requirements etc we don t have any expert for guidance on ml product pipelines algorithm improvement etc so anything i have implemented so far has been by taking help from the internet as a college fresher i got this job through a python interview i even left an offer of 2 5x pay in software development because i wanted to pursue data science i haven t interviewed for any other company since then i don t really know what is usually involved in data science jobs have i gained any useful data science experience in order to get the next job my work involves setting up internal products on demo servers and preparing a story through dummy data insights for customer demos or security conferences 40 designing database tables and creating reports and dashboards for the product initially it was in powerbi then jasper reports and now in an internal product framework 25 developing and maintaining python jobs and integrating model training detection with the main java product through flask 20 exploring drawing analogies and implementing unsupervised ml dl algorithms that can solve the use cases provided by product management 15 is this what is usually involved in a real data science job also i read that you should add in your resume how your work helped in making business value but since we don t have any customers what should i add
2,a review of neural anisotropy directions 2020 by flying scholars paper neural anisotropy directions 2020 by guillermo ortiz jimenez apostolos modas seyed mohsen moosavi dezfooli pascal frossard tl dr it is often said that deep networks prefer simple solutions to complex ones this study elegantly demonstrates that sometimes deep nets prefer complex non linear solutions to simple linear ones depending on the inductive biases in their architecture link to full review in the comments
2,how to use svm to sort vehicle can bus data i have seen several examples of python scripts which make use of svm to sort different types of data using the scikit learn library methods i would like to know if there is a good way to incorporate a stream of can messages from a vehicle and use the svm algorithm to detect can intrusion through malicious messages i will most likely be tracking the types of message headers received and classifying attack messages and normal messages based on that and i would also like to measure the degree of change in certain parts of the messages over time to see if there are any sudden irregularities in the stream of input of seemingly normal messages that could indicate an attack does anyone know a good approach to implementing something like this most code examples i’ve seen either use attributes that are too simplistic or techniques that seem very cryptic to understand i’m an electrical engineering undergrad by the way if that is any help
2,what are the pros and cons of a 1d versus 2d convolutional neural network in a time series problem i m trying to understand from a theoretical point of view what are the pros and cons between a one dimensional and a two dimensional convolutional neural net for specifically for a data set of time series features here we can think of things like your position over time speed and distance and some measure of ground quality etc etc a 1d cnn is pretty common for time series problems and the intuition there is it s collecting a lot of localized representations of time steps by feature information between the features and time as well at first blush it would appear that 2d is better but i guess the con would be that it s potentially underweighting the localized time series within the feature versus across the feature
1,does using an anova test make t tests unnecessary sorry if the question sounds stupid statistics is not my strong point i m writing my master s thesis that concerns 2 independent variables and 2 dependent variables subjects can be in one of four groups exposed to both iv s exposed to only the first iv exposed to only the second and exposed to neither concerning each dv i want to use an anova test to see differences in them between the four iv categories will using an anova essentially get all the same results as conducting a bunch of t tests between two categories at a time would is there any information i would be missing out on by not conducting t tests and just using the anova also is there a difference between the results i would get conducting a two way anova to assess both ivs at the same time vs conducting an anova for each iv
1,dangers of parametric models after doing a lot of thinking i think i am starting to better understand some of the basic concepts behind parametric models vs non parametric models historically it was thought that parametric statistical models with too many parameters e g regression coefficients neural networks with too many weights were said to be prone to overfit training data and generalize poorly to unseen data thus lots of emphasis was placed on methods like regularization how to simplify parametric models with too many parameters this includes approaches like l1 regularization pushes some parameters heavily towards 0 l2 regularization generally pushes all parameters towards 0 and drop out randomly cancelling some of the weights within the neural network apparently these problems contributed to the popularity of non parametric models non parametric models e g kernel based models such as svm support vector machines and gaussian processes e g gaussian process regression these models do not have parameters per say for instance gaussian process regression directly estimates the response variable by repeated simulation using conditional expectation formulas if you look at the estimation formula used in gaussian process regression there are no beta coefficients unlike standard regression somehow this absence of model parameters are desirable for statistical modelling seeing as this somehow mitigates potential overfitting and poor generalization all this is supposedly implied in the famous bias variance tradeoff simple models are said to be stable but are too simple to sufficiently capture complexity within the data complex models are able to capture complexity within the data but are said to be unstable poorly generalize machine learning is apparently about trying to make these complex models more stable and generalize better here is my question what initially lead researchers to believe parametric models with too many parameters are prone to overfit is there some mathematical formula that showed some relationship between the number of regression coefficients and error or variance or some formula showing the relationship between the number of weights in a neural network and the error or variance or was this all empircally observed i am curious to see the initial justifications and math formulas that first started to warn researchers about the dangers of having models with too many parameters note i am aware that models with too many parameters aren t necessarily doomed to generalize poorly apparently models like gpt 3 famous natural language model developped by ai researchers are said to have millions of parameters neural network weights and perform incredibly well in the real world however i am more interested in the general idea and mathematical justification relating to potential poor model performance linked to overparametrized models why were overparametrized models said to be more prone to overfitting is this really why non parametric models became popular because the absence of parameters made them more flexible and less prone to overfit is this all empirical or is there math behind it thanks
1,classifier technology and the illusion of progress 2006 hand i found this interesting paper over here where the author argues that more complex algorithms e g deep neural networks do not always have significant advantages over simpler algorithms in the real world hence the illusion the author brings up many reasons as to why this can happen some of the reasons are related to mathematics others are related to experimental design note the author brings up a point here that i am not sure why this is true conversely in the two class case although few real data sets have exactly linear decision surfaces it is common to find that the centroids of the predictor variable distributions of the classes are different so that a simple linear surface can do surprisingly well as an estimate of the true decision surface why is it common to find that the centroids of the predictor variable distributions are different why does this allow for a linear surface to estimate the true surface well here were the thoughts i had after reading this paper this was paper was published in 2006 before the deep learning revolution e g in 2012 when convolution neural networks clearly outperformed humans at the imagenet competition is it possible that the results from this paper are somewhat irrelevant and outdated researchers universities and companies e g google facebook microsoft have probably spent a billion dollars since 2006 on developing more and more complex machine learning models using common sense many of these models have performed well enough so that more research will be done in the future i agree that for certain problems perhaps simpler models e g linear regression decision trees can perform just as well as deep learning models but surely there are many problems in the real world which require more complex models can an argument made as to why complex models are required using concepts such as the vc dimension relating to problems such as the initial x or perceptron problem could we not say that big data data with many columns and many rows is less likely to be linearly sepperable i e harder to shatter shatter classify perfectly compared to smaller datasets could we not say that if there are more data points there exist more configurations that these data points can be arranged in making it less probable for them to be analyzed using a simpler model the vc dimension of a simpler model is lower than the vc dimension of a complex model does this fact alone somewhat justify the need to develop complex models
2,how has ai contributed to dealing with the covid 19 pandemic after months of drafting and editing we at the gradient are happy to share a new article we are proud of how has ai contributed to dealing with the covid 19 pandemic this piece reviews where ai research efforts have been realized as practical solutions i e being used by corporations governments or individuals practical in this case refers to being used in the field be it on an individual or collective level five general categories are used to distinguish the different contributions 1 clinical applications 2 epidemiological applications 3 applications for biochemistry 4 providing information 5 safety assessment the tldr is that the author believes the pandemic has strongly benefited from all the work that has been done on ai and that a future pandemic shall strongly benefit from ai and will do so even more widely and more extensively feedback on anything we missed is welcome this is a large topic to address so it may make sense to public a supplementary piece we ll see
1,typical difference in depth between neural networks and deep neural networks i know this might sound like a silly question but when people use the term deep neural networks is there a minimum number of layers neurons required for a neural network to be called deep
0,why does this linear transformation produces any eigenvector supposed we have a linear transformation a 2 1 1 2 why does when i use linalg in python and input this matrix it produces eigenvector and eigenvalues when this linear transformation has both changed the direction amplitude of the default coordinate system
0,linkedin open sources ‘greykite’ a time series forecasting library linkedin recently opened sourced greykite a python library originally built for linkedin’s forecasting needs greykite’s main algorithm is silverkite which delivers automated forecasting which linkedin uses for resource planning performance management optimization and ecosystem insight while using predictive models to estimate consumer behavior data drift has proven to be a great challenge during the pandemic in 2020 in such a situation predicting future expectations is challenging as well as necessarily helpful to any business automation which allows for repeatability can increase accuracy and can be used by algorithms to make decisions further down the line according to linkedin silverkite has improved revenue forecasts for ‘1 day ahead’ and ‘7 day ahead’ and weekly active user forecasts for 2 week ahead full summary github pypi paper
0,what are the best data visualization tools sorry i’m new and can hardly explain what i’m looking for basically what i want to know is what are the best network visualizaton tools software i’m trying to build a data visualization that represents networks kind of like bicycle tires and spokes id be interested in any software that lets me make an interactive diagram of different networks represented in an interesting way and showing how networks connect to one another
0,do any of you actually regret not doing a phd in statistics data science hello my goals are to work as a data scientist in industry at this point i’m just kind of unsure if i should pursue a phd or not my goals are to just be a data scientist in the industry for any of you in industry and i’m sure this is also based on the specific industry type do you guys regret not getting a phd in statistics for your job my plan is to get an ms in stats and work but for any of you did you regret not finishing through after your ms if you do have regrets what are they and how had the ms limited you if any
1,when statistics asks about itself hi looking for critiques to my blogpost it s all self reflective statistics questions and the contradictions that can arise from self referential questions like it
2,unsupervised learning with audio data i had a probably crazy idea for a project and i was wondering if you all think it would be in any way possible i m interested in analyzing sounds made by different types of animals for example bird songs or the croaking sounds made by different species of frog and looking for relationships between the sound and other factors like habitat or taxonomic classification if i could obtain digital recordings would it be possible to feed the data into a clustering algorithm in such a way that it could identify and compare important characteristics my fear is that most algorithms would prioritize extraneous details related to the quality of the recording or the type of compression over potentially meaningful factors like pitch timbre and pattern a google search uncovered a few possibly related articles this one proposes a k medioids approach but seems to focus on computer engineered sound waves rather than real life recordings this one uses a hierarchical algorithm and has a lot of good discussion on data cleansing and extracting low level descriptors to use as potential model features however the focus is on classifying music which is obviously a much richer feature space than what i m considering i have very limited knowledge in this area and would have to do a lot more research on data prep feature creation model selection and so on but just wanted to ask if this is completely insane before i go too far down this rabbit hole
2,neumann series for matrix inversion and its approximation have you heard of the neumann series 🤔 neumann series for matrix inversion and its approximation 6 min 45 sec x200b a short summary neumann series is the matrix generalization of geometric series which can be used for the approximation of matrix inversion but the caveat is that the contraction condition and the time complexity are important for successful applications as a real world example we briefly show that the neumann series can be used to optimize the millions of hyperparameters in deep neural networks through the work from lorraine et al aistat 2020 x200b cheers
0,how does everyone share their models etc across teams for re use effectively does anyone know of any good tools for knowledge sharing post fact we ve tried this but found it came up a bit short
1,forecasting principles and practice or time series with applications in r hello which of these books is more suited for an undergrad in a stats major hyndman’s forecasting principles and practice or time series with applications in r i’ve taken calc 1 3 and some linear algebra which one would you all recommend for an undergrad
1,need help figuring what test to use dissertation hi there for my dissertation research i made a questionnaire surrouding university student attitudes to immigration which i intend to analyse using spss however i m stuck on deciding how exactly i analyse the data any help
2,multi object tracking mot which are the best mot models available papers with code has a few top methods but the github repo or the corresponding models are unavailable
2,how to create a model like bert or gpt i am a sophomore and have studied ml and dl for last 6 months recently i started working on nlp i was wondering how these models bert gpt are created following are some questions 1 is it possible to create my own model i was thinking of making a hybrid of encoders and decoders of transformers 2 is it even feasible to create a model at my stage 3 how much time would it take to create it
0,did anyone here get their company to pay for their masters in a field of data science if so where did you work and what did you do just curious
2,gpt 2 annotated paper paper summary the gpt 2 model was a major breakthrough in the path of creating a general multitask nlp system that was totally unsupervised it demonstrated that given a large training corpus and a large model size the language model was capable of learning the knowledge required for solving these tasks it was not perfect however and performed poorly on some tasks as well i went through the paper and have written an informative summary of the paper the paper was quite easy to follow and the experimentation section had interesting observations check out the links below and happy reading paper summary language models are unsupervised multitask learners annotated paper
2,modern machine learning models for time series analysis does anyone know what are the most modern statistical models being used for time series analysis i have heard of transformer and attention mechanisms models that are used for modelling sequential data but these seem to be more relevant for modelling data from the nlp domain when it comes to classical time series modelling e g a vector of temperature measurements does anyone know what are some of the more modern models being used for this i did some searching online it seems like arima style models were some of the first ones followed by state space models hidden markov and the more recent ones being rnn and lstm are lstm and rnn the most modern models that are being used for classical time series problems thanks
1,pressed into upgrading my thesis to a meta analysis deadline in 2 weeks hey guys i m a university student from europe at the very end of my studies feel free to skip this bit but i feel some background info might be helpful for my degree our university demands we write and present a research work done by a student and lead by a teacher i found a nice teacher with plenty of published research met with him and he accepted me as his understudy and charged my with writing a systematic review on the efficacy of a surgical intervention on pain relief in patients with a certain illness i couldn t be happier at the time fast forward some months and the situation has changed dramatically the teacher ghosts me for weeks i often have to send the same email 2 3x times takes weeks to reply with useless information can t be bothered to read my drafts basically i ve been pressuring him since february to read the thing i m still missing part of the results portion of the thesis but i need help to go forward and finish the work so i can submit it to the university and present it i must do this to finish my degree last week we finally had a meeting he skimmed through my draft and non chalantly said that so far so good and that i have carte blanche to finish the results i told him i needed help with that hence this meeting but he replied i needed to do it on my own he also upgraded the research design to a meta analysis knowing full well i don t have the necessary knowledge to do it alone i was shocked for a second and after regaining my composure i told him for a second time that i needed help he dismissed me saying i needed to do it myself i don t want to sound like a drama queen but i never felt so abandoned in my life 🤦 i have considered charging a complaint but nothing useful would come of it in time i have to deliver this paper corrected to the university by mid may i have done research and introduction methods and prisma flow diagram data extraction to an excel spreadsheet number of patients outcome etc now i m stuck i ve recommended to use cochran s review manager 5 to do the data analysis but i have no clue what to do next my outcome is whether or not a given pain threatment is effective but i don t know how to take it from there and i ve only got 2 weeks split with classes and exam prep to do this got any clues on how to proceed thank you so much tl dr thesis got upgraded to meta analysis have no idea on how to move forward after data extraction
2,anyone here who can comment on the professional ms ml course provided by mila i have been selected for ms cs in bu and two canadian universities for canada my option is ms cs at concordia and professional ms ml program at mila since canada has friendlier immigration policies i am leaning more towards it and concordia is not exactly known to be an ml powerhouse so that leaves me with professional ms ml at mila can somebody who has gone through the program comment on it s quality i have been offered a scholarship so my costs would be much lower in mila but i ve heard mixed feelings about the course online can anyone who has gone through the program give me an idea about how good the program is my goals right now are strictly professional but i may look into research in the future
0,how often do you find yourself performing analytics engineering duties not necessarily data engineering etl elt work but architecture and development work inside of the database dwh
2,why is using tpus with tensorflow so hard i have been trying to convert my tensorflow model to one that works with tpu but can t seem to do so i have been trying to do so over a month now i am getting this error invalidargumenterror unable to parse tensor proto so i used a takedataset of 10 reduced the batch size to 16 and reduced the image dimensions still the same error what is wrong are tpus worth the hassle of conversion in terms of speed
2,taxonomy issue what is the opposite of learning based methods hi i work in robotics unmanned vehicles etc and i have the following doubt while i am preparing a presentation how would you call the class of systems which is not using any learning for their functioning an example to understand what i am talking about is computer vision there are ways to detect edges in an image which are based on image moments and stuff like that other methods are entirely based on cnns and received a lot of attention what i am trying to say in my presentation is i know that to solve problem x we could use learning based methods but i won t discard looking into and using methods if i find that they are more effective i am thinking about the following analytical methods not sure it is correct formal methods maybe too narrow classical methods too broad some other ideas thanks
1,a simple and concise introduction into the relationship between bias variance overfitting generalisation in machine learning models i wrote an article where i explain as simply as i can the essence of the bias vs variance trade off that plagues every machine learning model i then go on to link this to overfitting under fitting and generalisation using clear visual aids i think it s a decent introduction to the concepts so hope it helps someone
0,online machine learning or how to automatically update your model in production i m trying to find resources to learn more about a new subfield in machine learning called online learning the idea is beautiful and powerful your model in production trains itself with new latest data to react to changes faster however the classic ways to build the mlops infrastructure and algorithms maths won t do the job here so i m eager to learn more i ve found this post by standford s ml lecturer chip huyen to be a great introduction to the concept of online learning i ve found river to be a promising python library for online learning apart from that i don t know many resources out there do you any blogs to follow any titanic equivalents a simple problem to get going
2,pornhub uses machine learning to re colour 20 historic erotic films 1890 to 1940 even some by thomas eddison as a data scientist got to say it was pretty interesting to read about the use of machine learning to train an ai with 100 000 nudey videos and images to help it know how to colour films that were never in colour in the first place safe for work non porhub link
1,math theorems behind machine learning can someone recommend a source e g website blog book youtube channel that discusses the math theory behind machine learning but in such way that it s not pure math something detailed but not heavy in pure math
0,a checklist for professionalizing machine learning models i m curious to hear what people think especially about things i missed in my list i m sure there are some
2,what are suitable computer vision projects that can be implemented in office environments i was brainstorming with a bunch of friends we were wondering what are different use cases computer vision tasks in an office environment 1 social distance maintenance through video analytics can also be used for monitoring cigarettes alcohol etc in cafeterias 2 facial recognition application what are some other useful applications of computer vision tasks in an office environment looking forward to hearing from the community
2,dataset or simulator for sea waves marine maneuvering we want to mess around with autonomous maneuvering at sea this regards waves winds and currents the navigation part is easier are there any datasets simulators to start with many thanks for any info
0,your experience with knime hi everyone i was scrolling feeds of the group and did a quick search for knime it actually surprises me how unpopular as a platform is considering that the last post was a year ago i have started to learn more about knime required for job and wanted to see your thoughts on the platform based on the experience you had is there any substitute that does a better job than knime and this is the reason why it is not very popular any opinion is helpful
2,how do we use probability in data science and machine learning i have been studying data science and machine learning from past few months from various online sources i have built few projects also by using some github file as reference but i failed to understand the explicit use of probabilitiy in it can anyone help me understand this with example or provide some good source to learn this thank you
2,which machine learning model can achieve this effect？ this is a residual based anomaly detection model in the field of internet of things the red line is the true value of the sensor the blue line is the predicted value of the model and the yellow line represents the residual according to a certain residual threshold it is determined whether an abnormality occurs this graph is just one of multiple sensor data what i am puzzled about is the predictive component which has two typical characteristics 1 perfect prediction within the range of training data 2 the predicted value does not exceed the range of the training data i think that multiple inputs are used as labels at the same time in this model in the training phase i tried multioutputregressor decisiontree fit x x using sklearn framwork but the fit is not smooth enough which model or method can achieve this effect x200b
0,will my internship influence my career trajectory i got a data science internship and i ve just started i ve been in the role for about two weeks it s a research and development role for the company s product i m honestly really enjoying it the role is capturing meta data from data science tech stacks like google aws microsoft etc we get to build projects with the technology so there s free reign to do whatever i want to which is a lot of fun i get to use a lot of the new or popular tools data scientists use but i was wondering if it s detrimental that i m not actually creating models for production i ll hopefully be very proficient with popular cloud services offerings but it s more of knowing how to create pipelines setup services and a lot of the auxiliary things surrounding models like i previously said i do honestly enjoy the role it wasn t what i was expecting but i get to learn a lot at a manageable pace which is nice
2,discussion phonemes alignment in automatic speech recognition asr hello everyone i am really scratching my head to get this topic inside my brain it s obvious that even stackoverflow and google can t give me something satisfactory on this so i could use your help guys i know about nn cnn rnn lstm hmm and other heavy terms used in deep learning but still can t connect how exactly this works phonemes alignment it will be so helpful if you could provide me something on this thanks in advance
1,question statistics on manifolds i ve ran into a problem where i have data living on a riemannian manifold i ve looked around but didnt find any good resources how would onr go about defining mean sd etc as my manifold is not linear i can really define addition etc
1,what kind of job would i be able to score in data science analytics this is an unorthodox question in this thread but i ve been thinking lately so here we go basically i m a ph d candidate in a humanities social science field but since i ve been doing experimental work for more than 5 years i ve had to delve into applied statistics and analytics fairly deeply because the methods i needed for my actual work were not covered in the university curriculum lately i have been considering or rather playing with the thought of switching careers to something more lucrative than my original field and trying to get into data science would seem like an obvious choice given my background however since i do not have a formal data science statistics degree i m wondering what kind of data science analytics jobs would i be able to get if any with the following skillset which i can demonstrate at any time at a job interview despite having no papers and what kind of payment would i reasonably be looking at i am fairly proficient in the use of spss around 6 years experience doing own research and tutoring others doing data manipulation preparation cleaning essentially every kind of statistical test spss is capable of besides neural networks i m not a ml guy i have experience using r around 3 years on and off and with some online help and example code i can do whatever i need that spss cannot do cumulative logit models dwls factor analysis visualization whatever but i admit i m slower in r than i am in spss i would say i m an intermediate user i have some experience using python studied it as part of a compulsory course at university but i have never followed up on it and forgotten most of it i would probably be able to brush up on it in a few months if it was really required but i would prefer to use r or spss since i never really liked python compared to c which i also had to study i have substantial experience with ibm amos for structural equation models cfa i have working knowledge of excel word obviously i m somewhat more familiar with the latter but not latex i have been exposed to sql very briefly as part of a summer crash course but i wouldn t say i m an independent user in any way i am not extremely good with the pure theoretical mathematics mathematical statistics part well i m not a statistician by training but i have a fairly solid understanding of the basics of probability linear algebra and i would say i have a better conceptual understanding than the majority of applied researchers coming from a non stats background with a cookbook approach to stats what other skills would i need to hone to stand a chance at scoring a job even something like junior analyst at a decent company thanks for the feedback
2,rise and decline of regression models we all know that in the past century regression models linear and non linear were very popular for data modelling and we also know that neural networks have become synonymous with the go to algorithm for modern data modelling does anyone know at what point did this transition happen between regression models and neural networks was it the simple fact that regression models require strong assumptions to perform well e g distribution of errors directly specifying interaction terms within variables as well regression models with too many coefficients parameters are known to overfit the data i never understood why regression models with too many coefficients are known to overfit is this empirical or is there math behind this on the other hand was the rise of neural networks based on the fact that neural networks did not require statistical assumptions and for interaction terms in the variables to be manually specified did neural networks show any promise of not overfitting were there any arguments that suggested neural networks weren t as likely to suffer from overfitting compared to higher order polynomial regression models thanks
0,paying for personal projects and homework in my spare time i do some freelance work as a side hustle on some of the bigger platforms out there the amount of da ds stat and etc students that are posting their university homework and projects is preposterous it ranges from basic stuff like running simple models and cleaning datasets to big capstone projects i decline every interview from these types of students that really ramp me up and beat the whole purpose of studying they don t know the basics and have gone into the field just because they ve heard that you can make money there not saying that they can t learn it but going into a field without a deep interest or passion for it on average breeds bad practitioners have you run into people like these what is your opinion on this matter
1,is there a way to properly average bootstrap confidence intervals from different samples i have a deep learning classification pipeline in which i had created 10 independent train val test splits the pipeline uses large images which must be broken up into tiles which are assigned the label of the original image i have a script which gives me the auc and 95 boostrapped confidence interval on a given test set after running on all 10 splits i have 10 aucs and 10 cis i was wondering if there was a proper way to aggregate these and generate 1 single statistic auc and ci intuitively i would think to get the mean auc the mean lower bound ci and mean upper bound ci but i am not sure if that is correct and what to call that if it is correct
2,train a gan and keep both your kidneys hey guys x200b a while ago i trained stylegan2 to generate artificial overhead imagery on a dataset of aerial imagery of italy which i compiled it was a fun project and the results are kind of neat so i thought i d share the process x200b link to post x200b this is also my first time writing a blog post and posting it publicly which was arguably harder than training the actual network i d love some feedback on writing style and content
0,can someone please explain what the white color shades mean in this picture these pictures are supposed to show the decision boundaries of different machine learning algorithms on a binary classification task there are two classes for the response variable red and blue shouldn t all the decision boundaries either be fully red or fully blue what do the shades of white mean does this mean an overlapping decision boundary thanks
0,what are some good software engineering practices that all data scientists must know we all know that a significant portion of the value added by data science comes from having good data infrastructure and model deploying which are more related to software engineering than math statistics if one wants to be as well rounded as possible what are some of the best software engineering practices things like version control that are a must
0,differences between de and ds actual job along different companies based on my experience role titles such as data engineer and data scientist mean very different things depending on the company i see three main cases 1 normally bigger and older companies des do all mlops they build anything needed to gather and store data they put ds models into production too dss train models do exploratory analysis create variables validate hypothesis but don t put any of that into production themselves 2 normally modern tech companies des gather and store data dss don t just train but also deploy their models into production 3 normally smaller companies one profile does it all i work now for a startup whose product is a realtime end to end platform which eases the otherwise complicated mlops stuff i m writing some docs that explain how realtime is done now vs my company proposal and i m finding that defining how things are done now is not straightforward so do you agree with the three cases above would you add more where would you put data architects ml engineers etc
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
2,gpu memory is used but not its computation power i am running a single shot learning model which needs 3 gpu right now to run my model however when i check the gpu utilization using nvtop i see that memory is used completely but its not using gpu all the time i can see the sudden spikes in gpu usage graph but its just lasts for 10 seconds what can be the reasons for this data i am loading using torch dataloader with gpu
0,finding freelancing opportunities i m pretty happy with my full time job as a senior model developer but i do a lot of analytics on model results as well including the steady paycheck and job security it comes with but sometimes i don t get to do the most interesting work so i think about getting into part time freelancing i had one person in my network pitch me an app idea for his small business i had a conversation with their ceo he seemed super interested pitched me some requirements answered some questions i then roughed out the specific work required how much time it would take and used 50 hour as my baseline pay needed even though that s below what my regular job pays normally i d need more for working beyond my full time job because it would require sacrificing my social life but the project was extremely interesting and could have led to a good relationship and more work down the road but when he saw my estimate that it d take 200 hours to complete aka 10k he said my estimates all made sense but thanks but no thanks the fact that he didn t even attempt to counter makes me think he was expecting to pay like 1k for it even though the guy who pitched the idea to me said that for 10k they d recoup costs in a few months because they paid contractors to manually do the work my app would have automated plus they lose money on mistakes the contractors regularly make x200b so my question is what s the market like for work like this are people actually able to build custom data driven apps where they re paid 5 figures for completed projects or is my experience more typical where there s just a sticker shock what kind of clients are paying this how do you get your foot in the door would a good github or personal page with personal project examples be valuable
1,reporting lin s concordance coffiecent apa style does anyone know how to report lin s concordance coffiecient in apa style sample ccc output ccc x y ci z transform conf level 0 95 rho c est lwr ci upr ci 1 0 2795453 0 1218944 0 6023843 s shift 1 1 213954 l shift 1 0 2221029 c b 1 0 9582959 blalt mean delta 1 28 0 2 2 28 5 1 3 24 5 1 4 28 5 3 5 25 5 5 6 24 0 4 7 31 5 3 8 26 0 6 9 25 5 3 10 25 0 10 11 25 0 2 12 21 5 9 13 27 5 1 14 27 0 0 15 26 0 6 16 28 0 2 17 29 0 6 18 30 5 3 19 20 0 0 20 30 5 1 21 26 0 0 22 28 5 1 23 28 0 0
2,machine learning applied to game theory resource hey friends you probably get posts from people not in the field who are curious all the time so apologies i m looking for any resource i can on tools applications companies or resources that apply an ai machine learning approach to game theory problems for example in a covid like situation where there is a production limited good i e the vaccine and there are various groups of people with different beliefs 1 the vaccine will be lifesaving 2 it may be lifesaving but was developed too quickly etc etc etc i m interested in modeling different outcomes given different starting conditions i won t continue to use the vaccine analogy as it starts to break down here it s just for illustrative purposes so with that being said i d love even simply being pointed in the right direction for where to look for a tool application company or other resource
2,read eeg in plain text i m trying to find a way to read raw eeg data in plain text but it s difficult i thought of assigning words to eeg numbers like adding a full story tale to eeg by order then change words to numbers then train a model with eeg in one column and numbers that represent words in the second column the eeg to train represents a story the idea is to train it as a story then add new eeg and ml will create a new story by finding how eeg can be a story with the model that was trained previously after training we use new eeg file and we generate the numbers that represent words then we change the numbers to words and it s expected to be a complete and readable story that makes sense to read
2,deep learning frameworks inference speed compariosn hi everyone i m currently working as a ds in a small company we re dealing mainly in web search and recommendation areas we want to start utilizing dl for ranking and ctr calculations and we re debating on which dl framework would suit us best the most important thing for us is fast inference we tried googling for benchmarks comparison but we found inconclusive results everyone agrees that the low level apis have the potential to be faster but between them every comparison favours different framework other things that we care about include training time development time debugging complexity and flexibility in building the model while these things are important inference time is a necessary condition for our product does anyone have experience with the different frameworks and can help us understand which direction we should go thanks a lot in advance
1,how do models like arma and arima fare against sporadic memory is it fair to assume that standard time series models like the arma and the arima model are not well designed to handle sporadic and irregular memory patterns as i understand these models are usually used to handle data with well behaved notions of trends and seasonality e g you specify these in a given arima model when you start to deal with more complicated and irregular patterns do arma arima models tend to perform poorly was this the motivation for eventually moving towards neural network based models e g rnn lstm for time series analysis thanks
1,example of distributions which are difficult to sample from i am always confused when it comes to learning sampling algorithms techniques because they refer to distributions that are difficult to sample from for example the metropolis–hastings algorithm is a markov chain monte carlo mcmc method for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult wikipedia or importance sampling as well in all examples we use well defined distribution like a gaussian to test the algorithms which i understand fairly well i guess we are assuming the existence of a distribution that we can just evaluate but not sample at random from it is this what it would mean in general
2,new parallel computing framework hi i m in the early release stage for my new python workflow dataflow compute framework that makes writing parallel python processes simple and transparent it has cpu gpu scheduling containers hardware parallelism concurrency extensible localized or distributed etc very lightweight have a look comment or contribute with entangle you can run simple hardware parallelized code with conditional logic that looks like this there are some ai ml examples as well python result add add num 6 two if false else one subtract five two print result
2,minimum number of devices for a federated learning environment hi all i am currently researching federated learning on tinyml i would like to know the minimum amount of devices you would suggest i have for researching i m currently working with 2 devices would it suffice if not would you suggest i emulate a few raspberry pis or purchase a few extras
0,on your first ds project is it normal to ask a lot of questions or have a hard time understanding some of the business process aspects hello i m working on my first ds project where our goal is prediction sometimes i find myself having a hard time understanding a concept related to business therefore ending up asking questions from the person who gave me this project different time zone doesn t help and all the conversation is through email making it harder to communicate i don t set up meeting since i think i shouldn t waste someone s time and understand what i can through email only usually here s how it goes i send my eda results then the guy will reply with some more eda stuff i need to do so sometimes i don t understand that requirement and end up asking a few confused questions related to the requirements for people who supervise is there anything i should change or do you encourage someone like me a fresher beginner in this field asking questions is it possible that i ll end up irritating the other person for what it s worth i m not officially a data scientist just been working on a project because of my interest and having to study statistics in my masters thanks
2,loss function in generative models say i have training data x e g texts images a parameterized model m θ that generates those kind of data e g an rnn usually such model is trained with a loss function such as bceloss m θ x for good reasons however has any one done xxxloss f m θ f x that is what if i am interested in generating images that have a similar saturation level here f image would output the image s saturation as a scaler or generating sentences that have a similar anger level here f sentence would output the sentence s anger level as a scaler assuming we are given such function f as a differentiable blackbox would greatly appreciate some reference of papers doing this or on why this is hard to do thanks
1,data before models but problem formulation first formulating business tasks as data science machine learning problems is the hard part of statistical modeling not training and knob twiddling it s also where the magic happens and what distinguishes a senior data scientist from a junior data scientist what do you think
2,what left field approaches to ai do you know of over the years i ve come across the occasional alternative approach to ai huge efforts creating comprehensive hardcoded domain knowledge alternatives to neutral nets etc but i regret not bookmarking them so the above descriptions is all i remember do you guys know of interesting quaint but serious efforts of doing things really differently it would be nice to eventually get an overview of all the wierdness out there edit thanks everyone interesting stuff so far does anyone know the two examples i was referring to i d love to find them again one was a professor that as a alternative to gpt like nlp was handcrafting a huge database of concepts and how they related to eachother i read about this 2 3 years ago and the effort was ongoing the other one was a machine learning alternative to neutral nets i think it had a 3 letter acronym with an m and a c in it it also had one lone professor flagbearer can t remember much about it but it didn t fit into anything i had heard of i can t remember the general premise not svm on anything that common it was mid 2000 s tech
0,forecasting employee turnover with circular reference hi all i posted a question yesterday asking if anyone had any thoughts on how i could predict the rate at which employees in my would the company thank you so much for the help i ve manage to get a working model that does it based on my current employee base but when new employee s join i haven t quite got it working there what my analysis showed is that on average 5 of all new employees we get will leave in the first week due to this we should onboard an additional 5 here is where the circular reference starts we therefore onboard 5 more and so our attrition rate increases meaning we need to onboard more has anyone come across an issue like this before what assumptions can i take to stop the constant circle any thoughts advice would be really appreciated all
1,how to calculate statistical significance for percentages hi i conducted a survey in which participants had to chose between 5 options i am trying to figure out if any one option was chosen at a rate significantly higher than chance which would be 20 and i m not sure which test to use it s been a while since i was in a stats class there were only 36 people in the sample as a whole so i have both the counts for each of the 5 options and the percentages of each and i m just not sure how to proceed for example one of the options was picked 38 of the time and i need to figure out if that is actually significant thanks so much
2,bert experiment variance when training bert variants or probably even neural networks in general the performance varies a lot depending on random initialization the extent of this ofc depends alot on the architecture size of dataset training procedure etc but in the end there will often be a significant variance especially when comparing on benchmarks where you are like 1 2 better than the competition now in academic papers i took it for granted that the authors perform multiple runs with the same configuration and report mean and std recently i have seem many papers that don t do so or dont mention it even from faang one example is the recent fnet paper 1 do they actually perform multiple runs and just report the mean without mentioning it if so why not include the std as well 2 if they dont perform multiple runs how can one rely on the results i mean i can prove my great innovation by just running experiments until i get a lucky initialization even if it is complete trash 3 what are best practices for this how many runs per configuration should one perform to be confident about the result is the std of the performance interesting as well
1,does anyone know why decision trees can only make rectangular partitions in the data i have often heard that the decision tree algorithm can only make rectangular partitions in the data apparently these rectangular partitions i have heard these rectangular partitions being reffered to as linear hyperplanes are the reason why the decision tree algorithm is unable to capture irregular decision boundaries within the data conceptually i can imagine that the decision tree makes binary splits at each node in a 2 dimensional grid these splits will carve up the grid into mini rectangles all these binary splits constitute the decision boundary corresponding to the trained decision tree algorithm but at the end of the day is this sufficient justification for why the decision tree makes rectangular partitions
0,i researched the origin of unlimited pto at netflix and wrote up a case study unlimited pto paid time off some love it others think it’s a scam but it’s worth exploring why this policy was implemented in the first place and for that we go back to the early days at netflix it’s 2003 netflix is galloping along in pursuit of blockbuster there’s a buzz around the office the chase is on and an employee asks we are all working online some weekends responding to emails at odd hours taking off an afternoon for personal time we don t track hours worked per day or week why are we tracking days of vacation per year reed hastings ceo of netflix doesn’t really have a great answer after all he’s always judged performance without looking at hours get the job done in 1 hour or 10 hours doesn’t matter as long as you re doing good work hastings also realizes that some of the best ideas at work come after someone’s just taken vacation they’ve got the mental bandwidth to think about their work in a fresh creative manner something that’s not possible if you’re clocking in and out without any rest so hastings decides to pull the trigger he introduces netflix’s no vacation policy which puts the onus on their employees to decide when and how much vacation they need to take in his book no rules rules hastings describes getting nightmares when he first introduced this policy in one of these nightmares he’d drive to the office park his car and walk into a completely empty building those nightmares minus a few blips which we’ll get to in a bit never really materialized the policy was a success and soon other companies in the valley started copying netflix everybody wanted the best talent and implementing a no rules vacation policy seemed like a great differentiator except that the same policy which worked so well for netflix wasn’t working for anyone else other companies found that after implementing an unlimited pto type policy employees paradoxically started to take less vacation they would worry that their co workers would think they were slacking off or that they would get left behind come promotion time hastings was surprised after a bit of digging he realized the reason behind why these policies had failed the leaders at these companies were not modelling big vacation taking indeed if the execs were only taking 10 days off then the unlimited plan would deter other employees from taking anywhere near that amount or more than that as hastings put it “in the absence of a policy the amount of vacation people take largely reflects what they see their boss and colleagues taking ” modelling others around you this concept of modelling others around us applies not only to vacation taking but to all sorts of behaviors as we continue to move towards a new distributed remote first workforce there’s going to be a lot of ambiguity in the decisions that we need to make the companies that are able to best adapt to this changing environment will be the ones in which leaders model the right set of behaviors a big one will be written communication as the ability to just randomly walk up to someone at the office and ask them a question subsides we’ll need to document our practices much better and be able to communicate much more efficiently the more we see others especially our leaders invest in written communication and take the time to get better at it the more we will do it and never mind us seeing them do this reed hastings wants them to shout loud and clear just how much vacation they’re taking or just how much they’re investing in themselves so as to encourage everyone else to do it an example of good modelling in practice is evernote the company which also doesn’t limit employee vacation days actually gives a 1 000 stipend to anyone who takes an entire week off in order to encourage vacation taking source other things okay so there was one more thing that reed hastings found out it wasn’t enough for leaders to just model the right behavior they also had to set context and guidelines reed realized this when it was the end of quarter and his accounting team was supposed to be closing up their financial books but a member of the team in an attempt to avoid the annual crunch period took off the first two weeks of january no bueno so reed decided to put in place clear parameters and guidelines on what was acceptable within the context of taking time off for example it was imperative to mention things like how many people taking time off at the same time is acceptable and how managers must be notified well in advance of any such long vacations this would help prevent blows like the one above in the accounting department conclusion in the end it seems like unlimited pto can work but it also needs to be supported with strong management individuals need to model big vacation taking and put into place the right guidelines but i think the lessons here go beyond just vacation the behaviors we see and notice from those around us eventually have a strong impact on the type of people that we become this is especially true at the managerial level where the impact is 1 to n and can result in considerable cultural debt so just like this question of unlimited vacation the answer usually lies in its implementation context is king but that does t always make for good headlines now does it hope that was useful if you liked this post you might like my newsletter it s my best content delivered to your inbox once every two weeks and if twitter is more your thing i would love it if you retweeted the thread
1,trying to be a millionaire so out of boredom i started doing a statistical breakdown of the drawn mega millions numbers since they last changed their drawing matrix i found great information on frequencies of drawn numbers during all drawings but what i d be really interested in seeing is the frequency with which the numbers between 1 66 were the first number the numbers 2 67 were the second number 3 68 were the third number 4 69 were the fourth and 5 70 were the fifth i could sift through all the drawn number combinations since oct 2017 371 drawings and i know i said i was bored but i m not that bored does anyone have this info or know where it can be found the mega millions websites don t break it down that way and i can t find any other website that has that info
1,education need help deciding if health informatics is for me sorry for the weak title not sure what else to name this but i have handful of questions if anyone could help answer them just need help gathering my thoughts and i want to see others input based on their experience etc i graduate for my undergrad healthcare technology management in 1 year but i am interested in a masters and i am looking around and feel as though health informatics or something similar is something that interests me a lot currently i feel like i want to further my statistics knowledge and solidify what i know so far i have been thinking of doing an online stats course certificate during the summer this could be free youtube videos or a something like coursera suggestions and whatever i choose worth it anyone in health informatics or something similar could you give me insight of that field and your experience
0,how to compare accuracy of 2 time series datasets i am the cto of a startup with no ml ds background customers in the maritime industry use our software to get real time price estimates for fuel in ports around the world these prices are continually updated by an etl process which takes actual fuel prices a few other data points as input we want to improve the accuracy of our etl and potentially experiment with ml ms models we have our first dataset of actual prices timestamps that needs to be compared with another forecast generated by a model dataset of prices timestamps the timestamps and their frequency are totally different how do we compare accuracy between our actual and forecasts datasets
0,has anyone ever worked on a machine learning model for queues has anyone ever worked on a machine learning model for queues suppose there is a bakery the bakery has has n people working m people in line and q orders that they are currently working on the bakery is interested in making a machine learning model that predicts how long a customer will have to wait before the customer s order is ready and how long will the next customer have to wait before they can place an order has anyone ever come across a machine learning model which can predict waiting and processing times i have seen examples online where people try fitting exponential distributions to historical waiting times and see how well they fit as well as trying different m m k combinations but has anyone ever come across an instance where machine learning algorithms e g random forest neural networks are used to predict waiting times i saw something like this but there was no python or r code for this paper can anyone recommend some source blog github website book youtube lectures etc which show and provide computer code for analyzing queues using machine learning models thanks
1,eli5 pearson and spearman correlation i want to know if there is a relationship between my two variables but it has to be in terms of increase or decrease should i apply pearson or spearman correlation
0,how to geo cluster houses in a real estate dataset i have a fairly large portfolio of houses think thousands that i want to cluster based on proximity to neighboring houses and some house types fuel source detached apartment the goal is to create clusters based on the distance to other houses and the types e g cluster of 5 houses max 50 meters from each other which are all on the same fuel source and are detached luckily in my dataset it is most likely that houses next to each other will also be of the same type do you have any tips on algorithms approaches for this job i am proficient in python r thank you
1,using math to justify the performance of machine learning algorithms suppose we look at the mnist digit recognition problem referencing the mathematical properties of these algorithms how could we try to justify that for this problem random forest performs better than linear regression and neural networks outperform random forest
2,paper digest simswap an efficient framework for high fidelity face swapping by renwang chen et al reading time 5 minutes 🔑 keywords acm mm 2020 encoder decoder face swapping feature matching identity transfer 🎯 at a glance faceswap apps have been around for ages hence you might be thinking that swapping the faces of two people is trivial but in reality is far more complicated the authors from tencent suggest that the existing approaches are limited in two main ways they cannot either generalize to arbitrary faces or fail to preserve attributes like facial expression and gaze direction the proposed method simswap leverages a new id injection module and the weak feature matching loss that aim to solve both of the aforementioned issues ⭐️ complexity 🌕🌕🌑🌑🌑 🔍 main ideas 1 limitations of the deepfakes due to the nature of the model encoder and two identity specific decoders and the training procedure the encoder features contain the identity and attribute information of the target face yet the decoder can convert the target s features to the source identity which means that the identity information is stored in the decoder s weights that is why it cannot generalize to an arbitrary person 2 id injection module seeking a way to separate the identity information from the decoder s weights the authors propose an id injection module between the encoder and the decoder this module first extracts an identity vector using a face recognition network and then uses this information to inject the identity information into the encoder features via adain layers the patchgan discriminator is used to improve the quality of the generated images 3 weak feature matching loss just replicating the source identity is not enough to create a face swap it is also required to keep various attributes such as the expression position lightning etc from the target image the authors use a variation of the feature matching loss for exactly this reason the idea of the feature matching loss originated in pix2pixhd which used the l1 norm between the discriminator features extracted at multiple layers from the ground truth and the generated images since there is no ground truth in the face swapping task the authors only use the last few layers of the discriminator for the loss since that is where most of the attribute information is contained the overall objective is comprised of identity adversarial weak feature matching and reconstruction losses 📈interesting numbers main takeaways the model was trained on images of size 224x224 the qualitative results in the paper blow the baselines out of the water ✏️my notes 4 5 for the name i guess simsim was already taken and this was the next best thing imho the teaser image is quite poorly chosen i can barely tell the difference between the target and result images surprisingly there is no mention of training the model at higher resolutions such as 512x512 or 1024x1024 there are examples of video face swapping that look really neat in the code repository and a little bit in the appendix however they are not discussed in the paper have you dabbled with deepfakes before let me know in the comments 🔗links paper code 👋 if you found this paper explanation useful consider subscribing to my telegram channel for early access to deep learning paper digests twice a week here is a paper poster with some important figures from the paper simswap by casual gan papers p s send me paper suggestions for future posts
2,efficient net as a feature extractor in computer vision most of the feature extractor use resnet as backbone for feature extraction is there any implementation available with the efficient net as a feature extractor any reason it s not popular considering its give better or similar accuracy with less computation power
1,how can i show that anova is better than t test for comparing group means preface this is a a homework question where i had to choose a topic form a question and answer it and so i felt that it was appropriate for this sub since it is a topic of discussion for me if it isn t please tell me a sub where i can ask this from what i understand t test is used to compare the group means for two groups with n 30 whereas anova is used to compare more than 3 groups together i want to show why anova is preferred for multiple groups rather than multiple t tests according to this multiple t tests increase the error every time you conduct a t test there is a chance that you will make a type i error this error is usually 5 by running two t tests on the same data you will have increased your chance of making a mistake to 10 the formula for determining the new error rate for multiple t tests is not as simple as multiplying 5 by the number of tests however if you are only making a few multiple comparisons the results are very similar if you do as such three t tests would be 15 actually 14 3 and so on i want to prove this either with an example or mathematically are there any examples of this or any resources i can look into i was thinking of taking an example in r where i perform multiple t tests however i don t know how to get the error from it and moreover how should i perform the multiple t tests would every group be tested against every other group say if i have n groups would i need to run n 1 tests
0,tidying time series behavioral experimental data question hi all i recently came across the original tidy data paper if you haven t read this check it out i wish i had a long time ago and have been thinking a lot about how to best reorganize data from my experiments to make analyses and visualization easier however my datasets seem kind of unconventional and i haven t found great examples for how to best tidy them basically i continuously collect rodent behavioral data in sessions that span 60 trials i collect timestamps of events of interest e g lever press trial start so my data looks something like this currently i ve been playing with both matlab structs and python pandas dataframes for simplicity i shortened this example to just 3 trials x200b subject group date trialtype trialstart behaviora 1 0 20210601 0 0 1 10 40 60 8 12 42 65 2 1 20210601 0 0 1 10 40 60 15 22 26 48 53 3 0 20210601 0 0 1 10 40 60 12 42 80 1 0 20210602 0 0 1 10 40 60 4 22 45 63 2 1 20210602 0 0 1 10 40 60 30 44 68 3 0 20210602 0 0 1 10 40 60 12 29 43 63 68 x200b notice that the events contain arrays of multiple values which makes it untidy currently i have one row per subject per date this corresponds to my raw data files while this organization is intuitive to me it seems to make it harder to conveniently plot and analyze the data using readily available packages in order to have a single value i was thinking of reorganizing it so that each row corresponds to a time bin in every file instead of event timestamps the events would be binary coded so that a 1 would be placed at the corresponding timestamp and would be 0 otherwise it would look like x200b subject group date timebin trialstart trialtype behaviora 1 0 20210601 1 0 0 0 1 0 20210601 2 0 0 0 1 0 20210601 3 1 2 0 1 0 20210601 4 0 0 0 1 0 20210601 5 0 0 1 1 0 20210601 6 0 0 0 my event timestamps are sampled at 10ms resolution and i have a few hundred files so this could get huge but i guess i could downsample by rounding the timestamps this dataset is pretty much the template of most of my datasets but i also have more complex datasets where i am continuously tracking a variable over time e g i have video recordings i can use to track position velocity so i think organizing things by timestamp may effectively translate to those in the long run an alternative would be to have one row for each trial but this wouldn t be effective if i am also collecting a continuous variable i d love any feedback ideas about how to best organize these data that includes coding resources and packages if you have recommendations
2,self supervised learning in vision recent papers dino barlow twins paws etc video interview dr ishan misra is a research scientist at facebook ai research where he works on computer vision and machine learning his main research interest is reducing the need for human supervision and indeed human knowledge in visual learning systems he finished his phd at the robotics institute at carnegie mellon he has done stints at microsoft research inria and yale today though we will be focusing an exciting cluster of recent papers around unsupervised representation learning for computer vision released from fair these are dino emerging properties in self supervised vision transformers barlow twins self supervised learning via redundancy reduction and paws semi supervised learning of visual features by non parametrically predicting view assignments with support samples all of these papers are hot off the press just being officially released in the last month or so many of you will remember pirl self supervised learning of pretext invariant representations which ishan was the primary author of in 2019 youtube pod
1,career advice in stats with a focus in animal behavior biology hey all i have quite a conundrum ahead of me and help from any statistical professionals and students would be super helpful currently i have a master s in evolutionary anthropology and a bs in biology with a focus on animal behavior after i graduated from grad school i found a lot of my interests didn t really mesh with the current job opportunities something that i realized though is that a lot of grad students and even professors struggle with stats especially in the medical field and i thought that there could a position either in a lab or in a biology anthropology department to be the resident statistician if i got a masters in stats however i talked to a stats professor and he said that those positions don t really exist so i m kinda stuck at where i want to go next as i m not sure what else is out there that i can do stats wise that relates to biology and behavior any advice on what careers i can pursue in alternative what i had originally thought of that is related to animal behavior would be amazing
0,how to explain lack of meaningful work in your last job my current work is very lax and although i was hired to do data engineering work i have not done much of that my position seem to be a placeholder and i am given random tasks clean up data check if some mathematics can be done with that data image processing extracting data from weird s3 buckets and my work is not even very frequent i can go on days without practically doing anything i need to switch jobs immediately for family matters can i use these experiences in my resume and switch to a data science position my coding skill is nothing to brag about but my understanding of statistics analytics and model building should hold up edit thank you everyone for your responses i had some good ideas
0,how to find and draw a high dimensional bounding box hey all i am trying to draw bounding boxes on arrays i have for a 2 d set it s pretty simple i follow a process detailed here however my problem is now i may have much higher dimension data is there an accepted method of drawing bounding boxes on n dim data that scales well to high n for example if i had a 5d array that was filled with 1 s but with 2 separate areas filled with 100 s how can i find and draw boundary boxes around them i m really open to any idea so far i m using image filters for the 2d examples but for the n dim case i m not sure how well this will apply or if those filters really make sense to be used on dim 2 it s hard for me to imagine as well an nn esque approach think yolo since i will only have a few arrays so training would be an issue
2,sergey levine is reinforcement learning sufficient for designing generic and adaptable robots hello guys i enjoyed this conversation with sergy levine i want to share with people who maybe interested in we spoke about whether reinforcement learning sufficient for designing generic and adaptable robots generalization in reinforcement learning vs supervised learning i hope you find this conversation useful
1,predicting distributions instead of single values in regression i am curious if there is a good way to give richer predictions in regression in a form of probability density of the target for every observation i ve heard about quantile regression which requires fitting multiple models and hence is too computationally heavy and have read this article where normality is assumed so it is unusable in some tasks what other methods am i missing does the bayesian approach have anything to offer here
1,probability question i have someone who is trying to pass a multiple choice exam for state licensing each question has 4 choices and only 1 correct answer the exam question bank is roughly 300 questions the exam is 120 questions passing grade is 80 the person in question has taken and failed the exam 16 times if one were to randomly select answers how many times would it take to achieve a passing score
0,help scraping web map data hi all i m hoping you can help me scrape data off of this web map i haven t gotten too far but it looks like the data are loaded by post requests and come back as json i think a local web server might be needed to do this and it s a little above my paygrade if anyone could point me in the direction of some resources or is able to say oh this will take two minutes do this i ll be so grateful thanks so much
0,ramping up help from coworkers i ve started a new job as a sr data analyst i am ramping up and am about a month into the position ramping up in the pandemic is a different experience than when i ramped up in a previous role in the office obviously the most notable part is knowing when to reach out to others and seek out that sort of whiteboard session i feel that i am missing that now and don t have a great feel for my team i have a project now but ultimately i feel disconnected i am looking to do more catch ups and understand that with time rapport builds slack is part of the culture but the group chat isn t too active when i have ramped up other individuals in the past i try to meet often and be available for their questions i try to also have specific times to check in to help facilitate communication and let the person know there isn t a quota to their questions what are ways others have been able to ramp up in the pandemic or fully remote any tips edit grammar
1,how to tell when a variable doesn t follow a bernoulli distribution hi i am wondering if anyone has helpful resources for figuring out how to understand when response variables follow or don t a bernoulli distribution let s say for some given year we are using logistic regression to predict some binary response variable e g company bankrupt or not based on a predictor e g company income how would i go about thinking about whether for a given income level the bankruptcy status does or does not follow a bernoulli distribution i ve looked around online a lot but don t quite understand the answers since it seems like there s always going to be some combination of factors that deterministically decides whether or not it s bankrupt how can it be truly random thanks peoples
2,deploy dl on ecs for multiple requests i am trying to run docker containers on ecs the containers have dl models with flask with 5 10 requests i am getting response within my acceptable timeframe but if there are 100s of requests aws i taking minutes to scale new instances and meanwhile those requests are being forwarded to the already running tasks due to this tasks are failing bcoz of memory error how do i scale ecs correctly so that every request is processed where to route requests untill new tasks are started
1,hypothesis testing on ordinal data hi let s say there s ordinal data 1 low 5 high about the importance of x in doing something eg rate how important food is with regards to relationship length this is clearly ordinal in nature then someone puts forth a hypothesis food leads to longer relationships so my question is are there any statistical tests that are applicable here all my searches have led to two sample non parametric tests eg mann whitney test but this is one sample and according to the researcher there isn t a interest in different groupings eg we have male female so they don t necessarily want to see if there s a difference between males and females the second question would be is this enough data for any meaningful analysis because it seems to me some thing is missing but this area of statistics not being my area of expertise i cannot tell if that s correct any resources are welcome thanks
0,how do you organize your text datasets for nlp projects question in title context i’m looking to organize the text datasets that we have at where i work right now it’s a bit all over the place depending on the data source bigquery postgresql there are reasons why we have two separate dbs but i won’t get into that google sheets csvs etc besides of labeled datasets we also have many dictionaries that we use to build simple rules the motivation is each project usually has their own datasets but there have been a lot of cases where we can leverage other project’s datasets currently the only way is to ask the pic of the project it doesn’t seem to be very scalable now especially that we’re now having more nlp projects and more people working on them right now as a first step i’m thinking to just maintain a list of queries for the ones in db and keep all the docs csvs in one folder although ideally i’d love to have these datasets easily accessible much like tensorflow huggingface datasets not sure if we have the bandwidth to build a gui to explore the datasets though have you faced a similar problem i would like to hear any suggestions or your experience if any as well as the challenges you encountered thanks a lot
0,d optimal design in the workplace i don t know how squarly the argument fall into data science or experimental design i have worked in 2 r d departments of industrial machinery dedicated to pvd and cvd physical vapor deposition chemical vapor deposition how do you convince people at work that d optimal design is not a scam when the end game is formulating a model through multilinear regression edit i explained both the high level goal and the advantages but the supervisors don t seem convinced i wanted to ask whether or not someone applied the technique in the workplace
1,scaling of estimates after contrast coding hi i am trying to analyse subgroups of a mixed effects model using custom contrasts i have 2 factors f1 and f2 each with 3 levels my interest is in comparing the groups like this 1 f1 group1 vs f1 group2and3 1 f1 group2 vs f1 group3 1 f2 group1 vs f2 group2and3 1 f2 group2 vs f2 group3 i am using lme4 lmer and them emmeans emmeans the model looks good the contrasts look good uncorrelated etc the things that should be significant are and those that should not are not problem the estimates are obviously scaled the exact values are way too large if i do paired comparisons the estimates are fine i do not understand how exactly and can t find information on this anywhere i am very interested in the exact estimates not just the p values so i would like to figure out where the issue occurs any ideas or pointers contrasts look like this f1 f2 c1 c2 c3 c4 a a 2 0 1 1 a b 2 0 2 0 a c 2 0 1 1 b a 1 1 1 1 b b 1 1 2 0 b c 1 1 1 1 c a 1 1 1 1 c b 1 1 2 0 c c 1 1 1 1
2,bert base choosing optimal cloud infrastructure and environment setup long read quite often i optimize dl models aiming to get the cheapest placement within a certain model performance range find optimal instances and tuning the environment for training and inference etc after multiple such optimizations i’ve put together a quick framework a guide that i can refer to when i need it some approaches i came up with and results i got seem quite odd and counterintuitive so ideally i’d like to start a discussion with those dealing with model performance optimization does my approach make sense is the benchmarking the only way or am i missing something if tldr main points rules of thumb in choosing the instance type shape did not work for me when i try and guess cost or runtime based on gpu generation i always land on the wrong side of a 4 5x variability in some cases not so obvious options might provide decent performance p4 and costs p100 what works for me is optimizing for gpu utilization as a proxy for cost performance optimal placement duh to achieve that i have to deal with different bottlenecks in the system outside of gpu the biggest culprit being preprocessing on cpu and following data streaming to gpu so benchmarking with rudimentary monitoring is a must batch size optimisation can give as much as 4 5x in performance worker number optimisation vcpu count basically got me another 2 3x wtf driver cuda versions might influence performance much greater than expected 10x benchmarking is a pain in the ass as i typically run at least 100 benchmarks to gather a comprehensive picture below you could find a breakdown of those points above approach to illustrate my points i decided to go with the most popular nlp model bert base uncased according to huggingface because 1 this domain looks more suitable for a generalized approach for optimization 2 dataset and preprocessing are very similar across different models it took me around 110 benchmark launches to gather the data below so i put together a small repo github link using pytorch to run inference on a small mock dataset and run it on all gpu instances available for me on gcp tesla k80 tesla p4 tesla p100 tesla v100 tesla t4 this simple script can perform text input encoding the numbers of preprocessing workers and the batch size are input parameters it also can find the maximum possible batch size with linear bruteforce that saturates the gpu on top of that i backed it into 4 different containers to run it with different versions of cuda i played with the batch size and the number of processes used by dataloader to preprocess the data the goal was to maximize gpu utilization and find the optimal batch size of processes to get the best price performance for each type of gpu and then compare how much they would cost me per job after that i chose the best performing gpu and ran additional benchmarks for different nvidia driver and cuda versions to try to catch some optimizations there i used this approach link to install different drivers test results and observations to get the baseline i passed text to the model sentence by sentence it appeared that the number of processes does not change the picture much below is the summary of bench runtime and cost for the baseline x200b x axis legend as follow gpu batch size x of workers of vcpu where gpu accelerator family k80 t4 p100 etc batch size number of sentences i am pushing to gpu and passing to the model simultaneously of workers num workers parameter of the dataprocessor i e number of processes to perform data loading and pre processing of vcpu number of virtual core present in vm gcp allows varying number of virtual cores for gpu instances from 1 up to 8 for k80 12 for v100 16 for p100 24 for p4 t4 then i began increasing batch size and the number of data loader workers to maximize gpu utilization to illustrate the approach below are 3 graphs of gpu utilization for p100 instance with 4 vcpu cores 4 workers and maximum possible batch size obviously underutilized x200b p100 4 vcpu 4 workers then i increased the number of vcores and workers to 8 keeping the maximum possible batch size utilization jumps to 66 but still far from maximum x200b p100 8 vcpu 8 workers a further increase to 12 vcore and 12 workers finally did the job pushing utilization to 94 p100 12 vcpu 12 workers after playing with vcpu workers counts i got the following charts k80 maxed out its utilization at 2 workers i didn t go below 4 vcpu but in this particular case lowering the number of vcpu can bring additional savings it took 8 workers and 8 vcores to fully utilize p4 note p4 63x4 4 p4 63x4 6 and p4 63x6 8 launches it is a clear indicator that there is no much sense to have more workers than you have vcore in this particular case the same for t4 8 workers is enough to saturate this gpu the following two cases are the most interesting in both of them i had to go to 12 vcpu the maximum number of vcores gcp allows to assign to a single gpu vm another remarkable thing is that both these gpus showed an order of magnitude runtime improvement between the “one by one” approach and maximum possible parallelization of data pre processing p100 showed the maximum utilization as you can see on a chart above at 12 workers worth noting p100 146x6 4 and p100 146x4 4 it looks like overcommitting vcores might backfire x200b v100 was utilized only on 59 under 12 workers potentially it can be pushed further with a multi gpu set up where more than 12 vcpu per gpu can be added to the vm or if the data set is fully preprocessed before inference x200b below is the summary of cost runtime for different combinations of vcpu count gpu x200b t4 is a clear winner in terms of price per volume of processed data interesting to note that p4 appears to be a clear forerunner in terms of processed data per dollar after that i varied pytorch for different cuda libs version 1 7 1 can go with cuda 9 2 cuda 10 1 cuda 10 2 cuda 11 0 i tried all of these versions against the following drivers 460 32 03 455 32 00 450 102 04 440 118 02 418 181 07 410 129 384 183 was not able to install it on ubuntu 16 04 with the above mentioned gpus below is the summary of all runs i gathered all of them were for optimal t4 setup i e maximum possible batch size 8 workers on 8 vcpus x200b runtime sec y vs nvidia driver version x i struggle to explain the order of magnitude difference for certain combinations of driver cuda i have not seen this discrepancy of performance for drivers before although i did such analysis for different networks before with typically up to 15 variability i ran benchmarks for all outliers 3 times and the results were consistent crosses on the graph can indicate the amount of variance across different launches so there is at least an order of magnitude cost improvement available with rudimentary benchmarking monitoring but the driver cuda combination’s effect on the performance puzzles me to say the least has anyone seen something like that and what might cause that hope that might be useful
0,peer group benchmarking a cool concept recently i learned about peer group benchmarking from someone at another company in the context of converting a continuous score model output probability into a binary decision e g accept or decline the idea as she explained is to make a decision off of a continuous score not solely based on a fixed threshold but rather based on the score of its peers within a sliding time window i might be completely wrong but one example i can think of is this let s imagine we are building a fraud detection classifier and assume that from cross validation we decided if the outputted probability is greater than 0 8 we reject the transaction that is of course assuming the real world data will have similar distribution to our training set now let s say due to pandemic the behavior has changed rapidly and unexpectedly in a way that our model generally produces 0 1 higher probabilities this change in distribution might be different in different groups if we still decline based on 0 8 threshold we may end up declining a lot of transactions the peer group benchmarking comes into play here if we can see that a certain transaction now has a 0 85 score but at the same time it is still well below its peers in a group we may not decline it even though it is breaching our hard threshold of 0 8 i would like to learn more about this but i couldn t find any resources just by googling it i appreciate it if anyone is familiar with the concept and can refer me to a good reference
2,torchsr image superresolution for pytorch hi all i started torchsr a package for super resolution networks written in pytorch it s inspired by torchvision and should feel familiar to torchvision users check it out low resolution image super resolution x4 and ground truth at the moment i implemented many datasets the most popular models edsr rcan and a number of network improvements and data augmentation method plus the training script for people who want to develop their own next steps gan training and multiscale networks github repo python package
0,i m offended by having to scale my data i find it demeaning
2,jina cross modal search system open source search engine jina is a neural search framework that provides large scale indexing and querying of different kinds of data including video images text music source code and pdfs try it now google i o and jina video
0,cso option var model hi there all first time poster and so sorry in advance for misdemeanors i work in a commodity trading company and have been tasked with building sourcing a cso option risk model in this case the spread is a time spread between future contracts for crude oil i have a pretty good understanding on vba and there are people at the company who can work with python etc we have a sql database of future prices for the various contracts as well as the volatility delta price of each options contract from outright contracts to the cso options is anyone able to give me advice help on how to go about building this seems to be a mc simulation but my only experience with that is using matlab engineering degree which i have been told is not an option due to licencing costs x200b thanks x200b thpj20
1,do i have to report confidence intervals when data is not significant hello throughout exploring my data i have found no indicators that my independent variable has any causal effect on my dependent variable after running general and generalised linear models this is still the case do i have to report my confidence intervals still thanks
0,all else equal would hiring managers rather have a candidate with all their experience in their industry or prefer someone with experience in multiple fields i would think it would be a bonus to have experience in various industries but my encounters in applications and interviews makes me think otherwise
2,semantic similarity between programming languages and math terminology i ve wondered how good neural nets can get at predicting semantic similarity at a more abstract level between different topics for example finding context between math terminology like vector and matrix and programming terminology like list array and 2d array also finding context between common data types functions concepts between different languages and frameworks libraries ideally i d want a binary output that takes two strings as input to compare question is what would be the approach to a problem like this would it need carefully labelled data translating the terminology between the two topics or is a self supervised method at all possible i ve only recently got into data so i could be way off here on what is possible and what is not
