id,label,text
riknbs,1,have image generation project idea need tool pointers though i have a bunch of photographs i ve taken over the years primarily landscape i d like to feed them to a network of some kind to have it learn what my photographic style is then generate new images based on those what tools libraries should i be looking into i ve been programming for like 10 years but have never done anything ml related and honestly am not super interested in the technical details of a project more the output so the easier the tool the better fwiw i can supply probably 1000 high res landscape photos cheers
rnyeyl,1,free python courses from udemy learn python 3 from scratch python for absolute beginners practical python python for data science â€“ great learning starting python 3 programming for the absolute beginner
rktqol,1,are there any papers on this i was recently training a sentence classifier and discovered something kind of interesting if you calculate the distance from the weights of the word embeddings at the start of training to the weights at the end of training the distance corresponds to how important that word is for the classification task i was wondering if there is any research into this it seems like it would be useful for model explainability
r8r8qr,1,is there anywhere a offline running text to speech that works on linux and has a german male voice every single text to speech i find is either googles shitty gtts which requires internet and only has 1 female voice for german or pyttsx3 which works fine on windows with a german male voice but not on linux the only german voice is a horrible robotic sounding 1960 voice unless there is a way to get the windows voices to linux to use with pyttsx3 is there anything else
qsl5jj,0,opinion the recent paper on buggy resizing libraries is misleading a recent paper on buggy resizing libraries and surprising subtleties in fid calculation claims that the image downsampling methods of opencv tensorflow and pytorch are buggy and therefore pil should be used instead for fid estimation the corresponding twitter post was quite popular during the last week i believe that this claim and the main figure of the paper is misleading because the issue is caused by aliasing and it can be fixed by simply setting the right parameters in the functions they used x200b their main figure x200b my reproduction and fixed results bilinear on the bottom image you can see the reproduced and antialiased downsampling results in all the frameworks in all cases a single parameter modification was enough to mitigate the issue i shared the code and my complete opinion in this repository though i believe that the discussion on whether antialiasing should be a default in image libraries is valuable in my opinion none of these methods is buggy and the paper presents the issue in a sensationalist way edit just found out that there is another very thorough investigation of the same work highly recommend checking the blogpost out
r5pjtp,1,started learning machine learning today enrolled in andrew ng s course how to approach the course and get on with it how to get from theory to projects any particular sources etc for a beginner
qzjuvk,0,discussion neurips 2021 finally accepted submissions statistics the crawled data is here rating distribution x200b top keywords x200b consistency experimental results x200b
rvg08w,1,bootcamp graduation project generating new cooking recipes from a list of ingredients which models could fit the task hi we re a team of four working on this datascience project and we finished our second day pretty depressed about which models could fit the task at hand we have a week and a half left to complete our project python rather than using a kaggle dataset in english we re trying to make it work with data scraped on a french website we re aiming for 20 000 recipes this is a random recipe from the website for those interested we ve been thinking about separating the whole thing into two tables here are the features we ve decided to scrap so far for each table ingredients table recipe name recipe id ingredient one row per ingredient for each recipe ingredient quantity no units of measure for now e g 500g of salt 1 spoon of oil recipes table recipe id recipe name recipe steps we re having trouble deciding if this should be a single string or a list of strings we ve had two weeks of machine deep learning in this course and have seen a fair amount of library and tools like scikit learn keras pandas numpy we ve done some research but we don t have a clear understanding of the following models we ve been thinking about using for this task and would appreciate any sort of advice or source about which way to go next very short summarize simplernn lstm and gru rnn layers are too simple to generate proper text from what i gathered word2vec too in a different way gan and transformers gpt 2 bert albert seems likes the answer but i have no idea if it s realist for us to work on it with limited time or even where to start with it for textgenrnn i did not grasp how useful it could be tablegan library did not appear to be good at generating text x200b here is a very long text about my understanding of those models i would really appreciate any advice even without you reading any of what is below x200b x200b i don t have a deep understanding of how neural networks works but from my understanding rnn aim to predict the next future at least element of a sequence mainly for time series and nlp i m familiar with using simplernn and lstm and gru layers in keras models i don t understand the concept of vanishing gradient in depth for lstm but i remember it is supposed to be slower but better than simplernn i ve been told gru is faster because it uses less parameters than lstm about textgenrnn and tablegan i never heard of those two before looking at other works on the subject of cooking recipes both are python library textgenrnn appeared to be a better choice than classic rnn models above because it could be used on strings of different lenght and trained with the whole sequence adding character per character while tablegan opposed two models and returned tabloid data as what seemed to be a cluster of ingredients which could be interesting to use vectors on like i think word2vec does went back to the tablegan article it said he used countvectorizer from scikit learn but results gave back unusable long lists of ingredients what we did with word2vec was to feed it with a large amount of processed text tokenize lemmatize and other methods i don t remember input it with a word or short sequences and it returned a dict of score as key and words as value closely related and often used with the given string i honestly don t know if the range of application for it extends to much more for gans and transformers we basically had one lone slide on it that told us it was advanced models but we couldn t cover it during the course i don t fully understand transformers what i understand is that it seems to function with both encoders and decoders doesn t resort to rnn but do use vectors i guess i need to read attention is all you need asap to further my understanding of transformers my understanding of gpt 2 bert albert and other transformers is so low at the moment gpt 2 seems to be pre trained it calculates the likelihood of a word clustering and vectors occurring after a string based on its full content i tried it on this website and it made it seem like one of the better options it might already be too good and i feel like student of our level working on it might only make it worse do you think it is possible to make it predict words in between ingredients recipe steps like tomato curry chop and oven instead of giving words following that sentence i found bert and albert while searching for the best currents nlp models the last one seems to be a lighter version of bert which while training hides random part of the full input string while training on it to avoid over fitting it also increases its performances if i am right i don t know how to start with both gan and transformers with the deadline being quite soon to be honest but i d like to focus on one or two if they re the better solution x200b we would be very grateful if you could recommend to us some models that you think could or should be used for this task it would help us a lot to limit our field of research
rpnptp,1,product preference dataset i m looking for a dataset for some kind of consumer product e g soda with a lot of product features e g color sugar content bottle color etc that can be used for predicting personal preference any ideas i welcome any sort consumer product but soda seems to be the easiest the goal is to give my father a hands on demonstration of the power of nns and how we can observe correlations in inane properties
r6trwq,1,which python library is best for counting objects using edge detection i m wanting to create a program that will count a number of objects that are all touching for example a number of boxes that are tightly packed together and then counting each box any suggestions on which library to use or how to break down this project would be appreciated i m thinking tensorflow or opencv cheers
rtgxqy,1,i am majoring in statistics what computer science subjects do i need to learn happy new year everyone i am a second year university student majoring in statistics and i am interested in ml ai if possible i want to apply for a master s degree in ml at my school the programming courses covered by the statistics dept are 1 python computer modelling for scientists 2 database sql 3 math modelling i am currently self studying data structures and algorithms using c after that i am considering learning 1 computer architecture 2 computer network 3 operating systems may i ask if these subjects are a good fit for my need please and what other subjects do i need to take to have adequate knowledge in the coding part of ml thank you have a nice day edit formatting
rms1jn,1,check out my first blog post its on a similar image search i built over the last few days using pytorch hi ml community i wrote my first blog ever describing the project i built and explaining my code i would appreciate it a lot if you could read it and provide any feedback thank you
rmy3ct,1,introduce a site introduce a site or channel in which conferences on artificial intelligence technologies and the algorithms used in them are described in full detail with information on the latest developments
rvl86n,1,need help assistance in completing a project i decided to create an application a web application actually where spotify users can kind of match with their fellow friends mates or anyone else the matching is similar to that of spotify blend the spotify api gives us the ability to gather data about a userâ€™s listening history etc and key data about the songs they listen to this data might be able to help create an algorithm for this match but i really know nothing about ml but to drive this project to completion i wish i had someone who had some ml experience open to learn to help drive this project to the end if interested you could reply to this post if you canâ€™t help but can direct me in the right path with respect to things i could learn to complete this task you could share thanks
ri11hg,1,autoencoder for very high dimensional data i have data that has over 100k features and i wanted to try to reduce them with an autoencoder but just making the framework with all of those inputs take up way too much memory is there a way i can make an autoencoder with this many features
rgw4fr,1,class embeddings in vit i have been trying to implement the vision transformer in pytorch and there are some confusions regarding the learnable class embeddings x200b 1 how exactly does this embedding learn the corresponding label for the image 2 why do we only pass the class embedding related outputs from the transformer encoder to the mlp 3 would it not be better to get a linear projection from the transformer encoder output and input it to the mlp and ditch the class embedding entirely 4 finally how exactly can we implement it i have seen some blog posts using nn parameter class from pytorch as i understand it defines a random vector for which the the weights are learnt as the training proceeds is this the correct way x200b thanks alot
qk90b9,0,project bert tokenizers nuget package for c x200b inspired by the challenges i faced with using bert models with ml net i have built a small open source project and nuget package for easy tokenization in c ðŸš€ with this package you don t have to worry about different vocabularies and you can build input for bert models quicker ðŸ‘‰github ðŸ‘‰blog post
rovtsh,1,research 2021 looking for interesting ml papers to read for the break or the new year here is a curated list i made with video explanation short read paper and code for each of them the best ai papers of 2021 with a clear video demo short read paper and code for each of them in depth blog article the full list on github short recap video
rpqudp,0,paper explained federated learning for mobile keyboard prediction ever wondered how your mobile keyboard gives you the next word suggestions how do they give personalised suggestions while at the same time ensuring the privacy of individuals check out my blog post federated learning for mobile keyboard prediction which talks about how this happens in a privacy preserving manner blog post ppml series 3 federated learning for mobile keyboard prediction annotated paper annotated ml papers federated learning for mobile keyboard prediction
qnktqk,0,league of legends patch 11 21 game playing ai reinforcement learning supervised learning dataset this dataset is meant for anyone who would like to try to create a deep learning agent either using supervised or offline reinforcement learning to play league of legends the dataset contains 72 games from patch 11 21 last patch where the game ended in an early surrender these games were chosen as the game lengths were guaranteed to be low which kept the dataset from being too large to download the dataset go to this github link and click on the google drive link the dataset is stored as an sqlite database file and the schema should be relatively self explanatory happy to answer any questions this is just a preliminary dataset which demonstrates that this is possible within the next few days the dataset will contain 1000s of replays which means 10 000s of champions worth of data for each time a player plays a champion edit database now contains all 191 early surrender games games ending at or before 3 5 minutes in the dataset this table shows the top 10 champion occurrences within the dataset champion no nami 116 miss fortune 103 lucian 61 khazix 36 viego 35 lux 34 jhin 32 yone 30 camille 29 graves 29 edit 2 larger dataset containing 987 games targeting miss fortune in the early game up to first 5 minutes with the same schema and format as the first dataset also contains all game objects recorded 4 times a second the games were chosen by getting the games where the mf player lived the longest this gave a dataset where the players overall had a 64 4 win rate in roughly euw diamond ii edit 3 a further 728 games also targeting miss fortune in the early game up to first 5 minutes with the same schema and format as the first and second dataset this brings the total number of games for the mf longevity datasets to 1 715 or 1 715 games 5 minutes 60 seconds 4 frames second 2 058 000 frames in total this should now be enough to at least create a deep learning agent which can play miss fortune for the first five minutes of a game at least to a basic level edit 4 another day another dataset a further 773 games from the mflongevity dataset have been uploaded i have now also included a jupyter notebook to analyse the data from the 191 earlyff dataset which works completely standalone from google colab feel free to also run it locally if you wish to github link open notebook in colab edit 5 very in depth explanation for the process used to create the dataset blog post
qtsomm,0,reverse language reconstructing by consensus this wasn t a priority for me for a few months as i was collecting data but it is becoming one now for the next stage of my project i have 70 000 rows x 12 columns which i ve been working on to reconstruct some of the words from classic latin currently after lemmatization there are 9000 words that are unknown in english many were unable to be lemmatized however by translating latin into every language that evolved from latin i m able to fill the pieces for instance errat in english was errat i could not find it within the dictionaries we can assume what it means but when we follow it down the line the process is latin to german to english latin to spanish to english latin to catalan to english and so on and so forth going down the list we have to err irren err to be wrong err wander err shine to rail err errar to make a mistake made a mistake fout wrong we can conclude that it means error that s obvious err would have been perfectly acceptable as it is english however i can t go through this line by line especially not by myself i have tried to create a natural language consensus for each row but it doesn t work the issue is getting it the right data i ve tried a pattern such as 1 1 1 1 1 1 2 a 2 3 a 1 a 2 1 however it spit out gibberish instead of choosing the most appropriate column it s almost impossible to tell the machine that fout wrong and made a mistake are synonyms unless i feed it a massive thesaurus that s also an easy one there s a word which has supporter journey wandering traveler repairer refresher none are the same but they illustrate a very unique definition we can see that the main subject of the translated word is to be an explorer the indirect subject is someone who supports fixes refreshes or repairs i know back in ancient times that the rich had repairmen for wagons tents anything while traveling this would most likely be that person but the word is unknown to be so the best i could expect a machine to do would be to summarize the two main ideas from all of the definitions this could be super simple and i m just overthinking or it s very complex and i m underthinking would love your suggestions and opinions i guess it could be as simple as a summarization of each row
r79oqv,0,aaai manipulated reviewer scores without reviewer permissions a professor makes a serious accusation of review manipulation surprisingly realaaai chairs updated my review without my permission i never experienced this before and now i have to rethink whether i will submit to or review for this conference again anyone with more info shed some light on this
rrlgt6,1,what is the best resource for learning reinforcement learning can anyone suggest any resources to learn reinforcement learning preferably with python examples
rv5yj2,1,nn vs lookup table hi assuming one has collected the 24 pairs of the input output datasets for a target system x200b one can create a simple lookup table to describe the input output behavior and utilize this as a controller one can also train a dnn model to learn the relationship what is the benefit of using dnn in this case in my opinion for dnn one does not have to store the whole dataset for the lookup table if one gives a new input value that is not included in the training dataset the trained dnn would perform better since in the case of the lookup table the predicted output is just an extrapolated value from the previously known output any other benefits that can justify using dnn
qzvh6i,0,what are some design patterns that are actually fairly used in production ml hi i was wondering what are some of the design patterns that you have used in your code in a producation grade ml like i was wondering about maybe going for factory pattern and builder pattern for instantiating my model objects might be a good idea theres also the strategy pattern that i think can have its uses but overall i m nor that well versed in patterns nor in production ml what is your experience in this regard
rboc5r,1,running the model with slightly different attributes my question is about how you organize your notebooks i am testing different document representations with tf idf or doc2vec sometimes i make some small adjustments to the model i want to keep the results so that i can compare later but i am not sure whether i should open a new notebook i am asking this because i am self taught and i feel like my projects seems a bit this organized is there some framework maybe to organize machine learning code
r2t541,1,how would i train a bot to analyze movie scripts and then make its own iâ€™ve been researching these bots that analyze a series of movie scripts or text i e college humor infomercials a syllabus etc and then with the data collected they make their own i was curious on what would it take to make one i have a pretty good understanding of programming and i thought python would be the best language to write something like this i thought some of the results would make for a pretty funny project however i donâ€™t know where to start any sources or tutorials on how to begin thanks
rlkyv6,1,very basic of linear regression my first article please have a look and give inputs
rggtdu,0,exciting new effort to develop synthetic data for genomic research
qzj8qo,0,training with batch size of 1 i am working on a virtual tryone problem but the thing is that i really cant use more than batch size of 1 in training due to gpu memory limitations the network really benefits from batchnorm however with batch size of 1 it is really not giving good results i am using pytorch and any help will be appreciated what do i use instead of batchnorm i have tried instancenorm but that too doesnt work very well should i try playing with momentum of batchnorm
rknejs,1,machine learning algorithm that would learn when a barber rejects customers imagine a barber that works with appointments there would be a system that would track the times this barber would reject the appointments for example mondays this guy usually rejects appointments between 10 12 am or tuesdays 1 3pm the program then learns this guy s pattern and closes these hours for future appointments which machine learning algorithm would be suitable for this kind of work
rtvf85,1,got a list of practical ml projects that surprisingly can be done by individuals in a computer and are not necessarily limited to enterprise origin only
qphg92,0,amd launches mi200 ai accelerators 2 5x nvidia a100 fp32 performance source more info for todayâ€™s announcement amd is revealing 3 mi200 series accelerators these are the top end mi250x itâ€™s smaller sibling the mi250 and finally an mi200 pcie card the mi210 the two mi250 parts are the focus of todayâ€™s announcement and for now amd has not announced the full specifications of the mi210
qo8wvt,0,is it necessary to manually label desired objects in each image for object detection for example i have a dataset of thousands of images of 5 different objects and each object images already stored in different folders and i train my model on it now i give an image as input which contains all 5 of the objects and i want my model to detect all 5 of these objects in the image and draw bounding boxes around them to be able to do this when preparing my dataset do i have to label desired object in each image by drawing boxes around them and then train my model on it is there any better way to do this than manually labelling data of thousands of images
rfssk6,0,in transformers why are positional embeddings added to the tokens rather than multiplied element wise with them i think of a token as a vector describing the input word patch etc element wise multiplication of the positional embedding and token would change the token in a relative fashion whereas addition does so absolutely intuitively i would have thought that a relative modulation makes more sense how should i be thinking about addition in this context
rt139q,1,completed applied statistics and basic ml courses in college wanting to learn more where should i go from here hello last year i took an applied statistics course and intro to ml course in college both taught by the same professor while i got through them i didnâ€™t feel like i learned and retained all that much information from them and feel lost as to where i should go now in terms of ml like i remember what z scores normal distributions and confidence intervals are but i couldnâ€™t tell you what a gamma distribution is used for and what itâ€™s basis is should i review statistics first or is my knowledge good enough to jump back into ml iâ€™m on winter break now and was wanting to visit the library and read some books on the topic but iâ€™m clueless as where to start could anyone recommend me some good starting points for statistics and basic machine learning we did a lot of application of it in python but iâ€™m also wanting to understand the underlying theory iâ€™m hoping to refresh my brain on what i went over and hopefully then delve into some more advanced topics once i feel comfortable thank you and apologies if this is a repeat post been answered before
rfty6i,0,favor volume or quality for bert based text classifier ill train a binary classifier yes samples make up about 5 percent of all samples there are multiple persons doing the labelling they have a pairwise alpha of 0 65 scenario a label each sentence once and have every 10 sentence for all workers to check reliability resulting in 52000 single vote samples plus 6000 multiple vote samples by all together about 3000 positive labels scenario 2 tripple label everything resulting in 20000 samples where i can majority vote but only have 1000 positive labels in your experience is the better quality of samples worth the volume
rrbc6q,1,what advice do you have for a highly motivated newbie from a barely technical background these are my options for now 1 take an ai and ml 6 month post graduate program where iâ€™ll learn the fundamentals the curriculum looks like this intro to python numpy pandas exploratory data analysis matplotlib seaborn 5 statistics hypo testing probability courses supervised ml unsupervised ml feature engineering ensemble technique model deployment model selection and tuning ai and deep learning neural networks computer vision nlp and recommendation systems 2 take a 6 month data science and data management systems post graduate program first some ml concepts will be taught in the 6 month data science course and i have the option of adding 2 elective ai classes in this as well question do i need in depth comprehension of database systems and data science to understand the fundamentals of ml and ai 3 take a 5 month computer science for ai post graduate program here you learn fundamentals of computer science like abstraction data structures algorithms then of programming python sql css html then you learn python with ai graph search algorithms adversarial search knowledge representation machine learning reinforcement learning neural networks natural language processing and more so my main question here is holding other factors constant like difficulty level is it feasible to just start with ai and ml or will starting with data science help me understand ai ml even better or will computer science do that instead i have a long term goal an impact i want to make in the health field that requires the use ai and ml to accomplish but do i need ds and cs first
rak5zr,0,what do you do when you re not working it s no secret that this field is 1 immensely rich and 2 moves at a breakneck pace â€“ i spent this morning scrolling through the hundreds of new papers published at neurips this year which got added to the existing queue of to be read arxiv papers which sits alongside a queue of various bits of mathematics i d like to understand better which i ll eventually get to once i make my way through the backlog of my own research and other deadlines the point being it feels like there is a never ending mountain of things to do things to read things to learn in ml in some sense this is great and is what makes it imo so stimulating and exciting on the other hand it s exhausting and often makes it hard to set boundaries between work and the rest of my life i can feel myself getting more tired less happy and less productive and yet i m seemingly at a loss when it comes to thinking of ways to spend my free time three years of all ml all the time seems to have sucked out my ability to imagine other meaningful ways of spending my time i d be interested in hearing what kind of outlets other people in the community have and how they fit them in to their every day lives
qq4v0g,0,deep shallow fusion for rnn t personalization end to end deep learning models for speech recognition can produce highly accurate transcriptions but they are a lot harder to personalize this paper from facebook s ai team walks through some methods that help increase the accuracy of proper nouns and rare words from end to end deep learning models which i found really interesting i made a summary of this paper that you can read here and the link to the original paper from facebook ai can be found here
rgykys,0,i just found out that my 1 years worth of research has already been published i m a phd student in the middle of my studies a year ago i had an idea about designing a neural network for medical image segmentation using shape priors i have done a quick literature review at that time although i admit it might not have been thorough enough and i found that no one really tried to use those shape priors before especially for the task that i wanted to use them on these descriptors would fit the specific task especially well i worked hard on the implementation designing the network architecture writing the article and understanding all the necessary mathematical proofs theorems related to this task i just submitted the article a few weeks ago no word from it yet and today i found an article on arxiv no citations that has been published this spring and basically uses the same idea for the same task as i did the network architecture is different than mine and the performance evaluation is different but the main selling point of my article the usage of these shape priors has already been published i am a bit devastated at this point because this would have been my first 1st author paper and i really put a lot of effort and thought into this only to discover that my idea has already been discovered before obviously i need to do a much more thorough literature review next time so that this doesn t happen again but besides that i don t know what else i could do to mitigate the damage that has been done to my motivation i am even considering quitting phd at this moment because i feel like i wasted a lot of time because of my stupidity has anything similar happened to you before do you have any advice how could you cope with similar issues in your career
r5xx1d,0,extending trained gan hi i would like to know your opinion on extending already trained gan for example lets say i have gan trained on 3 types of classes to segment and now i wanna add 4th type to segment what is ideal approach for this can i keep already learned weights and just run training again for just 4th class or it is impossible because gan will forgot how to segment previous classes i am planning to use pix2pix model thanks in advance
r2trln,0,how do pretrained tokenizers work hi all this is my first post on this subreddit x200b i have been using the pretrained tokenizers available from the huggingface transformers library and they have been working well for my use case x200b however i have not been able to clearly understand 1 why do we need to pretrain tokenizers 2 how are they pretrained like not the code but the logic behind it 3 how do they work i tried searching for relevant literature on google as well as aclanthology but nothing insightful has turned up x200b can someone elucidate these concepts perhaps links to papers x200b your help is deeply appreciated
rd2kq6,1,need guidance for crop production forecasting so i found that my agro tech company has data for crop production from 90 countries and the forecasting model they currently use is barely 50 accurate the data has with date variety and quantity some other fields which might not be relevant my boss has told me that if i could even get a model to around 70 accuracy that would be good enough they want accuracy only on variety continent level i am new to ml have worked on simple kaggle models can someone tell me about what methods should i study which can be helpful are there similar datasets on kaggle where i can practice see work of others thanks
rfczeg,1,what happens to the output of the two lstm s in a bidirectional lstm i m implementing a blstm but i m confused as to what happens to the output of the 2 lstm forward and backward supposed we have 3 words each with a 4 length embedding i e input is of the form 4x3 one group of people say that the output of the two lstm s is concactenated i e the 4x3 goes through each lstm independently suppose their output is 4 and a 4x3 matrix is output for each lstm the result is conactenated fully meaning the result is a 12 length vector another group says that the output of the two lstms goes through an activation function independently meaning the output of the two lstm s two 4x3 matrices go through an activation function each word at a time for the first word fore example you have 4 4 inputs to the activation function another group say the output is just concactenated on a word by word basis meaning the final output is an 8x3 matrix 4 from forward and 4 from backward what i m currently using the older papers use a mix of these with the original paper using something closer to the third option thanks
rh4l9j,0,area under statistical power curve in machine learning there is the field of binary classification a common metric for measuring the performance of such models is the auroc area under receiver operating characteristics curve in statistical hypothesis testing we have the power curve which turns out to be the same as the roc curve both plot true and false positive rates while the area under the roc curve has a very nice interpretation i haven t heard anyone talk about the area under the power curve it also has an interpretation the probability a test statistic from the null will be higher than one from the alternate see here for a proof interpreting auroc in hypothesis testing by rohit pandey dec 2021 medium
qub6oz,0,does tpu v3 8 can train the big model momory for 1 batch is over 16gb i know the tpu v3 8 has 8 16gb each v3 128gb memory i want to know this v3 8 can train the big model which 1 batch size is over over 16gb 128gb memory is just like integrated memory or just we have to model parallel in this situation
rngpmh,1,how do we utilize our machine learning model in our website i m trying to make a chatbot website and is in the process of making the model once i have the model i want to implement the model for users to be able to interact wiith the chatbot through an interface made in react how do i enable this can i use pickle and pickle the model then load it into django and make endpoints for the conversations
rwc10x,0,i implemented conformer convolution augmented transformer i implemented google ai s conformer convolution augmented transformer for speech recognition paper it achieves the best of both worlds by combining cnns and transformers to model both local and global dependencies and improves the local inductive bias in transformers
r5fyxe,1,deep learning optimizers for beginners in very simple language with very neat python 3 codes learning optimizers may be the first step for beginners in deep learning but most of the time in the tutorials the codes are skipped and only mathematical notations are shown to them and people watching them simply jump to dl libraries and compile with an optimizer of their choice and sometimes beginners have no idea what is happening one example is the beginners wonder why their loss is increasing with training in the case of loss divergence problem so in these posts on medium i have talked about widely used optimizers with neat codes in very simple language i hope that these posts prove themselves useful to beginners sgd sgd with momentum sgd with nesterov acceleration adagrad rmsprop adadelta adam amsgrad adamax optimizers racing to the minima x200b if you like these posts then follow me on medium reddit and youtube for future posts in which i will talk about neural networks in very simple language with very neat codes
rhkcl9,0,does it harm your phd application if you ve worked before applying i hear this fairly frequently around me but am not really succeeding at finding evidence the rationale that i usually get is that if you work then you re away from research or lose the student mindset i m in a position where i was supposed to apply for phd programs this year but due to personal reasons am looking for work instead i m wondering if anyone here also thinks about the statement in the title
rnu744,1,recurrent neural network for time series forecasting hi i have been given a challenge for the artificial neural network course at my university which consists in forecasting 7 time series using a rnn what are some state of the art architectures i can try to improve the performance i achieve on such task thanks a lot for the help
qvfn86,0,heterogeneous processor architectures and machine learning the recently introduced intel alder lake processors and less relevantly to this application apple silicon take a new approach to processor design by providing two types of cores on the same cpu die on one side full sized high performance cores optimized for single core performance high clock rates and full support for smt on the other side smaller more energy efficient cores that make some performance compromises yet aim to be capable for more parallel tasks for the machine learning community these intel processors are really only relevant for prototyping workstations equipped with one or two gpus they donâ€™t scale to large production servers for someone intending to build a machine like this the intel 12900k and amd 5950x may be the most interesting contenders for processor choice as they deliver high performance without falling into the much more expensive workstation class xeon and threadripper processors intel has a slight edge on single core performance on most synthetic benchmarks but things are unclear when it comes to multi core performance no clear winner emerges this is somewhat digressing but there is also a discussion to be had about new forward looking features on the intel platform pcie 5 0 and ddr5 support â€” a set of upsides that also come with the early adopter instability and price inflation â€” versus the stable affordable yet not forward compatible status of amdâ€™s am4 platform as far as i am aware not much discussion has taken place over how an heterogeneous architecture might translate to training and inference workloads a lot of the press covering these new processors focus on gaming i have to admit i am myself not very well versed in the ways cpu side parallelism is leveraged in these tasks so the impact this might have on performance is not obvious to me correct scheduling assigning performance cores to the threads that actually need them likely is a part of this â€” meaning operating systems matter here i vaguely know certain models and architectures rnns rl and data related tasks augmentation and pre loading are more strongly tied to cpu performance i would be curious to have peoplesâ€™ perspectives and insights on this
rt2vwh,1,math to self learn for ai ml hello everyone i don t want to go for a cs masters so i m just thinking of buying textbooks that i can self learn from to prepare myself to know as much about ai and ml as a master s degree holder would so far i know calc1 calc2 intro level probability and linear algebra 1 if that s what the undergrad course usually is so next on my list is to learn multivariable calculus maybe from a textbook maybe from khan academy x200b but after all that what further math would i need to learn to know as much as a master s student would in ai ml x200b thank you so much
rew9uj,0,has the ml community grown too big edit probably should have titled this have ml conferences grown too big there are a lot of conferences relevant to the machine learning community general conferences like neurips iclr or icml are possibly approaching carrying capacity it also seems that 2022 will bring even more with new conferences such as colla conference on lifelong learning agents clear causal learning and reasoning and automl today a new journal tmlr transactions on machine learning research was also announced while transformers seem to be consolidating the field at least in terms of model architecture we can also clearly observe more branching of the community around specific subfields to address the next wave of problems to solve conferences for nlp computer vision robotics and ml theory already exist as well what are your thoughts on these divisions of the community how do you think future progress will be supported by these specialized conferences do you think neurips iclr and icml will hold their place as top venues or will research move away to be published elsewhere
r1sdpa,0,is it possible to work as an independent contractor in ml i am finishing my phd in nlp and if academia does not work out i was thinking going self employed as a consultant or giving courses etc i really value flexibility and independence more than money and i do not see myself in a 9 6 position i know a lot of people who are self employed in tech but they are mostly into web or mobile development is it feasible to work as an independent contractor in ml if so what would you advise edit apparently i m looking for a freelance job not for an independent contractor one
qkyyno,0,did anyone check ykilcher s video of siraj raval s interview i love yannic s video but i did not see any point of this interview i mean even in the interview siraj seemed like someone who has just started learning machine learning when he mentions about superintelligence digital organism god seems like he imagines ml as a hollywood movie much like the general person
qkbfst,0,nlp model for chatbot for inference on 11 gb gpu hello everybody iâ€™ve just found the amazing huggingface library it is an awesome piece of work i would like to train a chatbot on some existing dataset or several datasets e g the pile for training or fine tuning the model i have no gpu memory limitations 48 gb gpu is available for inference i only have a gpu with 11 gb available inference should be feasible in real time i e below around 3 seconds and the model should be adjustable i e the source code should be available to change the structure of the model what model is best when taking into account these requirements probably one of the best models is gpt j but i think for inference it needs more than 11 gb gpu are the models in the huggingface library fully customizable i e layers etc
rrxl86,1,any self taught ml engineers from a non cs background currently in my 2nd year of a mechanical engineering phd the research involves a lot of machine learning been self teaching concepts to do my research but iâ€™ve been thinking of mastering out because i just want to work already â€” tired of this student budget so iâ€™m wondering if anyone from a non cs background has self studied their way to an ml engineer role howâ€™d you do it and how long did it take you thanks
r8kfpf,0,dynamic batching for gpt j api hi i created a small fastapi back end for gpt j on huggingface with dynamic batching dynamic batching has the potential to increase throughput at the expense of the latency because it pads all the sequences in the batch to the length of the longest one in this project i determined the maximum amount of tokens batch size x sequence length that could fit in the memory by tuning the batch size while keeping the sequence length fixed until i would get the out of memory errors if anyone knows of a more reliable way to determine the optimal batch size i would appreciate any pointers the purpose of this project is to show how to implement dynamic batching for real production environments you should probably use c or go instead of python fastapi github link blog post
rqideg,1,projects for cs undergrads i m a soon to be cs grad trying to get an ml engineering job i know i need projects for my portfolio but i don t know how to go about them i have some experience with cnns and fully connected nns i m thinking of making a basic dog cat classifier for my first project but i figure that my employer will just suspect i copy pasted someone elses code since this type of project is so typical how do i make a differentiated project should i just code up a neural network in numpy to show i know how it works
rhohk2,0,what is the best way to perfectly overfit your dataset with minimal variables hello guys are there any approaches to reach 100 training accuracy using minimal variables what i can think of is just binary searching on of variables of a nn although the training of an nn doesn t guarantee a perfectly efficient usage of its parameters similar thing can be done with gbts i guess
rvt8ik,1,can i download a 20gb dataset and train my tensorflow model in google colaboratory hi guys i developed my tensorflow model in google co lab and i used a mini dataset to train the model but now i want to train my model with a real dataset which would be around 20gb of capacity i just wonder 1 how can i train the model with this much of a high volume dataset 2 it is possible to train my model with google colab 3 can i download the dataset programmatically to google colab and go ahead and start the training process fyi here is how i downloaded the mini dataset into google colab and proceed to the next steps can i download a large dataset just like this large dataset
r2rw37,1,what should i visualize for humor detection model to gain some useful insight i was going through bunch 1 1 2 2 3 3 of humor detection paper but most papers don t include any visualizations say some graph related to model being trained i was thinking to train some language models like bert gpt xlnet but was guessing what kind of some interesting visualization should i aim for in order to gather the data during training and gain some sort of insight or is it like that these fine tuning or zero one few shot learning based models don t have to train for long and does not involve significant learning from scratch or they are somewhat black boxes that s why there is nothing much to visualize 1 2 3
r8ob3o,0,visualize millions of datapoints using plotly with 1 line of code we created a python plotly figure wrapper that adds adaptive resampling to your plotly figure github repo this project aims to enable visualizing large sequential data in this example we visualize 110m data points i would love to hear your feedback on this
rkt0u9,1,last survivors of washington s army enhanced and colorized using artificial intelligence
rh9t9x,0,training machine learning models more efficiently with dataset distillation today s google ai blogpost focuses on dataset distillation arising from the infinite width limit theory of neural networks in which small learned datasets can be used to train kernels and neural networks to high test accuracy one highlight result is that by using only 10 learned images and labels 64 7 test accuracy can be achieved on cifar 10 blogpost papers discussed dataset distillation with infinitely wide convolutional networks neurips 2021 dataset meta learning from kernel ridge regression iclr 2021
r7e49h,1,recommendation just getting started in studying ml and general ai need podcasts for work hello ether i am starting my first college level courses in ml next semester and wanted to get a head start so i started taking udamy courses when off work what i want to do though while iâ€™m working is listen to a podcasts so i can maximize all my time so my question is 1 does anyone know a good podcast s that goes from a to z in regards to learning ml ai all other subjects related to the matter starting from scratch here but have a general understanding of technology 2 is there a podcast s that i should follow to keep up to date in the field or see whatâ€™s coming down the pipeline thank you all for your help and be safe in your travels
rllfmq,1,has anyone tried using linear regression not logistic as binary classifier
rjbsxn,0,need recommendation for demosaicing halftone on cpu as well as versatile film grain detection looking for existing ml programs preferably python based that are very good at demosaicing halftone printing patterns and retaining bringing back lost details as per my last post i ve only been finding ones that require cuda would also like recommendations for libraries that may be good at tackling this problem if i need to attempt to build something i do have some basic scripts that are straight image filters but would like to find or build one that is more robust for working with large batches and large sizes can work in float32 rgb and with raw formats like cr2 and dng so the metadata can be retained or float tiff and exr for when metadata is not needed as for film grain i ve noticed that most ml programs seem to handle denoising with a blanket number percentage does anything exist for setting a target grain size or channel independent grain attributes i find that with very old images you tend to get larger grain size think 1800s and the software needs to be pushed too far to remove it causing the image to become too blurry in the process thanks
rg2ba8,1,arima for prediction of hourly natural gas consumption hello all i m predicting natural gas consumption and i m trying some different methods out i have hourly observations the data has two seasonalities yearly and daily i have tried tbats for python but it takes ages to train i think this is because i have so many observations in my training set 24 365 2 17520 but i need that many to capture the yearly seasonality this is my estimator estimator tbats seasonal periods 24 365 25 24 x200b i also tried auto arima for python and used to fourier terms for modelling the two seasonalities but i keep running out of memory i m on a decent laptop i would try seasonal decompose twice and then just a simple arima but i m not sure how to add the seasonality back again does anyone have any good advice also how would i even evaluate my results i realize i can predict the consumption and then compare with actuals but how is that done on a larger scale thanks for any ideas you are able to offer
qnbrji,0,how does acl rolling review work hi folks i am going through the acl rolling review process and have some doubts so after submitting the paper at the open review website 1 if accepted the reviews comments and pdf on open review will be there 2 same for rejected papers if rejected can i withdraw my article from open review or it is going to be there along with rejected reviews on open review website my main concern is if rejected and submitted to another conference there will be plagiarism because the paper will be already on the open review website along with reviews then it will affect the other submission
rm4i9v,0,lottery ticket hypothesis paper implementation screencast hi all i created a video where i tried to implement the paper the lottery ticket hypothesis finding sparse trainable neural networks from scratch i made a lot of simplifications in order to fit everything in a single video e g only working with the multilayer perceptron and using the mnist dataset i also ran a couple of experiments to see whether i could get similar results to what the paper presented the video also contains a quick tutorial on pruning in pytorch lastly the video includes chapters so that it is easier to navigate hope some of you find it interesting don t hesitate to share any constructive feedback video link
rivass,0,looking for a deep learning python program for low light image recovery w cpu support i m on macos and looking for scripts that will run on cpu since no cuda available to me all the ones i m finding are cuda only i d prefer to use something existing rather than having to try to build it from scratch atm as i don t have the hardware for training have never created ml algorithms before and have only modified existing programs i personally need it for raw files cr2 dng etc as part of a larger project specifically this is to recover rebuild detail and remove digital noise artifacts and streaking when images are underexposed in dslrs if nothing like this exists that supports cpu perhaps you may know of some libraries that could help in building something like thisâ€”aside from the usual opencv numpy etcâ€”or perhaps some resources on modifying cuda scripts to function using cpu such as for pytorch i d appreciate a point in the right direction thanks
rie49f,0,scientific literature review generation v0 2 hello everyone i ve developed recently an algorithm to automatically generate a literature review hopefully that could be useful for the phds and the non phds more details on the algorithm here i ll be thankful if you have any remarks about it cheers
rnfmrr,0,learning both weights and connections for efficient neural networks research paper walkthrough neural networks are both computationally intensive and memory intensive making them difficult to deploy on embedded systems this research proposes a 3 step method for training efficient neural networks that are lightweight and can be deployed on device yet retaining the sota accuracy numbers paper summary paper link
rttp81,1,what i need to learn machine learning beyond a programming language i started a course on machine learning in python and everything was going well until mathematics began to appear so i leaved the course for another time and now i would like to take it up again i want to start studying statistics and probabilities but it is a very huge subject then what should i know before start learning machine learning
rdccjw,1,question for those who finished aurÃ©lien gÃ©ron hands on ml right now i m currently on chapter 17 i ve done all exercises etc but i ve problem in 3 chapters preprocessing data with tf ch 13 tfrecrods processing sequences using rnn and cnn ch 15 and nlp with rnn s and attention ch 16 these 3 chapters specifically i couldn t even handle the programming exercises and that s like killing me exaggerating ofc any help advice guidance whether was videos about this topic or only me who feel these 3 chapters programming exercises are hard i don t need the answers i ve the github s repo
rie4k9,0,machine learning research in quantitative finance automation with notion api i wanted an easy way to get a pulse on hundreds of quant research insights produced every day to share with my students at nyu the dashboard helps them follow arxiv srrn repec and published financial journal papers including important twitter and linkedin posts github repos and hundreds of blogs this can surely be done for other sub disciplines in machine learning after attempting multiple solutions over the past few months i came up with a tech stack that uses the notion api gcp functions cloud scheduler python scrapers and rss feeds ml quant website let me know if you have any questions or feedback i would be happy to share more and answer targeted questions
ro8oym,1,i want to solve andrew ng machine learning course assignments using python i am confused which library to use as there are number of libraries scikit learn pytorch pandas numpy tensorflow etc i don t know about these please guide me which one is easy for beginners
r7xih4,0,how to productionize the results of my data analysis i have a project that is supposed to do some prediction on the house prices as usual i started with doing exploratory data analysis eda in zeppelin i have reached a point where the data is enriched transformed enough that i would now like to add this to a pipeline use ci cd and so on the source data is going to be exactly the same and will arrive in batches on a weekly basis what is the deal here i now have to rewrite the stuff that i did in my zeppelin notebook as a spark application add unit tests and run it in a ci cd pipeline is this the correct approach assuming that i will receive new data every week should i keep training my model with this new data every week which kind of makes sense but what about the eda part isn t this double work i mean doing eda and then taking that code from the notebook into an application any ideas on how this is done professionally
qzr5z6,0,for those of you working as nlp engineers in industry what should you learn to get up to par basically i have some interviews coming up for some nlp focused mle jobs i m familiar with basic stuff like word embeddings rnn lstms and a bit about transformers however my knowledge of the latter is pretty shallow and there s definitely a ton more i need to learn currently i m working through stanford s cs224n problem sets and projects but my current schedule kind of forces me to skim over most of it i m also working on building an nlp web app that uses a gpt 2 model to generate the onion articles i m familiar with ml worked as mle before though mostly computer vision stuff and currently work as an mlops engineer but mostly avoided nlp until recently for those of you who currently work as nlp engineers what should you know to really qualify as an nlp engineer without an ms phd x200b edit cs224n not cs22n
rodati,1,looking to map which skills i need to pick up to implement an application idea hello there i m currently doing the andrew ng machine learning course and i m close to finishing it i have an idea for a little project which i ve been toying around with without going into too much conceptual information the rough idea is to allow multiple users to input some textual information to a website for the website to generate some summaries of identified objects and for the website to detect contradictions between provided information an example user 1 ravens are birds user 2 all pets are mammals user 3 i have a pet raven called jack whose feathers are white user 4 all ravens are black the user should be able to tell the application to track all information about the detected noun ravens the application should store all the information it knows about ravens and parse some sort of semantic understanding it should detect some second and third order contradictions too through some inference the application should detect and flag the contradictions between the facts introduced by all the users and flag them for resolution i d like to identify the holes in what i need to know i m a c developer but nlp is very new to me does anyone have any good resources or courses which are particularly applicable which i could take to point me in the right direction thanks
qqcrbh,0,intel optimized facebook dlrm with 8x speedup deep learning recommendation model intel leveraged sigopt s hyper parameter optimization platform to achieve a software speedup for dlrm additionally intel leveraged vertical split embedding lamb optimization and parallelizable data loaders
rgcrfj,0,zenml an extensible open source framework to create reproducible machine learning pipelines hey everyone i wanted to share with you an open source tool we ve been building over here in munich called zenml zenml is an extensible open source mlops framework to create production ready machine learning pipelines it has a simple flexible syntax is cloud and tooling agnostic and has interfaces abstractions that are catered towards ml workflows i think the metaflow project has similar goals but there are differences in approach not just in constructing the pipelines but also the abstraction layers provided for the underlying infrastructure could you feedback us on the vision it goes something like this create pipelines and deploy them on any stack with the above code there are of course other tools out there that look similar to the above but because zenml is focused on ml workflows here are some key advantages pipelines are data dependent rather than task dependent this means that artifacts flowing through pipelines can be modeled in a specific way to enable features like caching and lineage artifacts flowing through pipeline steps can be standardized adding a standard validation and deployment step for standard data and model artifacts steps can be standardized to enable the same effect you can then enable special features for certain steps e g distributed training for the trainer step zenml can materialize read write common objects like pandas dataframes and pytorch modules automatically regardless of the environment in which this pipeline is running local or in the cloud the data scientist can then use these objects natively as they always do this sets zenml apart from tools like airflow luigi prefect that are focused on data engineering use cases and hard to implement for ml specific tasks by both developers and data scientists all this is from the point of view of an application but what about infrastructure even with all the advantages above these pipelines and integrations need to work across varied environments and infrastructure requirements for any use case this is where the notion of a mlops stack comes in happy for feedback and looking actively for contributors create complex mlops stacks within your application we can then simply do zenml stack up this spins up the infrastructure for you on a target of your choosing in addition zenml takes care of deploying your pipelines to the relevant stack automatically e g try spinning up a kubeflow based stack on your local machine with this simple command zenml will build the container for you create the kubeflow pipeline and run it automatically with a simple command in the future we hope to expand this to include more complex deployments so what do you think links below github a star would be appreciated why docs
rj5rfb,1,need help hello i am not sure if i am being silly by posting this but i donâ€™t know anyone who can guide or help me with this this is why i am posting it i started working on nlp at the beginning of the lockdown almost 2 years ago since then i love working on nlp and i did invest countless hours on it before that i already had a good understanding of data science the more i learned about nlp the more i liked it there was a time when i was working full time and still giving 3 4 hours everyday to learn nlp while doing all this at the beginning of this year i did 3 months of non paid freelancing as well and there i learned more about nlp and web scraping eventually i was at a stage where i could say i know enough i guess to look for freelance or a full time nlp job finally when i started looking for freelance work or a fulltime job i realized in the nlp field people look for phd or ms holders candidates and in my case i am not even graduate i thought okay i do not have this degree but what if i will show my talents then i might be able to find the right people i started working on a few projects and posting it on different social media sites but that was not helpful now i am at the stage where i feel like giving up on nlp and trying to just focus on python and get something there but my heart still wants to work on nlp i am not sure what to do please help thanks in advance
rg44wl,1,international data analysis olympiad idao 2022 x200b we invite ml students and specialists from all over the world to take part in the international data analysis olympiad hse university and yandex are organizing it for the 5th time and the otkritie bank will be our platinum partner this year since itâ€™s our first anniversary we decided to change the format students and ml specialists are divided into two separate divisions only students are able to join the main competition â€” the student division all others can join the open division to participate hors concours for their own interest traditionally the first stageâ€™s task will be given by the laboratory of methods for big data analysis lambda hse university it will be about predicting the properties of two dimensional crystals of various configurations the task for the finals will be provided by the otkritie bank the olympiad includes two stages online stage 1 28 february 2022 â€¢ track 1 traditional machine learning competition on yandex contest platform you will need to make new predictions and upload them to the automatic verification system â€¢ track 2 come up with a solution for the same problem keeping within a rigid framework of time and memory used final 16 17 april 2022 moscow â€¢ top 30 teams according to the online stage results will be invited to the online final â€¢ in the final 36 hours of the competition participants will try not just to train the model but to create a full fledged prototype which will be tested both in terms of accuracy and performance registration is open till february 13
rwig2c,1,how to implement minibatch smote i have over 25000 images and 4 classes one of the classes has a small number of samples so i thought of using smote i need to use mini batch approach since all the images cant be loaded into memory pls help
r5x0v2,0,ama with david bau and i study the structure of the complex computations learned within deep neural networks david bau believes the members of this subreddit would be interested in his ama note that discussion questions and answers are in the linked thread above not this one
rb7c6j,1,using bayesian belief networks with numerical data is there a way to do this all the tutorials i ve seen are with categorical variables makes sense given the probabilities associated with each node is there a way to use bbns with numerical data without transforming it to a categorical dataset
rciodi,1,why does my model perform better when i consider an unimportant feature hey everyone i am currently a part of a kaggle competion where i need to predict a the probability of default of a particular dataset so i currently have a trains csv where i performed feature engenering one hot encode the categorical values and dealt with missing values outliers and a test csv where i test the performance the model considering unseen data all good here implemented a logistic regression got the appropriate metrics but did notice something odd i forgot to drop the variable customer id went back removed it the model performed worse and when using the predict function i am nonly predicitng 0s and not a single 1 however if i consider this variable the model performs much better and i am able to predict both 0s and 1s made a submission file for kaggle with the customer id and a column risk with the predictions and got a good score i am not understanding how is this even possible why am i only prediciting 0s if i do not consider this variable i have a notebook with me if someone need to look at a specific part thank you for your time
raly1j,1,can i use the mathematics for machine learning book as a one stop shop for covering my math essentials question mostly is in the title i am aware that the book omits proofs and isn t as rigorous as a math textbook and that it doesn t hold all relevant material this is why i am asking if you think it covers the essentials sufficiently to get one going i find myself not knowing what i m looking for when i search for general information and courses on different domains of mathematics it gets me confused because i don t have a good enough map of the math at that point i am trying to determine if there is a single pill i can take which would suffice my full question is do you think the topics covered plus the further reading portions of each chapter are sufficient in terms of what they teach and for getting a good map of what math is needed and why if not what other sources oriented at ml would you recommend to address the deficiencies thank you
r6314s,1,how do you feel about job recruitment in ai survey hey everyone my name is davis and i work for a consulting organization called illinois business consulting weâ€™re working for a company in the ai sector and are hoping to improve their recruitment we made a survey about ai recruitment so if you guys wouldnâ€™t mind filling out the survey below it would be awesome thank you again
qqckfw,0,message from r mlops announcing our first ama hi mod of r mlops here we finally managed to book our first ama which should interest some of the 2m members here the following is the content of the sticky over at our sub would you please help us grow our community of mlops enthusiasts by not burying this post å…« ï¼¾â–¡ï¼¾ i don t know if you remember but we were going to have amas here to celebrate the fact that there are so many of us naturally since this is a niche subreddit it wasn t as if top tier mlops superheroes were lining up to post an ama here but then a miracle happened i am delighted to announce that alessya visnjic will be doing an ama here this thursday so spread the word and let s make this ama be the first of many successful ones and just in case you are not as immersed in the mlops ecosphere as i am here is her bio alessya visnjic is the ceo and co founder of whylabs the ai observability company on a mission to build the interface between ai and human operators before whylabs alessya was a cto in residence at the allen institute for ai ai2 evaluating the commercial potential for the latest advancements in ai research earlier in her career alessya spent 9 years at amazon leading machine learning adoption and tooling efforts she was a founding member of amazon s first ml research center in berlin germany alessya is also the founder of rsqrd ai a global community of 1 000 ai practitioners committed to making ai technology robust responsible of course there s always an ulterior motive alessya will be focusing on the recent announcements by whylabs their round of funding and their new saas solution called ai observatory personally i think their corner of the mlops tooling space is super exciting and whylabs are doing some hard opensource groundwork additionally their marketing is not spammy so it s an honor to host them on the sub
rvn3dh,0,sieve we processed 24 hours of security footage in 10 mins now semantically searchable per frame hey everyone iâ€™m one of the creators of sieve and iâ€™m excited to be sharing it sieve is an api that helps you store process and automatically search your video dataâ€“instantly and efficiently just think 10 cameras recording footage at 30 fps 24 7 that would be 27 million frames generated in a single day the videos might be searchable by timestamp but finding moments of interest is like searching for a needle in a haystack we built this visual demo link here a little while back which weâ€™d love to get feedback on itâ€™s 24 hours of security footage that our api processed in 10 mins and has simple querying and export functionality enabled we see applications in better understanding what data you have figuring out which data to send to labeling sampling datasets for training and building multiple test sets for models by scenario to try it on your videos visual dashboard walkthrough
r3krw2,0,paper explained implicit mle backpropagating through discrete exponential family distributions video walkthrough backpropagation is the workhorse of deep learning but unfortunately it only works for continuous functions that are amenable to the chain rule of differentiation since discrete algorithms have no continuous derivative deep networks with such algorithms as part of them cannot be effectively trained using backpropagation this paper presents a method to incorporate a large class of algorithms formulated as discrete exponential family distributions into deep networks and derives gradient estimates that can easily be used in end to end backpropagation this enables things like combinatorial optimizers to be part of a network s forward propagation natively x200b outline 0 00 intro overview 4 25 sponsor weights biases 6 15 problem setup contributions 8 50 recap straight through estimator 13 25 encoding the discrete problem as an inner product 19 45 from algorithm to distribution 23 15 substituting the gradient 26 50 defining a target distribution 38 30 approximating marginals via perturb and map 45 10 entire algorithm recap 56 45 github page example x200b paper code tf code torch
rg650d,0,time to event survival analysis for churn prediction telecom hello so i ve been studying different models to do churn prediction in a time to event manner as such i ve came across simpler models like random survival trees support vector regression and simple nn and rnns i was wondering for anyone doing research or working in survival analysis what are the current state of the art models or more advanced models for time to event churn prediction thank you
r8ce7f,0,discussion the most painful thing about machine learning hi all i am curious how do y all approach debugging your models i am a software engineer and i find that with all other code its usually easier to test out and spot bugs quicker but for ml its always a very long process if the problem is actually in the code and not in the datasets are there any tools y all using and if so what are pros and cons about them compared to doing it manually
radg0c,0,using reduced size databases with alphafold multimer has anyone had success running alphafold multimer with reduced size databases e g small bfd by default it doesn t seem like this functionality is supported by deepmind so i d like a second opinion on whether something like this is currently possible
rgxzhq,1,how do i generate segmentation masks from pixel level annotated images i got csv file with pixel coordinates how do i convert those values to a segmentation mask
rur2j3,0,i like yolov5 but the code complexity is i like yolov5 but the code complexity is i can t deny that yolov5 is a practical open source object detection pipeline however the pain begins when adding new features or new experimental methods code dependencies are hard to follow which makes the code difficult to maintain we wanted to try various experimental methods but hate to write one time code that is never re used so we worked on making an object detection pipeline to have a better code structure so that we could continuously improve and add new features while easy to maintain and we applied ci formating linting unittest to ensure code quality with docker support for development and inference our docker supports the development environment with vim our code design from the beginning was to try various experimental methods with fewer efforts the features so far developed are as follows 1 you can easily use the trained model for another project without code copy and paste pytorch requires model code to use the model we build the model by the library that builds the pytorch model from the yaml file so the trained model is portable with pip install kindle 2 model compression support by tensor decomposition and pruning 3 export model to torchscript onnx and tensorrt 4 inference with torchscript and tensorrt 5 wip c inference with torchscript and tensorrt 6 auto search for nms parameter 7 wip knowledge distillation support 8 wip representation learning support ayolov2 also supports w b with model upload and load function to make trained models easy to manage python3 val py weights j marple ayolov2 179awdd1 for instance the above single command line will download the trained model from w b and run the inference by the time you read here you might wonder why the name is ayolov2 ayolov2 comes from auto yolo v2 our initial goal was to implement an auto model architecture search and v2 represents that there was v1 where did v1 go we have built an auto model architecture search based on the original yolov5 and it worked pretty nice but it became unmanageable please stay tuned nas feature will be coming soon if you have any suggestions or feedback any kind will be appreciated thank you and happy new year
revtrf,1,which is best youtube channel to learn machine learning i just learned python basics and now want to learn ml please recommend best yt channel and course if any
rbrx6y,1,need any insights possible on this a i project people of reddit save my soul hey folks of reddit i m in a dire situation i need to complete my college project on artificial intelligence but i m so lost i don t know how to build this as i don t have solid experience in a i i ve only ever built apps and websites the project given to me is detecting seat occupancy using a i so for example if i take a photo of 9 seats 3 of which are occupied by my friends the a i should be able to tell that 3 out of 9 seats are occupied how can i go about doing this i can some grasp terms in deep learning since i ve played around with it a tiny bit any help would be greatly appreciated cheers
qvt9vu,0,how do i integrate squeeze and excitation block in yolov5 from my understanding squeeze and excitation block improves performance of networks at the cost of additional computation i m trying to integrate it into yolov5 for object detection but i am struggling to figure out where and how to integrate it i have been unable to find any open source implementation for it so i would really appreciate any help
r8v65d,1,career advice how can i best prepare myself to get a purely r d data science position i am currently in the last semester of my master s and i was hoping to get a taste of industry for a couple years before pursing a phd or perhaps to pursue it part time either way my goal is to secure a data science position focused on performing research and with the potential to publish papers not just deploy and maintain models big data i have an offer to work as an associate data scientist at a local startup but they made it clear i would be spending half my time doing development and the other half r d the data science team is just two people and is just starting out so i m somewhat concerned that i won t have much opportunity to expand my ml skill set there however i also have a good chance of securing a full time research intern position through a government organization that my research advisor is involved with but the position is only for six months even so another opportunity could arise after this or i ll have an extra six months to hunt for jobs i m wondering a couple things 1 is a research intern position through a government org considered industry experience in regards to hiring managers i d like to not be restricted to applying to entry level positions 2 would it be better to obtain industry experience at the start up even if there s a chance i mostly won t be doing r d or to get more experience doing full time research even if it s temporary 3 is it plausible to secure a position doing 100 r d without a phd which of these options would be a better stepping stone towards that
r9kpez,1,can anyone explain me how do i make predictions on a test dataset hey everyone i am fairly new to machine learning so aplogiws for any incorrections for a school project i have to implement a logistic regression model to see the probability of a customer to enter a default state the teacher provided us with a train and a test csv and i need to make a submission in a competion on kaggle with the customer id and with the predictions so i developed a notebook performed eda selected the features that i wanted ploted confusion matriz and checked performance metrics i know have the test data set but i am not sure how am i suppose to procced for instance on the train dataset i did drop some features dealt with outliers and missing values performed one hot encoding should i do the same for the test set i know that i have to use the predict funtion in python but all i am getting is 1 i have a list with the colimns that i want to use to make predictios but i am only geting 1 as a result and cant understand why sorry for the long post but any help is really apreciated
ru4odm,1,what laptop should i get for machine learning i am just getting started with machine learning i figure it does not matter too much what type of laptop i get but any suggestions would be helpful i come here because data science machine learning will be my primary focus of study i assume early on it wonâ€™t matter much what i get but i want something that i will be able to use long term as i improve i have heard linux is the best os to use but other than that i donâ€™t have much knowledge on what i should get any recommendations trying to keep it under 2 500 us edit i am looking at the dell precision 7560 workstation processor 11th gen intel core processor i7 11800h 8 core 24mb cache 2 30ghz to 4 60ghz 45w operating system ubuntu linux 20 04 graphics card intelÂ® uhd graphics for 11th gen intelÂ® processors display 15 6 fhd 1920x1080 60hz anti glare non touch 45 ntsc 220 nits cam mic wlan memory 32 gb 4 x 8 gb ddr4 3200mhz non ecc sodimm hard drive m 2 2230 256 gb gen 3 pcie x4 nvme solid state drive thoughts
rhyhah,1,looking for data science mentees hi i am a software developer in goldman sachs and i am looking for websites where i can work part time as a beta tester or a mentor for students on an hourly basis please let me know if you are aware of such opportunities ex coursera mentor udacity mentor
qmq67b,0,feedback on our idea a platform that turns code into a monetized api hi we re building a platform that turns code into an api hosted by us infrastructure is paid by whoever calls your service we charge the exact amount the infra costs us if you choose to you can also monetize your service as a data scientist myself i m really excited about the possibilities of easily sharing my models capabilities as an api without dealing with dev ops but i m biased because i love what we re building i really want to know what do you think about using such a platform not mentioning the name not sure it s allowed how many of you both create ml models and know how to set up your own api and monetize it we want smart people to create smart solutions and be able to share them easily with anyone creating such a service would be free signing up is free and as a creator you can only earn if someone is using your service any thoughts offer
rd3hwy,0,wavenets a comprehensive presentation with code i have been studying wavenets for some time now and have published my experiences at as a presentation with code wavenets are exceptionally efficient in capturing distant relationships with few parameters and i think the concepts are not immediately clear to everyone the very property that wavenets are generative models allows us to use the same model to both generate new data continue existing data or progressively classify observations as shown below for illustration in my approach wavenets are not applied to sound as usual but to random fourier series and mnist images the image below shows progressive classification of mnist images by a wavenet trained on p x image y class classification is performed via bayes rule i would be happy if this presentation helps some of you to understand wavenets better and maybe use them in your next project
qmln6l,0,washington u google study reveals how attention matrices are formed in encoder decoder architectures in the new paper understanding how encoder decoder architectures attend researchers from the university of washington google blueshift team and google brain team propose a method for decomposing hidden states over a sequence into temporal and input driven components revealing how attention matrices are formed in encoder decoder networks here is a quick read washington u google study reveals how attention matrices are formed in encoder decoder architectures the paper understanding how encoder decoder architectures attend is on arxiv
rxf7wq,0,mdli ops â€“ a free conference to help you make sense of the mlops landscape disclaimer not my conference but a good friend s work he s doing an awesome job community building and i thought this might interest the community here as well i am not a sponsor hey r ml some of you might have heard about mdli â€“ short for machine deep learning israel it s an independent israeli community over 25k members for professionals in data science and ml and they are having a conference for everyone in english in just 14 days you can register here i know that sometimes these free events tend to feel like commercials but you should really check out the agenda some of the talks are going to be by ml teams at companies like appsflyer and bigpanda explaining how they built their internal stacks and ml systems there s also a super interesting talk by the nvidia team where they will talk about building supercomputers to train ml models on them this one seems crazy awesome to me it s also a good opportunity to listen to offerings by some cool ml startups which might give you a better understanding of how they compare and what you actually care about when choosing mlops tools the organizers will probably be here in the comments if you have any questions but in my opinion every event this community organizes is really awesome and i get to learn a lot so i really recommend this
qxive5,0,bias in criminal sentencing and parole i m a student at a high school and i have a presentation that i need to make on machine learning bias in criminal sentencing and parole if you have any information on this matter such as specific examples of when it was used or possible ideas to eliminate bias i d be glad to hear them
ri7m95,0,discussion how important is graduate degree i always suspect that graduate degree is needed in machine learning in industry so i basically scraped 500 job posts with mle and basically filter for education level key words and here are the results i just want to share with everyone with these statistics so of all 500 job posts that re mle out of the ones that did mention education level not all job posts mention education level 71 mention bs 58 ms 44 phd i guess my takeaway is that compared to other specialties ml does tend favor graduate degree a lot more compare to swe devops etc but it s not a necessity do u guys agree x200b source
rgmgcn,0,10loc pytorch wrapper of tokenlearner hey everyone i recently built a pytorch wrapper over tokenlearner and tokenfuser from the paper tokenlearner what can 8 learned tokens do for images and videos by google ai for better image and video understanding check it out it s fully compatible with existing pytorch modules and layers you can plug and play with vision transformers as mentioned in the paper do star and share if you find it useful
rmu8gx,1,dbscan clustering algorithm interesting article for dbscan clustering algorithm with hyperparameter tuning
rav2l3,1,starting machine learning from a natural science standpoint hi i am currently working on my phd in material science as for my research i am trying to accomodate machine learning into my work which requires quite a deep understanding of it i cannot just tensorflow my way out of this one most research papers actually involves creating a package from scratch as the ml concepts are being adapted to molecular atomistic calculations i am trying to find a good book that can give me 1 a good perspective of the things happening in the ml space what models techniques are available and what is its used for 2 a good rigour but not overly abstract introductions to the statistics and linear algebras used 3 equip me with skills understand research papers s methodology and hopefully dissecting their source codes that were made from scratch most oftenly coded in python c or fortran language isn t really a barrier to understanding but the implementation of the ml concepts instead some of the good book candidates that i have are a an introduction to statistical learning b pattern recognition and machine learning c deep learning d understanding machine learning from theory to algorithms e machine learning a probabilistic perspective f the elements of statistical learning i know that reading everything should be beneficial but thats a bit unrealistic especially for a phd student haha x200b any help and opinions is appreciated thanks
r43jhv,1,deep learning with python good for absolute beginners in ml hello everyone i will keep this post short basically i want to get into deep learning for nlp but i have absolutely no background in higher mathematics or ml ai or dl whatsoever i am an intermediate python programmer with basic skills in other programming languages i would like to now if this deep learning with python would be a goood start for me or would it be better to go for the standard machine learning with andrew ng thanks so much for your support and sorry for the newb question
rqq3xq,1,which of these 2 school courses would be most useful for machine learning advanced linear algebra eigenvalues eigenvectors diagonalization orthogonality svd complex matrices infinite dimensional vector spaces and vector spaces over finite fields probability iii finite markov chains stationary distributions time reversals classification of states classical markov chains convergence in total variation distance and l2 spectral analysis relaxation time monte carlo techniques rejection sampling metropolis hastings gibbs sampler glauber dynamics hill climb and simulated annealing harmonic functions and martingales for markov chains of course i have taken introductory courses on lin alg and probability up until these topics thanks
r5vyec,0,getting started with explainable ml are there any good resources to get started with explainable ml thanks
rhiqoc,0,project ru dalle diffusion better images from ru dalle with diffusion decoder link i previously posted this project which uses a ddpm instead of vqgan for the decoder layer this new model applies the same idea to ru dalle the ddpm model can produce much more detailed images compared to the default vqgan realesrgan setup decoding from the exact same image embeddings because the ddpm model is trained on 256px images with 16x16 token dimensions it will output 512px images when given 32x32 tokens from ru dalle the model architecture is the same as before i took the pretrained ddpm models released by openai and modified the middle block to take image embeddings as input fine tuning the pre existing openai model allows me to train it in a reasonable amount of time on a prosumer ml rig but it constrains certain architectural decisions specifically the middle block must take embeddings with 8x8 dims we do one downscale pass on the 16x16 embeddings it should be possible to get better performance with fewer parameters with a custom ddpm that s matched to the image embedding dimensions
rv8ubv,1,hugging face pytorch lightning tutorials hi everyone i recently graduated top of my class at a great uk university i also worked for an nlp lab during my studies i recently switched to a computer vision researcher role and work on ml models for the majority of my time anyway on the side i have started making practical coding ml tutorials as this is my favourite way to learn at the moment my guides focus on on creating hugging face language models this is due to my main focus at university revolved around hugging face transformer models etc during my studies i thought the content on youtube was good but not always perfect for what i was looking for hence me putting out these guides for people in the future doing similar projects have put out two videos so far one on the basics of the hugging face library the other a guide on multi label classification that beats the state of the art on the dataset to my knowledge future ideas include a masked language modelling task and a cifar10 cnn guide yes i plan to make tutorials for computer vision in the future would be great to get feedback and suggestions on my current videos let me know if you find any of it useful thanks multi label classification hugging face basics x200b x200b
qkxatt,0,aaai fasttrack 2021 review results good luck everyone results gonna be out soon for aaai 2022
r15jix,0,has anyone used algorithmia as a model deployment tool i m unconvinced by it algorithmia advertises themselves as an mlops platform for data scientists and they provide an easy way to host models on a scalable rest api this sounds like a perfect solution for a data scientist or hobbyist who wants to host models for cheap and not worry about the devops but as i ve gotten more familiar with it i have more questions for the base tier algorithmia requires you to host your model s request handling code on a github repository owned by them a separate repository for your request handling code seems like a strange pattern to develop in they also encourage you to develop in their web ui again another pattern that feels forced they also have an ominous section in their terms of service that says you do not transfer ownership of the software to algorithmia but you do hereby grant algorithmia a fully paid up and royalty free license to use and permit others to use the software which feels overly aggressive for forcing you to use their source hosting between an unnatural development environment and a sketchy ownership clause i m reluctant to continue using algorithmia has anyone had similar experiences with algorithmia am i just being overly skittish and misinterpreting the ownership clause are their better repository patterns git subtrees that other people have used with them are there better companies to host models or should i have never even attempted leaving the aws and gcp hosting land
rfvb02,0,advice on stabilizing unstable dqn card playing agent i m attempting to train a reinforcement learning agent to play the game sushi go using deep q learning with a memory recall buffer and separate model and target networks in its current form the q network takes as input a 25 element state vector and returns a 12 element vector of action values with one hidden layer in between all layers are fully connected with relu activation functions i ve been doing a search to tune the size of the hidden layer trying values between 15 and 30 neurons my methodology is to run five trials per network configuration as follows starting with a randomly initialized network run 2 500 training games then evaluate the resultant performance over 10 trials of 100 games each while doing this search though i noticed that the network s behavior is wildly unstable starting from a randomly initialized model no seed the same network configuration will sometimes reach a 90 win rate after playing 2 500 games while other times it will quickly plateau at a 10 win rate and show no improvement i ve tried longer trials 25 000 or 50 000 games for example and seen similarly unstable results anybody have any advice for tuning this performance i ve tried adding an additional hidden layer to similar results my next best guess is to start playing around with the state vector any thoughts are appreciated
r74ejm,0,data search engine for machinelearning hi there in my work as a ds i constantly have to solve the problems of increasing the quality of models and achieving the best result the most obvious way for me to increase quality is to add new meaningful data how do you search for data for your ml tasks
r736yo,0,pls help with a cv problem guys i am trying to make a denoising model for a specific type of noise on geological data kinda like diffusion waves it looks like the image on the right do you have any ideas on how i could model that noise to slap it onto synthetic data to make a denoising model or maybe you know about some dataset like that or an open source denoising model i tried 2d fft it doesnt really give this type of noise looking for some other optins details of the noise should for real look like this any other kind of noise wouldnt do
r6k9n4,0,detect ai generated images vs authentic photos there s more and more content generated by gans i thought it would be useful to detect whether a piece of content is generated by ai i m starting with images and here s a demo suggestions are welcome
r6kviy,0,are image transformers overhyped metaformer is all you need explained 5 minute summary by casual gan papers unless you have been living under a rock for the past year you know about the hype beast that is vision transformers well according to new research from the team at the sea ai lab and the national university of singapore this hype might be somewhat misattributed you see most vision transformer papers tend to focus on fancy new token mixer architectures whether self attention or mlp based however weihao yu et al show that a simple pooling layer is enough to match and outperform many of the more complex approaches in terms of model size compute and accuracy on downstream tasks perhaps surprisingly the source of transformersâ€™ magic might lie in its meta architecture whereas the choice of the specific token mixer is not nearly as impactful full summary blog post x200b metaformer arxiv code subscribe to casual gan papers and follow me on twitter for weekly ai paper summaries
rtymaq,1,ml map predicting hello i m not sure if this is the right place to post this but i ll give it a shot anyway i play game called warframe which is really fun but one of the ways you can progress is to open something called dragon vaults and the only way to do that is to go into a map and search for a door that randomly spawns around the map it normally takes me 5 to 10 minutes to find it which is way too long considering what you get i was thinking of doing a bunch of runs maybe even crowdsourcing taking screenshots of the map screen labeling where the dragonvault was and then feeding it back through ml to see if it can predict where the dragonvault will be because most things in games aren t actually random and there are some patterns i have noticed and i m not that good at pattern recognition
r7seqs,1,python interview questions greetings i made an android app called python interview questions it is intended for python software developers over the world it is helpful not only for job interview situations but also for refreshing many aspects of python programming language during normal working schedule it provides 140 python questions with answers and code examples the knowledge is divided by 8 categories including data types operators classes and oop numpy pandas and more there is also a random questions game try it to test your knowledge
ruobs9,1,an interesting post in deep learning for beginners wild cats image classification using deep learning wild cats image classification using deep learning x200b
rrapbr,1,need help hello everyone i m a beginner to ml do you have any tips or suggestions to start with also please suggest some good online resources
rev9yn,1,tutorials beginner friendly resources hi all in short iâ€™m a beginner to the whole ai ml world and iâ€™ve been doing some beginner tutorials for the past week or so i find i excel with project based learning and iâ€™m wondering if there are any go to tutorials for building a text classification sentiment model that anyone would recommend not too concerned about libraries or third party im a programmer of over 15 years btw and so when i say beginner beginner i mean specially to ml can read and somewhat write with python but no build experience in it
r7f3h3,0,aaai rejected paper despite accept reviewer and metareview scores we submitted a paper to aaai and obtained all accept and weak accept scores with the metareview also positive and recommending acceptance of the paper despite this our paper was rejected without any reason we contacted the organizers and obtained a default response borderline and weak accept scores typically do not directly translate to an accept decision unfortunately there was a great deal of inter reviewer variability across papers which required some calibration the negative remarks associated with some borderline rejects by some reviewers were comparable to those with accepts by others some meta reviews were less informative than others or failed to capture the totality of reviewer comments the ac often weighed in on the spc recommendations as did the associate chairs there were many cases where the rebuttal included additional details and experiments that were not included in the paper but unfortunately given the challenges of conference review there is no option of â€œaccept subject to revision â€ given the scale of aaai and the timeline involved it is not possible to reconsider decisions we hope the reviews are useful to you in revising the paper for submission to another top tier conference in the near future unfortunately two of our scores are not weak accepts but rather full accepts with the other two scores scores weak accepts what do you do in such a situtation i ve submit published papers at neurips icml iclr in the past and have never had such a terrible experience is aaai even a top tier conference feels extremely shoddily organized
r0428v,0,regarding phd admissions in ml how much will not having a first authored publication hold me back title is the question i m graduating with a master s in cs and don t have a first authored publication my undergraduate wasn t in cs so i pretty much learned how to actually code when i started my master s i managed to get my name on 3 conference proceedings and 1 preprint and also helped our lab s team win an award in a large ml competition recently but alas i don t have a first authored publication i recently attended a conference and talked with some phd students there and many of them said that it s pretty much a red flag in this day and age if a phd applicant doesn t have a first authored paper i m wondering how much truth there is to this obviously having one would be the best scenario but how much would not having one hold me back thanks
qr6bu6,0,discussion plaforms frameworks for backtesting and regression testing lots of models i have a need to test lots of models submitted by different teams there will be baseline curated datasets but there will also be updates as new data comes in from the field models may have different preprocessing requirements we ll need to retrain models on the updated data evaluate the models and archive the reports and models configuration management most of this will probably need to be queued up something like slurm along with the need to asynchronously monitor performance of multiple dgx servers are there commercial tools to manage testing of a fleet of models and the data must work offline
rfmxi5,0,what to do when you find a closely related paper has mistakes hello guys i am a new master student in ml and currently writing a paper i found that there is a very closely related paper submitted to arxiv several months ago but the main proof in this paper is wrong for certain i have contacted the author but heard no response so how should i treat this in my own paper of course i can t just ignore it and not cite it but if i point out this mistake in my paper will it make my paper dangerous during the review process in a conference sorry if it is a silly question i am just worried about my first paper
rmx04g,1,what is k means clustering a 2 minute visual guide x200b x200b ðŸ”µ k means clustering ðŸ”µ ðŸ”± the k means algorithm divides n data points into k disjoint clusters it is a well known unsupervised learning model this means that the k means algorithm only requires the data points and does not need the corresponding cluster that each point belongs to as this is what the algorithm figures out âœ¨ the clusters are found by allocating points in such a way that the total variance of the points within each cluster intra cluster variance is reduced or minimized although it iterates quite fast the k means algorithm can have varying cluster formations based on the initialization it has been widely implemented in many software packages the scikit learn package for python is the one that i have used most often ðŸ” the most common method to perform the clustering is iterative it alternates between two steps 1 assigning each point to a cluster based on the point s closeness or distance to the cluster center and 2 updating the cluster center called mean or centroid hence the name k means one for each of the k clusters based on all the points that belong to this particular cluster the iterations are usually performed until the centroids stabilize converge between consecutive iterations ðŸ¤“ k means is popular in scenarios where the data is known to consist of multiple groups distributions but it is unknown which point belongs to which group cluster it can be used for data analysis and splitting data that comes from multiple distributions image segmentation color quantization among other things i have been studying and practicing machine learning and computer vision for 7 years as time has passed i have realized more and more the power of data driven decision making seeing firsthand what ml is capable of i have personally felt that it can be a great inter disciplinary tool to automate workflows i will bring up different topics of ml in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike the posts will cover topics like statistics linear algebra probability data representation modeling computer vision among other things i want this to be an incremental journey starting from the basics and building up to more complex ideas i wanted something where i would have more of a personal touch so i decided to have them as hand written notes with self drawn figures it takes time to create a post from deciding the content making it concise and coming up with ideas on how to best convey with illustrations i wanted to give something that reaches a wider audience a shot so here i am if you like such content and would like to steer the topics i cover feel free to suggest topics you would like to know more about in the comments
rwy8za,1,do i need study data structures and algorithms for machine learning many people online are emphasize so much on the relevance of the knowledge data structures and algorithm as a programmer so it has left me wondering if its applies to one going into machine learning having spent the last 3 months learning python i dunno if i should take a detour and learn it before going on to study machine learning maths thanks in advance for your time
rxczzl,0,deepchecks an open source tool for high standards validations for ml models and data hey everyone i wanted to share with you an open source tool we ve been building for a while deepchecks is an open source tool for validating testing models and data efficiently deepchecks is a python package implementing validations and tests needed in order to trust an ml pipeline it contains many built in checks such as verifying the data integrity inspecting its distributions validating data splits evaluating your model and comparing between different models in addition it contains test suites similar to the test suites in software programs that can accompany you through all building blocks of the ml pipeline development each test suite contains checks necessary for the specific part in the pipeline the suite result looks something like this x200b suite result the suites and checks have a simple syntax and are highly customizable if you want to jump right in you can try it out in the quick start notebook what do you think iâ€™ll be happy to hear your thoughts and feedback
qlrxzz,0,learn ai together discord community is looking for experienced people to share their projects and willing to help other ai enthusiasts by answering questions from time to time hey everyone we are looking for researchers teachers teacher assistants or professionals willing to help and exchange with people learning ai by answering questions from time to time we are an ai focused community of over 20 000 people where members can chat ask questions share resources and projects share find job offers etc we are now focusing on getting experts or advanced members to join us and help us help others everyone else in the field is welcome to join as well more info about the community and to join us learn ai together excited to chat with you there note that this is not a paid opportunity and we won t be asking for hours or anything help whenever you can
r8ge6o,0,paper overview nÃ¼wa visual synthesis pre training for neural visual world creation video paper code abstract this paper presents a unified multimodal pre trained model called nÃ¼wa that can generate new or manipulate existing visual data i e images and videos for various visual synthesis tasks to cover language image and video at the same time for different scenarios a 3d transformer encoder decoder framework is designed which can not only deal with videos as 3d data but also adapt to texts and images as 1d and 2d data respectively a 3d nearby attention 3dna mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity we evaluate nÃ¼wa on 8 downstream tasks compared to several strong baselines nÃ¼wa achieves state of the art results on text to image generation text to video generation video prediction etc furthermore it also shows surprisingly good zero shot capabilities on text guided image and video manipulation tasks
rbgeq0,1,can someone help me derive the equation used in this paper about nmf x200b so i am reading this paper that used nmf for its data analysis of course i have already read the paper and have already gotten some insights on what some of these concepts mean but the majority of the concepts are still very vague to me because the terms are very unfamiliar to me as a biological sciences major can someone explain the following concepts to me like i m five 1 which language was this is this python if i want to learn how to utilize this tool should i take a crash course on python would i be able to understand what initialize and until convergence mean if i do that 2 what does it mean when you initialize how is it related to the return h below 3 what does the until convergence mean 4 what does the â† mean 5 can someone derive formula 1 and formula 2 from formulas a b and c 6 in formula one what does the v wh mean how can you divide a matrix over w and h 7 in formula one can the right side of the component wise multiplication be written as matrix w t Ã— matrix v Ã— matrix w 1 Ã— matrix h 1 if so i am aware that in matrix cross product the order of multiplication matters how will i know what is the order of the multiplication did i provide the correct order for the cross product 1 lastly in formula 2 what do the four vertical bars in hg mean
qnhtxj,0,a place to create your online data science portfolio and browse the community data science projects hey all i m a data scientist who has shifted career from the biomedical field now working at a tech company it was hard to learn data science skills showcase them to my first employers and stand out that s why i created datascienceportfol io you can create your own online portfolio showcasing your projects and skills in an effective way also you can get inspired by browsing projects created by the community still early days and i m now working on a section to browse projects of other people and get inspired please let me know what you think any feedback or improvement ideas are very welcome d thanks so much pasquale
rjk88d,0,cheap conferences neurips last week was my first conference only cost 25 dollars for me as a student are there any other good machine learning conferences that are affordable aaai 22 e g i find a little expensive at 145 dollars for students
r2x9eg,1,linear regression question code i am trying to implement simple linear regression but the code explodes when the independent variable x gets to high ranges here is the code it works for small data sets but when i try to scale it to anything moderately large like 1000 data points on the x axis the weight goes to nan actually this kind of seems like it makes sense based on the math tho let s say we have two samples with equal error one at x 1 and another at x 1000 if the linear model is off by 1 for both then the gradient of the loss is equal 2 y y pred 1 2 1 1 but the gradient wrt to the weight is wayyy bigger on the large x e g 1 for x 1 and 1000 for x 1000 what am i missing here
rc44cl,0,best in style transfer for distributions hi everyone i am working on a style transfer project inspired by the recent results of toonification projects based on stylegan e g 1 now all of the stylegan based papers that i ve seen use a reference image for the style transfer which on the one hand is great in that it s flexible but on the other hand it s not great because the reference image has too much of an influence on the output image for example in the toonification case it could change the color of the person s hair in the toon version to match the reference image even though the toon dataset may have had a more similar hair color within its distrubtion i know that a more straightforward approach to solve the style transfer problem between 2 distributions is something like cyclegan or the many architectures it influenced however from what i ve seen none of those seem to have as good results as the stylegan based ones thanks for the replies
relhfv,0,what s the difference between a posterior distribution predictive distribution and a posterior predictive distribution i m trying to understand the difference between these three terms and everytime i read an answer online i m back to square one
r01mcs,0,source code hi guys i m looking for the source code of this paper i reached out to one of the contributors and i will wait for a response however do other methods to obtain the source code of this paper exist thanks
ruwchh,0,spotted this post in lesswrong can anyone verify the rather fantastic claims being made here the writing has some red flags but it looks interesting enough having some trouble with my gpu drivers so i can t run it right now
rxhkxd,1,closing the data loop with dagshub annotations and label studio hey r learnmachinelearning nir from dagshub here i know working on data is less attractive than building a cool model the reality though is that many times improving your data leads to much better results maybe one of the reasons we donâ€™t do it as much is that working on data complicates our workflow significantly dagshub is the place to build data science projects itâ€™s like github but for data models experiments notebooks and pipelines not just code we recently launched dagshub annotations which lets you easily annotate data without needing to set anything up just push data open a labeling workspace with a few clicks and annotate we constructed the data labeling workflow to smoothly connect with your gitflow requiring zero adjustments you can check out the tutorial i wrote which is beginner friendly on how to get started with dagshub annotations spoiler alert you get to annotate elon musk ðŸ¤« check it out here would love to hear your thoughts
rrnctu,1,has anyone had azure associate data scientist certificate what is your experience and do you think it would boost a resume who is trying to break into to the field
rqwf4e,1,classification and object detection question i have built an object detector that is detecting birds at my bird feeder it is trained only for birds in general and without any regard for species in order to classify my data i am running a classification model on another computer with the data retrieved from the database of the detections x200b i am wondering if this is counterintuitive should i be trying to train my object detection model yolov4 running on a jetson nano to also perform the object detections with respect to species or am i on point with developing a classifier on a more powerful pc to handle the species classification
rcqvzh,1,how do games like while true learn help you learn machine learning i ve seen others but this game specifically was given for free at epic games basically at least for the first 20 or so puzzles you get a random input of shapes colors and you have to sort them to the outputs using decision trees if x up if y down else random expert systems if x up else down and others similars to fulfill the output requirements like red or blue any only red only red or blue triangles and green circles and so on i ve been playing and while i understand that the puzzles and logic behind it do have a relation with machine learning basics how could ml concepts be applied in solving those kind of puzzles how can you develop the logic path used in the game to apply that in ml i understand that you cannot build a ml system just by playing a game like this i know it involves a lot more between structure logic data treatment data collection and so on just wanted to ask about how those relate because playing that game and there are others its just that i personally tried this one but it can apply to others i usually just try sorting them out balancing outputs until i get satisfying solutions from my experience with programming and electronics i know this is not the way
ri8nfe,1,how to convert csv data into graph dataset anomaly detection hello there x200b i am working on an anomaly detection problem using a graph neural network however i am not sure which will be the best way to convert my csv data into graph data i have 115 different attributes for each timestamp you can find the example dataset here i want to test each time stamp for an anomaly which will be the best way to convert csv data into graph data i was thinking of considering each attribute as a node and connecting all of them with each other by the undirected edge since i don t know their connection to each other then i will have a single feature for each node and a lot of edges to be specific feature shape 115 1 and edge shape 2 6555 but i am not sure if this is a good way to model it and what will be the best method to work on this on node embedding or graph embedding ps i am using an autoencoder model to detect anomalies thanks in advance hope i am clear about my question vishal t
rcn2ha,0,pretrained models gpt 2 clip etc as api endpoints for building web apps hi all i recently built pretrained convect ml iâ€™ve been interested in the potential for building web apps on top of pretrained models that have been gaining popularity in the machine learning community one foundational piece for these apps would be access to predictions by these models with fast response times so iâ€™ve made models available via an api for a few ai tasks 1 text generation gtp 2 gpt neo 125m and pegasus for paraphrasing provide text and generate more text with a similar style and content use these models to build an ai writing assistant or even synthesize entire articles 2 computer vision clip measure the association between any sequence of images to a list of arbitrary texts use clip in an app to detect vehicles animals trees household appliances or other physical objects that you can describe with words 3 conversation blenderbot 400m distill build an ai powered chatbot that responds to user inputs tbh blenderbotâ€™s responses donâ€™t always make sense so iâ€™d be careful with this one 4 article summarization bart large cnn generate a summary of the salient points in an article use this model to build tools to help people consume information faster 5 text classification bart large multinli measure the association between any sequence of words to a list of arbitrary text labels a classification model can be used to detect topics e g send customer call center transcripts to this api to detect if customers are reaching out about specific topics of interest such as product defects or payment discrepancies 6 sentiment analysis distilbert base uncased finetuned sst 2 detect the sentiment of a piece of text this model for example could be used to measure overall customer approval levels for a product from social media posts many of the endpoint response times are sub second and could be used in applications to provide a near real time experience i have also included codepens in react for each of the models to make it easy to get started with building on top of them all the endpoints handle preflight requests from any origin so applications can be purely browser based if you want to do that iâ€™m not the first to make these models available for free over an api but am hoping to make the experience of getting started as easy as possible iâ€™d also love to chat with anyone who has been curious about using these models in their projects or if thereâ€™s a particular model that you wish was readily available as an api endpoint
qm6l31,0,paper explained efficientzero mastering atari games with limited data full video analysis reinforcement learning methods are notoriously data hungry notably muzero learns a latent world model just from scalar feedback of reward and policy predictions and therefore relies on scale to perform well however most rl algorithms fail when presented with very little data efficientzero makes several improvements over muzero that allows it to learn from astonishingly small amounts of data and outperform other methods by a large margin in the low sample setting this could be a staple algorithm for future rl research x200b outline 0 00 intro outline 2 30 muzero recap 10 50 efficientzero improvements 14 15 self supervised consistency loss 17 50 end to end prediction of the value prefix 20 40 model based off policy correction 25 45 experimental results conclusion x200b paper code note code not there yet as of release of this video
qrpx36,0,integrate your ml model with your favorite apps from a single python file hi all many here can build a simple machine learning model to predict whether a customer will churn if they get a nice pandas dataframe with customer data however it gets really complicated if you want this model deployed and integrated in production say a procedure as such 1 pull data from a new customer from shopify 2 predict for this customer whether they will churn 3 if we predict churn true 4 send a discount code to this customer with mailchimp suddenly we have to code data integrations etl pipelines deploy our original machine learning solution spin up an http server etc a huge pain indeed we are building a framework that takes care of exactly all the boring stuff described above we really believe this will bridge the gap between research and real world ml super excited to share this with all of you please let me know if you have comments or feedback
quclg6,0,how ml is helping us understand animal communication video hi r machinelearning visualizing sound is enabling us to analyze animal voices and get a better of sense of what information they contain this form of audio analysis is powerful because it empowers human speech recognition technology and is opening the door towards better understanding of how dolphins whales and prairie dogs communicate this is the second video in my look at spectrograms which have been fascinating to look at x200b related research dcase detection and classification of acoustic scenes and events the analysis and interpretation of animal vocalisations call for papers
rfhwpn,0,what happens to the output of the two lstm s in a bidirectional lstm i m implementing a blstm but i m confused as to what happens to the output of the 2 lstm s forward and backward suppose we have 3 words each with a 4 length embedding i e input is of the form 4x3 one group of people say that the output of the two lstm s is serially concatenated i e the 4x3 goes through each lstm independently suppose their output is 4 and a 4x3 matrix is output for each lstm the result is concatenated fully meaning the result is a 12 length vector another group says that the output of the two lstms goes through an activation function independently meaning the output of the two lstm s two 4x3 matrices go through an activation function each word at a time for the first word for example you have 4 4 inputs to the activation function another group say the output is just concatenated on a word by word basis meaning the final output is an 8x3 matrix 4 from forward and 4 from backward what i m currently using the papers use a mix of these with the original paper using something closer to the third option thanks
rvqyji,1,some of the classifiers are greyed out in weka i have some data that i am trying to classify using the weka environment but the classifiers i m interested in are greyed out i know this means that the dataset i am using isn t compatible with weka so i am trying to find out why this is the case when i try the dataset with just the first 200 instances the algorithms show up but at some point between 200 instances and 2k instances the algorithm stops showing up here is a link to a sample of the data in arff format can anyone help me to spot what s wrong
rq68jj,0,modern artificial intelligence 1980sâ€”2021 and beyond schmidhuberâ€™s talk schmidhuber uploaded a talk to his relatively new youtube channel from youtube description this keynote talk had its premiere on 3 dec 2020 at the aij conference in moscow where it was translated into russian it was also presented at nvidia s gtc 21 conference us 2021 the 2021 machine learning summit in beijing where it was translated into mandarin big data and ai toronto 2021 ific china 2021 ai boost lithuania 2021 and iconip 2021 jakarta 2021 according to some of the organizers it had millions of viewers outside of youtube abstract significant historic events appear to be occurring more frequently as time goes on interestingly it seems like subsequent intervals between these events are shrinking exponentially by a factor of four this process looks like it should converge around the year 2040 the last of these major events can be said to have occurred around 1990 when the cold war ended the www was born mobile phones became mainstream the first self driving cars appeared and modern ai with very deep neural networks came into being in this talk i ll focus on the latter with emphasis on metalearning since 1987 and what i call the miraculous year of deep learning which saw the birth ofâ€”among other thingsâ€” 1 very deep learning through unsupervised pre training 2 the vanishing gradient analysis that led to the lstms running on your smartphones and to the really deep highway nets resnets 3 neural fast weight programmers that are formally equivalent to whatâ€™s now called linear transformers 4 artificial curiosity for agents that invent their own problems familiar to many nowadays in the form of gans 5 the learning of sequential neural attention 6 the distilling of teacher nets into student nets and 7 reinforcement learning and planning with recurrent world models iâ€™ll discuss how in the 2000s much of this has begun to impact billions of human lives how the timeline predicts the next big event to be around 2030 what the final decade until convergence might hold and what will happen in the subsequent 40 billion years take all of this with a grain of salt though
rhum5j,0,research bayesian optimisation live series lecture in a small survey i conducted today i realized a lot of people are not very engaged with the bayesian optimization literature but would like to learn about it x200b would you attend a lecture series on this view poll
r6b27d,0,neurips best paper awards link so many awards this yearâ€¦ also noticed theyâ€™ve released the accepted papers from the datasets benchmarks track hope this track becomes a permanent part of neurips
qrygiv,0,phd postdoc positions at ut austin ml for complex systems chaotic time series cellular automata fluid dynamics hi iâ€™m looking for phd students interested in the intersection of machine learning and physics particularly chaos and fluid dynamics i m also informally looking for postdocs official ad coming soon about we are based in the physics department at ut austin and are affiliated with the oden institute for computational engineering sciences here is a link to the lab website projects are pretty flexible based on curiosity and mutual interest thereâ€™s room for more algorithm focused time series mining projects as well as pencil and paper dynamical systems and control theory problems as far as applications go weâ€™re particularly interested in projects that can eventually be used for biological data or fluid dynamics weâ€™re super open to applicants from uncommon academic or personal backgrounds here are some recent examples â€œchaos as an interpretable benchmark for forecasting and data driven modellingâ€ neurips 2021 â€œdeep reconstruction of strange attractors from time seriesâ€ neurips 2020 â€œcellular automata as convolutional neural networksâ€ phys rev e 2019 applying for grad students feel free to apply to any of these grad programs at ut austin the physics department due 12 1 the oden csem program due 12 15 other departments cs ee are probably possible too for postdocs please reach out to me informally our physics phd program does not require physics gre normal gre or a physics undergrad degree there are only four core courses and we have previously had students with undergrads in cs engineering bioinformatics etc our quals are research talks not written exams if any of this sounds interesting feel free to email dm me or chat with me at neurips or aps
r5rxbr,0,has anybody tried using openai gym to automate using a gui application can it do something simple like â€œcreate a new macro in excel â€ or is there any pretrained model that can translate natural language to mouse actions over an image a desktop
rr8key,0,i wrote a program with openai s codex that fixes errors x200b you can find the program on github the ai generates mostly wrong solutions but enough of the generated solutions are actually working for it to be useful it already helped me installing the right dependencies during a docker build when even many desperate google searches didn t help me what do you think
rx2vov,0,retrieval transformers for other domains i am just going through the research behind this and was wondering if there has been work like this for other domains say computer vision for example
r6op6g,1,inter rater reliability metrics understanding cohen s kappa i often see subtle misuses of interrater reliability metrics for example imagine you re running a search relevance task where search raters label query result pairs on a 5 point scale very relevant 2 slightly relevant 1 okay 0 slightly irrelevant 1 very irrelevant 2 marking very relevant vs slightly relevant isn t a big difference but very relevant vs very irrelevant is however most irr calculations don t take this kind of ordering into account so it gets ignored i wrote an introduction to cohen s kappa a rather simplistic and flawed metric but a good starting point to understanding irr hope it helps i welcome feedback
rlcc92,1,a little tip for aspiring data scientists and ml engineers you can take a coursera or udemy course work through some practical tutorials with jupyter notebook use fancy tools have a collection of helpful cheat sheets and books or have multiple certifications all is well and good but it doesnâ€™t mean you have a complete grasp of the subject test your skills by implementing a use case from scratch without using any ready made resources or instructions from data collection preparation understanding modeling training and optimization to a robust pipeline be able to explain and interpret what you have realized try to visualize it and describe it to someone who is not an expert do this several times and you will notice how what you have learned above will fuse into something unique and valuable excerpt from my book the ai thought book amazon link
r8007v,0,how do you improve vector search results so iâ€™ve been using use msmarco models from sbert and other models but apart from just the simple vector embeddings themselves how do people improve the search results bit of context been using an exact nearest neighbour solution but it seems like when i tried it across a few queries results seem suboptimal curious to know how others are improving it
r21dxk,0,project aim 3 1 open source images tracking and images explorer hey r machinelearning i am gev co author of aim sharing with you the aim v3 1 the new version contains the following three notable changes images tracking and explorer improved ui and backend performance better runs navigation images tracking is a major item crossed off the aim roadmap images explorer on aim ui a short aim ui images explorer video code web release notes bit ly 3clz6hy x200b would love your feedback on our work how can we make aim better
rw2uac,0,style transfer with noise vector hi everyone i m looking for a model which can perform style transfer but also takes an auxiliary noise vector similar to that for stylegan to generate many stylized images for a single input image is anyone aware of any model meeting these requirements my best idea so far is to first embed the image into the stylegan latent space with this paper and then add noise to that vector
qklvfp,0,why hasn t bert been scaled up trained on a massive dataset like gpt3 both architectures can be trained completely unsupervised so why has gpt been scaled up and not bert is it a software limitation
r3jz31,0,how to edit images with gans tutorial part 2 you don t need photoshop when you got gans in this tutorial you will learn the intuition behind discovering editing directions in the latent space of a stylegan 2 generator using these directions to manipulate generated images in a meaningful way how this pipeline is exported into an actual image editor telegram post blog post image source subscribe to casual gan papers and follow me on twitter for weekly ai paper summaries and gan tutorials
rl7jfz,0,oslo open source framework for large scale transformer optimization x200b oslo is a framework that provides various gpu based optimization features for large scale modeling as of 2021 the hugging face transformers is being considered de facto standard however it does not best fit the purposes of large scale modeling yet this is where oslo comes in oslo is designed to make it easier to train large models with the transformers for example you can fine tune gptj on the hugging face model hub without many extra efforts using oslo currently gpt2 gptneo and gptj are supported but we plan to support more soon for more information see
rq1cnm,0,diffusion models beat gans on image synthesis explained 5 minute paper summary by casual gan papers i have been dodging this one long enough it is finally time to make a paper summary for guided diffusion gans have dominated the conversation around image generation for the past couple of years now though a new king might have arrived diffusion models using several tactical upgrades the team at openai managed to create a guided diffusion model that outperforms state of the art gans on unstructured datasets such as imagenet at up to 512x512 resolution among these improvements is the ability to explicitly control the tradeoff between diversity and fidelity of generated samples with gradients from a pretrained classifier this ability to guide the diffusion process with an auxiliary model is also why diffusion models have skyrocketed in popularity in the generative art community particularly for clip guided diffusion does this sound too good to be true you are not wrong there are some caveats to this approach which is why it is vital to grasp the intuition for how it works full summary blog post guided diffusion sota generative art model for clip arxiv code subscribe to casual gan papers and follow me on twitter for weekly ai paper summaries
r29dv7,1,analysis of micro macro data i am trying to run some analyses on data with a somewhat peculiar structure and i am hoping that someone on this sub can help me identify relevant resources i have nested data where approximately 10 000 repeated measurements are nested within 400 individuals some individuals have one observation whereas others have hundreds of repeated measurements the weird part is that the outcome a binary outcome variable is at the level of the individual i e i have 10000 rows of data but i am only trying to predict values that varies across 400 unique individuals my initial research into this suggested that i can just make sure that my test train split prevents any one individual from contributing observations to both the train and test data i tried this with random forest using all 10000 rows and obtain near perfect prediction for my training set auc near 1 but not for my training set auc of 60 this remained true after iterating through varied hyperparameter values of note creating a test and train split while ignoring dependence between observations leads to great performance across both datasets but i don t trust it one bit next i tried to average my features such that my data now only contained one row per person it s heart breaking to see all of this data disappear i now get random forest that performs about the same on test and training sets auc of 60 i want to continue experimenting with other models or perhaps some feature engineering to see if i can improve prediction metrics however i am not sure which approach i should prioritize when experimenting evaluating whether the model generalizes independent cases feels absolutely right this is what i would want the model to be used for if it is ever rolled out in the real world getting similar performance estimates between my train and test data also make me think that anything i learn during tuning will generalize to my training set however i wonder if i can do more with my repeated data than compute means for each person feature also iccs are all over the place for my features which would suggest that for some features there isn t much individual level variability for me capture reliably i was able to find a paper that suggests a latent variable approach with this data structure is anyone on this sub aware of a similar ressource pertaining to applying machine learning to a comparable data structure surely there must be something out there but i am failing to find it even just reading a paper that seems to successfully deal with this issue would be helpful thank in advance oh and happy thanksgiving ðŸ¦ƒ
rghuvg,1,4 up to date techniques for image data augmentation i wrote a brief introduction on a few selected data augmentation techniques published in the top conferences of recent years i find them mysterious yet working particularly well feel free to have a look
r7xbw1,0,optimize gradient descent for small known problem hello i hope i m posting in the correct community because my problem isn t part of machine learning but i figured you d be the best help when it comes to gradient descent a part of a hobby project of mine is photometric stereo take images under different lighting conditions to calculate the shape of an object so i need to minimize a cost function depending on 64 measured values with variables to optimize being a direction vector and a scalar value being the albedo luminance of the object but hundreds of millions of times within a reasonable time i didn t find any fitting papers addressing what i need exactly typical photometric stereo with the added fresnel equation for s polarized light and robustness to shadows so i developed my own algorithm this works reasonably well but all optimizations i found have been more or less trying different stuff and seeing what works i have already added momentum to the direction vector part and can directly solve the luminance in one iteration hyperparameter optimization is also on the list so to my question how do i properly optimize a gradient descent algorithm that solves a small known problem and should converge with as little steps as possible or at least learn how to do it so i d also appreciate any links to research papers lecture slides
rwl4pe,1,can i learn ml after python basics i am in grade 11 junior and i have a pretty good understanding of some math concepts like functions and algebra and some basics trig but ive never done calc before can i get started with learning ml or do i have to learn other things such as web development flask and django before going onto more advanced things im new to programming and ive been exploring for quite some time about what to do after learning the basics of python please help me out thank you
r8tpdc,1,best way to label video for object detection hello i want to label some videos that contains some small fish in order to detect and track them i am looking for a way to turn these videos into a dataset so i can train yolo to detect them can you please suggest any free tool that i can use besides label studio somehow it doesn t work for my videos or maybe i just don t know how to use it well
r3j9ps,1,are there any simple hosted mlops or auto scaling solutions iâ€™m looking to deploy vqgan clip models at a pretty large scale 30 gpus and i want to explore options other than kubernetes on aws or gcp is there a hosted mlops platform where i can simply upload my ml service docker flask and the platform can completely take care of scaling and gpu provisioning by itself essentially outsource scaling up down and handling traffic to a product or service with me having to do minimal setup
qkf6mt,0,has anyone else received an e mail for the iclr review i got an e mail from openreview with a single review of the paper i went to the openreview website to see that it was deleted anyone else with a similar experience
rljh5w,1,is there a github repository that lists out topics and resources to learn them i saw this repository for data structures and algorithms so i was wondering if there s something similar for a beginner wanting to learn machine learning a quick google search returns lots of repositories so does anyone have experience with any of the repos i have also heard of microsoft ml for beginners which looks like a good resource and i will definitely start the lessons does anyone have any opinions on this it looks interesting
rhu6fw,0,as a researcher should i regularly review the basic math ml etc i m using anki which is spaced repetition app and there are some flashcards on ml linear algebra prob stats i should review one advantage the anki brings to me is that now i can choose what knowledge i remember since reviewing basic cards does not bring immediate advantage i wonder if it is better to discard those cards and re learn the knowledge when i actually need it or to invest times expecting it will save me a lot of time someday do you review your basic subjects regularly or simply re learn when you need it
r6789x,1,how to autusave or resume training after google colab shutting down brutly i am new to google colab and have bought a colab pro member i found it can run around 24 36 hours but brutly shut down each time you know after that i have to retrain all my gan sometimes i would download the pkl file manually and upload it to retrain but it looks stupid is there a way to autosave pkl files in google drive during each gan training snapcount thanks a lot
qv5nbi,0,interview with raquel urtasun of waabi and best of iccv papers on computer vision news dear all have a peek at computer vision news of november many articles about ai deep learning computer vision and more html5 version recommended pdf version dilbert on page 2 free subscription on page 74 enjoy
ql5hdb,0,why do we apply batch normalization between layers after batch normalization we are basically trying to get the unit gaussian output initialising the data with unit gaussian seems to be a good idea but doing so in between the network how does that make sense
qm6ieq,0,discussion applied machine learning implementation debate is oop approach towards data preprocessing in python an overkill tl dr i am trying to find ways to standardise the way we solve things in my data science team setting common workflows and conventions to illustrate the case i expose a probably over engineered oop solution for preprocessing data the oop proposal is neither relevant nor important and i will be happy to do things differently i actually apply a functional approach myself when working alone the main interest here is to trigger conversations towards proper project and software architecture patterns and best practices among the data science community context i am working as a data scientist in a big company and i am trying as hard as i can to set some best practices and protocols to standardise the way we do things within my team ergo changing the extensively spread and overused jupyter notebook practices and start building a proper workflow and reusable set of tools in particular the idea is to define a common way of doing things workflow protocol over 100s of projects implementations so anyone can jump in and understand whats going on as the way of doing so has been enforced by process definition as of today every data scientist in the team follows a procedural approach of its own taste making it sometimes cumbersome and non obvious to understand what is going on also often times it is not easily executable and hardly replicable i have seen among the community that this is a recurrent problem eg in my own opinion many data scientist are really in the crossroad between data engineering machine learning engineering analytics and software development knowing about all but not necessarily mastering any unless you have a cs background i don t we may understand very well ml concepts and algorithms know inside out scikit learn and pytorch but there is no doubt that we sometimes lack software development basics that really help when building something bigger i have been searching general applied machine learning best practices for a while now and even if there are tons of resources for general architectures and design patterns in many other areas i have not found a clear agreement for the case the closest thing you can find is cookiecutters that just define a general project structure not detailed implementation and intention example proposed solution for preprocessing for the sake of example i would like to share a potential structured solution for processing as i believe it may well be 75 of the job this case is for the general dask or pandas processing routine not other huge big data pipes that may require other sort of solutions if by any chance this ends up being something people are willing to debate and we can together find a common framework i would be more than happy to share more examples for different processes x200b keep in mind that the proposal below could be perfectly solved with a functional approach as well the idea here is to force a team to use the same blueprint over and over again and follow the same structure and protocol even if by so the solution may be a bit over engineered the blocks are meant to be replicated many times and set a common agreement to always proceed the same way forced by the abstract class imo the final abstraction seems to be clear and it makes easy to understand whats happening in which order things are being processed etc the transformation itself main pipe is also clear and shows the steps explicitly in a typical routine there are 3 well defined steps read parse data transform data export processed data basically an etl process this could be solved in a functional way you can even go the extra mile by following pipes chained methods as brilliantly explained here it is clear the pipes approach follows the same parseâ†’transformâ†’export structure this level of cohesion shows a common pattern that could be defined into an abstract class this class defines the bare minimum requirements of a pipe being of course always possible to extend the functionality of any instance if needed by defining the base class as such we explicitly force a cohesive way of defining dataprocesspipe pipe naming convention may be substituted by block to avoid later confusion with scikit learn pipelines this base class contains parse data export data main pipe and process methods in short it defines a formal interface that describes what any process block pipe implementation should do a specific implementation of the former will then follow from processing base import dataprocesspipebase class pipe1 dataprocesspipebase name clean raw files 1 def init self import path export path params self import path import path self export path export path self params params def parse data self pd dataframe df pd read csv self import path return df def export data self df pd dataframe none df to csv os path join self export path index false return none def main pipe self df pd dataframe pd dataframe return df dropnan reset index drop true pipe extract name self params extract pipe time to datetime self params dt groupby foo sum reset index drop true def process self none df self parse data df self main pipe df self export data df return none with this approach the ins and outs are clear this could be one or many in both cases and specify imports exports even middle exports in the main pipe method the interface allows to use indistinctly pandas dask or any other library of choice if needed further functionality beyond the abstractmethods defined can be implemented note how parameters can be just passed from a yaml or json file for complete processing pipelines it will be needed to implement as many dataprocesspipes required this is also convenient as they can easily be then executed as follows from processing pipes import pipe1 pipe2 pipe3 class dataprocesspipeexecutor def init self sorted pipes dict self pipes sorted pipes dict def execute self for pipe in pipes items pipe process if name main params json loads parameters json pipes dict pipe1 pipe1 input1 csv output1 csv params pipe1 pipe2 pipe2 output1 csv output2 csv params pipe2 pipe3 pipe3 input3 csv output2 csv clean1 csv params pipe3 executor dataprocesspipeexecutor pipes dict executor execute conclusion even if this approach works for me i would like this to be just an example that opens conversations towards proper project and software architecture patterns and best practices among the data science community i will be more than happy to flush this idea away if a better way can be proposed and its highly standardised and replicable if any the main questions here would be does all this makes any sense whatsoever for this particular example approach is there any place resource etc where i can have some guidance or where people are discussing this thanks a lot in advance ps this first post was published on stackoverflow but was erased cause as you can see it does not define a clear question based on facts at least until the end i would still love to see if anyone is interested and can share its views
rvn3vp,0,who knows the paper address of the code especially the spatio temporal curvature streaming
rntvid,0,how to make lower and upper bound constrained predictions using sci kit learn i m trying to run a regression model on sports player performance data such that the sum of the individual predictions falls between an upper bound and lower bound of a team s expected prediction distribution given the current covid situation many star players are missing games leading to many no name players getting unprecedented opportunity the problem is that the regression model underpredicts almost all of them say i know that the dallas cowboys have the best offensive line in football and they re going against the worst rushing defense in football their star running back ezekiel elliot is out for the game and hypothetically the two running backs filling in haven t played much and are averaging 5 yards a game and 10 yards a game respectively a regression model will predict somewhere in the ballpark of 5 and 10 for those players which implies the dallas cowboys will rush for 15 yards the entire game the whole world knows that s not going to happen and historical data says there s a 95 chance the dallas cowboys rush for between 75 and 125 rushing yards altogether is there any way to group predictions together in scikit learn and set lower bounds and upper bounds on the team s sum it s almost like a multilevel model type thing where the two have to agree with each other to make logical sense if we were predicting team rushing yards using team statistics it would predict somewhere between 75 and 125 yards so how can we get the individual player predictions to sum up to something that makes sense i m essentially trying to combine regression with constrained optimization thanks so much
rv50th,1,exploratory data analysis hello friends i have started studying the machine learning from few months back i have an idea on the process from collection of data to training the model i want to know about the exploratory data analysis eda part typically done before training the data could you suggest any books or tutorials for learning about eda and any resources for providing the steps to be taken for eda based on the particular type of ml models such as decision trees svm etc
qszlto,0,discussion about fine tuning language models for generating sql queries hey you all hope you are doing well i have a few things that i wanted to discuss about language modeling now i have a bunch of critical data stored in database format for my company i am working on something that can be used to train a language model to convert natural language to a sql query currently we have a rule based system which has tons of if else conditions and but it works the only issue i see is that if a new database gets added the currently existing rule based system will break so i am thinking of a way to automate this nl2sql using a machine translation model such as gpt 2 later followed by something bigger like gpt j i have looked at a few papers namely picard and the corresponding challenge called spider i wanted to discuss a few queries that i have and the genral thoughts that people have about this problem am i batshit cray to think finetuning large language models with a new data set would work well any alternative s that you would suggest to this how should i go about this just plain use the trainer api from hugging face other genral thoughts and opinions etc thank you for taking your time to read this i hope we can have a meaningful conversation about this
ragkis,1,grad probability bayesian book recommendations why do you need prob bayes i m a bioinformatician and my field deals with discrete probability quite a bit motifs patterns mutational frequencies normalizations we also deal with non normal distributions often my understanding is that dl and nn are often the wrong tool for the job and so a broader foundation is needed i can t just pytorch fast ai my way into success here what are you looking for i m enrolled in a ms in data science and i m startled by how little probability is emphasized especially bayesian probability any book recommendations for newcomers to bayesian stats or am i on the right track i d prefer graduate and professional level texts what have you considered i ve added statistical rethinking mcelreath crc machine learning a probabilistic perspective pml book murphy applied multivariate stat analysis johnson wichern pearson and of course bayesian data analysis gelman crc to my reading list based in part off of this post what level are you i ve completed sophomore junior level probability stats biostats linear algebra calculus and multivariate analysis coursework my uni delaware essentially rehashes undergrad probability in their graduate course with a little more calc and theory thrown in without real world applications or calculations emphasized it s total crap imo
rvmeo6,0,which tools can be helpful for annotation of videos for action recognition there is a team in my university who work on ergonomics they want to do action recognition on some videos they approached me for help i work on images i don t have idea about videos i have dataset i want to annotate key points in each frame please tell me which tools can be helpful for annotation of videos
qm5axp,0,adaconv explained adaptive convolutions for structure aware style transfer 5 minute summary by casual gan papers classical style transfer is based on adaptive instance normalization which is limited to transferring statistical attributes such as color distribution and textures while ignoring local geometric structures in the image but that is the stuff of the past let me introduce to you adaptive convolutions a drop in replacement for adain proposed by prashanth chandran and the team at disney research adaconv is able to transfer the structural styles along with colors and textures in real time full summary blog post adaconv arxiv code subscribe to casual gan papers and follow me on twitter for weekly ai paper summaries
qv5292,0,gan for animated gif video clip generation i am looking for some prior open source github code to generate animations video clips on google i am getting results for anime which i donâ€™t want is there work that trains a style gan on a bunch of video clips animated gifs and generates brand new ones
qs7g4t,0,causality research in ml is a scam warning controversial don t get me wrong causal inference are the most methods for application areas where we observe a bunch of random variable and want to figure out the causal relationship between them this rant is not about the method is itself but how ml research is recently getting exploiting the term causality for the sake of the hype and citations in ml we have two main paradigms supervised learning and rl work on causality e g bernhard schÃ¶lkopf judea pearl etc tells us that is impossible to determine the causal relationship between variables if we only observe them without performing any interaction therefore with supervised learning we cannot learn a causal model but we need to impose one period regarding rl tabular q learning is guaranteed to converge to the maximum expected reward policy period that s it nothing else needs to be said about it however despite these two fundamental statements there is currently growing a hype in general ml research about causality i am completely fine with causality research as long as it focuses on the application area mentioned in my first sentence but this recent trend brings the concept into computer vision nlp etc where things become vague quite fast exaggerated by the fact that research on causality can be already extremely vague and deeply philosophical e g what s the practical implication of newcomb s paradox in computer vision no causal model is known even the vision processing of humans or animals is very little understood moreover cv tasks are inherently under specified for instance is a cartoon drawing of an elephant still an elephant or is is out of distribution ood or its own class or multiple classes are we talking about the causal relationship of pixels patches or concepts what makes an elephant ear an elephant ear this vagueness combined with the general trend in ml of throwing a bunch of overly complex math statements into a paper to impress the reviewers is really concerning i bet that there will be hundreds of papers on this topic be published in the next years that contribute very little to our understanding but will create millions of self citations
qk5avf,0,how is mlops done in your current workplace i joined a startup recently where the the necessary backend to support ml deployment is pretty much non existent all we have are some simple templates for ci cd modified from those designed for generic microservices currently it takes data scientists at least 3 5 working days post r d for to put a model into production as a prediction end point with logging and observability this excludes setting up the necessary data pipelines between the models and other backend services whole process can take as long as 2 weeks my team and i are looking into setting up some framework and automation to cut the turn around time for putting models into production trying to establish some reasonable goals for this project and hope to get some insight from others have been through the same x200b which part of the production processes are automated by your mlops teams and tools how much effort do these tools help save and how much time does it currently take to put up a piece of r d work into production
r46h3o,1,a quick introduction to pruning and how to apply it to your model hey everyone i wrote a short article about pruning and thought it might be useful to this community i hope you will find it helpful and am looking forward to hearing your feedback
r5ox5x,0,best pretrained model for cleaning excel sheet is there any pretrained model perhaps bert or something on huggingface which you can pass a spreadsheet perhaps in csv format and it just guesses how you might want to clean it up itâ€™s ok if itâ€™s not perfect
r10kwg,1,questions regarding machine learning ai calculation power locally cloud hello everyone for my university i m doing an internship where i have to do research to formulate a solution to the current problems around machine learning artificial intelligence calculation power now on the internet i can find plenty of resources regarding aws azure to make this but one of the main things i have to pay attention too is the costs of setting a system up and finding out which one suits my university the best for this i was considering a local solution and a cloud solution as viable options but i have found trouble finding the correct resources that can point me into the right direction regarding local solutions most treads posts blogs i have found so far are 4 5 years old and while the info could be helpful i d much rather have the chance to see a more updated opinion or information so my main question would be to hear everyone s opinion about different solutions regarding machine learning calculation power that is cloud based or locally thanks in advance for the replies
rclejv,1,do you know any good gan tutorials hi i ve learned gan for mnist dataset and need more advanced gan tutorials with higher image resolution than mnist
rcke6u,1,i m a computer science student and i want to work in ml i m currently working on a double diploma computer science engineering management i really want to work in deep learning but i had few classes on it well one class one semester so like 10h ish of theory and 20h ish of a project though i did a 2month internship in deep learning i tried to recreate simclr based on the scientific paper contrastive learning and well i did better than expected but still not great results i now have to find a 6 month internship and i didn t do any ml since i m wondering what online courses youtube channels you would recommand to have better foundations in ml and have better chance to find an internship i don t necessarily want projects just theory can do thanks a lot
rwmvn7,0,blogs on fundamentals of score based and diffusion probabilistic models this two part blog describes the theoretical fundamentals of score based models diffusion probabilstic models and their relationship it is written to be a coherent documentation of the theoretical developements in this new class of generative model rigorous mathematical proofs are excluded in order to make it more readable sharing it in case anyone finds it useful part 1 score base models part 2 diffusion probabilistic models
qmzy8a,0,rudall e model is open source sberbank submitted an open source rudall e model inspired by openai s dallÂ·e for russian the model with 1 3 billion parameters is available under apache 2 0 license the pipeline includes image generation ranging results with ruclip and super resolution the large model 12 billion parameters will be available in the cloud rudall e is the biggest neural network project in the history of russia taking more than 20 000 gpu days of nvidia v100 to train it github model demo russian x200b here are some pictures generated with it cherry pick by authors avocado in the style of malevich cat looks at food anime chan trump hides the pain mystery forest salvador dali picture beautiful chan pepe frog grand canyon
qulcqf,0,bayesian models of perception and action book draft of an upcoming book by wei ji ma konrad kording and daniel goldreich to be published by mit press
r1v2n3,1,help with ai assignment hi guys i m new to ai and ml and i need help with an assignment could someone please walk me through what needs to be done on each question in the assignment because i don t understand a thing question in sports prediction large numbers of factors including the historical performance of the teams results of matches and data on players have to be accounted for to help different stakeholders understand the odds of winning or losing demonstrate the following fifa 20 dataset fifa 20 complete player data set is a collection of detailed attributes for every player registered in the latest edition of the fifa 20 database get the data on kaggle 1 demonstrate the data preparation feature extraction process 2 create feature subsets which show maximum correlation with the dependent variable 3 create and train a suitable regression machine learning model that predicts the overall rating score of a player based on his attributes 4 measure the performance of the model and fine tune it as a process of optimisation 5 use the data from another season which was not used during the training to test how good the model is 6 deploy the model on a simple web page using either heroku streamlite or flask and upload a link to the video that shows how the application on the website works
rlcpx5,1,train custom text handwriting models hi all i am testing the aws textract service and noticed that in the json file results there is the bounding boxes of the detected words i am wondering if this information can be used to build a dataset and retrain a custom model i am asking that because there is a lot of errors on the predictions si if i correct them and then train a new model once i have a good model i stop using textract what is your thoughts
rnnube,0,could we give a transformer long term memory by reserving part of it s attention window for world vector embeddings the inputs for a transformer are merely vectors like for every other ml architecture these vectors usually represent tokens but what if we allow a transformer to generate and store it s own world vector embeddings and to select a batch of them via attention for each new input it would create a functional loop for it to access it s own memory right
rdyyvl,1,what algorithms could potentially help improve a logistic regression model whilst maintaining the original loss function of the original model i am looking into bagging and i realize the bagging is particularly effective for unstable high variance machine learning models but not so much for more stable or have a high variance what algorithms would therefore be good for improving a logistic regression model while mainting the original loss function
rfk295,1,batch size in mlp and the difference from rnn hi all i am so confused by the definition a batch size is the number of training examples in one forward backward pass this sounds to me that the trained model is then using the information in other training examples to predict what is then the difference between rnn and a mlp with batch size 1
qoqpv2,0,measure the distance between two domains for transfer learning i know there are distances are defined to minimise for the domain adaption while i want to know does there exist any distance measurement that can measure the difficulty of performing the domain adaption from the source domain to different target domainsï¼Ÿ
rg8rw2,0,padl a unifying open source development framework with functional api for pytorch to better build deep learning models we are happy to announce our new opensource project that brings functional building of models in pytorch padl allows you to easily build pipelines using pytorch layers along with normal python functions classes together padl is a pipeline builder for pytorch may be used with all of the great pytorch functionality you re used to for writing layers allows users to build pre processing forward passes loss functions and post processing into the pipeline models may have arbitrary topologies and make use of arbitrary packages from the python ecosystem allows for converting standard functions to padl components using a single keyword transform github notebooks to try it out website pip pip install padl
rj4r42,1,i am still unfamiliar with unsupervised learning or clustering here are some questions do i still need to do one hot encoding for categorical features for clustering like i did in supervised classification i have mixed numerical and categorical features do i still need train test split like what i did in supervised learning di i still need to deal with imbalanced data i am using python btw
r2lfjf,1,what exactly is a machine learning algorithm hey guys i m just beginning to learn about machine learning and there is something i am confused about i am confused about the usage of the word algorithm in the context of machine learning because as with what i learned an algorithm means a step by step instructions to perform a task that takes some input and produces an output in machine learning for instance a linear regression algorithm model is just some linear function that is fitted to some dataset in a graph based from my understanding why is it called an algorithm can you guys eli5 me with that thanks update found an article that helped me further clear up the confusion you can check it out it might help you too d
rf2xv1,1,watered down resources for svm large margin classification i have taken linear algebra and calculus but feeling a bit lazy so am looking for something easier than andrew ng s coursera ml course to learn svm does anyone know of any resources for example this book really waters down linear and logistic regression
rhui36,0,made some pytorch modules for agent systems i am starting a little evolutionary algorithms project i know it s a bit frowned upon and noticed that if you are working with deep neural networks you need to instantiate them separately and iterate over them to do each network s forward pass which is very slow even in gpu for that reason i made this little package of pytorch modules the main class widelinear behaves as a family of linear layers each different but each running fully in parallel so you do a single forward pass through all of them at the same time even works in gpu this has some application outside of evolutionary algorithms but mostly still in agent based systems gradients work as expected i have a brief documentation in my github and it is available through pip
retq7g,1,live nsfw detection is it possible to be playing a video and to have this program running in the background and it will be processing your screen every time it changes and detect every time something nsfw shows up is it possible for it to be that quick if it s possible how would i approach this task
r1du84,1,forecasting using lstm in pytorch hi i want to use an lstm in a time forecasting problem actually i want to replace the convolutional layers in the rainbow algorithm rl network with an lstm to consider time series i have seen many examples on the internet but none of them helped me with my confusion my input is divided into an encoder part and decoder part which are both divided into continuous variables and categorical variables what is the best way to build an architecture that takes into consideration all the types of variables can anyone share any helpful resources also my confusion is with the loss function will i need to do anything like in the picture attached thanks a lot x200b
rbqnxf,1,learning or working with ai or simply want to connect with people in this field come join us we are a discord community with over 20 000 members ask questions find teammates share your projects find job offers and much more programming is way more fun when you learn work with someone help each other ask questions brainstorm etc there is just so much benefit to joining a community when you are in this field especially when you cannot find the question you are looking for on stack overflow ðŸ˜‰ this is the same thing with ai and it is why a little less than a year ago i created a discord server where anyone learning or working in the field could come and share their projects learn together work together and much more the community is now close to 10 000 members which is unbelievable so glad to see it growing and see everyone so active come join us if you are in the field of ai and introduce yourself when you re in
r7bns7,1,guys i m very keen to learn to create ai art how do i go about it as a beginner guys so as the question says i m very new to machine learning i want to create my own ai to generate the art i know it is a very long way to go and a vague question to ask but i m really keen on creating it i m a programmer with about 1 year of experience any kind of help will be appreciated
rnnx7b,1,sound event detection using machine learning video presentation want to learn how deep learning can be used to detect sounds here is a practical introduction to the topic which was given as a presentation at europython 2021 did you know that sound event detection can be used to track the progress of beer fermentation learn more about it in the following video sound event detection using machine learning youtube happy to take questions here
rr6l1h,1,how to conduct features scaling i m working on a classification problem and wanted to normalize the variables before conducting machine learning classification algorithms so that all the training and test variables are scaled within a range of 0 to 1 i wrote the following code sc x standardscaler x train2 pd dataframe sc x fit transform x train x train2 columns x train columns values x train2 index x train index values x train x train2 x test2 pd dataframe sc x transform x test x test2 columns x test columns values x test2 index x test index values x test x test2 however i got an error message as typeerror float argument must be a string or a number not pandas libs interval interval any advice on how this problem can be fixed
rm4fgz,0,a static analyzer for detecting tensor shape errors in deep neural network training code hi reddit my colleagues and i just shared a static analyzer pytea which detects shape errors in the pytorch project github arxiv pytea can successfully analyze the entire training evaluation path such as training imagenet classifiers in it also supports major ml libraries including torchvision numpy pil and so on i ve been implementing a basic integration with vscode so that you can interactively find any shape mismatch from your code editor please let us know if you need support for the library you use
ruekwq,1,any reading group for ml books or research paper does anyone know about any ml reading group could be for ml books or research papers something like a discussion group i would like to be a part of them
r1wirr,0,aistats 2022 reviews are out early i got caught off guard this morning when aistats reviews woke me up i got 5 reviews which is pretty amazing all very detailed and high quality thank you all reviewing and organizing
r5wc8j,0,stylegan3 overview tutorial and pre trained model a post covering stylegan3 it discusses architecture how it improves on stylegan2 and how to use it it also includes a pre trained stylegan 3 model
qzmp8l,0,how verizon uses ai for enhanced business decisions with anil kumar executive director â€“ head of ai industrialization at verizon thursday december 2 2021 at 11 30 am et hi r machinelearning i wanted to share this free webinar with you all below are the details from the event website featured speaker anil kumar executive director head of ai industrialization at verizon as companies are looking to leverage cognitive technology and deploying machine learning models organizations need to make sure they have the correct people processes and technology in place to succeed in this presentation anil will share what ai industrialization is and how verizon is moving from pockets of ai ml to ai ml being implemented across the organization he will also share some of the unique opportunities and challenges that adopting ai across an organization as large as verizon presents agenda 11 30 12 30pm featured presentation 12 30 13 00pm your q a and interaction link for free registration
rfw5vf,0,new open source project model validation toolkit my colleagues and i have open sourced a project for validating and monitoring machine learning models the model validation toolkit included in the model validation toolkit are modules for measuring concept drift specialized performance measures for biased data assessing credibility of performance measurements taken from small samples building interpretable neural networks adaptively setting thresholds to maximize performance while intelligently checking for what might be missed sensitivity analysis user guides documentation and notebook tutorials can be found on the project website please feel free to reach out to me here on gitter or via github issues for questions and comments
rbn0fh,0,ivis dimensionality reduction in very large datasets using siamese networks
qo3704,0,professors and research groups in neural program synthesis i want to collect a list of professors and research groups that work in neural program synthesis or program induction all you can find with a simple search is groups at microsoft google brain eth zurich and mit does any one know other groups that work in this topic especially outside usa ps i couldn t find any groups or professors in germany if you have any helpful tips how to do phd in this topic would be nice
rd2ngm,1,complete production example anyone know of a great complete ml project example i donâ€™t mean just a saved model but a data pipeline a working app etc
rjhlot,0,can you help me to find a book in the arxiv i am going insane edit found it thanks so much to u fni19wym who has definitely a better brain than me the book is the nicely titled algebra topology differential calculus and optimization theory for computer science and machine learning by jean gallier and jocelyn quaintance adding up to 1958 pages original post i lost my hard drive some months ago and with it a book whose table of contents i recall liking i was looking forward to read it but now i can t find it this is what i remember the book is written by a mathematician it was a hefty volume at least 800 900 pages and it was about the mathematics of machine learning or deep learning tbh i dont remember which one but it is possible it was not titled that way because i havent been able to find it using those terms and you know how mathematicians are elements of analysis and algebra in the study of universal approximators as noted in the title it was posted to the arxiv it had not been published as far as i recall it is the kind of book senior professors write summarizing lots of stuff the book was rigorous i think it starts all the way back to analysis or even set theory and spends like 500 pages before touching ground the author is a french mathematician i think although based in the us i know the link for the book has been posted here and also in hacker news obvious notes no the professor is not mr lecun no the book is not mml or the bengio et al one i have been racking my brain all day and nothing maybe one of you guys can provide relief i reserve the right to be wrong in some of the details you know the brain is a tricky bastard thanks
r8do0d,1,cuda cudnn docker container from nvidia license distribution question i am being told that the license does not allow a person to use the cuda cudnn devel docker image as a base to then build opencv with cudnn install a python program and distribute it to people to use the license reads as 1 license subject to the terms of this license nvidia hereby grants you a non exclusive non transferable license without the right to sublicense except as expressly provided in this license to 2 a install and use copies of the container and modify and create derivative works of samples or example source code delivered in the container if applicable to develop and test services and applications 3 b deploy the container on infrastructure you own or lease to offer a service to third parties without distributing the container or exposing the nvidia apis in the container directly to such service users and 4 c develop and extend the container to create a compatible as defined below derived container that includes the entire container plus other software with primary functionality to develop and compile applications and distribute such derived container to run applications subject to the distribution requirements indicated in this license as used in this section â€œcompatibleâ€ means that extensions to the container must not adversely affect the functionality of the other components in the container x200b is the pertinent part the non exclusive non transferable then what is the wording of extend and distribute from searching it seems if i build my own image using debian with cuda and cudnn and then distribute it thats ok but using the nvidia cuda cudnn container as a base isn t x200b thanks for helping to clear up my confusion
r5k4yh,0,deep learning in production book hello everyone i m proud to share with you the first edition of our new book on mlops and machine learning infrastructure deep learning in production is an effort to aggregate best practices on how to build train deploy and scale deep learning models the premise is that we start from a simple jupyter notebook and work our way towards building a fully function web application that can serve million of users the book is based on an old articles series we wrote on our blog so a big portion of the content is already available for free we just organized restructured some of the articles and we added some new material we use a variety of examples with libraries such as tensorflow flask uwsgi nginx docker kubernetes tensorflow extended google cloud vertex ai the full code and the articles can be found on github we will very much appreciate any feedback or suggestions so we can work upon it in a second edition thank you for your time
r3bs4u,1,how video of taking down for shooting game can extract i am making a system it makes a montage of valorant montage is videos of good playing in games in other words taking down to other players in a row at first i attempted to use cnn cutting video of playing the game myself as training data and cutting at each frame but i think there s a way of something better cutting data of about 100gb is too much hassle i am a beginner in machine learning maybe having way i don t know
qrhh89,0,what are the advances on encryption i m new to the topic i understand that it s easy for ai find a valid result from a predictable algorithm like a basic letter letter encryption with a small dataset i saw it as an example in a lecture but how about more complicated encryption algorithms 16bits 32bits 256bits tls how big would the dataset get to be able to easily predict a correct unencrypted text from encrypted text can ai be used to test if an algorithm is actually safe by solving it without really knowing how exactly the data was encrypted for example if an encryption standard that looks very safe to experts but has an unknown mathematical shortcut ai could find it is that possible where can i find more about this topic
r4vums,0,linking decision tree with nearest neighbors classifiers hello looking for opinions on this paper collapsing the decision tree the concurrent data predictor a variant of the nearest neighbors algorithm is derived by flattening the decision tree algorithm in particular i would be very interested in getting opinions on two aspects a morphing the decision tree into a nearest neighbors variant by evaluating more than one attribute at the time b the fact that the predictions of both decision tree and nearest neighbors converge to the optimum as the amount of training data increases please note that the focus is on data with categorical attributes is there anyone else exploring these subjects or something similar or have you in the past thanks
r8kxdc,1,is creating your own dataset for a popular ml problem is good choice i m want to build a project for mgr music genre recognition i was studying some papers on the famous gtzan dataset and it mentions there are some flaws in the data i myself have listened to some music samples from the dataset and found some repetition in the samples artists here are my inferences 1 some samples are from the same song 2 in some genres data is disproportionately sampled taking samples from single artist 3 only 100 songs are in a given genre total 10 genres 4 not all super genres are included i m planning to follow musicmap for major genre classification 5 one song is labelled under single genre only micheal jackson s songs can be labelled under both r b and hip hop there is another dataset which is used as alternative for gtzan fma free music archive haven t studied about that yet data size is certainly larger 100000 than gtzan 1000 fma also doesn t include all super genres as a given musical piece can come under many different genres i m thinking of training nn which will give percentage of super genres present in the song i think i can use the dataset from fma and add sample from the genres absent in the data is this a good approach
rc5zzf,0,ctrlgen workshop at neurips 2021 controllable generative modeling in language and vision excited by generation control and disentanglement come to our ctrlgen controllable generation workshop at neurips next monday december 13th we feature a mix of 7 talks on the latest in controllable generation a live qa panel discussion poster presentations of several interesting works creative demos of controllable generation systems and networking opportunities this is an effort organized with researchers from stanford cmu microsoft dataminr and the university of minnesota our invited speakers and panelists include researchers from facebook google deepmind university of washington new york university stanford and tel aviv university
rwj813,0,ideal deep learning library from researcher perspective what do you miss in libraries like pytorch or tensorflow what could be improved some possible examples the way how autodiff works debugging features working with axes einops something that just feel awkward inconvenient or incomplete i would very much appreciate it if you could share your thoughts on this
rqnjst,0,how do you guys tune hyperparameters when a single training run takes a long time days to weeks training a large model for example pretraining a large bert model can take weeks how do you guys do hyperparameter tuning in such scenario
rh7mf4,1,does the performance compute power of my local machine matter i m a 1st year grad student in ml and after my 1st term i realized the main stuff i m using on my laptop for ml are a lot of web browser tabs and a lot of visual studio code since my code are running on a remote gpu i m also considering getting a new laptop desktop so i m wondering as i step deeper into ml planning to get a phd will i be expected to use any heavier compute demanding stuff on my local machine or is it still just web broswer ide does the performance compute power of my local machine matter at all
r9k1u5,0,using the singular value decomposition for compact operators in my latest paper on dynamic mode decompositions this is something i have had trouble finding in standard textbooks on operator theory and functional analysis including conway s a course in functional analysis and pedersen s analysis now it s honestly surprising to me that it is really just a few lines past some of the theorems in those texts but i ve only really located discussions in a bunch of pdf s online on the other hand even though it is core to all of data science it also doesn t appear in textbooks there either and this is probably due to the idea that all real world data is finite dimensional again it almost follows the same argument save for some infinite dimensional considerations the singular value decomposition for compact operators is a tool i ve come to use in my work that overlaps operator theory and data science namely in the convergence theories concerning dynamic mode decompositions these decompositions rely strongly on a finite rank approximation of an infinite dimensional operator and my group and i have managed to show the convergence of some of our routines using the svd for compact operators one of the papers was just published this past week and you can find it linked below available freely through this link for a limited time by the publisher in the paper linked below we introduced a scaled liouville operator that was compact over a particular rhks what s interesting is that over a compact subspace of the workspace you can get a point wise approximation of the unbounded liouville operator by a compact operator so past a certain threshold the difference between them is indistinguishable on a particular data set so to help my students and burn it into my memory i put together this video discussing the svd for compact operators here is an older video on our perspective on dmd we leverage quite strongly liouville operators and occupation kernels you can find sheldon axler and steve brunton discuss the regular svd and i ve done that before on my channel too i think this is the first video on youtube extending that discussion to compact operators though i m happy to be shown otherwise let me know what you think of the presentation cheers paper link
ru0nzv,1,sentiment analysis on large datsets hello so for a project i have to perform sentiment analysis on really large datasets each one is 6 million strings and there are about 90 of them for this i have been using huggingface but it looks like the inference is super slow like i passed one of the datasets and it s been crunching away at it for 24 hours i am running it on the gpu and i optimized the batches too but it takes way too long i ve been considering pruning the dataset but there s only so much i can do like i can get the whole dataset down to 72mil ish but then that still doesn t solve the massive time issue is there a faster way of doing this apart from multiple computers solving subsets of the data thanks in advance
rlxggy,0,since gradient continues to decrease as training loss decreases why do we need to decay the learning rate too this has always bothered me i will write my question point by point to make myself more clear 1 the weight update for sgd is gradient times learning rate 2 initially when the training loss is high the gradients are naturally going to be very high usually we use high learning rate at this step 3 as the training progresses training loss falls and gradients also become smaller we usually lower the learning rate as the training progresses 4 so we are reducing the weight update through both learning rate and gradient 5 interestingly for adaptive optimization methods such as adam we normalize the gradient by it s second order moments which kind of counteracts the effect of gradients becoming smaller i m not too sure about this though 6 so my question is why do we need to decay the learning rate
rrwjyk,1,training u net model in pytorch hi guys i m trying to implement unet model in pytorch i have done it with tensorflow and it works fine but the pytorch implementation doesnt seem to work can you check my model and train fn model py train fn i m using nn crossentropyloss as the loss function there is no error in the code just my results are pretty bad some model with the same kernel size and same number of layers in keras works just fine
r5kjoq,0,oversquashing and bottlenecks in gnns and graph ricci curvature x200b over squashing is a common plight of gnns occurring when message passing fails to propagate information efficiently on the graph in a new blog post i discuss how this phenomenon can be understood and remedied through the concept of ricci curvature see the paper for details the second installment of this post will discuss whether and when diffusion improves graph learning analyzing the popular digl rewiring method of klicpera et al this post is part of a new series on graph neural networks through the lens of differential geometry and algebraic topology
ru91o8,0,paper explained author interview player of games all the games one algorithm video walkthrough special guest first author martin schmid games have been used throughout research as testbeds for ai algorithms such as reinforcement learning agents however different types of games usually require different solution approaches such as alphazero for go or chess and counterfactual regret minimization cfr for poker player of games bridges this gap between perfect and imperfect information games and delivers a single algorithm that uses tree search over public information states and is trained via self play the resulting algorithm can play go chess poker scotland yard and many more games as well as non game environments x200b outline 0 00 introduction 2 50 what games can player of games be trained on 4 00 tree search algorithms alphazero 8 00 what is different in imperfect information games 15 40 counterfactual value and policy networks 18 50 the player of games search procedure 28 30 how to train the network 34 40 experimental results 47 20 discussion outlook x200b paper
rlhwve,1,i know python but ml code seems meaningless to me i was trying to learn ml in a long time and i know python and math but every time i started a course on youtube or udemy about introduction to ml after tensors things get really confusing for me like loading data training model i can t understand what s going on i recently watching statquest and i really liked the ml videos but i wanna do my own projects and for that i need coding
rlli7q,0,axon deep learning in elixir repo hi everyone i just wanted to share a passion project of mine that i ve been hacking on and off for a little bit now before you ask op why would you bother creating a dl framework in another programming language just use tf pytorch jax insert framework here i didn t set out with the unrealistic goal of overtaking any existing tooling i really just enjoy learning more about the internals of modern dl frameworks algorithms etc and i really like elixir with that said i am also not here to convince anybody that elixir or axon are better than python and any other existing stack out there i m just here to share something that i think is cool disclaimers aside here s a little bit about axon built on top of the elixir nx project which supports automatic differentiation and jit compilation using xla heavily modeled after jax keras like model creation api with support for pretty much anything you can find in a modern framework as well as custom layers for implementing anything you don t already have optax inspired optimization api for creating and composing gradient based optimizers pytorch ignite inspired training validation testing api for creating and instrumenting training loops validation loops testing etc i m also actively developing a library on top of axon which converts to from onnx the library is still very wip and there are a lot of features i m still hacking away at such as multi device support but i ve found it to be pretty useable for every example i ve implemented so far feedback and questions are always welcome
rva1dk,0,why academia tends to under invest in engineering infrastructure tweet from jackclarksf asks an interesting question is there a good paper that explains how why academia tends to under invest in engineering infrastructure
r98ajp,0,is there a good pixel image upscaler i m trying to upscale this image from 256 to 512px 256px the issue is that the pixels lose their squareness and they become obtuse acute and they deform i used srcnn for this or whatever is using not real esrgan is there a good ml upscaler that works well for pixel images
rhsefd,0,anyone else suspicious concerned about the spread of data science degrees along with ms programs in artificial intelligence rather than an ms in cs stats with a focus on ai i could understand if say cmu wanted to have an out and out ms in ai which would probably be pretty good prep for a phd in the subject but for example yeshiva university has an ms in ai despite as far as i can tell having literally only two full members of the faculty working in the area so i m somewhat concerned about the rise of all these degree programs in ml ai ds specifically because they seem really specialized in a way that undergraduate professional degrees probably shouldn t be and aren t always offered by departments that i trust to deliver appropriate instruction to me it would be way more appropriate for students to do a degree in math statistics cs and then pick coursework and do research in a narrower specialty rather than potentially be left holding the bag once the hype dies down
rcy2tq,1,question about model stacking i m working on a small personal project and would like to implement model stacking however the approach by which i would like to do is proving to be unintuitive not sure if that s because it s not possible and or recommended or i just haven t stumbled across the appropriate method yet what my plan is is to split the records into one dataframe here would be good point to mention working in python that contains the numerical features and a second dataframe that contains the categorical features and maybe even keep a copy of the original dataframe with both and of course to split out the target variable all sharing the same index then run through a selection of models and feed the predictions to a meta learner i ve read a few articles about model stacking implementation via sklearn and the process seems to involve initiating a stackingregressor or stackingclassifier depending on task and passing the estimators and final estimator then fitting predicting etc in typical fashion my problem lies in the disparate dataframes it seems that you re only able to run a single x and y through the process you cannot tell the stacker to run models a and b on the numerical dataset and run models c and d on the categorical dataset correct but perhaps there may be a manual workaround
rsumxg,1,what is l1 distance a 2 minute visual guide x200b ðŸ”µ manhattan distance ðŸ”µ ðŸ—½you might have heard of euclidean or l2 distance but have you heard of the l1 distance also known as the manhattan distance ðŸš•the manhattan distance is computed by treating the geometry to be as if it were the street of manhattan one square city block followed by another so the only way to travel is to go along the right angled streets while the shortest euclidean distance between two points has a unique path the same is not the case for manhattan distance as you can have multiple paths with the same distance ðŸŒƒ mathematically the l1 distance is the sum of the absolute value of the difference of each coordinate of your point vector and can be extended to n dimensions ðŸ¤“ l1 distance or the l1 norm is also used to regularize model parameters regularization penalizes model parameters from over fitting the l1 norm forces model parameters to be sparse which would shrink the non important features towards zero the coefficients of the model could then be used to understand which features are more important i e the features that correspond to model parameters with larger values for example if you have a linear regression model with l1 regularization that predicts the price of a house with features number of rooms area color of house after fitting the model on your data you see the coefficients corresponding to number of rooms area color of house 0 5 0 6 0 01 you can see that the model treats number of rooms and area to be more important features than color of house as 0 5 0 01 and 0 6 0 01 in determining its price if you like such content and would like to steer the topics i cover feel free to suggest topics you would like to know more about in the comments
r13jq5,1,possibilities object recognition as a complete newbie in the ai and machine learning space i was wondering how far the possibilities currently reach i came up with an idea to recognize whether different types of glasses are filled or empty the idea is extremely novel still and therefore i m wondering if that s already possible with ai and machine learning at the same time am i wondering if something like this can be easily programmed or learned how to program by a beginner or that it requires a certain experience and can only be done by someone with experience in the field
rakmq4,1,learning how to learn so im trying to get into machine learning and so im trying to learn how to learn ive been programming for 30 years now so i have a fairly firm grasp of those portions of things but i dont even know how to approach learning how to do things for example i would like to do something like the following identify bikes identify bike parts identify bike statistics ie wheel size height etc identify specific bikes so how do i approach something like that i presume thats 3 different tasks maybe more so like how many pieces of data do i need to collect is 1000 marked up images of bikes enough to at least start i mean thats what im going to start with
raobgu,0,research what deep learning algorithms is best for forecasting covid 19 cases using exogenous variables for our research we want to forecast the covid 19 cases using exogenous data for our exogenous data we are planning to use time series data of the cumulative number of vaccinated individuals change in mobility and weather is it possible to incorporate these variables for univariate forecasting we are also looking to incorporate the data of population density of each location but this is not time series data can we still include this we are currently considering lstm but i m not really sure if it is the best algorithm for this kind of algorithm any suggestions would be greatly helpful
racb4q,0,optimal policies tend to seek power neurips spotlight summary there are more ways for some actions like staying alive to be optimal roughly because staying alive lets the agent do more things actions which help agents stay alive and keep their options open will provably be optimal for most reward functions the upshot is that it might be very very hard to design intelligent real world ai systems which let us deactivate and correct them if statistically most goals donâ€™t incentivize that behavior then our goals would conflict with the goals of most smart ai agents excerpts from the paper abstract some researchers speculate that intelligent reinforcement learning rl agents would be incentivized to seek resources and power in pursuit of their objectives other researchers point out that rl agents need not have human like power seeking instincts to clarify this discussion we develop the first formal theory of the statistical tendencies of optimal policies in the context of markov decision processes we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment these symmetries exist in many environments in which the agent can be shut down or destroyed we prove that in these environments most reward functions make it optimal to seek power by keeping a range of options available and when maximizing average reward by navigating towards larger sets of potential terminal states introduction omohundro 2008 bostrom 2014 russell 2019 hypothesize that highly intelligent agents tend to seek power in pursuit of their goals such power seeking agents might gain power over humans marvin minsky imagined that an agent tasked with proving the riemann hypothesis might rationally turn the planetâ€”along with everyone on itâ€”into computational resources russell and norvig 2009 however another possibility is that such concerns simply arise from the anthropomorphization of ai systems lecun and zador 2019 various 2019 pinker and russell 2020 mitchell 2021 we clarify this discussion by grounding the claim that highly intelligent agents will tend to seek power in section 4 we identify optimal policies as a reasonable formalization of highly intelligent agents optimal policies tend to take an action when the action is optimal for most reward functions we expect future work to translate our theory from optimal policies to learned real world policies section 5 defines power as the ability to achieve a wide range of goals for example money is power and money is instrumentally useful for many goals conversely it s harder to pursue most goals when physically restrained and so a physically restrained person has little power an action seeks power if it leads to states where the agent has higher power we make no claims about when large scale ai power seeking behavior could become plausible instead we consider the theoretical consequences of optimal action in mdps section 6 shows that power seeking tendencies arise not from anthropomorphism but from certain graphical symmetries present in many mdps these symmetries automatically occur in many environments where the agent can be shut down or destroyed yielding broad applicability of our main result theorem 6 13 conclusion many real world environments have symmetries which produce power seeking incentives in particular optimal policies tend to seek power when the agent can be shut down or destroyed seeking control over the environment will often involve resisting shutdown and perhaps monopolizing resources we caution that many real world tasks are partially observable and that learned policies are rarely optimal our results do not mathematically prove that hypothetical superintelligent ai agents will seek power however we hope that this work will foster thoughtful serious and rigorous discussion of this possibility links paper neurips recorded spotlight presentation neurips poster session tue 7 dec 8 30 a m pst spot d3 in gather town series of blog posts on this line of work twitter thread
r86c1r,1,resources to explain how to build a neural network from scratch need some resources to assist in building a neural network from scratch
qrn5qb,0,replacement for softmax when you want to activate multiple samples equally softmax has been used for activating normalizing a representation in way that the most important sample will have the largest value between 0 1 and the rest will be close to 0 i am wondering what if we want to activate multiple samples equally in the representation lets say i have a vector of 1x10 dimension and i know 5 of them are equally important and i want them to have same weight after activation so softmax wont work here one option is to use sigmoid and then use softmax on the top of it to make sure the sum is one but i am not sure if this is the smartest approach so i was wondering if people here have any other suggestion
rgwroe,1,trying to compute knn on a np array of weights l2 distances and a np array of labels with k x what would be the best way to approach this problem i am having issues with the actual writing part as i understand the theory of computing knn with the vector of the weights and the array of the labels and how the vector relates to the distance of the array of labels alternatively what resources could i review to better understand this problem this is for a class i am taking and i want to be sure i fully understand the concept of the problem before proceeding thank you for your time i would be happy to give any more information for clarification
r8vicl,1,pca explained from basic principles in this blog post i have arrived at the optimisation problem of pca from using some basic principles of mathematics click the link to read
r6hf44,0,papers about mathematics of deep learning simple classical ml algorithm can find good list of papers about mathematics of deep learning
rwnzi9,1,intutive source for probability i studied probability in the college but it was really bad course i searched for some sources to learn from it and i found tens of sources but it wasn t really intuitive just apply the equation any suggestions for courses that explain what is going on behind the scene
rndczs,1,hi there i am hoping to predict next year s fifa ratings based on ratings positions age clubs from the past 5 years missing data here is important as some players will not have played in the last 5 years ie 17 year olds so which ml technique could be applied to this case
r70stj,0,how a feature store be useful for my scenario i recently came across feature stores as a mechanism to operationalize ml pipelines in a commercial setting it sounds good but i m finding it hard to understand when and where to use it when doing exploratory data analysis for example consider a simple use case of prediction house prices for a given geographical location i source the raw data from a remote server that contains historical house prices with some columns features as below latitude longitude total rooms house size total bedrooms year of construction this is just a small representation of the feature set as a data engineer one has to probably look into the raw data do some simple statistical analysis like 1 identify null or nan values and impute them 2 identify the co relation of the features with respect to the target variable and determine if some features be dropped or not 3 identify the unique count for a numeric variable and determine to remove that feature or column if the unique count is below a certain threshold 4 delete duplicate rows 5 perform onehotencoding for categorical data 6 identify and remove outliers 7 perform dimensional reduction feature scaling now assuming that i would be performing just the first few steps or i would be performing all the above steps i would like to know how employing a feature store would speed up or rather operationalize my ml pipelines
rfyujd,0,is meta learning use the same test samples as transfer learning use in few shot learning 1 as far as i know meta learning and transfer learning are two common ways to adress image classification task in few shot senario but i found that we offen use the images in train set of the specified dataset for test procedure other than test set in meta learning while in traditional transfer learning we use test dataset to deal with test task i wonder if i was getting the wrong understandding and is the accurracy reported from meta learning method is comparable with the report from transfer learning 2 i found a word called meta dataset such as omniglot fungi etc and there is no testset in such dataset so can i use transfer learning strategy deal with the mata dataset if so how can i split the dataset into trainset and testset and weather the accuracy reported from transfer learning is comparable with metalearning strategy thanks
rs8a2z,1,how to create retraining protocol pipelines for machine learning models specifically in the case of computer vision models using cnns inevitably you will encounter data that the model is not well suited for one could keep a log store of such data points to use for later training but what is a justified way to retrain models over time especially in the case of something like face detection where the data distribution does not necessarily change over time but rather there are simply certain points that are underrepresented in the original training data let s assume that the model already performs acceptably on most data points what are the trade offs of 1 retraining on the whole dataset including those erroneous samples with good labels 2 training the model perhaps with early stopping and validation on the original new datasets only on the erroneous samples 3 training on the whole dataset preferentially sampling the erroneous samples x200b is there a name for this process where can i find best practices and theoretical discussions of this kind of process i would appreciate any help as this is a real issue that i wish to solve with a deployed model
qsrdyk,0,text to image rudall e kandinsky xxl 12 billion parameter model checkpoint is apparently available for download here is a download page for rudall e kandinsky xxl 12 billion parameter model checkpoint english translation i did not sign up to try to download the file s the 12b parameter model should be available per my interpretation of this press release edit according to this comment public availability should be on december 1 prior mention of rudall e in this subreddit text to image models rudall e kandinsky xxl 12 billion parameters and rudall e malevich xl 1 3 billion parameters a demo for the latter is available rudall e model is open source p
ruz0nc,0,what causes feature collapse for those of you unfamiliar feature collapse is when you train a model for classification and the model ends up mapping out of distribution data or data of different classes in very close proximity in multi dimensional space so for example once your model learns a cluster so to speak for cat during test it projects a dog into the center of that cluster and classifies it as cat some ways to sort of deal with this in cv is double gradient penalty and spectral norm of resnet blocks but what causes feature collapse
rhppgq,0,research new library for bayesian optimisation and hyper parameter tuning research i am glad that we open sourced a new library for bayesian optimisation both in low and high d domains the library includes hebo the algorithm that won the neurips bbo challenge can be used for hyper parameter tuning t lbo an algorithm that combines deep metric learning with latent space bayesian optimization to enable high dimensional opt can arrive at optimal molecules reducing 97 data demands compbo an algorithm based on our jmlr paper to efficiently optimize acquisition functions we will continue to grow this library and your contributions and comments are more than welcome please have a look share and star the repo if you find it useful view poll
r1owph,1,is a powerful laptop really needed when one is doing ms in data science or will macbook air be enough
rmdrrj,1,resources for deep recurrent q networks i m currently learning about partially observable markov decision processes for deep recurrent q networks drqn i ve looked at this paper but it s not massively useful are there any resources available to learn more about drqns
radok8,1,conditional gan for image generation w specified attributes hello am very new to this world so please accept my apologies for misuse of terminology right now i m just looking for pointers in the general direction of where i m going a project to look at a research paper with sample code etc i have a large dataset of labeled images of widgets which is to say each image has a number of known attributes size weight color model that i can specify in a metadata collection i would like to train a model on this dataset and then generate new images with different specific attributes which i can control tune along with some amount of randomness to make the new images novel to use an example w faces i would like to be able to specify the attributes of the generated face brown eyes blue hair male etc but have it still be new so far my reading research has led me to things like stylegan2 3 which focus on totally novel images w random seeds or things like dall e which focus on natural language processing or generation from unlabeled training data i don t need something that complex yet any ideas or pointers would be super helpful thank you
qrte64,0,why do we have to discretize the data before we use mask prediction for representation learning in fields as vision or speech a lot of recent papers learn representations by masking parts of the input and predicting the original to do this they often refer to masked language modeling mlm in bert style pre training and say that we must first discretize the continuous input into discrete tokens e g using a vq vae before making predictions i e classification task over possible tokens in the dictionary one of the argument is that the predicting discrete tokens allows the model to learn high level concepts whereas making prediction in the original input space e g raw pixels will force the model to learn high frequency low level details that are not useful for representation learning compression and is also computationally prohibitive since the original input space is likely very high dimensional however my question is why can t we do regression in a continuous latent space for the masked positions e g predicting the latent representation of a learned continuous vae for the masked positions instead of classification in a discrete latent space e g predicting the discrete tokens of a learned vq vae for the masked positions is there any theoretical advantage to using discrete tokens instead of continuous latents
re1f4s,1,ppml series 1 an introduction to federated learning i started a series on privacy preserving machine learning i wanted to do it for quite a long time and finally decided to start the first post is a short introduction to federated learning in this blog post i have written a more detailed version of my twitter thread check it out ppml series 1 an introduction to federated learning
rvyuhb,0,what are interviews usually like for ml positions for context i m applying for phd level positions should i expect technical interviews including coding challenges similar to swe any advice on prepping
rb6jow,0,news aaai 2022 moves to a fully online format x200b i m not salty you re salty
r7b0mv,0,snowball fight a multi agent competitive environment for ml agents hello i m thomas simonini from hugging face we just published snowball fight a deep reinforcement learning environment made with unity ml agents you can play the game and try to beat our agent here or if you prefer to train it from scratch you can download the environment here this is our first custom environment with unity ml agents that is publicly available and i m working on building an ecosystem on hugging face for deep reinforcement learning researchers and enthusiasts that uses ml agents i would love to hear you feedback about the demo and the project
r6jee3,1,how to load 85 6 gb of xml data into a dataframe hello everyone i am trying to load a large xml dataset 85 6gb into a dataframe in jupyter notebook python i have tried 1 pd read xml system crashes high memory usage 2 elementtree system crashes high memory usage 3 vaex opensource library does not handles xml data well throws an error anyone has encountered a similar challenge kindly share how did you go about it thanks in advance regards mustafa ml student python machine learning datascience
qn2jg5,0,what is the most effective way to mix scalar value s into cnn feature maps it feels like an easy task but i can t seem to recall or find much info on this what i m trying to do is use a scalar value and try to mix it into the intermediate feature maps of a cnn i know typically you might just concatenate these kind of scalars after flattening the feature map and before a fc layer but i want this value to be combined into the intermediate feature maps between convolutions and not at the end of the whole cnn encoder if it were categorical information i know i could use learnable embeddings and add concat but in my case this is a continuous scalar i ve seen suggestions to treat this scalar like a bias term and simply add but this doesn t look strong and i m not quite convinced i ve also thought about copying this value to a h x w x 1 array to concat before the next convolution but i m not sure about this either what is the most effective method to do this
rblylk,1,can i optimizie a keras model with two losses so one of the losses is below the other losses threshold i have a keras model for predicting a time series which i want to optimize in respect to two losses one loss i just want to get close to a certain threshold value and the other one i want to minimize the first loss is mae and the second loss is the mean error times a time series i ve read this stackoverflow question and it s quite close to my question except i m not entirely sure where to use the time series in the input parameter for the loss function help is much appreciated
r20dmh,1,predicting the number of citations on essays hello for a machine learning project i and some other students are working in a group to build a python based citations prediction algorithm we have a dataset of 10 000 essays in a json file each with a unique doi number citation count publication venue title topics authors and a small abstract of the paper we have to use this to make a model which predicts the number of citations for each essay we have tried a lot of models and did some feature engineering but can t seem to get a much higher r2 score than 0 42 we ve dived a lot into literature regarding the topic but none seem to specify which exact model works best just what features work best we ve made log transformations on original and created features like references years old oa yes venue count topic count abstract avg nr chars author count do any of you have experience with this and or if so can you give us any hints tips regarding model usage or feature engineering thank you in advance for any input it is very much appreciated
rmtcvh,0,shape changing and conformal mapping i am looking for shape changing algorithm meanly from linear to nonlinear shapes i think conformal mapping is related here does anyone has an experience with shape changing how it works
r04dcs,0,how to apply is or fid score on gan that generate time series instead of images data hello i m working on using gan generating time series data it is very common to use inception score or frechet inception distance score to evaluate the performance of gan however both of them rely on inception model which is designed for 2d image data and could not be applied to 1d time series data a straightforward way to solve this problem is to train a time series classifier in substitute of the original inception model like this paper tsgan did but lacking of enough data and computation resource it seems not be be a good choice for me another way is using stft to transform the 1d signals to 2d images and apply fid score method on those 2d images it might be a feasible solution but i don t know if it really make sense a stft image has absolutely no similarity with image from imagenet on which the inception v3 is pre trained so it s questionable whether the result produced by the inceptionv3 makes any sense ï¼›â€²âŒ’
qsc6y3,0,can data scientists still make a better predictive model using their talents by the evidence will script kiddies increasingly take over because of great software and ai that does more and more of it very well having studied and applied data science and machine learning since 2014 i am startled by the high quality free tools that are coming online in the past week month and year or two that i have tried they are that good i am talking about automatic model selection automatic tuning and nlp that is just breaking away from the old limitations and old ways of doing projects see on youtube lately i am watching all these data scientists posting videos where all they do is run the software that microsoft and google made and call it a day they don t even change the default settings and they add nothing of their own talents if they have them and the results are pretty good by golly that s the thing automatic feature engineering is one of the hallmarks of the top deep neural networks people have spent careers in linguistics and computer vision hand making parts and hand curating mathematical techniques that are now often completely bypassed by today s ml techniques baked into free software and that s just one example of many it s not a trivial question there s more to data science than making a predictive model on a canned fixed public dataset like iris titanic jewellery or even higgs boson there s also mlops exploratory data analysis study design visualization data wrangling data quality assurance data life cycle and many other areas data science has wide scope and many specialties predictive model making is only about 5 percent of the whole ml and ds job now according to a presentation i saw yesterday but modeling might be about to be fully automated soon can you as a ml engineer or data scientist bring your wide and deep talents to bear to actually show that you can make a model better than a script kiddie on predictive performance on any any at all well known open dataset is making predictive models a fully automated task now
rudcli,1,how to remove missing values in sklearn pipeline hi is there a straightforward way to implement a list wise deletion of missing values as part of a sklearn pipeline i want to delete all rows where one of the values is missing but it seems that there is no method for that in sklearn like there is for imputing is the only way to make my own class
radyhw,1,distributed training parameter servers vs mapreduce i was watching andrew ng s video on data parallelism where he talks about using mapreduce to update the weights after distributing the training to several gpus is this the same thing as tensorflow s parameter server approach the way i understand parameter servers is that we have several gpus sending their gradients to a server which aggregates them and sends them back for global weight updates this seems to be the essence of ng s video but he does not mention a separate parameter server but the gpus somehow combine the results see e g timestamp 12 18
qsv83w,0,are there any theoretical analyses on the success of alphago zero i am wondering whether there is a theoretical guarantee for the convergence of the network used in alphago zero or for the optimality of the searching algorithm
raaipx,0,integrating self attention and convolution tsinghua huawei baaiâ€™s acmix achieves sota performance on cv tasks with minimum cost in the new paper on the integration of self attention and convolution a research team from tsinghua university huawei technologies ltd and the beijing academy of artificial intelligence proposes acmix a mixed model that leverages the benefits of both self attention and convolution for computer vision representation tasks while achieving minimum computational overhead compared to its pure convolution or self attention counterparts here is a quick read integrating self attention and convolution tsinghua huawei baaiâ€™s acmix achieves sota performance on cv tasks with minimum cost the code and pretrained models will be released on the projectâ€™s github the paper on the integration of self attention and convolution is on arxiv
ripn6f,0,the human in the loop to drive ml model improvements while more data and compute is one way to improve ml models direct user feedback may be the best way to rapidly improve an ml model why is human in the loop necessary for mt the truth is that there is no existing training data set that is so perfect complete and comprehensive as to produce an algorithm that consistently produces perfect translations link an inconvenient truth about ai just about every successful deployment of ai has either one of two expedients it has a person somewhere in the loop or the cost of failure should the system blunder is very low deeplearning linguistics data neuralnetworks
rwtpxv,0,preparing for a comprehensive exam in ml i am a phd student based in canada and have a comprehensive exam coming up in 4 6 months this is an exam i have been nervous about since i began my phd i am fairly confident about the actual proposal and answering questions related to my field what concerns me more is fundamental background question as ml and statistics is so broad plus i am a little on the older side and my memory is a little poor have any students here taken a comprehensive exam if so what was your experience and how did you prepare is reading making notes from a textbook a good idea or is preparing a list of topics and reading extensively about them a better option
r0gnej,0,one sentence highlight for every neurips 2021 paper plus code for 200 of them here is the list of all 2 300 neurips 2021 neural information processing systems papers and a one sentence highlight for each of them neurips 2021 will be held online from dec 06 highlights code
rcovbp,1,what to do now in machine learning 1 i know how to code pretty well 2 don t know any of the math needed for data science can anyone please recommend some free resources books courses etc to learn the math and the order in which i need to study the math topics i am currently in 10th grade so according to you what i need to study first to further move on to the math needed for data science
rh942h,1,random variable vs instances in ml stats lets say i am talking about the dataset used as input to a stats ml algorithm for example in this paper they have the passage x200b should this dataset be considered a sequence of iid random vectors or a instances of those vectors i assumed it was the first but then all of the people in this stats stackexchange discussion claims its the 2nd
quutqc,0,online free hong kong machine learning meetup 17 october 7pm gmt 8 join us for hkml s4e4 a monthly free online machine learning meetup be aware technical content talks 1 application of natural language processing to unstructured financial text to create alternative data for finance 2 ai automation and how it s a game changer on the way we apply ai to business 3 tbd
qse6gc,0,deepmindâ€™s one pass imagenet a new benchmark for resource efficiency in deep learning a deepmind research team presents the one pass imagenet opin problem designed to study the space and compute efficiency of deep learning in a streaming setting with constrained data storage and to develop model training systems where each example is passed to the system only once here is a quick read deepmindâ€™s one pass imagenet a new benchmark for resource efficiency in deep learning the paper one pass imagenet is on arxiv
r5o8q0,1,what is this parameter tuning method called does it work hello i work for the meteorological service of canada suppose i want to tune some parameters for a prediction system however this is for a heavy prediction system like weather forecasting and the supercomputer takes an hour to finish one run then we get a score result for how good the prediction was there s ten parameters to tune which parameters should we try next how about training a nn even just on a few examples to model which parameters will output which scores we then use the nn to try thousands of combinations of parameters the next set of parameters we run for an hour for real are the best parameters according to the nn guess after the hour run we retrain the nn on the new example repeat is there a name for this does it work well example articles or discussions is there some better way thanks
rtndgm,0,best practices in machine learning this is a non profit that promotes best practices in machine learning specifically for responsible ml the practices are open source too which is cool link here i think their technical best practices seems a little stronger than the organisational ones thoughts this is their linkedin url
r9u2e2,0,secret santa stylegan hi guys i created a quick notebook this weekend that uses stylegan to create face morph comic strips x200b as a gag gift you can then print the comic on a custom mug this only costs Â£6 so is well within most secret santa budgets x200b let me know if you give the notebook a try and if you have any artistic suggestions to make the comics better a similar description can be found on github i am not affiliated with the mug company so this is not for profit lol
rhv6u9,1,synthetic time series data generation i want to generate time series tabular data most of generative deep learning models consists of vae and or gan which are for most part relating to images videos etc can you please point me to relevant tutorial souces if it includes code along with theory all the more better pertaining to synthethic time series data generation using deep learning models or other techniques
qpwdps,0,what does having more than three reviews mean on iclr 22 iclr 22 reviews are in hope you guys got good reviews i noticed that some papers got three reviews while others got four or five reviews why do some papers get more reviews and what does it signify
rez90o,0,machine learning wayr what are you reading week 127 this is a place to share machine learning research papers journals and articles that you re reading this week if it relates to what you re researching by all means elaborate and give us your insight otherwise it could just be an interesting paper you ve read please try to provide some insight from your understanding and please don t post things which are present in wiki preferably you should link the arxiv page not the pdf you can easily access the pdf from the summary page but not the other way around or any other pertinent links previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 121 130 week 1 11 21 31 41 51 61 71 81 91 101 111 121 week 2 12 22 32 42 52 62 72 82 92 102 112 122 week 3 13 23 33 43 53 63 73 83 93 103 113 123 week 4 14 24 34 44 54 64 74 84 94 104 114 124 week 5 15 25 35 45 55 65 75 85 95 105 115 125 week 6 16 26 36 46 56 66 76 86 96 106 116 126 week 7 17 27 37 47 57 67 77 87 97 107 117 week 8 18 28 38 48 58 68 78 88 98 108 118 week 9 19 29 39 49 59 69 79 89 99 109 119 week 10 20 30 40 50 60 70 80 90 100 110 120 most upvoted papers two weeks ago u catalyzex code bot paper link u pm me your profanity besides that there are no rules have fun
ri8f7a,0,has there been any work on detecting predicting smells seeing that there are electronics which can detect aspects of air like humidity temperature vocs other gases is there any ml research into detecting smells and research into what sensors would be needed to make decent predictions i m not thinking about detecting specific chemicals but more along the lines of how the nose works a lot of ml work is about image detection and language modeling how about other senses
rhdr2b,0,rudall e text to image 12 billion parameter commercial version xxl 12b is available press release project page russian english translation post about the 1 3 billion parameter free version of rudall e x200b examples that i created using the 1 3 billion parameter free version upscaled from 256x256 to 1024x1024 woman with rainbow hair sketch of a chipmunk semi abstract art
raen75,1,training to produce a text file dipping my toes into your world i am looking for some guidance from this community i have never used machine learning for anything but i think i have a use case for it and am hoping someone could steer me in the right direction background programming background is mostly in the c and python languages plus shell scripting for glue c for embedded programming and python as a general purpose programming language i have experience with some of the most popular dashboard libraries for python problem i have something like 100 examples of configuration files that are mostly generated based off of the contents of 1 file if i have 100 example test cases each will have a unique config source file pairing currently all new configuration files are filled out by hand and while i could partially automate it the way i have in the past parsing through the files i d like to see if i can learn something new and approach the problem from a machine learning perspective are there specific types of libraries for training a model to consume and produce simple text files is this an appropriate use case for machine learning is my training sample set too small have i overthought the problem would this require more work than it s worth any thoughts or criticism is appreciated
rkw26f,0,aws a visual interactive introduction and explanation to double descent i hope this is allowed here given it s not an arxiv link though the derivation in the second article can easily be thrown into one two articles about double descent the first introduces the concept in a visual manner second provides an explanation for linear models as a relation to energy in natural cubic splines both designed to be quick for consumption and understanding
rf1kph,1,need help making a transformer model and alterations to make to it using tensorflow on googlecolab i have been assigned to come up with some creative solutions for a neural network first how should i get started to make a transformer model most of the code is made for me i just have to build the model itself compile it and fit it next is more or less just trying out a bunch of different experiments or approaches and seeing how they affect the data what are some cool or interesting things i could add to my neural network to see how they would affect the outcome i d recommend exploring ideas and analysis options by conducting a series of experiments or data analyses in each experiment you make a change to how you are analyzing the data you might use a different optimizer or add another layer to your network which changes you make should be guided by questions or problems you encounter for example your model s accuracy might be low and you might try to improve accuracy by adding a new layer to your network for another example if you observe overfitting in your model you might try adding a regularizer to reduce overfitting you then observe the effect of each change you make on the data analysis so you can evaluate what the change does for example does adding another layer to the network improve accuracy does it also increase overfitting
r85ozs,1,your recommended sub to learn ml first time opining this sub i see that it is 65 memes and 35 showing what can be achieved with ml no question posts no links to useful articles books courses no showing off your own results with this in mind what is your preferred sub to actually learn ml
r4rd5h,1,object detection for handwritten signatures i m trying to run object detection on pdf documents to recognize the signature position do you know any pretrained model that can recognize signatures
ruvgva,1,ðŸ’Šyour daily dose of machine learning opencv this is a series of posts that i post almost daily i call them â€œyour daily dose of machine learningâ€ if youâ€™ve worked on image processing projects before then you probably heard of opencv itâ€™s a very cool framework for doing all sorts of things related to computer vision some of the things you can do with opencv include basic image processing techniques such as applying filters blurring edge enhancement â€¦ depth estimation using stereo cameras extracting features and computing descriptors for images classical machine learning such as linear regression svm bag of visual words deep learning for image classification and object detection although opencv is not a machine learning framework it still offers a variety of tools to build some powerful ml applications can opencv replace tensorflow in your codebase not really but if youâ€™re already using opencv in your codebase then itâ€™s worth taking a look at itâ€™s ml capabilities and you might be surprised by what it can do iâ€™ll share more insights about opencv in the upcoming days so stay tuned connect with me on your favorite social network
r4zaut,0,eliminate pytorch s cuda error out of memory with 1 line of code i ve been working on a fast pytorch wrapper that solves oom error automatically project link this project aims to be as flexible as possible and it works with existing pytorch code i would love to hear your thoughts on this suggestions are welcome
rbc9fd,1,does anyone have experience using riot games api to do machine learning tasks i am doing a final project in which i would like to use riot games api to do some basic machine learning analysis from my league of legends profile x200b i have been following this tutorial but there are some variables in it that are undefined when i followed the tutorial x200b i am just wondering if anyone could help me understand what i am missing or share a way that you used riot api to learn i am not doing something insanely hard or anything just want to do basic analysis
rp849m,0,research paper figure drawing hi all as a novice researcher i have one question about drawing great quality of figure for scientific research paper would you like to suggest me how mostly researcher draw very interesting figure with many customization in deep learning based papers could you suggest some great tools to make drawing much easier to express or add also any latex source code or website to customize the drawing or any video series where they talked about or demonstrate how to draw such figure for research paper thank you for your time and suggestions regarding this
qo5fhz,0,problems with a neural networks s output s order of magnitude i m working on a project where i m trying to train an airplane to control itself in a 2d setting the neural network that acts as the airplane s pilot has 2 outputs that are related to the control of the airplane s altitude angle of attack and throttle for those that are familiar unfortunately my network is outputing numbers that have a completely wrong order of magnitude i tried writing my own activation function to limit their values but i m simply getting the max or min allowable value since my actual output is off the scale is there any way to control the order of magnitude of the outputs i ve tried a ton of things from intitializing weights to be extremely small to normalization of the input
rbvz3z,0,edge weight prediction for undirected graphs hello everyone i m pretty new to graph learning as my research is in audio generation but there s a specific task i m working on where i think my data may be better represented as an undirected weighted graph in the abstract sense the problem would be given a set of vertices each with rich features and a weighted adjacency matrix this may be complete or it may be incomplete â€” i want to look at both formulations predict 1 the edge weight between an unseen test vertex and any vertex in the input graph 2 the edge weight between two unseen test vertices i d like to do some reading on this if possible before trying to hack something together so i m wondering if anyone has seen work on anything like this i m struggling to find any literature on a similar problem many thanks
r1p73d,1,time series forecasting daily temperature so i have this dataset where it contains monthly record of temperature can i still predict daily temperature even if i only have monthly record of temperature
rk117l,0,what will come after machine learning hi i would like to know according to your experience what will be the next hot topic some people might say machine learning data science will never die but i would like to know what will be the trend in the next couple of years would it be quatum computing if it will be machine learning what will be the topic in ml dl thank you in advance
rdfmll,1,best way to learn ai during break background 19m college student majoring in cs learned basic web dev data manipulation with java python software engineering with java some blockchain smart contract development love linux and cli anyways i just finished the hardest semester i ever had and probably will have and now i feel like i need to be learning something i want to spend this winter break learning about ai and maybe some other technologies i would love suggestions for other technologies what is the best place to start i have had wonderful experiences with udemy courses and i plan to find a course there but i don t know what area of ai to study i have some foundational knowledge 4 5 hours of research and i definitely will say gans are very interesting to me i would love to get hands on experience with if anything that can help me secure a high paying job in the future would also be nice so i guess my question is moreso what area of ai i should get into what has the most traction right now how do i get involved i want the experience to be able to predict where the technologies are heading
rb62hd,1,how do i code in ubuntu hi everyone i generally use windows and python via anaconda jupyter notebook today i installed ubuntu in my laptop in order to learn that as well parallely but i ve already spent so much time in understanding basics like how to install package and launch jupyter notebook from terminal etc i want to know if anyone can recommend me a brief tutorial for ubuntu terminal just enough to get me started
rhwua0,1,undersampling in r hi guys x200b i have a highly unbalanced dataset 93 accuracy on zeror and i want to undersample it a bit to reduce zeror accuracy for demonstration purposes in a data analytics course in uni x200b however i m uncertain how to go about it first of all how much should i remove of the overrepresented value and how do i do it x200b i kinda want to just cut maybe 30 of the overrepresented data simply by sorting the dataframe by target true false and then delete the bottom 30 of the values by their row number as the data is super jumbled anyways so sorting by target true and deleting bottom values removes a relatively random cut of the target false rows target false is the overrepresented value also how do i actually remove a set amount of rows based on the row number lol
qxdqt0,0,similar open source long list to tf like pytorch ecosystem tools i have found this pretty cool page to pytorch that contains great many libraries to pytorch ecosystem tools but i can not find any similar page to tensor flow if you know about it please let me know not just a top 10 article in medium but an actual longlist that contain a wide variety of options
qzic0a,0,zeroshot topics label your text data or infer topics in your text data automatically zeroshot topics github link hand labelled training sets are expensive and time consuming to create usually some datasets call for domain expertise eg medical finance datasets etc given these factors around costs and inflexibility of hand labelling it would be nice if there are tools that can help us get started quickly with a minimal labelled dataset enter weak supervision but what if you do not have any labelled data at all is there a way to still label your data automatically in some way that s where zeroshot topics might be useful to help you to be up and running quickly zeroshot topics lets you do exactly that it leverages the power of zero shot classifiers transformers knowledge graphs to automatically suggest labels topics from your text data all you need to do is point it towards your data x200b please check this out and share your feedback
rc0coi,0,gopher 280 billion parameters language model by deepmind blog post direct paper link seems like a compilation of their findings on scaling lm s a bit more than gpt3 retro a retrieval style model
rh0rb2,0,sota of adversarial examples i m having a hard time finding whats currently the state of the art of adversarial attacks and defenses from what i ve gathered is that its still hard to defend against cleverly engineered ones how does it impact ur research or ur decision to use cnns or so do other methods such as svm display the same weaknesses
r8v6rk,1,career change into programming data at 30 y o â€“ options in europe hi everyone before starting just a little bit of background about myself i am 30 years old french and therefore a citizen of the eu i already have a master s degree in engineering in chemistry materials polymer i have been working for 6 years as a process engineer in the automotive industry 2 years in france 2 years in slovakia 2 years in germany i have ended my job in germany but i am currently learning part time german in a school there to pass b2 i have a lot of free time 6 7 hours day i also speak english if necessary i can consider passing my c1 certificate and french so now after many months of reflexion i would like to change my career the field in which i can work mainly related to my studies does not suit me i have a great attraction for programming and computer science and even if i only have a very basic knowledge about it i would like to change into this field i m not totally sure yet but i would like to do and am therefore targeting one of the following jobs data analyst or data scientist and or probably later on a job related to machine learning ai i m just discovering it but i am getting super excited about it for info i have been learning python data science by myself for a couple of months now so finally here are my questions what are the best learning options for me to be employable or to have certified or company recognised knowledge in order to find a job in data science data analyst for example to start and which ones would you recommend i am looking to study in europe online if possible or in person in germany to find a job in germany switzerland austria for example it can be fast or long free or not i have some time and money to invest now but i don t want to if i realise it is not necessary different options i found so far self learning a couple of portfolio projects free but difficult to prove competences no certifications and no supervision bootcamps online expensive but quite fast and intense however i m not sure if this is recognized by companies and if it will really help me find a job master degree do you think i have a chance to find a master online in europe starting 2022 knowing that i donâ€™t have any educational background in cs master conversion course i have heard about it maybe it is only in uk i am not totally sure about how it works but knowing that i have already a master degree i probably donâ€™t have to start from the beginning bachelor or should i start first with a bachelor is a bachelor enough online would be better any city or country in europe or are there other schools which are not bootcamps not universities but something between that are recognized or certified and might help me a lot if you have any ideas or information that could help me i would be very grateful i am motivated and i can invest a lot in this project but i am not totally sure what would be the best option at the moment or what qualification is really necessary for this job thanks in advance
r74pmb,0,mito speed up dataset manipulation with no coding x200b mito is a spreadsheet provided as a python library which allows you to manipulate a dataset in a simple and fast way and above all in an interactive way mito provides a graphical interface within the jupyter lab environment so you can manipulate any dataset what can you do with mito 1 import dataset load a new dataset from the file system browse among the directories of the filesystem supported formats include excel xlsx and csv 2 export dataset download a manipulated dataset as a csv to your local filesystem 3 add column add a new column to the dataset you can change the column name as well as the column values you can either enter values manually or calculate values from the other columns double click on the first row of the column to insert a formula 4 delete column erase a column completely 5 pivot table build a pivot table the resulting table can be manipulated separately 6 merge two datasets you can choose among the following merge types lookup left right inner outer 7 plot a graph you can choose the columns associated to x and y axes and then choose one of the supported graphs
rn27l0,1,android studio with tf lite hey guys anyone experienced with using tensor flow lite in android studio i have trained a model exported in tflite and then getting and error while uploading it to studio that meta files are not included any suggestions
rweffw,1,finding good dataset for diagnosing crop disease project hi i have decided to work on a machine learning project i want to build an app that can be used to diagnose crop diseases you would simply snap a picture of the crop and it would use machine learning to predict the disease associated with the picture the most critical step in this project is finding a good dataset could anyone point me in the direction of any website that would have a dataset that i can use for this project
rkrcyh,0,inverting photodna with machine learning microsoft photodna creates a â€œunique digital signatureâ€ of an image which can be matched against a database containing signatures of previously identified illegal images like csam the technology is used by companies including google facebook and twitter microsoft says a photodna hash is not reversible and therefore cannot be used to recreate an image this project shows that photodna does not perfectly hide information about the source image used to compute the signature and that in fact a photodna hash can be used to produce thumbnail quality reproductions of the original image x200b rough body shapes and faces can be recovered from the photodna hash the face is from thispersondoesnotexist com more details about photodna and the approach used to invert photodna hashes are in this blog post code is available on github
qkb6ga,0,plagiarism case detected iclr 2022 news discussion x200b the submission was withdrawn by the authors before the program chairs posted a desk reject citing a serious case of plagiarism what is happening the figures and tables do look like they ve been lifted straight from previous papers
ricr2w,1,is gaussian likelihood loss equivalent to mse loss for a vae in this tutorial it appears that the author uses gaussian likelihood with a log scale as a reconstruction loss and explains it as the likelihood of encountering that image if you cannot view the link the code is here see the gaussian likelihood function and recon loss other vae implementations typically use the mse as a reconstruction loss instead and the typical justification is that they are equivalent for a gaussian distribution other places i ve seen dismiss mse as a reconstruction loss e g this link which states it s basically a gaussian with a fixed variance which from what i understand is not the same as what is implemented in the tutorial which is sampling from a gaussian distribution with a dynamic variance for what it s worth i swapped out the gaussian likelihood loss with an mseloss and it didn t perform as well the kl term basically went down to 0 almost immediately as did the reconstruction loss with the original implementation though it was significantly better and the kl term kept increasing over time i m rather new to vaes and a bit bad at statistics so i m most likely missing something what is the reason for this discrepancy
r6otxk,0,cohen s kappa â€” useful i often see subtle misuses of interrater reliability metrics for example imagine you re running a search relevance task where search raters label query result pairs on a 5 point scale very relevant 2 slightly relevant 1 okay 0 slightly irrelevant 1 very irrelevant 2 marking very relevant vs slightly relevant isn t a big difference but very relevant vs very irrelevant is however most irr calculations don t take this kind of ordering into account so it gets ignored i wrote an introduction to cohen s kappa a rather simplistic and flawed metric but a good starting point to understanding irr hope it helps i welcome feedback and am curious to hear the irr metrics you find yourself relying on most
rhgoj3,1,i am working on a speech to text transcriber any projects tutorials or tips hi guys i am working on a simple speech to text transcriber using tensorflow and python but i want to have a look at similar projects which have been done earlier for my research here is what i hope to achieve 1 imagine having a video stream input or a video file i want to transcribe what the speaker says to text based output 2 i want to develop a model train it 3 get the text based output 4 furthermore develop the model to work better and scale thanks in advance
rr17f9,0,4 5 times faster hugging face transformer inference by modifying some python ast recently ðŸ¤— hugging face people have released a commercial product called infinity to perform inference with very high performance aka very fast compared to pytorch fastapi deployment unfortunately itâ€™s a paid product costing 20k for one model deployed on a single machine no info on price scaling publicly available according to their product director transformer deploy is an open source alternative build over enterprise grade softwares inference server nvidia triton it takes queries and passes them to an engine plus adds features useful for inference like dynamic batching or multi inference engine dispatching inference engines microsoft onnx runtime for cpu and gpu inference and nvidia tensorrt gpu only it appears that without much effort it was easy to match the very few hf infinity public benchmarks but there was still an opportunity to push inference performances further that afaik is not yet leveraged by any other oss project gpu quantization for all transformer models please find below our measures on roberta base seq len 256 batch 32 mnli dataset classification source code repo performing gpu quantization requires modifying model source code to add some specific nodes called qdq on costly operations like matrix multiplication which is both error prone boring and a good generator of technical debts you maintain yourself the source code of your modified model we have done that work manually for several models and it appeared to us that it can be made automatically by just patching the model module abstract syntax tree aka the source code on the user end performing basic quantization of a model on the gpu looks like as shown in the benchmark to get a model 4 5 times faster than vanilla pytorch it costs 0 4 accuracy point on the mnli dataset which is in many cases a reasonable tradeoff itâ€™s also possible to not lose any accuracy the speedup will be around 3 2 faster of course the exact trade off depends on the model the dataset etc but it gives a basic idea itâ€™s a big improvement compared to a previous version of this project where speedup was costing over 1 accuracy point behind the scene transformer source code is parsed to ast and operators like matmul or layernorm are wrapped by a quantizer linear layers are replaced by quantized versions of them some tensorrt unsupported operators are replaced etc then new source code replaces in ram the original one right now we have successfully tested the process with albert bert including minilm distilbert roberta including camembert xlm r distilroberta etc electra it should work out of the box or with very little effort for any transformer model which can be exported to onnx format regarding cpu inference quantization is very easy and supported by transformer deploy however performance on transformer are very low outside corner cases like no batch very short sequence and distilled model and last intel generation cpu based instance like c6 or m6 on aws are quite expensive compared to a cheap gpu like nvidia t4 to say it otherwise on transformer until you are ok with slow inference and takes a small instance for a poc for instance cpu inference is probably not a good idea
qu0fvq,0,change the resolution of a model gan hi guys i want to change the resolution of a gan i m currently working with the generator and discriminator consist of convolutional layers conv2dtranspose and conv2d how do i know how to change each layer to get for example a twice as big resolution i tried to google it the whole day but couldn t find a feasible solution i really appreciate any help you can provide this is the code for the gan the tensorflow version
rb7cs3,1,can someone explain some code of vqgan clip for art generation here s the link i read a few articles that explained how vqgan worked like a gan and clip as a discriminator i looked at the code but didn t understand too much i just want to know which code block is doing what the fifth block for example defines functions like sinc lanczos and i actually don t know what they re being used for thank you
rx9kzo,1,how to incorporate normalization to inference dear machine learning practitioners intro as mentioned above i am currently training a neural network on a small 350 sample size dataset to perform regression i would like to use this trained network in my webapi which can be accessed by users with their own samples however the samples come in at a later stage question now comes the question i am using pytorch and the sklearn libraries so my code to normalize the dataset looks similar to this x200b normalize true if normalize sc minmaxscaler x train sc fit transform x y train sc fit transform y x200b now how can i save this minmaxscaler information and apply it to inference data at a later stage what is the smoothest way of doing this thanks for any pointers and your help
rthi58,1,customer survival analysis in saas company hello redditors i am about to start a survival analysis project using python for my thesis also since i will use data from the company that i am working for i thought that it would be a nice idea to create an end to end survival analysis project for the company also i am searching everywhere to understand what the survival analysis is what is the difference between non parametric and parametric analysis how can we use ml models to do survival analysis and many more the main question that i have and which would be super useful to have a main understanding is what s the result outcome of a customer survival analysis what questions can be answered if the analysis has a meaningful results how can the company make use of the result of this analysis thank you for your time
r3n2zl,0,the inherent limitations of gpt 3 i wrote up a little editorial titled the inherent limitations of gpt 3 it is not negative towards gpt 3 i hope but rather lays out some of the basic facts on what its architectural constraints are mainly so anyone worried it ll take their job or lead to agi can find this and hopefully relax would love feedback on it especially any corrections
qpenkt,0,project google movenet real time pose estimation used to control nintendo punch out hey ai fans i hacked the original nintendo punch out so that you control it with actual punches this is a boxing video game that now uses google s movenet real time pose estimation to track your movements and detect punches blocks and other moves and then sends those commands to the game you can check out the full video here with plenty of sweet movenet footage and play it yourself here x200b
r5a6vx,0,paper explained ext5 towards extreme multi task scaling for transfer learning video walkthrough the t5 model has been a staple for nlp research for the last years both its size and its approach to formulate all nlp tasks as prompt based language modeling make it a convenient choice to tackle new challenges and provides a strong baseline for most current datasets ext5 pushes t5 to its limits by pre training not only on self supervised mask filling but also at the same time on 107 different supervised nlp tasks which is their new exmix dataset the resulting model compares very favorably to t5 when fine tuned to downstream tasks x200b outline 0 00 intro overview 2 15 recap the t5 model 3 55 the ext5 model and task formulations 8 10 exmix dataset 9 35 do different tasks help each other 16 50 which tasks should we include 20 30 pre training vs pre finetuning 23 00 a few hypotheses about what s going on 27 20 how much self supervised data to use 34 15 more experimental results 38 40 conclusion summary x200b paper
rwb444,1,should i study proofs for the math i m learning hello everyone right now i m studying from an intro to probability textbook first undergrad probability course and eventually i want to learn the math needed for machine learning so should i bother spending time on the derivations of theorems and all like i know it is important to know how machine learning algorithms are formed under the hood but what i m talking about is should i bother to do stuff like derive through induction the inclusion exclusion formula for n events x200b thank you
rp0t4m,1,stacking models i am trying to stack multiple models i am using stacking regression from sklearn but the accuracy of the stacked model is not better than the individual models is there any other way or what can i do to increase my accuracy
rqe2bt,1,after python and r which of the following language is most worthwhile to learn for ml c scala java perl and iâ€™ve seen javascript and typescript used before
ro8p1o,0,dataset containing similar looking objects in the wild have you ve seen a dataset like this 10 images of car a and 10 images of car b both a and b looks similar this is almost what i am looking for but not exactly
rbsbgf,1,hi is anyone able to help me with my college project its about linear models with gradient descrnt neural networks and deep learning on python please dm me if you can thanks ðŸ˜Š
qliy7s,0,natural language only coding with co pilot stream 11 2 10pm pst kinda late to the party but i just got access to github s copilot ai backed code auto complete tool it can do some pretty impressive things i have not played with it for more than an hour and i m pretty impressed at 10pm pst today i will be streaming at twitch tv evanthebouncy for about 1 2 hours where i will be attempting to perform simple coding exercises by writing only comments and letting co pilot complete the code from my natural language inputs it ll be fun if you can come and spam some ideas in case you haven t had a chance to play with it yet i will also be giving some commentary reactions to it as i work in program synthesis for a living and this is a pretty cool piece of tech that will definitely change how people think about programming in the near future mod if this is kinda spammy feel free to just delete the thread idc
r6b4uu,1,why do we need regularisation l2 or l1 norm in logistic regression as i was revising through my logistic regression notes and came around the loss minimization interpretation of logistic regression which is argmin w log 1 exp zi 1 2 lambda w 2 where zi yi wi xi summation i 1 n i know that the l2 regularisation as used in the above optimization function is used to find a balance between a good seperating hyperplane decision surface and weight coefficients that are not too large tending to infinity to be overestimated i can t seem to intuitively understand as to how regularisation is working to balance the weight coefficients to avoid overfitting underfitting also i might be having a misunderstanding here but in the loss function optimization part of the expression if we consider that we are not using any regularisation then ideally to minimise the loss function for points that are correctly seperated the weights corresponding to features should tend to infinity such the value of zi tends to infinity which results in log 1 exp zi tending to 0 so we are minimizing the sum over correctly classified points but for the same plane with infinitely big weights if a point comes out to be incorrectly classified it s loss function value will tend to infinity which makes it working against the optimisation problem so accordingly the weights should get readjusted to smaller values such that the sum of loss is minimized without the need of a regularisation term so i am really very confused as do we even need regularisation in logistic regression if yes how regularisation term in the expression is working towards balancing the weights
r44hwo,0,professional voice ai hi folks i am looking for a realistic and informative voice changer text to speech tool i want to use it for producing professional 2d ads it should be based on ai and has to be really professional cost does not matter the result must be very proficient do you guys know any reliable platforms projects
rwl4xl,0,pretraining the discriminator of a least squares gan i am trying to train a gan to generate human poses in 3d space using the humans 3 6m dataset the output of the gan is thus the 3d coordinates of the human joints i have been experimenting with vanilla gans but the output is quite noisy i am now looking into least squares gan but was wondering if it is a good idea to pretrain the discriminator of a least squares gan since lsgans address the problem of vanishing gradients and loss saturation
r3ajsj,0,discussion face recognition remarkable papers i m new to face recognition can anyone help me with what the most recent remarkable papers in this field are i m started with facenet spherface cosface and arcface and i want to continue with some new papers
rm3cxf,1,i would like to ask a question about how to approach an ai project good morning x200b i would like to design an artificial intelligence that is able to improve its performance in a task by having another ai tell it what its mistakes are x200b concretely the scenario would be 3 intelligences capable of playing chess x200b intelligence 1 would not know how to play chess but it can learn by communicating with intelligence 3 intelligence 2 would not learn in any way but it is better than intelligence 1 and intelligence 3 is superior to intelligence 2 and every time it finishes the game it tells intelligence 1 what it detected as wrong x200b the ias don t have to speak in a human language for the project if they are able to communicate with a strip of ajdfgjjagfasdf asdfwgasryh tuds that works for me even if i don t understand it x200b my problem is that i know practically nothing about ml and i don t know from which angle to approach the problem although i have experience programming in fact the intelligence number 2 would be done but it is a minmax algorithm x200b a point to keep in mind is that the important thing is not that the intelligences end up being chess masters the point is to design a teacher student communication circuit that later can be extrapolated to other problems felices fiestas
r7mekf,1,in a class lost and confused greetings all x200b letâ€™s get the disclaimers out of the way first yes this is a graded project that makes up a significant part of my class grade no i am not looking for yâ€™all to do my homework x200b yâ€™all i need help i am so lost i do not even know the correct question to ask i am in a machine learning college class pre requisites for the class are linear algebra probabilities stats 2 and calc 3 iâ€™ve also got compsci classes under my belt i passed all of those with good grades for the past 12 weeks this class has been theory lecture and homework to show you can calculate the algorithm but there has been no application or functional use there is a textbook that we have not cracked open mine still has the unscratched access code inside the font cover we now have a project to do it has been discussed for the semester with the last few weeks for â€˜researchâ€™ and only yesterday did we get the rubric for the presentation in under 10 days i am not the only one in the class who is lost i said i would do a presentation on financial forecasting on a data set i pulled from a public source of historical data for ease of discussion iâ€™ll call it a 50x50 matrix â€“ 50 numeric observations rows with 50 categories col category averages wildly vary from each other so the first thing i do is normalize them 1 col col and drop into a new matrix ok great i also know i need to break it up into training verification and active use testing picked 70 15 15 so i lop off the first 35 normalized rows i am doing this all in matlab as prescribed by the course now what instructor has said â€œoh that will be perfect for support vector machine â€ and then a a couple days ago gave me a stack of finance time series journal article x200b what the frack do i do i donâ€™t know where to go from here the instructor i believe could help me if i had any idea what the question was to ask i can at least get either the classification app or the â€˜econometric modelerâ€ from the computational finance app section open that the data loaded and i donâ€™t understand anything of the outputs i just i am completely lost
r4fep0,0,news get code for ml ai papers anywhere on the internet google arxiv twitter scholar and other sites â¤ï¸ x200b browser extension on chrome browser extension on firefox
qnhf5f,0,tf keras in hugging face datasets edition hi all tensorflow maintainer at hugging face here i posted here a few months ago about the big change we were making to the library to make everything keras native and people seemed to like it so i thought i d give another update on what s changed since then we ve made a couple of big changes that reduce the amount of duplicate boilerplate code in common scripts massively and we d love to get people using the new approaches and get feedback what happened in last week s episode of hugging face the story up until now is that all our models are now keras models you can still write your own training loop or use the models as a layer in a larger model everything like that remains unchanged but it s incredibly convenient to just load a model then just immediately compile and fit it i gave some examples in the post i linked above last week usually refers to times less than four months ago you aren t telling me anything that isn t already in my performance reviews don t worry at least you delivered eventually what s new so the first big new change is a really nice integration with ðŸ¤— datasets if you re unfamiliar datasets is the data equivalent to hugging face s model hub you just load any uploaded dataset in one line of code the same way you load a pretrained model with the load dataset function it s not just for nlp people are using transformers for audio and vision and everything else these days so there s all kinds of data in there you should check it out to see an example of load dataset in action a standard workflow with datasets and transformers goes something like this from transformers import autotokenizer tfautomodelforsequenceclassification from datasets import load dataset load a pretrained model and its tokenizer model name bert base cased tokenizer autotokenizer from pretrained model name model tfautomodelforsequenceclassification from pretrained model name load a dataset we ll use the cola dataset from the glue benchmark data load dataset glue cola define a function to tokenize the data then apply it to the dataset the tokenizer returns a dict and map will add keys from that dict to the dataset as columns def tokenize function dataset return tokenizer dataset sentence tokenized dataset data map tokenize function so far so good but this is the point where problems start to arise because it s really hard to get the tokenized data into your model the data is often 1 quite large and 2 jagged because different samples will tokenize to arrays of different lengths as a result if you want to load the data as a single dict of np ndarray or tf tensor you end up having to do huge amounts of padding which bloats memory usage and massively slows down the model the way to get good performance is to load random batches of samples and only pad that batch not the entire dataset but doing that basically required you to write a custom training loop or at the very least a python generator before it would work with keras if anyone was using transformers with tf before now i d love to hear how you were solving this because it was a huge recurring pain for me so is there a solution now there is the solution is that we added the method to tf dataset to all our datasets this basically wraps the dataset in a tf data dataset which will do the just in time data padding you want we ve also updated our datacollator classes to work with this so you can generate your dataset like so from transformers import datacollatorwithpadding data collator datacollatorwithpadding tokenizer tokenizer return tensors tf tf dataset tokenized dataset train to tf dataset columns input ids attention mask labels batch size 16 shuffle true collate fn data collator note how the data collator needs your model s tokenizer that s because every god damned research group in every god damned university in every god damned country handles their data in a slightly different way and so there s no universal approach to padding that works for all of the hundreds of different models out there we do guarantee though that the tokenizer that comes with a given model will have a pad method that works for that model and that s what the data collator will use you have no idea how much pain you re being saved with that method okay calm down what do i do with this tf dataset that bit s easy a lot of people aren t that familiar with tf data but it s actually really cool once you have a tf data dataset you can pass it straight to model fit or just iterate over it in a for loop to get batches won t i need to compile this model before i can fit it what loss should i use that s a great question and that brings me to the second big change we ve made our models now automatically compute losses that are suitable for their task in a way that s accessible to keras in other words if you use tfautomodelforsequenceclassification that model will now compute a loss appropriate for sequence classification tasks i e crossentropy for you don t know what loss you need to train gpt 2 with no problem tfautomodelforcausallm from pretrained gpt2 will do it for you wait stop i m an advanced user and i want my loss not your loss don t panic you can still use whatever loss you want and all old code will work exactly as it did before the only change is that if you compile your model without a loss it ll interpret that as you wanting the default internal loss if you specify a loss argument to compile then it ll use that and not the internal loss in addition this only applies when using the keras api like fit if you re writing manual training loops or using the model as a layer in a larger model none of this is relevant to you this is just a convenience and it s easy to disable so i just skip the loss argument exactly if we continue on the code samples from above all you need to do is from tensorflow keras optimizers import adam optimizer adam 3e 5 transformers work much better with lower lrs model compile optimizer optimizer no loss argument model fit tf dataset and that s it dataset loaded tokenized and trained on with changes to a few lines almost any nlp task from translation to token classification or summarization can be handled in a similar way if you want to see more we have a bunch of example notebooks in both tensorflow and pytorch and all the tf examples should be up to date with these new methods
r7x490,1,is it always necessary to retrain a model on an entire dataset to further elaborate on this question i will give an example let s say we have a dataset consisting of 1000 datapoints we have trained a model on this data we deploy it and it starts making predictions every week however we gather more and more data lets say 250 new datapoints every week to improve our model we want the model to learn from these new points is it necessary to retrain the model on these 1250 datapoints or is there someway to keep its original learning and only have it additionally learn from these 250 points i can imagine that for a smaller dataset this does not really matter but once you get to much bigger datasets this can be a very costly process to have to retrain a model on all the data
rga91a,0,are you using pytorch or tensorflow going into 2022 pytorch tensorflow and both of their ecosystems have been developing so quickly that i thought it was time to take another look at how they stack up against one another i ve been doing some analysis of how the frameworks compare and found some pretty interesting results for now pytorch is still the research framework and tensorflow is still the industry framework the majority of all papers on papers with code use pytorch while more job listings seek users of tensorflow i did a more thorough analysis of the relevant differences between the two frameworks which you can read here if you re interested which framework are you using going into 2022 how do you think jax haiku will compete with pytorch and tensorflow in the coming years i d love to hear your thoughts
rpjnzd,1,generalized sequence from a set of similar sequences hello i have a problem i have some sequences like aaabbbccc aaabbbccd aacbbbcdcc xaabbbxccc these sequences will be similar what i want to do is to create a general sequence from them like aaabbbccdc i learned about sequential pattern mining and prefixspan algorithm but i am not sure if i am on the right track is there any algorithm where i feed these sequences and i get a generalized sequence or something close to it
qwmizo,0,benchmarking deep learning with m1 pro gpu metal vs colab gpu tesla p80 and kaggle p100 hey r machinelearning if someone like me was wondered how m1 pro with new tensorflow pluggabledevice metal performs on model training compared to free gpus i made a quick comparison of them in a nutshell m1 pro is 2x faster p80 p100 is 2x faster m1 pro and equal to m1 max however transformers seems not good optimized for apple silicon x200b p s pytorch gpu support is on the way too
qwbi8e,0,where to find the balance between writing code from scratch and just modifying code so for my final year aerospace project i want to implement ml i m a newbie but as it s my final year project i will need something presentable the odds are most of the things i need in ml like image recognition someone will have already written the code for it so it feels like i d just be copying and pasting their code i know i will be modifying it to some degree but still i don t think i d learn much from it obviously i don t wanna reinvent the wheel but still how does one go about this
ruk1ge,1,any good fluid dynamics data repo s hey everyone does anyone know of any good repositories of sensor data for fluid dynamics or for that matter of vibration data i m trying to use dmd to find the dominant coherent structures in a dataset but have limited funds for experiments and limited access to sensors thank
r0mq3l,0,openai s miles brundage on ai misuse and trustworthy ai some of you might find this new gradient interview with miles brundage interesting papers touched on will technology make work better for everyone economic possibilities for our children artificial intelligence and the future of work education and leisure taking superintelligence seriously the malicious use of artificial intelligence forecasting prevention and mitigation release strategies and the social impact of language models all the news thatâ€™s fit to fabricate ai generated text as a tool of media misinformation toward trustworthy ai development mechanisms for supporting verifiable claims timeline 00 00 intro 01 05 how did you get started in ai 07 05 writing about ai on slate 09 20 start of phd 13 00 ai and the end of scarcity 18 12 malicious uses of ai 28 00 gpt 2 and publication norms 33 30 ai generated text for misinformation 37 05 state of ai misinformation 41 30 trustworthy ai 48 50 openai policy research team 53 15 outro
rbc8s5,0,how do you organize track your reading list i feel like at any given time i have a million tabs open on like 20 different browser instances each tab linking an arxiv abstract i want to read don t even get me started on my github stars and browser bookmarks how do you keep track of stuff you want to read how do you keep track of things you ve read that you want to be able to dig up later how do you tie in your notes neurips has barely started and i already feel like i m drowning in stuff i want to dive into and not lose immediately afterwards are there some like top secret tools only the hip grad students know about or something how the hell do you collect and organize the stuff you want to read and or recently read please tell me there s a better way
r0fnl3,0,term for specifically trained model per object for my use case i train and use a specific model for each object such as a building is there some term that defines this case versus using one model for a task which is then applied for every building
quta9z,0,how to get access to the gpt 3 api in late 2021 i m trying to get access to the gpt 3 api and i applied following the guidelines form email to the cto ðŸ˜‡ yesterday should i wait a few more days or should i go creative is it still super hard to get access to the api thanks
r76sfb,0,discussion applied tracks of ml conferences in spite of having good set of reviews in aaai my paper was rejected citing that it was more appropriate for applied tracks of ml conferences even though i don t fully agree with my reviewers since there were considerable novel approaches in rl and it is extremely heart breaking to see a year worth of work get rejected based on one opinion it is time to find this child of mine a new home kindy suggest places where i can send this paper what all reputed ai conferences are coming up next which have a science track my work involves graph networks and rl so something along those lines is also fine thanks a lot for reading and to all those whose paper got rejected don t worry we will find a way out
rhwofs,0,is it right to use your phd student as an expert annotator on a dataset creation paper and not give them authorship hi guys i m sure you ve seen the recent discussions about the role of datawork in ml research for example here here or here some of that is definitely complicated and there s lots of room for disagreement but today i came across an example of a well known researcher choosing not to give a phd student authorship and it made me wonder what do you guys think here s the post in question the names of particular expert annotators the ones who wrote the dataset are acknowledged by name in principle that seems good i think what is surprising though is that some of these expert annotators seem to actually be phd students in the last author s department linguistics the data collection itself relied on their domain expertise presumably this is what qualifies them for the task it strikes me as questionable not to include a student in your department as an author especially when they contributed meaningfully to the dataset are we going to have a split system now where domain expertise as an active graduate level researcher isn t sufficient for authorship on the project is writing the dataset that much less important than running a model on it tl dr is it fair or fucked not to make a phd student an author on a project they worked on just because they worked on the data side of a dataset paper
r51wa6,1,difference between deeplearning framework teachable machine i learned deeplearning but i was wondering why companies didnot use teachable machine as it is already developed what is the difference why they are seeking deeplearning analyist why i just use the already developed app i am just new to this track
rcl0l0,0,quick tips about building a chatbot with gpt 3 or gpt j hello i realize i have more and more questions from people trying to leverage gpt 3 or gpt j for their next chatbot and usually questions are always about 2 things how to format my requests so the model understands that i am in conversational mode how can the model keep an history of my conversation i m answering these 2 points in this quick article i hope it will help i any question please don t hesitate to ask
riqxrq,0,do large language models understand us blog post by blaise aguera y arcas summary large language models llms represent a major advance in artificial intelligence ai and in particular toward the goal of human like artificial general intelligence agi itâ€™s sometimes claimed though that machine learning is â€œjust statisticsâ€ hence that progress in ai is illusory with regard to this grander ambition here i take the contrary view that llms have a great deal to teach us about the nature of language understanding intelligence sociality and personhood specifically statistics do amount to understanding in any falsifiable sense furthermore much of what we consider intelligence is inherently dialogic hence social it requires a theory of mind since the interior state of another being can only be understood through interaction no objective answer is possible to the question of when an â€œitâ€ becomes a â€œwhoâ€ â€” but for many people neural nets running on computers are likely to cross this threshold in the very near future
qrbkc7,0,calling out the authors of trajformer paper for claiming they published code but never doing it i read a paper from neurips 2020 titled trajformer trajectory prediction with local self attentive contexts for autonomous driving i found it interesting and the authors claim multiple times in the paper that we release our code at turns out they never did fine i thought perhaps they will in the future and starred the repo to check it out later many others raised issues asking for update on code release and they never replied finally it april they update the readme to say that they will release the code and that s been the last update i know this is a common trend in ml papers now but what sucks is that i emailed the authors both the grad student and the pi multiple times asking for an update an they never replied their paper is literally based on empirical improvements and without working code to replicate the results it is their word against mine i strongly think things have to change and i believe they only will if we call them out i waited long enough and made significant effort to contact the authors with no response i mean i don t mind them not releasing their code but at least don t claim that you did in the paper review phase and then disappear an undergrad in my lab asked why she should take time to clean up the code and document it before release while others just move on to the next interesting project and i don t have an answer
r1v2z9,1,generating confidence scores for clustering algorithms i have a dataset which requires confidence scores for each point in the dataset apart from the confidence score i require each point have a leader and a follower associated with it for the case of a leader i think assigning the point closest to the centroid for a particular cluster as the leader now i m not sure related to the confidence score or the follower of the points any advise is appreciated
rndy6z,1,ml algo takes into account derivative of timeseries x200b i have timeseries data and corresponding features and trying to determine which features variables to include as well as the best algorithm i have heard the lstm algorithm might be the best for my application however i am wondering about some aspects if i have a signal that predicts my output i want the machine learning model to take into account not only its current position but its past position it s current slope and it s current 2nd derivative acceleration do ml algos like lstm take into account slope and derivatives or will i need to add a slope column feature to capture that behavior thanks
r8ly8r,1,taking coursera nlp specialization but i don t understand the lecture i m halfway through week 1 s material but doesn t really understand those equations and i don t think i m able to write those python they re quite long i then realised the specialisation is intermediate level i read reviews and many said the class is too easy and not deep enough but i think the opposite i don t know anything abt nlp i only have taken python for everybody course by dr chuck and learned some python is there other nlp course that i should try instead or should i try andrew ng s machine learning course first i m planning to apply for nlp language technology master next spring semester thanks so much for taking your time to answer i m going to do more pyhton and ml courses first before diving into nlp
rm9rrt,1,machine learning design to evaluate generator sound hey guys i am thinking about following task and i am somehow not sure about solutions i am curious about how others would develop the system and to what extend is my approach right there is a company monitoring its electrical generators with the goal of detecting when generators need maintenance based on the sounds they emit generators are being continuously monitored with microphones with the audio data being stored for processing i need to design machine learning pipeline that would ingest this data and use it for training in order to detect sounds that would indicate the need for maintenance three main questions 1 supervised vs unsupervised learning i think that supervised is better approach because we will load labeled data into the model as labeled data i undrestand generators sounds two sets need maintenance does not need it 2 what ml architecture recurrent recursive neural networks rnns they are used to solve sequence problems like a speech processing etc 3 what stages of pre processing would your pipeline involve and why here i was thinking that i need to create lists of labeled data load them but i dont really come to more ideas looking forward to your ideas
rekxvo,1,need help or suggestions in data preprocessing hi all currently i ve started thesis in masters in computer science i have to work upon data preprocessing of a dataset the dataset contains accelerometer g values as x y and z there are approx 300 000 rows with x y and z values with a sampling rate of 1600 samples per second data collected for 3 mins is there any approach so that i can reduce the number of samples using machine learning in python thanks
rjw51t,0,dataset omicron daily cases by country covid 19 variant x200b hi guys unfortunately omicron is getting worse every day infecting more and more people around the world i ve made an auto updating daily omicron case by country dataset x200b it is available here x200b when i first posted this i just wanted to have a dataset for myself because i was curious about the speed at which omicron infects the world but as time progresses and the situation is getting worse this might be useful for everyone x200b the data 1 location this is the country for which the variants information is provided 2 date date for the data entry 3 variant this is the variant corresponding to this data entry 4 num sequences the number of sequences processed for the country variant and date 5 perc sequences the percentage of sequences from the total number of sequences for the country variant and date 6 numsequencestotal total number of sequences for the country variant and date x200b x200b enjoy yourself hopefully the dataset will become useless soon stay safe
r12jwv,1,i want to say this about that these are non ml generated titles that i conceived of in the bath for ml based nonverbal commentaries on the nature of being and time pyroclastic renubilation on the boundaries of the apothecary era reimagined for today s humans an apple in the eye of orange first tries at replacing human genitals with extra hair blood of thorns globular ellipsoids of phat tremor with beef placement of intriguity purple day glo revival spackle captions particle diving boards earlier mentioned just pursued caught and then retracted by our marketing department lark keepsakes that froze intact then woke up when injected into microdotted housefly larval brain incongruities where the sober rush to crazy titles started let s remember gcc on thanksgiving with a nod to history without statues
rb84aa,1,does anyone know how to increase weka s perfomance on linux i use linux for most of my tasks i edit the dataset and do all my readings but i still have to use windows to run weka because of permance issues on linux in linux it takes way longer and consumes way more resources and the heap size is more than enough for the program meaning it s heavy on the cpu does anyone know how to fix it it can get realy annoying to reset the pc everytime i need to boot windows just to run weka
r0zvrb,1,monte carlo simulation for approximation of pi number the number Ï€ is a mathematical constant approximately equal to 3 14159 it is defined in euclidean geometry as the ratio of a circle s circumference to its diameter and also has various equivalent definitions the number appears in many formulas in all areas of mathematics and physics the earliest known use of the greek letter Ï€ to represent the ratio of a circle s circumference to its diameter was by welsh mathematician william jones in 1706 it is also referred to as archimedes s constant the simulation works on the principle of generating random points in the square of the page r 1 based on the distance of the point of the center of the coordinate system it is possible to determine whether the point is inside or outside imaginary quarters of a circle how is a quarter of the surface of a circle frac r 2 pi 4 and area of square r 2 number pi can be approximated as frac k m 4 where k is the number of points within a quarter of a circle and m is the total number of simulated random points as the number of randomly generated points increases it is possible to calculate the number pi with greater precision at the expense of time efficiency
rrd2tt,1,does andrew ng s ml course teach octave hello everyone i m not trying to ask a lazy question but i tried to google this and couldn t find an answer so should i quickly learn octave before starting andrew ng s ml course or does he teach enough of it in the course eventually i plan to move to python anyways thank you
rvzum7,1,how much data do i need to collect before i can create a reasonably accurate model does it really need to be millions of lines hello i m working on a project to take inputs from a number of biometric sensors such as heart rate galvanic skin response and emg in order to try and detect when a user is close to peak sexual arousal orgasm i m not even convinced that i actually need ml for this but it s a good exercise in learning and it s fun as well so i m going to give it a go unsurprisingly i m struggling to find an existing model for this kind of data what a prudish society we live in eh so i m going to have to create my own model my thoughts are to bring the user to orgasm via various means and record all of the metrics every 5 seconds as i go with a boolean for whether the orgasm was reached or not this data will be recorded into a csv file and from there i think that i should be able to create a model i can then use with python and pandas the problem as i see it is that even if i manage to keep the stimulation before orgasm going for 30 minutes that s still only 360 data points and only one of those data points will have true in the orgasm column so i ll probably need to repeat the process dozens of times before i get anything significant as far as this data means you re close to orgasm is concerned i d love to know what folks on here recommend around how to approach this should i just go back to using a load of nested if statements is there a way for me to get the ml to improve on the detection side of things each time it runs based on the data as it flows in do i need a dataset of thousands of rows before i can create my model is this a project that s simply too big for a beginner to bite off i ve got a background in hardware and software engineering so i m not too worried about learning how to write the code itself it s the generation of models that i m struggling with thanks in advance
rdb1uw,0,uttt ai alphazero like solution for playing ultimate tic tac toe in the browser tl dr i developed ai solution mcts nn inspired by alphazero for playing ultimate tic tac toe game in the browser you can try it yourself here why ever since i started working in machine learning 5 years ago i have always wanted to do some cool project for my portfolio reading scientific papers gave me plenty of ideas but only after i read alphazero preprint i knew this is it alphazero is a third paper in alphago alphago zero alphazero muzero sequence in alphazero paper deepmind generalizes previous work so that ai can learn through self play not only how to master go but also chess and shogi i had read previous papers but it was alphazero specifically that sparked my imagination probably because i love simple and elegant engineering solutions and alphazero is mostly about that i discovered ultimate tic tac toe and implemented alphazero in early 2018 after a few weeks of work i realized it s not going to be an easy ride there were two major problems that essentially made me forget about this project for a long time firstly although ultimate tic tac toe uttt looks easier than chess or go it is still quite a challenging game the average length for uttt game is somewhere between 40 and 50 plies the average number of legal actions per position is somewhere around 7 my estimate from self play data it is difficult setup for a side project one of the key factors enabling alphazero success is massive computing power 5000 tpu v1 for self play and 64 tpu v2 for training i had to figure out much cheaper way to develop interestingly good ai under my personal budget secondly when i envisioned deploying alphazero in the browser i had zero knowledge of web development and frontend in general which meant i had to find some time to learn it not easy if you already have a full time job and other stuff going on in your life i decided to put the whole project on hold and said to myself maybe one day there will be better time for this fast forward to 2021 i left my job and decided to spend a year on a career break pursuing my interests i realized that i finally had enough time and resources to conquer this project i learned the basics of web browsers html css javascript and react i bought a desktop pc i ve managed to incrementally redesign alphazero self play training into something more executable on my computer i evaluated ai and confirmed it s superior in comparison to already existing implementations that you can access online other websites and mobile apps i built a react app tested it and finally deployed this week differences from the original alphazero much smaller policy value network architecture designed specifically for playing ultimate tic tac toe in the browser with only 5 million parameters 20 mb source code total separation of self play data generation process from the policy value network training offline rl this was crucial change there is no way i could succeed with a single script that implements online rl and runs for 10 weeks on my desktop this had to be broken down into more manageable stages more mcts simulations per position for training self play data quality over quantity the initial self play dataset was generated from pure mcts simulations random playouts are faster and better than random policy value network predictions search simulations are synchronous single threaded and sequential enabled data augmentation by flipping the board during policy value network training value target for mse loss function is defined as the root s mean state value rather than the game outcome masked kl divergence loss for policy head instead of cross entropy loss auxiliary policy head loss for predicting action values next to action logits there is no external benchmark to compare my solution with so i came up with my own evaluation setup details are on the main selling point is that the final policy value network checkpoint with 1k simulations is much better and faster than mcts with random playouts and 10m simulations 4 order of magnitude difference in other words policy value network learned useful information about ultimate tic tac toe enabling better and faster evaluations i haven t found any other publicly available ai for ultimate tic tac toe that can beat the best version online i found is which implements mcts with random rollouts and some custom modifications source code it keeps up for the first 10 15 moves with uttt ai but eventually does some mistakes and losses the game sometimes there is a draw various technical details and takeways from the project is build using react onnxruntime and deployed as a azure static web app shout out to microsoft for providing great service policy value network is running in the browser on your device using webassembly backend it utilizes only cpu i wanted to use webgl backend which enables gpu access but they don t support convtranspose2d layer yet this or i have to rewrite and retrain policy value network without convtranspose2d to learn web dev frontend i read the entire course twice watched plenty of deved videos and implemented many small throwaway projects my computing hardware for developing this project was intel i7 10700k with 8 cores x 3 80ghz 2 x rtx 2080 ti 64 gb ram the onnx format is great you can load pytorch model in javascript via very easily torch jit nad libtorch are brilliant tools for using pytorch model in c works best on desktop and gaming laptops on my desktop 75 simulations sec on my laptop 70 sims sec on my phone 2 5 sim sec i created a video showing ai self play with 100 000 simulations i think about recording another video with all games from nmcts2 10k vs mcts 10m evaluation to show how mcts is dominated by nmcts2 if you don t know the ultimate tic tac toe game rules my strategy for playing ultimate tic tac toe learned from ai is as follows start in the center square of the center subgame undoubtedly the best move unless you want to surprise the opponent with something weird the o response is to push to the corner subgame so then let the next 8 moves be played in the corner subgames when the o breaks out of the corner subgames jump between the side subgames these are the least useful to take but still one has to be careful not to mess up here maintain the overall balance on the board and wait for the opponent s mistake the game is a marathon not a sprint think twice before sending your opponent to the finished subgame being able to choose any move from the unfinished subgames is very powerful source code twitter thread
rk9s3c,1,is a linear regression model the best for my use case and if so what type and how do i do it ok so i have a data set with 200k lines of data i have a further test data set with around 100k lines with data gathered at a later time the data has three columns which i might expand in future and add a 4th there s the timestamp column the target column and then a column think it s called a feature column in ml talk that holds the data on which the target column is mostly dependent the csv header line is simply datetime usd unit price unit i visualized the data and from the graphs i can see that the price unit values have similar up and down movement as the usd unit values but it is slightly delayed by a few time intervals this is not stock data or some kind of market prediction stuff it s simply an educational data set it is clear though price units are directly linked to usd unit so while time is obviously important here the target column is directly impacted by usd unit i am trying to build a model where i can give it say the last 30 data points of both the usd unit and price unit values and it can predict the point at which the price unit which trails the usd unit in movement will catch up to usd unit i have done some reading and it looks like a linear regression model is what i need but then there are simple and multi something ones and there s all this talk about smoothing weighted averages and stuff i really don t understand i m an architect and devops engineer not a machine learning expert or mathematician i ve looked at examples of people doing this type of analysis and it just goes over my head from what i can gather python is the language of choice but then there seems to be 17 different modeling packages and frameworks and for some reason everyone starts by letting me draw a freaking graph with matplotlib while i already did that in excel it would be great if someone could guide me towards my goal please don t just point me at a linear regression for beginners tutorial i probably already saw that and it either went over my head or it just didn t work if someone is willing to take the time to explain to me what the heck i need or point me at a tutorial example and actually explain in plain simple i m a techie not a mathematician i like linux not linked differential equations oh and english is my second language english i d greatly appreciate that sorry for that i m a bit frustrated partly with the resources out there and mostly with myself for not understanding something that a lot of folks out there say is not that hard i swear i m not that dumb d thanks in advance
rpl389,0,categorical features in image classification i m training a cnn classifier with tensorflow on a large image dataset there are some obvious distinct groups of images in the training set which in a tabular data setting i would have included in training in the form of a categorical variable feature is there a way to do this with image data an obvious workaround would be to train two separate models one per group assuming 2 groups but would that be the only way i may be having more categorical features soon so having a separate model for all their combinations won t be practical i m looking for an algo library which takes both pixels and categorical data as input in training any suggestions
rnios4,1,easy to understand cnn tutorials hello i am trying to learn convolutional neural network and there are nice resources out there i got the basic ideas but some part are unclear for me is there any resource where every specific detail is studied and anyone can learn this topic easily thanks
ql9d4s,0,neurips 2021 accepted paper list list of accepted papers now appears to be public spot any particularly interesting ones
ru66do,1,how to improve performance in multi class hi i m classifying traffic using the cic 2020 darknet data set there are 8 classes in the data set i tried a lot of different ways over sampling under sampling complex sampling and outlier elimination feature selection model change cat boost rus boost decision tree random forest ovo ovr svm etc however the performance doesn t improve in the picture below i only selected features using rfecv and used light gbm this has the best performance x200b validation x200b test is there a good way to improve performance in multi class classification please help me
re1si9,0,phd internships forgive me if there is a better venue for this r cscareers does not appear to be appropriate as it s mostly software engineering discussions question what is the best way to land internships during a phd basically i started getting conference publications precisely when covid hit so i have not been able to attend any conferences to meet people and have zero personal connections and cannot get referrals that way in any case the few times i ve been lucky enough to actually land an interview the feedback was implicitly but overwhelmingly that i had no previous internship experience and was therefore not a top candidate this is especially apparent for the research engineering positions i found but was also true for the research scientist positions as well it appears to be a chicken and egg problem any kind advice context i have several first author publications and several more second author etc at all the big conferences neurips icml and iclr i come from a top european university and my phd lab supervisor is average somewhat well known but definitely far from being the top 10 labs in the world my h index is somewhere in the 5 10 range and my citation count is somewhere in the 200 500 range my research has been in and around the periphery of reinforcement learning but may be a bit niche so there s nothing obvious that an industry lab will be able to immediately plug me into i ve done zero internships so far because i have not been able to land any oh and it is clear that my supervisor is not going to be of any help they are more helpful should i choose to continue within academia which i really do not wish to problem i ve been job internship hunting for a whole year now my expected graduation date is coming up summer which is stressing me out as i have no continuation exit plan whatsoever the debt is piling high my stipend has been well poor and at this point i ve noticed that i m increasingly yearning more for any job at all just to be able to pay my bills than actually thinking about what i wish to work on do research on i fill out applications every day they simply seem to go nowhere again apologies if this is not the correct place for asking this appreciate any and all similar experience knowledge or advice
regt37,1,using amd radeon with tf in anaconda spyder hello i understand that tensorflow is geared towards proprietary nvidia cuda but is there a workaround for amd radeon gpu i m on a macbook pro with an amd radeon 580 external gpu card
rwqq81,1,savitzky golay filter for data denoising smoothing most important part of any data related problems require you to preprocess the data first one such step is data denoising smoothing most people don t do this and many a times high spikes in the dat can overfit the model and give bizzare results one such underrated smoothing technique that can be utilised is savitzky golay filter this low pass filter is mostly used in signal processing as a filter for signal fluctuations this could very well be used in smoothing out the data as well this filter tries to approximate the original function removing the useless fluctuations noise from the data which can very well misguide your model take a look at how it works and it s python implementation here i would love to hear some constructive criticism on my writing and subject thank you
r9hnkd,0,why can methods like resume chronotron and span only train single layer spiking neural networks resume chronotron and span all use stdp like local learning rules to implement their training algorithm though they approach the training differently e g span uses gradient descent via spikes transformed into analogue signals through convolution with alpha kernel whereas resume does not to my understanding the papers i ve read claim that they are not suitable for training multi layer spiking neural networks and can only be used to train single layer networks 0 1 but it is not entirely clear to me why and getting an answer has proven difficult is it because they use local learning rules and propagating weight changes across multiple layers is impossible 0 ponulak filip kasiÅ„ski andrzej 2011 introduction to spiking neural networks information processing learning and applications acta neurobiologiae experimentalis 71 409 33 1 kasabov n k 2018 time space spiking neural networks and brain inspired artificial intelligence springer series on bio and neurosystems 1st springer publishing company incorporated isbn 3662577135
qpi381,0,openai s gpt 3 cases of misusage and failures hello everyone name s alex 26yo from italy and currently studying marketing and a i at iulm university here in milan i grew quite an interest when finally gpt 3 came out and while discussing with one of my professors the topic of my bd thesis came up long story short i m gonna talk about gpt 3 one of the topics i d love to cover is a collection of known applications aka use cases while i found quite a decent number of more or less succesful cases i can t find anything i also tried on google since i only refer to google scholar to find reliable sources but still never managed to find anything so here i am asking if you guys know of any case where a company tried to use gpt 3 in some ways but the whole thing didn t end up quite as they expected thank you all
qoqqp2,0,stl file data annotation first time using reddit anyone know a software i can use to annotate a point cloud stl medical image file i m aware of several softwares for dicom files but none seem to work with stl ideally free software but willing to pay if good
rhdisq,0,any recommendation of books for information theory and statistics i want to know more about f divergence renyi shannon kl wasserstein divergences i am not a math student is there any book or any kind of resources that you recommend for the non math students to get a more general and high level understanding of these metrics divergences
r9yzub,0,in your opinion what areas of deep learning are under explored while many questions still remain unanswered there have been tremendous progress in different areas such the loss landscape optimization architectures etc in your opinion what areas problems are important but haven t received much attention my opinion is that initialization doesn t get the attention it deserves it seems to me most people just accept the standard guassian i i d initialization but i think there is a lot of potential to use other initialization schemes from my own experience using the default initialization schemes pytorch offers sometimes leads the neural net to have a bottleneck where information is not propagated forwards or backwards usually after fiddling with the initialization it works wonderfully so again what in your opinion deserves attention but doesn t get it
r1ba74,0,gans transformer sota compositional generator compositional transformers for scene generation explained 5 minute summary by casual gan papers there have been several attempts to mix together transformers and gans over the last year or so one of the most impressive approaches has to be the gansformer featuring a novel duplex attention mechanism to deal with the high memory requirements typically imposed by image transformers just six months after releasing the original model the authors deliver a solid follow up that builds on the ideas for transformer powered compositional scene generation introduced in the original paper considerably improving the image quality and enabling explicit control over the styles and locations of objects in the composed scene could this model dethrone spade full summary blog post gansformer2 arxiv code subscribe to casual gan papers and follow me on twitter for weekly ai paper summaries
qwsmnn,0,intelâ€™s prune once for all compression method achieves sota compression to accuracy results on bert an intel research team presents prune once for all prune ofa a training method that leverages weight pruning and model distillation to produce pretrained transformer based language models with high sparsity ratios applied to bert the approach achieves state of the art results in compression to accuracy ratio here is a quick read intelâ€™s prune once for all compression method achieves sota compression to accuracy results on bert the paper prune once for all sparse pre trained language models has been accepted for a poster session at neurips 2021 december 6 14 and is on arxiv
rwuhlg,1,how to specify which nodes in the feature map get applied with different filters layers in tensorflow for example say i wanted to apply a 1d convolution to fft and raw time series data in the first layer say the first 400 nodes as an example but use a simple feed forward network to some 1d statistical features on the remaining say 20 nodes x200b i m mostly used to just adding a layer which is able to interact with any node in the previous layer x200b any help is appreciated
rqeg9k,1,the best resource to learn and implement knowledge graph i have facebook groups data and have to create knowledge graphs for the data please recommend me best source to learn about how to implement it
riji40,0,gpu access without limit increases hi folks trying to get access to gpus for some urgent training jobs but looks like most cloud providers require 3 business days for turnarounds are there any alternatives someone can suggest so i could get started with a training job right away
rb29js,0,managing ml experiments as code with git and dvc experiment tracking tools log experiments to a central database and show them in a dashboard this makes it easy to share them with teammates and compare however in an active experimentation phase you may create hundreds of experiments so team members may be overwhelmed and loose the ability to effectively share experiments between team members the following article shows how with dvc tool you can push experiments just like git branches giving you flexibility to share experiment you choose don t just track your ml experiments version them dvc all the experiments you run are stored in your local repo and only the best experiments are promoted to the central repo github for example to share with teammates distributed experiments are shared with the same people as your code repo traditional experiment tracking tools log ml experiments to a central database and show them in a dashboard this makes it easy to share them with teammates and compare however in an active experimentation phase you may create hundreds of experiments so team members may be overwhelmed and loose the ability to effectively share experiments between team members with dvc experiment versioning treats experiments as code it saves all metrics hyperparameters and artifact information in text files that can be versioned by git you do not need a centralized database or online services git becomes a store for experiment meta information and dvc data versioning backs up the artifacts themselves anywhere
qxz09z,0,can embodiment help ml we ve all heard the question if embodiment is necessary some claim symbol grounding occurs when a system has a body and it s therefore needed some claim that movement is the real reason we have brains some claim that it s not it seems to me this view is prevalent in the machine learning community ml practitioners claim that statistics on data alone will get us there i have a novel argument in favor of embodiment it is specifically aimed at the ml statistics community i claim that embodiment allows conducting statistical experiments as opposed to performing observations creating and conducting statistical experiments where one can control the conditions of the experiments dramatically speeds up learning in layman s terms one can hit the corner cases that might never reveal themselves if something is observed without the ability to modify the experiment for example observing a coin on a smooth building lobby floor where people are kicking it once in a while versus an ability to conduct an experiment by flipping a coin what do you think about my argument and what are your arguments for or against embodiment
r5s348,1,given measurement values from 20 air pressure sensors collected over 3 months in a pipe system how to detect the location of broken parts holes in that system first thing that came to mind was outlier detection but as the size of a broken part increases the pressure in the whole system seems to drop and all sensors return more or less lower values the one that breaks drops first though therefore detecting the moment when something started to break worked fine with several outlier detection techniques but the localization did not work at all has anyone got some smart idea of an approach that could work better
rwcehg,1,best way to fuse metadata into a cnn i ve been working on a gan cnn that s producing good results apart from in one area the network takes a photograph as input and produces a stylised output as this is a supervised training model there is a ground truth the network is optimising itself toward the ground truth has different lighting to the input image and i need some way of informing the network of this so far i have tried two approaches the first is to write a column of pixels into the input image where the column s colour contains the needed lighting data the drawback to this approach is it causes artifacting in the output and the network stumbles around for a long time before learning the colour coding the second is to concatenate the input tensor just before the forward passes with an additional channel whose brightness contains the needed lighting data this might be a more appropriate solution although i m concerned it s a waste of convolution filters i would appreciate any insight that can be provided particularly on best practices thanks
r6aja2,1,can someone explain to me in simpler terms what this paragraph means in simpler terms i am not sure if nmf is directly under machine learning but i will try can someone explain to me in simpler terms what this passage means in this paper that i am reading about nmf i did not get how the above formula got transformed into the 2nd formula below
qp9bra,0,project jorldy opensource reinforcement learning framework hello world we are reinforcement learning rl engineers at kakaoenterprise in south korea we published an opensource rl framework and named it jorldy join our reinforcement learning framework for developing yours jorldy is opened for helping rl researchers and students who study rl the features of jorldy are as follows 20 rl algorithms pytorch and various rl environment are provided the algorithms and environments can be run with simple command algorithms and environment can be easily added and customized distributed rl algorithms are provided using ray benchmark of the algorithms is conducted in many rl environment jorldy github link as we mentioned jorldy is an open source rl framework accordingly our team wants to work with many people to develop jorldy into a better framework we would be very grateful if you use it widely and give us a lot of comments about jorldy thank you
qw603s,0,what sort of path does a neural network take through it s parameter space when training does it spend most of it s training getting slowed down from going through saddle points or does it spend more time overshooting the minima it finds going around it in a chaotic orbit how often does it reach a local minima just to be shot out of it by a sample with a large gradient and then roll towards a different local minima are these questions even answerable or have the answers been known for awhile
qyszpw,0,pyconverse conversational text transcript analysis library github project link pyconverse conversation analytics plays an increasingly important role in shaping great customer experiences across various industries like finance contact centres etc primarily to gain a deeper understanding of the customers and to better serve their needs this library pyconverse is an attempt to provide tools methods which can be used to gain an understanding of the conversations from multiple perspectives using various nlp techniques i have been doing what can be called conversational text nlp with primarily contact centre data from various domains like financial services banking insurance etc for the past year or so and i have not come across any interesting open source tools that can help in understanding conversational texts as such i decided to create this library that can provide various tools and methods to analyse calls and help answer important questions compute important metrics that usually people want to find from conversations in contact centre data analysis settings x200b things that can be done with this library 1 emotion identification 2 empathetic statement identification 3 call segmentation 4 topic identification from call segments 5 compute various types of speaker attributes word counts number of words per utterance negations etc identify periods of silence interruptions question identification backchannel identification assess the overall nature of the speaker via linguistic attributes and tell if the speaker is talkative verbally fluent informal personal social goal oriented or forward future looking focused on past identify inhibition please give it a try and share your feedback
r4fjus,0,discussion is there anything similar to google pathways out here currently so as many probably know google is creating pathways which should be a model that can do many things but they haven t released any details or a timeline i m wondering if there are other generalizable learners to use as of now
qrnozu,0,how much vram and ram do i need for nlp transformer models i m a phd student looking for a new desktop because my current personal pc has an amd gpu when i train a pre trained bert model using my cpu which takes forever i assume that it is using ram online i often read about transformer models using vram so my question does training transformer models exclusively use vram or do i also need sufficient ram and how much would be needed minimum to work with such models i am aware of google collab but it is not suitable for my work
rsstqr,0,top arxiv machine learning papers in 2021 according to metacurate io with 2021 almost in the books there are still a couple of hours to go at the time of this writing here are the top machine learning papers per month from the arxiv pre print archive as picked up by metacurate io in 2021 january 1 can a fruit fly learn word embeddings 2 switch transformers scaling to trillion parameter models with simple and efficient sparsity 3 muppet massive multi task representations with pre finetuning february 1 how to represent part whole hierarchies in a neural network 2 patterns predictions and actions a story about machine learning 3 fast graph learning with unique optimal solutions march 1 fast and flexible human program induction in abstract reasoning tasks 2 learning to resize images for computer vision tasks 3 the prevalence of code smells in machine learning projects april 1 retrieval augmentation reduces hallucination in conversation 2 getting to the point index sets and parallelism preserving autodiff for pointful array programming 3 nice an algorithm for nearest instance counterfactual explanations may 1 are pre trained convolutions better than pre trained transformers 2 content disentanglement for semantically consistent synthetic to real domain adaptation 3 klue korean language understanding evaluation june 1 scientific credibility of machine translation research a meta evaluation of 769 papers 2 time aware language models as temporal knowledge bases 3 multiplying matrices without multiplying july 1 deeptitle â€” leveraging bert to generate search engine optimized headlines 2 demystifying neural language modelsâ€™ insensitivity to word order 3 reading race ai recognises patientâ€™s racial identity in medical images august 1 mitigating dataset harms requires stewardship lessons from 1000 papers 2 program synthesis with large language models 3 how to avoid machine learning pitfalls a guide for academic researchers september 1 physics based deep learning 2 finetuned language models are zero shot learners 3 machine learning media bias october 1 learning in high dimension always amounts to extrapolation 2 non deep networks 3 lambeq an efficient high level python library for quantum nlp november 1 gflownet foundations 2 rebooting acgan auxiliary classifier gans with stable training 3 masked autoencoders are scalable vision learners december 1 player of games 2 linear algebra with transformers 3 ernie 3 0 titan exploring larger scale knowledge enhanced pre training for language understanding and generation about metacurate io metacurate io continuously reads a number of sources on ai machine learning nlp and data science it then aggregates the links to stories therein and scores them according to their social score that is the number of shares likes and interactions in social media for the 5 days after theyâ€™ve entered the system metacurate io retrieved 240 000 links in 2021 1 124 of which were links to arxiv papers published last year
r4hhs3,1,looking for study partners for applied predictive modelling hi aspiring data scientists applied predictive modelling is a great book for learning the ml foundations that are used in industry i ve looked into other books such as elements of statistical learning and they don t quite have the application focus as this one it s the top rated book from this r datascience book suggestion thread i m here to help keep you accountable to your learning goals discuss content and exercises and understand the content together we ll be meet up on a discord server and having discussion once a week let me know and i ll pm you a link if your goal is to find a data scientist role you ll be in a great position to find work 3 months from now there s 12 chapters with exercises and the aim is to complete 1 chapter per week if you don t have the book i m happy to hook you up cheers
qxepxd,0,on transferability of prompt tuning for natural language understanding we empirically investigate the transferability of soft prompts across different tasks and models to demonstrate some promising directions worth exploring in prompt arxiv nlp prompt
reaxwn,0,how to train your decision making ais the gradient hi just want to share our latest article on the gradient how to train your decision making ais it is a revision summary of the paper recent advances in leveraging human guidance for sequential decision making tasks written by the lead author of that paper personally i think it s quite interesting hope you enjoy
r84o2r,1,qualitatively speaking training loss seems to decrease strongly during the beginning of training but slows down over time is there a quantitative study or if i m lucky proof of this qualitative observation
rkewa3,0,what are your machine learning superstitions or things you believe that you have zero explanations for here s mine random seed 0 gives bad results random seed 42 good results even valued k in k means insightful segmentation
rpcifl,1,visual ads data set for pinterest visual ad recommendation project hello folks need your help i am currently working on a pinterest visual ad recommendation project knowing people s pins what ads categories of ads we can recommend them i found a nice with 2million visuals data set with users and their pins however i am struggling to find the visual ads category data set i would appreciate any pieces of advice on where to find such datasets or any apis or just ideas on how to approach my search thank you in advance
r3rzms,1,determining loss function for cnn autoencoder i m making an autoencoder to compress audio and using librosa stft s to make them images i chose adadelta optimizer and binary crossentropy loss based on this tutorial by tensorflow but when i train the loss becomes a pretty big negative number somewhere in the 100ks i know something is wrong but i m not sure how to fix it or even why this is happening i included the relevant model code below for more info class autoencoder model def init self super autoencoder self init self encoder tf keras sequential layers input shape 1024 1024 1 layers conv2d 16 3 3 activation relu padding same strides 2 layers conv2d 8 3 3 activation relu padding same strides 2 layers conv2d 4 3 3 activation relu padding same strides 2 self decoder tf keras sequential layers conv2dtranspose 4 kernel size 3 strides 2 activation relu padding same layers conv2dtranspose 8 kernel size 3 strides 2 activation relu padding same layers conv2dtranspose 16 kernel size 3 strides 2 activation relu padding same layers conv2d 1 kernel size 3 3 activation sigmoid padding same def call self x encoded self encoder x decoded self decoder encoded return decoded autoencoder autoencoder autoencoder compile optimizer adadelta loss binary crossentropy
r5k7op,0,nvim dvc neovim plugin for dvc more and more data science and machine learning teams are moving to dvc as framework for data version control as such and as avid neovim user i wrote a neovim plugin that allows to interact with dvc stages files and configurations from within neovim nvim dvc in a nutshell the plugin allows to populate the quickfix window with pipeline stages metrics storage files and the like for quick navigation likewise autocompletion enables to easily reproduce the pipeline stages of interest have a look if you are using dvc and neovim you may find it useful it is in early stage so you may encounter some errors please do point them out if so link to the repository
r1667s,1,is it possible to gather user info just using javascript i changed my major to information security this year and my teacher gave me an assignment that i don t really know how to do we started learning python this year and my assignment is gathering user information from our website like users interests but i m better with javascript so is it possible to do such a task with javascript sorry if my english is bad i live in europe
rde0ti,0,codeparrot ðŸ¦œ train your own copilot from scratch we are releasing codeparrot ðŸ¦œ my first project at hugging face what is it it is a code base to build large language models for code generation from scratch it includes the code to clean up github scale datasets train gpt 2 style models from scratch on distributed infrastructure and an evaluation benchmark of openai s humaneval dataset we are also releasing two trained models and some demos to play with the models this is my first project at hugging face and i thought this is a good opportunity to talk about some of the difficulties behind the scenes of such a project which are usually not so much communicated after its release the topic of the story distributed debugging is hard the first few small experiments training gpt 2 models on a code dataset went well so we decided to scale and train the first smallish model for longer for mysterious reasons the training just stopped after a few hours not with an error but the training loop just didn t continue even more interesting when repeating the experiment it happened again but always at another step and we never observed it in experiments with fewer workers what the hell was going on after literally weeks of debugging and experimenting finally some insight i started to log everything with the maximum verbosity possible interestingly the training stop always coincided with a little debug message concerning some sort of retry it did not always stop when such a message appeared but whenever it stopped such a message was there small details matter we used a feature called streaming in the the datasets library to read the data on the fly it has a retry mechanism when reading the next chunk from a file fails it turns out that when many workers are reading from the same file and one worker fails and tries to read again there is a tiny chance for a deadlock the more workers there are the higher the chance of a deadlock which also explains why we never observed this in smaller experiments the retries could easily be avoided by changing some streaming settings and later we switched to having a single data processing worker for iterable datasets later which resolved the issue altogether and also improved training efficiency by up to 25 my main takeaways i totally underappreciated how hard and stressful the expensive debugging sessions are at scale every experiment is expensive and might take a long time to fail which limits the iteration cycle considerably distributed systems also behave very differently to simple single process programs and it takes some time to adapt the mental model all this make scaling training and models not as simple as it sounds more resources if you are curious about the what it takes to train such a model checkout the blog post for a brief tutorial on training codeparrot ðŸ¦œ also all code is open source free to use and available here if you are interested in more details about the design considerations when setting up a large dataset building efficient tokenizers and architecture choices make sure you have a look at the codeparrot ðŸ¦œ chapter in the upcoming book on transformers and nlp
rrbth9,1,pytorch vs tensorflow in 2022 pytorch tensorflow and both of their ecosystems have been developing so quickly that i thought it was time to take another look at how they stack up against one another i made a write up comparing the two frameworks that i thought might be helpful to those on this sub who are getting started with ml the frameworks have much more overlap from a technical perspective than in previous years so the comparison focuses on practical considerations namely model availability deployment and ecosystems at the end i give recommendations for which framework to use based on your use case like this one for those in industry recommendation flowchart for those in industry there are flowcharts for hobbyists total beginners and those looking for a career change that might be especially relevant to this sub you can jump to the portion that s relevant to you with the above links or read the whole analysis here i d love to hear your thoughts regarding which framework is best for your use case or maybe you ve ditched both frameworks and have started with jax let me know in the comments
rw3nba,1,ml at home hi dudes i would like to know if you do ml at home what do you do if the answer is yes do you compute in the cloud or on your pc
raznry,1,ðŸ’Šyour daily dose of machine learning onnx framework this is a series of posts that i post almost daily i call them â€œyour daily dose of machine learningâ€ last week i posted about different approaches you can use to deploy your tensorflow model in c one of these approaches is onnx runtime this week i want to share more tips and insights about it so onnx open neural network exchange is a framework or a whole ecosystem that allows for the standardization of neural networks hereâ€™s the problem that onnx is trying to solve there are many machine learning frameworks and libraries pytorch tensorflow and scikit learn are famous examples which one should you use well the answer is it depends on so many factors there is no free launch so what onnx aims to do is to make your choice easier basically you can choose whichever framework you want train your model with it and then once youâ€™re happy with your model transform it into onnx format and keep the production code working in onnx only so onnx will help you achieve 2 things keep your options open when it comes to choosing a machine learning library for your training standardize your production code since youâ€™ll only need to maintain one main dependency which is for onnx moreover you can use your onnx models in python c or other languages more on onnx in the upcoming days connect with me on your favorite social network x200b
rfzbr7,0,training large language models all you need is waiting and hope training large language models need so many patient especially when you have few gpus finetune model and at the later period of finetune i am finetuning a 2 6b gpt 2 model after finetuned 10 of datasets the loss curve of continued finetuning has nearly no drop within 12 days it makes me feel doubt fear and uneasiness uncertain how the model will go divergence or no change anymore both will be bad and you will waste much compute time fortunately as long as your hyperparams and datasets were good codes and model has no bug then the slow improvement of loss at the later period of finetune should be ok it will always improve until the end of training update you can see loss curve in below image is keeping drop at the end of 12 days it reminds me of the last sentence of the count of monte cristo all you need is waiting and hope x200b updated in below image the loss before black line is finetuned in 12 days after black line is where the loss start to drop x200b after finetuned 10 37 of datasetsï¼Œloss curve of continued finetuning in 12 days x200b
ru3ba4,1,which nlp algorithm to consider if my dataset is very small 250 500 data points and i want to perform text classification and identify semantic similarity i have already tried distilbert for text classification and the model got overfit 99 train accuracy and 30 test accuracy currently i have 250 data points and the dataset is supposed to increase with time but not much would be in the 1000s i was wondering if there s any other algorithm for text classification which performs better on small datasets
rwc7zy,0,classification with imbalanced datasets question i ve been working on a medical classification project with an imbalanced tabular dataset i have 3 classes and each class has 44 16 and 14 rows of data respectively when i train a random forest classifier i see that my model is only predicting the dominant class for all test instances most of the time how can i get around to this also are there any recommendations you can give me for dealing with imbalanced datasets thank you
qlilnf,0,zillowâ€™s nn based zestimate leads to massive losses in home flipping business zillow announced that they are laying off a quarter of their workforce due to a 420 million loss incurred by zillow offers the home flipping arm of their business the business model was reliant on zestimate a neural network based model that forecasts housing prices this seems like a colossal misstep on their part it begs the question how can other companies avoid a similar fate if they are making large gambles based on machine learning models predicting market movements additionally how much should consumers rely on market predictions like zestimate when making financial decisions speaking as someone who recently bought a home and researched the market on zillow during the process
r5zkt2,1,giving reviews a rating based on existing reviews i have a big project i am working on an issue i ve run into is that some data on my review rating column is missing i would like to make an algorithm that can take the provided reviews and there ratings in my csv file and then make a predictions on those missing ratings my question is is there any simple tutorial about nlp that doesn t involve deep learning algorithms
ri4pqb,0,understanding alphazero neural networkâ€™s superhuman chess ability summary of the paper acquisition of chess knowledge in alphazero as a common and sometimes proven belief deep learning systems seem to learn uninterpretable representations and are far from human understanding recently some studies have highlighted the fact that this may not always be applicable and some networks may be able to learn human readable representations unfortunately this ability could merely come from the fact that these networks are exposed to human generated data so to demonstrate their ability to learn like humans and not that they are simply memorizing human created labels it is necessary to test them without any label following this idea the deepmind and google brain teams together with the 14th world chess champion vladimir kramnik studied their creature alphazero from this point of view alphazero is the descendant of alphago the super neural network that beat the world champion lee sedol in a best of five go match a turning point in the history of deep learning as can also be seen in the wonderful netflix documentary alphago unlike alphago alphazero is trained through self play i e it learns to play competing against itself and masters not only go but also chess and shogi this trait makes alphazero the perfect case study to explore this idea moreover given the fact that it performs at a superhuman level understanding its functionality is also particularly useful for highlighting unknown patterns which have never been discovered by chess theorists full paper summary by leonardo tanzi paper
quv04i,0,paper explained gradients are not all you need full video walkthrough more and more systems are made differentiable which means that accurate gradients of these systems dynamics can be computed exactly while this development has led to a lot of advances there are also distinct situations where backpropagation can be a very bad idea this paper characterizes a few such systems in the domain of iterated dynamical systems often including some source of stochasticity resulting in chaotic behavior in these systems it is often better to use black box estimators for gradients than computing them exactly x200b outline 0 00 foreword 1 15 intro overview 3 40 backpropagation through iterated systems 12 10 connection to the spectrum of the jacobian 15 35 the reparameterization trick 21 30 problems of reparameterization 26 35 example 1 policy learning in simulation 33 05 example 2 meta learning optimizers 36 15 example 3 disk packing 37 45 analysis of jacobians 40 20 what can be done 45 40 just use black box methods x200b paper
ruj3ja,0,iclr 2022 open discussion quality first time submitting to iclr i m wondering how is the open discussion on openreview net really different from the review rebuttal procedure used in other conferences for the papers i m reviewing about half the reviewers reacted to the authors responses clarifications modifications additional experiments etc as to my submission 5568 i run extra experiments and answered each of the concerns directly but received 0 feedback from the reviewers as a reviewer i think it doesn t matter if the rebuttal changes your mind about the quality of the submission but it s very basic manner to reply to the authors responses a simple thank the authors for the responses but i don t think these addressed my concerns would work saying nothing only means you are uncertain if the responses make sense and you just doesn t care to figure it out the authors spent a whole week running experiments to answer some of your questions and if you don t give about their responses just keep the questions with you and don t submit your review i was hoping for a different experience submitting to iclr and then i realized the discussion is basically broken
quddpi,0,open accessarticle detection of bovine mastitis in raw milk using a low cost nir spectrometer and k nn algorithm we have recently published on applied science this paper which some people in the community may find interesting it is an open access therefore no one should have problems to access to it the abstract is among the bovine diseases mastitis causes high economic losses in the dairy production system nowadays detection under field conditions is mainly performed by the california mastitis test which is considered the de facto standard however this method presents with problems of slowness and the expensiveness of the chemical reactive process which is deeply dependent on an expertâ€™s trained eye and consequently is highly imprecise the aim of this work is to propose a new method for bovine mastitis detection under field conditions the proposed method uses a low cost smartphone connected nir spectrometer which solves the aforementioned problems of slowness expert dependency and disposability of the chemical methods this method uses spectra in combination with two k nearest neighbors models the first model is used to detect the presence of mastitis while the second model classifies the positive cases into weak and strong the resulting method was validated by using a leave one out technique where the ground truth was obtained by the california mastitis test the detection model achieved an accuracy of 92 4 while the one classifying the severity showed an accuracy of 95
rplzc6,1,need help on competitions related to autonomous driving i want to build my career as an autonomous driving flight engineer what are some competitions forums opensource projects i can use something that is good for my resume as well
qqvsu0,0,modernizing gov finance data creating infrastructure for fed ai adoption with justin marsico bfs cdo thursday november 18 2021 at 11 30 am et hi r machinelearning i wanted to share an upcoming webinar with you below are the details from the website featured guest speaker justin marsico chief data officer and deputy assistant commissioner at the bureau of the fiscal service during this presentation learn how the us treasury s bureau of the fiscal service is building a better public understanding of federal finance as they continue to modernize their management of data and data sharing justin marsico chief data officer for the fiscal service shares unique opportunities around data at the federal level what it entails to create the infrastructure for ai adoption how they are recruiting data scientists as well as the challenges and opportunities in data governance security ownership and related data areas by offering clear accessible information citizens can see how their taxpayer dollars are spent and learn about the federal budget justin will give a live demonstration of the latest online resources so attendees can learn how the fiscal service is enhancing data education and building and enhancing public trust through greater transparency come join us for this great presentation around areas related to enhancing ai skills in the government workforce recruiting data scientists finding the most appropriate use cases creating the infrastructure for ai adoption and stick around for q a with justin at the end agenda 11 30 12 30pm featured presentation 12 30 13 00pm your q a and interaction link to website
rb2a1f,0,discussion what are the areas in computing not affected by machine learning i m not involved in machine learning science research but it seems to me that it dominates the current mainstream research and development efforts in computing pattern recognition computer vision data compression techniques audio and video synthesis to name a few is ml really that invasive or i m just blind to see further
rx732h,1,how to come up with a novel nlp project i m a third year undergrad starting a 10 week course in deep learning for nlp we are expected to complete an nlp project for our final i really want to do something novel and improve upon state of the art papers if possible i m struggling on coming up with a novel idea topic and would appreciate some pointers or ideas i ve looked through some projects from cs 224n and i m really impressed as all of the topics are novel and actually beat some state of the art results my areas of interest are tts and contrastive learning
r69q9n,0,check out my self made deep rl library written in tf2 x hi i d be delighted to receive some feedback for my deep rl mini library based on tensorflow 2 it is mainly educational when it comes to performance but it contains clear implementations for the classic rl algos dqn and variants ddpg td3 sac policy gradient a2c ppo the lib it is also dockerized mainly for convenience if you beautiful people could give me feedback on how to better present it how it would be useful for actual practical problems to you etc it would be nice
rj5xak,1,can we build a user user recommender with tensorflow recommenders title i want to make a recommendation system that specifically recommends profiles of people to other people similar to how tinder or bumble functions is this possible with tfrs so far iâ€™ve only seen examples of user item interactions and not user user per se please tell me if thereâ€™s any resource i could refer to for the same thank you so much
r6ns57,1,colab pro only p100s i ve recenetly upgraded to colab pro and as the title states above i ve only received p100s over the past week since i upgraded i was wondering if this was common or is it just very busy if it is a common issue i might as well have not upgraded even though i knew getting a v100 or a100 was still not garunteed just wish i recieved it once or i m the just the unluckiest colab user
rfb7c3,0,are there long running forex trading machine learning models or companies basically those that already stood the test of time by having robust models for trading also any thoughts on forex trading and using machine learning for long term usage
qr1pex,0,mel spectrum is it useful for non speech recognition classification tasks i am working on a long term project involving the development of a process monitoring program for an additive manufacturing process i have read a good deal on different techniques used in audio classification speech recognition etc from what i understand most machine learning models involving audio either use features extracted from the time and frequency representations of the signal or spectrograms as inputs when looking into audio feature extraction i have noticed it is very common to extract frequency domain features from the mel spectrum obviously the mel spectrum is very useful in speech recognition tasks since it is intended to align with how a human ear perceives sound my question is the mel spectrum useful for audio classification or anomaly detection tasks that do not involve human speech if so what is the reason it would be more useful than say the standard frequency scale spectrum for a non speech recognition task literature references would be helpful
rnhena,0,dimensionality reduction for geometrical data hi i have a dataset where each data is a set of geometrical 3d data points for example one data could be x1 y1 x2 y2 x3 y3 x4 y4 xn yn x200b i am looking for dimensionality reduction techniques that could fit such a data do you know what techniques i could apply thanks in advance
rl5msp,1,is there any book resource that provides a high level overview of deep learning problems tasks and their applications at a simple level as the title says i was wondering if anyone knows of any kind of book or resource that explains at a high level the different areas of deep learning and their problems and general approaches and applications for example for computer vision graph networks generative learning reinforcement learning something like an introduction for dummies it would preferably be nontechnical so after reading it one would understand roughly what the different kinds of tasks in each field and what applications they have it obviously wouldn t cover the technical details of the algorithms but just their names and a rough intuition and their applications was just wondering if anyone had written such a book before or if anyone came across such a resource that fits the bill cheers
qvh045,0,the risks of relying on inter rater reliability in ml data labeling does anyone else feel like irr is often relied on too heavily for assessing data quality now that so many ml use cases involve highly subjective tasks like content moderation sentiment analysis etc i think we need to reconsider how to think about the relationship between irr and data quality recently wrote a blog on this subject but curious for this community s thoughts on the matter too any examples of instances where an over reliance on irr has caused problems for you down the road
r5q2re,0,google cambridge u alan turing institute propose polyvit a universal transformer for image video and audio classification a research team from google research university of cambridge and alan turing institute proposes polyvit a single transformer model capable of processing multiple modalities and datasets polyvit is parameter efficient and learns representations that generalize across multiple domains here is a quick read google cambridge u alan turing institute propose polyvit a universal transformer for image video and audio classification the paper polyvit co training vision transformers on images videos and audio is on arxiv
rdd5f9,0,automl conf 22 we are excited to announce that after 8 years of automl workshops at icml in 2022 we ll hold the first international conference on automl website amazing speakers anima anandkumar jeff clune chelsea finn timnit gebru julie josse alex smola co located with icml 3 days currently planned in person submission deadline feb 24 2022 great co organizers frank hutter mihaela van der schaar marius lindauer isabelle guyon please spread the word submit your best work and see you all at the conference
r928oo,1,books online courses where to start with ml all i d like to study machine learning where could i start me i have a degree in physics a bachelor s in information technology both mid 1990s so 25 years ago i m a high school mathematics teacher and so do like things mathematical i also enjoy coding c sometimes javascript sometimes r sometimes visual basic or whatever they re all pretty much the same at some level i d consider my math good but currently only to high school level think calculus college algebra level etc beyond that i ve forgotten what i learned like i remember the name fourier series but can t quite remember what they re doing i remember doing well with karnaugh maps but can t really remember from 25 years ago and so on so could any fine folks here recommend what to look for for machine learning and the comp sci mathematics courses i should investigate i mean should i look for a course on linear algebra or some sort of algorithm course or what what what i can then find appropriate books online courses and the like hard to describe more hope i ve provided enough of a push to get the ball rolling thanks muchly
qmqhgg,0,stylegan3 wav2lip x200b due to the limit of my compute the quality suffers a bit
ru6niq,1,are transformers state of the art for every kind of task or do they suck at some tasks as well if so what kind of tasks do they suck at what kind of neural net architectures outperform them in those tasks
qsvzio,0,tsflex flexible and efficient feature extraction for time series are you looking for a time series feature extraction package that is both efficient and flexible tsflex has got you covered they just published a release that allows integration with tsfresh as well go check it out ðŸ‘‰
qqwa9u,0,motivation for a 3d bounding box in adas one of the common test sets for car recognition is called kitti until 2017 they had only a 2d bounding box test set but around that year they moved to a 3d bounding box test sets can anyone explain what is the motivation behind a 3d bounding box for adas and its features
r7f2wb,1,i built a pill identifier using edge impulse i wanted a way to classify images on a phone with or without using the internet so i built a project in edge impulse and used a deep learning cnn image classification model that was deployed to a phone simply stated my mom mixes up her pills when she takes them out of the pill containers and she doesn t always have good internet connection the model is not perfect but i think it shows a useful ml project versus just a cool trendy one that no one will actually use in the real world i wrote up the steps i took here and my edge impulse project is public and available here fyi edge impulse is free for devs
rljk7d,1,joining a project hi i am a cs and ml expert i do projects myself and i would like to join a team would you like to work with me
rvp1xq,1,what actually is the difference between ds ml and deep learning i know python tensorflow libraries such as numpy pandas sklearn matplotlib i have made projects using sklearn models csv datasets and deployed them in flask backend i have created cnn models using keras api made neural networks using keras layers like conv layer dense layer maxpool layer activation functions i have created gan model using tensorflow and keras i have completed andrew ng course on deep learning my question is knowing these things which category do i fall into beginner intermediate in data science or beginner intermediate in machine learning or beginner intermediate in deep learning or something else
qqqzmu,0,advice on buying pc i m doing a phd in ml and have decided i need a better set up computationally currently i m running most scripts either locally on my dell xps 13 no gpu or on colab but evidently both are insufficient as there s a budget in my phd studentship for equipment i m thinking of buying a desktop that i can ssh into from my laptop does anyone have experience with this and if so recommendations e g for gpu cpu specs info area of research not yet set though i will definitely be working with generative models though not huge models only tabular and time series data work often involves data preprocessing steps that might mainly require cpu power no fixed budget though i ll need to justify my purchase it seems buying a moderately good gaming pc with nvidia gpu is financially and computationally interesting any help is appreciated x200b edit budget wise i was thinking below 2000Â£ to be able to justify it to my funding body but if this is too limiting please let me know
r086ev,0,what regularizing method to use with a batch size of 1 besides dropout right now i m dealing with transformers with a good amount of encoder and decode layers as a result due to my limited memory i can only have one batch of the time given that i ve heard dropout is not as popular right now i heard that i should try another method what regularizing method do you recommend that works with a batch size of 1 so batch normalization probably won t work
rbgcu0,1,efficientnet and mobilenet for object detection recently i ve seen a paper for what would seem to be a detection problem would require object localization classification but the researchers say they use mobilenet efficientnet how would this be possible there was no mention of the localization step and it would be in scenarios with mostly background ie not zoomed in enough to skip the localization previously i ve seen software engineers use yolo for detection and then pass the cropped bbox to mobilenet effnet but that s definitely not the case here so how is it possible to use a classification network for a detection problem
r3g7kd,0,looking for a sponsor for a functional programming language for new ai hardware as the 20s proceed novel ai hardware to replace gpus will arrive at the scene but by the looks of things python c seems like it will be a dominant combination for programming them this is a huge pity as we could do better than languages created in the 80s and 70s for programming hardware created in the 2020s i am trying to change this destiny with spiral apart from some special features to control inlining and specialization which make it suitable for hardware with no dynamic memory allocation capability like the gpus spiral is quite similar to languages like f and ocaml it has static typing global type inference first class functions records tuples unions and more as any competent functional language would unlike toy languages it has a well done language server if you want to try out the language it is as simple as installing its plugin in the vs code marketplace about 3 years of full time work went into it i want to make backends for novel ai chips for it but those cost money which i do not have and have restricted availability so i might not be able to get them even if i had the money but various companies will have them and if you are in the position of utilizing them and want something better than old poorly designed languages for that kind of work consider sponsoring spiral at the very least i d need access to those chips in order to make a backend for them it would really be a waste of my skills if i spent the next few years doing other things while spiral languished in the background i believe that had i made it back in the late 00s it would have become the dominant language for programming gpus and writing ml libraries in background see my resume i am a master of functional programming and i ve been researching poker rl agents since early 2021 when spiral was good enough for first release i am quite good at implementing ml papers and have from scratch written a whole gpu based deep learning based library in the past for a previous iteration of spiral making rl agents and a poker game served well to try out the new language and debug it but pretty much all i ve tried failed on full holdem to make it work i d need significantly larger batch sizes which would make training take too long on my gtx 970 so it is a matter of both not having good enough algorithms and not enough compute more broadly than just trying to make an agent for a gambling game the massive parallelism afforded by these chips could be great for game ai in general they could allow for simulating games with a large number of independently acting agents beyond deep learning better ml algorithms of the kind the brain uses would also allow for approximately storing large amounts of procedurally generated world data that would be infeasible on current hardware these algos will get here before long and when they do ml frameworks like pytorch and tensorflow will be obsolete new ones will have to be written in the current time i d like to try out various things on these chips implement the holdem game directly on them and see whether the anticipated 100 1000x speedups are enough to actually enough to make current algorithms tractable for toy games since the current algorithms are trash try to evolve something better with the aid of these chips right now i would not even think of trying this on the gpu but novel hardware might make this kind of brutish research tractable i am too dumb to go beyond backprop directly but i might be able to figure out the principles if i were given the algorithm up front unsupervised learning for making art and music i ve spent 6 5 years doing unpaid work mostly around ml and i d like to create something tangible like a game others could play for my next project i am definitely tired of my old approach of course the above would be done in spiral and to make that work at the very least i d need to create a ref counting c backend for the ai chip and probably a python backend that connects to it this would not be hard for me going into game development is my current path in order to create my assets without the help of ml i am learning to 3d sculpt and draw on my own and will move on to learning musical composition after i am done but doing it all by hand would leave me with much regret and i really should be working on things related to ai trying to do game dev feels too much like a cope for not being able to hack it in rl i still have a lingering attachment to my old path my offer to the community is thus a better way of utilizing future hardware how interested are you in and in supporting this kind of work
r2qf51,0,discussion ratings of 837 iclr 2022 submissions have been changed during open discussion x200b updated data 12 01 2021 x200b the iclr 2022 statistics are available here 466 submissions have been withdrawn during 11 09 to 11 26 submissions from 3328 2862 837 2862 submissions ratings have been changed by the reviewers top submissions where the ratings got lifted top submissions where the ratings got droped unluckily x200b
rmfog1,1,alternative to google colab does anyone know a good alternative option to rendering vqgan clip projects i cant get colab pro in my country so im looking for another option
r61gwu,0,how much do reviewers check proofs in papers for the reviewers out there how much time energy do you spend on following the proof presented by a paper suppose you understand the paper and the idea makes sense in your head then how rigorous are you when it comes to the proof i know there are many different factors e g the reputation of the journal better journals might have more strict rigorous reviewers but i am just curious about a general sense what got me curious is that i found a paper that looks interesting and presents a theorem but i have not been to fully digest the proof even though i ve spent a while i know reviewers probably have a better understanding than me but still
rp7qdv,1,project advice for recommender system that generates useful insights from random datasets hi i am currently designing a recommender system that will generate useful insights from random datasets for my course of study i have two specific problems for which i would like to hear some other people s opinion giving an example to sketch some context let s say i supply a dataset with data of all tests done by students in a class in the dataset it appears that a certain student has a higher average score than the rest of it s class the recommender system could recommend to the end user eg a teacher to plot this insight in a bar chart to make it clear to the end user that this student scores higher than the average this insight could be described as follows the combination of 1 dimension student id with 1 metric score that is aggregated as average take the average for this student the average of the metric of datatype double is significantly higher than the average of the entire column outlier insight type the five properties of the insight i have just described 1 dimension 1 metric metric datatype aggregate as average and outlier could be seen as the features of the insight just like a movie might have a genre year of production this is a simple example but you can go quite far like allowing combination of 2 dimensions with 2 metrics my idea for the system at this moment 1 intake a given dataset 2 generate the recipe for all possible insights limit amount of dimension metric combinations 3 check for all these insights what the insight type is if there is one eg outlier trend reversal you could also quantify these types as an outlier can be x higher than the rest of your column but let s not take this quantification into consideration right now 4 remove spurious insights like 100 correlation low outliers 5 rank the insights that remain with the recommender system there are only a finite amount of insights possible so the insights are all predefined and it should be possible to match insights across datasets my questions in a lot of classic recommender system examples movies are recommended to users i like to compare these movies to my insights and the users to my datasets when trying to find the analogy between my problem at hand and the classic movie user example 1 i do not know if the user dataset analogy is a good one the dataset and it s columns have their own set of features amount of columns amount of dimensions kurtosis of certain metric variance of certain metric and i would like to recommend the same insights for other datasets who have similar values for these feature but this is not what is done in collaborative filtering right normally in the classic example a user is defined by it s ratings on movies not it s inherent features age country of origin 2 a new dataset in my system won t have any ratings for insights so i am looking for a solution where i would use the dataset s distance from other datasets in the dataset space to recommend insights that were useful in other datasets with similar features to my new dataset does anyone have any ideas links tutorials readings to solve this issue i think this is a little more complicated than cold start problem because i would like to always take into account the dataset features not only during the start because of the nature of the system recommendations would also only be done once the first time a user uploads a dataset so there isn t really any way recommendations for a certain dataset would become better over time after a user has selected or rated insights
rkrgq3,1,finding the subject part of a sentence containing multiple words hi i was trying to find out ways to extract the subject from a sentence in python i tried spacy and its nsubj tag sort of works but it associates just a single word to the subject for example if i have the following text three conspiracy theorists walk into a bar then i want the subject to be three conspiracy theorists but spacy tokenization produces the following three nummod conspiracy compound theorists nsubj walk root into prep a det bar pobj how can i achieve what i mentioned is there something that i am missing i am relatively new to nlp so any help would be appreciated thanks
rk4vk0,1,i have many doubts here any hints on how to solve these problems x200b for the 1st problem 1 have some approach but i did something wrong here idk where to take these trigonometric functions can you please tell me where i am wrong and what should be the right thing to do
qr5t70,0,nvidia gtc 2021 check out omnisciâ€™s session at the nvidia gtc 2021 for free learn how bidmc dept of endocrinology is leveraging omnisciâ€™s gpu accelerated analytics platform to explore massive amounts of transcriptomic data and how that has advanced their research processes register here
razoa5,0,why is audio so far behind other ml application domains like image processing and nlp i d like to gain some intuition and actual insight if someone has worked on audio on why we haven t seen major breakthroughs in audio ml tracking and localization as we saw with image processing and nlp ofcourse there is the issue of data and annotating which except for being a laborious task it s also hard to define e g how do you label a conversation of multiple individuals on a dinner table but i would assume that if needed large corporations would attempt such a project it seems to me that either people convert data to images and use image processing methods or use signal processing does that mean signal processing is more lightweight and outperforms a prospective ml approach
rgzppr,0,is arxiv worth it for the academic career i don t know if this topic has already been discussed here if that is the case feel free to delete my thread and point me to the right one i ve just finished my phd and now i am a postdoc researcher i want to pursue the academic career and my department uses scopus metrics in its public competitions i used to upload all my work on arxiv as soon as it was ready and i always received useful feedback from the community my papers were always automatically scraped by paperswithcode and other websites and that gave me nice exposure i even had a paper with 12 citations in the first two months which is a lot for me and received a lot of emails from other researchers asking for clarifications or collaborations in general posting preprints to arxiv has always been a good experience the problem is that by doing that i lost a lot of citations because everybody cited the arxiv version of the articles and not the journal one all these citations do not appear on scopus and do not count from a public competition point of view some colleges suggested me to use researchgate instead of arxiv because with that platform you have more control over the citations what do you think about that have you ever faced the same problem do you have any platform to suggest that gives exposure similar to the one that arxiv gives should i just stop publishing preprints and just wait for the journal conference to publish the articles
r3gz0z,1,making synthetic render of small electronics more realistic using real pictures with dcgan hi this is my first big dive into machine learning and the first real big project i am doing i am stuck at this stage however i have leaded the synthetic dataset and real dataset and the gan is taking in those inputs however no matter how long i run the gan for the final image seems grainy and while the resolution is getting better it is far from a normal shape the synthetic images i have are renders of an object from very different angles and the real images are basically pictures of the irl object from multiple positions taken at fixed angles 30 60 90 etc currently i have just put all real images in a folder and all synthetic images in a folder without worrying about the position and am wondering if that is the problem there are a few things that i can do here but would love some more input from you guys 1 label the real dataset so all the images taken from the same position will be together this however confuses me as it would make my dataset smaller and also i do not exactly know how to use all these positions together since the synthetic dataset positions do not correspond to the real dataset 2 i have been reading about cyclegans and other reddit posts suggest moving to that 3 something other than gans please feel free to ask any questions if need be i am properly stumped here and except apple s simgan paper i have no clue where to go the common celeb face example used in the tutorials is good but all the headshots are from the same angle unlike my electronics
r6ywq0,0,can a prediction interval and confidence index be obtained with a neural network my background is in material science and i have used neural networks with good success to predict material properties however i feel like i am missing two important ingredients 1 the ability to output a prediction interval as opposed to a single value 2 a reliability or confidence level indicating whether the neural net found close neighbors and a simple interpolation led to a high confidence prediction vs the prediction was made in an region with little data where extrapolation was needed low confidence for 1 maybe simply adding an additional output neurone could work one neurone for the mean highest density and one for the variance would this be an acceptable option for 2 i have no idea if algorithms can output such information i would be grateful is someone could share some information here i m looking forward to your feedbacks thank you
qvvd9c,0,benchmarking scaledyolov4 on out of dataset images scaledyolov4 is the go to model for object detection we decided to test how well it does on a dataset different from the one it was trained on we used the citypersons dataset for this experiment it is a subset of the popular cityscapes dataset which only consists of person annotations we found precision and recall values of 0 489 and 0 448 we also found that object detection on this dataset was pretty good even though the classes assigned to them were lacking at times checkout details of the experiment at you can also checkout the notebook we used for this experiment at
r8r78q,1,alternative for isolation forest there is requirement for finding the anomalies in detail we have a two files one from one cloud platform and other from aso we need to find the anomalies for communication between the tenants access to vm sorry to give very vague information i was asked to just give names of algorithm but the currently used algorithm is isolation forest just wanted to know is there any alternative or such or even any other similar algo for finding anomalies for a comparative study
r7vue4,0,where do you guys read your papers looking to read the latest research on ml and don t know where to look
r2rn3d,1,understand tensorflow train output understand batch size parameter hello i noticed that when changing the batch size in train model the status number changes as follows for my dataset batch size 10 epoch 1 10 552 552 9s 11ms step loss 0 3074 val loss 0 0041 batch size 20 epoch 1 10 276 276 6s 12ms step loss 0 3802 val loss 0 0107 x200b so far i thought the train function will use 10 20 from the train sample and do that 10 times but now seeing the numbers 552 and 276 being anti proportional to batch size i think i got that wrong d is it in reality so that the train model slices up the train set into batch size pieces and trains all of them per epoch and the number gets smaller because now it can parallelize better and need less steps to go through them
rsqktc,0,what are the recent breakthroughs for the generative models for art hey everyone i am interested in the recent developments or models in the area of neural style transfer i am also interested in other applications in art not only neural style transfer so feel free to point those as well what are the mostly used models in this area nowadays can you point me to the papers thanks a lot
qyy2ps,0,aesthetics of algorithmic art the idea behind our research is that we have created thousands of images each piece taking its own number of unique parameters to generate with enough votes from the public we can begin to distinguish what if any parameters make a good piece of art and adjust them accordingly we ve put together a website artvote that allows for voting on these thousands of images art is considered to be subjective but we want to find out if that s truly the case we hypothesize that there must be some aspect s to a piece that can be tailored to increase favorability and likeability when seen by large audiences as well as the individual the art you see on the website is broken down into multiple categories and although some of the pieces may look similar each one is unique it was intentionally designed in this way so that it would be easier to evaluate how small changes can impact these scores we imagine that if our hypothesis is correct that there exists a clear path in designing a piece of art or even an image this could have large impacts beyond simply generating art the advertising market would especially benefit as they can serve up more precise ads that are visually appealing to a specific user and increase click through rates
qr0rck,0,how to avoid cpu bottlenecking in pytorch training slowed by augmentations and data loading hello my colleague and i are training models on a few workstations and we are noticing some bottlenecks that are not leveraging all our gpus and stopping us from reaching full performance we are curious what techniques folks use in python pytorch to fully make use of the available cpu cores to keep the gpus saturated data loading or data formatting tricks etc firstly our systems 1 amd 3950 ryzen 128 gb ram 3x 3090 fe m2 ssds for data sets 1 intel i9 10900k 64 gb ram 2x 3090 fe m2 ssds for data sets we notice that both of our systems take the same amount of time per epoch ie we get no gains with 3 gpus vs 2 gpus which is frustrating some things we are observing cpus on both systems spike to 100 cpu on occasion but aren t always utilized disk throughput via iotop shows around 50 55 mb s max read which is way below ssd speeds surprisingly low gpu usage is very spikey here s an image of nvtop and htop for both systems some things we are doing we are using pytorch 1 10 pillow simd and the latest nvidia ngc containers we also use pytorch lighting for training we follow most of the best practices here we are setting to gradient to none instead of zero grad for performance small improvements we are setting cu dnn auto benchmark to true we are using the distributed data parallel accelerator we are using pinned memory we are using num workers 8 we see this behavior of low gpu usage without augmentations we ve reduced batch size as an experiment to see where the issues lie we are at 1 3rd max possible batch size and we see maybe a 10 20 difference in performance some things we have observed we get intermittent crashing if we increase num workers above 8 we ve noticed gpu 0 on our 3 gpu system is sometimes idle which would explain performance differences however its unclear to us why that may be similar to this issue our guess is image loading and pre processing appear to be the issue we aren t entirely sure if we are diagnosing this correctly how are folks getting around issues like these should we be pre processing our data set somehow and storing it in a more optimal format we are relying on pillow simd for image reading decoding and copying to tensors are there any good pragmatic guides to optimizing training thank you
rkla87,0,hyperparameter optimization for noise robustness imagine you propose a novel method that claims to be more robust towards label noise e g in a classification setting in the sense that its generalization performance does not suffer as much as other methods do when facing increasing degrees of noise a common scheme to demonstrate such behavior in an empirical evaluation is to vary the noise degree of the training data and report the resulting test performance of models trained on this data however many studies miss a proper hyperparameter optimization to conduct somewhat fair comparisons or they do not report the way they optimized the parameters in depth often it does not become clear to me whether the validation data think of e g single train val splits employing a cross validation makes it even less obvious is assumed to have the same noise as the training data or being a cleaner representative of the test data distribution what kind of evaluation scheme noisy train and val data clean test vs noisy train clean val and test data would be preferred when
qrvasc,0,handling bound constraints in cma es hi all apologies if the question is a bit naive as i only have passing familiarity with ml full stack developer by trade i m really just looking for more insight into the implementation and consequences of bound constraints with cma es optimization i did come across biedrzycki 2019 which discusses this topic and a lot of the paper s findings at least come across as fairly natural but i m not finding a lot out there and i m just wondering whether anybody else has any recommendations for further reading or personal stories of potential pitfalls in the problem i m dealing with specifically each of the coordinates in my mutant search point vectors need to be bound within the unit interval and i wanted to make sure i wasn t walking into any easily preventable mistakes background context i m currently working on a personal project inspired by this popular geijtenbeek et al 2013 paper where i ve implemented a hill type muscle model and rigged up a humanoid model with some 200 muscles approximating human skeletal muscle function now i m beginning muscle control experiments with inspiration from this paper wochner et al 2020 except i m plugging in cma es instead of the bayesian optimization used in the paper while utilizing the objective function insights
rcuyax,0,is there a paper application where a direct interpretation is given to the hidden state in a rnn as in after an rnn is optimized estimated the resulting hidden state sequence is used further or a direct interpretation is given to it based on the subject where the rnn is being applied to
r4txgv,1,ðŸ’Šyour daily dose of machine learning deploying tensorflow models in c this is a series of posts that i post almost daily i call them â€œyour daily dose of machine learningâ€ in machine learning projects there are several ways to deploy your final model it could be cloud deployment deployment on mobile devices deployment on embedded systems etc for this you can leverage several programming languages depending on the tech stack youâ€™re using in your day job or project in some companies that i worked for we needed to deploy our deep learning models in a c environment these models were mostly dealing with image classification and object detection i remember asking a question on stackoverflow almost 4 years ago on how i can deploy a tensorflow model in a c environment i received only 2 answers in this 4 years period thatâ€™s when i first started looking into the different options of deploying tensorflow models in a c environment iâ€™ve mainly tried 3 options because they seemed the most promising 1 deploy the models using tensorflow c api 2 deploy the models using opencv dnn module 3 deploy the models using onnx runtime based on my experience if you had to choose one of these 3 options then go with onnx runtime itâ€™s a great tool and it offers a lot of flexibility when it comes to models deployment iâ€™ll go a little bit into more details about this in upcoming posts follow me on your favorite social network
rcttt3,0,the carbon footprint of machine learning the energy costs of ai have risen 300 000 fold between 2012 and 2018 and the focus on large language models like gpt 3 will make this worse reducing the carbon footprint has become a critical need for the ai community are huge models the best way forward blog link x200b the outlook for ml training costs source ark investments llc
rxe28v,0,machine learning engineering conferences i was wondering if anyone could recommend and conferences suitable for mleâ€™s maybe with a focus on deploying models iâ€™ve been to traditional data science focused one such as the anaconda one though i wanted to see if any existed that were more closely connected to deployment
rwseha,0,what is the format of full paper presentations in general ml conferences like ijcai and aaai hi all this is my first conference season so i am curious about how do author of full papers not extended abstracts or student track not invited speeches present in conferences like icml aaai and ijcai i mean for instance are presentations performed in a panel with 3 5 presenters using slides or are they all presented as posters where authors stay available for a duration of time for the interested readers to show up and discuss or something else
rn7w3e,1,which os for machine learning tasks which os do you use recommend for machine learning why and which app software package distros do you use for data management and database building is conda the most spread should everybody use it just built a pc with my own gpu and i m running ubuntu with the conda package spyder for code writing and then copy paste to kaggle i wonder what the experts say
rvadz4,0,ðŸ¸yourtts towards zero shot multi speaker tts and zero shot voice conversion for everyone yourtts brings the power of a multilingual approach to the task of zero shot multi speaker tts it is possible to fine tune the model with less than 1 minute of speech and achieve state of the art results in voice similarity and with reasonable quality ðŸ¤– demo ðŸ‘©â€ðŸ’» code ðŸš€ blogpost ðŸ“Ž paper
qt90rt,0,algorithms for correlation of events issues generally any application software co exists with multiple other software where a problem with one software can have a cascading effect on some other software somewhere else in the stack e g 1 you deploy you application as a pod in kubernetes orchestration software 2 but the pod or container is not running after sometime because the ec2 machine or any virtual machine on which the pod was scheduled to run has some issues 3 the ec2 machine or the vm is having some issues because the autoscaling software that is supposed to manage the vms properly is not working correctly 4 the autoscaling software is not working correctly because of some other dependent system not doing this job 5 basically there can be a chain of issues where one issue can have a significant cascading effect on many other dependent software systems you can apply this logic to many other places imagine your application is not working properly because your load balancer is not working properly which is because you load balancer vms are having networking issues because you have some datacenter level failures etc you have all the logs files spread across the stack which reports these issues independently but usually it takes manual effort to correlate them to figure out what is the root cause typically the user sees the symptoms at a high level in the stack e g application is working properly and then they will start debugging and then finally you figure out somewhere down the stack something is wrong this usually takes specific expertise sre and takes time to arrive at the root cause basically what is happening is here is a chain of events with cascading effect there are ways to catch this using monitoring dashboards but the problem is usually those monitoring dashboard are setup in a static way manually setup and typically has a maintenance problems long term i e if there is a change in one of the software version which wants you to change a the dashboard accordingly then you might forget to modify the monitoring dashboard accordingly etc also setting up these monitoring dashboards are very specific to the problem this means you need to setup various different kinds of dashboards for different systems scenarios we want to apply ai to this problem if possible we want to come up with a well trained ai based system which can tell you which is the actual root cause issue if you give it 10 different issues that happened around a specific time interval e g we will have a model that detects various issues anomaly detection based software which we already have so we now have the list of issues that happened across the stack around a certain time interval lets say 10 issues now let s say we have 10 different issues that happens around a particular time interval 1 5 mins we would like to send all the 10 issues to ai based system and we want it to tell that out of these 10 issues 1 or 2 issues is most probably the root cause which could have caused all the other issues if we have such ai based system that will basically correlate multiple issues and tell us the root cause i e basically which of those issues is likely to have caused the other issues that will be very useful is this possible any thoughts guidance will be greatly appreciated what are all the typical algorithms approaches people apply for this kind of problem imagine a use case for this where if we have this systems i can send in a bunch of alerts coming in assume the alert has the relevant data about the issue attached to it and this ai system can process it and segregate those alerts in a such a way it informs the user which alert is actual issue which is causing other this would mean user can quickly resolve it getting rid of other alerts there are many such use cases like this
r12kku,0,federated learning microblog part 2 annotated paper i wrote another twitter thread that goes deep on the math behind federated learning how it is trained and how well it performs twitter thread annotated paper communication efficient learning of deep networks from decentralized data if you like it or have any feedback do let me know
rfmmxx,0,how much would you be willing to pay for a good scientific article recommendation app on monthly basis free trial beforehand personalised view poll
rn5rlo,0,can any article architecture be used commercially i am very curious to know how companies in the ml area work in this sense say for example some neural network architecture can any article architecture be used commercially if i find an implementation on github of that article would it be legal to use it depending on repository license or maybe if i implement it myself
refydd,0,has the ml community outdone itself it seems after gpt and associated models such as dali and clip came out roughly a year ago the machine learning community has gotten a lot quieter in terms of new stuff because now to get the state of the art results you need to outperform these giant and opaque models i don t mean that ml is solved but i can t really think of anything to look forward to because it just seems that these models are too successful at what they are doing
qr9ndf,0,machine learning tutorial in r does anybody know of a good tutorial that would help me do this in r a model to predict the probability of a home run for a given ball in play an explanation of your chosen model features a visualization of your model outputs identify the home run that was least likely to be a home run and the non home run that was most likely to be a home run describe why you think your model classified these plays less accurately than others advice for how to get started model to choose would be helpful as well but i do not want answers here i want to learn so far i have a model that has an overall prediction accuracy of 98 compared to 90 for completely random and a prediction accuracy of 63 when at least one of the predicted or actual result is home run 3 accuracy for random model the models i have been using are knn and random forest
rg86kv,1,can you recommend a website book article or a thesis that used deep learning to forecast climate in next ten years using historical data 50 years ago for example i am new with the term of machine learning and deep learning so beside the source is it possible to develop a model to predict the future climate using the normal laptop ram 4 gb how hard can be for a bigger to use deep learning for forecast future climate thank you
r23jpj,1,generating music on aws questions i just want to upload mp3 files and receive mp3 outputs i m looking for something different from aws s deepcomposer which creates a song output based on your input of a melody the difference is that i want to upload a dataset of many songs while deepcomposer seems to be a keyboard where you design a melody of a single song can you point me in the right direction of guides or tools or recommend ai ml models i have 550 files total size is 12 5 gb aws is a requirement to use the credits
r8mlwd,0,how to implement an efficient softmax cuda kernel all ops computed in deep learning frameworks are translated into cuda kernel functions on the gpu and softmax operations are no exception softmax is a widely used op in most networks and the efficiency of its cuda kernel implementation can affect the final training speed of many networks so how can an efficient softmax cuda kernel be implemented article code this article will introduce techniques for optimizing the softmax cuda kernel in oneflow and experimentally compare it with the softmax operation in cudnn the results show that oneflowâ€™s deeply optimized softmax can utilize the memory bandwidth close to the theoretical upper limit much higher than the cudnn implementation
r7ki6k,1,how to fix multioutput target data is not supported with label binarization error hello i m currently trying to figure out how to use scikit learn and when i try to use an mlpclassifier it spits out this error multioutput target data is not supported with label binarization i m not really sure what it means and the people who have asked about it on stackoverflow seem to get it for different reasons my x train is an ndarray where all the columns are float values except the id column which are simply integers my y train is also an ndarray with integers in both columns id class the classes are from ranges 1 to 10 i would really appreciate any pointers about this
qt1pnz,0,can a giou loss generalized intersection over union be used after an stn module spatial transformer network i have a model that uses an stn module for number detection and mean squared error loss but i would like to replace it for giou because mse doesn t take into account how much of the target area has been detected only how close individual coordinates are close to the target but i wonder if this makes sense has anyone tried it or has some insight
r5l11k,1,is there any ai ml model to use for predicting the side effects of previous taken decisions hello just to let you know i am totally newbie in the ai ml field i am a full stack php developer with knowledge in javascript html css etc i was wondering if there s any ai ml model that is capable to predict what will be the side effect based on previous taken actions for example let s say i have plant that i used to water it every day this is the normal process but if i forget one day to water it the flower leafs are turning down if another day i water the plant twice then the leafs are getting yellow in another day i water them but by mistake in the water there was petrol inside as well my wife water it with chlorine and finally the plant is getting die due to the petrol as you see according to some actions i take the plant has some side effect like remains healthy the leafs turning down the leafs getting yellow the plan is die so i was wandering if there s anything that can make link of the side effect and the actions taken previously like if you had watered the plant with water petrol and chlorine the plant died if you had water the plant too many times the leafs became yellow if you don t have anything in mind what it could be a possible scenario for creating such a model thank you in advance
qu2de6,0,need stochastic environments hello all after extensive search i am unable to find any good stochastic environments to train my algorithms on i only found some toy text based ones since my algorithm is a variant of dqn i wanted environment with huge state space but a discrete action space any help would be really appreciated thanks
ro1s0p,1,what is the minimal requirement to classify a neural network for instance if im classifying oranges could i use just one neural network image or do i need thousands
qrpwsn,0,research wav dataset for morse code does anyone know where to obtain a dataset containing morse code wav files i checked kaggle there was a competition once but the data is no longer available research
r2mi5a,0,research optimizing a kernel matrix i m experimenting with the kernel perceptron with the polynomial kernel and was wondering if there was a way to optimize the kernel matrix representation of the training set given an ordered training set x composed of n vectors from a high dimensional real space the kernel matrix m has size nxn the cell at index ij contains the result of k xi xj where k is the polynomial kernel and xi xj are the training examples at index i and j respectively a couple of observations m is a gram matrix which means that is positive semidefinite and because it is computed from real values it is real itself m is accessed only line by line due to the structure of the perceptron the total size of m increases with the square of the amount of training examples the access to the matrix is sequential therefore no amount of memoization will suffice storing the matrix in memory becomes very expensive very quickly with a training set of 60000 examples m occupies 28 8gb with 64bit floats given the simple structure of this matrix and accessing method i think is unlikely that there is no way to compute quickly a row without storing everything in memory the space saving solution would be to compute each line when needed but this is not very efficient because the algorithm is executed for a different number of epochs and with the execution may be involved some hyperparameter tuning in choosing the right kernel parameters this solution impacts very much the performance and also memory in cheaper than time a way to reduce the effective size of the kernel matrix would be to store only half of the matrix because it is symmetric and this technically works in practice is almost as efficient as not storing the matrix at all because computing a line can be done with vector oriented operations on the cpu gpu where composing the line from existing data is a strictly sequential operation and generally doesn t work that well in the end is not a good middle ground because you save half of the space but the growth rate is unchanged but the computation time for each line is almost the same as not storing the matrix
r74c38,0,extract and label data from wikipedia with dataqa hi all i recently added a new feature to dataqa to be able to extract entities from wikipedia all you need to do is upload a file with wikipedia urls example file with the wikipedia urls of unicorn companies only the url column is needed the tool then extracts and parses all wikipedia content including tables it divides the content into smaller paragraphs and sections that are easier to label simple upload and labelling of wiki tables the output file is a csv file with the text extracted from wikipedia and all the manual and automated rules based labels this file can then be used to train a custom ner model for wikipedia data or to simply have as a dataset for other use cases example output csv feel free to come talk to me on the comment section i would love to engage with people
rpbgpz,1,a simple post for beginners in machine learning and scikit learn prepare data for machine learning using scikit learn prepare data for machine learning using scikit learn x200b
rdhncs,1,how to deal with categorical features and which feature selection should i use i am starting on a project the target is categorical only 1 and 0 half of my features are categorical all categoricals are nominals there is no ordinals but some of nominal columns are given as integers such as 1 2 3 4 i will be using classifications or trees to predict the target can i use ordinal encoder because i have heard it works for tree models however i dont think those numbers would make much sense for feature selection or should i stick with one hot encoder but the problem is i originally have only 17 features i would end up with 70 features after one hot encoding then how do i select the features after the some features are broken into pieces especially the month feature it would break into 12 different columns which type of feature selection should i use can i also use logistic regression to rank them also for numerical features should i scale first or do feature selection first i am using python by the way
qwwjdv,0,any research groups interested in my work deep learning for combinatorial optimization i published a paper in a peer reviewed journal titled neural knapsack a neural solver for the knapsack problem article now i want to do my phd in the same topic in europe or canada but can not find any research group or a supervisor that would be interested i did other projects have quite good knowledge in deep learning and applied fields as computer vision can any one help me
r5ynbw,1,tsne back to the original feature space hello assume that i have a dataset whose number of features is 256 i apply tsne on this dataset for 2 components then i have 2 dimensions right now after i perform some operations on that 2 dimensional space i come up with a data point what i want to do is to expand this point to the original feature size which is 256 i meant the below flow 256 tsne 2 some magic 256 i am looking for that magic actually is there any way to do so
rr34rv,1,will freecodecamp math curriculums be enough to self teach math for start in ml are anyone was self teach math for start reading books on ml what do you think about freecodecamp math curriculums on their youtube channel i already googled on what math should i know to start learning ml linearalgebra no bullshit guide to linear algebra looks good for it calculus 1 2 limits differential and integral calculus statistics and probability i don t have a big lapses of school algebra and introduction to calculus i also reviewed some parts of which i was not sure i remember with khanacademy on the one side i understand that math is the same for whole world but is there big difference with accent to my purposes between learning math with fcc curriculums and prof in university or college i understand that all research works at google deepmind and others like this are reserved for masters and phd i am interested in machine learning as a tool for solving applied problems clasification recommended systems spam filters etc i have a grasp of common data structures and algorithms and some experience with leetcode shunting yard algorithm for example ps warmest wishes for the holiday season
rpa2pm,0,did tesla create it s own ml models from scratch or did they start by using another company s services i couldn t find an answer when googling so thought i d ask the brains trust musk doesn t seem like the kind of person to want to outsource this kind of task his ownership of openai would have also helped to build the initial models that being said many large firms have bought ml companies and integrated them rather than build them from scratch for example apple bought siri from nuance communications furthermore are mercedes and other automobile companies who hope to join the self driving future all building their own models or outsourcing this massive task to outside firms from a democratic position it makes sense to ensure no one company monopolizes navigation but from an optimisation standpoint it absolutely does i would have thought google would use waymo to create the operating system of the new age car it would have made perfect sense with their maps android and search services
qq2rbm,0,iclr 2022 reviews share your rants
r1uz2q,1,rtx 3080ti 12gb or quadro m6000 24gb for deep learning
rwju9t,0,minimum corpus size for word embedding extraction dear all i have a smallish 100mb corpus of historical text in a non mainstream language on which i want to apply word embedding is that enough shall i only consider frequent words how frequent does it help if i do some preprocessing such as stemming etc how do i choose the parameters especially the embedding dimensionality any libraries recommended are there some language agnostic unsupervised ways to evaluate the embeddings x200b thanks
ruofql,0,gui based machine learning applications i was previously using azure machine learning studio classic and of course it was discontinued last month any other free ml applications the new azure machine learning studio isn t free and this is a school project so i m aiming for free and simple any suggestions or maybe someone else is using studio classic and knows a way around this
r4v2i1,0,software engineering in mle maturity assessment soo this maybe such a basic question but apologies in advance i ve started getting interested in ml and recently got a chance to work on a side project helping a manager create a framework for assessing mle maturity in an organisation some of the criteria i was told to look at was software engineering having read several articles on ml ops i m finding it hard to isolate which parts of the process would be considered software engineering are there any articles or videos you d recommend that could explain how software engineering ties into ml
rkwwja,0,blog post about the birth and the evolution of the image augmentation library albumentations i was always curious how open source projects are born and evolve i had a chance to participate in the creation and development of the open source library albumentations the library is probably known to people who train neural networks for computer vision tasks for the sake of logging i wrote a blog post about the library how it was born how it evolved and how we promoted it it is long and probably full of unnecessary details but i wanted to cover as much detail as possible to make it useful to myself and to others who work in open source the library was downloaded 2 6 million times is widely used in machine learning competitions and even has adoption in academia the largest user of the library is china
rqycwh,0,which pretrained models are useful for getting low level image data there are quite a few popular and open source models used for getting image embedding which represents high level semantic meaning i would like to know if there are any models which can help encode low level image details like style of an image eg pixel art sepia vaporwave patterns prominent shapes color distributions etc i am aware some of these properties can be easily extracted using algorithms like fourier transform laplacian filters etc but if would be interesting to see if they can be extracted from a learned model
ra3kwq,0,looking for ocr datasets for benchmark hi everyone i am currently working on a complete benchmark of cloud ocr engines gcp aws azure ocr space etc to carry out this work i am looking for datasets where differences in performance could appear between the engines i already looked on kaggle and other public dataset available perhaps some of you might know some good datasets for my project thanks jeremy
qviuar,0,post undergrad paths for machine learning i m an econ major graduating in the spring from a top 40 uni just found out i really like machine learning and ai i m not too tech savvy or particularly good at math but i can get by i m a really fast learner currently learning python econometrics has probably been my favorite class where can i go from here i graduate so soon should i be looking at grad programs in finance data science etc or should i just try to find a job immediately any particular tips on how to break into the finance world with machine learning what kinds of firms should i be looking at for jobs would really appreciate any advice or tips
ru4tmp,1,predicting multiple targets multi class text classification hi i am currently working on project which requires me to predict category and subcategories and if it is organic product or not based on the ingredients list brand and description of the product can anyone tell me how to do this more explanation there are 6 columns a b c d e f i need to predict d e f based on a b c i want to predict 3 of the columns in one go thanks in advance
r9rcka,1,made a visualization for cost complexity pruning of decision trees i made an animation to visualize cost complexity pruning of decision trees hope it can help someone appreciate all feedback on style code interface etc
rg61dp,1,best machine learning resources for beginners i am planning on studying machine learning and i want some of the best resources courses books etc to get started andrew ng s ml course on coursera is an exception
qlq3rt,0,raspberry pi and ml frameworks any benchmarks for training or inference hey folks do you know any benchmarks of available ml frameworks tf pytorch for training toy data data sets and inference on a raspberry pi or other edge devices
rgxiav,0,state of the art online deep reinforcement learning algorithm for continuous action spaces hi what s the current state of the art online deep reinforcement learning algorithm for continuous action spaces i m looking particularly for one that has an open source implementation that i can download from github also does anyone know if the muesli algorithm muesli combining improvements in policy optimization by hessel et al 2021 has an open source implementation yet thank you
qlvjdy,0,using perplexity for evaluating language models hello assume a training dataset has ten sentences and they are used for constructing a probabilistic language model consisting of the maximum likelihood estimates for all of the different combinations of bigrams for these ten sentences to test the accuracy of this bigram language model a test dataset consisting of twenty sentences are used for calculating the probability for each of these test sentences using the bigram conditional probabilities that were calculated using all of the sentences in the training set to evaluate the accuracy of this bigram language model there is a need to calculate the perplexity measure for indicating the accuracy of the probabilistic language model because of the test dataset having twenty sentences is it correct to calculate the perplexity value for each of these twenty sentences using their probability values and then taking the average of these twenty perplexity values for calculating the overall perplexity value for the test dataset or is it correct to calculate the probability for each of the twenty sentences and then find the simple average of these twenty probability values to give a simple probability average which is then used for calculating the perplexity value for the test dataset would this procedure apply for a test dataset having any number of sentences would this procedure apply for a trigram model would this procedure apply for any n gram models like a quadrogram model the goal is to always have a probabilistic language model having the lowest perplexity value thanks
rv4u2w,0,paper that mathematically proves that gradient descent can achieve zero training error i think this is a well known paper but i have not been able to find it i am interested in the paper that mathematically proves neural nets can fit any set of datapoints so far what i have found mostly is papers that show empirically that or something related to that like this one i d appreciate any help edit u the new scientist shared this paper which is what i was looking for also i apologize for my vague description now that the paper is shown i hope it is more clear to future readers what kind of results i meant but in case that is not case i was wondering about this question under what conditions can a neural network achieve zero training error and in particular i am interested in papers with mathematical even without empirical results
r3fd4m,1,specifics on random forest decision tree prediction step greetings all my first post on this subreddit i m a phd that wants to use ml for various tasks for my future career so i figured i would dig into various ml algorithms and code them up that is to say i might be posting more in the future so i m grateful this page exists currently i am trying to code the algorithm for random forests after completing the decision tree algorithm and i am having trouble wrapping my head around the ensemble prediction step so as far as i understand we take some data create multiple subsets of that data and then we create decision trees from that data simple enough but then we are supposed to make predictions based on the decision tree model created my question is this how do we do the prediction step what data are we supposed to test the individual decision trees my impression is that we make one prediction for each decision tree but my naÃ¯ve understanding is making me think this is not the proper way to build the ensemble model can anyone give me a thorough explanation of how we aggregate the decision tree outputs thanks in advance
ru2pcb,1,local explainability using shap after feature engineering i am using shap to explain model predictions for a particular instance and was wondering how to interpret them when feature engineering is applied to data i can think of three scenarios 1 no features are dropped or added in this case shap would simply give a value for each feature indicating how a particular feature affected the result 2 some features were dropped shap would again output a value for each features but does it make sense to say that features that were dropped did not affect the model output sounds silly but not sure if it statistically correct to use shap in this case 3 new features were generated from existing features lets say you generate a bunch of features from original ones while retaining the original original a b c new a b c ab bc abc thanks in advance
r9latm,0,thoughts on hands on machine learning with scikit learn keras tensorflow by geron i m in the early portion of my data science masters but i m wanting to learn as much as i can so i purchased the book in the title i ve noticed that some of the code as i ve progressed through chapters 2 and into 3 don t exactly work as the book expects them to is this a common problem throughout the book for instance in chapter 3 where it goes into sdg classifiers i kept getting errors for the code used on pages 86 and 88 i was able to figure out how to correct it but kind of perplexing to me how the book has these errors
qzsrtq,0,what to do when an area chair posts a new review of your paper 6 hours before the author rebuttal deadline genuinely open to advice feedback on this weird situation if the area chair over your submitted conference paper posts a new review of your paper two weeks after the initial reviews were released and 6 hours before the deadline for you as the author to make any new changes to your pdf what would you do contact the program chair about the oddity of the circumstances the situation is complicated even more by the fact that two of the original reviewers of my paper marginally voting to accept 6 s have not yet responded to my rebuttals to their initial reviews i believe i have addressed all three original reviewers main concerns questions and reviewer 2 even raised their score by a few points in response to my rebuttal bringing it to a borderline reject 5 with one small suggestion to raise their score so before this new review was released i was anticipating positive responses from the original reviewers however now that a new review has been posted last second i am a bit concerned that the scope and trajectory of the discussion forum will be diverted and diluted as a result of my new reviewer s opinions on how the paper should instead be structured which notably are quite divergent from all three original reviewers opinions any thoughts on this scenario for instance would you consider it ethical to release a paper review less than 24 hours before the author has to post a formal rebuttal in response
rknwid,1,skewed imbalanced feature as i learned imblearn smote is for imbalanced label class es my dataset has a skewed categorical feature column pandas how can a balance this for numerical data box cox is a way but for my cat feature column i need evently counts n from each subgroup
r51g2v,1,github googlecloudplatform mlops with vertex ai an end to end example of mlops on google cloud using tensorflow tfx and vertex ai check out this repository by google cloud i find it very useful an end to end example of ðŒð‹ðŽð©ð¬ ð¨ð§ ð†ð¨ð¨ð ð¥ðž ð‚ð¥ð¨ð®ð using ð“ðžð§ð¬ð¨ð«ð…ð¥ð¨ð° ð“ð…ð— ðšð§ð ð•ðžð«ð­ðžð± ð€ðˆ 1 performing exploratory data analysis on the data in ðð¢ð ðð®ðžð«ð² 2 creating vertex ai dataset resource using the python sdk 3 generating the schema for the raw data using ð“ðžð§ð¬ð¨ð«ð…ð¥ð¨ð° ðƒðšð­ðš ð•ðšð¥ð¢ððšð­ð¢ð¨ð§ 4 preparing the data using ðƒðšð­ðšðŸð¥ð¨ð° 5 implementing a ðŠðžð«ðšð¬ ðœð¥ðšð¬ð¬ð¢ðŸð¢ðœðšð­ð¢ð¨ð§ ð¦ð¨ððžð¥ 6 training the keras model with vertex ai using a pre built container 7 upload the exported model from ð‚ð¥ð¨ð®ð ð’ð­ð¨ð«ðšð ðž ð­ð¨ ð•ðžð«ð­ðžð± ð€ðˆ 8 extract and visualize experiment parameters from vertex ai metadata 9 use vertex ai for ð¡ð²ð©ðžð«ð©ðšð«ðšð¦ðžð­ðžð« ð­ð®ð§ð¢ð§ð  10 clone the repository to the built environment 11 run unit tests 12 run a local ðžðŸðž ð­ðžð¬ð­ ð¨ðŸ ð­ð¡ðž ð“ð…ð— ð©ð¢ð©ðžð¥ð¢ð§ðž 13 build the ðŒð‹ ðœð¨ð§ð­ðšð¢ð§ðžð« ð¢ð¦ðšð ðž for pipeline steps 14 ð‚ð¨ð¦ð©ð¢ð¥ðž the pipeline 15 upload the pipeline to cloud storage 16 creating a ð‚ð¥ð¨ð®ð ðð®ð› ð’ð®ð› ð­ð¨ð©ð¢ðœ 17 deploying a ð‚ð¥ð¨ð®ð ð…ð®ð§ðœð­ð¢ð¨ð§ 18 triggering the pipeline 19 receive hyper parameters using ð¡ð²ð©ðžð«ð©ðšð«ðšð¦ ð ðžð§ custom python component 20 extract data from bigquery using ðð¢ð ðð®ðžð«ð²ð„ð±ðšð¦ð©ð¥ðžð†ðžð§ component 21 validate the raw data using ð’ð­ðšð­ð¢ð¬ð­ð¢ðœð¬ð†ðžð§ ðšð§ð ð„ð±ðšð¦ð©ð¥ðžð•ðšð¥ð¢ððšð­ð¨ð« ðœð¨ð¦ð©ð¨ð§ðžð§ð­ 22 process the data using on ðƒðšð­ðšðŸð¥ð¨ð° ð“ð«ðšð§ð¬ðŸð¨ð«ð¦ ðœð¨ð¦ð©ð¨ð§ðžð§ð­ 23 train a custom model with vertex ai using the ð“ð«ðšð¢ð§ðžð« ðœð¨ð¦ð©ð¨ð§ðžð§ð­ 24 evaluate and validate the custom model using ðŒð¨ððžð¥ð„ð¯ðšð¥ð®ðšð­ð¨ð« ðœð¨ð¦ð©ð¨ð§ðžð§ð­ 25 save the blessed to model registry location in cloud storage using ðð®ð¬ð¡ðžð« ðœð¨ð¦ð©ð¨ð§ðžð§ð­ 26 upload the model to vertex ai using ð¯ðžð«ð­ðžð± ð¦ð¨ððžð¥ ð©ð®ð¬ð¡ðžð« custom python component 27 model deployment 28 test model interface 29 create an ðžð§ðð©ð¨ð¢ð§ð­ ð¢ð§ ð•ðžð«ð­ðžð± ð€ðˆ 30 deploy the model to the endpoint 31 test the vertex ai endpoint 32 use the vertex ai endpoint for online prediction 33 use the vertex ai uploaded model for batch prediction 34 run the batch prediction using ð•ðžð«ð­ðžð± ðð¢ð©ðžð¥ð¢ð§ðžð¬ 35 set ð¬ð¤ðžð° ðšð§ð ðð«ð¢ðŸð­ ð­ð¡ð«ðžð¬ð¡ð¨ð¥ð 36 create a monitoring job for all the models under and endpoint 37 list the monitoring jobs 38 list artifacts produced by monitoring job 39 pause and delete the monitoring job 40 ðŒðžð­ðšððšð­ðš ð­ð«ðšðœð¤ð¢ð§ð  in gcp console
rmws0h,0,a mathematical framework for transformer circuits link to paper x200b in this paper we attempt to take initial very preliminary steps towards reverse engineering transformers given the incredible complexity and size of modern language models we have found it most fruitful to start with the simplest possible models and work our way up from there our aim is to discover simple algorithmic patterns motifs or frameworks that can subsequently be applied to larger and more complex models specifically in this paper we will study transformers with two layers or less which have only attention blocks â€“ this is in contrast to a large modern transformer like gpt 3 which has 96 layers and alternates attention blocks with mlp blocks
rssbiv,1,right hardware decision for mobile observation robot being outside i m not sure if the question suits to this subreddit but i generally wanted to build a observation mars rover be like robot which should be able to do object detection by a csi port nightvision camera which i already got but that shouldnt be all it should also include the opportunity to expand its knowledge on the go so i dont have to train its model at home on my laptop i already heard of the intel neural compute stick 2 which could be used with the raspberry pi 4 with 8gb ram which i also have for now my question is would the neural compute stick be enough for that mobile model training purpose or should i get some other hardware
rm1u3i,1,linear programming small example lp is very powerful subject here i have created one optimisation example with gurobipy which could easy illustrate the power of lp we got some new teachers which should be assigned to the nearest school but we must align the teacher salary expectation with our budget we have only one constraint this is an original my example i have enjoyed doing it please star if you like it for my motivation best regards and happy holidays
qsh3z0,0,best favorite format for writing without a conference in mind usually i have ideas do some experiments draft a manuscript etc and then afterwards edit it into a format for a particular conference or journal but i m wondering among the community what s your preferred format for writing something if you don t know where you re sending it yet if something s just on arxiv what format is visually the easiest to read i kinda like the jmlr format pretty plain pretty basic but i can appreciate that it wastes a lot of space on the author list the neurips seems like the default definitive format but i find the bars around the title kind of ugly the ieee formats always present visually as being just completely fucking impenetrable not a fan of the iclr format putting all the titles in caps
r6llo5,0,how do scalable gaussian processes compare to neural nets most discussions i ve seen on this topic don t discuss scalable or deep gps in any depth and on the other side don t discuss nns with uncertainty estimates so it s difficult to draw a solid comparison references would be excellent where possible
r5a7i5,1,how to separate overlapping clusters in kmeans hello i am trying to understand how i might best approach a clustering data science problem that i have encountered where clusters overlap i will explain with a conceptual example i am conducting a study where i am trying to detect how much time is spent using different water consuming devices for lack of a better word in households such as a bath tub or a sink etc i have water consumption readers on each water using device reporting water usage levels in this case gallons minute at chronological minute intervals and so each row indicates a minute s worth of water use at each indicated gallon s minute rate i am doing this across 10 households each with its own id a dataframe of this looks like so gpm gallons per minute id gpm 0 1 43 1 1 23 2 1 54 3 1 33 134 2 44 135 2 52 136 2 63 137 2 33 245 3 23 246 3 12 247 3 15 248 3 51 356 4 22 357 4 61 358 4 54 359 4 84 as shown each household id has at each reading an attributed water consumption value given how different water using devices will have different typical water consumption levels the water consumption values in the data will likely cluster where each cluster represents a different water using device each of these households has 5 of these studied water using devices and so i am using 5 clusters the goal is to create 5 unique ranges similar to bins that i could simply drop the values of any new dataset into so that i can find the number of minute observations that fall within each water using device s range to therefore say oh this new household spent x minutes over the week using the sink and x minutes over the week using the shower etc now while i could simply just run k means on the entire dataset itself ignoring id but this would not account for the consideration that each of these households could have different models and sizes of sinks bath tubs and showers etc meaning slightly different typical water consumption values thus distorting the clusters that is why i want to run clustering on each sub dataset grouped by id so the clustering is true to that household and so i would run kmeans on the gpm values attributed to id 1 then run the next kmeans on the value values attributed to id 2 and then run the next kmeans on the value values attributed to id 3 and so on in this example lets say i have 25 unique ids with corresponding gpm values what i want to do is create all encompassing summary cluster ranges that capture all of these individual kmeans runs specifically by this i mean that for each id s kmeans run i want to produce a min and max value for each of the 5 clusters and then from that i want to produce a single range for each cluster that captures all of the values in the whole dataset this means that i want to find the minimum min value and the maximum max value for each cluster across all of the ids from this i would make the simple summary dataframe min max 0 1 2 3 4 and so this would create a cluster 1 range that would capture all of the cluster 1 values across all ids and create a cluster 2 range that would capture all of the cluster 2 values across all ids and create a cluster 3 range that would capture all of the cluster 3 values across all ids etc the big problem is that these ranges in my summary dataframe overlap in theory it seemed like my idea would work that these final summary ranges would capture all of the values in the dataset correctly but it appears there is just to much shift between how the values cluster for each id i am having a hard time trying to find what to properly label this problem as but would someone perhaps know of any ways i could address this issue so that i can produce more accurate summary cluster ranges thanks please accept my apologies if my problem was not clear i would be happy to go back and explain better
rv4nt6,1,nlp hybridization of statistical approach and expert system hi everyone i have a question for you for context we aggregate on a platform the various ai apis on the market gcp azure etc and including nlp apis keyword extraction sentiment analysis ner etc the idea is that a developer doesn t have to create accounts with different providers and can have them all on one api to test compare and change whenever he wants however many customers ask us how to mix the statistical approach behind these apis with expert systems and how to achieve hybridization do you have any idea how to do this thanks
rhd64w,0,jupyterlab alternative with enhanced academic writing capabilities hi folks i am looking for self hosted or online options which combine jupyter notebooks and capabilities for academic writing bibtex export to latex etc i found curvenote but wondering if there are other options out there thanks rh
rf7fxo,1,questions regarding cnn lstm model it is my understanding that video data must be converted to individual frames and then converted to some sort of array data before passing it through a cnn what i donâ€™t understand is how to do that last step of converting it to array data could someone please assist
rnke9u,1,what are parametric and non parametric statistics a 2 minute visual guide x200b ðŸ”µ parametric and non parametric statistics ðŸ”µ ðŸ§‘â€ðŸ’» if you have written code before you have heard of a parameter it is what a function takes as input to do some computation on and return an output similarly probability distributions have parameters these define the properties of the distribution ðŸ”” in the case of a normal distribution the mean and standard deviation are its parameters the mean controls the position of the distribution while the standard deviation controls the spread or peakiness of the bell curve â›·ï¸ in the parametric approach the model consists of a finite set of parameters that characterize it the big assumption that a parametric model makes is that the model will do well on the task if the underlying distribution from which your data is sampled matches the model if that is not the case the model and data mismatch will hurt the performance on the task you care about ðŸ¤¸ a non parametric model does not rely on parametric assumptions and is generally more flexible it is a good choice if you don t have any prior knowledge about what could be a good model that reflects your data distribution a histogram is a good example that can model points sampled from an arbitrary distribution ðŸ§ however there is no free lunch you will need enough data points to get a good approximation otherwise the histogram will look nothing like the underlying distribution i have been studying and practicing machine learning and computer vision for 7 years as time has passed i have realized more and more the power of data driven decision making seeing firsthand what ml is capable of i have personally felt that it can be a great inter disciplinary tool to automate workflows i will bring up different topics of ml in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike the posts will cover topics like statistics linear algebra probability data representation modeling computer vision among other things i want this to be an incremental journey starting from the basics and building up to more complex ideas if you like such content and would like to steer the topics i cover feel free to suggest topics you would like to know more about in the comments
rkqfjc,0,how shopify applies ml for anomaly detection and forecasting at scale with dr ella hilal head of data science engineering revenue and growth at shopify at enterprise data ai jan 6 at 11 30 am hi r machinelearning i wanted to share a webinar coming up in january 2022 at enterprise data ai i put the info from the website below along with the link if you re interested i m interested in hearing how they built their anomaly detection engine and how that has been performing featured guest speaker dr ella hilal head of data science engineering revenue and growth at shopify at shopify we have over 1 7 million merchants across over 175 countries with hundreds of millions of consumers shopping at their stores weâ€™re focused on leveraging the scale of our data to not only empower shopify but to create new experiences for our merchants that are impossible without data in our daily operations at shopify we are highly data informed some of the ways weâ€™re leveraging advanced analytics is by building an anomaly detection engine that allows us to process over hundreds of thousands of metric segment combinations in a very accessible way in her talk dr hilal will share key tips on how you can apply machine learning for anomaly detection and forecasting at scale agenda 11 30 12 30pm featured presentation 12 30 13 00pm your q a and interaction link to website
rpo9he,1,creating an optimization algorithm for cost function for nn is possible to find an article or an example of a new optimization algorithm for cost function for nn
r3ldxf,1,embedding a ml model into a microcontroller to enlarge the functionality of a thermal station hi guys i recently built a sensor station that monitors thermal comfort in indoor spaces the monitoring station has 3 sensors and 1 microcontroller of the type esp32 now i would like to enlarge the functionality of this station with ml but since i am a beginner i am kind of confused what to do here any suggestions how to go about it thanks in advance
rwxuxp,1,converting categorical variable city to one hot vectors is giving me nan values i am currently working on a basic machine learning project which is revolved around predicting house prices given several different features about the house the only categorical variable i am using in my linear model is the city the listing belongs to but when i convert the cities to one hot vectors i keep getting nan values and my linear regression model is throwing an error does anyone know what is happening i have attached my code below i have 12 numeric variables in my linear model and the categorical city variable from the last screenshot you can seethe nan values occur at the third data point and for all of the columns that are a part of the one hot vector does anyone know what is happening x200b x200b code of convert city to one hot vector method one using get dummies in pandas housing pd get dummies housing drop first true housing head t x200b method two using onehotencoder from sklearn performing label encoder lab enc preprocessing labelencoder housing city encoded lab enc fit transform housing city x200b confirming label encoding housing city encoded value counts x200b dropping the city variable housing drop city axis 1 inplace true x200b one hot encoding one hot encode onehotencoder one hot encode df pd dataframe one hot encode fit transform housing city encoded toarray x200b x200b merging one hot encode df to the main housing dataframe housing housing join one hot encode df housing head x200b dropping city encoded variable housing drop city encoded axis 1 inplace true x200b x200b nan values x200b
r73zke,1,metalearning conditional weights i ve been reading this paper on class weighting and domain adaptation with my reading club and there s an issue that s still a bit unclear to us or at least to me it seems like the paper updates the model and sample conditional weights alternatingly but looking at algorithm 1 i m not sure how the epsilon values are being updated line 13 are they being backpropagated through the updated model theta or what
r9bwjd,1,getting better at nn i m a recent cs grad and took several classes that discussed neural nets and also an internship i now got a job where one of my tasks is implementing and testing new nn architectures for a task that wasn t solved with ml before i know building an nn is a lof of trial and error but i was hoping for guidance on how to continue learning are there any books or online courses that could help me become better at building new nns how to choose an architecture hyperparameters tricks like batch norm etc or just expose myself to new ideas and options to try out
rqt6w6,1,google colab s tpus are training my fashion mnist model about 5 6 times slower than on cpu is there a reason for this and is there a way to make it run faster any and all help is appreciated thanks solved x200b colab notebooks cpu tpu x200b update thanks to some very nice users who were kind enough to help me i managed to diagnose the problem the reason why the model was training slower was because i was not passing enough data through at once this is easily fixed by changing the model fit function from model fit train images train labels epochs 10 to model fit train images train labels epochs 10 steps per epoch 60 in my scenario thanks to all the people in the comments who helped
r7ch51,1,while true learn game free on epic games i haven t played it and i don t know how much you ll learn but while true learn is a machine learning game and it s the free game this week on epicgames com
rv7yos,0,how to measure accuracy of knn imputation i have a dataset in which the the best way to impute missing values is to use knn but before i go ahead and do that i d like to check what kind of accuracy i have with that form of imputation in this specific dataset and which k should be used my original solution was as follows 1 from my original dataset remove all rows with missing values 2 from this dataset impute nans randomly throughout the dataset with the same frequency that they were missing originally and store the values that were replaced with nan in a new dataset as the ground truth 3 impute using knn 4 check the accuracy of the imputed values against the ground truth values stored in step 2 using mae for different k values is there an easier way to do this if not should i be using mae or another accuracy score
replf5,1,how can i set the weight of basic and simple 2 layer neural network this is a very basic question for this sub but i canâ€™t find the answer by myself i remember that when i studied neural network more than 15 years ago there was a very basic math formula that made me clearly understand the basic of neural networks i believe itâ€™s a simple exercise useful to teach the very basic of how a node works i remember i was able to get a pencil and a piece of paper and give an example of a simple working 2 layers neural network i helped many people to understand the very basic of what at that time was an alien concept the learning propose of this exercise is why i ask in this sub i was able to create a 2 layers nn with 4 5 nodes for each layer the network was able to recognise two different patterns example 1 1 1 0â€“0 0 vs 0 1 0 1 0 1 and the second layer was able to recreate that pattern even if the input of the first layer had some mistake example input 1 1 1 1 0 0 output 1 1 1 0â€“0 0 i forgot everything and i need your help how did i calculate the weight between the layers i remember it was a very basic math formula for each of the weight
ru7drf,0,quick deploy optimize convert and deploy machine learning models hello reddit releasing one of my oss projects quick deploy github blog post x200b it s in the very early stage feel free to contribute or give a star ðŸ™‚
rbq0oq,1,how to organize machine learning infrastructure let s say you want to start using machine learning in your company how would you go about organising machine learning infrastructure for example i was thinking between 1 having one big service in which you have all the machine learning code and models nlp churn prediction user clustering etc maybe a django service with multiple apps and each app has it s own endpoint you can call for predictions 2 have multiple rest apis each with it s own model you can call for predictions please motivate your answer pros cons for each approach if possible and also you can share resources links books etc
r5rn66,0,how to handle your advisor hi i know the title might sound offensive but i mean how to deal with your advisor i am a ph d student in machine learning and for the last year i couldn t find a thesis topic because whenever i go to my advisor he responds that we need to wait for the data which is supposed to come from a company i started looking for different projects and datasets online to work on my advisor doesn t spend time reading many papers or keeping himself updated about the field so i thought i would find a way for myself i noticed that if i bring some ideas to him he takes them to his master s students and involves them which is not a bad thing but the problem comes when he asks them to lead it although they are not some big novel ideas but still for me that is a way to progress in my research and develop a strong profile recently i shared an idea with him and he just called some other students to work on it and i was completely removed from the scene i want to hear from other ph d students about how you handle these situations of course i don t want to have troubles with my advisor and talking to him will not help he seems to be too aggressive at times i want some suggestions on how you guys manage these type of situations thanks
r6cobf,1,ðŸ’Šyour daily dose of machine learning tensorflow c api this is a series of posts that i post almost daily i call them â€œyour daily dose of machine learningâ€ last time i shared with you how i used opencv to deploy tensorflow models in a c environment today i want to share with you my experience deploying deep learning models using tensorflow c api tensorflow is built using c and it offers an api to make it relatively easier to deploy models and even train models if you wish to in c this sounds great until you start trying to implement a program that uses this api as a developer you know that the documentation is very important but tensorflowâ€™s c api is very very very limited itâ€™s very hard to find the information youâ€™re looking for just by reading the documentation secondly there is no support for windows 32bits so if you have such a system then youâ€™re facing a wall here and itâ€™s better to try looking for other options but the great thing about this api is that you donâ€™t have to worry about the compatibility of your model that you trained in python with the c api especially if youâ€™re using the same version of tensorflow in python and c iâ€™ve personally deployed image classification models and object detection models using this api and apart from the limiting factors that i mention above the models worked exactly as expected the last option that i personally tried when it comes to deploying tensorflow models in c is onnx runtime more about this in an upcoming post follow me on your favorite social network
rw7go4,1,iterating model fit fast forward to a model you think is worth using thereâ€™s always a little bit of randomness involved in the training of a model right youâ€™ve tuned on the validation set and youâ€™re ready to try the model on the test set is it bad practice to iterate the fit process like 1000 times store the model with the least loss and then deploy that against the test set or is this just asking for overfitting to corrupt your model assume the time required to perform the of iteration of n loops of the training process is reasonable
qplveb,0,maritime grand challenge abu dhabi came across this maritime grand challenge that i thought others might find interesting it combines drones robotics and ai and thereâ€™s a 2m first prize and 3m overall in prize money open to universities research institutions companies and individual innovators hereâ€™s a link and a video the competition is to further development of real world solutions to illegal fishing piracy smuggling and coastline security first deadline initial phase includes white papers and registration â€“ is jan 31 2022 i found out about it through this story
qq5hrq,0,community sourced open audio datasets â€“ hacktoberfest 2021 hey r ml a month ago i posted here about dagshub supporting hacktoberfest for ml datasets we wanted to do something that was geared towards the ml community and we decided to create an open source catalog of ðŸ”Š audio datasets the response has been truly amazing we received 40 dataset contributions which are now publicly available and viewable on dagshub they cover various tasks languages and sizes and you can use them all for your projects if you want to check out the list of datasets i can t wait to see what everyone builds with these a huge thank you to everyone who participated you are what made this possible the fact that hacktoberfest is over doesn t mean you can t continue contributing we d love to see more datasets both in the audio domain and others
rvr3lk,1,what does this funtion do np c i am confused as np c np array 1 2 3 np array 4 5 6 and np c np array 1 2 3 0 0 np array 4 5 6 gives different results
rrk8j1,1,i am interested in mentoring 1 person whoâ€™s motivated getting into ds i work as a data scientist in ai company and also finishing my msc ds studies
ri2cpj,0,from thousands to billions an overview of methods for scaling graph neural networks graph neural networks gnns have become very popular in recent years early approaches easily handled small graphs with a few thousand nodes but scaled poorly to large graphs with millions of nodes several approaches have been proposed for scaling gnns to large graphs we recently published an overview of a few core approaches for gnn scalability read on scalable graph representation learning with graph neural networks we welcome constructive feedback cheers
rk77ho,0,how to build a binary classification model on an imbalanced dataset that performs better than a naive model which always predict the majority class i m working on building a binary classification model on an imbalanced 95 majority class 0 dataset the model will ultimately only be evaluated on a similarly imbalanced test set for which the only goal is highest overall accuracy even though i understand one typically shouldn t use accuracy for evaluation on an imbalanced dataset my best models so far never predict the minority class 1 giving me 95 accuracy but i m lost as to how i can make this higher given that these models are only performing as well as a naive model which just predicts 0 all of the time should my training set include more of the minority class e g do smote up down sampling even if the model will only ultimately be evaluated on a similarly skewed test set should i be using a different model currently logistic regression and random forest work best should my model instead output probabilities for which i can set a threshold myself to make predictions any resources on this is there a way in python to change the loss function to punish the model for getting the minority class wrong
rsn858,0,drop your best open source deep learning related project would anyone like to share their open source projects in deep learning federated learning blockchain deep learning distributed job or parallel scheduling based on pytorch or tensoflow to look at
r66avr,1,really noob question is there a way to look through the clip database manually i just think it d be neat to see all the images used to train clip
qsdnc2,0,invitation to participate in study the labour of ethical ai this is an invitation to participate in a study titled â€œthe labour of ethical aiâ€ my name is james steinhoff and i am a postdoctoral fellow at the institute of communication culture information and technology at the university of toronto mississauga the aim of this research is to understand the labour that goes into ethical ai research research intended to promote ethical responsible democratic human centered non profit or socially beneficial ai the organizations in which such work is conducted the problems facing workers in this field and how the field is connected to industry academia and government the goal of the study is to interview people who work study or intern in ethical ai in order to gain empirical insight into the working conditions in this sector if you have experience working in the sector you could provide great insight into some of the challenges and opportunities facing workers if you are willing your participation would involve meeting me for an online interview that will last approximately one hour where i will ask you questions about your working conditions why you chose the ethical ai sector and the promises and problems facing that sector participation in this project will be confidential i will supply potential participants with an informed consent form that outlines in greater detail the project and the parameters of participation i am happy to answer any questions or concerns you might have thank you for your time and consideration sincerely james steinhoff phd j steinhoff utoronto ca postdoctoral fellow institute of communication culture information and technology cct building 3071 3359 mississauga rd mississauga on l5l 1c6
rie3i9,0,how to collect speech datasets for voice enabled technology some of the good examples of speech data collection technology alexa and siri
rqi2zq,1,quickly train classification model for text classification i want to use a ml for text classification for my app i have different sentences which i want to classify by there content not sentiment with gpt 3 api i just need to provide 100 sentences there clarification for each classification done downside cost money to use the model on there api i looked into deploy own model on firebase which is pretty easy is training your own model much harder then the descriped process on a gpt 3 model would appreciate any tips or helpful links i dont want to learn everything from scratch justy quickly train and deploy the model
r8uwp1,1,nlp should you preprocess your text before running your deep learning model is it recommended to preprocess the text before training by preprocessing i mean things like lemmatization stemming etc intuitively i think the first answer is no as any preprocessing might remove some information from the original data or it can introduce its own noise would the answer be the same for a kaggle competition where the processing time and recourses are limited x200b thanks
r81wi5,1,where can i find a good overview on which ai techiques to use on which kind of data and how to combine different techniques into pipelines i have taken some ai courses in my university and while i was able to grasp the concepts i am struggeling to make choices on what to use when especially when it comes to creating my own pipelines this lack of orientation really gets in the way as i don t know what goes well together and what should rather be avoided i realize there often is no easy solution to a specific use case and you always need to fiddle around but some kind of overview on where to start and what to try first with certain kinds of data would be a great help i am not asking for a blueprint but merely for a rough orientation to tackle new projects
r2gm3v,1,can i separate out the steps of learn in stable baselines3 i m working on a project where two agents train simultaneously but each agent only sometimes needs to make a decision is it possible to have code that follows roughly the following structure model a2c mlppolicy env verbose 1 learning rate 0 0005 obs env reset for i in range 2000000 action states model predict obs obs rewards dones info env step action model update from experience obs action reward does this type of function exist i m also not married to stable baselines to if there s a way to do this in another library that would also be greatly appreciated thanks
r7ejz8,1,machine learning maths good evening i am very new to machine learning and i have a question which for some reason is non trivial to me i know that we are not likely not operate with the data which consists of small amount of observarions and a great amount of variables however in such cases as recommendation systems computer vision and e t c we just cannot avoid it so how do the ml models face with the case described above and why some models are better than others in these terms thank you
rqvrwx,0,x mlps highly configurable all mlp architecture built on jax and haiku i wanted to share a new project i developed x mlps this library provides a flexible all mlp architectural foundation so you can quickly implement mix and match and test various sota mlp blocks and architectures a key pattern used throughout x mlps is the factory function enabling arbitrary blocks to be created and stacked in a network it s built on jax and haiku the initial release implements several mlp blocks you can use out of the box including resmlp mlp mixer gmlp and s 2 mlp i plan to implement several more as well along with some qol improvements however my primary goal with x mlps is for anyone to be able to create their own blocks and rapidly experiment with different approaches code is pretty well documented so there should be enough info in the repo to learn how to extend it let me know if you have any questions repo tiny attention is not yet implemented for gmlp
rvv2j4,1,moving from keras python to mlpacl c hey guys i have been doing my first steps with keras and implemented a nn for a new usecase i now want to translate it into mlpack c while the definitions seem quitedifferent to me i am struggling with this maybe someone could answer me the following questions 1 when i use the ffn class is model add linear equivalent to keras dense layer i e fully connected 2 is it viable to replace dense m activation relu with model add relulayer m 3 is there an equivalent to a lambda layer thanks in advance guys
qseien,0,a dilemma of an ml guy in industry i m a machine learning engineer in the healthcare sector and i ve been thinking about this a lot with the rapid as fuck advancements in research on ai do i keep up with research more or should i focus on learning about engineering the solutions pipelines etc example reading a paper using a new gan vs reading about a case study
qyxxcs,0,simple questions thread please post your questions here instead of creating a new thread encourage others who create new posts for questions to post here instead thread will stay alive until next one so keep posting after the date in the title thanks to everyone for answering questions in the previous thread
qm2ov5,0,project discover ongoing ml ai competitions looking for feature suggestions for mlcontests com it s been two years since i posted here about my then new project ml contests it was well received and since it s been a while i thought i d post an update and ask for feedback main page mlcontests com the main page just lists ongoing competitions there s also a newsletter where i occasionally send out updates about the competitive ml space and a separate page which compares cloud gpus for ml you can visit the site at mlcontests com traffic is steady the newsletter is growing there are no ads and i d like to figure out where to take it next i d love to hear your thoughts on what you want from the site ps if you want to contribute it s all open source
rin63t,0,best tool for drawing u net style diagrams i m struggling to find a good tool for drawing u net style architectures the likes of plotneuralnet and nn svg seem like excellent tools but they don t seem to lend themselves to drawing u nets correct me if i m wrong though since i m otherwise familiar with both i don t need anything particularly fancy just want to do something similar to the standard u net diagram i linked above
rq4wp4,0,has anyone got autokeras working on any sort of scale i have played around with autokeras over the last while and while it does exactly what it says on the tin in regards to small scale single gpu configurations when i scale up to multi gpu setups the performance benefit just isnâ€™t there the documentation says thereâ€™s ostensibly support for multi gpu setups via the mirrored training strategy but that doesnâ€™t seem to increase performance but rather decrease it
r4eumq,1,unable to download casia irisv4 dataset hello everyone i have a project to train my model on the iris dataset but i have a problem downloading casia iris v4 to download it seem we must sign in but i get the error the link could someone guide me on how to download it thanks
qwwf82,0,what happen with the neural turing machine differentiable computer line of work recently i saw some renewed interest in algorithmic reasoning by petar velickovic essentially augment traditional discrete algorithms with continuous pattern recognition of dl and it reminds me of the neural turing machine differentiable computer mostly spearheaded by alex graves which i believe share the same motivation with the algorithmic reasoning approach i haven t heard much about any major new work since the neural differential computer and was wondering why i was and still am fascinated by the idea and this research but since i am not working on the topic i m not sure where are the challenges and pitfall i was aware of some instability in implementation but i thought the open source code would have help there anybody has insights on why this direction has not been explored more in recent years or is this one of those schmidhubered s idea that too ahead of its time and once people have squeezed out all the internal memory capacity from transformer this idea of external dynamic memory would bounce back
r7otcv,0,methods for measuring label consistency within object detection i have a relatively small custom dataset for multi class object detection i can get a maximum of 0 48 map my data is labeled by one person who practices the field in which my dataset is related xrays but i would want to measure the label consistency and correctness of the annotations are there methods to do this
rgyhjb,1,filtering time series hi i am a beginner and doing my first ml project with python and tensorflow in which i have a lot of time series i am wondering if and how i am supposed to filter them case no 1 sometimes there is noise in data for ex speed is oscillating quickly between 5 ans 20 m s for no clear reason except sensor sensitivity or wind gusts and i honestly canâ€™t say what is supposed to be the right answer case no 2 other times the time series is pretty stable but has some impulse nan or 0 value iâ€™m a bit rusted with signal processing but i tried to add a lowpass filter and itâ€™s ok when oscillations are around a clear value but itâ€™s not enough in case no 1 and my lowpass filter is not great with impulse i know i can use another filter for this impulse problem if only i take time to implement it so thatâ€™ s ok but i also wonder should i really filter data i could add a column np diff myvariable for example and the model would see the impulse is wrong when it comes to the wind sensor the mean std deviation skewness etc values could be great to know the error or sensitivity what do you think about that is this believing in magic or some kind of overfitting should i really filter the time series before i feed the ml model thanks for helping
rpneke,1,any ml study group or meet up working in projects research papers hello i am an indian student who has done be in electronics from bits after my 1 year stint as sde i am doing mtech in computer science at iit hyderabad i have done 2 3 official courses in machine learning besides the infamous coursera course and deep learning specialization currently i am working in an official project which is 75 computer networks and 25 machine learning however my current interest is towards ml i would love if there is a study group that is working in ml based projects or research papers where i could contribute or even study group that primarily discusses ml based concepts thank you
rr6eb8,1,can there be a set of questions that can give an end user an idea if machine learning can be implemented in his business case i m looking to build ml as a service in my org and needed to understand if any use case validation questionnaire which answered can give us an idea if implementation of ml in this case is valid
rcg62g,1,having difficulty understanding pix 2 pix s discriminator i m trying to understand the pix 2 pix gan which is used for image translation paper here the discriminator looks at 70 by 70 pixel patches and evaluates whether they are real or fake it takes both the conditional image and the synthesised image as an input according to the tensorflow tutorial s implementation these 2 images are concatenated together my question is if the discriminator is only looking at 70x70 patches at a time how does it understand how the 2 images relate and check that the conditional input has actually informed the image that has been generated any help greatly appreciated
rucns3,1,using a genetic algorithm for a minimax fitness function i recreated a 2 player turn based deterministic board game like chess checkers etc and i thought it d be fun to try and create a simple ai that plays that game i began reading online on the matter and what seemed to be fitting for my use case is to use a minimax algorithm and inspired by some videos i ve seen on genetic algorithms and additional reading i thought it d be a great idea to use a genetic algorithm to create a good evaluation function to my surprise i had a hard time finding examples of genetic algorithms being used for generating a fitness function to use in a minimax algorithm also the more i thought about it the less it made sense how do you even relate a chromosome a board state and a single number output
r8tsv6,0,discussion why are einstein sum notations not popular in ml they changed my life i recently discovered torch einsum and now i am mad at every friend mentor acquaintance for not telling me about it they are just way more intuitive and can handle most operations that i would want to do with tensors so elegantly no more of having to remember which way is axis 0 no more of having to remember which way is dim 1 and no more of remembering so many numpy and torch functions only to misuse np unsqueeze and torch expand dims it takes only 30 mins or so to learn the notation and become somewhat proficient but then you are sorted for life what are the arguments for and against using einstein notations for everything will i be writing code which others find difficult to understand kindly pitch in your thoughts and theories on why are they so seldom used when they are one size fit all
rxbg0z,1,fun way of creating gpt j chatbot found a cool article explaining how to create a gpt j chatbot in a couple of steps
rgadbt,1,svm model interactive visualisation in jupyter notebook code in this blog you will learn what is svm and how to visualise svm interactively in jupyter notebook
rlpls7,1,where to start on ml hey guys i m an undergrad informations technology student about to go to my final semester in university last semester i had to submit my final year project and the topic i chose was using machine learning to analyze procurement data in my proposal defense i figured i would do something with spend analytics of procurement data x200b but last semester i was balancing university with a full time customer service job so i didn t have the time to study much on the topic and i m fairly new to machine learning as a whole where would be the best place for me to start and what path should i follow also if anyone has experience with procurement data it would be great if they can advice if spend analytics is really the way to go or is there something cooler out there it would be really cool if you can tell me where can i source procurement data from i searched on kaggle and i couldn t find any helpful data
qllc70,0,research towards the generalization of contrastive self supervised learning some interesting results of contrastive learning theory 1 the generalization ability of contrastive self supervised learning depends on 3 factors strength of data augmentations alignment of positive samples and divergence of class centers 2 data augmentation enables self supervised learning good algorithms which optimize the alignment and divergence factors well with weak data augmentation still have bad performance 3 barlow twins which aims to decorrelate the different vector components of the representation implicitly optimizes the geometry of embedding space to satisfy the alignment and divergence factors actually
r1rghr,1,programming the gradient descent from the scratch this is the most fundamental and essential part for understanding the machine learning here i made for the purposes of my learning functions and explanations for programming the gradient descent from the scratch purposely i made it for linear regression when i understood it on linear regression it was no pain to fully implement it on any kind of neural networks give a star for my further motivation if you like it
r75y47,1,ðŸ’Šyour daily dose of machine learning onnx runtime for deep learning in c this is a series of posts that i post almost daily i call them â€œyour daily dose of machine learningâ€ this week i shared with you my experience deploying tensorflow models using c i shared with you how we used opencv dnn module and tensorflow c api today i want to share with you my experience deploying tensorflow models in a c environment using onnx runtime onnx stands for open neural networks exchange and itâ€™s a whole ecosystem that aims to standardize the representation of machine learning models itâ€™s developed by microsoft what onnx aims to do is make it easier to deploy any kind of machine learning model coming from any type of ml framework including tensorflow to deploy tensorflow models using onnx in c you need to do 2 things convert your tensorflow model to onnx format there is an open source tool for this called tf2onnx use onnx runtime to deploy your converted model iâ€™ve personally tested this approach on so many deep learning models and it works great for example i converted almost all of the models that are in the tensorflow object detection api into onnx format and i was able to run inference with them with no problem i fell in love with this tool after it was suggested to me by a friend and after seeing all of its capabilities in future posts i might go into more details about the capabilities of this tool i also post almost daily on linkedin and twitter follow me there
rkl5ev,0,weather station project should get trend symbols powered by sklearn rbf trend prediction hello fellows x200b for my weather station project raspberry pi bosch bme280 temp hum pressure sensor i want to show trend prediction symbols because the raw input from the sensor variies and because i want to become in touch with ml i want to use sklearn rbf for interpolation of the temp hum pres data already working and for trend prediction not working i just want to say from now its getting warmer etc according to my model if i just comprae the last raw values it doesnt work good because the raw values fluctuating x200b what i think i have to do i think i have to calculate the second confidence german 2 konfidenz lokale extremwerte then i know if i have a local minimum maximum so from now its likely that the temperature is rising sinking x200b my weather station uses rbf kernel do draw a nice and smooth line for temp hum pres i feed numpy array to sklearn rbf and get result numpy array as result based on 1 but i dont know how to calculate predict if im on al local extrema and therefore have to change my trend symbols x200b 1 x200b what do you think x200b edit fixed typos
rhl2pj,1,trying to see if anyone has a clue to what is hidden in my screen not sure i m doing this right either new to reddit how would i upload a screen picture to show what i m speaking about
rgiu1t,1,industry certifications i was curious are there any industry certifications around ml ai in other fields you have like cissp cisa ccna ckad etc for networking security and everything else i was curious if such a thing exists in the ml ai space thank you
rjzi1g,1,can someone educate on machine learning why i got the wrong answer a which of the following beam programming concepts that can also be created from in memory data where it is both the inputs and outputs for a particular step in the pipeline 1 pcollections 2 ptransform 3 pipeline 4 i o transforms b which of the following scripts is not included to train in cloud mle 1 distributed training sh 2 hyper tune sh 3 single instance training sh 4 create prediction service sh 5 none of the above c which of the following scripts is not needed to make prediction service using scikit learn 1 export job name 2 export model name 3 export gcs file dir 4 export test file 5 none of the above d which of the following is not a bi products partner of google cloud ml engine 1 tableau 2 power bi 3 looker 4 none of the above e which compute service lets customers supply chunks of code which get run on demand in response to events on infrastructure wholly managed by google 1 cloud functions 2 compute engine 3 kubernetes engine 4 app engine can some point out which is the correct answer and why do you think it is also on e question i thought its cloud functions but accidentally chose no 3 and got it correct can someone enlighten me by the way this is the failed exam and would like to learn from my mistakes in order to be better this is one time exam every time i take the exam it changes the question
rgbl0q,1,what is markov diffusion operator hello i am reading a paper and the authors mentioned markov diffusion operator what is that
rkwowu,0,improving the expressive power of gnns using subgraphs the expressive power of message passing graph neural networks is inherently limited due to their equivalence to the weisfeiler lehman graph isomorphism test several concurrent recent works show that this limitation can be overcome by applying a gnn on a collection of subgraphs obtained by removing nodes or edges from the input graph in a new post co authored with leonardo cotta fabrizio frasca haggai maron christopher morris and lingxiao zhao we review the common themes and nuances of these different approaches x200b
rp7kc6,1,seeking internship hey everyone i hope you are all doing fine i am ahmed abbassi studying computer science and electrical engineering at bilkent university for the spring semester of 2021 as an exchange student i wanted to ask you all if you could help me get an internship anywhere even remotely in the field of ai below i wanted to give a brief introduction about myself i am from tunisia that is where i did my pre engineering studies math and physics where i ranked 49 out of more than 2000 students in a nationwide exam this allowed me to enroll in one of the top public engineering schools of tunisia the higher school of communications of tunis where i worked hard to get selected from the top students for exchange program opportunities and that is where i chose turkey as my destination aside from that i worked on numerous projects from writing machine learning algorithms from scratch all the way to building full scale models and even scraping the data myself when needed if you need my cv just click here x200b thank you all 3
qlbye5,0,text to image models rudall e kandinsky xxl 12 billion parameters and rudall e malevich xl 1 3 billion parameters a demo for the latter is available technical report russian technical report translated to english by google translate demo for rudall e malevich xl github repo for rudall e malevich xl more links from my other post
r2o0mu,1,help withml class project hello iâ€™m currently conducting a research study for a class project iâ€™m examining employeesâ€™ attitudes toward telecommuting i e work from home and how we can use machine learning to predict that the survey will take approximately 5 10 minutes to complete so if you have some time to spare please help me fill out the google form survey below no identifiable information will be collected also if you know others who might be interested in taking this survey please feel free to forward this survey link to them thank you
rjye12,0,why does a sudden increase in accuracy at a specific epoch in these model i am learning convolution neural network now recently i have been reading papers related to optimizers such as radam when looking at the visual results of papers i found that their images showed a sudden increase in accuracy at the 80th epoch the figure6 in paper on the variance of the adaptive learning rate and beyond or 150th epoch ï¼ˆthe figure3 in paper adaptive gradient methods with dynamicbound of learning rate ï¼‰ no matter what kind of algorithm can anyone tell me why this happened thank you x200b radam x200b adabound
raxyt7,1,do you guys know a course tutorial that s not mnist i need to learn tensorflow 1 x for a project basically i have 6 features and want 1 binary ouput i ve tried adapting mnist code to perhaps work with what i need but it s just not working it doesn t seem to be training at all and just stays constant no matter the epochs the basic mlp graph x tf placeholder tf float32 shape none 6 w tf variable tf zeros 6 1 b tf variable tf zeros 1 1 y tf nn softmax tf add tf matmul x w b my highly likely dumb reshaped stuff y train reshaped tf reshape y train 800 1 y test reshaped tf reshape y test 200 1 the placeholder for the correct result real y tf placeholder tf float32 none 1 loss function cross entropy tf reduce mean tf reduce sum real y tf log y axis 1 optimization optimizer tf train gradientdescentoptimizer 0 5 train step optimizer minimize cross entropy maybe new cost and optimizer cost tf reduce mean tf nn softmax cross entropy with logits logits y labels real y train step tf train adamoptimizer minimize cost initialization init tf global variables initializer with tf session as session epochs 1 session run init for in range epochs session run train step feed dict x x train real y y train reshaped eval correct prediction tf equal y real y accuracy tf reduce mean tf cast correct prediction tf float32 network accuracy session run accuracy feed dict x x test real y y test reshaped eval print the accuracy over the the data is 2f format network accuracy 100
r3esql,1,paper overview florence a new foundation model for computer vision video paper abstract automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks similar to human vision computer vision foundation models which are trained on diverse large scale dataset and can be adapted to a wide range of downstream tasks are critical for this mission to solve real world computer vision applications while existing vision foundation models such as clip align and wu dao 2 0 focus mainly on mapping images and textual representations to a cross modal shared representation we introduce a new computer vision foundation model florence to expand the representations from coarse scene to fine object from static images to dynamic videos and from rgb to multiple modalities caption depth by incorporating universal visual language representations from web scale image text data our florence model can be easily adapted for various computer vision tasks such as classification retrieval object detection vqa image caption video retrieval and action recognition moreover florence demonstrates outstanding performance in many types of transfer learning fully sampled fine tuning linear probing few shot transfer and zero shot transfer for novel images and objects all of these properties are critical for our vision foundation model to serve general purpose vision tasks florence achieves new state of the art results in majority of 44 representative benchmarks e g imagenet 1k zero shot classification with top 1 accuracy of 83 74 and the top 5 accuracy of 97 18 62 4 map on coco fine tuning 80 36 on vqa and 87 8 on kinetics 600
rjde7a,1,ppml series 2 federated optimization algorithms fedsgd and fedavg this is my second post on privacy preserving machine learning this post talks about the optimization algorithms which are used to train machine learning models in a federated learning setting fedavg was a prominent algorithm that came out in 2016 but is still used today i wrote a bit about it earlier in my twitter thread read more here ppml series 2 federated optimization algorithms fedsgd and fedavg
rwrv2l,1,cannot install pycaret getting this error x200b error failed building wheel for scikit learn failed to build scikit learn error could not build wheels for scikit learn which is required to install pyproject toml based projects i have the python version of 3 9 5 and pip and pep are up to date the latest versions pretty much tried everything on stackoverflow but still facing this issue anybody who faced it would have an idea i d really appreciate your help thank you
remf8e,1,ocr for handwritten devanagari script hi everyone i have to do a project for handwritten word recognition in devanagari script using deep learning but i can t find good resources or even datasets can somebody please help me out here edit i am trying to implement a cnn rnn hybrid model which does not require segmentation of characters from a word and takes whole word as model input
rt6zny,1,how would you approach the book deep learning by ian goodfellow from a research perspective for context i am a undergrad student currently in my senior year i have some prior experience to ml and dl and i am also currently pursuing couple of research projects with a couple of professors but i also want to do something in my own on the side i was wondering what would be the best way to get started should i read the book mentioned above in order to get some ideas of my own or should i try something else thanks
rjgkbl,1,text analysis in low resource language this is a long shot but i ve hit a dead end in my research and i hope someone from this community can help my company is doing an internal project for a text analytics product but for arabic i ve found a few great resources that have helped me so far arabert arbert etc but there are a few things such as keyword extraction hate speech detection and ner that aren t very available it s mostly research paper with pilot studies or fairly outdated notebooks i understand text analysis and nlp in general for the arabic language is still in its early stage but would anyone know anything that can help any models transformers i can fine tune datasets i can use particularly in the three things i mentioned above thanks
r2trr7,1,best way to learn ai ml help with my current path hey guys i m a few months into my coding journey i know python rather well and am now midway through the first course on fast ai my career goal is to be able to work with and develop artificial intelligence however i am in need of a job as soon as possible and since i know that i will not be a master in artificial intelligence i know that i will likely need to get into a similar field as i work on my skills my question is what should i learn in order to get a job in as close of a field as possible to ai ml i m pretty sure data science is not too far away from ai however i m sure the barrier to entry is close to as high as ai i was thinking about taking some courses on sql and possibly software engineering to further my resume but i am unsure if that is the correct route to take i know that many here have much more experience than i do in the field of computer science as a whole and would thus love to hear from you guys thanks in advance
r18rjz,0,what are your long term career goals 10 years hello guys i am trying to figure out long term goals for myself as i feel that i reached some plateau as senior ml engineer i was wondering what other people in the field long term career goals were
r4aio0,1,model to predict sales to different customers hi i would like to build a model for predicting the sales of certain products to approx 100 different customers i have to predict the montly sales to the more important customers and the total volume sold that month i have the time series of the volumes sold each month to each group of customer in the past 10 years and they show a very seasonal pattern unfortunately many customers became such only in the past few years and they do not place orders each month so the timeseries have many zeros i initially thought about adding some autoregressive terms maybe considering all of them with an exponential decay a categorical for the seasonality and i also tried to cluster the group of customer according to the correlation of their timeseries and use the clusters as categorical variables but the dendrogram shows that many timeseries are clustered only at a very low correlation making the clustering almost useless i don t really know what to do how do you suggest to build the model
qravhd,0,is anyone working on code generation other than openai are there any other works in code generation models like codex from openai i have seen some open source version of codex but no variants of that model
rpv5va,0,looking for papers that prove that deep learning cannot solve a given problem hey guys i am looking for works where it has been proven that machine learning cannot solve a given problem because of the lack of information present in the input data ps this is very relevant in fields involving high stakes decisions like healthcare crime social good etc
qqmhiz,0,mit ai researchers introduce â€˜parpâ€™ a method to improve the efficiency and performance of a neural network recent developments in machine learning have enabled automated speech recognition technologies such as siri to learn the worldâ€™s uncommon languages which lack the enormous volume of transcribed speech required to train algorithms however these methods are frequently too complicated and costly to be broadly used researchers from mit national taiwan university and the university of california santa barbara have developed a simple technique that minimizes the complexity of a sophisticated speech learning model allowing it to run more efficiently and achieve higher performance their method entails deleting unneeded components from a standard but complex speech recognition model and then making slight tweaks to recognize a given language teaching this model an unusual language is a low cost and time efficient process because only minor adjustments are required once the larger model is trimmed down to size read the paper checkout the project 5 min read mit blog x200b
rlc7b7,1,machine learning andrew ng on courseera i would like to learn machine learning upon research this seems to be the most popular is it the same cs229 course says 12 weeks to finish but my winter vacation is just 4 weeks long
rqdpqs,0,ml in agriculture hey i m working on a software product for fruit growers helping them organize tasks through the season and have better quality fruits i m wondering what is the potential for ml here has anyone had an experience in this field ideas predicting the yield for each field using historical weather data actual yield for each year actions taken in the field predicting the price of each fruit variety based on total users yield predictions amount of searches for that variety on the marketplace subpage what i was hoping to focus on though was the quality quantity of produced fruits there are so many factors like temperature humidity of air soil from weather stations or even our own sensors to make it more accurate macro micro nutrients unfortunately available once a year requires lab actions like tree thinning hard to digitalize perhaps using image recognition that have impact on yield quality firmness color diameter fungi pest infection risk appreciate your ideas opinion on that
ruhkky,1,good colab notebook for very large batches of text hey i ve been interesting in using gpt 3 or gpt 2 to generate things like film scripts or social media posts like the deep leffen bot on twitter i d like to train it in a very very large amount of text does anyone know what model is best and if there are any good colab notebooks for this specific task feeding in very large batches of text instead of only small prompts
ruwidq,0,how to deal with huge categorical data i have a dataset that already contains about 55 columns and out of this around 10 columns or so have categorical data in it if i were to onehotencode them i will end up having a column count of more than 300 is this something advisable how do you people deal with such huge number of columns i mean 300 columns is not a big deal but i would like to know your opinion and thoughts on this
rc5jb0,0,how to optimize your model hyperparameters and build the best synthetic model possible we explore the popular open source package optuna to demonstrate how you can optimize your model hyperparameters and build the best synthetic model possible
rlixkx,0,mindall e pytorch implementation of a 1 3b text to image generation model trained on 14 million image text pairs hello i introduce an open source project which released the checkpoint of the text to image generation model dall e link example 1 text to image generation of mindall e example 2 text to image generation of mindall e the naming of this project is mindall e which is originated from the mingpt mindall e is the transformer with 1 3b params and trained on cc 15m in addition larger model 4b will be released soon to the best of my knowledge it s the first model that supports english check the project and enjoy to generate various images conditioned on your own texts
ruy2vw,1,how play something on netflix and spotify work i have always wondered what is the technology and model behind netflix s and spotify s play something button is it related to the conventional recommender system and gives the top result or a different thing altogether also what about the technology does it use rl or simple multi arm bandit any links would be really helpful
rv5ufl,1,are batch normalization and kaiming initialization addressing the same issue internal covariate shift this is a repost of a question i asked on cross validated stackexchange and haven t received answers to yet reposting here for visibility with the hope that perhaps someone can help here is the body of the question pasted from se below for legibility in the original batch norm paper ioffe and szegedy 2015 the autors define internal covariate shift as the the change in the distributions of internal nodes of a deep network in the course of training they then present batch norm as a solution to address this issue by normalizing layer inputs across each mini batch from my understanding this internal covariate shift is the exact same issue that is typically addressed when designing our weight initializaiton criteria for instance in kaiming initialization he et al 2015 the central idea is to investigate the variance of the responses in each layer so to avoid reducing or magnifying the magnitudes of input signals exponentially as far as i can tell this is also addressing internal covariate shift is my understanding correct if this is the case why do we often make use of both techniques it seems redundant perhaps two solutions is better than one if my understanding is incorrect please let me know thank you in advance references ioffe sergey and christian szegedy batch normalization accelerating deep network training by reducing internal covariate shift international conference on machine learning pmlr 2015 he kaiming et al delving deep into rectifiers surpassing human level performance on imagenet classification proceedings of the ieee international conference on computer vision 2015
rh3hyx,1,recommendations for multi terabyte time series machine learning project so i have cleared a hurdle and can dig deeper into a project that i ve wanted to learn more about and that s time series data analysis this is for an agriculture and mining company data set this obviously impacts stock market indicators that we are also taking into account but this is more of an accurate mapping of what was the predictor variable is why customers buy and there have been truckloads of ink spilled on this but they want to look at the data they have for those of you who have done this with a large amount of data i m also going back to records from the 1700 s so needless to say there is no lack of data but it where would you start if the end goal is to understand buying patterns this is to be more of a calculated chunks over time process but i m trying to find and isolate methods i can apply that will identify time series trends there are so many techniques to choose from so i am feeling like its boiling the ocean by proxy in the gpu time we have there are a few ml books i have that i ll be reading back through but some of those are older and have a more general focus 1 what books or papers do you know of 2 what did your project discover using what methods 3 what thinking outside the box can you recommend
rmimjq,1,how do they give control of a game to an ai there is probably lots of ways they can do they but in its simplest abstraction let s say a game such as league of legends bare in mind i m just one person so it wouldn t be feasible for me to even attempt this i ask our of curiosity how do they give control to ml algorithm do they just input the controls and let it do its thing
qxlk34,0,permutation invariant neural networks for reinforcement learning link to the blog post the permutation invariant neural network agents presented here can handle ill defined varying observation spaces our agents are robust to observations that contain redundant or noisy information or observations that are corrupt and incomplete
rkseqq,1,optimal number of cart trees in random forest hi guys x200b so as part of trying to learn machine learning i ve stumbled upon the question determine the optimal number of cart trees ranging from 1000 to 10000 in increments of 1000 so 1000 2000 3000 this question is as part of a random forest model question and i m having a hard time figuring it out as i thought cart was an entirely different model of trees how do i determine the optimal amount of cart trees in a random forest
ra4er4,1,how to capture words order in a sentence hi guys i m data science student and i m trying to capture the words order in a sentence for checking if in n triples subject predicate and object this order is respected for example given the phrase rougue is a comedy movie and given these 3 triples 1 rougue is movie 2 rougue movie is 3 movie is rogue in this example only the first triple is correct for my task i guess that in order to achieve my goal i have to vectorize the reference sentence but i don t understand how to capture the correct order so that only the first triple turns out to be right how can i do this thanks all edit my dataset for more than 90 is composed of simple sentences where the order s v o is respected so even the triples that are extracted as a second check should follow this order as a first check i thought about using the bag of words or tf idf to check the presence of the words contained in the extracted triple within the reference sentence however it is still not clear to me how to check if the word order within the triple is respected i know it s a coarse job however it serves as a basic control for skimming
rdwanv,1,what is the difference between xgboost and gradient boosting xgboost is more enhanced version of gradient boosting and give more better performance can anyone explain how they are different mathematically
ro12hr,0,drawbacks to the louvain leiden algorithm for community detection i ve been looking for the drawbacks to the louvain algorithm and the more recent leiden algorithm for community detection an internet search turns up almost nothing except that louvain can lead to disconnected communities which is fixed in the leiden algorithm however surely the leiden algorithm is not the end all be all of community detection is it what are some of the drawbacks of the louvain and leiden algorithms what are some instances where it performs poorly what are instances where i d prefer other community detection methods e g spectral and how come i don t see it very often in classic data clustering where one could build a knn graph and run louvain leiden on top of it
rrcav2,0,i made a modern data catalog tool for anyone using a word document or excel sheet as a data catalog iâ€™m curious if anyone would like to try it out at my old job it seemed like i d have a new project with a new dataset every few weeks the hardest part of my job was understanding the data not completing the project last year i built a data catalog using the no code platform bubble and shared it here we ended up with quite a few people testing it out and using it on personal projects in the last 12 months i took the original platform i built and leveraged some open source platforms like amundsen to rebuild a modern data catalog focused on making data documentation transparent collaborative and straightforward for anyone or company we have a sandbox environment with dummy data that we re looking for user feedback on if anyone is interested in giving it a spin please let me know we re planning to release a public version for anyone to use early next year happy new year and i appreciate anyone willing to give it a try
re6jy9,1,can i use matlab instead of octave for an ml course i wonder if they are literally the same performance wise and in terms of syntax
r55v90,1,in a statistical learning theory setting why is the hypothesis class chosen before looking at the data in a strict statistical learning theory setting a data scientist has to do supervised classification they consult with the domain experts and choose some features which they believe to be predictive of the label they also choose a hypothesis class aka a model in which they believe contains a hypothesis having a low true error aka population error all the things mentioned above were done making use of some prior knowledge of the classification task in question the data scientist then draws a big enough i i d sample collects the data and does empirical risk minimization on it training what would be different if we choose the hypothesis class after doing data exploration or after a google search regarding which model to use i m confused about what could be thought of as prior knowledge and what cannot
rpctc4,0,is crowdsourced evaluation of model a good idea is it a good idea to evaluate a model by letting users grade the model on some scale which might vary from problem to problem on the basis of its correctness which i believe may be subjective in some cases
r8t0g2,0,multi input multi output problem hello recently i have been wondering how to solve a machine learning problem and i would like to discuss ideas to solve it objective predict a list of products e g grocery given various of input conditions e g size type and location of the store performance of the store etc the output list of the predicted products will meet input criteria and might consist of multiple products the list can consist even of 100 records dataset explanatory variables can be around 1 000 due to one hot encoding of the categorical variables response variables currently i have 1 column with 1 000 distinct classes i e types of the products in the text form i was thinking perform one hot encoding however the drawback is that i will have 1 000 response variables potential solution train cnn there is no relationship between products in the response variables taking into consideration one hot encoding over response variable and multi class classifier
ruijl9,1,sieve we processed 24 hours of security footage in 10 mins now you can search per frame hey everyone iâ€™m one of the creators of sieve and iâ€™m excited to be sharing it sieve is an api that helps you store process and automatically search your video dataâ€“instantly and efficiently just think 10 cameras recording footage at 30 fps 24 7 that would be 27 million frames generated in a single day the videos might be searchable by timestamp but finding moments of interest is like searching for a needle in a haystack we built this visual demo link here a little while back which weâ€™d love to get feedback on itâ€™s 24 hours of security footage that our api processed in 10 mins and has simple querying and export functionality enabled we see applications in better understanding what data you have figuring out which data to send to labeling sampling datasets for training and building multiple test sets for models by scenario to try it on your videos visual dashboard walkthrough
r1w2hr,1,45 worked examples in machine learning energy medicine banking retail physics finance
rjblex,1,multi layer neural network from scratch in go this post covers the concepts behind neural networks and walks through the implementation of a multi layer network in go that performs quite well on the mnist dataset i put this post together with a focus on better understanding backpropagation in particular hope some others may find it useful
qr3b6w,0,fast cc taylor transform for computer vision potentially train nerf in matter of minutes hello we just released our latest paper on arxiv we explore the idea of fast summation algorithms equivalent of the fast fourier transform but with taylor series instead in the context of computer vision in terms of computational complexity our approach disentangles the number of model parameters n and model evaluations m i e our approach is in o n m instead of o nm this allows us to reduce the number of flops required for training and inference 150 200x depending on the problem unfortunately our current implementation is very flop inefficient and a flop efficient implementation requires a good bit of custom cuda code we are working on it video abstract paper link let me know if you have any questions
rx8l60,1,how would i finetune gpt neo to respond to a prompt rather than just predicting what comes after specifically training it on titles of youtube videos and predicting the comments
qynzlz,0,a high level microblog on federated learning i wrote a high level not too technical thread on federated learning twitter thread if you found it informative do let me know
r13iiq,1,federated learning implementation hi i was wondering if anyone has come across an implementation of a federated machine learning system i want to build one for the hospital system and hardware is not my forte could i can spin vm on the cloud system of the respective hospitals and make sure they can communicate between each other thanks
r980d7,0,are there a lot of projects in machine learning which do not allow international students to participate i ve heard projects that are funded by nsf industry are usually fine but dod or doe related projects tend to require a security clearance i ve heard more than 70 of the projects in ml cv nlp are funded by nsf industry and less than 30 of them are dod or doe related and thus international students can participate in most of the ml related projects is this true it seems international students can participate in nsf funded projects but usually not in dod or doe funded projects if an individual is not a u s citizen can the individual apply for a grant can a permanent resident serve as a principal investigator at a u s institution except for nsf fellowships which by statute can be made only to citizens nationals or lawfully admitted permanent resident aliens of the united states there generally are no nationality restrictions in any nsf program a proposing institution in the us may designate as principal investigator anyone it believes to be capable of fulfilling the role
rcnfgx,1,find feature importance for an svm with an rbf kernel i m a bit beside myself i m trying to find the best way to evaluate feature importance in a 5 feature data set for an svm with an rbf kernel from sklearn in python because the kernel is non linear i cannot use the coeff attribute searching online presented permutation importance but is there anything else i ask because logistic regression and xgboost have other evaluation methods for feature importance and it would be good sanity check the only method i have thus far found for an rbf svm thanks in advance
rl2913,1,i have the general idea of machine learning i have the hardware i have data collected but how do i get to the point of just training models it feels so far away even though it seems like i have all the pieces already i don t know how to bring it all together
r7brz3,0,time crystalline study published in nature journal observes a new phase of matter in a quantum processor a team from google research stanford university university of massachusetts university of california columbia university princeton university max planck institute for the physics of complex systems and university of oxford uses a quantum processor to observe a time crystal a new phase of matter which could be one of the most significant physical discoveries in decades here is a quick read time crystalline study published in nature journal observes a new phase of matter in a quantum processor the paper time crystalline eigenstate order on a quantum processor is on the nature website
rg69um,0,state of the art methods for identifying dag parameters say i have written down a directed acyclic graph a causal model with a few dozen variables and specified the functional form of the corresponding posterior pdfs moreover i have a dataset with observations for many though not all of the variables for simplicity let us assume that all variables are categorical which methods are state of the art for identifying the model parameters the posteriors in the graph
r4mlvk,1,how do i get this basic lstm example working i ve created a basic scenario so i can see the effects further development has on my model i ve hit a snag when including an lstm layer as you can see with the code below without the lstm and assuming the inputs are all 100 correct the model produces 100 accurate forecasts however when including the lstm layer the forecasts go way off since the inputs all sum to the correct output the model shouldn t learn anything and should output the correct answer all the time so what have i done wrong here import numpy as np import pandas as pd from tensorflow keras models import sequential from tensorflow keras layers import dense input lstm from tensorflow keras preprocessing import timeseries dataset from array x np arange 100 df pd dataframe index x df input x df input addition 1 df target x 1 x df input input addition y df target max bars back 1 batch size 2 train timeseries dataset from array x iloc 0 40 values y iloc 0 40 values max bars back batch size batch size validation timeseries dataset from array x iloc 40 70 values y iloc 40 70 values max bars back batch size batch size test timeseries dataset from array x iloc 70 values y iloc 70 values max bars back batch size batch size model sequential model add input batch input shape batch size max bars back len x columns model add lstm 1 kernel initializer ones return sequences true model add dense 1 kernel initializer ones model compile loss mse optimizer adam metrics mse model fit train epochs 3 validation data validation shuffle false test index y index 70 outputs pd series index test index predictions model predict test for x in range len predictions outputs iloc x predictions x 0 0 test df df loc test index test df outputs outputs test df
r97cfd,1,buddies to watch ml lectures with i am planning to watch some machine learning lectures beginning today would appreciate buddies to keep company as we learn something new together my first plan is to watch lectures for pca and svm followed by others feel free to dm me or leave a comment below and we can connect on discord
r3j0d7,1,courses books pdfs on how to take action from eda i haven t done any ml in a while and i ve gotten pretty rusty because of it i started a new project and i m just totally blanking on what to do with my data in response to the eda i ve conducted could you fine people recommend some resources that cover some actionable steps in response to exploratory data analysis edit lol downvoted is eda not a crucial part of learning ml just throw the data in and giddy up alrighty
r8sfwj,0,impact of initialization on gans hi i was wondering how important is the initialization for gans i heard that normal init leads to better convergence but i cannot find any paper that support this claim do you guys have any resources on the topic some practical advice x200b thanks
rtpci8,1,should we drop the ai when referring to ai ml i wonder why these two seem to always come in a bundle ai is a superset of ml and i realize that there are some examples of ai that are not ml however it seems that most of the work discussed in articles or even job listings are specifically machine learning hence shouldn t the ai portion be dropped thanks
rl12cu,0,i made blockly for machine learning hello everyone i developed a visual programming language from google blockly to generate and run python code which supports basic ml algorithms this ui allows you to download and upload xml layout too github hosted webpage please star the repo if you like and if you think addition of any new feature will be a bonus don t forget to raise an issue
qsfiw7,0,causal learn causal discovery for python we are excited to release the python causal learn package for causal discovery see the package and documentation any feedback is welcome x200b
r6oxjl,0,machine learning for factor analysis rule derivation etc i was considering a question where i wanted to know if any patterns could be found in a question of classification i wanted to know if a list of factors or criteria could be deduced for why a terminologist who often prepared a glossary considers a phrase a term for example in a vaccine instruction manual â€œmrnaâ€ would be a good term for a glossary the word â€œthereâ€ would not so hereâ€™s the thing in order to study which factors tended to correlate and to what degree i e some kind of model a mathematical function of any structure perhaps a linear equation i need to have factors in the first place how can i generate an ample list of factors to begin with like letâ€™s say the answer in this scenario is â€œa good term should be highly specific it should be important to have a good quality translation and it should have high frequency in the source text â€ is it possible ai could perform that level of inductive reasoning to observe classified data points in their context term not a term given some text and not only predict what is a term but somehow produce any kind of structure we could observe to understand what the factors it found were i really hope someone can help me know about what ideas exist in the field on this question thank you
rrjqt5,1,how to find intrinsic dimensionality of a pretrained network hello i have a image dataset for classification task i am using transfer learning with some pretrained networks in pytorch i want to find the intrinsic dimensionalities of the deep features however i don t know where to start i even don t know whether i defined the problem properly or not any suggestion would be nice
r40m4u,1,iâ€™m about to start producing circuitmess batmobile a diy robot car that teaches kids about stem sciences hi everyone as you may remember my name is albert and i m a 22 year old diy electronics enthusiast who also happens to be a huge batman fan ðŸ¦‡ earlier i have posted here about circuitmess batmobile a project i have developed after getting the license from warner bros and which is aimed to teach kids technologies of the future such as ai machine learning computer vision and more i have received really nice feedback from the people in this group so i wanted to notify you all that my batmobile is now funded and that i will officially start producing this kit soon âœ¨ the crowdfunding campaign will be live for 5 more days meaning that you can still pre order your batmobile or support the project if you find it interesting hereâ€™s the link to the listing where you can also get more info about batmobileâ€™s software and hardware thank you all for your support so far
rxbl9w,1,record chess moves with webcam for online play hello i would like to share with you my project which records moves made on your real life chess board using webcam it also transmits the moves to any chess website for playing online
qksrhl,0,what tools exist to determine the most useful type for perdiction i ve messed around with ibm s and google s automl frameworks i can t remember which output a report of which data was most helpful in predictions if i m not using the correct terminology i m sorry basically if i train an ai model on data such as car steering angle gyro acceleramotor speed etc and ground truth it to a more precise car steering angle i want to figure out which of these data types were most useful for a good prediction that way i can feed in a whole lot of data train the model and know which data sources are irrelevant what tools exist out there for this
rtapnm,1,why no sigmoid activation function for the discriminator from tf s dcgan tutorial i m referring to this tensorflow tutorial why don t they constrain the discriminator s output to be within 0 1 what advantages does their approach have
ruifyk,1,understanding architecture of dnns specifically cnns what is the intuition behind how many neurons each layer in a dnn has for example in a resnet50 the first layer is a 7x7 64 kernel convolution but why 64 kernels specifically instead of 63 or 66 or any other random number is this just the industry accepted number or was the number of kernels treated as a hyperparam and validated or in tfâ€™s tutorials they use a dnn to classify mnist digits with the layers flatten input 28 28 dense 128 relu dense 10 but why does the second layer output to 128 neurons it just seems like a magic number as i canâ€™t find any reasoning behind it
rbo6s4,1,comparing bayesian linear regression model with a decision tree regression model how would i go about comparing a bayesian linear regression model with a decision tree model
rchy2l,1,what do you guys thinks about this ai dream app how do u think was it created tools and resources used for this app what will it take to create a text to video generation app
qwu7nc,0,synapseml a simple multilingual and massively parallel machine learning library today weâ€™re excited to announce the release of synapseml previously mmlspark an open source library that simplifies the creation of massively scalable machine learning ml pipelines building production ready distributed ml pipelines can be difficult even for the most seasoned developer composing tools from different ecosystems often requires considerable â€œglueâ€ code and many frameworks arenâ€™t designed with thousand machine elastic clusters in mind synapseml resolves this challenge by unifying several existing ml frameworks and new microsoft algorithms in a single scalable api thatâ€™s usable across python r scala and java website paper blog x200b x200b synapsemls api aims to unify and simplify distributed ml
rgg36d,0,results of the nethack challenge at neurips 2021 report excerpt from the results section the results of the showdown showed that â€” for the time being â€” symbolic bots red quite clearly have the upper hand in this difficult environment the top three spots in the overall best agent all went to agents from the symbolic agent track then the following three went to top of the neural agents track with the winner of this track being the only highly hybrised model in the competition alternating between symbolic and neural play depending on the proximity of monsters the margin of victory was significant with the top symbolic agent beating the top neural agent by a factor of almost 3 in the median score this was in fact increased when looking at the very best agents from each team where frequently we might see almost an order of magnitude improvement in the median score between the best symbolic and neural agents while our best symbolic teams had moderate to expert nethack domain understanding we were surprised to find they often had extensive ml experience as well in fact both winning symbolic teams said they had intended to enter the neural track but found their symbolic methods scaled much better in over half a million evaluation games no agent managed to ascend what does it all mean for ai â€œdespite the game of nethack being far from solved by these agents seeing them descend over 20 levels deep into the dungeons of nethack to achieve scores of over 100 000 points is very encouraging past versions of nethack have a rich history of symbolic agent type bots so the methods of machine learning may have some catching up to do in this specific realm of playing nethack but i am optimistic for the future of both methods after seeing the results of the challenge it has been amazing to see a game that i cherish so dearly be used to make new advancements in machine learning and artificial intelligence and i look forward to seeing how teams improve in next yearâ€™s challenge â€ â€“ tonehack nethack is far from solved first of all the results show that nethack is still a tremendously hard challenge for ai agents whether they are symbolic bots or deep reinforcement learning agents the top median score of 5 000 is several orders of magnitude short of typical human ascensions and while some bots managed to achieve much higher scores in a few limited runs most runs did not descend very deep into the dungeons of nethack and instead stayed within early stages of the game median score is good â€” but ascensions would be better the challenge highlighted the complex relationship between score and ascension we found many entrants elected to â€œcampâ€ in the early stages of the dungeon grinding out a high score by killing monsters instead of progressing into the dungeon while this undoubtedly helped the weakest â€˜rolesâ€™ in the game like tourist or healer it will not lead to winning the game we learnt that score and ascensions are not always well aligned and our objective may be due a rethink in future challenges that said the focus on the median agent performance is still important incentivising the creation of robust general agents but the focus on in game score may be less so symbolic bots can strategize like a human can neural ones nethack benefits â€˜strategicâ€™ play â€” good play often involves executing a series of actions with a well defined expressible sub objective eg â€œfind sokobanâ€ or â€œapply a unicorn horn to cure poisonâ€ symbolic bots found it easy to define â€˜strategyâ€™ like subroutines and to decide when to deploy them based on rich human legible representations of the game state this made it easy for participants developing symbolic bots to incorporate their domain knowledge neural agents struggle in this area since hierarchical rl is an open problem in the research field and it is hard for agents to discover â€˜strategyâ€™ type patterns of behaviour in environments featuring such a large action space and sparse reward symbolic bots can be know it alls neural agents find it harder the game of nethack is only partially observable only a single dungeon level is ever visible and the many objects and player states are often hidden unless inspected remembering a discovery or incorporating extra knowledge is often key to making a good decision symbolic bots excelled in keeping the full game state in memory and incorporating external knowledge into their strategies they found it easy to transfer domain knowledge to the decision making process in contrast neural agents find it harder to maintain information in memory especially if there is no reward directly associated with it
r6ldxk,1,ai recruitment survey hey everyone my name is reeti and i work for a consulting organization called illinois business consulting weâ€™re working for a company in the ai sector and are hoping to improve their recruitment we made a survey about ai recruitment so if you guys wouldnâ€™t mind filling out the survey below it would be awesome thank you again
reo2yy,1,need help with a cnn lstm model hi guys iâ€™m trying to train a cnn lstm model on some videos how do i convert the video into data that the cnn can read previously iâ€™ve used models offered by keras like the vgg19 but i was wondering if there was a way to convert it without the need for the keras model
rj628g,0,land pattern annotation hi i would like to train a semantic segmentation model to detect chunks of soil and shadows the annotation itself without any helper tools is time consuming below is the sample image so far for annotation i have used mostly cvat and labelme for such patterns however it would take too much time to annotate a single image i would be thankful if anyone recommended any helpful tool or approach to annotate the images like the one above
rav3gq,0,hierarchical topic modelling over time hello reddit i am proud to present you htmot for hierarchical topic modelling over time this paper proposes a novel topic model able to extract topic hierarchies while also modelling their temporality modelling time provide more precise topics by separating lexically close but temporally distinct topics while modelling hierarchy provides a more detailed view of the content of a document corpus the code is easily accessible on github and a working interface provides the ability to navigate through the resulting topic tree with ease
qk79s9,0,physics informed neural network suggestion and recommendation hi guys i am learning about physics informed neural network actual research focussing on autoencoder however i just got in this new field and really want to get in depth knowledge in this area would you recommend any related work or papers that i should read thanks a lot
rchlwg,1,deepwalk embeddings differentiates nodes with degree zero hi i have been using deepwalk publication implementation in order to embed the nodes of a network into a latent space by mistake i had some unconnected nodes in my network so deepwalk should not be able to generate any random walks on them in the implementation the random walk that is returned would just contain the node itself interestingly i found that the model actually learned to separate nodes with degree 0 from other ones while this itself isn t too surprising the model also found some clusters within these nodes how can that be how can different nodes which aren t connected to anything in the network have different representations thank you very much for your input
rqy3kd,1,which data science ml development processes would you like to see more automated i e have a python library for what parts of ml projects do you find really tedious and do you think should be wrapped in a python library looking for project ideas
r6h3h8,0,i created an auto updating kaggle dataset that collects high frequency crypto market data updates daily 20 related trading notebooks i am happy to announce that i finally finished cleaning organizing creating baselines and developing an automated collection pipeline that collects minute by minute market data for cryptocurrencies it updates on kaggle every day and will keep doing so until the competition is over maybe even more the whole project took me a lot of time to develop and is not easy to maintain so please if you find this of value your feedback support is highly appreciated the competition as some of you know there is crypto forecasting competition is running on kaggle g research crypto forecasting in this competition we need to use machine learning for forecasting short term returns of popular cryptocurrencies such as bitcoin ether dogecoin we are provided a dataset of millions of rows of high frequency market data dating back to 2018 which we should use to build our models on once the submission deadline has passed the final score will be calculated over the following 3 months using live crypto data as it is collected auto updating kaggle dataset to make things more interesting i created an auto updating kaggle dataset that collects high frequency market data for multiple cryptocurrencies updates daily on kaggle available for anyone to play with also i also released 20 starter notebooks each demonstrating a different model or method for forecasting future returns this project was meant to be for the currently running crypto forecasting competition by g research however since it is publicly available i assumed many others would like to also have a look mimics real life better than typical datasets this is a unique opportunity to work in a much more real life setup than usual kaggle because the datasets update daily so if you mess up and overfit you see it tomorrow ðŸ˜‚ anyway this is an ongoing project that is also beginner friendly since it is highly documented many more time series finance related notebooks will be released in the future so this can also serve as a first stop when studying time series analysis baselines starter notebooks cv model hyperparam optimization time series models feature engineering neural network starter ae analysis 1 lightgbm starter analysis 2 catboost starter written from scratch series agg xgboost starter supervised ae janestreet 1st ae janestreet 1st engineering transformer volatility features reinforcement learning ppo starter about the validation grouptimeseriessplit â³ in the making fork them as you please enjoy yourself auto updating full price datasets i created an up to today auto updating dataset which contains the full historical data for all assets of the competition so you can easily build models that utilize it the datasets are split to each asset since they are much heavier than the competition data the datasets have also been labeled as described in the competition overview and had been organized in a way that they are at the exact format of the competition data the goal of this is to provide a dataset that 1 contains the full history for each asset currently the competition data goes back to 2018 this dataset contains data from even earlier 2 auto updating daily due to the high volatility of the cryptocurrency market we should train our models on the most recent data available these datasets have a backend pipeline for collecting formatting and reuploading to kaggle they are scheduled to be updated daily every single day until the end of the competition 3 preprocessed the datasets had been ffilled to overcome any missing values issue that is present in the original competition dataset the datasets binance coin bitcoin cash bitcoin cardano dogecoin eos io ethereum ethereum classic iota litecoin monero maker stellar tron bonus dataset i ve also uploaded a dataset containing the most powerful source for predicting cryptocurrencies movement elon musk s twitter ðŸ˜‚ it is simply an updated dataset of all elon musk s tweets ðŸ˜‚ i must check if elon musk can help us win ðŸ‘Œ you can play with it yourself here technical details about the data for every asset in the competition the following fields from binance s official api endpoint for historical candlestick data are collected saved and processed 1 timestamp a timestamp for the minute covered by the row 2 asset id an id code for the cryptoasset 3 count the number of trades that took place this minute 4 open the usd price at the beginning of the minute 5 high the highest usd price during the minute 6 low the lowest usd price during the minute 7 close the usd price at the end of the minute 8 volume the number of cryptoasset u units traded during the minute 9 vwap the volume weighted average price for the minute 10 target 15 minute residualized returns see the prediction and evaluation section of this notebook for details of how the target is calculated 10 weight weight defined by the competition hosts here 11 asset name human readable asset name indexing the dataframe is indexed by timestamp and sorted from oldest to newest the first row starts at the first timestamp available on the exchange which is july 2017 for the longest running pairs enjoy yourself and thank you in advance for your support this is not an easy system to maintain
rd7tlx,1,can someone explain what the author means to say here pic in this figure how is machine learning helping people to learn
rvv6f8,1,proposal for simple batch scheduling of local gpu s given a list of say predict jobs how best to keep 1 gpu s busy with one cmd at a time per gpu i want a simple solution for one workstation for now so am skipping pbs and slurm like solutions here s my idea for a lightweight cmdline interface update seems to work fine gpu q py batch file where batch file looks like predict py params1 predict py params2 the meat of it import threading queue q queue simplequeue with open cmds r as f for cmd in f q put cmd import subprocess def worker gpu while true try item q get block true timeout 10 except empty break item item replace predict py predict py gpu str gpu subprocess call item shell true for i in range n gpu threading thread target worker args i start
rlil7y,0,cfp first international symposium on the tsetlin machine grimstad norway june 20 21 2022 hybrid x200b call for papers first international symposium on the tsetlin machine welcome to the first international symposium on the tsetlin machine in grimstad norway june 20 21 2022 hybrid we invite submissions of papers on all topics related to tsetlin machines as well as learning automata novel ai algorithms explainable and interpretable ai energy efficient ai systems design new ai applications and intelligent data preprocessing paper submission deadline march 4 2022
rwp07l,1,tflite conversion from keras model gives low accuracy hi i was converting two keras models to a full integer quantize tflite model one tflite model gives a high accuracy 98 and another one gives very low accuracy 0 4 i have checked the quantized model and found that the scaling factor change significantly here is come quantization parameters for the model with 98 accuracy tensor index 1 quantization parameters scales 0 0077935 zero points 1 tensor index 2 quantization parameters scales 0 02159962 zero points 0 tensor index 3 quantization parameters scales 0 00016834 zero points 0 here is come quantization parameters for the model with 0 4 accuracy tensor index 1 quantization parameters scales 0 00770933 zero points 3 tensor index 2 quantization parameters scales 0 00948102 zero points 0 tensor index 3 quantization parameters scales 7 3092306e 05 zero points 0 as you can see the scales are changed significantly for the model with low accuracy is the scaling value makes that difference in the model accuracy
rj4ju4,1,am making a project what do you guys think about the idea hello we need your help in voting for a program â€œdoctor in a boxâ€ dib that analyzes medical data through artificial intelligence and from there dib suggests what the patient can do or eat before his health problem does not increase the program presents medical reports in a simplified way that is understandable to average people to achieve a healthier life and reduce the need for regular hospital visits dib also provides a reminder for medications timing adding dib can act as a lifesaver in case of emergency because dib will automatically contact the hospital or one of the relatives in the event of loss of consciousness or illness voting link is
rqsyn9,0,paper explained glide towards photorealistic image generation and editing with text guided diffusion models video walkthrough diffusion models learn to iteratively reverse a noising process that is applied repeatedly during training the result can be used for conditional generation as well as various other tasks such as inpainting openai s glide builds on recent advances in diffusion models and combines text conditional diffusion with classifier free guidance and upsampling to achieve unprecedented quality in text to image samples x200b try it yourself x200b outline 0 00 intro overview 6 10 what is a diffusion model 18 20 conditional generation and guided diffusion 31 30 architecture recap 34 05 training result metrics 36 55 failure cases my own results 39 45 safety considerations x200b paper code model x200b more diffusion papers
rjmsc4,1,what do you think about the educational game while true learn
r3aoz0,0,the sensory neuron as a transformer paper implementation screencast hey there i created a video where i tried to implement the paper the sensory neuron as a transformer permutation invariant neural networks for reinforcement learning from scratch it is very much based on the official code however i introduced multiple modifications and simplifications to be able to fit it in a single youtube video most notably it only focuses on the cartpoleswingup environment lastly i ran a couple of experiments to test the permutation invariance and robustness to noise hope some of you could find it useful i would be happy to get any feedback or answer any questions video link
rim2dg,1,any online course recommendations for learning computer vision title for context just finished andrew ng s ml course and have finished one term of uni with an ml module i don t know for certain what difficulty bracket is fine after those but id greatly appreciate any beginner intermediate course recommendations you guys have
rg6vp7,1,best what to plot a user flow ok let me be more specific i m working on an analysis of chatbot data the bot s dialogs contents are represented by ids my company wants to see which routes are more accessed by users i already have that information but the lead data scientist said it would be nice to plot this in a flowchart kind of way he told me to look into sankey diagram but from what i read i don t think it s the best way to do this because it requires information that we don t have here s an example of the id routes x200b most acessed ids 7 43 342 78 92 33 7 123 56 73 23 22 7 89 76 125 48 77 7 89 76 125 48 77 as you can see they are stored as lists the idea would be to plot this in a way it showed the flow of user experience so to sum everything up 1 is there a way to do this using a sankey diagram 2 is there a better way to do this edit btw i m using python
rpcwbh,1,questions about the mitx courses on probably and statistics i am interested in taking the introductory course on probability offered by mit through edx see link below i am also interested in taking the introductory course to statistics following the probability course listed above see link below but it seems like both courses require students to be familiar with single variable and multi variable calculus the statistics course also requires student to be familiar with vectors and matrices i ve only taken calculus 1 in my first year of college and that was more than 10 years ago i would prefer to get the certificates for these courses but i am afraid that i might not be able to complete them for those that have taken these 2 courses will i be able to do well in these courses without the prerequisites thanks in advance
rdfdcv,0,collection of 33 psychology related datasets finally released i am happy to announce that i finally finished cleaning organizing creating baselines and uploading the first version of the massive 33 datasets collection of openpsychometrics the whole project took me around two weeks to clean and organize way more than i planned so please if you find this of value your feedback support is highly appreciated why would you want to work on them small size is similar to titanic clean someone on this cleaned them a lot intuitive you see what was the question you see what the individual answered nothing more nothing less funny fun to work with questions like would you rather own a gun or do the dishes has the prettiest starter notebooks you have ever seen in your life seriously i invested an embarrassingly amount of time on the basline notebooks check them out the collection the collection includes 32 psychology pseudo psychology datasets links are all here
rhxfwl,1,how to make a simple stock price prediction google colabs hi all i have a very simple csv file that is just the closing s p 500 price for x amount of days and the date found here what i m trying to do is just make a prediction of some sort using this data i was going off of this scikit learn estimator but it brings me to lasso when i thought i would use something like ltsm anyone with any help be appreciated
rxep30,1,hardware good gpu on a bad computer bad idea heyo i m bit short on the money side especially now that buying a gpu costs a liver and a half but i wanna upgrade my hardware because i m very limited in what i can do due to vram limitations and my graphics card being generally not great thing is i don t possess a good desktop pc just a gaming laptop and an old desktop one 10 years old and buying the whole beast is a bit out of budget for me right now how much of a bad idea is it to mount a recent good gpu on an old computer for video game purposes it would be plain stupid since there is a lot of interaction between gpu and cpu but for ml since everything is loaded up on gpu and left there to run i thought maybe it would be ok thx for the help sorry if i posted on the wrong place
qrw5pb,0,article an introduction to language models in nlp hey we re working on an intro series to language models would love any feedback on this first installment
ra1brw,0,what is the gold standard of visualizing train vs validation loss if you are using cross validation for hyperparameter training i have divided my data into six equal parts five for training parameter fitting cross validation and the last one as a hold out test set each of the five training sets is used as a validation set while the model trains on the other four i am randomly drawing parameters 20 times and therefore training 20 different models before moving on to the next validation set this means i am training 100 sets comparing 100 sets of parameters i am wondering if there is anything i should do differently i am also trying to plot both the training and the validation loss which is producing 101 graphs 100 for training one for the final test any ideas edit never mind i couldn t see the forest because of all the trees i am measuring the loss of all validation runs and the hyperparameters of the best run are used in the end so i will just plot the loss curve for the best validation run and for the final run
r5iyb6,1,i need help with this tutorial hi i m completely new to learning machine learning and i was watching this video on how to make your first ai with python it was going well until i ran the model fit cell and i got this error and the only idea i have as to what could be the issue is that some external file is outdated but otherwise i have no clue you guys and girls have any idea as to what might be wrong thanks alot for any answers and have a good one
rekhev,1,how to detect and tag persons on photos i m a software engineer no ai ml experience think it s something interesting to play with i have thousands photos taken for the family on onedrive i d like to download all of them and store on a local server meanwhile hand pick a few to train a model and categorize multiple persons eith the output then assign names say myself wife son daughter use these as input continue run through all photos at the end i ll have a database table photo1 is myself and son photo2 is wife and daughter etc of course i can also edit the exif info on the photo and attach these tag information later i can search for photos that re tagged with my son a different way would be to run all photos altegother output would be person1 on photo1 person 2 3 on photo2 etc then i can assign names from the final output which could be a simple mapping in database person1 myself not sure i m making any sense how do i get started
rw8jxd,0,ray skorch distributed pytorch on ray with sklearn api tl dr train pytorch models on large tabular datasets with a scikit learn skorch api hi r machinelearning i m the principal author of ray skorch a library that lets you run distributed pytorch training on large scale datasets while providing a familiar scikit learn compatible skorch api integrating well with the rest of the scikit learn ecosystem under the hood ray skorch uses ray train for distributed pytorch training and ray data for handling and shuffling large datasets ray skorch works only with tabular data currently it can use numpy arrays pandas dataframes and ray data datasets pip install ray skorch you can switch your skorch code to ray skorch just by changing a few lines import numpy as np from sklearn datasets import make classification from torch import nn pip install pytorch tabnet from pytorch tabnet tab network import tabnet from ray skorch import raytrainneuralnet x y make classification 1000 20 n informative 10 random state 0 x x astype np float32 y y astype np int64 net raytrainneuralnet tabnet num workers 2 the only new mandatory argument criterion nn crossentropyloss max epochs 10 lr 0 1 tabnet specific arguments module input dim 20 module output dim 2 required for classification loss funcs iterator train unsqueeze label tensor false iterator valid unsqueeze label tensor false net fit x y predict proba returns a ray data dataset y proba net predict proba x to pandas more examples including ones on bigger datasets can be found here the package is experimental and iâ€™d love to hear your feedback both on the package itself and on the concept of distributed training on tabular data with simple familiar apis any comments suggestions or bug reports are hugely appreciated
r3elk4,1,tips for interesting school project ideas maybe you read an interesting paper or have done something before that you can recommend iâ€™m familiar with pytorch and tensorflow so it doesnâ€™t have to be beginner level but it shouldnâ€™t be too advanced so that you canâ€™t build it from scratch and finish it within a month
rm67i0,1,notebooks release 30 crypto trading notebooks for kaggle s crypto competition i released 30 starter notebooks each demonstrating a different model or method for the crypto forecasting competition currently running on kaggle x200b this is an ongoing project that is also beginner friendly since it is highly documented many more time series finance related notebooks will be released in the future so this can also serve as a first stop when studying time series analysis x200b the whole project took me a lot of time to develop and is not easy to maintain so please if you find this of value your feedback support is highly appreciated x200b selected example baselines starter notebooks x200b g research lstm starter notebook ðŸ”¥ x200b g research reinforcement learning starter x200b crypto prediction technical analysis features x200b crypto prediction volatility features x200b purged time series cv lightgbm optuna ðŸ”¥ x200b ðŸ”¥ðŸ”¥ wavenet starter notebook ðŸ”¥ðŸ”¥ updated x200b let s talk time series validation x200b and many many more links inside the notebooks x200b fork them as you please enjoy yourself
rhnnaw,1,how to approach a scheduling problem i ve got a problem whereby i m trying to match resources to tasks think of planning the maintenance of a train and i need to know which people go to which trains i understand this is more of an optimization problem but i m struggling to understand how to frame the problem any pointers or papers or topics would be appreciated
rbc6qb,0,what transformers might know about the physical world
rckmfb,1,what would be involved in training a machine to convert complex images into a simplistic art style i have heard and seen results from programs that can generate images which follow the rules of certain art styles such as creating fake pokÃ©mon sprites that despite not looking like they depict anything clearly use the same style and i know there is software that can look at images to try and figure out the contents this makes me curious what would be involved in for example having a computer look at images of other fictional beasts like the thousands of mario enemies or digimon using the images from their franchise wikis and then try to convert them to something that follows those rules
r8d36l,1,can i split a training set hi all i need a little help with one of my homeworks so i have a training data set and a testing data set two separate data sets is it possible for me to split the training data set 70 30 and then use the testing datatset to actually test the model is there anything wrong with this i believe the 30 of the data would be a validation data set and im not sure if there is anything wrong with doing this thanks
qu1e0q,0,examples of when humans ml model working together outperform either in isolation maybe when interpretability explainability methods used in an ideal world automated explanations of model outputs would make it possible for a human expert and a classifier working together to outperform either working in isolation like an ensemble except instead of having n models that you combine in some way you have 1 model and 1 human who chooses whether to accept the model output or to reject it and go with their own opinion instead â€œsuccessâ€ is if this setup outperforms both the model and the human acting alone there s some classic literature by paul meehl arguing that when you allow human experts to second guess the outputs of even simple regression based classifers like this having a human expert in the loop doesn t help and generally makes things worse but intuitively it seems like methods for explainability interpretability could help the human know when to defer to the model and when to go with their own opinion for example maybe a skin cancer classifier predicts that some particular lesion is a melanoma but the dermatologist can see from a saliency map that the classifier is not paying attention to the right part of the image and rightly ignores the classifier in that particular instance whereas in a different case where the dermatologist is less certain and the saliency map makes sense they defer to the machine classification is there research that looks at whether the combo of human expert s using ml model with some automated â€˜explanationâ€™ of the ml model outputs saliency maps lime whatever outperforms both the human experts and the ml model in isolation i feel like this should be a super common evaluation paradigm in the interpretability literature and perhaps it is and iâ€™m just using the wrong keywords but iâ€™m not finding a lot would be especially interested in cases where the ml model in question is a language model but anything at all in this realm would be helpful edit thanks so much for all the great responses really pleasantly surprised that this sparked so much discussion lots to dig into here i appreciate it
reva1d,0,researchers propose â€˜proxyflâ€™ a novel decentralized federated learning scheme for multi institutional collaborations without sacrificing data privacy tight rules generally govern data sharing in highly regulated industries like finance and healthcare federated learning is a distributed learning system that allows multi institutional collaborations on decentralized data while also protecting the data privacy of each collaborator institutions in these disciplines are unable to aggregate and communicate their data limiting research and model development progress more robust and accurate models would result from sharing information between institutions while maintaining individual data privacy for example in the healthcare industry histopathology has undergone increasing digitization providing a unique opportunity to improve the objectivity and accuracy of diagnostic interpretations through machine learning the preparation fixation and staining techniques utilized at the preparation site among other things cause significant variation in digital photographs of tissue specimens because of this diversity medical data must be integrated across numerous organizations on the other hand medical data centralization involves regulatory constraints as well as workflow and technical challenges such as managing and distributing the data because each histopathology image is often a gigapixel file often one or more gigabytes in size the latter is very important in digital pathology paper github short summary by nitish
r247qp,0,dimensionality reduction on dataset with linear and nonlinear features being used for regression i have a data set with 900 features and 12000 examples i believe that some of the features have a linear relationship with the target variable and others have a nonlinear relationship iâ€™m looking to reduce the input space to save time on training and prevent over fitting by using either feature selection or some sort of feature compression method i believe pca is out of the question because of its assumption of linear relationships amongst features iâ€™m currently looking at using mutual information to do this but iâ€™ve never used this method before what are your thoughts
r78ctn,0,project predicting group behavior based on its members behavior hi everyone i ve been asked to do a small project and i m not completely sure how to tackle it some details i have a dataset that describes the behavior of a few dozen groups each group comprises a number of individuals each group has a score that indicates whether the group is going to become more aggressive or less aggressive this is based on a model that was not created by me i ve been asked to find certain conditions that lead to groups becoming more aggressive for example if half the group s members do a certain activity it means there s a 75 chance the group will become more aggressive there are two options i m considering i can look at statistics of groups that became aggressive say 40 of the members of groups that became aggressive did activity 1 therefore say if 40 of a group does activity 1 the group is likely to become more aggressive i suspect this is what i m expected to do but it confuses causation and correlation the fact that x of a group did something and became aggressive does not mean that groups that do so are likely to become aggressive plus in this case not sure where i will get the probability from i can potentially create a large number of models that proactively try to predict all the groups behavior based on certain conditions if 25 of a group do activity 1 is the group likely to become aggressive if 50 of a group do activity 1 is the group likely to become aggressive if 75 of a group do activity 1 is the group likely to become aggressive if 25 of a group do activity 2 is the group likely to become aggressive if 50 of a group do activity 2 is the group likely to become aggressive if 75 of a group do activity 2 is the group likely to become aggressive and so on however there are too many possible permutations so i don t think this is the right approach so i m unsure how to tackle this i d be very grateful for any thoughts and ideas
qp8897,0,open nsfw 2 tensorflow 2 implementation of the yahoo open nsfw model detecting not suitable for work nsfw images in particular pornographic images is a high demand task in computer vision the yahoo open nsfw model originally developed with the caffe framework has been a favourite choice but the work is now discontinued and caffe is also becoming less popular this open nsfw 2 project provides a tensorflow 2 implementation of the yahoo model with references to its previous third party tensorflow 1 implementation please take a look
qxjr94,0,scikit learn feature selection using ai hi i just want to let you know that the sklearn genetic opt version 0 7 0 is now available it implements feature selection using evolutionary algorithms it uses a multi objective function to optimize the cross validation score while minimizing the number of features used it s compatible with any sklearn classifier or regressor let me know if you have any question or suggestion this new feature is compatible with all the callbacks tensorboard and mlflow for example using the progress bar callback you can check here the docs if you would like to contribute or to check the implementation here is the repo
qquog8,0,good advertising papers using rl or dl hey everybody i m currently working in adtech and i m searching for innovative products in advertising ctr prediction digital inventory pricing online learning for costumer segmentation or other stuff if you can help me please suggest me a good paper in the comments or where i d be able to find good papers in those subjects i ve looked into fb research and arxiv mainly
rsjil0,0,a pretty extensive 6 part blog series on ai accelerators article i found this to be a pretty extensive and solid resource on the dizzying array of specialized hardware we see nowadays in ml
r76igz,0,discussion rant most of us just pretend to understand transformers i see a lot of people using the concept of attention without really knowing what s going on inside the architecture and why it works rather than the how others just put up the picture of attention intensity where the word dog is attending the most to it people slap on a bert in kaggle competitions because well it is easy to do so thanks to huggingface without really knowing what even the abbreviation means ask a self proclaimed person on linkedin about it and he will say oh it works on attention and masking and refuses to explain further i m saying all this because after searching a while for eli5 like explanations all i could get is a trivial description
qv0jg1,0,does getting into top4 stanford cmu mit berkeley ml phd program requires more than 3 first author papers at top conferences i was reading this post and one of the comments mentioned that 3 first author ml papers in top conferences 1 spotlight or oral paper co organized a conference workshop met with the pis professors before applying masters or double bachelors major in cs statistics or math from a top 20 school the candidate should satisfy at least 3 4 of the above criteria to get into the top 4 ml phd programs is this true is it really this hard i mean if a person satisfies 4 of the aforementioned criteria doesn t he or she can just get a phd right away also is it much harder for international students compared to us students given that the international student graduated from american university
qznqdl,0,microsoft asiaâ€™s swin transformer v2 scales the award winning vit to 3 billion parameters and achieves sota performance on vision benchmarks microsoft research asia has upgraded their swin transformer with a new version featuring three billion parameters to train images with resolutions up to 1 536 x 1 536 and advance the sota on four representative vision benchmarks here is a quick read microsoft asiaâ€™s swin transformer v2 scales the award winning vit to 3 billion parameters and achieves sota performance on vision benchmarks the associated code will be available on the projectâ€™s github the paper swin transformer v2 scaling up capacity and resolution is on arxiv
qn1erq,0,in theory could we make a code that bridges different music genres i know little to no about coding but i have been playing with vqgan recently and this idea has popped into my head in theory could we write an ai code that gets feed stems that is each instrument track as a separate audio file for two different songs and rebuilds the first song in the style of the second one the code would look for relevant information from the first song such as melodies rhythms structure and maybe even allow the user to feed in the lyrics to the vocals then it would study how these elements are employed in the second song and rebuild the first song using the second song s style this code could rebuild a rock song as a rap song for instance
r8zwc5,1,what deep learning model would work for this kind of string translation task if i have a dataset which looks something like this nlvpmvatv casspvtggiygytf glctlvaml csardgtgngytf nlvpmvatv casrpdgretqyf nlvpmvatv cassetgfgnqpqhf nlvpmvatv casslapgatneklff where the thing before the comma is the input and the thing after is output what would be good dl model to learn this kind of structure the data is always in capital letters and doesnt have any special characters i think a character level model would made sense here but i really have no idea what to start with does anyone have any experience with similar task of string translation anything would be helpful thanks
rl4nqz,0,why is deep learning regarded as black box why do people continue to insist that we don t know how it works or why it does what it does genuinely curious multiple papers i ve read over the past year make the claim that a clear understanding of how dnns work and why they work is yet to be achieved perhaps it is ignorance but i view these claims incredulously a dnn is a giant logistic regressor made up of smaller logistic regressors and it is fitting a curve there is absolutely no magic it is actually very mechanical and borderline mundane so why the proliferation of magical claims professing our ignorance what exactly are all these authors referring to for reference though foundation models are based on standard deep learning and transfer learning their scale results in new emergent capabilities and their effectiveness across so many tasks incentivizes homogenization homogenization provides powerful leverage but demands caution as the defects of the foundation model are inherited by all the adapted models downstream despite the impending widespread deployment of foundation models we currently lack a clear understanding of how they work when they fail and what they are even capable of due to their emergent properties a foundational model here is literally a model such as those in the imagenet zoo used for transfer it s got a vast diversity of feature representations the layers responsible for highly fundamental and therefore transferable features have converged closer to some effervescent global optima it is a very big regressor like what is the mystery here am i just stupid
qpbci5,0,hubert how to apply bert to speech visually explained recently facebook ai released hubert a bert like model for learning powerful speech representations at first glance this model looks similar to wav2vec 2 0 but the training process objective is actually very different i made some detailed illustrations to visually explain the pre training process of hubert and how it compares to wav2vec 2 0 both of these models are already available in the huggingface transformers library hope this is helpful
r1kzu7,0,how to build a knowledge graph with neo4j and transformers knowledge graphs are essential for information extraction in this article we show how to build a knowledge graph from job descriptions using fine tuned transformer based named entity recognition ner and spacyâ€™s relation extraction models enjoy the read and if you have any questions leave them below
r65cbg,1,mlb came out today to say that 2 different types of baseballs were used last season what type of clustering algorithms would you use to determine which pitch sequence contained which type hi just wanted to share a question i thought would be interesting and discuss with the subreddit how this might be accomplished this is an unsupervised learning problem but it s much tougher than most you do have a definitive answer for number of clusters at least but you have a mostly small sized varying length pitch sequence to make a determination with and have a few variables to account for such as pitchers and which stadium you re at
r2ruzj,1,how to properly structure training data hello i am trying to fit a neural network with training data but keep getting errors like this when reaching the fit function valueerror failed to convert a numpy array to a tensor unsupported object type list i don t understand why i get this error when fitting because as you can see from the official keras documentation here the fit function accepts lists and arrays i have tried to pass both but always get errors passing lists gathering all trainig data model sequential model add embedding dictionnary size 120 input length nr of training vectors model add spatialdropout1d 0 4 model add lstm 176 dropout 0 2 recurrent dropout 0 2 model add dense 1 activation sigmoid model compile loss binary crossentropy optimizer adam metrics accuracy print type type traininginput print type 0 type traininginput 0 print type 0 0 type traininginput 0 0 print n print type labels type traininglabels print type labels 0 type traininglabels 0 print type labels 0 0 type traininglabels 0 0 model fit traininginput traininglabels epochs 5 batch size nr of trainig vectors verbose auto prints type class list type 0 class list type 0 0 class int type labels class list type labels 0 class list type labels 0 0 class int valueerror failed to convert a numpy array to a tensor unsupported object type list i have tried converting the data like this as well model sequential model add embedding dictionnary size 120 input length nr of training vectors model add spatialdropout1d 0 4 model add lstm 176 dropout 0 2 recurrent dropout 0 2 model add dense 1 activation sigmoid model compile loss binary crossentropy optimizer adam metrics accuracy traininginput tf constant traininginput dtype tf float32 traininglabels tf constant traininglabels dtype tf float32 print type type traininginput print type 0 type traininginput 0 print type 0 0 type traininginput 0 0 print n print type labels type traininglabels print type labels 0 type traininglabels 0 print type labels 0 0 type traininglabels 0 0 model fit traininginput traininglabels epochs 5 batch size nr of trainig vectors verbose auto but this leads to the following error valueerror can t convert non rectangular python sequence to tensor lastly i have tried giving all my input training vectors the same length because the vectors are a series of integers and have varying lengths for i in range nr of trainig vectors traininginput i traininginput i 10 model sequential model add embedding dictionnary size 120 input length nr of training vectors model add spatialdropout1d 0 4 model add lstm 176 dropout 0 2 recurrent dropout 0 2 model add dense 1 activation sigmoid model compile loss binary crossentropy optimizer adam metrics accuracy print type type traininginput print type 0 type traininginput 0 print type 0 0 type traininginput 0 0 print n print type labels type traininglabels print type labels 0 type traininglabels 0 print type labels 0 0 type traininglabels 0 0 model fit traininginput traininglabels epochs 5 batch size nr of trainig vectors verbose auto but this again leads to issues type class list type 0 class list type 0 0 class int type labels class list type labels 0 class list type labels 0 0 class int valueerror failed to convert a numpy array to a tensor unsupported object type list at this point i am not quite sure anymore of how i have to structure my vectors in order to not have issues any more what do i have to do at this point
ri20hq,0,why do good research ideas fail hi ml peeps i did a write up on the major lessons i learned doing ml research at google and pathai including my time working with samy bengio and ian goodfellow the main questions i m curious to answer are 1 why do so many good seeming ideas fail and 2 what should we do about it so i m curious to know y alls answers on those questions too
qqfe6y,0,how to train gans really fast projected gans converge faster explained 5 minute summary by casual gan papers despite significant progress in the field training gans from scratch is still no easy task especially for smaller datasets luckily axel sauer and the team at the university of tÃ¼bingen came up with a projected gan that achieves sota level fid in hours instead of days and works on even the tiniest datasets the new training method works by utilizing a pretrained network to obtain embeddings for real and fake images that the discriminator processes additionally feature pyramids provide multi scale feedback from multiple discriminators and random projections better utilize deeper layers of the pretrained network full summary blog post projectedgan upd i originally included the wrong links arxiv code subscribe to casual gan papers and follow me on twitter for weekly ai paper summaries
qpzewi,0,ml datasets for commercial use hi all there are a ton of datasets to ml researchers stemming from different areas but when looking more closely the vast majority of them have very restrictive licensing only allowing to be used for research purposes but not in a commercial environment i am now wondering what the strategies are for obtaining high quality datasets for commercial purposes so let s say i want to build a car object detection model for my company one of the most well known detection use cases i can neither use any of the public datasets as they do not allow commercial usage nor can i use any of the pre trained models for this task as they have been trained on these datasets i could now collect my own data pay crowd annotators to annotate the data buy data i would be specifically interested in the last point is there a way to acquire these types of datasets is there a market for it how are other handling this in their companies
rjybto,1,regularization techniques for beginners l1 and l2 penalties dropout and layer normalization i hope that these posts are helpful to someone who wants to understand how these regularization techniques work l1 and l2 regularization dropout layer normalization part 1 layer normalization part 2 you can join me on youtube and reddit
rhod30,0,project deepmind s perceiver io available through hugging face deepmind s perceiver io the first transformer based neural network that works on all kinds of modalities text images audio video point clouds was added to hf transformers blog post example notebooks x200b i wonder if someone has real life experience working with this one and what are the consideration of choosing it over other dedicated models unless the use cases involves a mix of modalities
r6k36f,1,finding big data sets hello i m a newbee trying to make an elearning or ehealth website and i want to some big data sets to implement machine learning ideas into the website where do i find such data sets and which is easier to find data for elearning or ehealth and i d appreciate some machine learning suggestions if you guys have any
qvc3cb,0,how do you create new neural architectures blocks or layers i keep using the models that others invent and come up with how can i learn how to create my own type of layers or blocks how did they come up with squeeze and excitation layers in efficient net for example where do i start
rmewsq,1,linux desktop windows laptop for data science student i m starting a data science masters program in january i ve got an hp z840 desktop dual 12 core zeon 256gb ram and an quadro m5000 along with a gen 2 thinkpad x13 i m thinking of putting linux ubuntu or mint on the z840 and and keeping windows on the x13 for when i need office or windows specific software thoughts on this plan seems like a great combo to me but looking for some other thoughts on the setup for running data science workloads locally i also have at my disposal my work system with unlimited aws access for larger projects if i need it
r7n8k8,1,recommendations for creating a nlp text harmonization algorithm hello looking for recommendations tips on how to solve this issue hopefully it s allowed here this same question was deleted in stackoverflow and i don t know where to ask for help x200b let s say i have this set company kpmg usa kpmg europe microsoft corporation microsoft dhl global forwarding dhl express but i want to harmonize company names to get a result like the following x200b company company harmonized company kpmg usa kpmg europe kpmg microsoft microsoft corporation microsoft corporation dhl global forwarding dhl express dhl x200b this problem can be segmented into two core problems 1 matching fuzzy matching 2 harmonization providing the cleanest and most representative name for the matching part i know there are several pyhton packages to fuzzy match through tokenization algorithms such as python record linkage among others for the harmonization part i have no clue what is available seeking recommendation for this part additionally i am looking to implement a form of supervised nlp algorith so that it can learn with time how to output the best harmonized name and how to match correctly seeking recommendations for this part as well i already have a large dataset with around 1m rows and i can get more if needed with examples like kpmg usa kpmg
qvm71a,0,do big tech companies do coding interviews for ml ph d research student scientist intern positions i just want to know if i need to practice on leetcode
rav68s,1,tqdm and printing info x200b so everytime i print new progressbar is created i tried position 0 leave true is there way to stay with one progresbar
rtrbso,0,neuron outputs as weights
r8bbsb,1,requesting help for my situation hello i am an undergraduate researcher in my senior year of my computer science and engineering degree the research is focused mainly on computer vision i am currently working with pose estimation image segmentation and object tracking i have been working on this research for about 3 months i have had enough machine learning background to complete my tasks so far but the last 3 months have made me realize i have some gaps in my knowledge about 2 years ago i completed a deep learning course on udacity using pytorch it went through all of the basics for machine learning including a lot of the essential mathematics however i didn t apply what i learned for awhile after the course was over and i m a bit rusty on some of the details additionally i took an autonomous vehicles course at my university which included some basic introductions to computer vision i have also learned a lot over the past 3 months basically i m comfortable doing things like utilizing existing models to solve a specific problem and tuning models hyperparameters structure to boost performance however the mathematics and theory behind what i m doing is fuzzy in my memory and i would really like to rebuild my understanding from the ground up next semester i will be required to do some more advanced work using computer vision techniques to either fine tune an existing animal behavioral analysis network or finish building one that has already been started that will most likely benefit from some additional learning on my part and i would like to use my winter break a little over 3 weeks to do it my question is how can i best utilize this time i know 3 weeks isn t a ton of time but i want to be as efficient as possible with the time i have i know the coursera stanford machine learning course is highly recommended however i am concerned it is too broad introductory for my case as it covers a decent amount of topics i am already comfortable with on the other hand there is a good chunk of information in that course that will be great to fill in the gaps in my knowledge would it be better to do a more advanced course that focuses on computer vision specifically even though some of my fundamentals are missing rusty thanks
rxj5d6,1,looking into the black box of a neural network hey guys i ve recently started working on a research project analyzing a cancer prediction algorithm and was hoping to get y alls advice the algorithm is described in this paper but effectively it uses a cnn on amino acid sequence data from t cell receptors to determine whether they are responding to cancer or not this algorithm performs remarkably well even when public t cell receptors are removed which indicates there s some biochemical difference between cancer and non cancer t cell receptors my responsibility is to analyze the neural net and determine what specific features are heavily weighted in determining the difference between cancer and non cancer t cell receptors hopefully this leads us to the specific biochemical difference i m a bit lost as to where to start with this however how would y all go about looking into this black box any advice would be much appreciated
ruja9s,0,machine learning wayr what are you reading week 128 this is a place to share machine learning research papers journals and articles that you re reading this week if it relates to what you re researching by all means elaborate and give us your insight otherwise it could just be an interesting paper you ve read please try to provide some insight from your understanding and please don t post things which are present in wiki preferably you should link the arxiv page not the pdf you can easily access the pdf from the summary page but not the other way around or any other pertinent links previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 121 130 week 1 11 21 31 41 51 61 71 81 91 101 111 121 week 2 12 22 32 42 52 62 72 82 92 102 112 122 week 3 13 23 33 43 53 63 73 83 93 103 113 123 week 4 14 24 34 44 54 64 74 84 94 104 114 124 week 5 15 25 35 45 55 65 75 85 95 105 115 125 week 6 16 26 36 46 56 66 76 86 96 106 116 126 week 7 17 27 37 47 57 67 77 87 97 107 117 127 week 8 18 28 38 48 58 68 78 88 98 108 118 week 9 19 29 39 49 59 69 79 89 99 109 119 week 10 20 30 40 50 60 70 80 90 100 110 120 most upvoted papers two weeks ago u catalyzex code bot paper link u rakshith291 u rakshith291 besides that there are no rules have fun
rqycm4,0,elon musk talking to lex fridman about ai at tesla this is the third interview and there are a few interesting points about how they manage the training and inference of neural nets at tesla some of the points are repeated from other talks like how its all dot products all the way down and repeatedly saying vector space some high level topics the term hydranet multi headed prediction network is not used any more and they have some new type of architecture large focus on the extensive code written in c c to reduce the inference time especially on device this point is mentioned many many times the risk of jitter volatility of inference time and how that can affect the robustness of the overall system retraining the networks based on raw image data instead of processed image data also a small comment that some of the networks they use were built on old hardware and the process of updating new versions of the networks will rely on multiple layers of neural networks instead of some heuristics this implied as they move closer to version 11 that there has been a large redesign distilling giant bags of points or outputs of models that create inferences on multiple outcomes back into something digestible
rbq46a,0,knowing how features affect the target variables in the neural network i am currently working on blood glucose forecasting using lstms of course insulin decreases blood glucose and carbohydrates increase blood glucose is there any way to feed this in the network i am using keras and simply inputting insulin and carbs as an array in the network but i know for certain how each feature will affect the forecast i e negative and positive correlation how can i incorporate this is the network i am sure this isn t specific to my problem in most ml prediction forecasting problems we would know if a particular feature increases or decreases our target variable how can we utilise this information
rc56gb,1,how to work with data that updates daily i am currently working an ml portfolio project where the data updates daily all of the projects i have worked on to date have been with flat datasets where the data isn t live or updating so the model building has been pretty vanilla i wanted to challenge myself and start pulling data from an api where the data is in fact being updated for example i have a status feature that contains values like received in review approved appealed and closed which updates at the end of the day i should note that frequency of updates is very small some values can stay in their current status for months on end my original plan was to build the model one time and deploy it in a streamlit app but after thinking it over this seems flawed as the model won t necessarily account for changes in the data and the performance might drop over time as the updates are being made what would be the easiest way to handle this
rq6hvs,0,open source desktop app to read display and export car sensor data i made this app with electron js framework and python in the back end using the python obd library to communicate with the car then i can export my data in a neat csv file for machine learning tasks i only made this because all the other apps cost like 20 bucks and aren t customizable if i wanted to create some kind of a pipeline within the app i haven t tested it on other devices a lot yet but it is only supported on windows and with cars that support obd ii protocol which is most cars made after 1996 you can read more on that online though my car in the video is a kia forte this also gave me practice with application development and paves the road for me to do machine learning with car sensor data i hope it works for other people also and allows them to tinker with ml demo video is here code and some instructions are here p s give it a star on github if u like it
rd8wum,0,how to make the best out of neurips i will be starting to work on my phd project in a few months i am already enrolled but the first months are devoted to looking for a supervisor and phd project i am attending neurips and i would like to make the best out of it the amount of papers events etc is daunting for someone who s just starting what recommendations do you have for me and for people in my situation
rmuptq,1,what language to write a conversational ai so basically my ultimate goal would be to have my ai able to hold a conversation know what the words mean and answer while maintaining context even if the topic isn t clearly stated in each message also to have some amount of memory i e remembering names and relationships like who my mom is or that i have an so with a name i don t know any languages other than a working knowledge of html css and javascript but i don t think that s really helpful here so i m interested to know what language s i should learn because i m determined to make my brainchild a reality thank you in advance
rlgrnt,1,high school student doing ml let me just start by saying that i have no idea how to do what i m trying to do i have to do a research project for one of my high school classes and my grade is really taking a beating because of this issue so basically i m super new to ml and i have a research project where i m trying to predict solar flare activity using ml basically what this guy has done i just don t know how to convert a large dataset in fits format all to csv and preprocess it if you go to the first link there s a section called getting my hands on data and data preprocessing basically i don t know how to do either one and i ve been stuck for like 2 weeks i met with my teacher today and he s visibly pissed that i haven t made much progress i m also not that good at programming to make matters worse i know it s a lot to ask but can anyone help me i have three days before i either terribly fail or do well in the class
r3cvu4,1,do you have difficulty keeping up with new research and applications in ai ml and your to read list just gets bigger easy fix do you have difficulty keeping up with new research and applications in ai ml and your to read list just gets bigger easy fix follow my channel or blog where i explain new applications papers in short videos and articles weekly i hope it can be useful to you and let me know your thoughts on the content so i can improve and better answer this need
rey855,1,how to make a dataset i ve been progressing along on my journey and i ve been wandering for a while about how do people come up with their datasets i know for image recognition it could be a simple as collecting images of a certain class that you want but let s say something like style gan how do people go about creating the dataset and how do people know that their dataset will work
rptzd6,0,project idris and xla linear algebra and probabilistic modelling w dependent types in june i announced i d started work on a probabilistic modelling library in idris this post is to announce the first major milestone basic linear algebra in idris backed by xla right now this is only addition but other ops are easy to add from here what s the project mission well it s evolving but roughly we ve seen a number of impressive numerical computing projects some lead the pack in performance while others leverage advanced language features and theory for expressive apis with this project we hope to explore both some highlights design including user friendliness is paramount features come second dependently typed tensor shapes are verified at compile time expect to support gpu in future and possibly other accelerators xla for competitive performance see the comments for more detail
qsebrh,0,cfp ai for design and manufacturing workshop adam aaai 2022 deadline extended hello r machinelearning the deadline for submitting to the ai for design and manufacturing workshop adam aaai 2022 has been extended for a week due to multiple requests if you re working in the intersection of ai and design manufacturing scientific computing and geometric modeling do consider submitting a 4 page workshop paper we invite paper submissions on the following and related topics new theory and fundamentals of ai aided design and manufacturing novel ai based techniques to improve modeling of engineering systems integration of ai based approaches with engineering prototyping and manufacturing novel methods to learn from scarce sparse or heterogenous or multimodal data novel ml methods in the computational material and physical sciences novel ml accelerated optimization for conceptual detailed system design novel ai enabled generative models for system design and manufacturing ml guided rare event modeling and system uncertainty quantification development of software libraries or benchmark datasets and identification of key challenges and opportunities for future research workshop website submission website submission deadline november 19th 2021
qm578v,0,python toolboxes for probabilistic graphical model inference hi folks which libraries or toolboxes would you recommend ideally based on personal experience for performing inference in probabilistic graphical models for an actual practical application and not for academic toy examples minimal requirements must be able to specify a bayesian network or factor graph consisting of categorical nodes some of which are hidden and others observed and use a set of observations to identify the factors dependencies i think essentially any pgm toolbox will fulfill these requirements bonus points given for good documentation maturity stability works efficiently with many factors and many datapoints implements many different inference methods and modeling paradigms simplicity of use i do know of a few promising toolboxes such as pgmpy pymc3 and pyro but have not used either of them for this purpose and am at a bit of a loss picking one to start with
rgzza1,1,the colonel trick x200b
rf08z6,1,ml beginner looking for the best course direction to find catalogue the few audio matches similarities in a large database my searches have been getting railroaded by nlp which seems unnecessary for my purposes i have a large database of audio files 8 to 30 seconds and i m looking to pull the ones that match certain words or phrases i don t need the ml to understand the words or phrases i think i just need it to match within an acceptable level of error the sounds currently it s pure human listening going through a per audio basis the per audio time down to about 3 5 seconds to determine if it s a match but over the course of thousands of audios not only is it inefficient it s also mentally exhausting i m running into so much nlp that i m having a hard time finding audio processing and a harder time finding a niche in there i know little about ml but after getting an extremely basic understanding i figured i d ask any help is much appreciated i am hoping to do this in python as it is the most known language but am not unwilling to go a different route if it fits my use case
rppsrl,0,the new cppe 5 dataset this paper introduces the cppe 5 medical personal protective equipment dataset a new challenging image dataset with the goal to allow the study of subordinate categorization of medical ppe gloves masks coveralls face shields goggles unlike any other existing dataset furthermore you can easily get started to use this dataset with the tutorials and data loaders in the code repository or use one among the models from the model zoo for this dataset supporting links code repository paper
qnxu7h,0,is cvpr really only for computer vision i know cvpr literally means computer vision and pattern recognition but does it really means that any submission in the ml field with no direct link with computer vision will be discarded thanks
rgo7s6,1,recommendation for affordable ml courses with certificate hello guys hope you all are doing great i would like recommendations for ml beginner courses that are affordable and give certificates by affordable i mean less than us 100 i was really looking forward to the andrew ng stanford ml course on coursera but i can not for the life of me use that platform it won t accept any of my credit cards and there is no costumer support whatsoever thanks very much for the help
r9sm6d,1,ml project help hi i am currently taking a theory of computation course in my uni and had a question on ml on my assignment i do not understand any part of it so if any of you has little bit of time and could help me out would appreciate it
rrxtnj,1,question about ml approaches hello guys a bit of context i am currently searching for a way to estimate the sex m f and he age slice kid young adult adult old of a person based on a full body picture with mask 80 success would be enough i have found many approaches giving a sufficient result analyzing the face without mask but based on the current situation this is out of the question the result on masked people are obviously terrible question are there existing repositories out there solving this particular problem and if there are not as i am a beginner do you believe that problem could be solved by training a model i e will i be able to get a working solution after learning how the whole process works
qwt9ni,0,graph neural networks through the lens of differential geometry and algebraic topology differential geometry and algebraic topology are not encountered very frequently in mainstream machine learning in a new series of posts i show how tools from these fields can be used to reinterpret graph neural networks and address some of their common plights in a principled way first post in the series introduction x200b part ii will discuss the expressive power of gnns and topological message passing part iii will deal with geometric flows and non euclidean diffusion pdes on graphs part iv will show how the over squashing phenomena can be related to graph curvature and offer a geometric approach to graph rewiring inspired by the ricci flow
rjjaht,1,how to start working in dl rl hello first of all sorry if this kind of post is not allowed i m a web developer currently trying to start my journey as a machine learning engineer and specially in deep learning reinforcement learning i would like to be a freelancer but for the purposes of starting i m ok with being an employee i ve already done 2 courses one formal and the other one via youtube and been doing practices since 2 years i am right now doing competitions in www kaggle com what would people already working in this niche recommend to successfully insert myself any guidance would be appreciated thanks
qwzhbb,0,colab tpu low performance i wanted to make a quick performance comparison between the gpu tesla k80 and tpu v2 8 available in google colab with pytorch to do so quickly i used an mnist example from pytorch lightning that trains a simple cnn for some reason the performance on tpu is even worse than cpu gpu 52 it s tpu 9 it s cpu 13 it s here is the colab notebook can those results be legit maybe small conv kernels are not suitable for tpu or is this an issue of pytorch xla or lightning any suggestions
rjdy4s,0,discussion should there be a new ide built from the ground up for ml engineers practioners there are a lot of plugins for things like vscode and folks who use pycharm and the like i ve even found some interesting early projects like this focused on making ml engineering easier curious to hear from the community what might an ide built purely for ml engineers from the beginning look like should something like this exist i m someone who has been working in this space for a while and there are so many tools that i keep coming back to this idea of creating an ide just for this space
r4yvl3,1,which is the best beginner book for machine learning which is the best beginner book for machine learning with python something that s not super rigorous but also doesn t gloss over details and covers almost everything for a beginner along with implementing machine learning algos
rm7kxf,1,data scientist vs machine learning engineer hi everyone i didn t know of an appropriate forum to pose this question on so apologies of it is out of context i have received offers from 2 companies offer 1 data scientist at a big oil and gas corp the job profile involves research in process mining offer 2 machine learning engineer at a popular analytics consulting firm the profile involves deploying machine learning and deep learning models using kubernetes heroku dask etc both options are at my choice of location and offer 2 is also providing me a fully remote workplace option offer 1 pays better but leaving aside the pay i am confused about which one i should go ahead with
r9cz9r,1,reproducing webnlg challenge 2017 on opennmt py hi guys i m data science student and i m learning to use opennmt py for my master degree thesis i reproduced the challenge with the old deprecated repository now i would like to replicate it with the updated repository as i will need it for a similar task within my thesis i am now approaching the nlp field but i am not very clear about some things since it is not a translation task is it necessary to build a vocabulary like in the machine translation opennmt py tutorial the epochs command i noticed has been deprecated now it works with train steps however i am not clear about the conversion so to speak with the old repository the number of epochs to train the model with was 13 i tried this by looking at old problems from these repositories default train steps 100000 deault batch size 64 13 epochs number of the old repository 20313 is this reasoning correct thanks everyone for your attention
r1lspw,1,best way to plot soft clustering models in python hi guys i m in a machine learning course and we re doing an assignment on hard vs soft clustering algorithms in particular k means vs fuzzy c means i m at the point where i want to plot my clusters k means is easy because it s either part of the cluster or its not so i can set colors on the data points in matplotlib accordingly however for the c means it s possible for clusters to overlap i could just assume it s part of the cluster with the largest weight but i feel like that is removing possibly important information about how things are clustered and would end up producing a similar plot to the k means are there any good ways for plotting soft clustering algorithms in python
rbabt8,1,how do you deal with multiples dataframes of the same structure but with different values over the years i m trying to learn more about data science from the too basic courses i took during my past bachelor s degree in computer science until now i have mainly worked with already strucured datasets that i found in various places i always cleaned these datasets with pandas and then applied supervised or semi supervised methods just by manipulating variables names the structure of the dataframes has always been the same for example x200b id gender country age 0 242 m brazil 40 n 815 f canada 38 x200b so i decided to challenge myself by building my own dataset with indicators over the years i found from different free databases after a first batch of headaches with data processing and data cleaning i finally managed to obtain two dataframes with the same structure years as index and names as column headers except for the values which are different one indicator per df here is an example first dataframe with weight name 1 name 2 name name n year 1 138 129 185 130 year year n 174 155 134 220 second dataframe with size name 1 name 2 name name n year 1 49 51 49 55 year year n 62 61 59 64 x200b i don t have the impression that a merge or a concat are possible since i have almost 10 000 individuals over about 50 years with the names of the columns as labels of the individuals themselves where i block is how to obtain same kind of structure that in the first exemple in order to apply some ml methods as usual i have turned the problem around in several directions and i must have made a mistake somewhere but without managing where exactly yet i dare to assume that this problem must be common in data science unless i am wrong how can you perform an analysis with python in this case
rprmq3,0,sentencepiece wordpiece bpe which tokenizer is the best one there are several popular tokenization algorithms that i frequently encounter byte pair encoding sentencepiece wordpiece and less often unigram the title is formulated somewhat provocatively and i assume there is no single best algorithm between the candidates but what are the key differences and situations where one might be preferred over the others
rgksre,1,how to choose an algorithm this was something that i was thinking when should we choose a neural network xgboost or random forest algorithm for an ml problem sorry for the noob question i am trying to pick up machine learning and this was a question i had
r9qmhc,1,machine learning equivalent of deep learning book goodfellow i came across goodfellow et al s deep learning book and am wondering if there is a machine learning equivalent to this book any suggestions thanks in advance
rkdxfs,0,100x faster nerf explained plenoxels radiance fields without neural networks 5 minute summary by casual gan papers every now and then comes along an idea so pertinent that it makes all alternatives look too drab and uninteresting to even consider nerf the 3d neural rendering phenomenon from last year is one such ideaâ€¦ yet despite the hype around it alex yu sara fridovich keil and the team at uc berkley chose another approach to focus on perhaps surprisingly without any neural networks at all yes you are still reading a blog about ai papers and even more surprisingly their approach coined plenoxels works really well the authors replace the core component of nerf the color and density predicting mlp with a sparse 3d grid of spherical harmonics as a result learning plenoxels for scenes is two orders of magnitude 100x faster than optimizing a nerf and there is no noticeable drop in quality whatsoever crazy yeah letâ€™s learn how they did it full summary blog post plenoxels 100x faster nerf arxiv code subscribe to casual gan papers and follow me on twitter for weekly ai paper summaries
rb06pu,0,locally deployed model management software so i need to deploy and manage multiple computer vision models get the models as input deploy them as apis so that it is accessible by another app on the same computer so i don t really need a cloud service like aws for a few reasons it will get expensive quickly the data i m using is sensitive security camera video so it better if it doesn t leave the computer the computer i m running this on is powerful enough to run the models faster response time since i won t have to use the internet everything is in place doing the deployment each time manually can get very hectic very easily so my question is is there a software solution where i can manage my models and get api links for them for local usage or should i try to do it myself
r8w3zc,1,how does a neural network pipeline look hey in my third year of cs ug and decided to learn ml and neural networks and make cool projects as it will also help with my cv i started with classical ml algorithms like linear regression logistic reg and other classifying algorithms i learnt the math behind them which i enjoyed like how gradient descent works why scale your features etc and can implement them using sklearn no problemo but i absolutely hate the feature engineering part i find it very mundane and boring maybe because i don t know where to start or becuz i suck with plotting graphs i m not sure i tried to do a kaggle competition on titanic and got an accuracy of about 80 mostly becuz i did not chose the features properly so given i don t have much time i signed up for the andrew ng s deeplearning ai first course and i quiet like it i find it really cool how matrices that i learnt in highschool are put to use to do stuff efficiently among other things also i maybe wrong but i find it more straightforward than machine learning where you have all these models to chose from and here you have only one thing nn so my question is does working with nn need good feature extraction like ml for example image detection or stock market prediction if im talking about harder projects i feel like it is important since feeding my model with redundant info may slow the training process or even result even worse accuracy am i correct if there is no way out of the feature extraction hell how do i become decent at it also it ll be helpful of you guys can give a top view of a nn pipe line edit any constructive criticism on my naivete is appreciated as long as it steers me the right path
rorax0,0,can i use my own images for vqgan clip generation i am a newbie to machine learning i am now using google colab to play with the vqgan notebook i can t code but can understand a bit i am wondering if i can modify the notebook and use my own set of images to generate images let s say i would like to generate a fashion portrait with my wardrobe styling and model can i take thousands of the wardrobe details and the model and then put them into the gan and generate an abstract image if yes how x200b bunch of thanks
rnr1vu,1,i need some guidance i have been into computer science for a couple of years now 3 to be precise and till now i was just wandering here and there trying every field web dev game dev app dev data science etc now i know that i want to make a carrier in data science i don t know much math as i am just in 10th grade i have prior programming experience with python c and javascript can anyone give me any good quality free learning resources for math i am currently learning statistics from the book think stats
qqdkeo,0,gpt 3 in the style of shel silverstein i ve recently been playing with the openai api beta for gpt 3 and used their fine tuning api to create a model that has been fine tuned on a corpus of all of shel silverstein s poetry the resulting model generates whimsical poems based on a prompted title here s one example i liked walking on a whale i am walking on a whale i feel it move and swell i feel the mist come in and float i feel the rain and the cold but i don t mind at all it s just like walking on the ground while the model is quite good at understanding semantics and even has a flair for the vaguely metaphorical it didn t pick up any notion of rhyming it seems like this would be a hard thing for the model to learn given the training data it s seen since only very rarely does the rhyme scheme affect the conditional likelihood of a word question for any commenters if you had a rhyming dictionary where you could simply look up whether two words rhyme instead of trying to infer this as some latent attribute how could you update a large nlp model like this to take into account such declarative knowledge more examples and discussion here
rud2m5,0,tensorflow keras implementation of vision transformer an image is worth 16x16 words vit excellent results compared to sota cnns while requiring fewer computational resources to train paper code
rcgkhi,0,who owns the rights to images produced by an ai thought about this when using an ai art creator if you used a text to image ai like starryai or neuralblender who owns the rights to the image it creates do you own it since you made the prompt or does the ai s developer own it since they made the ai or since the ai was trained on pre existing images do the owners of the intial images have the rights
r8gbid,0,iclr 2022 possible competing bias review my paper got 6663 the reviewer who gave 3 kept comparing our paper to a concurrent work posted on arxiv only a few days before the iclr deadline and asked us to cite acknowledge explicitly compare with this concurrent work in our paper is it common how likely is this reviewer the author of that competing concurrent work this reviewer also delivered a lot of deliberate attacks during the reviewing process we were really frustrated what can we do under this situation update pc responded to that review
rwo3ia,1,messing up with cnn to convert grayscale to rgb hello fellow learners you might have seen old black and white photo being colorized but have you ever tried to do it it can be done with a simple architecture of auto encoder by tweaking colorspaces you can read more about converting grayscale image to rgb here please leave some feedbacks to improve content quality thank you ðŸ™‚
rpmcot,1,collection of free python courses hi guys i collected for you some free python from udemy hope that can help every one to start learning python for data science â€“ great learning an introduction to python programming learn python 3 from scratch python for absolute beginners practical python python oop object oriented programming in python
r3odcu,1,mobile app for machine learning deployment is there a package like streamlit which have pre made templates for mobile application for convenient creation of machine learning apps if any please suggest
r23juf,1,matching pursuit explanation does anybody can explain me the matching pursuit algorithm with all its steps and their meanings
r7o01i,1,swimming thoughts wouldn t it be cool to evolve train deep sea creatures such as literature search openai gym swimmer v2 train the three link fish to swim to a direction fish schooling rl in a navier stokes environment swimming at microscale with rl nice model details
rftozk,1,advice on how to get started hi everyone this is my first post here i am asking for some advice on how to get started with ml i am currently a math undergraduate but i have strong interests in machine learning and statistics i have done some part of the cs231n courses online but havenâ€™t really implemented any â€œrealâ€ model so i am thinking about doing my own project just for fun during the break i have read some blogs online and decided to go for ocr which i know is a bit too much for a beginner but i was hoping to learning much from implementing it also itâ€™s so my first question is are there easier ways to get started second question would it be a good idea to read the papers look at other peopleâ€™s implementation and try it myself any suggestions would be greatly appreciated thanks very much
qstdsm,0,language model rugpt 3 13b is apparently available for download from an english translation the rugpt 3 13b model contains 13 billion parameters and is capable of continuing texts in russian and english as well as in programming languages download page english translation i did not sign up to try to download the file s reference english translation
qqzpf6,0,list of iclr 2022 papers with review scores iclr 2022 reviews are publicly available now we have compiled a list of papers sorted by the review scores weighted by confidence link mean review score is 4 9
qmxns9,0,sagemaker linear learner hi everyone i was just wondering does anyone know where there is a corresponding library in cran or scikit learn for the linear learner in aws sage maker i do not have access to aws so i can t tell whether it is just a interface to different regressions or something more sophisticated enjoy your day fella
relg9r,1,bridging the gap between theory and trying it out in kaggle so i just finished andrew ng s course on coursera in august i also finished a course in my university where i could try out scikit learn and tensorflow with keras filters etc but when i came to kaggle i am at a complete loss of where to start or what to do is there some kind of a bridge that will help me connect the two tbh historically i have been lost at implementing the theory when it comes to practice
qv9rfy,0,best mobile architectures for real time semantic segmentation hey guys recently i ve been working on a project for mobile involving semantic segmentation recently iâ€™m running quantized unet with mobilenetv2 backbone deployed using tflite however the fps performance isn t good enough for real time segmentation what are the best performance wise best fps rates segmentation models to run on mobile currently i ve been researching architectures on but those are very often evaluated on gpus
r89r8r,0,are neurips authors required to purchase a ticket to register question in title first year accepted should i be purchasing a ticket the regular way
rb1rcj,0,uc berkeleyâ€™s sergey levine says combining self supervised and offline rl could enable algorithms that understand the world through actions in the new paper understanding the world through action uc berkeley assistant professor in the department of electrical engineering and computer sciences sergey levine argues that a general principled and powerful framework for utilizing unlabelled data can be derived from reinforcement learning to enable machine learning systems leveraging large datasets to understand the real world here is a quick read uc berkeleyâ€™s sergey levine says combining self supervised and offline rl could enable algorithms that understand the world through actions the paper understanding the world through action is on arxiv
rsbm0e,1,cool projects for college app i ve just finished andrew ng s beginner ml course on coursera i was wondering if you guys had any project ideas that you enjoyed making or that you think might be an interesting challenge i m willing to learn a lot and really struggle on the project so really any idea goes ðŸ˜ƒ thanks
rmcp62,0,why does convolution lead to translation equivariance i asked a similiar question before it is my understanding that translational equivariance is one of the main properties that allows convolution to be so powerful at finding patterns in images however i am having trouble finding concrete proofs for how this happens does anyone have any papers that explains why convolution leads to translation equivariance preferably from a linear algebraic perspective secondly does anyone have any papers that explains why translational equivariance allows convolutional neural networks to be so successful at image recognition or explains how this idea of translation equivariance relates to what is happening at each layer thanks
qr6hhb,0,what is the current state of deep learning theory as i assume many people on this subreddit are i m interested in understanding why currently state of the art models work and if we can use the understanding of the underlying princples of these models to create better more powerful ones i mean this in the sense that physics has theory or classical computer science has one in that both develop a mathematical framework to understand and predict their subject there is of course a difference between them as one assumes laws from experiences and onr is more firmly rooted in math but both still use a mathematical approach i sometimes see some papers on this subreddit that look very relevant to this vmatter and while i try to read some of them given my current math education which is somewhere between a practioneer and a ba student i do not believe i can truly understand and therefore judge them so for those of you who are experts in this do we have an accepted predictive theory for deep learning models if not what do we have
rbl2r4,1,seeking guidance hi i am jay aslaiya i am in second year of bachelor of engineering in computer science branch i want to pursue machine learning as my career and i dont know how to become good in it so i am asking for guidance or road map how to pursue it i know what all things are required to be known in this field but dont know the resources from where to learn so please guide me and it would be great if i get resources suggestions too i know python and know that have to do maths as next step so any resource and guidance will be helpfull thanks a lot
qlcj9r,0,what s the best simple machine learning api service i m looking integrate machine learning into my application for example i want to classify images users are uploading to my site i m aware there s a lot of these saas machine learning companies but i was wondering if anyone here had recommendations as to which ones worked best for them i basically just want to send all my data to a service train a model the be able to call an api to get an answer from the model
r47kvq,0,google research open sources â€˜saviâ€™ an object centric architecture that extends the slot attention mechanism to videos multiple distinct things act as compositional building blocks that can be processed independently and recombined in humansâ€™ understanding of the world the foundation for high level cognitive abilities like language causal reasoning arithmetic planning and so on is a compositional model of the universe therefore itâ€™s essential for generalizing in predictable and systematic ways machine learning algorithms with object centric representations have the potential to dramatically improve sampling efficiency resilience generalization to new problems and interpretability unsupervised multi object representation learning is widely used in various applications these algorithms learn to separate and represent objects from the statistical structure of the data alone without the requirement for supervision by using object centric inductive biases despite their promising outcomes these approaches are currently constrained by two major issues 1 they are limited to toy data such as moving 2d sprites or extremely rudimentary 3d scenes and they struggle with more realistic data with complex textures 2 both during training and inference it is not clear how to interact with these models the concept of an object is imprecise and task dependent and these modelsâ€™ segmentation does not always correspond to the tasks of interest to overcome the problem of unsupervised weakly supervised multi object segmentation and tracking in video data a new google research introduces a sequential extension of slot attention called slot attention for video savi check out the github and paper read the short summary x200b
rlu1bk,1,beginner here need advice review on my pytorch project hi guys i m new to this channel python and machine learning i ve watched tons of videos and read stuff about ml and decided that i want to make a tetris ai here s the problem i started coding my own tetris agent and model in python and used pytorch seemed like a good choice for beginners now the bot is starting pretty strong but it feels like as if it s fixing itself to single decisions in the later runs e g holding right left spinning permanently etc here s my repo code ignore the readme please d literally any help is very much appreciated i want to know whether i did something completely wrong and want to learn from those mistakes greetings and thanks in advance
rn6m8o,0,reinforcement learning with initial policy hi guys happy holidays so i understand the difference between passive rl and active rl is that in passive rl weâ€™re merely learning the values of the states under a policy we feed the agent but in active rl the agent can learn the optimal policy itself whatâ€™s the best course of action if we have an initial policy we think is good but we also want to find if there exists a better policy i thought approximate q learning might be suitable where we initialize the q states in our policy with values 0 and all other q states with values 0 and then set a low epsilon to encourage testing that policy first before increasing it over time to see if there are better policies however this seems kind of backwards because usually weâ€™d want to explore more first starting from a â€œblank slate â€ and then exploit is there another way of going about this perhaps combining with some form of passive rl
qx0enm,0,all bias in ml comes from biased data in this post i am referring to bias in a social sense racism sexism â€¦ and not to bias in a strictly mathematical sense obviously if we train a model on biased data the trained model will have inherited that bias some people say that this is the main or only way that bias finds itâ€™s way into models however assume that imagenet were a biased benchmark which it probably is and most vision model architectures are developed to do well on imagenet this bias could in some way also be inherent to the resulting architectures not just the learned weights am i wrong here if not what are other ways besides biased data that one should be aware of
r5la61,1,valueerror using a target size torch size 64 that is different to the input size torch size 64 2 is deprecated please ensure they have the same size i am trying to build a binary classification model with cnn however getting the above error here is my code class parallel all you want nn module define all layers present in the network def init self num emotions super init transformer block maxpool the input feature map tensor to the transformer a rectangular kernel worked better here for the rectangular input spectrogram feature map tensor self transformer maxpool nn maxpool2d kernel size 1 4 stride 1 4 define single transformer encoder layer self attention feedforward network from attention is all you need paper 4 multi head self attention layers each with 40 512 40 feedforward network transformer layer nn transformerencoderlayer d model 40 input feature frequency dim after maxpooling 40 282 40 70 mfc time nhead 4 4 self attention layers in each multi head self attention layer in each encoder block dim feedforward 512 2 linear layers in each encoder block s feedforward network dim 40 512 40 dropout 0 4 activation relu relu avoid saturation tame gradient reduce compute time i m using 4 instead of the 6 identical stacked encoder layrs used in attention is all you need paper complete transformer block contains 4 full transformer encoder layers each w multihead self attention feedforward self transformer encoder nn transformerencoder transformer layer num layers 4 1st parallel 2d convolution block 3 sequential conv2d layers 1 40 282 16 20 141 32 5 35 64 1 8 self conv2dblock1 nn sequential 1st 2d convolution layer nn conv2d in channels 1 input volume depth input channel dim 1 out channels 16 expand output feature map volume s depth to 16 kernel size 3 typical 3 3 stride 1 kernel stride 1 padding 1 nn batchnorm2d 16 batch normalize the output feature map before activation nn relu feature map activation map nn maxpool2d kernel size 2 stride 2 typical maxpool kernel size nn dropout p 0 3 randomly zero 30 of 1st layer s output feature map in training 2nd 2d convolution layer identical to last except output dim maxpool kernel nn conv2d in channels 16 out channels 32 expand output feature map volume s depth to 32 kernel size 3 stride 1 padding 1 nn batchnorm2d 32 nn relu nn maxpool2d kernel size 4 stride 4 increase maxpool kernel for subsequent filters nn dropout p 0 3 3rd 2d convolution layer identical to last except output dim nn conv2d in channels 32 out channels 64 expand output feature map volume s depth to 64 kernel size 3 stride 1 padding 1 nn batchnorm2d 64 nn relu nn maxpool2d kernel size 4 stride 4 nn dropout p 0 3 2nd parallel 2d convolution block 3 sequential conv2d layers 1 40 282 16 20 141 32 5 35 64 1 8 self conv2dblock2 nn sequential 1st 2d convolution layer nn conv2d in channels 1 input volume depth input channel dim 1 out channels 16 expand output feature map volume s depth to 16 kernel size 3 typical 3 3 stride 1 kernel stride 1 padding 1 nn batchnorm2d 16 batch normalize the output feature map before activation nn relu feature map activation map nn maxpool2d kernel size 2 stride 2 typical maxpool kernel size nn dropout p 0 3 randomly zero 30 of 1st layer s output feature map in training 2nd 2d convolution layer identical to last except output dim maxpool kernel nn conv2d in channels 16 out channels 32 expand output feature map volume s depth to 32 kernel size 3 stride 1 padding 1 nn batchnorm2d 32 nn relu nn maxpool2d kernel size 4 stride 4 increase maxpool kernel for subsequent filters nn dropout p 0 3 3rd 2d convolution layer identical to last except output dim nn conv2d in channels 32 out channels 64 expand output feature map volume s depth to 64 kernel size 3 stride 1 padding 1 nn batchnorm2d 64 nn relu nn maxpool2d kernel size 4 stride 4 nn dropout p 0 3 final linear block linear softmax layer to take final concatenated embedding tensor from parallel 2d convolutional and transformer blocks output 8 logits each full convolution block outputs 64 1 8 embedding flattened to dim 512 1d array full transformer block outputs 40 70 feature map which we time avg to dim 40 1d array 512 2 40 1064 input features 8 output emotions self fc1 linear nn linear 512 2 40 2 softmax layer for the 8 output logits from final fc linear layer self softmax out nn sigmoid dim 1 is the freq embedding define one complete parallel fwd pass of input feature tensor thru 2 conv 1 transformer blocks def forward self x 1st parallel conv2d block 4 convolutional layers create final feature embedding from 1st convolutional layer input features pased through 4 sequential 2d convolutional layers conv2d embedding1 self conv2dblock1 x x n batch channel freq time flatten final 64 1 8 feature map from convolutional layers to length 512 1d array skip the 1st n batch dimension when flattening conv2d embedding1 torch flatten conv2d embedding1 start dim 1 2nd parallel conv2d block 4 convolutional layers create final feature embedding from 2nd convolutional layer input features pased through 4 sequential 2d convolutional layers conv2d embedding2 self conv2dblock2 x x n batch channel freq time flatten final 64 1 8 feature map from convolutional layers to length 512 1d array skip the 1st n batch dimension when flattening conv2d embedding2 torch flatten conv2d embedding2 start dim 1 4 encoder layer transformer block w 40 512 40 feedfwd network maxpool input feature map 1 40 282 w 1 4 kernel 1 40 70 x maxpool self transformer maxpool x remove channel dim 1 40 70 40 70 x maxpool reduced torch squeeze x maxpool 1 convert maxpooled feature map format batch freq time time batch freq format because transformer encoder layer requires tensor in format time batch embedding freq x x maxpool reduced permute 2 0 1 finally pass reduced input feature map x into transformer encoder layers transformer output self transformer encoder x create final feature emedding from transformer layer by taking mean in the time dimension now the 0th dim transformer outputs 2x40 mfcc embedding time feature map take mean of columns i e take time average transformer embedding torch mean transformer output dim 0 dim 40x70 40 concatenate freq embeddings from convolutional and transformer blocks concatenate embedding tensors output by parallel 2 conv and 1 transformer blocks complete embedding torch cat conv2d embedding1 conv2d embedding2 transformer embedding dim 1 final fc linear layer need logits for loss output logits self fc1 linear complete embedding final softmax layer use logits from fc linear get softmax for prediction output softmax self softmax out output logits output softmax self sigmoid output logits need output logits to compute cross entropy loss need softmax probabilities to predict class return output logits output softmax def criterion predictions targets return nn bceloss input predictions target targets optimizer torch optim sgd model parameters lr 0 001 weight decay 1e 3 momentum 0 8 define function to create a single step of the training phase def make train step model criterion optimizer define the training step of the training phase def train step x y forward pass output logits output softmax model x predictions torch argmax output softmax dim 1 accuracy torch sum y predictions float len y compute loss on logits because nn crossentropyloss implements log softmax loss criterion output logits y compute gradients for the optimizer to use loss backward update network parameters based on gradient stored by calling loss backward optimizer step zero out gradients for next pass pytorch accumulates gradients from backwards passes convenient for rnns optimizer zero grad return loss item accuracy 100 return train step
rl0rmt,1,pruning with reinforcement learning hello i have a seminar in university witch is about pruning in nns i want to spezialice me to pruning with reinforcement learning but after i read five important papers i didn t found something else witch is relevant do you have more interesting papers to this theme
rwots2,0,methods to create monolingual language model from pretrained multilingual model apart from just fine tuning the pretrained multilingual language model on the target language is there anything more sophisticated that people are doing
rphiin,0,2022 research topics or areas hi good people would you like to suggest some latest interesting research areas to work on 2022 specially on deep learning deep reinforcement learning federated learning meta learning areas thank you
rnkwp7,1,good beginner exercise for improving programming monte carlo simulation of the approximation of number pi Ï€ this is good exercise for understanding some basic data science concepts as monte carlo simulation
rirflx,1,machine learning book for behavior analysis with r this book is for those interested in learning machine learning concepts using the r programming language the book focuses on behavior analysis but the concepts can be applied to any field a complete free version of the book is available at x200b
r4lty2,0,is there an automl commercial solution for joint image and text classification i have a set of images with title captions i d like to do a joint classification task using both inputs but i haven t seen a commercial automl solution to train it does anyone know of any solution
rngzdq,1,can gpt j be fine tune to create sales copy will it creates the kind of result if compare with fine tune gpt3
rl5j1e,1,what other things i need to concentrate on other than my course curriculum hi i have started my master s in computer science with a specialization in artificial intelligence and i aspire to become a machine learning engineer it would be helpful if anyone can suggest things other than my course curriculum that i should concentrate on thank you
rs21jj,0,are there good attempts at recognizing asl other sign languages beyond simple static signs like numbers and letters i ve seen a lot of projects aimed at recognizing the asl alphabet but a lot of sign language depends on movement what s the current state of actual sign language recognition translation i haven t been able to find anything online
rasfxo,1,what parts of khan academy statistics should i learn hey i jumped directly to this book but it was a bit too hard so i thought i would learn some python i work as a c dev so i thought i can figure it out someway and some statistics from khan academy but the thing is there is a lot of content over khan academy so i wanted to ask what units should i learn what parts are unrelated or could be skipped i get that everything in statistics is probably worth learning but i mean like what parts are actually useful in practice
r6fzxo,0,discussion sharing of node information in mcts runs in scenario of a single player game with a pretrained value and prior probability model if it is given that a state can be reached through different order of actions imagine something like a knight move followed by a bishop move and vice versa where both the series of moves lead to the same state what are the different ways used today to reuse the information about states already explored is it possible to connect nodes of different branches of search tree when it is known that those states are identical with some sort of a hash function in an effort to save the roll out time is it possible to also reuse the current approximation of q s a based on w and n for a particular action when two actions lead to the same state
r79a3h,0,great r d content with code on computer vision news of december 2021 have a peek at computer vision news of december 2021 many articles about ai deep learning computer vision and more html5 version recommended pdf version dilbert on page 2 free subscription on page 52 enjoy
ro9ti6,1,amd hardware deep learning related question i have a ryzen 5 3500u with radeon vega 8 i heard deep learning requires gpu i wanted to know what is an alternative for cuda as it is for nvidia only
qmm9z7,0,hierarchical transformers are more efficient language models a team from google openai and university of warsaw proposes a new efficient transformer architecture for language modeling setting a new state of the art on the imagenet32 for autoregressive models
rphw9x,1,good model for seismic signal what would be a good model for a regression problem where the inputs are seismic signals in csv form specifically something but not quite along the lines of earthquake prediction from seismic signals thanks
rbo3c7,1,recursive feature elimination with cross validation for imbalanced dataset i m currently new in machine learning and i just learned about feature selection in my project i have a dataset with 89 being a majority class and 11 as the minority class also i have 24 features i opted to use recursive feature elimination with cross validation rfecv in the scikit learn package to find the optimal number of features in the dataset i also set the scoring parameter to f1 since i m dealing with an imbalanced dataset furthermore the estimator i used is the random forest classifier after fitting the data i had around 12 features with an f1 score of 0 94 is using rfecv appropriate for imbalanced datasets
rx3vgj,1,mit s opencourseware ml courses anyone working through or have gone through mit s opencourseware courses intro to machine learning or machine learning the latter of which is a graduate level course if so how did you find the experiences i m planning on using the knowledge to do research in machine learning so i m only reading the handouts and listening to the videos i m not working through the hands on stuff
rc32jz,0,looking for website like arxiv but in foreign language i would like to do some document classification topic modeling of non english documents for other nlp work i have downloaded many articles from arxiv their faq says that they accept submissions in multiple languages but i have not found a way to search for them ideally i m looking for a few hundred documents on 4 or 5 different topics and i d like them all to be in chinese or all in russian does anyone know of an open free source for journal articles in other languages
qkes8a,0,top 7 books to boost your data driven outlook in this post i will cover the best 7 books for data analysts these data analytics books will teach you about the power of big data and ways to harness it i started my career as a software developer and switched to data science 8 years ago when big data software projects were difficult to predict and risky to conduct due to large volumes of unclassified data and many types of metrics using machine learning data analysis and visualization approaches was essential for facilitating informed decision making throughout the software development and testing process mastering data analysis was one of the most challenging experiences in my life wading through tons of books to figure out where to start and which methods and techniques to use in a particular case can be extremely daunting and time consuming if you have been studying data analytics for some time choosing the right educational resources is crucial to launching and advancing your career within this area 1 storytelling with data a data visualization guide for business professionals as a data analyst your aim is not just to retrieve data but also to make it intelligible which requires you to be able to present the data in a certain way however presenting data does not imply dragging and dropping data fields into a chart it entails creating a meaningful visual representation of the data this book is based on real life scenarios and will give you some idea on the difference between colorful visualization and intelligent visualization explaining why you should closely examine each line and color on your visual interface this book provides excellent guidance examines criteria and presents examples of how to properly deal with data 2 mastering tableau 2021 implement advanced business intelligence techniques and analytics with tableau 3rd edition as a business analytics practitioner i search for publications that can simplify complicated topics in a manner that everyone can understand the book contains several tips and techniques that will assist you in understanding when to utilize particular chart styles at what data granularity and with what sort of presentation for the end user you will begin this fascinating trip by learning essential strategies for using sophisticated math to tackle challenging situations these strategies involve the inventive use of several sorts of computations such as row level aggregate level and others besides you will get concise instructions on using tableau to solve practically any data visualization problem by knowing the tool s inner workings and thinking creatively about possibilities expanded capabilities after reading the book you will be equipped with an arsenal of advanced chart types and methods that will allow you to display information to a range of audiences in an effective and engaging manner using clear efficient and engaging dashboards explanations and examples of effective and inefficient visualization approaches well planned and badly created dashboards and compromise choices when tableau users do not embrace data visualization will expand your knowledge of tableau so that you get the most of this powerful tool 3 machine learning with the elastic stack second edition this book is a one of a kind resource for users using elastic search i with actual case it focuses on the substantial growth of machine learning technology in elastic search providing actual case studies and extensive explanation this book is similar to having a one on one conversation with a subject matter expert if you need to refresh your practical skills in machine learning the book offers examples of how to apply elastic ml in your environment get valuable insight into your data and how you can turn machine learning from static to intelligent if you want to understand not just how to build tasks but also tap into the underlying models and variables machine learning with the elastic stack is the ideal option for you 4 data analytics made easy analyze and present data to make informed decisions without writing any code with data literacy being such an important component of a data driven mindset this book is an excellent resource for data science students looking to obtain practical information and learn how to apply their analytical skills the author does an excellent job of introducing readers to knime a low code data analytics framework that allows to instantly evaluate data furthermore his presentation of machine learning is user friendly with an emphasis on theoretical knowledge and handling a variety of use cases more significantly de mauro assists readers in comprehending the significance of becoming a great data presenter a vital talent to cultivate in order to influence decision making 5 fundamentals of machine learning for predictive data analytics second edition algorithms worked examples and case studies fundamentals of machine learning for predictive data analytics is a detailed analysis of the most important machine learning methods used in predictive data analytics encompassing both theoretical principles and actual implementations technical and mathematical knowledge is complemented with instructional practical examples and case studies show how these models may be employed in a wider business setting following a description of the journey from extracting data to gaining insights and making a prediction the book delves into the most essential machine learning techniques data based learning correlation based learning probability and error based learning each of these strategies starts with a no tech description of the core principle followed by quantitative models and algorithms demonstrated with extensive practical examples the authors discuss the procedures in a straightforward and succinct way without referring to any specific programming frameworks or languages they do a fantastic job of introducing the main concepts before diving deeper into the complexities of the logic and math underpinning the algorithms 6 analytics stories using data to make good things happen analytics stories how to make good things happen is a serious intelligent and entertaining look at how analytics can tackle real world problems and situations analytics stories fills the gap between data analytics and the particular challenges it solves with topics ranging from sports to finance politics healthcare and commerce the author does an outstanding job of conveying the notion of data storytelling to the reader he develops around 50 business cases on topics ranging from education to sports dr winston mostly utilized ms excel to interpret analyze display and successfully convey the data 7 data pipelines pocket reference moving and processing data for analytics a data science pipeline is a set of procedures that transform raw data into meaningful business responses data science pipelines streamline data validation extract transform load machine learning and modeling revision so their implementation is crucial for data analytics success the difference between having data and truly deriving value from it is moving data from various sources and processing it to create context this helpful reference describes common pipeline failures and key decision factors like batches vs streaming data input and building vs purchasing this book delves into fundamental concepts that apply to open source systems consumer applications and homegrown solutions as well as the most common decisions made by experts data pipelines pocket reference is a precious resource for all of the everyday problems and activities you are likely to encounter if you work in data analysis or a related field that will assist you in making data driven decisions for many years to come conclusion having a thorough grasp of data analytics and knowing how to gain actionable data driven insights are essential for a successful career in data science anyone interested in expanding their knowledge of data analytics can benefit from the books mentioned in this article since they provide the most recent industry information illustrated by examples of best practices
r0ezps,0,discussion what is the biggest computer vision model ever opensourced i m wondering that everyone is telling that they trained a multi billion parameter model but none of them opensourced like fbs seer what is the biggest computer vision model ever released only ru dalle is in my mind
qrm0o7,0,what are simple projects i can do with openai i m thinking about making a sql query generator from english language any other ideas are welcome thanks
rvw32g,1,beginner learning path question i m a python and c developer and i m starting to learn ml i have a question regarding which path will be the most convenient for my porpuses i have a project in mind which is the automatic placement of labels in cad following certain rules see image this is a very time consuming task specially when we have thousands of objects example image i have finished the ng course but now i m confused what should i learn next should i learn ml and then reinforced learning can i start directly with reinforced learning i do not want to skip any stage but i would like to learn as i progress with my project x200b thanks
r61irj,1,need help with theory questions asked for a research position i was asked these questions in an assessment for a research position in machine learning any intuition on how they can be solved reference to solve would be really helpful 1 non uniform weights in linear regression you are given a dataset in which the data points are denoted by xn tn n 1 Â· Â· Â· n each data point is associated with a non negative weighting factor gn 0 the error function is thus modified to where Ï† Â· is any representation of the data a find an expression for the solution w âˆ— that minimizes the above error function b give two alternative interpretations of the above weighted sum of squares error function in terms of i data dependent noise variance and ii replicated data points 2 given d dimensional data x x1 x2 Â· Â· Â· xd consider a linear model of the form x200b now for n such data samples with their corresponding labels xi ti i 1 2 Â· Â· Â· n the sum of squares error or mean squared error function is given by x200b now suppose that gaussian noise Ïµk âˆ¼ n 0 Ïƒ 2 i e zero mean and variance Ïƒ 2 is added independently to each of the input variables xk find a relation between minimizing the above sum of squares error averaged over the noisy data and minimizing the standard sum of squares error averaged over noise free input data with a l2 weight decay regularization term in which the bias parameter w0 is omitted from the regularizer 3 solve the problem given below by hand no programming implementation required a consider the training set and test set given in tables 1 and 2 x200b we use the linear model fÎ¸ x1 x2 Î¸0 Î¸1x1 Î¸2x2 and the logistic regression function Ïƒ fÎ¸ x1 x2 consider the initial weights as Î¸0 âˆ’1 Î¸1 1 5 Î¸2 0 5 and learning rate as 0 1 for gradient descent i what is the logistic model p Ë†y 1 x1 x2 and its cross entropy error function ii use gradient descent to update Î¸0 Î¸1 Î¸2 for one iteration write down the updated logistic regression model calculate and report the accuracy precision and recall to evaluate this model at this iteration
rahy9m,0,mahalanobis distance for ood detection i was reading this paper the core idea is given a test sample x and a set of classes c to compute a score m x which is the maximum of the c negative class specific mahalanobis distances what i struggle to understand is how to compute a auroc score using this distance if the ground truth is 1 for in distribution and 0 for out of distribution how to compute a auroc if m x is e g 639 2
rwl2hh,1,style transfer from multiple style sources i m planning for a simple web service that should get an image input and send it back in the style of a series of particular paintings i ve got i ve been out of the loop of ml for a while from a quick view i ve seen that the style transfer seems to only work with one source and one target style when i ve tinkered with ml in the past i used larger datasets and since i have a 40 ish image as style dataset i was wondering if there s some technique to employ them and not just one something not to hard that is already implemented in python possibly
raq8ti,1,can anyone help me with this multi part question question 1 suppose that you are working for a phone company that wants to predict churn let actual be a binary random variable meaning that a customer actually churns and let flag be a binary random variable meaning that a certain classifier predicts that a customer will churn note that the classifier makes yes no binary predictions the base rate for churn is p actual 1 0 05 similarly p flag 1 is the fraction of customers that are predicted to churn define a p actual 1 flag 1 and b p actual 0 flag 0 your expensive data mining consultant dr zuckerberg claims that he can train a classifier that achieves a b 0 9 a explain in english the meaning of dr zuckerbergâ€™s claim b would such a classifier be useful for making decisions or not c write down a confusion matrix and show how to obtain p actual 1 and p flag 1 in terms of the entries in the confusion matrix d show how to obtain a and b in terms of the entries in the confusion matrix e give a precise and clear argument that shows that dr zuckerberg is wrong because no such classifier can possibly exist f is it possible that dr zuckerberg has fooled himself by falling victim to leakage
r6dcj3,1,exercises collection i think the best way to learn is by doing otherwise it doesn t stick so is there anywhere online a list of machine learning exercises ideally with example code solutions i ve found so far the following various textbooks those rarely have code and the data is hard to get sometimes coursera mit courses homework usually just 1 2 easy problems per chapter if that hackerrank many problems with community solutions but not that well structured and often poorly curated
ro224d,0,how ad is implemented in jax tensorflow pytorch hi everyone i heard that there are two ways to implement automatic differentiation source code transformation sct and operator overloading oo which type does jax use and same question for tensorflow and pytorch
qybvz4,0,asking for multi model architecture papers hey all i m trying to read about the gap between the resources demanded by multi model applications and the resource limitation on commodity hardware right now i am looking more into the former for exemplification deepeye employs 4 5 cnn models that each perform a unique task on a single device if anyone is aware of similar papers that employ multi model architectures it would be amazing if you left the paper in the comments i m new to this community so if i need to specify the question or need to make any edits please let me know looking forward to participating in this community
rfks1b,1,can we use autoencoders to change an existing image instead of create one from scratch i m trying to think if we can use auto encoders to edit an existing image instead of say creating a new one from scratch to give an example say i train my data on the mnist dataset if i now give my model a 9 and ask it to convert to a 8 would it be able to alter the pixels to create 8 if yes can you point me to some resources for doing this thanks
rplc4j,1,get stuck in linear algebra hi everyone i am a newbie in machine learning and now i m having some stuff at learning linear algebra for machine learning because linear algebra takes lots of time to learn i have read that only some contents of linear algebra is useful for machine learning so can you guys recommend me some books or course for learning linear algebra for machine learning p s sorry for my bad english thanks for your help
rkm5dv,1,can same neural network on same dataset fetch different results on different operating systems gpus even when i used the same code and dataset for fitting on neural network i got different accuracy in every epoch on my local system compared to google colab and i think this should be possible the time taken can definitely vary on different systems but accuracy of neural network in 100 epochs doesnâ€™t even get close to each other
qq644r,0,evaluating the effectiveness of text generation i m using gpt3 to generate text based on a q a dataset the data is domain specific based on data scrapped from various internal company sources the challenge i am facing is that the quality of the output is somewhat subjective this makes it hard to improve the model output i ve easily been able to move beyond outputting gibberish to something which works reasonably well however i am not finding it hard to evaluating the effectiveness of minor model changes e g temperature prompt design tweaks to the dataset etc i m considering crowd sourcing input from my colleagues giving them model output with various tweaks and asking them to score the results however this has obvious limitations so i was wondering if there are techniques that people have developed that make it easier to fine tune models where the output has a subjective quality
r6l8nh,1,algorithm selection basic music generation i m looking to generate a very basic sound snippet around 40 50 notes based around a small training set for 5 different genres but i m new to machine learning and am having trouble determining which algorithms i should pursue to implement this i m working in matlab and will use the frequencies of the first 40 chord of a song about 5 10 songs per genre to create a prediction of what the next frequency to play will be based on the previous frequency i e the genre is indie pop putting each song in its most basic form just chords the first frequency of the song is a bflat major chord and the second chord is a g minor chord these chords would be randomly generated knowing that it is part of the indie genre i d want to predict the next 40 chords of the song no rhythms for now just frequencies any algorithm suggestions for this
rs3qef,0,neural pseudo random number generator let s say we have a neural net that generates numbers called g and a neural net predictor called p g s goal is to not let p guess what it generates and p s goal is to approximate g ie g maximizes the difference between g and p s outputs while p minimizes it are there any papers that explores this kind of setup
rhnfm7,0,can neural networks tell whether they have already seen a given example we want our neural network to perform the following task given a training dataset d x 1 x 2 x 3 x n the network should output yes when provided with one example in d or a very similar one and no when provided with one example not similar to any in d examples not in d cannot be provided at training time therefore the network would need to learn to memorize what it has seen and answer no to anything not similar to it to me this looks like an out of distribution detection task but i am not very familiar with the literature so any reference or idea would be really appreciated
qpf5t0,0,introducing metaicl a language model meta training framework for few shot in context learning a research team from the university of washington facebookai research and the allen institute for ai introduces meta training for incontext learning metaicl a new meta training framework for few shot learning where an lm is meta trained to learn in context â€” conditioning on training examples to recover the task and make predictions here is a quick read introducing metaicl a language model meta training framework for few show in context learning the metaicl code and data will be made available on the projectâ€™s github the paper metaicl learning to learn in context is on arxiv
qmj7tq,0,local latin hypercube refinement for multi objective design uncertainty optimization many real world systems consist of input features with aleatoric i e irreducible uncertainties in engineering design applications such uncertainties may arise from production tolerances operational conditions as well as other environmental factors thus the distribution of these features can be measured and to some extent modified e g by moving the mean value design uncertainty optimization seeks to find the distribution parameters of input features which optimize metrics such as the failure probability and the variance of key performance indicators besides the expected objective values since these often require uncertainty quantification of black box functions the whole process is computationally quite burdensome in this work we propose using machine learning methods in combination with sequential sampling to reduce the required amount of computation and accelerate the uncertainty optimization task due to the small data setting we limit the investigation to gpr and svr but argue that a more suitable model can exist depending on the problem which could be used within the proposed framework instead
r7gb1g,0,aaai2022 who has the final say if a paper gets accepted or rejected a paper that we have submitted to the aaai received 1 weak accept 1 reject and 2 accepts and the meta reviewer recommended a weak accept however the paper was rejected is this usual who has the final say if a paper gets accepted or rejected
qyvuzj,0,daily working routine as a machine learning engineer hey r machinelearning i m currently studying business informatics in germany next year i will finish my bachelor and then i want to do my master s degree study i m doing my bachelor part time in a software company so i already have some practical experience in the field my interest are very focused on machine learning and artificially intelligence in general i find it fascinating that a computer can learn from data and then predict outcomes because of that i already did 2 semesters in a data science team in my company where i did mostly statistically analysis i liked the tasks in general but the daily work was more focussed on creating presentations for business people and not really the technical part currently i m in a team wich works on natural language understanding problems there i m doing more infrastructure tasks e g docker openshift deploying models building api s wich communicate with the models i really like the more technical part but it s not that ml related usually in the last semester you work in a team where you are going to work after your bachelors my current plan is to go to a team wich solves vision problems e g identification of loose screws in trains i thought that a position as ml engineer there would be my fulfillment but recently i had some doubts about that 1 i m scared that my day just consists of annotating data 2 i want to think by myself not just implement different models and then see wich is the best 3 i want to be creative e g implement own things not follow instructions on how to implement model x 4 i m also scared of that if i don t like the tasks i can t change in a different path because ml is so specialized i really liked the summary of ml researchers positions but i don t know how hard it is to get a research position here in germany in general i m open minded about doing a phd in a ml field but i don t know how hard this is going to be in comparison to my current study i think i m currently one of the best in my year but the uni is not really the hardest do you guys have any experience as a ml engineer and can take away my fears is it really so hard to land a position as ml researcher should i go for a different career path because ml is so fast living
rwehvc,0,is a scalar reward enoguh to create agi check out this paper which discusses the idea that a scalar reward is not enough to create agi what are your thoughts on this
r36vlk,1,could someone help me understand how parameter estimation and finding optimal predictors contributes to the big picture of a simple ml model like polynomial regression please iâ€™m wondering how all of these things fit together iâ€™m taking my first ml course in university right now and itâ€™s basically a math course to provide the foundation of the calculus probability statistics and linear algebra involved in ml i feel like i donâ€™t have a good understanding of how some of the sections relate to each other yet though for example i know about parameter estimation and how we can use mle map bayesian etc to estimate parameters for different distributions i know we can create an optimal predictor by minimizing the expected value of the cost function for example the optimal squared error cost function being the conditional expectation e y x x but iâ€™m not sure how this fits together with something like polynomial regression and how to use all of these things in the model how it relates to fitting the data how the coefficients for the polynomial are found etc could someone please explain how this all fits together starting from the initial data matrix how do these things come together to create a final model that is able to make predictions thank you in advance
r132f4,1,good ai mlcourses youtube channels and books for beginners hi guys which courses youtube channels and books do you recommend for absolute beginners learning ai ml
r9n6rs,1,can someone please give me a breakdown of the math i have to study for machine learning and related data science hi i m 20 and from south asia sl and i think i ve studied up to pre calculus very basic graphs statistics and matrices i studied biology chemistry and physics due to unavoidable reasons in high school but my mind was always stuck with it i ll be starting my information systems bsc degree in a few months now i m working as a data science intern some data automation basic ds and ml with a very good mentor guiding me so the lack of math knowledge isn t affecting me much however i do know that i have to understand how certain functions work and matrices for dl if i want to be good at what i m doing i really love the field and i want to enter the field of deep learning ofc through ds and then ml i m very unfamiliar with math since i ve had virtually no interactions in the past 5 years but i m quite comfortable with computer science especially python now i know that for a beginner level i have to study linear algebra statistics probability and differential calculus what i don t know is what sections of those aforementioned fields i have to study i m thinking about hiring a private tutor to gain at least basic knowledge within the next 4 5 months the tutor asked what exactly i wanted to study and i didn t know hence my request could you please give me a structure of the topics i should study beginner level e g x200b linear algebra topic 1 topic 2 statistics topic 1 topic 2 is it possible to be familiar with the topics you suggested within the mentioned time frame could you please suggest some good books for self learning your advice suggested resources books sites are greatly appreciated thanks a lot for your contribution feel free to ask me any questions
r6lbdj,1,clustering data sets of mixed types hi all in your experience what s the best way to cluster data sets of mixed data types k prototypes and or agglomorative single average and complete linkage thanks
rf7mpc,1,should i take machine learning my first semester my sophomore year hi i m currently a freshman my first semester and taking a python class i ll take a java class next semester along with intro to stats linear algebra and database management sql i did ap cal in high school hence have a solid base of cal i and ii i think i want to ask if that would be a good base for me to learn ml my first semester as a sophomore if not what should i do to prepare better appreciate any contribution
rhy0x6,1,nlp model and document length i need some advice on some basics around nlp i ve created a model in python in which i predict the political ideology of the author based upon a tweet 3000 single tweets per author total dataset is around 300k model is logisticregression utilizing scikit learn how does doc length effect accuracy on a trained model i e it was trained utilizing the standard length of a tweet what happens if i feed in something much smaller or much larger does a model hold up regardless of the size of the document or does it only excel when utilizing a similar length just trying to wrap my brain around the concept in general not really related to the single model thanks
r22tyr,1,nothing shows up when i try to create a bounding box around my object so i trained a model on jupyter notebook and it worked with an error of 0 8 it generated a yolo weights file that i now am using to create a bounding box around an image i have the following code to create a bounding box using yolo and ocv but nothing shows in the image no bounding box at all the training worked for sure but i don t know whats wrong here is my code it doesn t throw me any errors but the dog image shows up with no box around it
rwm2qi,1,please suggest some resources to learn ml online preferably at no cost
rb3wh8,0,can we create a self funding decentralized and transparent research lab this post is not about research or ml directly even though it all started with a qd algorithm searching for those very interesting lenia creatures i always wondered if one could create a self funding research lab is it even possible even better if this lab would be transparent from the ground up i had belief in openai first because they understood the state of the field at the time and then because it started as a non profit but we all know how it evolved how to be safe from this path maybe all you need is to be radically transparent and decentralized from the ground up also be optimistic we are trying to start such a lab i would love criticism ideas involvement from the community cheers to all of you
rhzj3y,1,gis ml newbie seeking help for land classification project hello all d i ve recently started learning a bit about gis and deep learning in order to learn more and practice my skills i m trying to create a dataset from open source data and training some models on it to try and recognize various types of land classes usage here was my initial thought process of my project 1 acquire data about land usage which will be the prediction target and acquire real map data 2 combine them to obtain labeled images 3 use supervised training on the labeled dataset 4 use the trained models for new inferences x200b so far i have found an interesting gpkg for my truth labels on urban atlas using geopandas i m currently able to open this gpkg and access several columns of data such as land class example water of industrial as well as the geometry of each sample contains multipolygon objects i would like to use some jp2 band files obtained via copernicus sentinel 2 program merge them as one tif file containing the different bands and combine overlap these real satellite images to the land usage map contained in the previously mentionned gpkg x200b i thought i would then cut out square pieces rasters images of the map and save each of them with the label class as filename and later train a cnn on them just like with the eurosat dataset using cnns might not be the best correct approach though i saw articles mentionning r cnn and u net architectures which seemed interesting but i don t know much about their use case scenario and if they would be interesting to use in my project x200b is there some kind and knowledgeable soul out there who can guide me help me figure this problem out and or enlighten me on basic gis ml concepts that i might seem to misunderstand any help is appreciated x200b i m adding that i m working with python currently using a jupyter notebook i m trying to only use open source stuff and if possible staying away from using software like qgis
r4aaj9,1,suggestions on applying machine learning in structural health monitoring or mechanical fault prediction i am a mechanical engineer working for a reputed organisation in recent years i am being asked to take up projects related to machine learning and apply the concepts in fault prediction and similar things i have a very rudimentary idea of ml algorithms and had used sklearn some 4 5 years back can anyone suggest how to apply ml in fault prediction field any books tutorials or suggestions will be really appreciated
rlgygp,0,solo learn 1 0 3 new methods support for transformer architectures better evaluation improved docs and additional results hi reddit the solo learn team is back again with interesting news about its ssl library during the last few months we worked hard to improve the library by adding the following features 1 new methods nnbyol nnsiam and vibcreg 2 support for transformer architectures vit swin and poolformer to keep up with new research 3 better evaluation with k nn and object detection detectron2 on pascal voc and coco 4 improved docs by adding tutorials code of conduct and contribution guidelines 5 additional results for moco v2 and barlow twins on imagenet feel free to ask questions here or contribute on github enrico and victor
rw50hg,0,deep learning is the future of gaming hey everybody i know this isn t hard core ai research but i have been thinking a lot about deep learning and gaming recently and put together a little presentation on how i see things unfolding lots of cool research featured in the video i go over 1 photorealistic neural rendering 2 deepfakes for gaming is a better example than the obama one i used 3 gan theft auto and dreaming up game engines with neural networks 4 large language models for building realistic npcs and storytelling 5 using openai codex to automatically program games it s really clear that deep learning is the most important technology to impact gaming since the advent of 3d graphics would love to talk with anybody who is working on stuff in this space
rd359m,1,best resources you ve found i ve enjoyed deep learning by francois choillet book still going through it haven t used kaggle yet what s yours
r45wdo,0,ai and the everything in the whole wide world benchmark really interesting criticism of general benchmarks e g glue and imagenet and their construct validity issues in the 1974 sesame street childrenâ€™s storybook grover and the everything in the whole wide world museum stiles and wilcox 1974 the muppet monster grover visits a museum claiming to showcase â€œeverything in the whole wide worldâ€ example objects representing certain categories fill each room several categories are arbitrary and subjective including showrooms for â€œthings you find on a wallâ€ and â€œthe things that can tickle you roomâ€ some are oddly specific such as â€œthe carrot roomâ€ while others unhelpfully vague like â€œthe tall hallâ€ when he thinks that he has seen all that is there grover comes to a door that is labeled â€œeverything elseâ€ he opens the door only to find himself in the outside world as a childrenâ€™s story groverâ€™s described situation is meant to be absurd however in this paper we discuss how a similar faulty logic is inherent to recent trends in artificial intelligence ai â€” and specifically machine learning ml â€” evaluation where many popular benchmarks rely on the same false assumptions inherent to the ridiculous â€œeverything in the whole wide world museumâ€ that grover visits in particular we argue that benchmarks presented as measurements of progress towards general ability within vague tasks such as â€œvisual understandingâ€ or â€œlanguage understandingâ€ are as ineffective as the finite museum is at representing â€œeverything in the whole wide world â€ and for similar reasons â€” being inherently specific finite and contextual
qlffsv,0,neural architecture search and neuroevolution hi fellow readers i ve come recently to a slight confusion about how to understand nas and neuroevolution this is why i would like to hear your explanations my current understanding is as follows nas technique which can be used to automate the process of designing optimizing neural networks nns 1 the technique is further divided into three components such as search space defines types of layers depth type of connections search strategy defines the approach used to explore search space evaluation strategy evaluates the performance of build ann from its design neuroevolution a technique that harnesses evolutionary algorithms ea to design optimize nns it can augment the nn by changing the topology connections and weights 2 based on observed action in the environment confusion therefore from the above description i don t understand if nas and neuroevolution can be explained as applied techniques to design optimize nn topologies where nas can use any kind of algorithm to design nn topologies rl ea gradient descent and neuroevolution uses only ea or neuroevolution can be just defined as a search strategy in nas this means neuroevolution can be used in nas search strategy like for example reinforcement learning rl 3 to simplify i would like to understand how can i think about nas and neuroevolution when researching my goal is to understand how i can put all the puzzles together when building an automated machine learning process for a specific task as anomaly detection if i made any mistakes or silly comparisons please point them out in the comments that future readers can grasp my mistakes and your knowledge any comments are more than welcome thank you in advance
rvvfoa,0,why is vae used instead of autoencoder in the world models paper hi all i was just reading this paper and was wondering if we just want to achieve a compact version of the original representation we could just use a traditional autoencoder is there any specific reason the vae is used thanks
r688iw,1,can somebody explain a classification reports meaning can somebody explain a classification report done on an sgd classifier to me i have 79 precision and 100 recall on 0s but i have 0 precision and 0 recall on 1s
rhliul,1,loan credit score for a beginner which model to use hello i am new to machine learning and i am intending to start on a project on loan credit score the project is basically using information from users to produce a credit score grade for example in information we have both categorical data and continuous data e g categorical employment status business owners 1 employed 2 homemaker 3 continuous biz last financial year revenue arranging into tiers making them into different categoricals tier 1 0 100 000 tier 2 100 001 200 000 finally i want to produce a credit score grade credit score employment status biz last financial year revenue many other variables i want to use weighted variables for example in employment status business owners 100 employed 70 homemaker 50 and also in biz last financial year revenue tier 1 100 tier 2 70 tier 3 50 and the final credit score in the context of business owners 1 and tier 2 credit score 170 100 1 70 2 x200b 1 how do i go about using weighted kind of linear regressions are there any reference sites 2 since i am using a lot of variables and have many unnecessary variables is lasso regression possible for the tuning and getting the job done 3 how do i go about improving my model are there other ml models that can be considered thank you from a beginner
rjvm5e,0,over under parametrizing in random feature models let s say i have a dataset x nxd from which i construct a feature dataset f nxp doing relu xw where w is a random matrix how will the relation between d and p influence the train and test loss is there a sort of bias variance tradeoff
ruv7tc,1,understanding model overfitting not learning certain classes i m currently training a model to classify types of bikes i ve trained it using a resnet50 architecture attached with a simple output layer i ve trained it from scratch twice and both times the model failed to pick up on two of the classes i only have 6 classes total the classes were different both times i tried using different batch sizes learning rates image augmentation and shuffling around the training and validation sets but no improvements have been seen does anyone know why something like this could be happening or any potential solutions i can try thanks in advance also if more information is required lmk this is my first time posting in these communities and i would like to provide all the information needed
r8n8qx,1,any suggestion for best use of datacamp as i have just started using datacamp
qzsrdw,0,test set just a glorified validation set everything seems to click really well when it comes to training and validation sets but the ideology behind a completely held out test set has never really sat well with me at the most basic level i guess a test set is there so that you don t cheat but at least to me it becomes a lot more nuanced than when you start thinking about how it s used let s say that you ve trained a model that you re proud of and as a final sanity check you test its performance on the test set to see how well the model generalizes on unseen data if the model performs poorly what kind of decision making does this inform do you start over do you tweak the model and try to figure out what s going wrong as soon as you tweak a single thing doesn t the test set instantly become some kind of glorified validation set here are some of my thoughts on the matter and i d be interested in hearing from other people too we know that a test set should not affect the model otherwise it just acts as a validation set thus if we act on a bad test result and change the model the test set becomes a validation set although it is not as involved as a validation set that is used for early stopping or parameter tuning in other words a test set must be useless the moment it is useful it becomes a validation set although to be more precise a test set is not that useless because it probably lowers your expectation about the later performance of the model in production as an example in a kaggle competition the final set is a test set since it does not have any involvement with model training however as soon as the final leaderboard is announced that test set becomes a validation set e g it affects which algorithms we later choose i e those of top competitors in summary it seems that most of the time we are using less involved validation sets to double check more involved validation sets
qysmyc,0,a webapp for monitoring gpu machines hi all i created a webapp for monitoring the gpu machines i am working with maybe it will be useful to someone else here the app aggregates the output from gpustat across all machines and displays them on a single page i have been using the app for a while now and it is working well for me but it has not been extensively tested if you encouter any problems while setting the app up don t hesitate to open an issue github
rbbv4g,0,clip nerf explained zero shot text guided object generation with dream fields by ajay jain 5 minute summary by casual gan papers do you like generative art i love it and it is about to get a whole lot crazier because ajay jain and the minds at google behind the original nerf have dropped a hot new paper that is right we all thought about putting together clip and nerf and they actually did it with dream fields it is possible to train a view consistent nerf for an object without any images using just a text prompt dream fields leverages the fact that an object e g an apple should resemble an apple regardless of the direction that you look at it from which is one of the core features of clip the basic setup is simple render a randomly initiated nerf from a random viewpoint and score this image against a text prompt update the nerf and repeat until convergence as for the juicy details well continue reading to find out full summary blog post dream fields chair in the shape of arxiv code not released subscribe to casual gan papers and follow me on twitter for weekly ai paper summaries
rnadmh,1,decoding openimages v6 mask coordinates hi new to ml i am using v6 for semantic segmentation in segmentations csv it has mask coordinates under the header clicks these are set of three coordinates first two coordinates being pixel values x and y with last coordinate being 0 or 1 can anyone please explain to me what does this last coordinate stand for thanks
qkyini,0,what do machine learning engineers at facebook do i was approached by an interviewer for this position and had a hard time grasping what they work on on a day to day basis thanks in advance
r5lf8h,1,the reward is enough hypothesis hi guys here s a video i made explaining the reward is enough paper from deepmind s david silver in it they posit that reinforcement learning is the right paradigm to further the development of general artificial intelligence hope you enjoy it
qs98gv,0,create semantic search applications with machine learning workflows x200b create semantic search applications with machine learning workflows the demo above shows how various nlp pipelines can be connected together to build a semantic search application txtai executes machine learning workflows to transform data and build ai powered semantic search applications txtai has support for processing both unstructured and structured data structured or tabular data is grouped into rows and columns this can be a spreadsheet an api call that returns json or xml or even list of key value pairs some example workflows summarize news articles summarize and translate research papers load and index data via a csv schedule a recurring job to query an api and index results for semantic search references live demo github article notebook notebook
rchzpy,0,what do you guys thinks about this ai dream app how do u think was it created tools and resources used for this app what will it take to create a text to video generation app
rocen2,0,gans and probability distributions on images when training gans either with the classic loss or wasserstein loss we try to minimize the distance between the probability distribution of the real data and the probability distribution of the generated data in the case of gans for images e g trained on celeba how does a probability distribution over images look like what is an intuitive way to understand this concept
rhrqcf,1,unsupervised learning for string matching in python can i have advice on how to go about this hello all i m a data engineer with around 8 months experience who is currently working for a company with one recurring requests fuzzy matching two lists of names against each other our current solution is using ssis fuzzy lookup feature although in my opinion this simply doesn t scale well and has limited features what i envision to create is something which is ideally an ml model which will improve based on whether a user tells the model something is or isn t a match at the moment the threshold for ssis is simply too static and is missing out matches i d like to have included on top of that having a manually maintained model is something i don t like the idea of i want to make this fairly hands off i m familiar with working in python and it s various libraries and have a very basic understanding of ml although am not particularly sure if i m doing the right thing i have tried fuzzywuzzy and rapidfuzz as well as tried dedupe and the recordlinkagetoolkit however i can t seem to wrap my head around what i d need to do in order to persist a model and have it improved in the way that i m trying to achieve above i totally get this isn t a small project so ready to put the time in what i d like to know is can i achieve what i want with the libraries i ve already mentioned if not can i incorporate scikitlearn into fuzzy matching using one of the libraries above is there a service which already exists and can handle enterprise scale fuzzy matching thank you
qphreq,0,intuition for meaning behind magnitude of covariance covariance matrices are pretty essential to many ml algorithms and probabilistic models when two variables have positive covariance they are correlated when they have negative covariance they are inversely correlated and when the covariance is zero they are not correlated however the degree of correlation cannot be read from the magnitude of the covariance value my question follows well what can be read from this magnitude what does it mean if two variables have a very large covariance value opposed to a small one
raz2bt,0,eccv 2022 is it happening so far there has been very little information about eccv 2022 which should according to schedule accepting submissions around early march the only information i can find is an old tweet announcing the venue no official website no important dates does anyone know what is going on
rp7wfs,0,anyone know any projects involving generating objects in the coco segmentation dataset using gans i know this is not the intended purpose of this dataset but it seems pretty good given the size of the dataset and object bounds masks
rsx4n0,1,more data more problems live by these words when you build your models more data more problems just because you have a ton of data does not mean you should use it all to train your model the larger the dataset the easier it is for garbage to hide in it thousands of data points gathered by different methods spanning across years trust me there are massive problems hiding in that data models amplify and further obfuscate anything missed in data analysis and cleansing use as little data as possible control data sourcing aggregating cleansing and analysis or research it for existing data as much as possible do not use data you do not fully trust reduce features from the start we hear about massive datasets used by google or facebook these are expertly curated by that i mean years have been spent reviewing and grooming data best practices require this level of obsession with detail until the business has built that capability smaller is better
rsm6ec,1,error loading image i have installed all the necessary libraries but am still unable to figure out the problem x200b please help me to figure out the problem
qlxl01,0,zero shot models as input features in nlp classification tasks so places like huggingface offer zero shot models that pretty decently as a zero shot classifier and are easy to implement i was thinking what if i just took a few zero shot models and added them as features and then fit a classifier on top so let s say that i m trying to classify toxic tweets i might use an embedding model or a verctorizer to turn the tweet into model features and then fit some classifier on top of this to try to predict the label from these text features but what if i add some simple zero shot binary features is angry model1 is sad model1 is news model1 etc and do this for one or more models or even more simply use is toxic model1 is toxic model2 and use their predictions as input features into a classifier i was planning to experiment a bit with this but wanted to get some thoughts on it and if there has been previous similar work i can use for reference thanks
raa9y7,0,an arxiv sanity like view of neurips 2021 papers the image thumbnail of the paper is one of my favorite features of arxiv sanity it helps to get a quick overview of whether the paper includes nice graphs that summarize the work or convey the intuition of the proposed approach i wanted to get a quick overview of top neurips 2021 papers so i created an arxiv sanity like view of all the 2334 papers and ordered them by rating it includes links to code tldr section and other meta data available in openreview a link to the project yeah some people still go over the publication lists p
rl8jrb,0,milvus vector database 2 0 is now cloud scalable objectives of milvus vector database when the idea of the milvus vector database first came to our minds we wanted to build a data infrastructure that could help people accelerate ai adoptions in their organizations milvus in ai ml applications development cycle we have set two crucial objectives for the milvus project to fulfill this mission ease of use ai ml is an emerging area where new technologies keep coming out most developers are not entirely familiar with the fast growing technologies and tools of ai developers have already consumed most of their energies finding training and tuning the models it s hard for them to spend additional efforts handling the large amounts of embedding vectors generated by the models not to mention the manipulation of a large volume of data is always a very challenging task thus we give ease of use a very high priority since it could significantly reduce the development cost low running costs one of the primary hurdles of ai in production is to justify the return of investment we would have more opportunities to put our ai applications into production with lower running costs and it would be conducive to lifting the margin of potential benefits design principles of milvus 2 0 we made a start towards these goals in milvus 1 0 but it s far from enough especially in scalability and availability then we started the development of milvus 2 0 to improve these points the principles we have laid down for this new version include aiming for high scalability and availability building upon mature cloud infrastructure and practice minimum performance compromise in the cloud in other words we want to make the milvus database cluster cloud native the evolution of database clusters the vector database is a new species of database as it handles new types of data vectors but it still shares the same challenges as other databases with some of its own requirements in the rest of this article i will focus on what we have learned from the existing database cluster implementations and the thinking process of how we designed the new milvus group architecture if you are interested in the implementation details of milvus group components please stay on top of the milvus documentation we will continuously publish technical articles in the milvus github repo milvus website and milvus blog the ideal database cluster let s first list the critical capabilities an ideal database cluster should have 1 concurrency and no single point of failure users connected to different group members can simultaneously have read write access to the same piece of data 2 consistency different group members should see the same data 3 scalability we can add or remove group members on the go honestly all of these capabilities are hard to acquire together in the modern implementations of database clusters people have to compromise some of these capabilities people don t expect a perfect database cluster as long as it can fit into the user scenarios however the shared everything cluster was once very close to an ideal database cluster if we want to learn something we should start from here the key considerations of a database cluster the shared everything cluster has a more extended history compared to other modern implementations db2 data sharing group and oracle rac are typical of shared everything clusters many people think shared everything means sharing disks it s far more than that a shared everything cluster only has one kind of database member in the group users could connect to any one of these symmetric members to access any data what is everything that needs to be shared for making this work the sequence of events in the group first the group event sequence is crucial to resolve the potential conflicts caused by the concurrent access from different groups members we usually use the database log record sequence number to represent the event sequence at the same time the log record sequence number is generally generated from the timestamp thus the requirement of group event sequence is equal to the need of a global timer if we could have an atomic clock for the group that would be fabulous yet milvus is an open source software project which means we should rely on commonly available resources to date an atomic clock is still a premium option for large companies we have implemented the time synchronization component in milvus 2 0 database cluster you can find the link in the appendix global locking the database has a locking mechanism to resolve concurrent access conflicts whether optimistic or pessimistic locks similarly we need global locking to resolve simultaneous access conflicts across different group members global locking means different group members have to talk with each other to negotiate the lock requests several vital factors would impact the efficiency of this global lock negotiation process the speed of inter system connections the number of group members who need to participate in the negotiation process the frequency of group conflicts the typical group size is no more than 100 for example db2 dsg is 32 oracle rac is 100 those group members will be placed in one server room connected with optical fiber to minimize transfer latency that s why it is sometimes called a centralized cluster due to the group size limitation people will choose high end servers mainframes or minicomputers which have much more capacity in cpu memory i o channels etc to consist of the shared everything clusters this hardware presumption has dramatically changed in the modern cloud environment nowadays cloud data centers comprise high dense server rooms full of thousands of commodity x86 servers with tcp ip connections if we rely on these x86 servers to build the database cluster the group size should increase to hundreds of even thousands of machines and in some business scenarios we will want these hundreds of x86 machines to spread in different regions thus implementing global locking might not be worth it anymore as the global locking performance will not be good enough in milvus 2 0 we are not going to implement the global locking facility on the one hand there is no update for vector data people should rather delete then insert instead of update so we don t need to worry about the multi writer conflicts on the same piece of data in the milvus group with sharding arrangement meantime we could use mvcc multi version concurrency control a lock avoidance concurrency control method to resolve the reader writer conflicts on the other hand vector data processing consumes a much higher memory footprint than structured data processing people are looking for much higher scalability in vector databases shared in memory data cache we can briefly divide a database engine into two parts the storage engine and the computing engine the storage engine is responsible for two critical tasks write data to permanent storage for durability purposes load data from the permanent storage to the in memory data cache aka buffer pool this is the only place where the computing engine accesses data in the database cluster scenario what if member a has updated the data cached in member b how could member b know its in memory data is expired the classic shared everything cluster has a buffer cross invalidation mechanism to resolve this issue the buffer cross invalidation mechanism will work similarly to global locking if we maintain a strong consistency across the group members as stated before it is not practical in the modern cloud environment so we decided to lower the consistency level in the milvus cloud scalable group to an eventual consistency manner in this way the buffer cross invalidation mechanism in milvus 2 0 can be an asynchronous process shared storage shared storage is probably the first thing people would think about when discussing a database cluster storage options have also significantly changed in recent years of cloud storage evolution the storage attached network san was and still is the storage foundation of the shared everything group but in the cloud environment there is no san the database has to use the local disk attached to the cloud virtual machines using local disk introduces the challenge of data consistency across the group members and we also have to worry about the high availability of the group members then snowflake made a great role model for cloud databases using cloud shared storage s3 storage it inspires milvus 2 0 too as stated before we intend to rely on mature cloud infrastructure but before we could utilize cloud shared storage we have to think about a couple of things first s3 storage is cheap and reliable but it is not designed for instant r w access like database scenarios we need to create the data components which we call data nodes in milvus 2 0 to bridge the local memory disk and s3 storage there are some examples like alluxio juicefs etc we could learn the reason we can not integrate these projects directly is we focus on different data granularity alluxio and juicefs are designed for datasets or posix files while we focus on the data record vector level when the vector data is settled on s3 storage the answer for metadata is easy store them in etcd how about the log data then in the classic implementations the log store is also based on san the log files of one database group member are shared within the database cluster for failure recovery purposes so this was not a problem until we got into the cloud environment in the spanner paper google illustrated how they implemented the globally distributed database group with paxos consensus algorithm you need to program the database cluster as a state machine replication group the redo log is usually the state that will be replicated across the group redo log replication by consensus algorithms is a powerful tool and it has substantial advantages in some business scenarios but for the milvus vector database we don t find enough incentives for creating a state machine replication group as a whole we decided to use the cloud messaging queue platform apache pulsar apache kafka etc as an alternative cloud shared storage for the log store by delegating the log store to the messaging platform we acquire the benefits below the group is more event driven which means many processes can be asynchronous it improves scalability the components are more loosely coupled making it much easier to perform online rolling upgrades it improves availability and operability we will revisit this topic in the later section so far we have wrapped up the crucial considerations of the database cluster before we can jump to the discussion on the milvus 2 0 architecture let me first explain how we manage vectors in milvus data management and performance predictability milvus stores vectors in collections the collection is a logical concept equivalent to a table in sql databases a collection could have multiple physical files to keep vectors a physical file is a segment the segment is a physical concept like a tablespace file in sql databases when the data volume is small we can save everything in a single segment physical file but nowadays we are constantly facing big data when there are multiple segments physical files how should we spread the data in different data partitions although data comes first rather than indexes we have to store data in the way that the index algorithm prefers to make the data access efficiently in most cases a frequently used strategy in sql databases is partition by the range of partitioning key values people usually create a clustered index to enforce the partitioning key overall this is a decent approach for sql databases data is stored in good shape optimized for i o prefetch but there are still defects data skew some of the partitions might have much more data than others the distribution of real world data is not as simple as the numeric range access hotspots more workload might go to some of the data partitions imagine more workload goes to partitions with more data we need to rebalance the data across the partitions when these situations occur this is a dba s tedious daily life the clustered index for vectors we can also create a clustered index for vectors an inverted list index but that is not the same case as sql databases once the index is built in sql databases it s very efficient to access the data through the index with less computation and less i o operations but for vector data there will be far more computation and i o operations even with an index so the defects mentioned before will have a more severe impact on vector database clusters moreover the cost of rebalancing vectors across different segments is very high due to the data volume and computing complexity in milvus we use the strategy of partition by growth when we inject data into a vector collection milvus will append the new vectors to the latest segment in the collection milvus will close the segment once its size is large enough the threshold is configurable and build the index for the closed segment in the meantime a new segment will be created to store the upcoming data this simple strategy is more balanced for vector processing the vector query is a process to search for the most similar candidates in the vector collection it is a typical mapreduce procedure for example we want to search the top 20 similar results from a vector collection with ten segments we can search the top 20 on each one of the segments and then merge the 20 10 results into the final 20 results since each segment has the same amount of vectors and a similar index the processing time on each segment is almost identical it gives us the advantage of performance predictability which is essential when planning the scale of the database clusters new paradigms in milvus 2 0 in milvus 1 0 we implemented a read write splitting sharding group like most sql databases it was a good attempt at scaling the milvus database cluster but the problems are quite obvious too milvus 1 0 sharding cluster in milvus 1 0 the r w node has to take total care of the latest segment including vector appending searching in this unindexed segment building index etc since each collection only has one writer the writer is very busy if the data is continuously streamed into the system the performance of data sharing between the r w node and the reader nodes is also a problem besides we must either rely on nfs not stable or premium cloud storage too expensive for shared data storage these existing problems are hard to tackle in the milvus 1 0 architecture thus we have introduced new paradigms into the milvus 2 0 design to resolve these issues milvus 2 0 cloud scalable vector database actor model there are two models to program concurrent computation systems shared memory that means concurrency control locking and synchronous processing the actor model aka message passing means message driven and asynchronous processing we can also apply these two models in distributed database clusters as stated before most high profile distributed databases use the same method redo log replication by consensus algorithms this is synchronous processing using consensus algorithms to build a distributed shared memory for redo log records different companies and venture capitals have invested billions of bucks in this technology i didn t want to comment on this until we started to work on milvus 2 0 many people regard this technology as the only way to realize distributed database systems this is annoying if i don t say something people might misunderstand that we were reckless in distributed database design in recent years redo log replication by consensus algorithms has been the most overestimated database technology there are two key issues the presumption that redo log replication is better is fragile vendors mislead people s expectations on the capability of consensus algorithms let s say we have two database nodes the source node and the target node in the ever beginning they have the exact copy of the data we have some change operations i u d sql statements on the source node and we want to keep the target node updated what should we do the simplest way is to replay the operations on the target node but this is not the most efficient way thinking about the running cost of an i u d statement we can divide it into the execution preparation and the physical work parts the execution preparation part includes the work of sql parser sql optimizer etc no matter how many data records will be affected it is a fixed cost the cost of the physical work part depends on how many data records will be affected it is a floating cost the idea behind redo log replication is to save the fixed cost on the target node we only replay the redo log the physical work on the target node the cost saving percentage is the reciprocal of the number of redo log records if one operation only affects one record i should see significant savings from redo log replication what if it s 10 000 records then we should worry about the network reliability which one is more reliable send the one operation or the 10 000 redo log records how about one million records redo log replication is super in scenarios like payment systems metadata systems etc in these scenarios each database i u d operation only affects a small number of records 1 or 2 but it s hard to work with i o intensive workloads like batch jobs vendors always claim consensus algorithms could provide strong consistency to the database clusters but people only use consensus algorithms to replicate the redo log records the redo log records are consistent on different nodes but that doesn t mean the data views on other nodes are consistent either we have to merge the redo log records into the actual table records so even with this synchronous processing we can still only get eventual consistency on the data views we should use redo log replication by consensus algorithms in the appropriate places the metadata system etcd and messaging platform e g apache pulsar used in milvus 2 0 have implemented consensus algorithms but as i said before for the milvus vector database we don t find enough incentives for being a state machine replication group as a whole in milvus 2 0 we use the actor model to organize the worker nodes the worker nodes are lonely they only talk to the messaging platform getting commands and sending results it sounds boring the actor model is asynchronous it is suitable for scalability and availability since the worker nodes don t know each other there is no impact on other worker nodes if some of the worker nodes join or are removed separation of availability and durability in milvus 2 0 we do operation replay rather than log replay because in the vector database there is not much difference between operation replay and log replay we don t have the update function nor the insert with select function and it s also much easier to do operation replay with the actor model so multiple worker nodes might execute the same operation from the messaging platform according to their responsibility i mentioned before we decided to use the s3 cloud storage as the shared storage layer of the milvus database cluster the s3 storage is very reliable then is it necessary for different worker nodes to write out the same data to the shared storage thus we designed three roles for the worker nodes the query node maintains an in memory data view according to the assignment the work of the query node includes doing vector search and keeping the in memory data updated but it doesn t need to write anything to the s3 storage it is the most memory sensitive node in the group the data node is responsible for writing the new data to the s3 storage the data node doesn t need to maintain the in memory data view so the hardware configuration of the data node is quite different from the query node the index node builds indexes for the segments closed by the data node when the size of the segments reaches the threshold this is the most cpu intensive work in the group these three types of nodes represent different kinds of workload they can scale independently we call it separation of availability and durability learned from the microsoft socrates cloud database
rge3b1,0,a new dataset for text classification and domain adaptation in social media a dataset of 22 500 labeled documents across four different domains you can find it here
qw55w6,0,project an overview of methods for text segmentation text segmentation is the task of splitting text into meaningful segments there weren t many good overviews of this online so i put together a project that outlines the different approaches models that exist how to evaluate these models and some open source datasets that can be used for training text segmentation models for the full overview you can read my outline here
rovzdh,1,am i correctly learning the dnn model hi i m new in the machine learning field i wrote a simple neural network using tensorflow for discovering heart diseases using data from here is it a correct way how professional data scientists solve such problems thanks a lot for the feedback please tell me something if i m doing wrong my solution is here on github
rhjpcu,1,question about gated recurrent units and mlps hello everyone question about a personal machine learning project that i m working on i m currently working with features extracted from a cnn which learned to correctly classify images from a biological dataset now my goal is to use those feature vectors and re learn to cluster them according to a ground truth distance matrix i have successfully done so with a simple mlp that uses a simple mean squared error loss which tries to minimize distances between the learned features and those of the ground truth distance matrix a simple regression task that tries to emulate the distance matrix my question is the following would it be appropriate to use something like a gru for instance instead to work on this task i understand that grus are usually used with sequence data but in my case they seem to work slightly better than mlps when it comes to making predictions and i m not sure why is that reasonable at all or are there more appropriate methods that can be used to learn distances between feature vectors additional info i am trying to make sense of the feature vectors and find a pattern in those sequences and learn the ground truth distances which is why i thought about using grus not a computer scientist so would appreciate any advice thanks in advance
rqi1qq,1,online resources that explain while coding i kinda want a resource that will teach me machine learning while coding and explaining out cause i feel like most resources that have been recommended feel like i can get bored easily
rrhhkh,1,multi class classifier i have trained 100 binary classifiers to classify 100 classes separately now i want to add all these trained model into one model to classify 100 classes is it possible if yes how can i do it thank you in advance
rv63gk,1,starlite a python async asgi api framework hi people i wrote an article introducing starlite a new asgi api framework i discuss what it is how it came about its relation to fastapi and its core functionalities the article can be found here i d be happy to answer any questions you might have
r76otc,1,sanebox clean up your inbox in minutes keep it that way forever implement sanebox into your workday and never waste time on email again sanebox simplifies the email process by using powerful algorithms that learn your email behavior to organize your inbox an average sanebox customer saves 12 hours month it works with any email provider client or device keep your email organized with sanebox after experiencing a clean inbox it will be hard to imagine life without it sanebox is for the average or power email user it works everywhere you check your email and fits into your existing workflow customers choose sanebox it saves time and boosts productivity sanebox pricing overview they have a 14 day free trial don t miss the chance to start now sanebox pricing starts at 7 00 as a flat rate per month they do not have a free version
qoptlo,0,flops calculation please consider the following table x200b which is from facenet publication what i am trying is to learn flops calculations with the help of this model so i can calculate for the below ones if i am not wrong the first row should have 110 110 64 7 7 3 113 836 800 flops which is a bit lesser than given value likewise the second convolution row conv2a 55 55 64 64 12390400 which is again near but not exactly the same x200b am i on the right track what am i doing wrong also mentions multiplying by 2 after all those operations which further confuses me
qxiywa,0,spann a highly efficient billion scale approximate nearest neighbour search thatâ€™s 2Ã— faster than the sota method a research team from microsoft peking university tencent and baidu proposes spann a simple but efficient memory disk hybrid vector indexing and search system that guarantees both low latency and high recall and achieves a 2Ã— speedup over the state of the art nearest neighbour search anns solution while retaining the same recall quality and memory cost here is a quick read spann a highly efficient billion scale approximate nearest neighbour search thatâ€™s 2Ã— faster than the sota method the associated code is available on the projectâ€™s github the paper spann highly efficient billion scale approximate nearest neighbor search is on arxiv
qyl5fc,0,asking for go to methodologies for 2 different tasks with correlated labels hi everyone assume that i am presented with a typical small size dataset let s call it dataset2 and also assume that a model needs to be learned using this dataset2 so that it can generalize to unknown datapoints however there is also a larger dataset1 with labels that are actually informative of the labels of dataset2 what i mean by this is that assuming that i train a model on the larger dataset1 if i use this model to acquire some predictions for dataset2 the prediction obtained with the actual labels from dataset2 correlate to an extend however the tasks meaning the nature of the labels for the 2 datasets are different and they do not mean the same thing now i know that there are many options here to further boost the performance for the task regarding dataset2 like a use the model gained from dataset1 to acquire predictions for dataset2 use those predictions as features when training a smaller model on the dataset2 labels b fine tune dataset1 model s weights to the dataset2 labels using a standard transfer learning regime i already tried this with mixed results c train a separate model for dataset2 and combine both predictions for better generalization d do multi task learning by combining both datasets could there be any other better options than those 4 given the above scenario i was thinking that knowledge distillation could be used to an extend but the tasks are not the same even though they are inherently closely connected thanks in advance and apologies for the bad english
rimqij,0,how to embed powerbi report in jupyternotebook powerbi reports could add new dimension in the visualization capabilities in jupyter notebook i have tried to demonstrate in below post about how one can access their custom powerbi report inside jupyter notebook
r1ledn,1,salary swe vs ml oftentimes i see posts from people in either here or r machinelearning and the ops talk about having been in the field and are making less money than those who do vanilla or basic swe work
rbqomm,0,how should a training dataset be distributed sorry if this is a beginner question suppose i m building a text to speech model i was wondering if my training dataset should be realistically distributed i e same distribution as the data it will be used on or should it be uniformly distributed to make sure it performs well on all kind of sentences thanks for your insight
r0k72x,0,walkthrough of keras model internals includes distribution performance optimizations callbacks training loop and more the source for the keras model class has grown to be several thousand lines of code this makes it incredibly challenging to sift through especially for beginners i wrote a blog post that walks through how the keras model class works under the hood the guide explains what various pieces of the model class do and implements a simplified version of the model class the guide doubles as a tutorial on tf distribute and batched execution after following the guide you ll be able to write a fully optimized custom training loop the post is available at there s a companion git repo any feedback is appreciated happy to incorporate changes provide deeper explanations on specific components of the model class i ve been using keras for many years so i may have glossed over some details please let me know if this is helpful to anyone
qt1vuy,0,nvidia jetson thoughts has anyone used any of the nvidia jetson models for production use cases we re considering using them for a health application but i haven t heard much about the developer experience one way or the other the alternative would probably be running the application on a mobile device
ru70fv,0,raising errors while using accelerators why is it so hard to raise exceptions when ml pipeline is using a gpu when for example you make a classic index out of bounds error in libraries like pytorch you get some generic cuda error and you can t see the exact error until you transfer the tensors explicitly to cpu and rerun the code do you think there is a possibility for this to improve in the future sorry if this is more cs related question
rvz50d,0,neural networks using a generic gpu framework i have a personal ml project that uses cnns but i have two little problems 1 not everyone has a nvidia gpu at home myself included sadly 2 the cnn needs to be trained every time it is used it s photo to photo style transfer so what would be a good framework to implement the cnn for training targeting desktop only i thought about using opengl but i don t know if using glsl shaders would be a good fit for it
r24rp7,0,peer review is still broken the neurips 2021 review experiment yannic kilcher yannic kilcher s thoughts on the 2021 neurips reviewer experiment frankly i agree with him completely the review process is completely broken and arbitrary yes phenomenal papers get accepted but this is a given and phenomenal papers will get traction whether they are published at a peer reviewed venue or not however the sheer level of randomness with regards to the good but not phenomenal papers is a searing condemnation of the review process itself and as kilcher discusses it completely invalidates the notion of publishing as a metric of value with respect to phd students tenure track professors grant applications etc
rbdq2o,1,neural network determines gender of severely blurred images with 88 accuracy first of all let me say that i am a layman who is passionate about ai and ml as a hobby i have no programming skills so i am experimenting with an image classification program that is very intuitive lobe i got from the website 35 male and 35 female faces blurred them quite a bit and labeled them m or f accordingly lobe claims 88 accuracy after training and when i tested it with 10 test images i can roughly confirm that how on earth is that possible the website mentioned above creates the faces completely randomly only the image section is always the same skin color age head position and hairstyle are very different so how can the model manage to find generalized criteria to judge the pixel mud i attach 2 examples each for m and f i m sorry if the layman speaks out of me and the answer is obvious i still welcome explanations x200b the key question is how a computer can extract info about gender from a complete pixel mud x200b x200b male 1 female 1 male 2 female 2
r74784,0,does working with tensorflow affect my chances of getting research internships i have been practicing ml using tensorflow for over 3 years now and i m looking for research internships but after plenty of rejections from professors and institutes that primarily use pytorch i m starting to wonder if there is a correlation between the rejections and me focusing on tensorflow as my primary tool have any of you experienced this note that i am in no way trying to say that i am the ideal applicant but i m trying to improve if it turns out that this is true then i have no problem switching to pytorch i just want to be sure if the switch is necessary because i was planning on learning jax flax for research in the future due to its optimized xla support
qx42a5,0,is anyone working on explainability of foundation models recently researchers from stanford came up with this term foundation models for these huge transformer models that they claim to be able to solve a number of natural language and even logical problems i was wondering whether anyone is working on explainability for these transformer models thanks
qordhq,0,what is something you took the time to learn that benefitted you the most saw a thread in cscareer questiosn and i thought it was a great question that could help a lot of people in machine learning since there is so much to learn in this field and could use some direction
rdefe9,1,any idea of a model to use my data is like this and i want to predict the next letter is for a and z it is like a loop for example after b it can go only for the next letter c or after it can go for b or y bcdcbcbcdedcbcdcbcb y yxyxyxyxwxy bcb yxy yxwxwxyxy yxwxwvwxwvwxwxy yxyxwvuvutuvutsrqpqpqponmnopqrqpqrqrqponmnopqrsrqrtutuvutututstuvutstututsrsrsrqrstutstrqrqrqpqpopqrqpnmlklmlkjkjkjihgfedcbcb bcb yxy b yxyxwvutsrqrsrqrqrstutuwxyxy y bcdcdedefedefgfdedefghihgfghijijkjihgfedckjkjkjkjihihiklkjihijihijklkjijijkijijkjihgfghghgfghghijijklmlmlklmlkjijihghihghijijihghghihgfedededcbcb y xy bcb bcb y yxwvwvwxyxyxwutsrstutsrqpqponmlmnopqrsrqrqpqpqponopqrststuvwvututstutstsrqpqpqpononlmnopqpqpopqpqrstsrqrststsrsrqpqrqpqpsrststuvuvutsrqrqrqrsrqonmnonoponmlmnmlkjijihghihihihgfedefefgfghghghghgfdefefefedcbcbcb b bcdedefghghghihghghgijkjkjijijkjkjklkjihihghijihijihikjihghijijkjijlmlmnmlkjijihihijhghihijkjlmlklmnonmlmlkjkjijijkjkjijihijkjihgfgfghgfefghijijijijijihghgfghihghijijijihfededcbcb bcbcdcbcb b bcb yxwxwxy yxyxwvwvutstsrstsrqrqpqrqrstvwvwvuvwxyxy yxy b bc y b y ywy bdcdcdcedefefgfghghgfefgfededefhghihijijkjklklkjmlmnmlmnonmnopopqrqrqpopqonmlmnopopqrqrststststutststutusrqrqpqrqrstststuvuvuvwxy yxyxyxwvuvutstststuvwxyxyxyxwvutuvwxwxwxyxwvuvuvtsrsrqrqrststuvwvwvwyxwvwxy b bcbcbcb bcdededefghghghgfghihihgijijihghghgfhijklmnmnopqrqpqrsrstsrststutsrqponoponoponmlmnonmnopqrqpqrsrststuvwvwvwxy b yxyxy bcdedefghgfefhihghihghihghgfededededefefghihghghgfgfgfefghghihgjihghgfededcbyxy y yxyxwxy b y b b b yxyxyxyxwvwvwyxyxy y bcbcbcb yxyxwvwvwvwvututuvwxwxwxwxyxyxwvwvwxwvuvuststsrqrsrsrsrqrqrsuvututututsrqrqpopopqrqrqrqpqpnmnmlkjihijijklkjkjijijkjijkjkjihghgfdcdcdedefgfghijklmlkjihgfdefgfedefgfefefghghgfghghgfedcdcdcbcb yx y b y bcbdededefghgfgfgfhghihghghgfghijijijklkjihijihihgfgfghihjihgfgfgfededefgfgfededcb b y yxy bcdededcbcdcdedcdefdefefefefghijijihghghgfedcbcbcb b y bcdcbcdedcb yxyxwvwvuvwvutstststststuvututsrsrststsrqrqrstuvwvuvwxwxwxwxyxyxyx yxyxyxy bcbcdedcdedefghghghgfghihghghghghihijkjijijijihgfgfgfgfededefghijklklmlmlmlklmnmlmnononopqpqponmnoponopopopqrqrstsrqrqrqponononmnonpqrsrstuvuvutututsrqpopqpqrsrststutstututstsrqpqpopqrqrsrstsrqrqrqrqrststuvututsrqrqrqrstutsrststutuvwvututsrqrqrqrstutuvuvuvwxy bcdcbyxyxy bcdedededcbcdedcdcb yxwvwvuvwvutsrsrqpqrsrqponopqpqrqpqrqrqrqpqrststsrqponopoponopqrpqrqrsrstststsrqrsrsrqpononmnmnonoponopqponmlklmlmlmlkjijijkjijijkjkjklmnononmnmlkjijklmlmnonononlmlklmnopqrsrstsrqpqrqrqpqrstutststutstvwvusrqpqrqrqrqrstututsrqrsrststsrqpqpqrqrqrqrstsrqrsrqrsrqrqrqrqpqponmopqrstututstsrststsrqpqrsrqpopopqrqpqrqrqpqrstututsuvwxy y bcbcb y y yxy ywxwxwvwvwvwvwvwvutstsrststutuvuvwvuvwxyxwxy yxwxyxy y bcdcdcbcdefghghghgfgfefghijkjkjijihijhghgfghghgfghghjkjkjijijijklmlmnopopqrqrsrqpqpopqrqrsrqrqpqrqponopqrsrqrqrqpopqrqrqrqrstsrstututststsrqponopqrqrstsrqpononmlklmnonopqrstutsrqpqrstststsrqrqrqrstststutututstuvwxyxy y bcdededcbcbcdcb b bc y bcbcb yxwvwvtsrqprstsrstutuvwvuvwvuvwvwvututsrqrqrsrqpqsrsrqrqrstuvwxy yxy b y yxwvwxyxyxywvwvututstututststsrqrqrqsrpqpqpqrqrstsrstststststsrqpqrqpononmnopqpqrqpqpqrstststststutstsrqstutututstsrsrqrstuvwvwvutututututuvwvutsrqruvwvpqrqrqpqrsrqpqrstuvwvwxyxyxwxwvwxyxwvwxyxy bcdedefgfghghgfededefghikjihihihijihghijijijkjihgfefedefedededefghihgfghghihgfefgfefededcbcbcbcb y yxwxy b bcbcdededefedcbcdedefghijijijijihijijijijkjihijkjihgfgfededededc yxy yxyxy y yxyxyxwvutststutstststsrqpqpopqpononopopqpqrstutsrqrqpopqronmlmlklmnmlmnmlmnonmlmlmlmnmlkjkjklkjihijklklmlmnonmljklmnmlmnopqrsrststutsrqrqrqrqrstutsrsrqrstututstsrsrqrsrqrstututstsrqrqrqpqrqpqrqrqrqststuvwxwvwvwxwxyxwvwxyxyxwxwxy b xyxyxy yxwvwxy bcdefghgfededefedefgfgfghijihghgfgfghghghihijihghihijijihgfgfghgfghghihghgfedcbcdededefghihijijijkjijkl
qr44bi,0,can a trained discriminator in gan be used for multi class classification i have been thinking of using discriminator as a classifier after it is trained once with gan but i have no idea on how to make discriminator into classifier because discriminator only says if it is fake or real any leads please
rrn7xe,1,data image loading help for pytorch hey guys so a couple months ago i posted about making an image classifier for a school project so far i have the project mainly working albeit with a pre trained resnet model from online now i need to replace that with a custom built model in order to make the project more complex iâ€™ve decided on the caltech256 dataset as that seems to be a decent size while still being manageable to process on a laptop iâ€™m looking into how i can load the dataset into my program as that seems to be the first step and iâ€™m at a bit of a dead end really i canâ€™t seem to get the torchvision class for caltech256 to work no matter what i try iâ€™m fairly new to python so i imagine that doesnâ€™t help i also had a look at trying to do it manually but iâ€™m not sure how doable that will be if anyone can guide me in the right direction that would be much appreciated i would prefer to use less libraries if possible but beggars canâ€™t be choosers and if torchvision datasets is a lot more beginner friendly then thatâ€™s fine too thank you
r7gt1m,0,data platform generator data solution blueprints hi everyone we took inspiration from matt turckâ€™s data ai landscape and scott brinker s martech landscape to generate visual data solution blueprints to inspire you in the design process of your data platform and help you to find alternative combinations of data technologies our idea was simple let s take some technologies from these landscapes and generate simple visual data solution blueprints to inspire other data aficionados in the design process of a data platform also expose them to different technologies that exist on the market which could be good alternatives to already better known technologies create a generator to display one picture at a time we have millions of combinations can you discover all of them we used for the first time no code applications like bubble and airtable we launched this page today this is a concept site we call picture data solution for all the data aficionados out there have fun be inspired and we hope you enjoy it don t forget to share it with your friends if you like it we also appreciate your brutal and honest feedback
rdtf5w,1,how do i go about studying gans i m looking for any prerequisites there are to understanding gans i m familiar with a bunch of ml algos at the level they were discussed in intro to stats learning i m comfortable with probability and linear algebra at a college level please suggest how i should go about this 1 what papers do i need to start off with 2 how familiar do i need to be with other deep learning network architectures cnn rnns 3 what other dl concepts do i need to familiarise myself with 3 is the deep learning book by goodfellow a good resource to learn gans
riy24d,0,what does bert s self attention actually look at the self attention mechanism is central to the success of the transformer and with it all of the large language models gpt2 gpt3 bert t5 that have spawned from it this towards data science article summarizes the work of a variety of researchers digging into what these self attention heads are actually looking at the surprising answer is that a large fraction of them are just staring at meaningless separator tokens and only a small handful are performing anything analogous to traditional nlp tasks like coreference resolution
rn9f19,0,what are some cool random forest ml applications i have seen some jaw dropping examples of neural networks and deep learning e g deep fakes i am looking for similarly awesome examples of what random forests can do please share
rd5uti,1,any resources or learning materials for learning machine learning
razsj2,0,why do people â€œreadâ€ as many papers as possible iâ€™ve got a few colleagues who always claim to be reading papers but the way they â€œreadâ€ is so damn superficial as an example i had just finished fully reading comprehending a paper and i wonâ€™t lie took me a solid couple days to understand everything fully and reading things multiple times meanwhile in the daily meetings we have i mention the paper and how we should try and use some of their components in our own work and someone says â€œoh ya i read that in like 15 minsâ€ so we decide to have an impromptu discussion on it and jesus christ i swear the only thing he read was the abstract and maybe glanced at the network architecture iâ€™m sorry this is turning into an rant it just really grates my nerves when people say they read something and in reality all they did was look at the abstract iâ€™m a firm believe that reading comprehending and fully understand 1 single â€œkeyâ€ paper from whatever field youâ€™re studying is a much better investment of your time than skimming through 100 regurgitated ideas edit guys just to clarify i do believe in skimming abstracts and looking for interesting papers i go through dozens a day myself youâ€™d be lost otherwise haha i take issue though when someone claims theyâ€™ve â€œreadâ€ something when all theyâ€™ve done is gone through the abstract and glanced through it
r47k8g,0,smart solution to reduce an impractical number of iterations in a time series model hello folks we have a timeseries based ml solution that helps with anomaly detection and finding the root cause for this anomaly how it currently operates is if i see an anomaly in a metric if the actual is far from prediction we try to examine the associated dimensions and associate the anomaly to its root cause accordingly for example considering covid related deaths across the us if our model senses an anomaly steep rise fall we try to break down the available data by state and see which state is responsible for such a steep change in this example there is only one dimension which is the state so our model in essence iterates over 50 states hence 50 iterations when we try to scale this up to multivariate that involves for example 5 dimensions to the already existing state say we add gender age group income group health rating the number of iterations is going to explode what was earlier 50 is now going to become 50 states 4 genders 10 income groups 12 health ratings 24 000 iterations when such a model has to detect anomalies in say e commerce data where the number of dimensions is going to be larger i hope you see how can t our model scale at least without parallelization how do you guys think can we solve this any suggestion is greatly appreciated thanks
re71yb,1,how to choose hyperparameters of gaussian process hello there i am working on a project where i have to model some plots using gaussian regressions i would like to use grid search to make an exhaustive search of as many hyperparameters and kernels as possible that being said i don t really have any experience with gp so i don t know which hyperparameters and which combinations of kernels i should be testing out i tried to look for some examples but either the examples are too specific to be transfered to my case or i found very complicated math explanations that didn t really help me concretely any help would be very welcome thanks
r2njr1,1,what model should i use for this simple animation generator i ve been getting into computer vision recently and i got this idea to make the simplest possible video generator like this one i m still thinking if i should make it as like 1 input frame and output is 3 sec animation predict next frame also i know that for example gans look good but are complex and i may not need them i m not really sure if i should use a gan stylegan biggan pix2pix or an autoencoder vae or something else
rxgv6l,1,any resources to improve coding in ml ai i am looking to improve upon my coding skills specifically in ai ml i would say i am relatively advanced in terms of ml knowledge however i struggle to implement papers concepts on the fly from scratch which is mostly due to me being weak in coding but also due to some little knowledge gaps as well x200b i m wondering if there are any good resources for tutorials on implementing research papers from scratch cool projects i ve watched aladdin persson s videos and they re super good i am looking for something similar x200b thank you
rlewuz,1,face filter project using mediapipe and p5 js hey everyone christmas is coming so i ve tried to make a small 2d face filter web application using mediapipe and p5 js youtube check it out note please use chrome in order to run it i m getting some css issues on firefox as i m not very good at css and if anyone is looking for source code you can also share your screenshot with me with the filters hope you enjoy the project cheers
r4yj1u,1,why is 1 the best value for laplace smoothing hello everyone i have applied laplace smoothing with k values lower and higher than 1 on my naive bayes classifier comparing the accuracy and f1 scores it is obvious that k 1 is the best value for smoothing i was wondering why would be grateful for any feedback
r519uc,1,what is most beginner friendly python machine learning library i just wan to start with machine learning i searched on internet for python library for ml what is the best library for beginners
r1sqzw,1,where should i start with ml hi everyone i m 22 and i have a beginner knowledge of python from past learning experiences i m really interested in the world of machine learning neural networks and artificial intelligence but i don t know what the roadmap is for learning about this if anyone could point out the steps and maybe recommend some good learning material or courses i d be really grateful
rs31ic,1,what is l2 distance a 2 minute visual guide x200b ðŸ”µ euclidean distance ðŸ”µ ðŸš€ when we talk about distance we generally mean euclidean or l2 distance as this is the one that is commonly taught in school and euclidean geometry has a wide variety of applications ðŸ‘½ the l2 distance is computed using the pythagorean theorem and the cartesian coordinates of the points in space l2 distance is the p norm when p 2 but of course like all other distances it can be applied to measure the closeness of two objects in higher dimensions as well ðŸ”· about the squared circle since many shapes and concepts in geometry involve distance the way you define this distance can have an effect on this and its properties when you choose the distance to be defined by l2 distance you get the familiar round circle but when you do that with the l1 distance you get a rotated square as a circle ðŸ¤“ l1 and l2 norms are used to perform the regularization of parameters of a model if you have a neural network or a simple linear regression model you can regularize to prevent overfitting lasso and ridge regression use l1 and l2 regularization respectively if you like such content and would like to steer the topics i cover feel free to suggest topics you would like to know more about in the comments
r1sj1e,1,which gan make people not real smile best i d like to train a gan to get a people from sad to smile but the people is not the real people it should be like the people in famous painting like mona lisa which gan model or pretraned model can i use and which dataset can i use please thanks a lot
rv2j9k,0,the illustrated retrieval transformer gpt3 performance at 4 the size hi r machinelearning i spent some time wrapping my head around deepmind s retro transformer and visualizing how it works hope you find it useful all feedback is welcome
qmy3ir,0,why do we need the random noise z in conditional gans obviously we need some kind of input for the neural net but in the case of conditional gans we have another kind of input does the random noise z then only serve to introduce variety for a given condition e g many different faces all with blonde hair if i didnâ€™t care about this variety could i just do without the random noise or is there some other justification for why we need the random noise z makes training easier some theoretical reason â€¦
rf59gu,0,jina an open source framework to build scalable deep learning applications in mins jina is a neural search framework that empowers anyone to build sota and scalable deep learning search applications in minutes think building image search video search semantic search and more quickly â±ï¸ save time the design pattern of neural search systems native support for pytorch keras onnx paddle build solutions in just minutes ðŸŒŒ all data types process index query and understand videos images long short text audio source code pdfs etc ðŸŒ©ï¸ local cloud friendly distributed architecture scalable cloud native from day one same developer experience on both local and cloud ðŸ± own your stack keep end to end stack ownership of your solution avoid integration pitfalls you get with fragmented multi vendor generic legacy tools how to get started pip install jina checkout jina on github this is a design pattern that will get you to speed quickly and make good choices for your next deep learning app i m seeking your feedback do you find it too much opinionated or too much abstract
rrh38q,1,sample size justification for ml i m in a position where i need to write up a section on sample size calculations for an ml study this is targeting a medical journal and they basically want a justification for the size of the final test set the thing is that i haven t done anything approximating sample size calculations in this field before the task involves using a deep learning algorithm to predict an angle measurement on an image continuous and comparing that prediction to angles manually measured by experts the final metric we re targeting is icc between the prediction and expert raters we produce this metric on a hold out test set of images so my question is how do sample calculations really apply here i m not trying to really prove any effect size the only thing i m attempting to show is whether the predicted angle agrees with the expert measures
ret3mw,1,introduction to artificial intelligence i like to share a short article about artificial intelligence this article is supposed to be a short introduction on the subject and any comments are welcome
rux10m,1,unsupervised classification for optimal images for ocr i want to categorise the image into 2 categories if we can see numbers properly on the image like in 1st image and if number are rubbed off like in 2nd image 1st image x200b 2nd image
rlvyvl,0,what architectures exist for time series map prediction i ve been given a problem of generating a map of predicted dust clouds imagine you have an n by m grid and you know the amount of dust in each cell you also know how much new dust is being produced in each cell intuitively the amount of dust in a cell at time t 1 will depend on the amount of dust in the nearby cells at time t the amount of dust being produced in nearby cells at time t and other factor such as wind for simplicity lets ignore the win i m curious what architectures exist for this sort of problem my initial idea was to have an n by m by 2 input shape and an n by m by 1 output the input would be the current dust map in the first layer and the map of the newly produced dust in the second layer the output would be the dust map after one time step in this case a convolutional autoencoder seems like the simplest solution this technique is used in this paper very few models for dust exist so i ve been researching weather prediction models as they are very similar to my problem i ve seen techniques that use gans although the discussions on this forum about it seemed to have some doubts about its effectiveness i ve briefly looked at r cnn methods i don t think they would be applicable as they are more for object detection but feel free to tell me otherwise i ve also considered that if my grid is small enough i could probably get away with not using convolutions at all and could just run it as a straight rnn problem but that might require a lot more data to converge as its missing the adjacency context the convolutions provide any ideas are much appreciated
rggzay,1,french beginner needs help for a very important project hello i am a french student and i have a citizen project to do i am trying to set up a political program comparator for the french presidential elections in 2022 to fight against the abstention which affects mainly the poorest in france i am a beginner in ml but i managed to collect through a survey about 10000 answers on the 10 favorite themes of the french for the first step i just want to be able to classify the user between two categories left or right according to his answers to the questions and in a second step ask more specific questions to match him with the candidate who answers the most to his expectations my problem is how to exploit the data obtained and what algorithm would allow me to best classify the new users into two categories i thank you for your help
r7dbuf,1,which laptop is better for machine learning view poll
qvmkjg,0,surprisingly simple sota self supervised pretraining masked autoencoders are scalable vision learners by kaiming he et al explained 5 minute summary by casual gan papers the simplest solutions are often the most elegant and cleverly designed this is certainly the case with a new model from facebook ai research called masked autoencoders mae that uses such smart yet simple ideas that you canâ€™t stop asking yourself â€œhow did nobody think to try this before â€ using an asymmetric encoder decoder architecture coupled with a data efficient self supervised training pipeline mae pretrained models outperform strong supervised baselines by learning to reconstruct input images from heavily masked image patches 75 blank patches full summary blog post mae upd i originally included the wrong links arxiv code subscribe to casual gan papers and follow me on twitter for weekly ai paper summaries
qvhazn,0,is it possible to generate text based of a previously generated text with huggingface i currently have a gpt 2 based text generator that i trained on shakespeare plays currently the generated text looks like this first murderer what is t mark antony sirrah i pray thee thou wilt fight with my sword cassius if i know thee sir i cannot tell but if i knew t we should have it but that tis not well thou art no sooner mark antony why what s t timon so are the very hour tis but to have the best to make it and i am sorry of it malvolio and he his lord king claudius if that had been but i will not king henry vi my lord talbot florizel ay sir tis better than i prospero in the measure of it shall be well come in thy mouth come i ll be some water or else for if thou couldst hear me think thy face were quick but by thyself my heart with this new heart i could have it with thee come no no for thy heart o he shall have the blood which he is to a fair and thy bed his death s in peace now for thy heart did he not look it must be an hour s day a death which shall i have it however the problem is that the text is very disconnected and not really coherent i m not really sure if huggingface provides an api to do this but i d like to be able to have the x i generation somehow take into account the x i 1 which i feel like would make the text a lot more coherent and less disconnected in this case each piece of generated text is started with the all caps character name does anyone who knows a bit more about this stuff know a better way to generate realistic and large amounts of coherent text the problem is that we can only generate 768 tokens at a time so obviously models that write entire paragraphs of text have figured out good way to use previous output in text generation as some sort of initial state for the future generation i ll also add that if i just pass in the previous input ids into the model generate method then it doesn t generate any more text since the previous text already has an eos token
r93h2k,0,dealing with missing values for anomaly detection with numerical data i have a small numeric dataset with around 400 attributes all numeric and 200 instances the target is a categorical attribute with 4 different categories indicating the state one class represents normal condition and the other 3 represent anomaly types the instances representing the normal state no anomaly have no missing values however the instances representing anomalies have missing values for most of the attributes in addition the names of the attributes for which there are missing values correlate with the anomaly type e g for all instances of anomaly class 2 attributes from 10 to 400 are missing while for all instances of anomaly class 3 attributes from 300 to 400 are missing what would be the best way to deal with the missing values in this case for the supervised and the unsupervised case i m not sure which methods i m going to use but i would need to cover both supervised and unsupervised cases
r8jea7,1,what specific ml technique should i use for predicting a country s religion through the attributes of its flag colors stripes crosses etc it s my 2nd year in the computer science field and machine learning already broke my brain just trying to learn it don t get me wrong the concept of machine learning is so cool and that s why i gambled myself into learning it and making something out of bit we are required to create any system that can predict something out of a dataset my idea was to have a dataset of countries with its flag and its attributes still trying find it somewhere online or i ll create it myself and predict the country s religion either nationally recognized or practice by the largest population however i can t seem to think of a technique that would help this project i m thinking of neural networks but it might be a bit difficult technique to start but i won t mind doing it as long as it is the best option that s why i m here asking the humble but powerful redditors to guide me peace
ri76tj,0,geo deepfakes are not far away here is a webapp demonstrating the capabilities of geo deepfakes
roy4nw,0,comparison between player of games and alphazero in this blog post i describe the differences between deepminds new algorithm player of games and alphazero i describe what gt cfr is with some code examples of how to implement it in python blog post
r6asy1,1,poll on why data quality remains the top level concern for most machine learning teams a recent study by concluded that data quality is increasingly becoming a top level concern among companies implementing ai being in the labeling and annotation business ourselves cogito tech llc have noticed several cases where organization approached us for labeling work for various reasons i wanted to know from a larger community that why do you think are the key reasons the demand for data quality is still increasing despite so many datasets being available and so much work being done on them view poll
r90f3v,0,embedding consecutive video frames close to each other in latent space what if we embed consecutive frames of a video close to each other it would mean that if we walk the latent space we should get a video instead of visually similar frames as in vae i think it may prove useful for example in world models where it will make it easier to predict the next state thanks to the fact that it will be close in the latent space what do you think about this idea did somebody already try any papers on this topic
r2ouvr,1,how large should my dataset be hey guys i m an ai student and i m new to deep learning i want to work on breast cancer detection using cnn or other architecture i found an ultrasound imaging dataset which contains 437 benign 210 malignant and 133 normal images each image with its mask do you think that this is a sufficient amount of datat to train validate and test my model thanks in advance
rfb0c0,1,designing cnn from the image below i want to design a cnn from scratch using the figure i m new to the concept and getting a little confused about adding the conv2d and maxpooling layers how should i go about it
rmrifl,0,multi output generation with sparsely fixed output i am trying to use generative deep learning to synthetically generate certain biological parameters given some experimental constraints i am using gans to do this and i have been successful so far for the next step i want to fix certain parameters in the output vector to some experimentally verified values and then generate the rest of the parameters in the output vector based on this the features in the output vector of the gans are obviously coupled by some complex non linearities so changing or fixing one of them will propagate to the other outputs as well is there some way to do this what i have already tried researched 1 transfer learning on a trained generator with a curated training dataset where the output features i want to fix are already constrained it works the gan learns to fix the values of certain output features but this is because they are already fixed in the training data 2 context encoding in gans i am aware that context encoding and similar tasks have been already showcased where gans are used to fill in missing regions in an image based on the rest of the image but extrapolating this to my case is hard to make a direct comparison my case is analogous to having an image that has only a few pixels that are shown and the rest of the image is masked and need to be inferred i could not find any literature on such sparsely fixed output if anyone is aware of some literature or blog that tackles a similar problem please let me know
r2bbvl,0,what is causal machine learning i ve been hearing this term a lot and recently my advisor supervisor asked me to look into it but i just dont get the idea isnt causality supposed to be impact and effect of actions taken before any given time and how they impact the current decision isn t this exactly rl and nlp i don t understand why a new field is being defined as causal machine learning also what are some implications of research in this area is it just theoretical i see a lot of language vision and rl models apparently performing well already why do we need causality is this just a hype or really an important concept x200b sorry if my questions are basic simple i just dont understand this causality yet thanks in advance
qka32i,0,2d models on 3d tasks convolutions simple replace 2d tasks enjoy a vast backing of successful models that can be reused for convolutions can one simply replace 2d ops by 3d counterparts and inherit their benefits any extra steps to improve the transition not interested in unrolling the 3d input along channels pubs code help
rrlgee,1,fun and simple cat mouse challenge how would you solve this numberphile link let s assume that we re not clever enough to come up with the solution presented in the video how could we derive a solution with ml which algorithm s would you choose
qo5in1,0,gpt 3 is no longer the only game in town hey there i just put out a little article that you might find interesting gpt 3 is no longer the only game in town it catalogues the appearance of models akin to gpt 3 over the course of 2021 like hyperclova and such hope you enjoy it tldr organizations face significant challenges in creating a model similar to openaiâ€™s gpt 3 but nevertheless a half dozen or so models as big or bigger than gpt 3 have been announced over the course of 2021
qv3xpt,0,cycada feature level gan loss in the cycada paper equation 5 in the paper makes use of f s however the corresponding portion in figure 2 orange only makes use of f t additionally f t makes more sense since the input is in the target domain should the f s in equation 5 be f t
ru57dw,1,what are the infrastructure requirements for deploying an nlp model i am a freelancer who builds ai models for small companies and one of them asked me to deploy the model into their production environment and wanted me to give the infra req related for that i don t know what are the requirements regarding hardware software and integration needed for the same any good articles on this would be appreciated the one i m getting on google are very generic need something specific the nlp model is based on distilbert 250mb in size tia
r4okv5,0,what are some ways to reduce model gpu vram usage hey guys my gan models take 10gb vram when i try to generate a 512x512 image we need to scale stuff up and i plan on productionizing this on a kubernetes cluster but want to optimize them first to save on costs ideally i want to run multiple inferences on the same machine without facing a cuda out of memory error what are some good ways to optimize vram usage
qkwk6j,0,how to correlate the results from an extremely imbalance data to performance relative to a random guess hi all at a project at work i handle with an extremely imbalance dataset 699 cases of positive outcome while around 15 000 negative cases in addition our cases are relatively hard to separate and in some cases domain knowledge experts are struggling with manual classification in that context i was asked to explore different classifiers and present them in a poc report at first i tried a naive approach and dumped the data as it is used a train validation test splitting with stratification option on all the models predicted 0 all the time to maximize accuracy then i used over sampling with smote package in python and changed the criteria to the area under the precision recall curve in the text set i predicted correctly 30 21 70 and i had a false positive rate of around 12 regardless of possible model modification or boundary predict 1 if the model predict a higher score than 0 65 for example i am having some problems in defining my metric to evaluate the results our data is imbalanced so 30 is good in addition false negative is strongly worse than false positive in addition today i presented the results to my manager and he asked me to prove him or argue that the results are better than random guessing i thought about two things to evaluate my results randomly draw 1000 observations with a prior of 5 equal to 1 the rest to 0 than randomly guess 5 of them to be 1 and compare it to my results bootstrap this scheme to get a distribution and check where is my model performance along the distribution take the examples from the test set assign the same number or normal observations and give the hr experts to classify them manually then compare results i will be glad to hear what do you think about the problem and the suggestions thanks
ril4sy,1,gpt 3 libraries has anybody made a full fledged library of functions powered by gpt 3 does spacy use gpt 3 in any of their methods for example thanks
qq21s6,0,why does amd do so much less work in ai than nvidia or is the assumption in the title false does amd just not care or did they get left behind somehow and can t catch up x200b i know this question is very vague maybe still somebody can point to a fitting interview or something else
rqtl0z,0,how to create a question answering system with a potentially very large corpus of text hello guys so i was wondering how you would go about creating a query answering system based on a potentially large corpus of text i had some decent exposure to nlp and i realize how we could use a transformer for say answering questions in squad format where the reference text is small and you can pass it together with the question to the transformer to get an answer however how would you answer the questions based on say a very large corpus of text when it will not be possible to pass the whole text to the neural network each time you ask a question one option that i see is going through the text looking for similar words sentences however that might be very costly for each question ideally it would make sense to create some kind of knowledge base based on the provided corpus and use it to get the answers but i am not entirely sure how it should be done so do you guys have any references best practices thanks so much in advance
rebagw,1,what is the process of creating a machine learning model using u net have i understood it correctly for my university course i am learning about u net and how that can be used for image segmentation e g in cancer biology however can someone please tell me if i have misunderstood things wrong about the workflow of things i ve tried to find research papers online that give me an overview but i can t find a dang thing apart from articles that just outline the basics 1 step would be to clean and sort through the data e g making sure you there are no duplicates in the image dataset 2 divide the dataset into the test set validation set and training set 3 the training set is used so that the model can learn what to do 4 the validation set is used to look at how accurate the model is and this is where we can change the hyper parameters to make it more accurate what type of hyper parameters can be used though when looking at image segmentation 5 then we use the test set for the final test and evaluate how well it performs can we evaluate how well it performs using things like the dice coefficient after we have used the test set can we go back and refine the model or is that supposed to be during the training phase thanks in advance
r8tnzq,1,batches in sequential iteration of sequential data hello all i have been studying rnns from the d2l ai book in the part where they describe how we are reading the data sequentially there is a choice they have made and i have been wondering if there is any reason for this for example let s assume our sequential data is as follows my seq 1 2 32 33 34 when they iterate through the data they get the following values for the first two batches x tensor 0 1 2 3 4 17 18 19 20 21 first batch x tensor 5 6 7 8 9 22 23 24 25 26 second batch my question is is there any reason that we are not iterating through the batches as x tensor 0 1 2 3 4 5 6 7 8 9 first batch x tensor 10 11 12 13 14 15 16 17 18 19 second batch if you want to read more about the chapter i am talking about you can check the corresponding chapter thank you
r74ci2,0,adding a classification layer after a clustering model is this correct in a current project we are given a list of clients to be segmented using a clustering model for this we use kmeans we get decent clusters well defined and all the issue comes after when a new client is to be inserted in the database it needs to be added to one of the segments for this we thought about adding a classification layer using the data with cluster information for training and test first attempts logistic linear svm decision tree resulted in overfit and through some downsampling and analysis we finally managed a not overfitted svm the doubt still rose is this a correct approach to the problem might there be some sort of data leakage occurring are we unintentionally biasing the results aby insight is highly appreciated
r91vk3,1,why can methods like resume chronotron and span only train single layer spiking neural networks many papers which discuss them mention that they are primarily suitable for training single layer spiking neural networks however it is not clear to me why this is the case is it because they use local learning rules
rlmr8s,0,a hand picked selection of the best python ml libraries of 2021 hi all we have been compiling a list of the top python libraries released or popularized in 2021 hoping that you ll find some good ones here that you might have missed during the year most of these are related to data science machine learning workflows we decided to expand the post by listing many more than 10 libraries although we do have several main picks which are related to ml awkward array jupytext gradio augly skweak evidently jina and finetuner hub the full list with expanded descriptions is available here in case you are curious there are many more listed under extra picks all relevant to 2021 or late 2020 so what do you think about our picks did we miss any good ones
rkoj9z,0,what open source dataset lacks annotations hi r machinelearning data scientist at dagshub here i m personally excited about open source software and open source data science as part of the recent hacktoberfest we decided to host a community challenge to create and curate open source audio datasets â€“ we got over 40 contributions from you which is awesome these are all easily accessible here we want to keep the momentum and continue contributing to the open source dataset ecosystem â€“ this time by focusing on a labeling challenge we want to make something useful for everyone so i d love to hear from you what open source dataset lacks annotations and what you think will help the community the most the only limitation is the data must be open source so it can be made available to everyone i d love to hear your thoughts and feedback
qnq3m4,0,open source project to collect data from multiple databases apps saas tools and prepare for ml tasks we have lots of business data scattered across different databases and apps rudderstack can integrate data from various sources and then activate this data in your warehouse or business tools for ml operations this unlocks the potential applications of the data which was hard to do before rudderstack e g ux personalization business analytics etc i m seeking feedback what can i improve and how would you want to use it ama
rqfcx3,1,isolation forest question do you agree that outlier fraction and a priori formulation of that parameter is main drawback of iso forest algorithm it demands very presice knowledge of anomaly appearances
rwwdh0,0,multi agent deep reinforcement learning hello i hope youÂ´re doing well i am working on a multi agent system with maddpg at time t when an agent asks for a task the other agents are busy i e the busy agents are those that are still processing a task they didnÂ´t finish it yet so with this configuration in the learning phase i donÂ´t know how to mask the state of the busy agents when injecting the state and action pair to the critic network thank you
rni9dt,1,where can i download the unsolved coding assignments of the nlp course provided by deeplearning in coursera by youness mourri and lukasz kaiser i have currently completed the 1st course which was free but for the other courses the coding assignments are locked and i can t afford all these courses i also checked some of the github repos but only solved solutions were available
qukqtz,0,knowledge graph applications with ml and ai and open source database links in 2022 a new terminology is coined by google in 2012 â€œknowledge graphâ€ this knowledge graph has its own significance in the field of machine learning due to which performing capabilities of machine learning techniques are getting better day by day with a high accuracy rate read more
r52qrb,1,how to transform categorical data to numerical data using pandas i am writing a python program that uses logistic regression to predict an outcome based on survey data from a csv however i m running into the issue that some survey data is non numerical i need to transform categorical data to numerical data without knowing which columns are categorical or how many categories per column there are ahead of time be able to map the numerical data onto the category labels later any suggestions on how to approach this i sincerely appreciate any thoughts example data weight systolic blood pressure has diabetes 155 119 no 210 131 yes 301 143 yes example output weight systolic blood pressure has diabetes 155 119 0 210 131 1 301 143 1 diabetes dict 0 no 1 yes
rbvn1s,1,cannot load data file into weka i download a data set which is of type data file i can open it in notepad and its content contains lines like this 18 0 8 307 0 130 0 3504 12 0 70 1 chevrolet chevelle malibu i convert it to arff using this link but the type doesn t change and my weka cannot recognise this file sry if this is a dumb question but its my first time using weka edit for anyone that has a similar problem a friend of mine gave this article to follow and solve itl
qxidh8,0,machine learning bootcamp train for future ml contests we invite everyone to the online bootcamp on machine learning participation is free we decided to hold the machine learning bootcamp to train participants for future ml competitions ml bootcamp is a two day intensive training that will help you increase your machine learning knowledge acquire practical skills and study basic deep learning tools we welcome undergraduate and postgraduate students making the first steps in the world of ml contests you just need to register there are no strict prerequisites but potential participants should at least be familiar with classic ml algorithms like linear models and decision trees pandas numpy matplotlib and sklearn libraries when 4 5 december 2021 where online join
rsyiyj,1,shared weights between different implementations hey all i am currently implementing a convolutional neural network in rust goal is to compile to wasm and deploy on the browser so far i have an implementation that matches tensorflow up to almost perfect accuracy we have some very slight errors that are probably unavoidable with floating point computations f e 0 70710677 vs 0 07071068 now the goal is to port a model that already works from python to rust i already have the weights from the trained python model and simply load them into rust the problem is those small numerical differences add up harder and harder each layer while after the first layer the outputs might differ by something like 1e 6 a few layers up we suddenly have completely different values are there any ideas to prevent this or is there no other way than training in rust pretty unfeasible as we would then basically have to write a complete backprop and training procedure â€“ our conv implementations likely aren t fast enough for a nice result x200b greetings
ra1vjy,0,artist seeking to learn more about high res image synthesis hey there r machinelearning i m a photographer and artist preparing for a small gallery show in february and i ve been experimenting with vqgan clip and taming transformers as well as image scraping tools like flickr scraper and the likes in the hopes to achieve generative images based on famous artworks i m hoping the kind folks on this sub can provide me with a little guidance here s what i want to do produce ai generated photo realistic images from source images of well known photographs i ve tried creating my own image segmentations to feed to taming transformers but it appears that it doesn t handle photographs involving people very well only landscape images is there any way to get somewhat photo realistic images generated by ai
rtjo93,1,yolov4 add new class to already trained model without training on entire dataset i have already trained my model on a dataset for letâ€™s say 50 classes itâ€™s trained to recognize letters now letâ€™s say i want to add a newly discovered letter so i want my model to also be able to recognize this letter how would i go about this i would prefer to be able to train my existing model with a dataset only containing the new letter to â€œmake it aware ofâ€ this new letter too appreciate any help
rmd67z,1,inerview question repositries can someone give some good resources on interview questions on ml with answers theoretical question and case studies which are tougher practical and than basic ones which we get on google
qoqp4w,0,discussion mlops tool for image data management and exploration hi there x200b large part of the current work of our developers is dedicated to manual work on data this is done in several stages of the development 1 in the beginning when we get the annotated data we go over it thousands of images for segmentation tasks we observe the annotated images to validate the annotation correctness and images relevance for our tasks some images may be correctly annotated but not relevant to our task or use case so we remove them 2 we check statistics of the data to validate that the data isn t biased for example that we have enough samples from each class and that the distribution of instances sizes e g number of pixels in the segmentation mask within the classes is reasonable 3 when we have a trained model we go over images for which the model gives poor results and try to find similarities to understand weaknesses of the model and find root causes this often leads to change of the data and repeating stages 1 2 and eventually 3 these are very time consuming tasks and we are looking for mlops tools that will make this work more efficient optimally the tool will also take care of other mlops aspects like experiments orchestration experiment tracking and data versioning we can also combine several tools but using few one tools is preferred open source or proprietary paid product are both possible any recommendations thanks
rur95m,0,anyone switched from vision to robotics iâ€™m about to finish my phd and the whole field of robotics looks so exciting right now especially applications like farming and recycling has anyone switched from more pure deep learning vision nlp to robotics and how did it happen did you just get a robotics related job focusing on the vision side of things or is it key to have more experience on the robotics side before getting a job also iâ€™m curious whatâ€™s the best location for robotics like how you go to hong kong new york for finance sf for software or shenzhen for hardware
qrwuvy,0,what must every phd doing ml know before graduating i am a 3rd year phd who finally finished all program requirements classes etc and am fully focused on research my question is what are some things that your average ml phd should be good at at this point i know its subjective and depends on their research field but what are some common things that i should be well versed in
qx5mjd,0,which reference letter dossier do most universities ml phd programs accept i have my 3 references lined up and i m applying for phd programs in ml then one of my references throws a wrench in my plans i m only sending to one place says one of my former supervisors he suggests to me to use interfolio as the reference letter dossier for him to send to i m applying to cmu mit stanford mila cornell uoftoronto and uofillinois does interfolio or is there a dossier system that sends to all these schools
rlg44n,1,grab your digital copy of tensorflow workshop hurry packt has published the tensorflow workshop grab your digital copy now if you feel you are interested as part of our marketing activities we are offering free digital copies of the book in return for unbiased feedback in the form of a reader review get started with tensorflow fundamentals to build and train deep learning models with real world data practical exercises and challenging activities here is what you will learn from the book 1 get to grips with tensorflowâ€™s mathematical operations 2 pre process a wide variety of tabular sequential and image data 3 understand the purpose and usage of different deep learning layers 4 perform hyperparameter tuning to prevent overfitting of training data 5 use pre trained models to speed up the development of learning models 6 generate new data based on existing patterns using generative models key features understand the fundamentals of tensors neural networks and deep learning discover how to implement and fine tune deep learning models for real world datasets build your experience and confidence with hands on exercises and activities please comment below or dm me for more details
rd609b,0,3d printing process parameters optimization hi i am doing my dissertation and i have to make a machine learning model to optimize 3d printing process parameters any 3d printing process the probelm is i can t seem to find any data online all i have is 2000 records of data of an l pbf process 4 predictor variables which is largely synthetic i have been trying to research if i can actually make an ml model based on so less data any help would be appreciated p s i know this is a machine learning subreddit but any leads regarding 3d printing process parameter dataset would also be appreciated thanks
r277kg,1,supervised semi supervised unsupervised and self supervised learning when i first began learning machine learning i had difficulty understanding what exactly supervised and unsupervised learning are i wrote an article describing my understanding of them with the addition of semi supervised and self supervised learning hope you will like it
rtv9cd,1,ml learning advice i am a 3rd year economics student i have completed my statistics calculus mathematical economic based vastly on matrix algebra microeconomics and macroeconomics courses with a and my gpa is 3 51 i am familiar with web development for almost 4 years also so i have more than a general knowledge in programming i am currently taking econometrics algorithm design and analysis and introduction to database systems courses i am very interested in quantitative analysis ml what advices could you give me to improve myself in this field i also want to pursue an academic career
r76vis,1,help with first complex ml code hi all i have limited experience with ml and would like to create somehting a little more complex i am not expecting full solutions but if anyone could just tell me a few keywords i would need to google to get myself started with what kind of ml could solve this if any it would be very appreciated baisically i work for a contract manufacturing pharma company when we get a new drug we need to make there is a list of around 20 30 different liquids that are involved in making the drug itll come in the form like this liquid volume needed l corrosiveness factor needed in step 1 needed in step 2 water 10000 1 x x acid 5000 50 x etc x200b what i want to automate is the long and tedious process of allocating which of our tanks 1 make and 2 store each liquid so i have a list of tanks like this 1 for preparation tanks and 1 for storing tank name volume l material p 001 20000 steel p 002 5000 reinforced steel etc x200b and to complicate this i have another table like this tank p 001 p 002 etc s 001 x x s 002 x etc x200b basicially its a long table showing the connections between the preparation and storage tanks an x means that there is piping available to pump the liquid from for example there is piping between prep tank 001 and storage tank 001 but not from p 001 to s 002 therefore you cannot prepare a liquid in p 001 then store it in s 002 there is also another table tank step 1 step 2 s 001 x x s 002 x etc this shows the connections between the storage tanks and the acutal manufactuing floor if a liquid is needed in step 1 and 2 it should be stored in s 001 to avoid needing new piping etc so there is a long list of rules x200b you must strictly follow the preparation storage connections as no new connections here can be made if a liquid has a corrosiveness factor number above 30 it needs preped stored in a reinforced steel vessel but less corrosive liquids can be stored in any tank volume of the tanks should be sufficient to prep and store all of the liquid its inevitable that some new connections between the storage tanks and the manufacturing floor might need to be made but the allocation should be designed so as to keep this a minimum output will look something like liquid prep tank allocation storage tank allocation water p 001 s 001 acid p 002 s 002 etc along with the number of new pipings that need to be made between storage and manufacturing i understand this is complex but is there any method of ml that can take multiple tables like this along with a strict set of rules along with around 30 previous allocation projects and then make its own algorithm to determine allocations if you give it a list of liquids with their volumes corrosiveness and where they are needed in the process thanks
r3a0dx,1,is ai ml hard to learn i applied for artificial intelligence machine learning course in my college nd im stressed coz of this everyone says machine learning is tough difficult i want to know basics about this course so that i can motivate myself to learn more hope i can something from u guys
qpuqtk,0,the how and why of bayesian nonparametric causal inference a nice summary paper at the cutting edge of bayesian causal inference link abstract spurred on by recent successes in causal inference competitions bayesian nonparametric and high dimensional methods have recently seen increased attention in the causal inference literature in this paper we present a comprehensive overview of bayesian nonparametric applications to causal inference our aims are to i introduce the fundamental bayesian nonparametric toolkit ii discuss how to determine which tool is most appropriate for a given problem and iii show how to avoid common pitfalls in applying bayesian nonparametric methods in high dimensional settings unlike standard fixed dimensional parametric problems where outcome modeling alone can sometimes be effective we argue that most of the time it is necessary to model both the selection and outcome processes
rrlenc,1,should we do pca before running naive bayes for achieving conditional independence the features need to be independent and uncorrelated would applying pca give better results with naive bayes model or are there any other ways to make this model more accurate
rvol2l,0,what are the reviewers score of the submissions nominated for best paper award in top ml conferences such as neurips icml aistats etc i submitted a paper to aistats 2022 that can be a breakthrough with outstanding contributions the paper received 876 scores from reviewers that could only improve to 877 after the rebuttals what are the chances that our submission enters the short list for best paper recognition what are the average reviewers score of the ones getting nominated in these top conferences
ro7xei,0,paper explained â€“ linear algebra with transformers why would one build a transformer to solve linear algebra problems when there is numpy linalg paper charton franÃ§ois linear algebra with transformers video abstract most applications of transformers to mathematics from integration to theorem proving focus on symbolic computation in this paper we show that transformers can be trained to perform numerical calculations with high accuracy we consider problems of linear algebra matrix transposition addition multiplication eigenvalues and vectors singular value decomposition and inversion training small transformers up to six layers over datasets of random matrices we achieve high accuracies over 90 on all problems we also show that trained models can generalize out of their training distribution and that out of domain accuracy can be greatly improved by working from more diverse datasets in particular by training from matrices with non independent and identically distributed coefficients finally we show that few shot learning can be leveraged to re train models to solve larger problems check out the video to find out why f charton trained transformers to solve linear algebra problems and how these transformers work video outline 00 00 linear algebra with transformers 00 41 weights biases sponsor 02 21 why throwing transformers at linear algebra is cool 08 08 how do transformers solve linear algebra 09 50 encoding matrices for transformers 11 28 training data and results 12 43 generalization 16 05 few shot learning
qr5per,0,paper explained autoregressive diffusion models full video walkthrough diffusion models have made large advances in recent months as a new type of generative models this paper introduces autoregressive diffusion models ardms which are a mix between autoregressive generative models and diffusion models ardms are trained to be agnostic to the order of autoregressive decoding and give the user a dynamic tradeoff between speed and performance at decoding time this paper applies ardms to both text and image data and as an extension the models can also be used to perform lossless compression x200b outline 0 00 intro overview 3 15 decoding order in autoregressive models 6 15 autoregressive diffusion models 8 35 dependent and independent sampling 14 25 application to character level language models 18 15 how sampling training works 26 05 extension 1 parallel sampling 29 20 extension 2 depth upscaling 33 10 conclusion comments x200b paper
r8v4fq,0,project dall 3 generate better images with fewer tokens through clip guided diffusion link to images and code link to diffusion model colab this is a project i created that combines transformer image generation with clip guided diffusion it s named after the 3 projects it s based on dalle pytorch clip guided diffusion and vqgan btw a big thanks to lucidrains rivershavewings openai and the vqgan team for their work if in general ddpm gan vae why do transformer image generators all use vqvae to decode images wouldn t it be better to use a diffusion model i was wondering about this and started experimenting with different ways to decode vector quantized embeddings with a diffusion model see discussion here after a lot of trial and error i got something that works pretty well the resulting diffusion model basically absorbs one scaling factor so you can get similar image quality with 16x16 tokens diffusion compared to 32x32 tokens vqgan i then trained a relatively small dalle pytorch model with these embeddings and put the whole thing together in the github colab above in terms of generalization ability eg avocado armchairs there s no substitute for a large scale transformer but for common object categories people food etc the small transformer diffusion works surprisingly well so this is kind of a proof of concept the conditional diffusion approach should provide a flat boost in quality to any transformer model including cogview ru dalle vq diffusion and openai s original dall e the parameters for this architecture is mostly dictated by the pretrained models i m leveraging there are a lot of potential optimizations that i haven t explored for training from scratch such as removing the final unet downsampling layer but i ll leave it to someone with a gpu cluster
r5bqyg,1,do nodes lose activation when activated i understand that nodes take in a sum from other nodes its connected to checks that sum against an activation function and then activates depending on that value a bias but when it activates does it retain that activated value and have it added to its activation every loop retain that activated value and have it added to its activation post activation normalized set its activation back to 0 hope my question makes sense thanks
qniktz,0,how do you structure your cv after i made my first cv i ve been working on the same company for about two years and worked in multiple projects now i m interested in looking out for a bit and reworking my cv and don t really know in what way should i display my projects and skills any senior ai related engineers want to share your cvs or suggestions
rbpj3s,1,ðŸ’Šyour daily dose of machine learning converting deep learning models to onnx format this is a series of posts that i post almost daily i call them â€œyour daily dose of machine learningâ€ so as i mentioned in yesterday s post this week i am sharing some tips and insights about onnx if you re a tensorflow developer or pytorch developer then the first step for you to use onnx in production would be to transform your model to onnx format to transform your tensorflow models into onnx format you can use a tool called tf2onnx the image below shows how to use this tool you can learn more about this tool on their github repo if youâ€™re a pytorch user then you can transform your model from torch to onnx format directly using pytorch this is done using the module torch onnx an example of this connect with me on your favorite social network
qk144p,0,anyone with powerful computers deploying locally i have my own computer that has a pretty good cpu gpu i d rather not spend more time to get a static ip to set up my computer as an official server through my isp or move my model and setup an expensive instance in the cloud is there an easier way to run an inference server on my machine that i am not aware of
rb4c2w,0,how do you go about creating a data driven model just trying to get a bead on how people optimize their models and or create ml models from scratch for a particular problem say you are given a dataset and told to predict some attribute what s your approach after initial data visualization mining from what i ve gathered it seems like googling around for a model that worked previously on a similar model and then modifying it so it works a bit better for their use case seems to be common practice but i was wondering what would the strategy be if all you did not have those models available similarly once you have a decently performing model how do you optimize for the last 2 3 increase in performance without overfitting
rr61kh,1,ðŸ’Šyour daily dose of machine learning different types of gans this is a series of posts that i post almost daily i call them â€œyour daily dose of machine learningâ€ there have been several types of generative adversarial networks gans in the past few years hereâ€™s a quick summary of them ððšð¬ð¢ðœ ð†ð€ðð¬ the first form of gans where you have a generator and a discriminator competing with each other ð‚ð¨ð§ðð¢ð­ð¢ð¨ð§ðšð¥ ð†ð€ðð¬ extension of gans where you have conditional sample generation this allowed for controlling specific modalities for data generation ex generate a face with more or less beard ð–ðšð¬ð¬ðžð«ð¬ð­ðžð¢ð§ ð†ð€ðð¬ an alternative algorithm for training gans where the wasserstein distance was used and also other techniques like weight clipping this made the training more stable ðƒð‚ð†ð€ðð¬ cnns were used instead of mlps for image generation ðð«ð¨ð†ð€ð progressive growing of gans where we increment the generator and discriminator networks gradually this helped generate high resolution and high quality images ðˆð§ðŸð¨ð†ð€ð enabling gans to learn disentangled representations to have more control over different aspects of the output eyes color nose shape hair type â€¦ ð’ð­ðšðœð¤ð†ð€ð gans that can generate images from text ðð¢ð±2ðð¢ð± an image to image translation with conditional gans for example to turn real images into cartoonish images ð‚ð²ðœð¥ðžð†ð€ð we got rid of the need to have pairs of images for image to image translation which was the case for pix2pix ð’ð­ð²ð¥ðžð†ð€ð an extension of progan for generating high resolution facial images ð‘ð‚ð†ð€ð gans for time series data where cnns were replaced with rnns recurrent neural networks to accommodate for the nature of this type of data ð“ð¢ð¦ðžð†ð€ð another time series gan where new techniques were introduced such as a stepwise supervised loss and an autoencoder connect with me on your favorite social network
qmhthd,0,what s the difference between top tier papers and others hello guys although i read a various of papers that in top conferences such as iccv eccv icml i don t know what s difference in the first second and other tier papers from the top paper all i can know it s very obvious in writing skills and technologies that i don t understand yet x200b to be specific what s different in the way that distinguish them
qydn0p,0,pip package for managed ml training on aws spot instances deep learning is expensive and even if you have aws gcp credits you ll quickly run out of it we built the tool to make training cheaper and to have these credits longer spotml is a command line tool that automatically manages ml training on aws spot instances which are 3x cheaper it lets you handle spot interruptions by resuming training using the latest checkpoint documentation link to try it out looking for feedback from early testers you would be an ideal candidate if you have a side project that you re spending your own money to train acknowledgement spotml is built on top of existing open source library spotty
rvmiyr,0,paper summary rethinking segmentation from a sequence to sequence perspective with transfromers hi i have just published my latest medium article it is a summary of a scientific paper that aims to eliminate the effect of locality which is one of the limitations of cnns in this attempt researchers tried to reform the image semantic segmentation problem then operate a proposed transformer and finally introduce three different decoder architectures please read it and give me your feedback if you find it interesting you can share it with others who are interested in ml as well also if you find it helpful you can follow me on medium to be updated on my forthcoming articles ðŸ™‚
qvklgh,0,how to run the huge language models in transformers hii everyone i m very interested in trying out t0pp from the big science project and lately there appear more and more big models which we are not able to run on our machines or colab i have colab pro but still not able to run t0pp i m wondering how do you experiment with those big models as they don t fit in colab
rdj5t7,1,trying to implement neural networks from scratch i have gotten a class of neuralnet with an init parameters but not sure how to continue please dm or comment if you can help me please thank you
rlaltq,0,do you know any tools libraries frameworks that are intuitive enough for teenagers for a practical introduction to ai hello i am a computer science student and i am trying to set up a workshop for high school students to give them a first hands on introduction to ai more specifically to computer vision so i was wondering if you know any frameworks or tools or libraries that would raise awareness about ai and give a first incentive about it to teenagers any help would be very much appreciated cheers
rbu0e7,0,a visual guide to prompt engineering with gpt language models hi r machinelearning the rise of gpt language models is making a lot of people start to experiment with using them for text generation and general problem solving for language tasks this is a guide to help get people started in thinking about prompts hope you find it useful all feedback is appreciated x200b
rxh1kl,1,which one to go for guys i want to get into machine learning and time to get a system has came option a base model 8 core cpu 7 core gpu macbook air m1 8 256 option b lenovo ideacentre g5 gaming desktop amd ryzen 7 3700x 8gb 1tb hdd 256gb ssd windows 10 nvidia rtx 2060 cd 6gb gddr6 graphics raven black 90q1004gin comparing the base model of macbook because both are costing me same and also adding anything extra on it will escalate the price of macbook a lot here in my country india at least yes yes doing a build on my own would cost me a lot but gpu are very expensive and rare to find at the moment as we all know x200b thank you for reading
r7t0nu,0,generate and read summary of any research paper on twitter using rax bot rax bot a twitter bot to help you read research papers more effectively you can trigger bot by to replying to a research paper tweet that you want to summarize and mention with the summarize keyword and the bot will send you a summary link back on twitter bot also follows some accounts which share research papers regularly and automatically reply to them with a summary link examples
rhtdrw,0,don t know why but you can make rudalle generate similar images to the input one by optimizing its text embedding input image input text elon musk result image colab that runs out of memory disclaimer i m not a scientist nor a developer in fact i m an industrial designer and i wanted to see if i can make a tool that would generate variations on the input pictures in my case the design of objects like headphones i first tried clip guided image generation to make changes to the images but it is lacking on creating particular objects but seems to retain object features quite well this is forest from the input image above my method is to optimize the text embedding of the transformer in order to make the output closer to the input image same thing as fine tuning but optimizing text embeddings instead of model weights i had to modify model s forward pass to make it retain the gradient sorry for messy code my aim however is to take it one step further and achieve text based image modification same as in the original example by openai but without using image prompts also does anyone know if it s possible to reverse text embedding in these models i guess not would be interesting to see what the final image accounts to
rcszg7,0,research survey silent bugs in keras tensorflow framework hello our research group at polytechnique montrÃ©al under the supervision of prof foutse khomh is conducting a survey on â€œsilent bugs in keras tensorflow frameworksâ€ we have prepared an online survey that takes around 10 minutes of your valuable time to complete asking about the relevance and severity of observed silent bugs in keras tensorflow moreover you could kindly share this survey with colleagues students who you consider eligible to participate we will only send a reminder in a week to remind you about completing the study please if you do not want to take part in the study simply ignore those two emails we will not disturb you further than that the results of this survey will be publicly accessible through arxiv org in anonymized form at no point in the survey will we ask you for your name and we will not be logging your ip address to allow anonymity if you would like to know more about this study feel free to contact us with your questions link we really appreciate your time supporting our research best regards amin nikanjam amin nikanjam polymtl ca mailto amin nikanjam polymtl ca swat lab polytechnique montrÃ©al montrÃ©al canada
r4ngnz,0,is the true few shot setting described in recent papers reasonable or am i not understanding the concept properly i ve been coming across a few papers in few shot learning that claim to be under the paradigm of true few shot learning inspired by the paper true few shot learning with language models perez et al 2021 the basic claim is that the current n way k shot setting that s widely used in few shot tasks is unrealistic due to the fact that the models still have access to many data points in other classes not to mention that these models also usually have access to a development set to tune their hyperparameters the authors claim that these assumptions are unrealistic on the grounds that assuming access to other classes data points is unrealistic and that if we have access to a dev set then we might as well use it for training i m writing this post because i m not sure if i agree with those claims or if i m just not understanding the paper properly and would like to seek some other opinions why is it unrealistic to have access to a dev set if that s the case why do we even have a test set as well under the logic presented in the paper if we have a test set then we might as well include that in training too furthermore if we have a test set wouldn t it actually be more realistic to split that to have a dev set as well
r3esd8,0,paper overview florence a new foundation model for computer vision video paper abstract automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks similar to human vision computer vision foundation models which are trained on diverse large scale dataset and can be adapted to a wide range of downstream tasks are critical for this mission to solve real world computer vision applications while existing vision foundation models such as clip align and wu dao 2 0 focus mainly on mapping images and textual representations to a cross modal shared representation we introduce a new computer vision foundation model florence to expand the representations from coarse scene to fine object from static images to dynamic videos and from rgb to multiple modalities caption depth by incorporating universal visual language representations from web scale image text data our florence model can be easily adapted for various computer vision tasks such as classification retrieval object detection vqa image caption video retrieval and action recognition moreover florence demonstrates outstanding performance in many types of transfer learning fully sampled fine tuning linear probing few shot transfer and zero shot transfer for novel images and objects all of these properties are critical for our vision foundation model to serve general purpose vision tasks florence achieves new state of the art results in majority of 44 representative benchmarks e g imagenet 1k zero shot classification with top 1 accuracy of 83 74 and the top 5 accuracy of 97 18 62 4 map on coco fine tuning 80 36 on vqa and 87 8 on kinetics 600
r4639a,1,checking understanding of neural networks hi i just wanted to check whether i have actually understood how a neural network works so you first initialise a network with an input layer and some hidden layers and an output layer each with some nodes is that the right word should i use perceptron or neuron instead the input layer takes the weighted sum of the inputs and adds a bias weights and biases are usually random at first this is then run through an activation function usually sigmoid which represents the probability that the neuron is active the input is fed forward through the network in this way and an error is calculated at the end after that backpropagation is used to adjust weights and biases and then the input is fed forward and another error is calculated this way error reduces over time this network can then be tested on test data how accurate is this description also how is this different to a cnn or dnn thank you
qnhn4s,0,cvpr policy for posting on arxiv i am planning on submitting a paper to cvpr 2022 and i have some questions regarding the process i am finding the information on the site a bit confusing what time frame are we allowed to post preprint versions of our papers to arxiv are there certain considerations we need to know about thanks in advance
qpk1tt,0,landing ai gets 57 million series a to build a data centric mlops platform news are data centric mlops tools about to take off cnbc techcrunch
rs0vl3,1,decent summer college programs offering machine learning programs for high school students iâ€™m a high school student interested in computer science specifically ai and machine learning i am currently learning tensorflow although iâ€™m still a beginner next summer i wanted to go to a college summer program that preferably offers college credit however i canâ€™t find any related to ai ml that offers credit if anyone knows of any or has suggestions please let me know also on a side note how realistic is a high school student getting a medal on kaggle
rms9uj,1,deeper learning with colu activation
rjg6uz,0,paper explained resolution robust large mask inpainting with fourier convolutions w author interview at the end of the video is an interview with the paper authors lama is a system that is amazing at removing foreground objects from images especially when those objects cover a large part of the image itself lama is specifically trained to reconstruct large masked areas and includes global information throughout its forward propagation by using fourier convolutions in its layers this makes it incredibly effective at reconstructing periodic structures with long range consistency compared to regular convolutions x200b outline 0 00 intro 0 45 sponsor clearml 3 30 inpainting examples 5 05 live demo 6 40 locality as a weakness of convolutions 10 30 using fourier transforms for global information 12 55 model architecture overview 14 35 fourier convolution layer 21 15 loss function 24 25 mask generation algorithm 25 40 experimental results 28 25 interview with the authors x200b paper code online demo
qk9uet,0,what makes multi armed bandit problems contextual hi everyone i dive straight into my problem what makes a multi armed bandit problem contextual i read on tensorflow agent tutorial that the agent receives the context vector which is just the observation at every step that makes a bandit setting contextual isnt every agent in an bandit setting doing this since in the mab problem the agent needs to know on which machine bandit he is and how much he knows of the probability of the machine so how does contextual mab defer from the standard mab is it for example extra information ontop for example he knows wether a machine bandit has a higher probability if the wether is raining or not and the second part of my question is i m currently working with stable baselines 3 is here the normal observation function the correlating observation function context vector from tf and using the observation in every step making it contextual couldnt find any information in the sb3 documentation how the contextual settings work to be more specific my extra context in my mab problem is a state machine the bandit uses and each state is an one armed bandit i hope this isnt a beginner question and i am tolerated here
ql2qjz,0,scientific literature review generation hello everyone i ve developed an algorithm to automatically generate a literature review hopefully that could be useful for the phds and the non phds for those curious to understand how it works i ll be thankful if you have any remarks about that cheers
qn8com,0,optimization of hugging face transformer models to get inference 1 millisecond latency deployment on production ready inference server hi i just released a project showing how to optimize big nlp models and deploy them on nvidia triton inference server source code project description please note that it is for real life large scale nlp model deployment it s only based on open source softwares it s using tools not very often discussed in usual nlp tutorial performance have been benchmarked and compared with recent hugging face infinity inference server commercial product 20k for a single model deployed on a single machine our open source inference server with carefully optimized models get better latency times that the commercial product in both scenarios they have shown during the demo gpu based don t hesitate if you have any question
ra70fe,1,bayesian linear regression i ve been tasked with doing some regression on a specific dataset i ve built a decision tree regressor model and a random forest regression model i can compare the two of these using the r squared value on a test dataset i ve also computed a third model using bayesian linear regression however i have no idea how to determine how good a model the bayesian linear regression computes as far as i m aware there s no r squared metric for bayesian linear regression i ve sampled from my posterior distributions but am unsure what my next step is essentially
rcuyl8,1,good start for tts development with custom speech based on recordings
re99pq,1,huggingface is a great idea poorly executed for a project i m trying to use huggingface transformers library to build a particular classifier with keras but jeez i m having nightmares every time i try to understand how to use their api they have tutorials but i find them extremely hard to understand and the api is incredibly ambiguous i m just an idiot or this sentiment is shared
rf95xz,1,nltk lemmatizer does nltk lemmatizer and pos tagger require an active internet connection while executing code
qwygwu,0,has anyone tried deepmind haiku i have just started to learn jax and i am not sure what library to use for making neural networks i then thought haiku looks quite interesting i ve just taken a look at it and i m not sure what to think on the one hand it looks quite capable and fast however there is almost nothing in the documentation in regards to examples tutorials as well as issues on github that are over a year old one person has advised against it is it worth learning or should i stick to flax i have experience with tf 1 2 keras scikit learn etc but not pytorch
r8tdrm,1,a logo detection nn outperforms by almost 2x a logo detection nn which is only one year older what am i missing we have these two nn architectures logo yolo 2020 paper benchmarks osf logo 2021 paper benchmarks both nns were tested on the logodet 3k 1000 dataset but even though the papers are only one year apart the map scores from the new paper are almost double the ones from the older paper take for instance cascade r cnn it has an map of 48 14 on the older paper and 89 1 on the newer paper yes cascade r cnn seems to have different backbones on each paper but can it make such a big difference also resnet 101 has 101 layers while r 50 fpn i assume has 50 layers is it even realistic that in one year you get a backbone architecture with way fewer layers and it s way more performant than the previous one so which model is state of the art in logo detection thank you
r8yq8n,0,how to assert whether the complex high dimensional data have a predictive power or not i am assigned with a binary classification of multi channel time series signal data but the data collection process is expensive so they give me a very small sample of data 15 samples per class and check whether the data is good enough to make a prediction they want me to check the prediction power of the data so they can retune the sensor hoping for more different signal or just give up if the task is too hard and the sensor is not good enough to capture the difference the problem is i don t know how to assert that if it s an rgb image classification task it s easy since we can just look at the image and see the features but this one is complicated and hard to understand the feature i did some simple plot comparison between channel of each class it s still look similar but that s just to my eyes with enough data it may be able to tell the different i imagine doing some kind of eda but for the signal processing to extract some kind of statistics but not sure if i can extract any useful information to be confident that it have a predictive power please suggest me if you have some ideas anyway how do you guys usually dealing with this not limiting to signal but complicated dataset in general is it my job to assert that or should i just tell them that it s not easy to know unless we have enough data which in this case will be very costly if i gonna use deep learning i probably try to do traditional signal feature extraction first like fourier transform and feed the extracted data in simpler model rather than let the model learn the feature itself
rflmju,1,what is the best off the shelf model to use for question answering over internal documents with a serverless architecture hi i have many gbs of internal documents that i need to build a search tool over currently i am using haystack nlp library to do this task but i would like to know if another library tools exists where i can make haystack model into a serverless architecture i would like something that can be on demand vs turned on all the time from a computational resource perspective thank you in advance
r84reh,1,common uses of ml hello i m trying to recap all the very common main real world usage of ml things that a company would need or find helpful here is the list of what i have for the moment time series prediction classification image text pattern recognition text summarization anomaly detection voice recognition text keyword extraction chatbot do you have any other ideas of what can i add to that i want to see other s perspectives i m sure i missed some thanks for consideration
qt4k2c,0,a quick history of gans 8 years of gan evolution and the intuition behind it explained by casual gan papers this tutorial covers the intuition behind variational auto encoder vae the og gan stylegan vqgan telegram post blog post subscribe to casual gan papers and follow me on twitter for weekly ai paper summaries and gan tutorials
rv37yq,0,what are your hopes for machine learning in 2022 hi r machinelearning i was just wondering what some of you are hoping ml can accomplish or overcome in this new year interested in hearing your thoughts
rtvqdl,1,any idea of how could one create a local and personalized ai pair programmer what kind of tools and algorithms could be required the title Â¿why because why not also considering possible limitations there could be conditions such as 1 the ai would be trained from scratch by the pc user 2 i can only work with one programming language maybe there s stuff i m missing here anyway i will be pleasured to read your insights i m personally more curious about whether one could create such program using modern tools like julia to work with modern and complex languages like rust to accelerate development in the most personalized way possible
rftcum,1,good part time data science ms programs with bad gpa
rucjmx,0,simple questions thread please post your questions here instead of creating a new thread encourage others who create new posts for questions to post here instead thread will stay alive until next one so keep posting after the date in the title thanks to everyone for answering questions in the previous thread
qy8gfg,0,paper explained learning rate grafting transferability of optimizer tuning w rant about reviewer 2 the last years in deep learning research have given rise to a plethora of different optimization algorithms such as sgd adagrad adam lars lamb etc which all claim to have their special peculiarities and advantages in general all algorithms modify two major things the implicit learning rate schedule and a correction to the gradient direction this paper introduces grafting which allows to transfer the induced learning rate schedule of one optimizer to another one in that the paper shows that much of the benefits of adaptive methods e g adam are actually due to this schedule and not necessarily to the gradient direction correction grafting allows for more fundamental research into differences and commonalities between optimizers and a derived version of it makes it possible to computes static learning rate corrections for sgd which potentially allows for large savings of gpu memory x200b outline 0 00 rant about reviewer 2 6 25 intro overview 12 25 adaptive optimization methods 20 15 grafting algorithm 26 45 experimental results 31 35 static transfer of learning rate ratios 35 25 conclusion discussion x200b paper openreview old paper arxiv
r1bd7l,1,no data no problem unsupervised learning tsdae and sentence transformers hi all i put together an article and video covering tsdae fine tuning for sentence transformer models basically how we can use plain unstructured text data to fine tune a sentence transformer not quite no data but close from the tsdae paper you actually only need something like 10 100k sentences to fine tune a pretrained transformer for producing pretty good sentence embeddings i was achieving same stsb evaluation with a tsdae train bert as i was getting with my own nli labeled dataset trained bert using softmax loss so pretty cool imo although in reality supervised methods produce better performing models if you have no labeled data unsupervised is the way to go it was really cool learning about this planning to do more on unsupervised sentence transformers in the future let me know what you think
ruyjmn,1,what s the purpose of cyclic layers in a network hi all currently doing the tensorflow advanced techniques specialization on coursera the instructor mentions that for a custom resnet architecture we can actually make it make a layer cyclic particularly as they have the same weights this is illustrated in code as x200b for i in range 1 4 x self block2 x where block2 is a defined residual layer isnt this basically cycling the input multiple times through the same layer what s the purpose here
rfr6vf,1,looking for resources to learn about ml techniques models for forecasting times series data does anyone have any recommendations they can provide
r4um3h,1,can anyone recommend courses that will give the solid foundations of nlp guys can you please recommend good nlp courses any free i did computational neuroscience robotics masters and had a very solid computer vision course there that gave me all the basic knowledge like optics matching geometry basic algorithms etc overall it provided a great overview of the field before we started on cnns and i then moved to gans myself now i will need to use nlp techniques for the analysis and i am looking for similar courses that will give the basic info first
qq18tc,0,improving segmentation masks i have a dataset with some segmentation masks for objects or better polygons around the objects i am interested in but the quality is not very good the polygons around the objects are correct but very rough a low number of edges with huge chunks of background in there is there some algorithmic way to improve those i tried grapcut but the performance is not very good huge chunks of background are still included and stuff like hair is done very poorly
ro1ebu,0,redditors who work at big tech does 0 1 improvement matter to your company i have seen from time to time that when a paper is on the borderline of acceptance because of marginal improvement 0 1 0 2 accuracy of some sort the authors can always argue that a marginal but consistent improvement can be a large sum of revenue for companies that make billions of dollars ml in industry is not so simple although it is just a new method redeploying a whole pipeline can be very time consuming i want to ask people who are working at big tech does that improvement really matter are your companies willing to redeploy this improvement
rnhyeo,0,how to visualize ground truth boxes on images hello guys as you might have guessed from the topic i want to visualize the ground truth boxes on my images from the pascal voc dataset can anyone of you help me out with this dilemma i have been trying to find out ways on how to display the ground truth boxes on the images any help would be good
qup0fe,0,discussion thoughts on manually modifying a model s output for more optimistic results hi i m currently working as a freelancer on a delivery company which predicts an order s estimated time of arrival eta using machine learning what is strange to me is that they have information about how saturated the delivery area is whether it s because of weather traffic etc and after getting a model s prediction they check for saturation and add x minutes to the model s predicted eta thus manually modifying the model s output for more optimistic results what is your opinion on this is this bad practice why would or wouldn t you take this approach
rdv2xw,0,i m releasing three of my pokemon reinforcement learning ai tools including a computer vision program that can play pokemon sword autonomously on nintendo switch video proof source code available hullo all i am tempest storm background i have been building pokemon ai tools for years i couldn t get researchers or news media to cover my research so i am dumping a bunch here now and most likely more in the future i have bots that can play pokemon shining pearl autonomously using computer vision for some reason some people think i am lying after this dump that should put all doubts to rest get the code while you can current work title pokÃ©mon ai shining pearl random ai wild battle honorablesaladai talk source code available topics covered 1 pokemon shining pearl random ai demo 2 why i have a patent and autonomous navigation explanation 3 honorable salad ai model overview 4 pokemon sword random ai demo old videos from 2019 2020 let s start with the video proof below are videos that are marked as being two years old showing the progression of my work with computer vision and building pokemon bots the videos above were formerly private but i made them public recently repos keep in mind this isn t the most up date version of the sword capture tool the version in the repo is from mar 2020 i ve made many changes since then i did update a few files for the sake of making it runnable for other people tool 1 mock environment of pokemon that i used to practice making machine learning models tool 2 i transformed the pokemon showdown simulator into an environment that could train pokemon ai bots with reinforcement learning tool 3 pokemon sword replay capture tool video guide for repo presentation i am working on a presentation for a video i will record at the end of the week i sent my slides to a powerpoint pro to make them look nice you can see the draft version here interested in computer vision i have a create a yolov3 custom object detector course conclusion i will do a presentation of my journey of bring ai bots to nintendo switch hopefully sometime next weekend you can learn more about me and the repos then
r0g2h0,0,microsoftâ€™s debertav3 uses electra style pretraining with gradient disentangled embedding sharing to boost deberta performance on nlu tasks microsoft releases debertav3 improving the original deberta model using electra style pretraining with gradient disentangled embedding sharing to achieve better pretraining efficiency and a significant performance jump here is a quick read microsoftâ€™s debertav3 uses electra style pretraining with gradient disentangled embedding sharing to boost deberta performance on nlu tasks the code is available on the projectâ€™s github the paper debertav3 improving deberta using electra style pre training with gradient disentangled embedding sharing is on arxiv
rh3nj8,1,rnn predict why am i getting 13 outputs number of columns in x train set when using model predict function using keras i need one output i canâ€™t find anything online about this issue model code regressor sequential regressor add lstm units 50 return sequences true input shape x train shape 1 1 regressor add dropout 0 2 regressor add dense units 1 regressor compile optimizer adam loss mean squared error history regressor fit x train y train epochs 10 batch size 32 predict code train predict regressor predict x train test predict regressor predict xtest test predict shape out 48 2142 13 1
r2w8o7,0,discussion federated learning in practice hi does anyone know of any in detail descriptions surveys of fl deployments in practice what type of aggregations do people use and how they ensure privacy do most deployments rely on tf federated i tried googling around but am struggling to find much information thanks a lot
rpfuh7,1,are differential geometry topics like manifolds usually used with gnnâ€™s i am interested in using differential geometry for a project based ml course that iâ€™m doing next semester i need to know the place to start learning about this type of ml and whether or not that place is gnnâ€™s i think differential geometry is a really cool topic in math i want to do an ml project that involves this math because of the interest and it also seems like geometry is becoming more prevalent in ml so i want to start learning more about it during this course what iâ€™m wondering is if gnnâ€™s are the right method to start looking in to if iâ€™m interested in using manifolds in ml is a gnn the best tool to allow me to analyze information embedded in a manifold donâ€™t know if this is the right terminology if you think using manifolds and differential geometry in a project is too advanced for someone that doesnâ€™t know much about it yet then please tell me and any suggestions on where to start are appreciated thank you in advance for any help
qutuo6,0,resources on on line machine learning i am wondering if there are any books articles tutorials about on line machine learning x200b for example this website has nice lecture notes from lec16 on some of the aspects x200b or this book x200b x200b x200b i can t seem to find much resources on this i m trying to understand the basics not read research papers if anyone can share resources that would be nice
r246im,1,leave columns out for training a model i have a basic classifier for a dataset which includes about 40 columns the first two are an id and the associated name i eliminated all the non numeric columns so my classifier only uses the numeric values for its prediction so i eliminated the id and name column too now i want to associate the predicted data and the test set with the ids and names which i excluded before the classifier training is there a way to either leave the ids and names in the data set and tell python not to use them during training or to associate them back after the prediction i use python with pandas sklearn etc the classifier is a logistic regression with sklearn
rer126,1,improving convergence of dqn network i ve been trying to train a dqn on the carracing v0 environment of openai gym but i m having trouble getting it to retain the knowledge its learned over time i m using prioritized experience replay and the model architecture is a relatively simple cnn here is a link to my code i let it train overnight and here s my total reward plot per episode a few ideas i had but looking for more guidance increase model complexity maybe it s underfitting decrease learning rate over time what could be causing this fluctuating reward
rmxri9,0,why and how do residual skip connections work looking for literature you can share any comments insights discussion or particular works you have in mind aside i ll just mention an extremely subtle hint about a recent work i remember coming across but cannot recall its title or other info that work was analyzing residual connections in one of its sections and commented that a function block with a skip connection was easier to train as it only has to learn the domain shift from the original input data space not sure if they specifically used the words domain shift but it was close i d not met this particular framing before that paper besides everything else you may want to comment share here i d appreciate if you can comment on or share papers dealing with particular framing as well it does not have to come from ml literature stats math etc are also welcome with some help to clarify the more jargon heavy parts existing relevant threads
qnu4bg,0,real world challenges for agi by deepmind either i did not understand the article or they just inserted the letters a g and i randomly in between their current progress with weather prediction and plasma control for fusion
r5ktec,0,which one do you prefer you had an idea you already implemented and tested it in pytorch now you want to test it on architecture a which is publicly available but in tensorflow would you 1 implement your idea on a s framework or 2 implement a on your framework would you change your answer if you want to further test your idea on a few more architectures
qvl3d5,0,pytorch lit infer large models that don t even fit in main memory deep learning models are rapidly growing in terms of size and complexity and inference on end devices is becoming impossible gpt j with 6b parameters for example only requires 24 gb of ram in full precision mode to be ready for execution which may be impossible in most systems even a powerful gpu like rtx 2060 with 6 gb of memory can t even contain gpt j in half precision mode making direct inference impossible pytorch lit solves the problem by running large models on end devices and loading parameters from a secondary memory as needed for the time being we are using disk as secondary memory but we intend to implement faster alternatives in the future github
rgsvpq,1,in few shot learning how do i determine the number of query images for each class hello i am studying few shot learning in general i know that 7 to 15 query images are allocated for each class however the dataset i use for training has a minimum of 2 and a maximum of 200 pieces per class so i have only used one query image i have two questions 1 is it okay to use only one query image like this 2 and is it meaningful to see good performance when using fewer queries
rwrnk2,0,can you recommend funny papper like single headed attention rnn stop thinking with your head i really enjoyed reading this for a change to the textbook papers abstract the leading approaches in language modeling are all obsessed with tv shows of my youth namely transformers and sesame street transformers this transformers that and over here a bonfire worth of gpu tpu neuromorphic wafer scale sil icon we opt for the lazy path of old and proven techniques with a fancy crypto1 inspired acronym the single headed attention rnn sha rnn the authorâ€™s lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly differ ent acronym and slightly different result we take a previously strong language model based only on boring lstms and get it to within a stoneâ€™s throw of a stoneâ€™s throw of state of the art byte level language model results on enwik8 this work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the authorâ€™s small stu dio apartment far too warm in the midst of a san franciscan summer2 the final results are achiev able in plus or minus 24 hours on a single gpu as the author is impatient the attention mechanism is also readily extended to large contexts with minimal computation take that sesame street
r7lq0u,1,resources for practicing advanced maths for data science machine learning i am familiar with the basics i can solve easy ones but when i encounter difficult questions recently faced some in a job screening test i get stumped i don t want resources to learn the basics i want resources where i can try to solve and see the solutions to difficult maths questions preferably in the context of data science machine learning i tried googling but only found entry level courses
qqzuh0,0,cedille the largest french language model 6b released in open source ðŸ“ demo ðŸ“˜ repo we have spent the last 3 months of our lives teraflops of compute and gone through 300gb of text to bring you cedille ce que j aime quand je mange une baguette c est quand celle ci est craquante je ne saurais dÃ©finir le terme craquant mais je sais que lorsque c est le cas je peux Ãªtre sÃ»re que la baguette est bonne the entirety of french spirit captured in measly 6b parameters ðŸ‡«ðŸ‡·ðŸ¥– more seriously we are super excited to share cedille the so far largest french language model you can play with it right now on our playground as long as servers hold ðŸ˜… we are proponents of â€œopen aiâ€ and as such have released a checkpoint for the world to use mit license another aspect we had fun with is dataset filtering we have run the whole c4 french dataset through the detoxify classifier to clean it up ðŸ¤¬ some acknowledgements cedille is based on gpt j the 6b model developed by the wizards at eleutherai cedille was also generously supported by the google tfrc program
rvdjaw,1,what metric would i use iâ€™m trying to write a regression algorithm that makes predictions based on empirical data the goal iâ€™ve been given is that i need to have success thatâ€™s explained as â€œ95 of predictions to be within 15 of the actual valueâ€ statistics is my least strong science by farâ€¦ what is the best way to show or explain my results based on this criteria
rhm5u9,1,ml dataset to help derive insights into the nature of un sdgs i m sharing an open dataset for those building machine learning models related to un sustainable development goals sdgs the dataset contains tens of thousands of text excerpts which were validated by more than 1 000 citizen scientists from over 100 countries with respect to the sdgs the dataset is called osdg community dataset osdg cd and is freely accessible on zenodo if you end up using the dataset share your results with us via email mailto community osdg ai we d love to see what you come up with
r2qcer,0,kwai kuaishou eth zÃ¼rich propose persia a distributed training system that supports deep learning based recommenders of up to 100 trillion parameters a research team from kwai inc kuaishou technology and eth zÃ¼rich builds persia an efficient distributed training system that leverages a novel hybrid training algorithm to ensure both training efficiency and accuracy for extremely large deep learning recommender systems of up to 100 trillion parameters here is a quick read kwai kuaishou eth zÃ¼rich propose persia a distributed training system that supports deep learning based recommenders of up to 100 trillion parameters the code is available on the projectâ€™s github the paper persia an open hybrid system scaling deep learning based recommenders up to 100 trillion parameters is on arxiv
rwrhfh,1,how to learn mechine learning as a beginner hi all i am an android developer and have a interest in machine learning and i am planning to start my learning in a proper manner as far as i know machine learning is basically about mathematics after googling for couple of days i preapred a plan which i wanna share with you all and i request you all to please give your valuable suggestions tips comments 1 complete the mathematics for machine learning 2 complete the 6 041 probabilistic systems analysis and applied probability 3 complete the stanford cs229 machine learning i already know python but have no hands on experience in machine learning related libarary i have few doubts 1 do i have to watch the whole lecture 6 041 probabilistic systems analysis and applied probability i have watched it till discrete probability i e till lecture 7 2 from where i should be learning statistics x200b i again request you all to please give your valuable suggestions tips comments x200b thanks a lot for stopping by and taking the time to read
roaey3,1,getting overwhelmed i wanted to ask you guys whether it is normal to get overwhelmed while learning machine learning i have been studying since almost last 6 months and i feel like i am getting nowhere i did andrew ng s machine learning course and also a course in my university which was based on linear regression with sci kit learn and cnns with tensorflow inspite of all that i really feel lost when i read a lot of stuff on this subreddit and also on kenjee s discord i almost understand everything but sometimes there are things that i don t understand at all and i worry that i am far behind and if i will ever get to the point to be confident to do projects etc
r3ab0x,1,does anyone have the answer manual for the practice of computing using python would be much appreciated thanks in advance
qz3qtv,0,5 considerations for deploying machine learning models in production â€“ what did i miss i wrote a post about considerations for deploying machine learning models in production below are the considerations what did i not consider i know the first consideration seems obvious but i thought it was worth mentioning 1 use your laptop for development as a best practice consider your development environment first most data scientists or ml engineers invariably use their laptops for development testing or debugging code because of simplicity easy to access and install the latest ml libraries practitioners overwhelmingly prefer laptops over clusters for development we are spoiled by ides and syntax highlighted editors for good reason python developers like to customize their environments to match their staging environment with library dependencies using conda or python virtual environments ideally as a best practice if the same code developed on their laptop can run with minimal changes on a staging or production environment on the cluster it immensely improves the end to end developer productivity consider your laptop as a preferred choice of development environment with the possibility of extending or syncing your code to the cluster environment in the cloud consideration number 1 use your laptop for development as a best practice 2 training at scale and tracking model experiments unlike the traditional software development cycle the model development cycle paradigm is different a number of factors influence an ml modelâ€™s success in production first the outcome of a model is measured by its metrics such as an acceptable accuracy second achieving an accuracy that satisfies the business goal means experimentation with not only one model or ml library but many models and many ml libraries while tracking each experiment runs metrics parameters artifacts etc as vital as accuracy is so is a developerâ€™s choice of ml libraries to experiment with third accuracy is directly linked to the quality of acquired data bad data results in a bad model data preparation feature extractions feature selection standardized normalized features and data imputations encoding are all imperative steps before the cleansed data lands into a feature store accessible to your model training and testing phase or inference in deployment fourth a choice of programming language that is not only familiar to your data team â€” data analysts data scientists and ml engineers â€” but also supported by many ml libraries employed during model experimentation and training phases python seems to be the de facto choice alongside a choice of a programming language is the choice of an ml framework for taming compute intensive ml workloads deep learning distributed training hyperparameter optimization hpo and inference â€” all at horizontal scale â€” from your laptop single node multiple cores to multiple nodes with multiple cores and finally the ability to easily deploy models in diverse environments at scale part of web applications inside mobile devices as a web service in the cloud etc consideration number 2 consider using model life cycle development and management platforms like mlflow dvc valohai weights biases or sagemaker studio and ray ray tune ray train formerly ray sgd pytorch and tensorflow for distributed compute intensive and deep learning ml workloads 3 managing machine learning features feature stores are emerging pivotal components in the modern machine learning development cycle as more data scientists and engineers work together to successfully put models in production having a singular store to persist cleaned and featurized data is becoming an increasing necessity as part of the model development cycle feature stores address operational challenges they provide a consistent set of data between training and inference they avoid any data skew or inadvertent data leakage they offer both customized capability of writing feature transformations both on batch and streaming data during the feature extraction process while training and they allow request augmentation with historical data at inference which is common in large fraud and anomaly detection deployed models or recommendation systems aside from challenges and considerations of putting models in production operationalizing ml data is equally important model accuracy depends on good data and feature stores help manage precomputed and cleansed features for your model training and production inference during model serving consideration number 3 consider feature stores as part of your model development process look to feast tecton sagemaker hopsworks and databricks for feature stores 4 deploying serving and inferencing models at scale once the model is trained and tested with confidence that it met the business requirements for model accuracy seven crucial requirements for scalable model serving frameworks to consider are framework agnostic a model serving elected framework should be ml framework agnostic that is it can deploy any common model built with common ml frameworks for example pytorch tensorflow xgboost or scikit learn each with its own algorithms and model architectures business logic model prediction often requires preprocessing post processing or ability to augment request data by connecting to a feature store or any other data store for validation model serving should allow this as part of its inference model replication some models are compute intensive or network bound as such the elected framework can fan out requests over to model replicas load balancing among replicas to support parallel request handling during peak traffic request batching not all models in production are employed for real time serving often models are scored in large batches of requests for example for deep learning models parallelizing these image requests to multiple cores taking advantage of hardware accelerators to expedite batch scoring and utilize hardware resources is worthy of consideration high concurrency and low latency models in production require real time inference with low latency while handling bursts of heavy traffic of requests the consideration is crucial for best user experience to receive millisecond responses on prediction requests model deployment cli and apis a ml engineer responsible for deploying a model should be able to use model serverâ€™s deployment apis or command line interfaces cli simply to deploy model artifacts into production this allows model deployment from within an existing ci cd pipeline or workflow patterns of models in production as ml applications are increasingly becoming pervasive in all sectors of industry models trained for these ml applications are complex and composite they range from computer vision to natural language processing to recommendation systems and reinforcement learning that is models donâ€™t exist in isolation nor do they predict results singularly instead they operate jointly and often in four model patterns pipeline ensemble business logic and online learning each pattern has its purpose and merit machine learning engineers adopt two common approaches deploy these patterns of models in production one is to embed models into a web server and the other is to offload to an external service each approach has its own pros and cons with respect to the seven considerations above consideration number 4 look to seldon kfserving or ray serve for all these seven requirements 5 observing and monitoring model in production model monitoring often an overlooked stage as part of model development lifecycle is critical to modelâ€™s viability in the post deployment production stage it is often an afterthought at an ml engineerâ€™s peril models have an afterlife of viability that viable life in production needs a constant watchful or sentinel eye in fact monitoring as a phase is simply a continuation of the model serving why consider model monitoring for a number of practical reasons this stage is pivotal letâ€™s briefly discuss them data drifts over time as we mentioned above our quality and accuracy of the model depends on the quality of the data data is complex and never static meaning what the original model was trained with the extracted features may not be as important over time some new features may emerge that need to be taken into account for example seasonal data changes such features drifts in data require retraining and redeploying the model because the distribution of the variables is no longer relevant model concept changes over time many practitioners refer to this as model decay or model staleness when the patterns of trained models no longer hold with the drifting data the model is no longer valid because the relationships of its input features may not necessarily produce the modelâ€™s expected prediction hence its accuracy degrades models fail over time models fail for inexplicable reasons a system failure or bad network connection an overloaded system a bad input or corrupted request detecting these failuresâ€™ root causes early or its frequency mitigates user bad experience or deters mistrust in the service if the user receives wrong or bogus outcomes systems degrade over load constantly being vigilant of the health of your dedicated model servers or services deployed is just as important as monitoring the health of your data pipelines that transform data or your entire data infrastructureâ€™s key components data stores web servers routers cluster nodesâ€™ system health etc collectively these aforementioned monitoring model concepts are called model observability this step is now an acceptable imperative in mlops best practices monitoring the health of your data and models should never be an afterthought rather it ought to be part and parcel of your model development cycle consideration number 5 for model observability look to evidently ai arize ai arthur ai fiddler ai or whylabs ai thanks to this excellent pytorch reddit post for inspiring the formatting of this post
rwxmw4,0,search engine for time series hi last year i developed a passenger flow forecasting model passenger flows are heavily influenced by lockdown measures and the weather so i wanted to incorporate features relating to that into my model doing so i encountered various frustrations x200b data providers generally focus on a specific domain e g covid weather ecommerce forecasts however can be influenced by data from many domains finding these providers signing up and reading documentation is very time consuming it takes a lot of code just to get the data you want for example to get covid data for my province i first had to call a list endpoint then retrieve the province identifier and then loop over the data endpoint because the maximum range was 2 months i think the core problem is that apiâ€™s as the acronym indicates are meant to be interfaced with programmatically but that data scientists end up calling apiâ€™s manually before the model runs in production my proposed solution is to build a platform that partners with many data providers indexes their time series and make them searchable with a semantic search each result would be a single time series this search would be usable via a web ui and endpoint screenshot of web ui prototype library i currently have a working prototype with pretty decent search but haven t really validated my assumptions it would be great if you could provide some feedback does it make sense to focus on time series or would a general semantic data search make more sense how do you currently get external data into your model what are your major pain points do you semantic search could speed up the process of acquiring external datasets any other thoughts thanks for your feedback
r8lq9b,1,need to learn image recognition for an interior design project hi everyone i m a devops engineer who has studied cs in college worked on linux for more than 10 years and know python obviously i m not a developer but i know how to code and have the basis for any it related topic for an idea i need to learn image recognition i need to be able to write a program that can identify the surfaces and not now but in close future measure the dimensions of a room the application should be able to identify the floor walls ceiling big objects door windows table softa etc and later on be able to interact wtih the texture change the color the surface type etc using a phone or a vr headset i ve checked out youtube did some research online etc up to now i ve found out opencv and tensorflow have libraries that can get the job done but i want to start smart i want to follow a route and use tools that can certainly takes me to the right track even if the project turns out into a big one creating a full feature app what are your recommendations fellow redditors which technology to use and where to start and what to look for and how much time and effort is needed for someone like me to build a simple app like that
r0mok4,0,nvidia releases web app for gaugan2 which generates landscape images via text description inpainting sketch object type segmentation map and style image gaugan2 blog post gaugan2 web app more links and other info are at this comment from another post examples that i generated text description trees turning color in autumn text description a winter mountain landscape near sunset text description a stream sketch computed by app from previous image text description a stream sketch in previous image style change using one of the app s style images
rb3tfn,0,data augmentation in nlp hi i d like to ask what you think or is your experience with data augmentation in nlp particularly in intent based chat bots i know it s a common practice in computer vision but as to whether some downsides with synonym replacement resulting in thousands of examples within one intent can occur i m dubious ðŸ¤” my common sense tells me it can help overfitting and as long as i keep dialog balanced i can t see any downside other than training time i d love to hear all your inputs ðŸ™‚
r6lfdy,0,pytorch dev day going on now twitter broadcast youtube broadcast
robzib,1,a model that can learn and replicate the art style of a twitter artist is this real Â¿where can i find it for example characters my request can also be explained following these steps 1 download all the artwork from a twitter account 2 use one sample of twitter art as a model output for a set of inputs 5 to 10 images of a character in its original basic art style 3 let the model find the relationship 4 repeat steps 2 3 but with other samples characters as much as you want 5 once you find it appropriate start submitting a set of inputs 5 to 10 images of a character that has never been considered by the artist 6 the model after learning from past processes generates a new output that resembles the artist s style hope you get the idea however please consider that i m still new to this area if there s something you think i m missing here please let me know merry christmas
rab8fe,1,discussion about applications of deeplearning i have to questions related to each other i have just completed my course in deeplearning without any background about ai or machine learning and i really understand it so when i made my model is that has any affect like do i have not enough informations when i design my models and the second thing as i told you i have just finished my course can anyone suggest to me a ink or page for a suggestions ideas or projects for the types of models application idea for ann cnn rnn som autoencoder rbm so i can expand my ideas about real life situation application
rae9ui,0,strategies for handling large conference proceedings hi r machinelearning happy neurips week to you all this time of year and around other large major conferences in our field have always been exciting times waiting to see what new work or break throughs our colleagues and community members might have released i personally work in a more applied signal processing space so it s always a task to find papers that might provide insight or outline a new method for learning that i might be able to adapt in my own research however as i look at this absolutely massive list of accepted papers even just filtering papers by the titles has become incredibly time consuming and daunting so i wanted to pick your brains for insight into how you manage with this massive amount of work to grudge through do you wait for the community to self select these do you go for the top papers in terms of the awards do you just wait for the papers to become relevant enough that they ll pop up when you re doing background reading on the subject i guess one of my personal struggles is fomo on something that is highly relevant well done but just slips past the hive mind of the community
r7kj61,0,neural noise is one of the cool anyone know what causes it i ve been messing around with style transfers and a lot of them especially ones based on illustrations have this specific type of noise x200b good example of the noise it s splotchy rgb noise that varies a lot in size x200b a gif where you can see it developing and a still where you can see the noise really well
r1c03v,0,paper explained parameter prediction for unseen deep architectures video interview w first author boris knyazev deep neural networks are usually trained from a given parameter initialization using sgd until convergence at a local optimum this paper goes a different route given a novel network architecture for a known dataset can we predict the final network parameters without ever training them the authors build a graph hypernetwork and train on a novel dataset of various dnn architectures to predict high performing weights the results show that not only can the ghn predict weights with non trivial performance but it can also generalize beyond the distribution of training architectures to predict weights for networks that are much larger deeper or wider than ever seen in training x200b outline 0 00 intro overview 6 20 deepnets 1m dataset 13 25 how to train the hypernetwork 17 30 recap on graph neural networks 23 40 message passing mirrors forward and backward propagation 25 20 how to deal with different output shapes 28 45 differentiable normalization 30 20 virtual residual edges 34 40 meta batching 37 00 experimental results 42 00 fine tuning experiments 45 25 public reception of the paper x200b errata boris name is obviously boris not bori at 36 05 boris mentions that they train the first variant yet on closer examination we decided it s more like the second x200b paper code
rd0hm6,0,utility scripts for coco json format for object detection segmentation sharing a simple collection of functions scripts for manipulating object detection segmentation datasets in coco format being the dominant dataset format some dataset conversion utilities are provided as well feedback contributions are welcomed
rh71mv,1,best books you would recommend to learn the math for machine learning i have a math physics background from college so iâ€™ve taken linear algebra probability and the calculus series already i just forgot most of it i am looking to refresh my skills and trying to find the appropriate books for the subjects would something like james stewartâ€™s calculus work well i know for linear algebra itâ€™s recommended that gilbert strang is the go to what about for probability
rh8s5l,1,beginner how to integrate ml on a web page x200b
rjsh06,0,for those of you who don t own a gpu how do you run your experiments or train your models i m aware that many people use cloud computing services like aws or gcp but i m curious if there are any other methods out there that i m not aware of edit i should have clarified this when i say don t own a gpu i meant don t have access to a privately owned gpu i used to use my lab s servers and gpus but now that i m graduating i can t use those anymore
r0ht8v,0,text to paint connecting neural painters with clip goal paint telling the machine in natural language what to paint neural painters paint using strokes and colors rather than generating images at the pixel level i ve been working in a text to paint together with diavlex in an ai art collaboration the idea is to specify a prompt in natural language ant tell the machine what to paint using strokes and not a pixel level as an human artist would do here is the overview of the implementation the links to the notebook are below so you can try it yourself note that it is the same notebook hosted in kaggle and colab if you are part of kaggle s community it is probably a better option given that a currently kaggle offers a nvidia tesla p100 for all notebook sessions whereas using google colab you might get a less powerful gpu for your session notebook at kaggle notebook at colab tl dr the following steps capture the essence of our idea note that we use pseudocode based on python which does not necessary reflect the models api please have a look to the notebook itself for the actual code and methods used 1 specify what to paint e g prompt black sheep 2 encode the text using clip s language portion to obtain the text features text features clip model encode text prompt 3 initialize a list of brushstrokes or actions and ask the neural painter to paint on a canvas at the beginning the canvas will look random canvas neural painter paint actions 4 use the vision portion of the clip model to extract the image features of this initial canvas image features clip model encode image canvas 5 the goal is to teach the neural painter to modify the strokes i e its actions depending on how different is what it is painting to the initial text request prompt for example in the perfect case scenario the cosine similarity between the text and image feature vectors should be 1 0 using this intuition we use as the loss to guide the optimization process the the cosine distance that measure how different the vectors are the cosine distance in our case corresponds to loss 1 0 cos text features image features 6 we minimize this loss adapting the neural painter actions that in the end should produce a canvas as close as possible to the original request enjoy neural painting x200b example for prompt black sheep showing the evolution during the optimization only 13 strokes used x200b x200b paint for prompt black sheep and the last canvas painted stroke by stroke x200b paint stroke by stroke for prompt black canvas using 13 strokes only fin
qlqqow,0,researchers from seoul national university nvidia and microsoft release â€˜acav100mâ€™ an automatically curated video dataset for self supervised audio visual learning audio visual av learning is defined by delivering and applying instructional content that includes both sound and visual information the natural relationship between visual observations and their accompanying sounds has shown strong self supervision signals for learning video representations that is why the massive amount of online videos has become a valuable source for self supervised learning among research communities however due to overdubbed audio online videos frequently provide imperfectly aligned audio visual signals therefore the models trained on uncurated films have been shown to develop poorer representations as a result of the misalignment difficulties the existing techniques typically rely on manually curated datasets with a predetermined taxonomy of semantic ideas where the audio visual connection is highly likely to overcome this gap researchers from seoul national university nvidia and microsoft have released an automatic dataset curation pipeline and a large video dataset for self supervised audio visual learning termed acav100m automatically curated audio visual dataset the dataset is made up of a massive number of uncurated web videos the researchers took 140 million full length videos and reduced them to 100 million segments with the best audio visual correspondence checkout the paper codes project microsoft blog video presentation and a short read from us x200b x200b
r3t02h,0,unsupervised topic segmentation of meetings with bert embeddings research paper walkthrough in this paper the authors propose a bert based unsupervised topic segmentation method for the task of dividing multi person meeting transcripts into topic blocks ðŸ”¥ finally the online meeting recordings are useful d paper walkthrough paper
rq0b9k,0,is using model checkpoints on validation sets while doing 5 fold cross validation an issue suppose i am training a neural network on a dataset and performing 5 fold cv i am saving the best model weights by checkpointing on improvements to a particular metric on the validation set and evaluating later using them on the same validation set i just had the thought about whether i should be doing this model checkpointing like this i mean in theory for cross validation to give an idea of generalized performance on an unseen test set shouldn t the validation sets be treated like i would treat a test set as in not accessing it during the training would appreciate any feedback
qlqls4,0,are gnns gcns viable for graphs with no node features with only the unique node ids are they different from deepwalk at that point i started to dig into gnns for the first time and i have trouble understanding its advantages over nlp inspired embedding methods like deepwalk and node2vec do gnns only shine with node features or can they handle ids giant one hot vectors as well does the usual input for gnns only consist of a vector of handcrafted features are gnns used directly for tasks like link prediction or they are just embedding generators for other models i appreciate all and any explanations
qqu6xh,0,rebooting acgan a new gan that achieves sota results and harmonizes with various architectures adversarial losses and even differentiable augmentations neurips 2021 a research team from pohang university of science and technology introduces a new type of acgan the rebooted auxiliary classifier gan reacgan to overcome unstable training and poor generation performance of acgan here is a quick summary of the paper gradient exploding in the classifier of acgan can cause an undesirable training collapse simply normalizing feature embeddings can resolve the problem using the normalization technique we propose the rebooted auxiliary classifier gan reacgan reacgan achieves state of the arts generation results on benchmark datasets reacgan harmonizes with various gan architectures dcgan resnet big resnet stylegan2 adversarial losses and differentiable augmentations ada diffaugment arxiv
r110aq,1,machine learning for profit not classification hi guys i am learning about machine learning and have a question it seems that most machine learning is used for classification purposes for example if we are looking at horse racing predictions some models are simply interested in classifying the winner but what if in our example of horse racing we are interested not in classifying the winner but in producing a model which maximizes profit out of the learning data set it would or course be much to find a model that can predict a steady load of high odds winners rather than lots of smaller priced winners what models can do this and are built for this purpose
rotyst,0,is there a repo on which many light weight self attention mechanism are introduced hi guys i am searching many applications of self attention reaching efficiently computation or fast convergence such as reformer longformer or bigbird could i ask you there is organized paper or repo of them
qlsp3s,0,which apps in the real world would you like to connect your ml models with hi all as a side project i am building a tool to connect the ml models you make in your jupyter notebook to apps in the real world with just a few lines of extra code as an example think of building a model that predicts customer churn in a jupyter notebook we make sure that it pulls new customers from the mailchimp account of your company and make it run as a discord or slack bot you can sign up for early access to the tool here i am building fast and i wanted to ask what apps you think would be awesome to integrate your machine learning models with so far we have mailchimp shopify slack google sheets thanks in advance
rgdwjy,0,new datasets to democratize speech recognition technology hey we at the gradient just published new datasets to democratize speech recognition technology written by the folks at mlcommons org since this has to do with datasets seems like it would be of interest to folks in this sub here s a tldr preview over the last year we at mlcommons org set out to create public datasets to ease two pressing bottlenecks for open source speech recognition resources the first is prohibitive licensing several free datasets do exist but most of sufficient size and quality to make models truly shine are barred from commercial use as a response we created the peopleâ€™s speech a massive english language dataset of audio transcriptions of full sentences see sample 1 with over 30 000 hours of speech this dataset is the largest and most diverse freely available english speech recognition corpus today the second is that these datasets are heavily english centric we also present the multilingual spoken words corpus mswc a 50 language 6000 hour dataset of individual words sample 2 contains random examples of â€œhelloâ€ in multiple languages single word transcriptions are useful for training keyword spotting kws models such as the ones used to activate google voice assistant alexa or siri this dataset provides a significant leap in diversity of available keyword spotting datasets together these datasets greatly improve upon the depth tps and breadth mswc of speech recognition resources licensed for researchers and entrepreneurs to share and adapt
qwxil2,0,advcl protecting contrastive learning models against adversarial attacks adversarial contrastive learning advcl is a new technique by mit ibm watson ai lab to make cl models robust without reducing downstream classification accuraction the paper has been accepted at neurips 2021 key highlights advcl replace two view contrastive loss optimization with four view cl including adversarial perturbations and high frequency component view a pseudo supervision stimulus generation component uses clustering algorithms to create pseudo labels for training data advcl addresses the cross task robustness transferability which happens because of the mismatch between the loss function of the cl training stage and the downstream supervised finetuning this can be pivotal for the security of ml applications that are data constrained and rely on cl analysis of paper interview with lead author full paper code
ra5420,1,mixture model of different distributions hi if one trains a mixture model using em where we have two observed variables which depend on a hidden variable does this mean that we can multiply the probabilities together while running the em algorithm im thinking that it should be possible however i am not sure because looking at the graphical representation of the model it seems that they are not independent because the path between them is tail to tail with respect to the hidden variable
r3k9b3,0,discussion i wrote an article on inferencing with large transformers with gpus instances with databricks how practical is that for most ml practitioners what do you use when you want to run millions of rows through large transformer models is there such a need at all i need feedback so that i can write more helpful content for the community the article i wrote high performance inferencing with large transformer models on spark
r8lljf,0,i implemented transformer in transformer here is my implementation of the recent transformer in transformer paper which uses pixel level attention paired with patch level attention
rtispn,1,a good course about ml dl with big data hi all i m looking for a course that essentially describes what is big data and shows how to use it with ml dl models i have a deep understanding of ml dl so i m just looking for a course that add the big data puzzle piece to my tools preferably the course should demonstrate its work have the exercises be written in python i mostly use coursera as my go to since it teaches the technical mathematical background and not only here is a piece of code try it type of course so any course that cover both theoretical practical sides of the topic is welcome
rcdbiz,1,how to version your machine learning experiments instead of just tracking them distributed versioning vs centralized tracking for ml experiments with dvc and git ml experiments often get split between git for code and experiment tracking tools for meta information because git can t manage or compare all that experiment meta information but it is still better for code the following guide explains how to apply dvc for ml experiment versioning that combines experiment tracking and version control don t just track your ml experiments version them instead of managing these separately keep everything in one place and get the benefits of both like experiments as code track meta information in the repository and version it like code versioned reproducibility save and restore experiment state and track changes to only execute what s new distributed experiments organize locally and choose what to share reusing your existing repo setup experiment versioning treats experiments as code it saves all metrics hyperparameters and artifact information in text files that can be versioned by git which becomes a store for experiment meta information the article above shows how with dvc tool you can push experiments just like git branches giving you flexibility to share experiment you choose
r24q0v,1,creating numeric word representation of input sentences resulting in memoryerror i am trying to use countvectorizer to obtain word numerical word representation of data which is essentialy list of 160000 english sentences import pandas as pd import numpy as np from sklearn feature extraction text import countvectorizer df train pd read csv data train csv vectorizer countvectorizer ngram range 1 2 token pattern r b w b min df 1 x vectorizer fit transform list df train text then printing x x 160000x693699 sparse matrix of type class numpy int64 with 3721191 stored elements in compressed sparse row format but converting the whole to array to get the numerical word representation of all data gives x toarray memoryerror traceback most recent call last appdata local temp ipykernel 11636 854451212 py in module 1 x toarray c users crrma virtualenvs humor detection 2 8vpiokuk lib site packages scipy sparse compressed py in toarray self order out 1037 if out is none and order is none 1038 order self swap cf 0 1039 out self process toarray args order out 1040 if not out flags c contiguous or out flags f contiguous 1041 raise valueerror output array must be c or f contiguous c users crrma virtualenvs humor detection 2 8vpiokuk lib site packages scipy sparse base py in process toarray args self order out 1200 return out 1201 else 1202 return np zeros self shape dtype self dtype order order 1203 1204 memoryerror unable to allocate 827 gib for an array with shape 160000 693699 and data type int64 for the example in the linked schikit learn doc page they have used only five sentences thus for them x toarray seem to have returned the array of numerical word representation but since my dataset contains 160000 sentences in error message it seems that it is resulting in vocabulary of size 693699 which contains both unique unigrams and bigrams due to ngram range parameter passed to countvectorizer and hence facing insufficient memory issue q1 how can i fix this i am thinking to simply reject x and separately transform in mini batches as shown below is this correct x batch list df train 10 text do this for 160000 batch size batches x batch encoding vectorizer transform x batch toarray x batch encoding array 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dtype int64 x batch encoding 0 shape 693699 q2 i am thinking to train neural network and decision tree on this encoding for humor detection but i guess it wont be great idea to have 693699 length vector to represent single sentence right if yes what should i do instead should i opt to use only unigrams while fitting countvectorizer but it will not capture even minimal context of words unlike bigrams ps i am creating baseline for humor detection i am required to use countvectorizer
r1z1ig,0,70 page paper from yoshua bengio team gflownet foundations in the new paper gflownet foundations a research team from mila university of montreal mcgill university stanford university cifar and microsoft azure ai builds upon gflownets providing an in depth formal foundation and expansion of the set of theoretical results for a broad range of scenarios especially active learning here is a quick read 70 page paper from yoshua bengio team gflownet foundations the paper gflownet foundations is on arxiv
r4oghg,1,how to recap a whole tv series and movies using artificial intelligence i am working on a college project where i decide i would do something like this but i have no vague idea how to do this i have just started taking ai ml andrew yang classes and i can t figure out how to do this can anyone help please
rbm3yp,1,iâ€™m interested in machine learning but were do i start iâ€™m looking for advice or tips about how to start learning about it
rkasgc,0,an easy framework for pretraining lanuguage models project link hi ml redditors i and my colleagues u seopbo and more made la nguage framework for s elf s upervised l earning lassl lassl aims to provide an easy to use framework for pretraining language model by only using huggingface s transformers and datasets currently bert roberta gpt2 and albert are provided and the model will be continuously updated in the future also in order to see the effectiveness of the code we will continue to add models trained using it to the model hub along with downstream evaluation we hope it helps whoever want to make their own language models to make it easy and fast i hope it will be helpful for someone cheers
rl0jmt,1,model learns on training data but not on test data i train with keras a model witch is similar to alexnet on audio data witch i transformed to normalized mel specrograms on a equally distributed validation dataset i get good train and validation accuracy from 80 to 90 but on my test data it lears just the same class any idea what the problem is here
rbno04,1,cnn trained on fashion mnist hello how would a cnn trained on the fashion mnist perform on categorizing mnist thanks
rfdieq,1,should i learn linear algebra the traditional way or through python by traditional way i mean the way it s taught in the undergraduate studies i ve found this fantastic resource by mit intro to linear algebra gilbert strang in comparison the course book of codethematrix com teaches only the essentials needed for ml it seems to take considerably less time to finish it and i m also learning lot of real applications which means a lot for practical learning
qse2ov,0,why do cnn kernel weights reach high values i ve recently read a bunch of literature about network pruning a common criteria in the field is to select kernels that are to be removed by their l1 magnitude e g 1 2 as the heuristic apparently catches relevant kernels quite well most often the time the cnns are trainied with some form of weight decay this is imho intended to regularize the model and prevent single kernel weights from dominating the entire set of parameters and distribute relevance across multiple channels also it is normal to add batchnorm to the architecture as it stabilizes the training procedure i d argue that 1 the only relevant thing for detecting a pattern is the relative weight between the kernel weights the absolute value does not matter as it will only change the value range cnn kernels are a matrix multiplication and therefore a single scaling factor would do the same as scaling the entire thing 1 as the kernel is followed by a batchnorm the values get automatically scaled and zero meaned before getting scaled and shifted so there is less reason for that when using conv bn relu ordering 2 with any weight penalty this should lead to continuously slowly decreasing values of the kernel weights except in the batchnorms which is way more penalty efficient than a kernel the only reason i could think of is that one will run into some numeric stability issues when one approaches the minimum resolution of the data format i e float16 32 maybe this introduces some sort of noise into the optimization process however as the evidence shows that we get higher weights i have to be wrong and would be happy to get shown where my thought process breaks down 1 comparing rewinding and fine tuning in neural network pruning 2 learning efficient convolutional networks through network slimming
rvqr95,0,play against an ai to detect fake audio hi everybody i m a phd student interested in audio spoofs voice recordings faked with the help of ai and have developed an online game you play against an artificial intelligence and try to distinguish spoofed from real audio recordings it s fun and very much supports my research all partificpation i e playing the game comments or suggestions are welcome
rglz0f,0,reaction to a beautiful moment do y all ever have moments of pure glee when developing your networks i have an encoder decoder type network that had a custom designed intermediate function acting upon the output of the encoder thus far training per batch took north of 180 seconds batch of 8 with number of samples took a holistic training time north of 13 hours and just now i got it reduced to 1 second per batch for training never felt happier
rfhp4g,1,ml roadmap hi there im actually learning ml and was wondering if there is any kind of roadmap of study or some guide to be focused in what is most important to learn ty for reading
rgpqix,0,is this exaggerate to what extent is gpt 3 capable of reasoning to what extent is gpt 3 capable of reasoning the reasoning ability of gpt3 shown in this article shocked me but my own experiment using gpt 3 api is far less effective than the results in this article this confuse me is there any trick making gpt 3 answering like that my result is the same as the last comment gpt 3 just produced one of the dumbest things i ve ever read
ra89tu,1,i am just starting off my masters in data science iâ€™m trying to get ahead of the game and learn as much as i can any book online instructional youtube or other recommendations
r6axta,0,continue training using vgg16 on higher resolution dataset hello iâ€™d like use the popular trained model vgg16 trained from fixed size 224x224 rgb images to continue training on my own dataset the only thing is my dataset is of size 896x896 rgb images can i still use the vgg16 model or is there any tweaks i need
rcszwx,1,best path to learn ml given my experience hello i currently work professionally as a data engineer but would like to learn more about machine learning i think i d like to eventually try out transition to a machine learning engineer but i m not too sure where to start i m comfortable with python sql aws and remember some basic calculus linear algebra from college studied chemical engineering any tips or guidance on how to start learning more about ml
r9jd2h,0,how to go beyond data parallelism and model parallelism talking from gshard this article lists papers on gshard presents background information and inspiration from the papers and finally evaluates what else can be done to improve gshard from similar work that has been done in oneflow article oneflow paper code the paper of gshard contains two main parts of work one on parallel apis and one on mixture of experts the former part is more interesting and i will only discuss this part the contribution on parallel apis is outlined clearly in the abstract of the paper gshard is a module composed of a set of lightweight annotation apis and an extension to the xla compiler gshard paper
r78jko,0,platforms for python off the shelf for genetic algorithms and aco hello i am somewhat new to machine intelligence and it s subsideries so apologies if this is a silly question i am currently tasked with finding and implementing python code using off the shelf i ve done some research but nothing has jumped out at me nor very applicable to my task wondering if anyone knows the best sites platforms for finding things like this as i have been unlucky thus far also worth noting i m required to reference anything i find so the more in depth the better thank you
qskjq5,0,what simulation software would you use to train a custom robot in say i want to use reinforcement learning to train a custom virtual robot to stand what simulation software would you recommend the requirements are the following good training times for a billions steps i would want the step to restart if the robot hit the ground hard i would like to get lots of training steps in as fast as possible inputs and outputs to and from python to observe the state of the simulation and take actions at every one of the robots joints position balance and velocity observations will be needed as well ability to observe visual sensors on the robot very fine construction of the virtual robot in terms of size and weight of components exact positioning of the components precise force of the robot s motors at each joint i see gazebo and mujoco are popular but i m not sure they can do what i need if there isn t an option maybe i will write my own physics engine
rjnwo2,0,sinusoidal kernel for nearest neighbors i have a partially ordered set and want to find n subsets where the set consists of numbers that have the least error when in the original order the values are evaluated for harmonic motion some weird stuff the motions will all have the same frequency the phase may be different there is not a geometric lattice i e some elements of the subset may go away in a fixed window but come back later and the order can therefore change this will change the frequency but erroneously i can evaluate the data for the gist of the ground truth in a more manual way but iâ€™m hoping to find a vectorized solution that uses a target function a number of subsets to find and uses something efficient to solve for set membership like a knn approach my theory is i can do this by fft with a target window size and then make use of the imaginary part to deconvolve the set
rs6k65,0,most important metrics for labeling data there are a lot of great articles about measuring the performance of data annotatorsâ€™ agreement on labels like this one i see mentions in a lot of places of cohenâ€™s kappa krippendorfâ€™s alpha fleischerâ€™s kappa comparing to predefined ground truth etc if youâ€™re managing an annotation process in your organization how do you evaluate your annotators and what challenges have you faced in the process as a side note is anyone using programmatic labeling in a real dataset thoughts
rn63vl,0,cliprcnn tiny text guided zero shot object detector we propose a new tiny architecture of zero shot object detector inspired by the classical r cnn object detector named cliprcnn how it works 1 first we should generate region proposals so as we need a class agnostic region proposals generator we chose a classical algorithm named selective search instead of any modern pre trained object detection networks this step is similar to proposal generation in r cnn network 2 after we compute clip loss between each proposal and all user s prompts texts and images 3 last we return top k best proposals with minimum clip loss as a prediction of the cliprcnn model this approach isn t perfect at all but it is really simple and works after writing just a few lines of code you can find our implementation of the cliprcnn here
ru06dy,0,coding practices my job is to work with ml engineers and provide them with whatever they need to experiment with train test deploy ml models gpu infrastructure distributed training support etc when i interface with their code i almost always find it so poorly written with little to no thought given to long term stability or use for code that they 100 know is going to production they re brilliant people far smarter than me and really good at what they do so it s not a matter of them not being good enough i feel from my very limited experience so i m happy to be wrong like ml engineers are incentivized to write poor code the only metric for evaluation seems to be accuracy loss and all the plots that come up in research i understand completely that s where the focus lies but in industry i ve seen many models perform poorly because the code is so hard to read and refactor that big issues remained unspotted for months together and this is especially befuddling because for a field that is completely fine with spending months to get an roi of single digit increases in model performance metrics during the experimentation phase they don t seem to care about anything that might go wrong in production that just feels like a fundamental disconnect since without the core ml stuff working perfectly none of the other stuff like what i do has any value and even so i m taught to hold my code to a much higher standard than the critical stuff which i m happy about since i can now write production code by default but it s just weird like the vending machines at a nuclear power plant being better engineered than the reactor is this a common problem or is this a localized issue that i m facing
ro1ke3,1,how do you make a neural network classified fruits
rut9hs,0,is there flow based method which treats input data as different lengths each hello i am searching the researches that different size of data are generated through the flow based network not super resolution task such as continuous mapping i want to generate output as time aligned scalar data for example x200b input noise sampling b x t x c output scalar data b x t c 1 x200b with introducing the variational data augmentation technique in vflow which can output high dimensionality as concatenate noise vector for input and output both for output but there s a problem time dimension t is different for each of all data input how can i treat this problem x200b p s i am very appreciate if i can read the flow based research in nlp task
qkvy6l,0,any movie dataset with movie summaries do you know of a dataset that contains movie summaries do you know if researchers are legally allowed to download imdb movie summaries for research purposes
rkxmt2,0,question about collaboration with data scientists and engineers hello newby to the group here i was wondering if you would mind elaborating on potential demand for ml operations collaboration with data analytics departments and data scientists when i first started in data science the last thing i wanted to do was integrate with apis and perform etl clean and format data into feature sets and manage models i had the most fun calculating interesting statistical reports and building and running simulations i m wondering if with today s data science landscape and tools available folks are still feeling that problem if the problem exists how are folks handling it particularly when building production capabilities from the research and models a little background if you are interested i started developing over 20 years ago because of an interest in data science particularly in finance and economics multi agent systems and machine learning in lisp at the time but my career application and hosting platform engineering and architecture took me away from that objective and i gradually became unhappy with what i was building and managing i made a choice to get back to that passion and i started working on a platform to do the underlying plumbing of managing cloud infrastructure and data integration and processing so i personally could create multi agent simulators and self optimizing recommendation engines but admittedly i am behind on my knowledge about what is really going on in the data science market i created this open source project after my brother died and i would have built it even if nobody else in the world wanted it because it connected my experience with my passion and i really needed something fun to do the reason i am thinking the demand might be there is i have a friend who has a small ai consulting shop and i have been able to work with his team on a us federal government project which allowed me to work with others on model strategy and implementation they did the strategy and initial model implementation in notebooks and rough libraries i translated that into more pluggable and scalable application code and my open source software is handling the production ops and recommendation engines we all got to do what we loved and it seems to be working out well for the client this project took me back to a place i loved in software engineering and i am wondering if this would be a good market to serve because starting in the new year i have to start generating some more revenue and i would prefer to do that in a area i am passionate about
rh1viq,0,layer wise relevance propagation in pytorch hi there i have set up a basic implementation for layer wise relevance propagation lrp in pytorch that comes with an additional relevance filter method for much crisper heatmaps i was not so happy with the lrp implementations that were available for pytorch so here you have it the implementation is easy to understand and should be extensible without much effort here is a small preview the repository can be found here thanks
rcs88b,1,resource i m releasing three of my pokemon reinforcement learning ai tools including a computer vision program that can play pokemon sword autonomously on nintendo switch video proof source code available hullo all i am tempest storm background i have been building pokemon ai tools for years i couldn t get researchers or news media to cover my research so i am dumping a bunch here now and most likely more in the future i have bots that can play pokemon shining pearl autonomously using computer vision for some reason some people think i am lying after this dump that should put all doubts to rest get the code while you can videos let s start with the video proof below are videos that are marked as being two years old showing the progression of my work with computer vision and building pokemon bots the videos above were formerly private but i made them public recently repos keep in mind this isn t the most up date version of the sword capture tool the version in the repo is from mar 2020 i ve made many changes since then i did update a few files for the sake of making it runnable for other people tool 1 mock environment of pokemon that i used to practice making machine learning models tool 2 i transformed the pokemon showdown simulator into an environment that could train pokemon ai bots with reinforcement learning tool 3 pokemon sword replay capture tool video guide for repo presentation i am working on a presentation for a video i will record at the end of the week i sent my slides to a powerpoint pro to make them look nice you can see the draft version here qa some people might have questions for me it will be a few days before i get my slides back if you use this form i will add a qa section to the video i record discord in the event people are interested in the code and want to learn how to run it join the discord it has been empty for years so don t expect things to look polished current link who am i my identity is no mystery my real name is on the slides as well as on the patent that is linked in the slides shining pearl bot it is briefly shown at the beginning of my custom object detector video around the 1 minute 40 second mark conclusion i will do a presentation of my journey of bring ai bots to nintendo switch hopefully sometime this weekend you can learn more about me and the repos then
rv3yty,0,bringing serverless to ml stateful arbitrary dependency serverless for ml serverless infrastructure is yet practical to use for ml but we think it could bring lots of benefits so with a friend we decided to make serverless easy for ml and we are building a platform to solve the main issues we find in serverless for ml stateful we donÂ´t want to reload a whole model every time a user calls model predict arbritary dependecies normal python code with any package dependencies you use and love just many many times in parallel scale up and scale down scale up with ease and auto shutdown to keep resources consumptionyou can visit our webpage try the demo and request early access to use our platform webpage happy to receive questions and comments on what we are building
rb5r86,1,machine learning and ai hey guys im doing an assignment for machine learning and ai what topics should i cover
rhmz6m,1,have you ever taken andrew ng s machine learning course in coursera my friend and i are thinking to enroll in this course to start the new year of 2022 until then we saw that it s an 11 week course and now we re thinking if we should start it with a community to increase accountability and completion rate you may wonder why we don t just take a ml cohort based courses well that s a good idea but we don t have a budget it s rare to find good cbcs in a low cost that are well taught by famous instructor like andrew ng from stanford that s why we find this course valuable since it s free but we don t want to study alone or just the two of us either so do you know anyone who s also planning to start a year by learning a new skill in ml it would be nice to start it on january here s the link by the way
qsaonk,0,adversarial loss understanding in total relighting learning to relight portraits for background replacement paper hi i hope to get some help understanding google s total relighting learning to relight portraits for background replacement paper i m stuck on the adversarial loss in 4 1 paper says we add an adversarial loss on the face region to help the network learn to plausibly remove high frequency shading effects from the input image while maintaining image detail we use a least squares discriminator mao et al 2017 disc alb to add a loss between a crop of the face from the ground truth albedo ð´crop gt and a matching crop of the face from the predicted albedo ð´crop predicted albedo ð´crop is the face crop of the output of u net like network no other details provided and i m struggling to understand the setup here since in the paper they re using both ground truth and predicted albedo as the inputs for the loss i can imagine two scenarios 1 use the loss from the original lsgan paper and train the discriminator during the model training which seems counterintuitive to me 2 use l1 or l2 distance between pretrained discriminator s output for ground truth and prediction but that is not really adversarial loss i guess i have no experience with such discriminator usage and therefore can t choose between these two or come up with something else reasonable is there a common way to use gan discriminator for the loss calculation of non gan networks because from the paper it sounds like something that doesn t require deeper explanation
rgu4u8,1,running collaborative ml experiments with dvc tutorial sharing ml experiments to compare your models is important when you re working with a team of engineers you might need to get another opinion on an experiments results or to share a modified dataset or even share the exact reproduction of a specific experiment the following tutorial explains how you can bundle your data and code changes for each ml experiment and push those to a remote for somebody else using a google drive folder to check out using dvc data version control tool running collaborative experiments the tutorial shows how setting up dvc remotes in addition to your git remotes lets you share all of the data code and hyperparameters associated with each experiment so anyone can pick up where you left off in the training process
r7wdou,1,i need help understanding the meaning of the loss values of wgan with gradient penalty hey guys i am currently working on training a auxiliary classifier wasserstein gan with gradient penalty i based my implementation off of to which i added the auxiliary classifier functionality i have a model trained off of quite a few epochs of emoji images and i am now trying to filter out bad or vice versa good samples from the set of generated examples i understand that the best way to automatically do this is to utilize the trained discriminator and its loss values to evaluate the fakeness of the images in order to be able to select ones that are able to fool the discriminator now i understand that the discriminator using the wasserstein distance attempts to separate the loss values of fake samples as much as possible from the ones of real samples given that my discriminator output for a randomly chosen set of generated images looks something like the below table i think that the images with maximum or minimum values should be the highest and lowest graded examples 22 96732 12 37780 23 39248 44 45711 14 15668 11 19169 35 99777 9 65943 16 71531 9 35125 25 98240 4 36232 8 58446 24 78805 7 47653 19 14746 28 33695 30 18404 2 67499 8 63077 this should mean that my most fake and most real generated image are the samples with 44 4 or 2 67 but neither of these outliers are particularly more real looking than the other and both look much worse than randomly chosen examples see here how should i interpret these loss values would it make sense to go for a median loss value to get more of the good looking average loss images if so why would it make sense to run a 2 means clustering algorithm on the losses to try and separate realistic and fake samples anyways thanks in advance for your help
rrsjbi,1,list of statistics topics need for machine learning hello everyone i ve been endlessly googling this and i can t get a definite answer can someone please tell me a list of topics in statistics i should know to get started with machine learning and will further statistics be learned while one learns machine learning or does one have to specifically learn more stats topics before moving onto the next phase of learning ml thank you so much
ra1vy3,1,artist seeking to learn more about high res image synthesis hey there r learnmachinelearning i m a photographer and artist preparing for a small gallery show in february and i ve been experimenting with vqgan clip and taming transformers as well as image scraping tools like flickr scraper and the likes in the hopes to achieve generative images based on famous artworks i m hoping the kind folks on this sub can provide me with a little guidance here s what i want to do produce ai generated photo realistic images from source images of well known photographs i ve tried creating my own image segmentations to feed to taming transformers but it appears that it doesn t handle photographs involving people very well only landscape images is there any way to get somewhat photo realistic images generated by ai
r7uqrp,1,sentiment analysis api vs custom text classification which one to choose kdnuggets
rg4r3l,0,how can i decrease the response time of meanshift algorithem in sklearn currently i am using a m1 macbook pro and i want to use the meanshift clustering algorithem to segement an image but it takes like 2 3 seconds to give the output is there anyway i can reduce the time
qv1t0h,0,labeling tool for re id dataset i want to create re id dataset with 10 classes however i couldn t find tools that allow id assignment the closest thing i found is cvat but it doesn t allow me to create bounding box with specific id does anyone know other tools that are suitable for multiclass re identification
qx479l,0,reinforcement learning for traffic light control i have read this very interesting paper about rl for traffic lights reinforcement learning benchmarks for traffic signal control i have already worked with rl for train scheduling but traffic lights are new to me here s what it says in a nutshell many people have developed rl approaches for traffic light tl control many of these people have claimed sota ðŸ˜… in this paper they introduce a well defined benchmark using the sumo simulator they show that all things considered recent methods don t perform so well and a simple independent dqn approach is best the flavor of this dqn approach happens to be described in the first author s previous paper this sounds very interesting and wouldn t be the first time that a cleaned up benchmark reveals that a simpler method was the best all along but a few things surprise me independent dqn beats everything else this surprises me as some of the other methods use cooperative approaches i know marl is hard but still in the setting of a complex city i would expect traffic lights that are able to communicate to outperform those that can t although i d expect them to converge more slowly the convergence speed and stability look crazy to me really solving a region scale problem with 21 tls and 4 2k car trips in fewer than 200 episodes of 360 timesteps each that doesn t sound realistic to me in rl land last point i want to bring attention to the amazing intas project this guy made a realistic traffic scenario for ingolstadt germany using actual data from traffic lights public transportation shop opening hours etc that he got from the relevant offices the presentation is worth watching i m curious to hear from more experienced practitioners
ru6fvr,1,trying to get into ml with a specific project and need some help getting started hi i have worked with ml before with simple classifications in the particle physics realm but now i want to go a bit beyond that and have chosen to do something relating to a hobby of mine the hobby is a 5v5 video game where the end goal is to have something where i can feed it what one team is doing and have the model give me what the other team should be doing to have the best chance of winning as a first start and want to have a model classify team1 s chances of winning based on just their trajectories where i train it with their trajectories on the game map and the label if they won the round or not but already here i am not sure how to best build the model because of the trajectory data for each of the 5 players on team1 i have they x y z position for the first 20 seconds of the game but i also dont want the model to care about which players causes which trajectory so the model should treat player 1 trajectory1 player2 trajectory 2 the same as player1 traj 2 player2 trayj 1 it would be great if someone could point me to ressources that allow me to understand how exactly i should build the model for this first step
r6nzv3,0,animegan for videos what would your anime look like using this gradio demo you can find out by recording a video from your webcam it applies animeganv2 across frames of your recorded video stitches it back together and lets you check out the results try it out now on hugging face spaces ðŸ¤—ðŸš€ ðŸ‘‰ demo ðŸ‘‰ announcement thread
rbw8g2,1,help with nim game using reinforcement learning i recently was given this assignment i have already submitted it but it s taking a while to get back and i would like some feedback to know that i m on the right track and actually understanding the content here is my code get reward of a state float get reward int state int turn float retval 5 if state 20 retval 100 if turn 0 return retval else return 1 retval get potential of current state int get va int state int turn if state 20 return 0 turn turn 1 2 float reward1 get reward state 1 turn float reward2 get reward state 2 turn float v1 reward1 gamma get va state 1 turn float v2 reward2 gamma get va state 2 turn multiply each potential by its probability of happening 0 5 v1 0 5 v2 return 0 5 v1 v2 decide which value to play returns either 1 or 2 int get value int score float one reward get va score 1 0 float two reward get va score 2 0 printf 1 f 2 f n one reward two reward if one reward two reward return 1 return 2 my thinking was to get the potential of each move i know i do this each time and is inefficient but for now that s fine and play the move with the highest potential each time i m a bit skeptical of this though since i just assumed 50 probability of each play rather than the reality of the other opponent playing the best move i would love some feedback i didn t share the rest of the code since it is my professor s and i don t want to get in trouble for sharing that but it is pretty simple it just prompts the user for an input keeps track of the score and calls the get value function when it is the computer s turn edit i just found a way to make it not work here are the logs make run compiling running who says first 20 who goes first you 1 computer 2 1 we are at 0 add 1 or 2 1 1 3 000000 2 2 000000 we are at 1 computer adds 2 we are at 3 add 1 or 2 1 1 3 000000 2 3 000000 we are at 4 computer adds 1 we are at 5 add 1 or 2 1 1 1 000000 2 3 000000 we are at 6 computer adds 1 we are at 7 add 1 or 2 1 1 4 000000 2 0 000000 we are at 8 computer adds 2 we are at 10 add 1 or 2 1 1 8 000000 2 0 000000 we are at 11 computer adds 2 we are at 13 add 1 or 2 1 1 17 000000 2 8 000000 we are at 14 computer adds 2 we are at 16 add 1 or 2 1 1 25 000000 2 50 000000 we are at 17 computer adds 1 we are at 18 add 1 or 2 2 you win
rwmsd6,1,bias in a fully connected deep feed forward neural network i m coding up a neural network from scratch to increase my knowledge of neural networks and am having trouble with bias should each node have a bias value number of biases number of weights or should there only be one bias value per layer any insights would be much appreciated
qs5igm,0,how do i lower my standards for code quality to match my research team i recently joined a lab working on research in robotics reinforcement learning as a research intern i graduated college recently all my previous work experience is ml engineering but not active research rather implementing things people have already done i ve been in the lab a few months now and it s become clear to me that the standards for code in research are a lot lower than i expected i am the only one on my team pushing for things like ci pipelines refactoring documentation regression tests etc and when i do raise such concerns the usual response is that such things are not worth spending time on my supervisor once told me that the job of a researcher is not to write good code it s to find a correct problem formulation describe it correctly and then pass it to a software engineer whose job it is to create the high quality implementation they emphasised the importance of rapid iteration of the idea rather than going slowly because they re concerned about getting scooped in our current project this is true to some extent because we are a small and resource limited group whose experiments can take hours to run working in a popular field quadruped legged locomotion i struggle a lot with this mindset i hate looking at the crappy code written by myself and by my teammates and i hate debugging it trying to understand it a lot of my time is wasted on things like checking which experimental configuration a model was trained from because we don t have automated logging of that aesthetically i also just dislike it i have high personal standards of code quality uncle bob s clean code anyone which i feel like i m constantly breaking when i have to write code for work i m looking for advice on how i can learn to tolerate this better or other similar resolutions to the problem thanks in advance edit thank you all for the kind replies so far i have received many more responses than i was ever expecting to get it s going to be difficult to reply to each one but i will do my best to read all of them and consider i will respond here to some general points i ve seen made repeatedly 1 i m not arrogant enough to think that i know everything about research and coding after 3 months in a lab and as many have rightly pointed out there s no need for research code to live up to the standards of long term production code i do recognize that in this case the problem likely lies mostly with me hence the title of how do i lower my standards and the question of how do i learn to tolerate something i m not comfortable with seeing how popular this post is i m sure this struggle resonates with many others as well as a result i hope for there to be less cynicism about my motives the discussion so far has been largely productive and on topic i appreciate very much it staying that way 2 to give some additional context i am working right now as part of a group i have a main project which i work on with 1 other person also a recent graduate my supervisor actually doesn t look at the code we write at all and as far as i can tell does not care much about code they seem to implicitly trust that we are able to correctly implement things according to the description which i believe is true for both of us at our weekly meetings we present our progress in the form of videos graphs slides this means that i and my teammate are solely responsible for maintaining the quality of code in our current project and as mentioned we have philosophical disagreements about how much code quality matters i know i am not necessarily right 3 the project i am working on is very much not a one off experiment our main codebase is inherited from somebody else who is no longer in the lab which was itself an iteration on some open source implementation we are iterating various elements of the robot s sensor setup in addition to the reinforcement learning algorithm and training curriculum with well known off the shelf envs like those in mujoco i d be pretty happy accepting that it was working as intended with code i write myself or code written by other people which is not tested it s a lot harder to have that confidence 4 as some have correctly pointed out using a standard logging tool like w b would be an easy fix to the time spent on resolving experimental configurations that issue in particular could be resolved easily by setting up the appropriate pipelines and would likely be a net plus to my team more generally i could benefit from critically examining the practices i am used to and seeing which have concrete benefits in my current workplace and then propose those to the team 5 i am a strong proponent of unit tests unit tests save my butt from making simple mistakes it s already incredibly difficult to figure out why experiments fail sometimes i can at least eliminate some of the possible causes by unit testing code where easy and appropriate also when i do fix a bug i write a unit test to enforce the fix and then i can mostly forget about it that mental real estate can go to things that can t be automated as easily 6 similarly i am a big fan of simple ci pipelines like github actions perhaps the term ci comes with a lot of associated bells and whistles which are rightfully not used in research code but to me ci is simply a way to automate the manual running of unit tests with a bit of know how github ci workflows are easy to set up from a cookiecutter template and they save a lot of time as well as mental worry i like to include linters as well but it s not necessary to the core functionality
rhcxmu,1,which one is better uva masters in ds or georgia tech masters in data analytics see links below i am currently a software developer trying to get into data science ml iâ€™ve been taking online classes on my own but my company is willing to pay for my masters program of my choice which one should i go with uva or georgia tech uva program gtech program
rwenkn,1,the ml dojo get daily updates on the latest in the field of artificial intelligence ðŸ§  and machine learning ðŸ¤– link to the ml dojo letâ€™s start with a question how do you keep yourself updated with the latest happenings in ai or ml really think about it for a second before moving forward i have asked the same question to a lot of ai ml practitioners and a quick summarization of their responses includes â€” following top scientists or researchers or ai labs on twitter finding good articles on medium or recent papers on arxiv or posts in reddit to even scanning through interesting questions on stack exchange not a complete list by any means ðŸ˜Š performing one or two of the above practices can be possible but are you sure you are not missing something important by skipping the rest but the flip side is more daunting â€” are you okay spending hours daily just browsing through multiple platforms hoping to stumble upon something interesting if your answer is no the ml dojo is here to help ðŸ–– with this context i am ready to answer some of your burning questions what is ml dojo ml dojo is a daily report on the latest and upcoming research experiments topics articles papers â€¦ you name it in the field of artificial intelligence and machine learning why ml dojo there are two main reasons to subscribe 1 we publish daily yes no more waiting for the weekly or monthly newsletters 2 we cover a wide variety of platforms to make sure you donâ€™t miss anything important we keep an eye out for the latest feed on twitter reddit stack exchange medium and more coming soon in short you get the best of all of the worlds and that too daily ðŸŒŽ see you guys there ðŸ˜€
rjzlf7,0,simple questions thread please post your questions here instead of creating a new thread encourage others who create new posts for questions to post here instead thread will stay alive until next one so keep posting after the date in the title thanks to everyone for answering questions in the previous thread
rhp5gj,1,from where to learn python for specifically for ml i know python basics now i have started to read book on ml hands on ml with scikit there are some codes in python i can t understand them i want to learn python for ml perspective please suggest me resources
rade21,1,writing own inference engine with quantization from float32 to int8 hello i have a question about quantization i m trying to write an inference thing manually i ve already written a working version using float32 it is an image processing network so the size of intermediate layers is mostly effected by the resolution of the input i wanted to try to use int8 instead of float32 to see how well it worked in pytorch when i convert to quantized int it only converts the weights not the bias intuitively i think i would need the weight bias and input data to be in int8 to run the network with the smaller memory usage is this intuition true if so how should i convert the bias to int8
quip0a,0,siraj raval youtube video siraj has put up an interview that lex fridman did about two years ago lex fridman interviews siraj raval lex took the video down from his channel siraj asked for permission to repost the video lex didn t reply so siraj took it upon himself to post it anyway
rwumjj,0,vq vae are there heuristics for the number of embeddings and the embedding dimension hi r machinelearning does anyone with experience training vq vaes know if there are good rules of thumb for the embedding size e g given data of dimension n use m embeddings of size p thanks for any help
rev18h,0,announcing the transactions on machine learning research announcement of a new ml research journal with this post weâ€™re happy to announce that we are founding a new journal the transactions on machine learning research tmlr this journal is a sister journal of the existing well known journal of machine learning research jmlr along with the proceedings of machine learning research pmlr and jmlr machine learning open source software mloss however it departs from jmlr in a few key ways which we hope will complement our communityâ€™s publication needs notably tmlrâ€™s review process will be hosted by openreview and therefore will be open and transparent to the community another differentiation from jmlr will be the use of double blind reviewing the consequence being that the submission of previously published research even with extension will not be allowed finally we intend to work hard on establishing a fast turnaround review process focusing in particular on shorter form submissions that are common at machine learning conferences
rs5cjb,1,introduction to machine learning for beginners hello we are a group of students which has made an introduction for machine learning for people who doesnt know anything to teach them the basics we have 4 models full explained in english with lots of comments which can be used to test online and know the potential of ml also we have some videos in spanish where we explain everything from which is machine learning until what dangers does it have through the 4 models provided and its uses in some fields like medicine or social networks i hope you find it useful and that it encourages you to enter this world
ria7qt,1,training multiple regressors with independent weights on same dataset pytorch hi i d like to train multiple linear regressors 20000 odd in pytorch that take the same input features but have different y true ground truths also i d like to have different weights for every one of these predictors the architecture that i m supposed to use for this task is a simple 2 layer neural net i m stuck with how to come up with a way to train these classifiers independently i thought of making a list of nn module objects and then invoking one by one with different y s however i can t seem to find any documentation against the same could someone point out to such resource
rv4nah,0,nlp hybridization of statistical approach and expert system hi everyone i have a question for you for context we aggregate on a platform the various ai apis on the market gcp azure etc and including nlp apis keyword extraction sentiment analysis ner etc the idea is that a developer doesn t have to create accounts with different providers and can have them all on one api to test compare and change whenever he wants however many customers ask us how to mix the statistical approach behind these apis with expert systems and how to achieve hybridization do you have any idea how to do this thanks
rjgbxc,0,memory efficient attention self attention does not need o n 2 memory this project is unofficial implementation of self attention does not need o n 2 memory for jax and pytorch implementation is almost the same as the one proposed in the paper with additional masking and adding bias compatibility batch dimensions support and pytorch implementation for computing attention the proposed method requires only o sqrt n memory and the provided functions can be used as a drop in replacement for attention calculation github
r2o6jr,1,need help with project so as a part of my final year project i ve decided to do a project of leaf disease detection using image of leaf i ve just started studying machine learning for few months and only know some basics the problem is i ve searched for papers and tutorials for this kind of project and they all use cnn but my teacher hasn t allowed me to use cnn since i ve just started into machine learning he says i need to use traditional methods like random forest svm etc so can anyone help me which method should i use and provide a basic roadmap for my learning
qmq7hb,0,is there any way for gan to generate arbitrary length of time series signal hello i m working on using gan to generate some signals as i have viewed some related works i found that most of them merely sample a latent vector from some distribution with fixed size e g a latent vector of dim 64 and after some upsampling operation they will get signals in a fixed window size e g a signal of 4s 256hz 1024 points i want to get rid of the annoying limit of fixed window size and be able to generate continuous signal with arbitrary length i tried to code by myself i have designed a gan framework in which the generator takes an input of an arbitrary length of vector and outputs a signal in same length as the input the upsampling process is thrown away here so the generator merely do some modification on the input signal instead of upsampling on it as for discriminator i use global average pooling in replace of the linear layers however my code failed to work so i think maybe i need some new ideas i come to you guys for help do you know any paper that might be helpful for me or do you have any good idea thanks
r8pr04,1,picking a kernel for svm i am working with higher dimensional data and and only need to differentiate two classes with svm if i do pca the training data for the two classes separate very cleanly so cleanly that i can just manually draw a vertical line though the x axis of the pc plot and it does a great job of separating the classes it seem like each tutorial page i look at people use slightly different parameter dictionaries for grid search gridsearchcv from scikit learn in my case i am kind of wondering how much of an art this step is like if i can intuitively see linear kernel is probably the right choice should i always still iterate through rbf poly
r23zpp,0,about adding tags to images in the stylegan dataset i m not experienced in ml i have read somewhere that a data set for stylegan2 stylegan3 training can be divided to separated classes or tags by putting images with common features to separated folders in the main folder and then making a tfrecord from this root folder if you do so can it actually improve quality of a model stability of training
qqopdj,0,google automl s prices i m trying to understand google automl s pricing and i have three questions 1 what is price for forecasting here 2 how much will i pay for having an endpoint available 24 7 to which i can post data and execute previously trained model assuming simple numerical data with classification 3 can i upload my own model and have it ready for predictions thanks
qtiqlw,0,walk forward target encoding and data leakage hi i am working on a time series problem where i need to also test historical predictions through time using a walk forward procedure there is a data leakage when dealing with time series and using simple target encoding features like df groupby col target transform mean x200b i am wondering if anyone has built a custom function to build these target encoding features without leaking data kind of like a walk forward target encoding function
rl6pnt,1,task too difficult for intern recruitment process hey everyone some background i am a college sophomore trying to pursue ml dl one of the startups my college is incubating wanted some interns for ml i applied and got through the interview round for the second task however i receive this as next step towards the selection kindly take a ml dl use case of your choice create a suitable gui and integrate it with the solution ensuring utility for an end user mind you this is supposed to be an unpaid internship with a certificate at the end and their time the time given for the assignment is 41 hours that is just under 2 days this is my first internship is the startup asking too much or am i just too inexperienced edit i conveyed to them about this task being above my skill level thank everyone for their responses
rfaz1q,0,how do you choose correct lambda values for your loss function how do you choose correct loss lambda value are all losses should be same scale when your lambda selection process do you just try different lambdas using any search algorithm random search grid search eth i wonder that because i work with deep learning models i use lambdas but i don t know exact intuition
r015mw,0,death threat or cat meme why context matters in machine learning is this a death threat â€” or a cat meme is this post rooting for cancer â€” or it s demise more examples here for those interested context can be absolutely crucial when building ml models how can we expect machines to effectively understand and analyze our world when we re only training them with isolated data then again it s complicated and computationally expensive to build these context inclusive models for those who are actively building ml would your use case benefit from including context would love to hear any and all experiences on the topic
rkf0fh,1,yolov4 custom classifier hi all i am starting a computer vision project to identify images of fires i have mostly only worked in tensorflow and keras but i have had to try and work with darknet to get yolo working i have an annotated dataset and get a weights file but i can t get it to predict anything really i have tried working through some tutorials but it hasn t really helped people seem just to use weights from coco i was hoping someone could suggest another resource to help me or would be willing to connect to help me troubleshoot i am cs student and i would really appreciate the help
r4001a,0,which vectorizer do you use when building a search engine system based on embedding retrieval there are so many vectorizers to choose from which one do you use and why also curious to hear about other use cases and which vectorizers others are using view poll
r6bttm,1,back to basic linear regression with python code in 2 minutes one of the biggest fields of application of python these days is machine learning and one of the first things anyone learns about machine learning is how to do linear regression linear regression is attractive because it is very simple to understand at least compared to other learning models we also know how to implement it efficiently and more importantly we can make sense of the results i made a 2 minutes video exactly about this topic by design a 2 minutes is never exhaustive but hopefully this will serve as a good intro to the topic and will give you the motivation to dive deeper if it sounds like something you are interested in i know that when i started out i wished there were resources like that available so i would love to hear what you think about this format
rp1af8,1,need help in deciding an approach i m currently working on a personal project and have encountered a problem which is keeping me up and i m having trouble in deciding what approach should i take i cannot reveal the whole problem but it is similar to this one i have the data for varying weights of lets say 100 people with time and i need to predict the time after which the weight of majority of people would be more than a certain threshold any suggestions on how i should approach this would be great
r761vg,0,clip vs starspace after reading both the paper accompanying clip form openai as well as starspace from fair meta ai from a couple years ago it is not very clear to me what the exact novelty is besides a prettier presentation even though images are not considered in the starspace paper that method essentially also works through potentially cross modal contrastive embedding learning what s the hype
rbzzpz,1,â€œâ€ whatâ€™s the best way to analyse behavioural activity data to get correlations with health insights i have a large dataset with activity behavioural data date abs time stamp of when this activity took place self reposted mood associated with each activity and geolocation data whatâ€™s the best way for me to derive health and wellness correlations from this data based on a userâ€™s activities can i build a recommendation engine to suggest do more of something or less of something based on user goals these activities are activities of daily living such as working reading shopping cooking cleaning watching tv etc any suggestions to get started would be helpful
qxeu1x,0,stress testing models to ensure robustness hey i was wondering if we could get a discussion going regarding stress testing ml models to ensure their robustness maybe someone has some ideas or paper recommendations i know it s possible to test the robustness of image recognition models by for example rotating flipping adding noise to the data but i was wondering if there was some more scientifically thorough guides for this also could generative models or adversarial attacks be used for this as well and to what degree curious to see what people here think about the topic
rvtxxh,1,need help with first job as an ml engineer hello everyone i recently graduated with a master s degree in cs and i got my first job last month as soon as i started i realised that even though i was hired as a backend engineer my tasks would be more akin to those of a machine learning engineer the company is an ad company and my task is to predict the revenue of an ad using data from the last 4 months from september to december of last year i took the data built a feature vector with it tried basically all the regression models available on scikit learn and got a r2 score of 0 7 using 80 of the data for the training set and 20 for the test set the problem is that while the model works really well with instances from those 4 months when i try to predict the revenue of an ad from january it fails miserably my question is how do i deal with this does this mean that my model is overfitting or is it just because it has never seen an example from that time period thank you for all your help
qsjngz,0,can unity3d ml agents use gridsearch hello i am trying to find a way to do gridsearch for hyperparameter tuning for reinforcement learning under the unity3d machine learning platform i googled but it feels like no options are available right now if there is alternative ways please share thank you guys
r11355,1,how to evaluate your model s performance at labelflow we know firsthand how important it is to choose the right metrics to evaluate your ai model s performance check out our latest blog article where we walk you through how to evaluate your model and what metrics to use as we take the example of an image classification model machinelearning ai evaluation
rb2fsv,1,managing ml experiments as code with git and dvc machine learning experiment tracking tools log ml experiments to a central database and show them in a dashboard this makes it easy to share them with teammates and compare however in an active experimentation phase you may create hundreds of experiments so team members may be overwhelmed and loose the ability to effectively share experiments between team members the following article shows how with dvc tool you can push experiments just like git branches giving you flexibility to share experiment you choose don t just track your machine learning experiments version them dvc all the experiments you run are stored in your local repo and only the best experiments are promoted to the central repo github for example to share with teammates distributed experiments are shared with the same people as your code repo traditional experiment tracking tools log ml experiments to a central database and show them in a dashboard this makes it easy to share them with teammates and compare however in an active experimentation phase you may create hundreds of experiments so team members may be overwhelmed and loose the ability to effectively share experiments between team members with dvc experiment versioning treats experiments as code it saves all metrics hyperparameters and artifact information in text files that can be versioned by git you do not need a centralized database or online services git becomes a store for experiment meta information and dvc data versioning backs up the artifacts themselves anywhere
r7vrf6,0,discussion dot product vs addition followed by transformation for attention networks consider that we have a set of vectors v i quad i in 1 k and a set of queries q j quad j in 1 q there are two ways of finding attention alpha ij for value v i 1 addition of keys and queries after bringing to appropriate dimension and then linear projection to scalar after a non linear activation e ji v t attn tanh u attn key v i w attn q j alpha ji softmax e ji 2 dot product of keys and queries after after linear transform so that they are of the same shape this gives one scalar value for each pair i j because of which we can simply take softmax along the dimension of values such that sum i k alpha ji 1 for each query q j which method of these is more appropriate for which usecase in general specifically i want a set to fixed length vector conversion where the number of elements in the set is variable thanks a lot
rpkn6w,1,starting route for beginner hi guys i have completed my bachelors in mechanical engineeering i have taken classes like calculus statistics probability lineer algebra and differential equations and also i have good knowledge about python and its libraries but i think i need to take these concepts again that is why i thought i would start with according to this person whom i follow i am a beginner so that road map kinda covers the general topics about ml python data science handbook essential tools for working with data jay l devore for python libraries probability and statistics for engineering and the sciences books for remembering the statistics and probability concepts with these books i thougth i would learn the exploratory data analysis and feature engineering after learning these i can go for machine learning concepts do you think that it would be a good route or do you have any suggestions
qss5os,0,micro grants are there micro grant sources besides ai grant and unitary fund
qo03fo,0,google mum details google has anounced mum several months ago in the blog post it says that it uses t5 framework and is multimodal but that is about it the name multitask unified model does not help a lot does anybody know how it is trained and how it combines text and images the only thing i can think of is to train it to generate image captions and then finetune it on multiple tasks something like bigscience t0pp
raod73,0,how to tune learning rate for large models i know there are lr finder from pytorch lightning and fastai and deepspeed but for large models above 2b params these methods are two slow are there any other faster methods to try thanks
ruyk5i,0,new paper a relational tsetlin machine with applications to natural language understanding x200b relational tsetlin machine the paper introduces the first relational tsetlinmachine which reasons with relations variables and constants the approach is based on first order logic and herbrand semantics taking the first steps toward the computing power of a universal turing machine the approach can take advantage of logical structures appearing in natural language to learn rules that represent how actions and consequences are related in the real world the outcome is a logic program of horn clauses bringing in a structured view of unstructured data in closed domain question answering the first order representation produces 10Ã— more compact knowledge bases along with an increase in answering accuracy from 94 83 to 99 48 the approach is further robust towards erroneous missing and superfluous information distilling the aspects of a text that are important for real world understanding ml ai nlp machinelearning logic relational
r10z1o,1,finding optimal sell limits in a stock trade based on historical data and buy signals this is for crypto i know i know i have the following data inputs available historical data of the open low high close prices of 4hr intervals of eth usd buy signals on close of some intervals ie buy here these appear to be pretty reliable 60 70 of the time q if i buy on the buy signals how could i start training a system to detect the optimal sell limits for my trade ie does it rise 0 7 before i sell does it fall 2 before i sell at loss etc i ve coded quite a bit but am new to ml hope to use tensorflow js but open to ideas i guess any conceptual starting points or useful resources would be helpful
rbz69k,0,lazy regularization for wgan gp training i was reading the paper about stylegan 2 in there they talk about using lazy r1 regularization which is applied every 16 steps training a wgan with gradient penalty significantly slows the training has anyone ever tried lazy regularization for the gradient penalty any thoughts
qpuax4,0,alibaba damo academy creates worldâ€™s largest ai pre training model with parameters far exceeding google and microsoft 10t parameters according to the company the m6 has achieved the ultimate low carbon and high efficiency in the industry using 512 gpus to train a usable 10 trillion model within 10 days compared to the gpt 3 a large model released last year m6 achieves the same parameter scale and consumes only 1 of its energy thoughts the pace of foundational models is starting to get scary seems like a bigger and bigger model is pushed out every week
qqkwas,0,discussion iclr 2022 submission statistics the statistics of iclr2022 submission can be found here 1 number of reviewers for each submission submissions with maximum reviewers 2 item in total each with 7 reviewers submissions with minimum reviewers 20 item in total each with 2 reviewers 2 rating variance submissions with highest rating variance submissions with highest rating gap max min submissions with lowest rating variance x200b x200b more data can be found here
qszmuu,0,analysis of iclr 2022 review scores we analysed the relationship between iclr 2022 review scores and factors such as social media popularity and presence in arxiv twitter thread here are some of the results of the analysis papers that were present on arxiv had higher recommendation scores mean review scores 5 1 papers on arxiv 4 7 papers not on arxiv papers that were shared on twitter with 5 likes also had higher recommendation scores than other papers mean review scores 5 4 tweeted 4 8 not tweeted papers that had source code available or promised to upload soon with empty repos also got better reviews mean review scores 5 2 with code 4 8 without code scatter plot of likes on twitter against the review score there is a small positive correlation between twitter likes and review scores the correlation coefficient is 0 2 all iclr 2022 submissions sorted by review scores can be found here
qlugqf,0,what is the state of the art for few shot text classification say i have many text snippets that can be one of four classes i also cannot get a large scale labeled dataset i have 30 50 labeled examples per class what methods are currently state of the art for such settings
rcftpj,1,why doesn t my complex model look like is overfitted can anybody tell me why my model does not overfit i want to intentionally overfit it but the training validation and test set all seem fine even though my model is very complex for the dataset accuracy of the model i use sklearn on the iris dataset here is my code
qus9jr,0,speech to text google api or something else hi all would google s asr api be considered the best thing around or are there others that are equally good if not better i m particularly interested in any open source or freely available code that could be customized specifically i m looking into building an app that i can train with my own voice and then use for proofing audio recordings against pre prapared texts i m hoping to work with max msp as i m no good at code i ve seen examples of max patches than can query google s api for speech recognition and i believe there s at least one example of a max object op recognize that can also do speech to text that s as far as i ve got so far so thought i d ask here
qyo2mu,0,how to implement a permute transpose op 6 times faster than pytorch this article will introduce the techniques to optimize the permute kernel in oneflow and compare it with pytorchâ€™s permute and the native copy operation in experiment the results show that the deeply optimized permute operation is much faster and more bandwidth effective than pytorch and the bandwidth utilization is close to that of the native copy operation
r44r7q,0,combinatorial optimization for panoptic segmentation a fully differentiable approach there is recent interest in incorporating algorithms as layer in neural network in our recent work cops at neurips 21 we tackle a similar task with the following questions 1 is it possible to train a pipeline containing neural networks combinatorial optimization 2 is such pipeline extendible to real world large scale tasks 3 using 1 2 can we create a fully differentiable approach for panoptic segmentation we answer all of the above questions with yes and show benefits insights into training such hybrid pipelines contribution 1 backprop through combinatorial optimization co layers there has been much recent work in this direction but such methods were not previously applied to a large scale tasks b non optimal co solvers we extend previous work of 1 to compute better gradient estimates and obtain faster convergence contribution 2 transformation for backward pass previous works for gradient estimation through co x argmin x in c c x apply loss on x and perturb costs c by incoming loss gradient on x this is not the case for panoptic segmentation our scenario is x z argmin x in c z in d x c x and loss is applied on z here we need to perturb the costs associated with z which does not exist to remedy this problem we solve a different co problem in the backward pass to compute gradients w r t c contribution 3 we show a differentiable surrogate of panoptic quality metric tldr 1 backprop possible through co for large scale tasks even with non optimal co solver 2 to achieve 1 we smooth gradients in backward pass 3 solve another co problem in backward pass for optimizing variables not appearing in objective 4 propose a panoptic segmentation approach with fewer hyperparams better results than comparable approaches code available at 1 black box backprop
rsv2o4,0,machine learning alternative to mcmc or nested sampling for work i m regularly in the position of having to fit complex models to datasets for the most part it involves defining some kind of likelihood function merit function that uses a certain set of parameters to find the distance between a model and data currently the standard methods for this in my field are usually mcmc and their variants and nested sampling mcmcs and ns are a robust and safe way to find the global minimum maximun but in high dimensional parameter spaces they become very slow i m right now running a possibly month long ns chain just for an exploratory test in a 32 cores pc while a month long run could be ok ish for a definitive result it is not feasible for exploratory exercises i really can t do one each time i tweak a thing is there any fast efficient alternative to mcmc or nested sampling out there something that while might not be as safe as nested sampling could give me a good estimate of the best set of parameters in a significantly shorter run
rh25sh,1,iâ€™ve made a search engine with 5000 quality data science repositories to help you save time on your machine learning projects iâ€™ve been working in data science for 15 years and over the years iâ€™ve found so many awesome data science github repositories so i created a site to make it easy to explore the best ones the site has more than 5k resources for 60 languages but mostly python r c in 90 categories and it will allow you to have access to detailed stats about each repository commits number of contributors number of stars etc filter by language topic repository type and more to find the repositories that match your needs hope it helps let me know if you have any feedback on the website edit here is the link to the site gitsearcher com
rutxdj,1,is machine learning a z course on udemy enough to learn basics of machine learning hey guys i m studying machine learning i have some knowledge of regression topics i joined this udemy course recently they are offering some basic very little explanation on the theory and go straight to how to use the scikit learn class for that algorithm for the regression topics that i know about i was able to join the dots from the few lines they ve said to my previous knowledge i do not believe they spend much time on the math behind it i feel i can get that explanation from youtube video and just look for python class in documentation and find an example too in the same docs will this course be enough to learn the basics do i need to supplement with some other course to cover the math explanations behind it i don t want to give any negative impressions on the course i ve only completed a few chapters i just am a bit doubtful if its better to be supplemented by some other material i have a feeling some guy hired by the course people will answer too xd
qry8i4,0,palette image to image diffusion models website paper samples x200b x200b
raqkuf,1,i want to learn ml on a long offline plane flight any suggestions hi i have a long plane flight of about 12 hours coming up and i ve been interested in machine learning for a while i m a web developer and also familiar with python quite a bit unfortunately the airline i m flying with doesn t even offer internet on the plane this makes it hard for me to do any relevant work i m not very familiar with the ml environment i know most people are going to use google colab notebooks for running their ml code i m guessing that s not going to be an option without internet so i m planning to just download an entire udemy course on pytorch or tensorflow or something and prepare everything so i can churn through the course offline is this even a reasonable thing to try or did anyone ever try doing that how far can i get with ml code offline can someone suggest a good course where i can go through the materials offline what would i have to prepare on my laptop other than python and relevant libraries i have a m1 mac and a power outlet so i should be good doing whatever i can without internet appreciate any help tldr can i download a full udemy course on pytorch tensorflow and go through it all without any internet
rahzmf,1,how to optimize my gradient boosting model hi there ml noob here so i m sorry if it a stupid question but for a machine learning exercise i needed to train a few models to make a decision on the best model for predicting defaults of bank loans of the 5 models i have tested the gradient boosting model came out on top with an accuracy of 0 66 sensitivity of 0 68 and specificity of 0 60 no i need to optimize my model even further but am quite at a loss on how to exactly do this due to the inbalanced dataset i have decided that sensitivity be my metric of focus what is the best way of doing hyperparameter tuning to get my model working even better x200b thanks in advance for the replies and help
qxmw3r,0,monitoring defi activity with open source tools here s a very detailed post about how the cloudwall capital team uses moonstream s python api with dagster a data pipeline framework to monitor defi activity it s rare to see such clear writing in a technical blog post definitely worth the read
qq6jgo,0,virtual mlops round table i m putting together an mlops roundtable focused on peer learning ml and mlops practitioners can share how they utilize mlops to automate and scale their ml processes and learn about how other teams structure theirs it works like this we break people up into small groups of 5 7 people based on their team size and what they are working on there is absolutely no selling or pitching the focus is pure peer learning you can sign up here if you re interested let me know if you have any ideas thoughts or feedback
rovtz1,0,research looking for interesting ml papers to read for the break or the new year here is a curated list i made with video explanation short read paper and code for each of them the best ai papers of 2021 with a clear video demo short read paper and code for each of them in depth blog article the full list on github short recap video
r9io6e,0,simple questions thread please post your questions here instead of creating a new thread encourage others who create new posts for questions to post here instead thread will stay alive until next one so keep posting after the date in the title thanks to everyone for answering questions in the previous thread
rwwjal,0,a paper suggests most time series anomaly detection papers are wrong i just stumbled on this very nice paper a which will appear in aaai 22 the title seems much too modest they show that a random algorithm can achieve apparent sota results in this domain this seems to be a stunning result that casts doubt on the contribution of dozens of papers for some reason the area of time series anomaly detection seems to be the wild west of dubious papers and sloppy thinking as an aside there is a benchmark set of 250 datasets here b that can be evaluated in a way that is free of the flaw my post title reflects my understanding of the paper the authors may have a different preferred claim a towards a rigorous evaluation of time series anomaly detection b www cs ucr edu eamonn time series data 2018 ucr timeseriesanomalydatasets2021 zip
rwcikx,0,predicting future labels where the future values of only some features are known rnns time series i ve done a fair bit of work with rnns and time series data but i now realize there may be a fundamental gap in my knowledge i was reading through this and it raised some questions about the different kinds of time forecasting problems so this is my summary of how it all works please correct me if iâ€™m wrong scenario 1 you want to predict the value of some stocks into the future letâ€™s say you have k stocks and n days of data you donâ€™t have features labels stocks rather the input is each stocksâ€™ current and previous values and the output is each stocksâ€™ future values your two main options are the â€˜auto regressiveâ€™ approach where you predict the values one step at a time and feed them back in or â€˜single shotâ€™ approach where you predict all values a fixed amount of time steps into the future at the same time this all seems pretty standard and is a well understood problem scenario 2 you have 3 time series features a b c and one label y you know that y is dependent on a b and c you goal is to predict the future values of y my understanding is you canâ€™t use the auto regressive approach because you arenâ€™t predicting a b or c only y but for the single shot approach your input would have shape 4 w and your output would have shape 1 p where w is the number of warmup timesteps and p is the number of timesteps you want to predict yâ€™s value now what if you know the future values of a b and c p timesteps into the future this extra information would obviously help the prediction essentially you could allow your model to peak into the future but only for a b c however complications arise because you canâ€™t pass an input with shape 4 w p because you donâ€™t know the future values of y thatâ€™s why we are trying to predict them iâ€™ve not been able to find any papers or information on this type of problem most likely because i donâ€™t know the name of this type of problem but iâ€™ve had a couple of thoughts have the future values of features with known future values as separate features so you would have an input shape of 7 w rather than 4 w since you know the future values of all 3 features some sort of recursive approach where you first come up with some arbitrary prediction of y p time steps into the future then when training you would pass an input of shape 4 w p the past and future values of a b c and the past and predicted values of y and an output of shape of 1 p your loss would be the distance difference between your predicted future y values and the actual future y values you then replace the arbitrarily generated future y values with the ones you just generated and repeat the process not 100 sure if this would work though iâ€™d have to suss out the details scenario 3 similar to the previous scenario but instead of knowing the future values for a b and c you only know the future values for a for example let y be the temperature and a b c be rainfall humidity and cloud cover respectively letâ€™s say you use meteorological rainfall predictions as your know values into the future for a you would â€˜knowâ€™ some future feature information but not all obviously there are some issues about using another prediction as a future value of a feature but letâ€™s leave that aside for now iâ€™m curious as to what work has been done in this area my intuition says that treating a b c and y as all inputs when you know y is dependent on a b c adds an extra layer of context that would be good to incorporate into this problem
rgvkki,1,interpretation of singular value decomposition svd i was reading up on the singular value decomposition and i m trying to figure out what it signifies the math makes sense to me but i m finding it hard to interpret the decomposed matrices for example we have a 2x2 matrix a of rank 2 the svd gives us a expressed as a sum of 2 rank 1 matrices u1 v1 t sig 1 u2 v2 t sig 2 1 what do these matrices signify 2 if we view a as a linear transform is there something special about the linear transforms of the two matrices we get after decomposition 3 i m assuming in a technique like pca we leave out some of these matrices what does that signify
r0y56t,0,python library to optimize hugging face transformer for inference 0 5 ms latency 2850 infer sec we just launched a new open source python library to help in optimizing transformer model inference and prepare deployment in production itâ€™s a follow up of a proof of concept shared on reddit scripts have been converted to a python library apache 2 license to be used in any nlp project and documentation has been reworked we also added direct tensorrt support which provides another boost in performance compared to the ort trt backend it will usually provide you with 5x faster inference compared to vanilla pytorch and up to 10x in specific cases on a rtx 3090 perf analyzer reports over 2800 inferences per second throughput want to try it ðŸ‘‰ the readme includes some benchmarks on small base and large transformer architectures to give you an idea of how large the benefit can be to learn more about the whole process you can also check this article showing how this open source library can beat some commercial product from hugging face company why this python library basically most tutorials on how to deploy in production a transformer model tell you to take fastapi and put pytorch inside there are many reasons why itâ€™s a bad idea first of all the inference performance is very low on the other side of the spectrum there is nvidia demos here or there showing us how to build manually a full transformer graph operator by operator in tensorrt to get best performance from their hardware itâ€™s out of reach for many nlp practitioners and itâ€™s time consuming to debug maintain adapt to a slightly different architecture i tried plus there is a secret the very optimized model only works for specific sequence lengths and batch sizes truth is that so far and it will improve soon itâ€™s mainly for mlperf benchmark the one used to compare dl hardware marketing content and very specialized engineers the usual way to perform model optimization before deployment is to automatically convert your existing pytorch tensorflow model to some kind of graph apply some optimizations and deploy the artefact in a production ready inference server for the optimization part this project leverages both nvidia tensorrt and microsoft onnx runtime then you can choose the best optimized models benchmark is performed after optimizations for the inference server the library will generate the whole configuration for the nvidia triton inference server triton is a mature tool its api is clear its documentation covers all typical use cases etc some features may require some ml deployment knowledge but nothing complex for tensorrt itâ€™s another story the documentation is both vast and sometimes incomplete its api evolves rapidly there are many traps like in the way you setup model precision or allocate memory in the gpu ram we have not found a single oss project to take a random hugging face model and simply optimize it with tensorrt still the tool provides the best performance and we hope this library will help most nlp practitioners to benefit from it as we only target hugging face transformer models we have made the experience very simple it only requires a single command line for the whole process if tensorrt and triton are unknown to you please find below 2 slides from the recent nvidia gtc 2021 conference from slides at at the amazon presentation we learned that amazon search and amazon ads aka the ðŸ’¸ðŸ’°ðŸ¤‘ generators are also built over triton inference servers from slides at still not enough to convince you that you may benefit from them check that article from microsoft where you will learn that microsoft bing is built over nvidia tensorrt you got it if onnx runtime tensorrt and triton are the big guys tools they may also help you in your own projects letâ€™s democratize them
rwq3tb,0,normalizing flows for distributions with finit support i need to learn a map from gaussain distribution to gamma distribution with some custom parameters so for both distributions i can sample and evaluate probability density the first thing that came to my mind is using normalizing flow most approaches include log target probability density evaluation in the loss function obviously normalizing flow sometimes returns negative values and this term equals to infinity positivation functions on top of the nf break bijection properties for some regions of space if not theoretically but numerically defenetely does nf approach is inapplicable from the box for such a simple problem or i m missing something
qotk5u,0,state of ai 2021 forth edition if you read it or skimmed it what was the most important information in your opinion
r4s0wx,0,aaai 2022 paper results the aaai 2022 outcomes have been released how were these outcomes for your papers
ru7k5y,0,machine learning research hi everyone i ve compiled the trusted sources of ideation based on top tier conferences on machine learning and deep learning worldwide this repository includes datasets tasks state of the art and more repository github
qplwld,0,commercial distribution of openai jukebox songs hello what is the copyright process for music created by an artificial intelligence i have made a song using openai s jukebox and am wondering if i can commercially distribute it in streaming platforms such as spotify
r1f130,0,how deep is your knowledge as a data science trust mle i think iâ€™ve learned the applications of most major ml techniques linear regression logistic regression gda naive bayes svms ffnn lstms gmms knn kmeans and a little tiny bit on transformers however i canâ€™t say i understand all of these through and through i know when and where to apply them but would really have to study the algorithm again before doing so to make sure iâ€™m not violating any assumptions how much do other mles know and how deep is your understanding do you go as far as pulling out hidden states from lstms to feed to future states and try to completely derive new models or do you stick with the basics and tune well known models iâ€™m the first person to apply ml dl at the company i work for and iâ€™ve learned everything on my own but i still feel i know so little i really donâ€™t have anyone to compare myself to i plan on moving on to another company soon but iâ€™m scared of getting fired because iâ€™m not up to par like the other mles at said company
r6t6u1,0,to compare models is reporting the standard deviations enough or do we have to also report significance testing many papers only report the standard deviations they don t bother reporting the significance tests countless of papers if you search an example with 3k citations is there any article to show that reporting the standard deviations is sufficient thanks
rmaqvu,1,is a bootcamp the best way to learn the ml talk hello i searched and did not find this scenario so was hoping to canvass some views or experiences i already have 2 years of decent fortune 50 multiple deployed projects paid professional ml under my belt not because i sought it out but because i ve been the go to guy for other oddball problems and got chucked into it as it turns out i like ml data and trying to make models predictive i ve been quite lucky to have oceans of data to play in and relatively easy softball problems to try to solve but i am self taught google taught udemy taught at the moment i have major gaps in my knowledge and i know it i know it every time i talk to other ml teams and they throw the 5 dollar words around i m about to be handed a vp of data science role and will be expected to talk the talk about ml far more than i do now potentially to far more talented folks than i this is very much a peter principle situation and i d like to stay ahead of it rather than become an incompetent potato with pointy hair i have been considering a boot camp to level up my ability to formally discuss ml concepts implementation isn t the primary goal but i do love tinkering with this stuff and would benefit from some new hands on techniques also mainly though i need to learn the lingo as others talk it and i need a broad and shallow set of talk rather than a specialization is a bootcamp a good path for this i am in the sf bay area there are dozens of them or is there some better path i should take pausing to sit a degree program won t match my time load appreciate any thoughts on the matter
rkmiay,1,do i need any background deegre for ml hi that s the question i m learning python and going to do a course of ml from standford university i did 3 years of a biochemistry degree but changed my career to computer science this year so is it possible to learn and get a job of ml without a degree
rm4xps,1,best ml and ai courses on yt using python tensorflow and google collabratory hi i am a beginner in ml and i want a good course which uses python tensorflow and google collab i hv been searching for a course like this for a long time now and i really want someone to help me and link me the best course that is interactive and fun as well pls help
r6t3fo,0,ml for audio and signal processing conferences and journals there is a lot of discussion about conferences and venues for core ml dl research but very little discussion about where to publish for ml in audio and signal processing can you advise some good venues for the same i know icassp and interspeech but what about the others i know a lot of work is still submitted to journals in signal processing due to the ee influence so how about ieee transactions there are so many of them
r29dks,0,interactive compute platform recommendations for ml research i m looking to switch to a small laptop while i travel that won t have much capacity for running experiments i d love to hear opinions of people who use interactive compute platforms specifically i m looking for some sort of vm i can ssh into and use like my computer except i also want something that is easy to hook up with gpus tpus with ideally an on demand payment scheme i think google microsoft and amazon all have some version of this but there is so much to them it s hard to tell which one i should go with some key factors i m looking for are ease of use value per dollar and the ability to easily upscale downscale the amount of compute i m using having the ability to open notebooks via the web would also be nice
qkh9jg,0,model performance monitoring in production we ve recently introduced model performance metrics in graphsignal basically by just logging a label and prediction the model specific metrics are automatically computed visualized and can be monitored graphsignal is currently saas so a free account is necessary no raw data is sent only statistics more details in the blog post monitoring model performance in production and the logger repo is i hope it can be useful for those who need to monitor models in production and do not want to build own pipelines for continuously computing accuracy and other metrics implementing alerting etc
r07nms,0,inference server for gpt j 6b on huggingface with optimizations to fit into g4dn xlarge hi i created an inference server for gpt j 6b on huggingface using fastapi the main challenge here was making sure that the model fits in the smallest g4dn instance on aws with 16gb of ram and vram to save on the server costs check it out and hope you find it helpful here is also a write up that explains what optimizations were applied
rvo19z,1,where do i start for generating new images iâ€™m reading about gan dcgan msggan bmsggan and my head is spinning i have a set of 500 1000 black and white photographs iâ€™ve taken and i want to use it as a dataset i just want the output to be similar looking but entirely new images itâ€™s only an art project and i have low level coding abilities it seems like image size is a significant constraint and iâ€™m fine to begin by resizing my data set and then supersampling later they are still rectangular though which seemed to be a problem with some i was looking at so where do i start whatâ€™s the right tool for this iâ€™d appreciate any help and guidance edit i donâ€™t want to use anything
qqssgq,0,new open source vector search solution meet our new open source vector search solution vektonn we offer an opportunity for product teams and data scientists to solve the problem of reliable vector data storage scalability and undisturbed availability we store embeddings and their attributes which are more interesting to users since they can use real world objects for example they can identify objects using their real identification we support changing indexes as new data arrives delete change or add data to the index parallel with search queries you can expand multiple indexes over a single data source vectors and attributes and seamlessly transition to new versions of indexes you can expand different indexes with different parameters of the same data you can work with vectors of any type for example you can use bag of words to solve word processing problems and load appropriate sparse vectors into vektonn we d appreciate any feedback or suggestions for the project and welcome github stars to join in if of course you find it interesting ðŸ™‚ learn more see what we have done
qp9mnn,0,a unified view of relational deep learning for polypharmacy side effect combination synergy and drug drug interaction prediction x200b git paper abstract in recent years numerous machine learning models which attempt to solve polypharmacy side effect identification drug drug interaction prediction and combination therapy design tasks have been proposed here we present a unified theoretical view of relational machine learning models which can address these tasks we provide fundamental definitions compare existing model architectures and discuss performance metrics datasets and evaluation protocols in addition we emphasize possible high impact applications and important future research directions in this domain the paper provides a unified model of drug pair scoring models with a general architecture design recipe model design comparisons based on architecture and input modalities evaluation metrics used by the most important papers public datasets that are relevant evaluation regime designs for stratified splits the github repo comes with paper links with implementations links to the datasets
rchvfb,1,what s the point of hardswish a while ago swish got some attention as it outperformed relu in quite some tasks as far as i understood it it s main benefits are that it keeps the nice properties of relu while being differentiable everywhere and the fact that it s a bit like leaky relu for negative values so what i took away from that is that it s a fully differentiable leaky relu i have been using it here and there and it always worked great it wasn t implemented in pytorch so i always defined it myself easy enough today i found out that torch 1 10 has hardswish which has very similar values to swish but is a composition of 3 functions and is much faster to calculate but as far as i understand it it isn t continuous in the points where it switches from one functions to another taking away one of the big benefits that swish had so we basically give up a fully differential function to save some compute my questions are am i understanding all of the above correctly or are there some errors or misconceptions in my thinking and as in the title is it worth it why and when use hardswish over swish does that bit of more compute actually make a difference
ru86ji,1,time series classification hi everyone i like to take pictures of freight trains and in my area the railroad broadcasts some simple data over unencrypted radio that can show some very basic info about what s going on you don t get the exact position of a train just confirmation that there is a train on a certain piece of track which can be 30ft or 30 miles long and some info about what the track signals are showing long story short i am able to collect this data and i have written programs to identify when a train passed a given part of the track below is an image of this data plotted over distance and time i have manually highlighted what i would like to be able to do with ml which is to classify a given train detection at a location as belonging to a given train trains moving up and to the right are eastbound and trains moving down to the right are westbound trains almost exclusively make their journey all the way from one end to the other it is very common that an eastbound will stop at a siding to let one or more westbounds pass fairly rarely a train will start out one direction turn around and come back the other way complication some radio data is missed depending on various conditions so i can t guarantee that i will pick up every train at every control point but i can pick up enough data to fill in the gaps at least with my own brain i am a half software half electrical engineer and i have taken a couple beginner courses on ml but i m not really sure what type of classifier i should use for this or how i should organize the data i m also not 100 sure that this is a task for ml but it seems natural that it would be especially considering that sometimes missing data has to be inferred i would be very appreciative if you all would be willing to help me out screenshot 20220102 062841 2 png
rgmdwn,1,questioning whether or not i should use a bag of words in my portfolio project i am at odds with whether or not i should use text as a feature in my classification project the problem isn t inherently an nlp one like sentiment analysis but more like a traditional classification problem that uses census and urban data neighbourhood profiles to make predictions on neighbourhoods that being said i have a text column in my dataframe that i can t help but think would make for a good feature if converted into a bag of words i m not versed in nlp but a bag of words with pca makes sense to me i am worried that this might complicate things though and send me down some unnecessary rabbit holes trying to learn about nlp when i might be able to make due without it
rva0ey,1,autonomous driving hi guys do you have some advanced books or material about autonomous driving perception and decision making for this field thanks
r53277,0,cost of distributed deep learning on aws hi everyone i am trying to learn some distributed deep learning techniques such as parameter servers and ring all reduce i found this paper by uber where they describe their implementation of ring all reduce while the implementation looks fairy straight forward i am unsure about the costs the problem is that i don t have access to multiple gpus so i thought i would use aws ec2 since i have never used it before i have used gcp a fair amount but not for distributed deep learning i was hoping someone here could shine some light on the costs of using say 2 gpus my initial idea was to train a cnn using imagenet 1k so i would use s3 and ec2 with two basic gpus as my goal is simply to see how much faster i can reach convergence rather than achieving state of the art times would this be too expensive for a personal project i d rather not spend over 200 gbp
rhdofe,0,audio speech â€œharmonizationâ€ task is there an equivalent of image harmonization compositing task for audio speech not music data say we want to combine clean speech data and natural scene background so that the speaker naturally sounds like they are in that background scene generally what iâ€™m asking is how do we make a clean speech data more â€œnaturalâ€ given a natural sound data as a reference iâ€™m not familiar with sound design or sfx either so please let me know if this particular task is achievable with non neural approach
rd2l00,0,1st ever method to perform gpu quantization on most ðŸ¤— hf transformer models 2x faster inference quantization is a technique to significantly accelerate inference by replacing high precision tensors by lower precision representation in a way where accuracy is kept intact or close to itâ€™s quite common in cpu inference a lot less on gpu even if the performance boost is significant end to end method we also added many other benchmarks the library repo apache 2 licence to give you an idea of the latency speed up x200b roberta base classification mnli latency benchmark batch 32 seq len 256 afaik 3 methods of gpu quantization exist for the 2018 vanilla bert architecture 2 from nvidia and 1 from microsoft but none exist for any other architecture roberta electra distillbert deberta etc limiting the benefit of gpu quantization to old models we hope that this project will help generalisation of quantization in nlp and ease nlp big models deployment itâ€™s a big deal as quantization is rarely used on gpu unlike cpu because it requires some nvidia tools not well known from most ml practitioners like tensorrt in the lib we have wrapped those tools so they are transparent to the final user the result is a model that is always several times faster than vanilla pytorch on gpu on any batch size seq length for any transformer flavour small base large xx large etc this work is based on a very recent model qdqbert 2018 vanilla bert which supports quantization added by nvidia to the hugging face transformer library a few weeks ago next to the generic method we have developed we have also implemented in the lib a new model as a proof of concept qdqroberta and no roberta is not identical to bert of course the idea is to extend the process to all transformer architectures there are not that many both approaches offer different trade offs in generalisation and accuracy details in the notebook there are many things which are still to be experimented for instance for qdqroberta and other future qdq models is roberta source code almost trivial modification a good idea should we just patch onnx files like microsoft does or build from scratch the graph directly in tensorrt like nvidia does or leverage the new pytorch fx interface all approaches should in theory lead to the same result accuracy speed but offer different ease of use final user vs ease of maintainability library maintainer trade offs gpu quantization is not very discussed known in nlp so please donâ€™t hesitate to comment ask questions below so we can improve the tuto and democratize gpu quantization
reys09,1,my validation accuracy for siamese networks is not improving and is stuck at 0 5 i have posted this in another subreddit because i forgot this also exists i was implementing the network to learn about yoga poses i have so far crested a dataset of 6000 images the positive and negative pairs are all arranged alternatively the positive and negative pairs are divided equally in both the training and validation datasets the training accuracy is improving to 0 90 in just a few epochs but the val accuracy is stuck at around 0 5 can anyone tell me what to do i am doing this for a week but cannot get over this please answer my question i have been stuck for a week now
rja4ma,0,project introducing fastshap for quick kernel model explanations the purpose of fastshap is to make the calculation of shap values as lightweight and fast as possible this is accomplished by two batching routines which keeps the process inside vectorized operations as often as it can there is also some intelligent numpy slicing involved info on how this works and how you can determine optimal batch sizes is on the github on the boston dataset using all 506 rows as a background set this runs in about 26 seconds the original shap package kernelexplainer takes about 11 minutes this difference grows more pronounced with larger datasets a few notes 1 this package was really designed to be used on tabular data as of now features can t be grouped i e no superpixels this feature will be added in the near future though 2 only a kernel explainer is implemented which calculates shap values for any arbitrary model model specific methods i e treeexplainer are still much faster 3 can automatically handle pandas dataframes or numpy arrays 4 background dataset stratifying methods are available 5 can only calculate shap values for 1 dimensional outputs as of now n dimensions coming soon 6 the linear model from which we get the shap values from the coefficients can be any of the models from the sklearn linear model module this is available now on pypi coming soon to conda forge
qm0upg,0,extended submission deadline â€” evomusart 2022 conference good news the submission deadline of evomusart 2022 has been extended to november 24th ðŸ™Œ you still have time to submit your work to the 11th international conference on artificial intelligence in music sound art and design evomusart if you work with artificial intelligence techniques applied to visual art music sound synthesis architecture video poetry design or other creative tasks don t miss the opportunity to submit your work to evomusart evomusart 2022 will be held in seville spain between 20 and 22 april 2022 ðŸ’ƒðŸ‡ªðŸ‡¸ for more information visit the conference webpage evostar org 2022 evomusart
r8r78p,1,alternative for isolation forest there is requirement for finding the anomalies in detail we have a two files one from one cloud platform and other from aso we need to find the anomalies for communication between the tenants access to vm sorry to give very vague information i was asked to just give names of algorithm but the currently used algorithm is isolation forest just wanted to know is there any alternative or such or even any other similar algo for finding anomalies for a comparative study
qmm6uh,0,ethical concerns for ml to predict race gender iâ€™m working on a data product that primarily uses image and name classifiers to identify race and gender this means that someone who buys this product is now able to see race and gender data associated with people and or companies in their database the use case behind this is to report on and make decisions to improve diversity i e an investment firm seeking to invest more in underrepresented groups an hr company reporting on industry trends iâ€™m looking for feedback on ethical design quality concerns in regards to some of the following factors we are primarily leveraging publicly available training data sets models and classifiers gender classification includes only male female options we use a publicly available photo for classifying each person none of the data we provide is self reported nor does the product communicate that to the customer we do not yet provide a confidence score or any feedback or correction feature race and gender sex are legally protected classes in some cases while we are not using these to make any decisions in our product we are the ones generating this data which our customers will use at their discretion iâ€™m worried we are not doing enough due diligence for the intentional choices weâ€™re making and the unintended impact they may have any resources for designing fair systems especially ones that attempt to generate rather than consume this type of data would be appreciated
rd6vlv,1,best way for unet to generate masks for different image sizes i am dealing with a dataset that has different size images and so i also have to predict masks of different sizes i would like to have an automatic way of dealing with it that doesn t distort the prediction now i downscale all images to the smallest resolution which appears the most in the dataset the problem is that during backtransformation to bigger resolutions the mask itself gets distorted because there are bigger images and also the bigger ones are mostly not squared the problem now is that my model uses the keras input layer which doesn t allow dynamic inputs i think also the same for the output i hope that anyone can help me
rs65ei,0,adapting class activation maps for object detection and semantic segmentation hi r machinelearning is a project that has a comprehensive collection of pixel attribution methods for pytorch like the package name grad cam that was the original algorithm implemented class activation maps can help diagnose properties about the model predictions like where does the model see a cat in the image after many requests i added support for object detection and semantic segmentation and wanted to share this with you here you can find detailed notebook tutorials about this a tutorial on using class activation maps for object detection a tutorial on using class activation maps for semantic segmentation x200b computing the cam for object detection computing the cam for semantic segmentation the problem class activation maps are usually researched and applied for classification models a repeating request in this repository and also in some object detection projects was to add support for grad cam for object detection one challenge with this is that object detection frameworks typically don t output tensors you can back propagate through to compute gradients they typically output dictionaries with bounding boxes labels etc after a lot of processing and don t expose any way to compute gradients with respect to those detections if you want to compute cams for them you typically have to dive into the code of these object detection packages and create solutions that work only with them there was no generic tool that just works and can be adapted to new object detection models the solution gradient free methods some class activation map methods don t depend on computing the gradients examples of these eigencam computes pca on the activations and returns the first principle component it s very fast since it requires a single forward pass but it doesn t have good enough class discrimination in case you might have several different objects in the same bounding box ablationcam ablates individual activations and measures how the output score drops this is a sota method with class discrimination but it s much slower since it requires many forward passes for doing the ablations object detection networks are already heavy and ablating these activations makes it slower in practice many of the activations don t contain useful information at all and if we can identify those we can just skip ablating them i tried coming up a heuristic that computes a binary mask for where the objects might be based on comparing eigencam with a low threshold and then scoring the activations according to how much of their pixels values fall inside the mask then we can control the ratio of activations we actually want to ablate where a lower ratio makes it faster this seems to give good results with dramatic run time reductions we can use these methods applied on the activations from the feature pyramid network in object detection networks to adapt them for object detection custom cam target functions for object detection segmentation a cam is computed to target some property about the image like what parts of the image is important for the dog category we can adapt this for object detection by targeting properties like what parts of the image are important to get a high iou with the original bounding box detections and score high on the same categories you can similarly adapt this for segmentation by asking questions like what pixels in the image are important for predicting the car pixels x200b i hope you find this useful and that this will be a good starting point towards applying cam methods more in practice and in production for monitoring and diagnostics of vision models
quvv7h,0,a deep generative model enables automated structure elucidation of novel psychoactive substances over the past decade the illicit drug market has been reshaped by the proliferation of clandestinely produced designer drugs these agents referred to as new psychoactive substances npss are designed to mimic the physiological actions of better known drugs of abuse while skirting drug control laws the public health burden of nps abuse obliges toxicological police and customs laboratories to screen for them in law enforcement seizures and biological samples however the identification of emerging npss is challenging due to the chemical diversity of these substances and the fleeting nature of their appearance on the illicit market here we present darknps a deep learning enabled approach to automatically elucidate the structures of unidentified designer drugs using only mass spectrometric data our method employs a deep generative model to learn a statistical probability distribution over unobserved structures which we term the structural prior we show that the structural prior allows darknps to elucidate the exact chemical structure of an unidentified nps with an accuracy of 51 and a top 10 accuracy of 86 our generative approach has the potential to enable de novo structure elucidation for other types of small molecules that are routinely analyzed by mass spectrometry check out our new paper on automatic structure elucidation of new â€œlegal highsâ€ using artificial intelligence and mass spectrometry published at arxiv
r17z2y,0,deepmind google brain world chess champion explore how alphazero learns chess knowledge deepmind and google brain researchers and former world chess champion vladimir kramnik explore how human knowledge is acquired and how chess concepts are represented in the alphazero neural network via concept probing behavioural analysis and an examination of its activations here is a quick read deepmind google brain world chess champion explore how alphazero learns chess knowledge the paper acquisition of chess knowledge in alphazero is on arxiv
rjyll8,1,here s a list of 8000 programming resources i hope this helps list you this is a list of resources for python machine learning web design etc list write your feedback in the comments
qx6giu,0,does faang interviewing research interns for summer 22 already i have applied to many big tech companies for phd research intern positions mostly focusing on machine learning and deep learning i recently got several offers from non faang companies my initial goal is to get an internship position at faang but i have not received many opportunities to get interviewed by them i am having an interview with amazon and will have an interview with facebook soon that is all for faang i am curious when is the usual hiring interview period for intern students perhaps i am not attractive to those big techs and that is why i have not got many interviews from them or i just wonder if they are gathering applications and start interviewing applicants from december because i got an offer from some companies i need to decide soon whether i will accept it or not but because i don t have any idea if the hiring process at faang is almost finished or there will be more opportunities it is hard to decide 1 if you have any experience in the hiring process of research intern positions at faang could you let me know rough dates of when you had interviews and when you got offers from them 2 also because this is my first internship during my phd i don t know what to consider when choosing a position all positions are research intern positions and the topic of projects roughly aligned with my interests and most of the time i was told that the specific topic will be decided when i start internship by discussing with my mentor i hope i could get a position that has a high possibility that i could publish a paper but it is true that there are so many uncertainties could you give me any advice for selecting an intern position when you have multiple options any advice regarding internship will be very helpful thank you inadvance
r76jtx,1,bayes probability network with pomegranate package does anybody familiar could explain me what model predict proba method actually doing i suppose it calculates the conditional probabilities when we have particular input based on input data but i am not sure here i made the i hope good learning example for my own learning and i also implemented predict method which gives integer outcome based on probability i hope i understood this well star example if you like it and if anybody has some idea how to improve it i would be grateful
rlsa2y,0,discussion hot reloading headless deep learning rig hey folks not sure if this has been asked before i am plenty experienced with ml scripting but am looking for opinions on setting up a hot reloading external headless rig to offload some rl training my employer uses a kubernetes based ml platform which is really neat for distributed big data jobs via daskâ€¦ hot reloading a heavy duty argo wf via tilt across up to 1 600 nodes each with a gpu and 32gb memoryâ€¦ i want to dig into some hardcore deep rl and want to set up a personal development environment that is compatible with remotely hot reloading from my macbook so like upon saving any changes to my local scripts they are updated and try to run on a headless linux rig whilst logs are returned and printed any folks got recommendations on some open source ml development toolkits that are ideal for this
rulr8h,1,model consistently under forecasting i am working with time series data and trying to do forecasting for the next 90 days the model is an ensemble of prophet and arima the model does a pretty good job in terms of accuracy but it is almost always under forecasting consistently what suggestions would you have to fix that client is expecting some sort of ups and downs between actual and forecast
qqhct3,0,interesting bit of info from htc keynote nvidia selene 500 node superpod trained gpt3 in 11 days i m not sure the cost to run such a machine but i d imagine that represents a big cost and time reduction vs 1 year ago
qxlsmm,0,imperial college london researchers propose a novel randomly connected neural network for self supervised monocular depth estimation in computer vision depth estimation is one of the fundamental problems in computer vision and itâ€™s essential for a wide range of applications such as robotic vision or surgical navigation various deep learning based approaches have been developed to provide end to end solutions for depth and disparity estimation in recent times one such method is self supervised monocular depth estimation monocular depth estimation is the process of determining scene depth from a single image for disparity estimation the bulk of these models use a u net based design although relative depth is perceived very easily by humans the same task for a machine has proven quite challenging due to the absence of an optimal architecture to tackle this issue more complex architectures are chosen to generate a high resolution photometric output the hamlyn centreâ€™s research team from imperial college london introduces a unique randomly connected encoder decoder architecture for self supervised monocular depth estimation the model architectural design capable of extracting high order features from a single image and the loss function for imposing a solid feature distribution is credited for the ideaâ€™s success quick 5 min read paper imperial blog
reosda,1,can i use any python library instead of matlab octave my course this semester is numerical computing and the professor told us that he would prefer using matlab can i use any python library instead like numpy or something
rab6lt,0,pytorch distributed training libraries what are the current options currently when i do distributed training i either use some manual implementation with torch distributed or just use pytorch lightning which also has some nice bonuses like fp16 training then there s also deepspeed however i m unsure if deepspeed is only beneficial for multi node training and when my model does not fit into gpu ram or if deepspeed would also bring benefits for standard data parallel multi gpu but single node training where the model would fit into gpu ram do any of the practitioners here have insights into this which other libraries frameworks am i missing
rofn2t,1,looking for some direction on a project that i d like to do for my capstone project i will be doing a capstone project this semester and i would like to do a project with neural representations i am interested in the differential geometry side of math and recently i have been learning more about how manifolds can be used in ml for representations i m going to do a project using manifolds and time series data for the project i will be using time series data like audio signals or eeg meg brain signals i am going to see if it s possible to do the project with a professor who would be able to help me through this but as far as i know i haven t seen any professors at my school that specialize in neural representations using manifolds so the initial guidance for how to get started with something like this would be extremely helpful an example of what i m talking about is this paper or this video on neural circuits constructing representations i d like to do something like this and i don t really know where to start i can learn quite a bit during the semester but i would like to get the basics down and have a direction for what kind of machine learning i ll need to do before the semester starts at least my project doesn t have to be as complex as this paper but something at least in this direction is what i want to do would anyone be willing to provide some guidance on how to work towards building a project like this this semester any advice on how to start learning about this area or any resources that would give me a framework to start with for this kind of ml any help would be greatly appreciated
r1pf4c,1,image is tinted blue even when opened afresh x200b original image displayed image hello so i have some code that pre processes some images for me however the image is tinted blue by the end of the process for some reason i don t understand og image imread test image1 png cv2 imshow og image even when i use imread and imshow to display the original image again towards the end of that file it still looks blue for some reason i understand if it turns blue somewhere along the way but why does it do that even when i read and display the original image afresh if anybody knows any common mistakes that lead to this please do tell me the libraries i ve used are skimage numpy and cv2 couldn t find anything useful on stackoverflow or anywhere else sorry i can t post my entire code here for obvious reasons but i ll try my best to share as much as i can if it ll help understand the problem i m kinda new to this so any help is greatly appreciated
rul8l2,1,padding vector for variable sequence of vector multivariate lstm regression task i m aware in regular nlp that you would use a padding token for a regular seq2seq mode what would i use for padding if instead of a sequence of tokens i have a variable sequence of continuous vectors
re2787,1,personal ml models hey i m looking for the best practices in training targeted personal models let s say i have 1000 customers using my product i plan on training 1000 individual models to make the best possible recommendation for each customer individually how should i go about training them to get the best possible outcome should i first train with the data of 999 customers and then retrain on the target customer with a lower learning rate or freeze all but the last couple of layers or any other tricks i m working on a time series model for predicting where the user wants to go next on a web page i have a medium amount of data 50 000 events for each customer on the order of page visits and the length of time since the previous action a quick sample below 1 customerid pagename seconds 2 123 dashboard 0 3 123 detailspage1 5 9 4 123 detailspage2 3 1 5 123 action5 21 2 any learning material best practices or just keywords to help in my search are highly appreciated thanks yall
r2fjja,1,cheese cake doge x200b i just implement optimization based clip styletransfer open it in replicate github
r70j6e,0,questions about possible self plagiarism in this neurips paper neurips 2021 is around the corner and i came across the only scene text related work titled centripetaltext an efficient text instance representation for scene text detection paper a and it is an interesting paper the authors claimed that they introduced a new text instance representation in scene images based on text kernels central text regions and centripetal shifts upon reading their paper i found out that the main idea and the proposed method are closely related similar to another paper bidirectional regression for arbitrary shaped text detection paper b of the same original authors strangely paper b is published before paper a but it is not mentioned at all in paper a and i think paper a has a high similarity to paper b timeline of both papers paper b was first submitted and accepted to icdar 2021 submission deadline 8th feb camera ready deadline 17th may paper a was then submitted and accepted to neurips 2021 submission deadline 28th may camera ready deadline 26th oct questions i have 1 is it common that the same contributions text representation and proposed method have been introduced twice in two independent papers and submitted to two separate conferences 2 should the authors cite their latest previous work and explain their similarities differences in this case here the authors have already known that the icdar paper is accepted right before their submission to neurips 3 is this considered a case of self plagiarism and against the rules of publication to conferences especially dual submissions
rw5sqt,1,question about kernel in svm i was just wondering what would you say is the most important hyperparameter that comes naturally with picking any kernel i m sorry if the question seems ambiguous i m just repeating word for word what another person asked me and i m not sure what to answer
r5bkph,0,how important is initialization could you share papers that discuss the topic of initialization from a theoretical point of view it is my feeling that this topic is usually overlooked and i don t know if justifiably so or not from my anecdotal experience initialization is crucial moreover for very simplistic experiments small 3 layer nn it is not hard to come up with a non trivial initialization that won t allow the loss to decrease if the function to be learn by the nn is the identity function one example of the kind of papers i am thinking of is this one on lazy training in differentiable programming i am asking here because maybe someone knows of wants to share a paper they find interesting
r88p0z,0,k means producing very different centroids even after initialization with canopies doing some big data coursework for my ds masters and i am pretty confused by what i m seeing i m running k means clustering on mahout and understand you can generally get different final centroids in k means because it initializes them randomly but i ve initialized clusters using canopy clustering and even after that am seeing wide variance in the final density values here s one example x200b run inter cluster density intra cluster density 1 0 430836339 0 590516787 2 0 618821774170647 0 581433137 3 0 4960634 0 567159394 i ve run this for different k values distance metrics and max iteration values the behavior is similar this is problematic because the purpose of this project is to compare performance across different parameter values with such divergent results within the same parameter set i m forced to run each set of parameters multiple times and take an average which is drastically increasing processing time and computation cost is this expected behavior did i misunderstand something
rh1jwm,0,ai generated pokemon made using a finetuned rudall e model this was a test on my end to see what would happen if i tried to finetune rudall e on a specific domain with a relatively low sample size 900 images the results are much better than expected the ideal goal was to see if the finetuned model could preserve rudall e s ability to write prompts unfortunately it didn t it s a tough balance
rr95gq,0,semantic search with finetune in realtime using clip here is a personal project i made just after the release of clip almost an year ago i provides a web front end to search images within a local folder it supports search by text or images just drag and drop i think that the most interesting feature is the more like this button you can finetune your input query in real time it still lacks some functions like save the results or export the finetuned embedding but i m planning to add these features in the next weeks any other ideas comments are welcome
r6hwc4,1,prerequisites to cs229 elements of statistical learning hi i want to do the cs229 course and read the elements of statistical learning but i m not sure if i have the prerequisites to them i have a quite advanced level in calculus and linear algebra and i understand some probability theory is that enough or should i learn something else statistics if so what are the best books resources available
rdg7fs,1,i need some help with correct data type for model predict sample code in post detail hey everyone i m reading a book about techniques for natural language processing it has a github page for a simple implement of bidirectional lstm for sentiment analysis however since i m still new i can t for the life of me figure out what s the correct data format to use model predict they only reach model evaluate in the sample code i tried using the same training data as prediction and i always get this error x200b i know it has something to do with the shape of data for model predict but i just don t know what can anyone look through it and help me a bit with what data format i should use
qtzbu1,0,machine learning wayr what are you reading week 125 this is a place to share machine learning research papers journals and articles that you re reading this week if it relates to what you re researching by all means elaborate and give us your insight otherwise it could just be an interesting paper you ve read please try to provide some insight from your understanding and please don t post things which are present in wiki preferably you should link the arxiv page not the pdf you can easily access the pdf from the summary page but not the other way around or any other pertinent links previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 121 130 week 1 11 21 31 41 51 61 71 81 91 101 111 121 week 2 12 22 32 42 52 62 72 82 92 102 112 122 week 3 13 23 33 43 53 63 73 83 93 103 113 123 week 4 14 24 34 44 54 64 74 84 94 104 114 124 week 5 15 25 35 45 55 65 75 85 95 105 115 week 6 16 26 36 46 56 66 76 86 96 106 116 week 7 17 27 37 47 57 67 77 87 97 107 117 week 8 18 28 38 48 58 68 78 88 98 108 118 week 9 19 29 39 49 59 69 79 89 99 109 119 week 10 20 30 40 50 60 70 80 90 100 110 120 most upvoted papers two weeks ago u catalyzex code bot paper link besides that there are no rules have fun
rvw0rv,0,launching dagshub 2 0 â€“ git integrated data labeling and smart ml discussions tl dr â€“ dagshub is integrated with label studio and you can now open datasets from git and dvc remotes label them and commit labels back without doing any devops you can also comment on labels bounding boxes or any file check out the example project or try out the tutorial comparing annotations hi r ml i m one of the creators of dagshub we help ml practitioners create a central repository for their projects where they can leverage open source tools to version datasets and models track experiments and starting today â€“ label data and comment on anything like github for machine learning you probably heard that before but we mean it our vision is that anyone could jump into an open source data science project and contribute code data labeling models experiments via pull request just like you would with an open source software project we take awesome popular open source tools and connect them to the place you build data science projects lowering the barrier by doing the devops work for you and creating a coherent workflow that makes sense for production oriented teams and our open source community how does it work 1 version your code with git notebooks also work 2 version your data models and pipeline with dvc 3 git push to your dagshub repo 4 dvc push to your free dvc compatible dagshub storage or to your preferred cloud storage which we support 5 you get a zero config access controlled mlflow api endpoint to log experiments we saw that labeling was a big challenge for many community projects as well as teams in the industry we spoke with we wanted to make our communityâ€™s life a bit simpler and since many of the challenges were around integrating labeling into the rest of the ml lifecycle thatâ€™s exactly what we built with dagshub annotations and the label studio integration the main additions are 1 zero setup labeling â€“ we did all the work so after you push data to dagshub just click a button to fire up a labeling instance 2 sync data from your git dvc remote â€“ after itâ€™s up you can select what data you want to label and it will be waiting in your workspace for you this is something we know many of you have wanted for a while 3 commit back to git â€“ when youâ€™re done with a version of labels we created a â€œcommitâ€ button that just adds the annotations to git making it easy to pull them for model training or any other need you have while preserving the project history 4 diff and discuss annotations â€“ you can see annotations on the data itself check out the example project below and have discussions on them in a way that preserves context even comment on bounding boxes on images committing annotations back to git we created an example project that you can play with to see how it actually looks and of course i d love to hear your thoughts and feedback and answer any questions you might have thank you if you want to read more about the launch check out the launch blog
royf9n,1,bit confused for grocery purchase recommendation approach so i got an assignment to make a grocery purchase recommendation system to maximize sales by recommending items that were popular sellers with those already in cart after a bit research i found that my problem lies in market basket analysis and apriori algorithm was the top recommendation but after reading this paper by amazon about item item collaborative filtering it occurred to me that apriori approach would not be able to suggest items item pairings which are not above its support level i can t keep the support too low as it would then go against apriori s key goal of keeping search terms less to process faster so should i go with a approach which makes a list for each item and its popular pairings to ensure that my code will at least recommend a related sale of the item to the customer or is there a faster and more time and space efficient implementation for the same problem any insight related would be appreciated
rcix6r,1,community databricks no longer available how can i access community edition every time i try and sign up i get taken towards the paid version
rboh2r,0,are transformers overhyped i ve been wondering about this for a pretty long time since i ve never seen anybody say anything bad about transformers while to me they seemed pretty flawed from the moment i ve read the paper i m in no way an ml expert i m only an aspiring phd student who s not even specializing in nlp so if i m any way wrong i d really like to hear it tl dr i believe that transformers are in the long term a pretty small contribution to the world of nlp and may even be damaging due to shifting the focus of the research community in the wrong direction why they don t address the long term dependency problem before transformers nlp used to be dominated by rnns and specifically the encoder decoder architecture in the case of translation the encoder would encode the input sentence in a fixed length vector and the decoder would then decode this vector into an output translated sentence now transformers also use encoder decoder architecture but there is one big difference for rnns encoding and decoding actually happens at every step of the way words i know it s tokens but i ll call them words are inserted sequentially into the rnn for every single word the encoder rnn had to look at the current encoding vector and the input word and then choose how to update the encoding vector in a meaningful way the problem with this approach which i ll call long term dependency arises when the rnn has to look at a very long sequence of words humans can easily distill the information that they ve read and remember only the important bits for example the name of a character that was mentioned 5 pages ago but rnn models had trouble encoding what happened 5 sentences ago the research community starts solving this problem with the original attention paper but then transformers come out so out comes the transformer and starts dominating the nlp world what does the transformer do it is a huge model that when encoding simply takes 512 input words or some other arbitrary number and looks at all of them simultaneously and it works wonders look the transformer can remember what happened 5 sentences ago because the previous 5 sentences combined have less than 512 words hooray can it remember what happened 10 sentences ago though uh well no can we improve it in some way to solve the long term dependency problem well we can be smart about which sentences we feed into it but that means we still have to distill information from a large body of text so we re back at the beginning it s obvious that we have to solve the long term dependency problem if we ever hope to achieve human like nlp models and to me it seems that transformers do nothing to solve this problem so why are they dominating the field of nlp research maybe the optimal solution will include a combination of both the transformer and some other model for information distillation but if we still need to solve the long term dependency problem why are throwing out rnns so quickly
razw0l,1,requesting help i am a beginner in ml i am working on a tuberculosis tb prediction project i am doing binary classification using tensorflow i am using chest radiography images as the dataset for cnn there are 3500 normal chest x ray cxr images and 700 tb infected images do i need to perform data sampling or can i find better dataset which optimizers activation function how many convolutional layers and dense layers to use should i use data augmentation i am looking for suggestions
qvueo6,0,is microsoft cmt down for cvpr 2022 good god i m dying wtf i cannot access to the cmt official page for cvpr 2022 anyone else
rnpl4x,1,object detection using rcnn hello guys i have to do object detection without using any pre trained models it is very difficult to do it as i don t understand neural architecture as rcnn requires a pre trained cnn so i trained a cnn network in cifar 10 but i don t understand how to plug it in rcnn architecture so guys can somebody guide me with explanation or a link for reference thanks a lot for reading and helping
qk7tuv,0,thoughts on pathways by google research i recently found out about this proposal called pathways by jeff dean but the article seemed very obscure there were just ideas and not a single hint of how that would be solved whenever a question was posed the word pathways was thrown at it is it another huge transformer from google just wanted to know what everyone here thinks about it
rm1da7,1,how much should mlops engineers know about inner workings of algorithms how much should mlops engineers know about the inner workings of machine learning algorithms think of the job of mlops engineers as taking models from data scientists and deploying them as production assets integrated within business operations let us know your thoughts in the comments ðŸ‘‡ðŸ» machinelearning datascience mlops view poll
qkert8,0,reusing parts of an open source code for a potential publication and a new open source code i am currently developing a new method that builds up upon an existing work in the literature in order to address the limitations and provide reasonable improvements to what has already been done earlier i reached out to the authors for possible academic collaboration but i have not received a reply from them their work has already been published as a conference paper two years ago and their code is available on github is regularly maintained and has also been deployed as pypi package that can be installed using pip my question is clearly about how to use certain parts of their work without plaigarising or breaking any copyright agreements to what extent is it acceptable to rely on other people s work for producing a new method especially when it is open source since the method i am working on is largely inspired from the existing method it seems that i am currently on track to adopt around 30 of their code and follow their general code structure and oop layout
r7vnzk,1,can i say that my model is doing nothing if rmse sd of the validation set i am training a univariate regression model using neural network the training set has sd of 0 6 when i train my model the val rmse get around 0 6 also as i understand both rmse and sd have the same unit and similar formula so if sd rmse can i say that my model is just predicting the mean value of the dataset which is basically useless and if rmse sd can i say that my model is a poor predictor
qmrglg,0,isomorphic labs just unveiled today a new alphabet company led by deepmind s demis hassabis plans to tackle drug discovery using ai even as an insider i found the idea of a deepmind offshoot pretty surprising curious what you folks think about it what are the odds it ll succeed will alphafold even be useful for drug discovery tweet unveiling the company website
r9f3nh,1,need help with extracting number of objects of different classes using yolo i was trying to extract number of objects that yolov4 detect in a video in a frame into a variable which i can further use for processing but i couldn t find any particular method to do so i think i will have to save it in a variable before printing it but couldn t find a way to do this thanks
rakeii,1,i want to do product recommendations for an online shop no idea where to start any model suggestions what data i need to collect this is for my school coursework
rfcrr3,0,how difficult easy is to learn nlp once you have experience in a cv hi i have 3 years of industry experience in different cv deep learning tasks as an ml engineer and recently started to scratch the surface of the nlp for educational purposes and to be honest it s quite interesting as far as i could notice nlp and cv share some concepts
ru5xw8,0,are nn actually overparametrized i often read that nn or cnn are overparametrized but for example resnet18 has 11m parameters while cifar10 has 50k 32 32 3 153m data points how is that be an overparametrized network on cifar10 or even on mnist which has 60k 28 28 47m data points
r489w8,1,resources on sequential pattern mining in r i m looking for good resources on how to apply the sequential pattern mining on my app data specifically in r the most common package for the job is apparently arulessequences and the cspade algorithm i found this blog post and am basically looking for something similar all help is appreciated thanks in advance
r0um7e,1,opinions on books for core concepts of machine learning hi i want to buy some books for christmas for myself and i m currently trying to develop my machine learning skills i m just wondering though because of how much things tend to change with jobs like machine learning and data science is it actually worth buying books or are they going to become outdated relatively quickly am i better off saving my money and learning with google i really want to buy core concept books that are going to last a long time and act as a foundational reference if these types of book existl does anyone have any recommendation s please
r0jl0p,0,has anyone heard of the ai foundation how does there nlp work i m curious as to how their nlp works they seem like they are doing some sort of language personalization on an individual level looking at their virtual beings demo they look way more advanced than what the big tech companies have out unless this is an extremely cherry picked example i would like to believe this is not cherry picked or a marketing stunt but i also can t find any papers or technical details so i can t be sure x200b edit omg i misspelled their in the title
rlfkvk,1,what is overfitting a 2 minute visual guide x200b ðŸ”µ overfitting ðŸ”µ ðŸ§ overfitting is a common phenomenon the machine learning community tries to avoid like the plague this is because when a model overfits it performs extremely well on the training data that it is provided but performs poorly and fails to generalize on unseen data ðŸ’¾ you can imagine overfitting with an analogy when one assumes that the questions in the exercise session of a lecture are exactly what will be asked in the exam and end up memorizing them during the exam they realize that this rote learning would not be of any help in answering unseen questions ðŸ˜® overfitting can be avoided in many ways 5 common ways to do so is by 1 regularization of the model parameters using l1 or l2 norm for example see previous posts for more details 2 gathering more training data to let the model cut through the noise 3 early stopping by monitoring the training and validation error curves 4 reducing the number of features by selecting better features and 5 by performing data augmentation this is the first post and hopefully the first of many to come i have been studying and practicing machine learning and computer vision for 7 years as time has passed i have realized more and more the power of data driven decision making seeing firsthand what ml is capable of i have personally felt that it can be a great inter disciplinary tool to automate workflows i will bring up different topics of ml in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike the posts will cover topics like statistics linear algebra probability data representation modeling computer vision among other things i want this to be an incremental journey starting from the basics and building up to more complex ideas if you like such content and would like to steer the topics i cover feel free to suggest topics you would like to know more about in the comments
rp61rj,1,machine learning data scientist role fully remotely no location restricion hi ml community what are your experiences with working completely remotely outside your country for example i live in england and work fully remotely in portugal do any of you have similar experiences i am interested because i want to find a job outside my country if someone has similar experiences it would be nice to share their stories about 1 the employer s willingness to hire in this way 2 matters related to the employment contract 3 tax matters 4 and the most important where is it best to look for such positions any portals websites
r4rxry,1,looking for teammates hi everyone i am looking for teammates to participate in the kaggle competition please dm if interested
rrfeyx,1,what kind of technologies and programming languages would it take to build an ml ops platform like valohai
rhpymx,1,citable literature hey guys i am writing a chatbot for my bachelors thesis in python using tensorflow i already got the book from chollet deep learning with python however for the first part of my thesis i have to give an overview about the different techniques such as reinforcement learning supervised learning etc this is why i came here i wanted to ask if you have any advise on literature about these techniques i need them to be from academic journals academic books etc so no wikipedia online tutorial and so on any help appreciated
rs7juu,0,is rust stable mature enough to be used for production ml is making rust based python wrappers a good choice for performance heavy uses and internal ml dependencies in 2021 hi i m an ml engineer and while our team is using python extensively and most of the code we re using are libraries with c c backened there are a couple of specific situations that are happening right now that we need to really write some non python dependencies libraries to get far better performance in most cases people will blindly jump into writing c c but i wanted to see would it make sense to go for a rust backend is rust mature enough i ve seen huggingface writing their tokenizers in rust and those are some badass tokenizers and that was my main inspiration of asking this question if you ve gone this route what were your hurdles what should i look out and what should i look forward too was packaging rust and python hard was dockerizing packaging the environment hard and it made model deployment challenging thank you
rgbbc5,0,spring 21 reproducibility challenge results and support for fall 21 edition tl dr we announced that we d support the reproducibility challenge by awarding 500 per paper reproduced and we re announcing the award winners for the spring 21 edition as well as our support for the fall 21 edition of the challenge check out the awesome papers below ðŸ‘‡ hey r ml dean from dagshub here a while back i announced our support for the papers with code ml reproducibility challenge and that we d award participants 500 per paper reproduced according to the guidelines to align incentives and put our money where our mouth is today i m really happy to share the teams that were given the award and the projects they worked on â€“ read the full blog here i honestly think the full read is interesting and worth your time but here are the highlights from the papers 1 contextual decomposition explanation penalization cdep â€“ the original paper proposes a method to reduce the chance of models learning spurious correlations instead of the actually important features the team that reproduced it re implemented the original project in tensorflow rewriting some functions completely from scratch along the way they made a contribution to the tensorflow addons repo 2 self supervision for few shot learning â€“ as its name suggests this paper tests the importance of self supervised learning in few shot learning contexts the team that reproduced it explored different input configurations than the one proposed in the article and found out that it significantly affects the performance 3 ganspace discovering interpretable gan controls â€“ a proposed method to use simple pca to create controls for gans that are more humanly interpretable while being more computationally efficient the team re implemented the original implementation in tensorflow and trained the model with a few benchmark datasets they have a lot of very cool examples of the method in their report thank you to everyone who took part in this challenge none of this could be possible without you and we learned a lot in this process so what s next â€“ well we ve decided to continue the support the fall 2021 edition of the reproducibility challenge we want to host more reproduced papers since this makes the ml field better for everyone if you want to take part and move the field forward on the reproducibility front check out the guidelines for more information on how to take part
rjy067,0,anyone got a hypothesis on why icing on the cake method improves accuracy from my understanding the icing on the cake paper claims higher accuracy by saving the feature maps generated by convolution layers before the classifier layer during training and using them to retrain just the classifier fully connected layers again the authors comment they have no idea why i hypothesize that assuming the classifier training feature maps are given the same order as original training because the feature maps from early in training are very poor cnn not trained and bad at extracting features they actually act as a regularization method for the classifier then the much better feature maps from the end of original training re converges the classifier to a slightly different point in the loss landscape this seems to be similar to layer wise regularization through noise injection perhaps what are your guys thoughts
rklgw8,1,from where to python for ml u have learned python basics now i have started ml i am following book hands on machine learning which is popular in recommended books and coursera machine learning course by andrew ng i just observed that in book as well as course authors don t teach python for ml specifically can you please share some resources for python for ml
r27jbb,1,do you use data augmentation online or offline hi there do you use data augmentation in image based tasks online or offline what i mean by this is do you augment your images at the time of training or do you augment them before the training and then you use the final augmented dataset for training your model why one and not the other i ve been experimenting with both and it seems like both approaches have some advantages and disadvantages i would love to hear your take on this especially if you re working on projects in the industry
qz4uun,0,how to edit images with gans part 1 your digital metaverse avatar this tutorial covers the intuition behind image inversion with gans the editability vs reconstruction tradeoff projecting images into the generator s latent space telegram post blog post this is an image of me edited with styleclip subscribe to casual gan papers and follow me on twitter for weekly ai paper summaries and gan tutorials
r31m4v,1,after generating a weights file from training how do i check it s accuracy i trained a model to recognize an object using yolo and the model generated a weights file how do i validate the accuracy of the model now with the weights file
qkef2i,0,is there a good guide roadmap on deeplearning with large datasets in clouds is there a good guide roadmap on deeplearning with large datasets in clouds i have around 50 200gb of data in npy format to feed into a tensorflow pipeline preprocessing itself takes a few hours too or should i completely do that offline and change the pipeline structure
r06rs4,0,what are some strategies to deal with label sparsity when training a protein function prediction model the protein function prediction task requires you to take a sequence of amino acids think words in a sentence but if there are only 20 words and output the functions that protein can take there are around 30 thousand labels for protein function and these labels are not mutually exclusive so protein function prediction is essentially a huge binary prediction multitask now the catch is some labels are very common and others are very rare and overall a protein is only labeled with a small number of labels out of the 30 thousand labels that biologists came up with in total so if you represent each protein s functions as a binary vector where each 1 entry represents a function it carries then all vectors will be extremely sparse and the majority of labels will only have a few positive examples in the training set which leads to extreme label imbalance what are some strategies i can use to deal with these problems
rdbkhu,1,is there a forum where i can show my data and get recommendations on which algorithm to use and how to create model around the data
quvctp,0,mlops a new era of devops powered by machine learning with the interesting mix of terms â€œai mlâ€ and â€œdevelopment operations â€ mlops is an assortment of methods that is utilized for ai and its life cycle computerization and its calculations in execution for enormous scope it clears a smooth way for a coordinated effort between an information researcher and it proficient and consequently goes about as a scaffold between the abilities methods and apparatuses utilized in information designing ai and devops read more
r9iqi3,0,discussion will machine learning eventually make it into high school math curriculum eager young people interested in math will someday in the not too distant future begin clamoring for classes in ml at the high school level mathematicians and ai experts are increasingly using machine learning to solve previously intractable math problems getting a foundation in ml at a young age will expedite this trend
qzostq,0,question regarding bounding boxes in dataset hi i am relatively new to ml and i am looking at training my own model for use as part of a license plate anpr program for my dataset i am generating random characters in the same font as the uk plate in a rectangle and placing these on random backgrounds from the sun dataset see below examples x200b for the bounding boxes for these images can i use the exact cooridnates used in the generation of the fake plates or do i need to use a typical box by this i mean can i give co ordinates that may draw an irregular rectangle rather than the style i am seeing in packages such as labelimg would i have to use the same as the black box or could i use the green style for training x200b
qjxfu9,0,machine learning wayr what are you reading week 124 this is a place to share machine learning research papers journals and articles that you re reading this week if it relates to what you re researching by all means elaborate and give us your insight otherwise it could just be an interesting paper you ve read please try to provide some insight from your understanding and please don t post things which are present in wiki preferably you should link the arxiv page not the pdf you can easily access the pdf from the summary page but not the other way around or any other pertinent links previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 121 130 week 1 11 21 31 41 51 61 71 81 91 101 111 121 week 2 12 22 32 42 52 62 72 82 92 102 112 122 week 3 13 23 33 43 53 63 73 83 93 103 113 123 week 4 14 24 34 44 54 64 74 84 94 104 114 week 5 15 25 35 45 55 65 75 85 95 105 115 week 6 16 26 36 46 56 66 76 86 96 106 116 week 7 17 27 37 47 57 67 77 87 97 107 117 week 8 18 28 38 48 58 68 78 88 98 108 118 week 9 19 29 39 49 59 69 79 89 99 109 119 week 10 20 30 40 50 60 70 80 90 100 110 120 most upvoted papers two weeks ago u icko patches are all you need u catalyzex code bot paper link besides that there are no rules have fun
r2gl7w,0,can i separate out the steps of learn in stable baselines3 i m working on a project where two agents train simultaneously but each agent only sometimes needs to make a decision is it possible to have code that follows roughly the following structure model a2c mlppolicy env verbose 1 learning rate 0 0005 obs env reset for i in range 2000000 action states model predict obs obs rewards dones info env step action model update from experience obs action reward does this type of function exist i m also not married to stable baselines to if there s a way to do this in another library that would also be greatly appreciated thanks
r1zsaw,0,graph as an output of ml modelling hi i am currently thinking about tackling a rather specific problem using ml or any kind of statistical approach really i have the problem that for a given set of nodes with their names i want to predict an acyclic graph of how these nodes will be arranged i also have a set of meta attributes that put the graph into context and i also have a set of historical graphs together with their meta attributes the basic idea is that we assume that graphs with similar meta attributes will follow a similar pattern also wrt to their node names ultimately i want to build a model that is able to forecast the resulting graph the only solution i could think off is building a model to forecast the single entries of the adjacency matrix essentially fitting a function f meta node1attributes node2attributes 1 if node 1 node 2 else 0 is there any smarter way of doing this
qq75zu,0,how do you choose an optimizer and why are there so many choosing an optimizer for the training of anns is one of the most critical design choices because anns are black boxes the theoretical guidelines on the overall design are very limited they are mostly anecdotally and strongly depend on the developers experience x200b when it comes to optimizer there are hunderts available from sgd to adam to very specific ones it feels like there is a custom tailored optimizer for every problem and every architecture x200b why is it so hard to come up with something more general why are there this many optimizer and how do you choose an optimizer for your project
qkfuzn,0,to phd or not to phd i made the following post on other subs too just posting it here to get the input from larger machine learning community hi all i recently completed my research based masters in computer vision and currently working in a company as a computer vision researcher my current role requires a lot of paper reading to improve the existing models i really like doing research and am satisfied with my current role i have the following questions 1 if i decide to pursue a phd i will not be able to save money for next 3 to 4 years which is better 4 years of phd or 4 years of research job experience 2 my long term goal is to get a job in big companies like google and facebook most of the computer vision roles in big companies require a phd with multiple publications can i join such companies without a phd 3 my company encourages publishing papers letâ€™s say if i publish some papers in next three to four years would that help me in competing with phd degree holders or i would still need an official degree 4 how hard is to get admission in a good uni after some years of research experience with no publication record i would be thankful if someone could comment on my questions
qpw3br,0,recursive ml strategies i m looking for ideas on how to use recursive ml strategies possibly utilizing multiple individual models where one model uses the output of another model to make more accurate predictions for example i use two sklearn randomforestclassifier models to provide a simple signal about the direction of the stock market the first takes n inputs and outputs a prediction the second takes the original n inputs plus the output of the first to make a new prediction it doesn t provide earth shattering results but it appears to be slightly better than only using the one model random forests also provide the ability to use out of bag samples which could also be used i m just curious if there any established methods papers i should look at etc that discuss meta or recursive strategies to get the most out of ml models
qoyyy7,0,what happened to compressive transformers they promised to solve one of the transformer architecuter s greatest weaknesses it s lack of long term memory but i can t seem to find any bigger experiment using them did they not work out
qsigso,0,discussion is your data quality suffering because of a first mile reliability problem if your data product is fueled by tens to hundreds of external data sources then this may be relevant to you when schema changes volume anomalies late deliveries plague the first mile they go on to infect your downstream warehouse tables and business processes when the reliability of all those data sources are questionable they cascade into points of failure that are out of your data teamâ€™s control awareness if you d like to learn how to improve your data s first mile reliability check out our latest blog post here
rpqwao,1,federated learning for mobile keyboard prediction ever wondered how your mobile keyboard gives you the next word suggestions how do they give personalised suggestions while at the same time ensuring the privacy of individuals check out my blog post federated learning for mobile keyboard prediction which talks about how this happens in a privacy preserving manner blog post ppml series 3 federated learning for mobile keyboard prediction annotated paper annotated ml papers federated learning for mobile keyboard prediction
rui1c7,1,sklearn pipeline breaks when using functiontransformer hey i m learning to use pipelines as they look more clean so im working on the tabular playground competition on kaggle i m tryna follow a pretty simple pipeline where i use a functiontransformer to add a new column to the dataframe do ordinal encoding and finally fit the data on a linearregression model here is the code but the code breaks on the first step functiontransformer and gives me the following error to assemble mappings requires at least that year month day be specified day month year is missing which is weird since i can print inside the function being executed which shows it is in datetime format even transform x train date on the functiontransformer works as intended but it doesn t seem to work when all the steps are joined any help is appreciated thanks
r5tfm4,1,unbalanced predictions on mnist so i ve implemented a simple lenet model to learn and predict the mnist dataset the purpose being to experiment with a new loss function evidential deep learning which makes the model aware of epistemic uncertainty i e a cat dog classifier can say i don t know to a picture of a whale the training set i ve balanced the labels so there are the same number of samples for each label the weights are randomly with no seed initialised the dataset shuffled and yet the predictions are weighted to the lower digits see image and rather substantially to the 0 digit the implementation is in pytorch does anyone know of methods to mitigate this x200b
rpirvh,1,machine learning use cases in telecom industry apart from churn prediction customer segmentation and anomaly detection how is data science used in the telecommunications sector are you aware of industry research use cases in that field
qqxcgt,0,microsoft india proposes varuna scalable low cost training of massive deep learning models a microsoft research india team presents varuna a system for training massive deep learning models on commodity networking that eliminates the need for specialized hyperclusters and alleviates the cost scale and resource utilization challenges of deep learning model training here is a quick read microsoft india proposes varuna scalable low cost training of massive deep learning models the varuna code has been open sourced and is available on the projectâ€™s github the paper varuna scalable low cost training of massive deep learning models is on arxiv
rlin1a,1,ml classification supervisedlearning hey everyone i am looking for an algorithm which helps me to sort the values from a sensor in three classes f e nan or 1 2 3 i have a lot of trainingdata where i already declared at which time and value which class is the best one so i choose supervised learning classification which algorithm would you suggest do you find a better way to solve this problem
rsusfc,1,custom h5 for dickpics hey i want to detect nudity in a live video stream i tried to train my own model but it does not detect anything i couldnt find an already trained model for penises anywhere do you guys maybe know a source thank you in advance
rec0nk,0,karpathy on the consolidation of the field iâ€™ve definitely noticed a trend in the papers iâ€™m seeing this year with people combining building blocks in creative ways i wonder how far this trend will go and what it will unlock
rksnv1,1,project advanced methodology for intelligent diagnosis of systems and processes the goals of intelligent diagnostics of systems and processes â€“ increasing the level of fault tolerance of the diagnostic object by reducing the probability of incorrectly determining the state of the systems errors of the third kind in classifying the state of the systems when monitoring systems obtaining stable effective estimates of unknown values of design parameters of functional elements of systems corresponding to the found state the choice of a rational control law for the object of diagnostics for the identified on the basis of the forecast state algorithm for the numerical solution of the problem of intelligent diagnostics of systems and processes on the base of agent oriented approach by means of reinforced learning 1 data pre processing 1 1 structural analysis of the diagnostic object construction of fault trees and corresponding event trees identification of the most probable scenarios of system failures as a whole caused by single or multiple defects in functional elements on the basis of monitoring data of the state of the prototypes of the system in operation 1 2 formation of a subset of controlled state variables the values of the quantities of which are check in by measuring instruments 1 3 a training sample of alternatives dataset of the values of the quantities is generated as well as the corresponding confidence intervals measurement accuracy for the regime control variables and controlled state variables based on the data of experimental studies of the diagnostic object in healthy state 1 4 data cleaning from anomalous values of the quantities outliers 1 5 identification of the complete mathematical model of the system based on the solution of the problem of multicriteria optimization in a stochastic formulation using dataset clauses 1 2 1 3 1 6 the search for the values of the objective functions corresponding to the boundaries of the intervals of the operating modes of the diagnostic object in healthy state based on the solution of the direct analysis problem using the complete mathematical model paragraph 1 5 1 7 searching of the solutions of the inverse problems of interval analysis for each failure scenario of the system as a whole the values of the boundaries of the intervals for the design parameters and controlled state variables that correspond to the healthy state of the diagnostic object based on the solution of multicriteria optimization problems in a stochastic formulation based on the results obtained a database is formed that contains the boundaries of the intervals of acceptable values of design parameters and controlled state variables for each scenario of failure of the system as a whole these data can be used for tolerance control of the states of the diagnosed object 1 8 a total dataset of alternatives is generated each alternative includes subsets of design parameter values regime control variables and controlled state variables objective functions the total dataset includes subsets of alternatives corresponding to different failure scenarios of the system as a whole including the healthy state the values of the controlled state variables and objective functions for alternatives can be obtained on the basis of modeling the characteristics using the complete mathematical model as well as directed experimental studies of the diagnostic object with defects in functional elements 1 9 data cleaning from anomalous values of the quantities outliers normalization of data 1 10 cluster analysis â€“ the determination of the possible number of states failure scenarios of the system as a whole using regime control and controlled state variables paragraph 1 8 if the number of identified clusters coincides with the specified one and the distances between the clusters are statistically significant then the subset of monitored state variables can be considered complete otherwise the diagnostic object should be equipped with new measuring instruments see paragraph 1 2 1 11 development of robust metamodels using data paragraph 1 8 1 9 a a multidimensional logistic regression in the form of analytical dependences of the posterior probabilities of different failure scenarios for the system as a whole on the regime control and controlled state variables b a multidimensional observer model in the form of analytical dependences of controlled state variables on design parameters and regime and control variables c a multidimensional diagnostic model in the form of analytical dependences of the objective functions on the regime control and controlled state variables 2 monitoring the state of the diagnostic object during operation 2 1 measurements are made of the values of the regime control and monitored state variables at the current moment of operation of the diagnostic object 2 2 determination of the scenario for which the maximum a posteriori probability of its realization corresponds with the observed values of the quantities paragraph 2 1 based on the solution of the classification problem using the metamodel 1 11a 2 3 tolerance control of the diagnostic object which uses a database paragraph 1 7 containing the values of the boundaries of the intervals of acceptable values of the monitored state variables for the scenario defined in paragraph 2 2 2 4 robust estimation of the values of the design parameters of functional elements corresponding to the scenario defined in paragraph 2 2 of the diagnostic object based on the observed values of the regime control and controlled state variables paragraph 2 1 the search for the values of the design parameters is carried out on the basis of solving the problem of multicriteria optimization in a stochastic formulation using the metamodel 1 11b 2 5 tolerance control of the diagnostic object using the database paragraph 1 7 containing the values of the boundaries of the intervals of acceptable values of design parameters for the scenario defined in paragraph 2 2 2 6 robust estimation of the values of the objective functions corresponding to the scenario defined in paragraph 2 2 of the diagnostic object based on the observed values of the regime control and controlled state variables paragraph 2 1 using the metamodel 1 11 c 2 7 development of robust multidimensional models of control the state for the diagnostic object in the form of analytical dependencies predicted from the measured values of the regime control and controlled state variables in the monitoring process 2 8 forecasting multidimensional time series of controlled state variables based on multidimensional models of control the state for the diagnostic object paragraph 2 7 2 9 reducing the dimension of the space of controlled state variables based on the analysis of the informativeness of the variables of the robust multidimensional models of control the state for the diagnostic object sensitivity analyzes estimating the rank of time series cointegration 2 10 determination of the state of the diagnostic object based on the solution of the classification problem using the metamodel 1 11 a and the predicted values of the regime control and controlled state variables 2 11 synthesis of a rational control law for the diagnostic object for the state identified on the basis of the forecast the given statements are reliable as they had confirmed by the experience of using developed by us â€œrod idsâ€ software in different fields of activity look please on articles and presentations of our results in the section achievements publications we have the right to consider ourselves as specialists in the development of effective machine learning methods mlmâ€™s for solving â€œrod idsâ€ problems we offer you our software in order to resolve specific problems or we can do calculations on our own for you in addition we are always open to have contacts and discuss how our methodology computational methods and it realization theirs in form of interactive compute decision making support software system â€œrod idsâ€ can be helpful for your company we hope that your use of our mlmâ€™s will increase customer demand for new versions of your software
qo1sdh,0,gptsd transfer of trauma from human to machine back to human via machine learning models x200b gptsd is a series of images and text created with gpt2 which explores the transfer of trauma from human to machine back to human via machine learning models by converting human portraits to text gpt2 is able to recreate new text base portraits and dream recollections based off the dream diary of a vietnam war veteren
rd7pd2,0,what are techniques for few shot domain adaptation i am trying to do domain adaptation from synthetic to real images the task is anomaly detection usually the problem for domain adaptation is that there are a lot of target images without labels in my case i only have a few target images with labels therefore the common technique of creating pseudo labels for the target domain is not useful my current idea is to do some kind of style transfer from the synthetic images to the real images cycada contrastive unpaired image to image do you have other ideas of domain adaptation where i have a lot of source images but only a few target images but with ground truth i would be glad to be pointed in the right direction
r7hd01,1,from zero to research on deep learning vision in depth courses google colab tutorials anki cards hey i m arthur a final year phd student at sorbonne in france i m teaching for graduate students computer vision with deep learning and i ve made all my courses available for free on my website x200b tree of the deep learning course yellow rectangles are course orange rectangles are colab and circles are anki cards we start from the basics what is a neuron how to do a forward backward pass and gradually step up to cover the majority of computer vision done by deep learning in each course you have extensive slides a lot of resources to read google colab tutorials with answers hidden so you ll never be stuck and to finish anki cards to do spaced repetition and not to forget what you ve learned the course is very up to date you ll even learn about research papers published this november but there also a lot of information about the good old models tell me if you liked and don t hesitate to give me feedback to improve it happy learning edit thanks kind strangers for the rewards and all of you for your nice comments it ll motivate me to record my lectures
qkdfwe,0,tars task aware representation of sentences for generic text classification paper summary state of the art approaches for text classification leverage a transformer architecture with a linear layer on top that outputs a class distribution for a given prediction problem while effective this approach suffers from conceptual limitations that affect its utility in few shot or zero shot transfer learning scenarios ðŸ”¥ this paper proposes a novel formulation of text classification that addresses these limitations paper
ris9cv,1,ml engineers who work for a company or are in research which subfield do you work in and what are the key resources you used to learn please do be as specific as possible so that new comers have a good idea of what diverse fields are there and what are some good resources for such fields
rpg57g,1,yolo v5 inference speed slower on exported onnx model compared to pytorch model hi everyone i ve been using the official pytorch yolov5 repo to perform some object detection task i have trained the model using my custom dataset and saved the weights as a pt file i also exported the weights as an onnx model as well using export py in the repo x200b running detect py using the pt weights the inference speed is about 0 012 seconds per image however the onnx weights require 0 2 seconds per image i have installed onnxruntime gpu and i am sure the gpu is utilized when i run the onnx model x200b is the slower inference speed a known behaviour for onnx models
r3xwru,1,k means high cluster count if i use high cluster count than what suggested by elbow method in kmeans it may lead to some extra data analysis but will it lead to mixing of data points of one cluster into another to a high extent i am using 200 clusters instead 50 as suggested by elbow method as high of data was left unclustered
rr4y9y,1,can overfitting pass unnoticed because of a small dataset let s say i have a small dataset i trained and tested a model and the training set and test set accuracies are high and similar cross validation accuracy is also similar and has low variance is it possible to overfit because the dataset is small
r2p2vo,1,are derived or computed inputs bad for cnns i am building a cnn and am wondering if inputting derived or computed inputs are generally bad for the effectiveness of cnns or just nns in general by derived or computed values i mean data which is not raw and instead is computed based on the raw data for example in a very simple form using timeseries data as the raw data and computing a 30 day sma as a derived computed value and as another input is this bad practice at boosting the networks effectiveness if it is not bad practice are there any tips at what kind of computed values someone should consider when adding new inputs the goal of my nn is for building predictions in timeseries data sorry if this is a newb question i am indeed new to ml
qn0ynq,0,fuzzy c means clustering on line graph data hi i ve been trying to apply fuzzy c means on data that can be represented as line graphs hourly electrical load profiles i understand that i must cluster the points on each hour what i don t get is how do i relate the clustered points on each hour to the adjacent hour so that i can obtain the result which is a clustered line graph here is an example of an input and the desired output input electrical load profiles x200b output clustered results
rerlx0,0,question regarding downsampling in convolutional networks in every convolutional network which has some kind of encoder there are downsample steps which transforms a tensor with shape batch size h w filters into a tensor with shape batch size h 2 w 2 filters 2 this is always done in one of two ways using a convolutional layer with strides 2 2 and filters filters 2 using a maxpool2d layer followed by a convolutional layer with filters filters 2 there is a option that is never used concatenating a maxpool2d layer and a convolutional layer with strides 2 2 and filters filters why is this at least for me apparent third option never used what are the drawbacks
r15mte,1,has anyone used algorithmia to deploy models i m unconvinced algorithmia advertises themselves as an mlops platform for data scientists and they provide an easy way to host models on a scalable rest api this sounds like a perfect solution for a data scientist or hobbyist who wants to host models for cheap and not worry about the devops but as i ve gotten more familiar with it i have more questions for the base tier algorithmia requires you to host your model s request handling code on a github repository owned by them a separate repository for your request handling code seems like a strange pattern to develop in they also encourage you to develop in their web ui again another pattern that feels forced they also have an ominous section in their terms of service that says you do not transfer ownership of the software to algorithmia but you do hereby grant algorithmia a fully paid up and royalty free license to use and permit others to use the software which feels overly aggressive for forcing you to use their source hosting between an unnatural development environment and a sketchy ownership clause i m reluctant to continue using algorithmia has anyone had similar experiences with algorithmia am i just being overly skittish and misinterpreting the ownership clause are their better repository patterns git subtrees that other people have used with them are there better companies to host models or should i have never even attempted leaving the aws and gcp hosting land
rlon3o,1,hackthisai adversarial machine learning ctf challenges a couple years ago i took a one day adversarial machine learning workshop it s a fascinating field but i had trouble finding a clear entry learning path i m prototyping some capture the flag style challenges to provide that sort of skill development i d appreciate any playtesting thoughts on the format recommendations etc
r5cvb6,0,are there any research using chaos theory to study deep neural network i just know and reading this book emergence from chaos to order but it just studied simple shadow neural network are there any research using chaos theory to study deep neural network
qukghv,0,what is riemannian manifold intuitively recently i was studying dimensionality reduction when i come to a state of the art dimensionality reduction algorithm umap i couldn t understand their mathematics part i think the first obstacle to understand it is i do not understand what is a riemannian manifold i have watched multiple videos and tutorials on the riemannian manifold but i still cannot catch the idea easily seems like the first thing they explain already requires complicated maths knowledge i would like to ask if anyone can really explain it in one simple sentence that a normal computer science student or year one undergraduate stem student will understand thank you very much
r8geii,1,paper overview nÃ¼wa visual synthesis pre training for neural visual world creation video paper code abstract this paper presents a unified multimodal pre trained model called nÃ¼wa that can generate new or manipulate existing visual data i e images and videos for various visual synthesis tasks to cover language image and video at the same time for different scenarios a 3d transformer encoder decoder framework is designed which can not only deal with videos as 3d data but also adapt to texts and images as 1d and 2d data respectively a 3d nearby attention 3dna mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity we evaluate nÃ¼wa on 8 downstream tasks compared to several strong baselines nÃ¼wa achieves state of the art results on text to image generation text to video generation video prediction etc furthermore it also shows surprisingly good zero shot capabilities on text guided image and video manipulation tasks
rv6xf2,1,how do you add existing classes from pretrained tensorflow models into your projects that include new classifications i started learning how to train a custom object detector from this tutorial which shows how to use a pretrained model for detecting hand signals most other tutorials show similar pipelines use a pretrained model to detect only new things that are annotated and trained upon let s say i wanted this model to detect hand signals and people do i need to train the model on people again or is there some way to switch back on detection for the class people it was already trained on
qkzi4q,0,can icra reviewer see the names of author i submitted a paper to icra but i forgot to put the names of the ourselves on the paper question can the reviewer still see the author names through the system
r3j2j2,0,are there any simple hosted mlops or auto scaling solutions iâ€™m looking to deploy vqgan clip models at a pretty large scale 30 gpus and i want to explore options other than kubernetes on aws or gcp is there a hosted mlops platform where i can simply upload my ml service docker flask and the platform can completely take care of scaling and gpu provisioning by itself essentially outsource scaling up down and handling traffic to a product or service with me having to do minimal setup
rlj83w,0,how important is numerical analysis for machine learning iâ€™m already signed up for a numerical methods class from the cs department which is very application focused and was originally planning on doing a sequence of graduate numerical analysis from the math department next year which is very theory focused but now iâ€™m wondering if iâ€™m better off taking some other class instead of the sequence ie a graduate algorithms class and a graduate real analysis class is that a good idea is an upper division undergrad numerical methods class sufficient p s iâ€™m aiming for a phd in statistics after undergrad
rai9r9,0,the reviews and my rebuttals for my rejected icml and neurips submissions to increase transparency about the review process and in the hopes that it might help others shape their papers rebuttals i m sharing the reviews and my rebuttals for my icml submission of baller2vec my neurips submission of baller2vec and my neurips submission of baller2vec each of which was rejected notably i received the same highly negative reviewer on all three submissions reviewer 2 reviewer 7vsi and reviewer grxc respectively which was a bummer particularly because it s clear that the reviewer s negativity caused another reviewer to lower their score on both the icml and neurips submissions of baller2vec the reviewer claimed to be an expert in both sport and machine learning yet repeatedly mischaracterized and misunderstood aspects of the models going as far as to state the methods theory section is explained in a needlessly complicated and uninformative manner in their icml review in contrast the other three icml reviewers said our writing is very clear reviewer 5 they quite enjoyed reading this paper reviewer 1 and our writing was very clear reviewer 4 even ignoring their ratings i think this raises important questions about what it means to be peer reviewed given that there are probably many hundreds thousands of researchers in the field of multi agent spatiotemporal modeling why was this same individual given three opportunities to judge my work on behalf of the community why are their anonymous opinions considered representative additionally i found the dataset criticisms from some of the reviewers incredibly frustrating notably the above reviewer did not mention anything about additional datasets in their icml review but did in their neurips reviews at least two other multi agent trajectory modeling papers that were accepted to neurips this year only evaluated their methods on a single large real world dataset 1 grin generative relation and intention network for multi agent trajectory prediction evaluated their method on a small simulated dataset 50k training sequences and a preprocessed nba dataset 100k training sequences and 2 collaborative uncertainty in multi agent trajectory forecasting evaluated their method on the nuscenes dataset 1 000 scenes and argoverse 206k training sequences so it doesn t seem like this expectation is evenly applied by reviewers
rcemnh,1,feeding mri data into spherical cnn hi this is my first time posting here i am currently working on my master s thesis which involves detection of certain diseases using the mri scans of brain tissue of individuals using spherical cnn i am dividing the brain into left and right hemispheres i have mri surface pial sphere and curve files for each hemisphere from free surfer i can get and create a 3d point cloud map using nibabel and open3d for pial and sphere files the same files also contain the co ordinates for mesh triangles curv file contains a list of values whose number equal to the number of points in the point cloud map is there any way to display the surfaces that is read from the pial and sphere file so that we can display it like what is shown in the freeview part of free surfer to display the free surfer files or save the output from freeview in the form of numpy array that can be read in using nibabel screenshot zip
rw401v,1,how do i install caffe framework on mac m1 i m working on a deep learning system in c using caffe i installed all the dependencies from brew and installed caffe but when i compile with make i get the following ld can t map file errno 22 file usr local lib caffe for architecture arm64 clang error linker command failed with exit code 1 use v to see invocation make build error 1 i m not sure if i have the wrong build or if it is just incompatible with mac m1 any help is appreciated thanks
qlwcgx,0,twitter cortex proposes lmsoc for socially sensitive pretraining in the new paper lmsoc an approach for socially sensitive pretraining a twitter cortex research team proposes a simple but effective approach for learning both linguistically contextualized and socially sensitive representations in large scale language models here is a quick read twitter cortex proposes lmsoc for socially sensitive pretraining the lmsoc code is available on the projectâ€™s github the paper lmsoc an approach for socially sensitive pretraining is on arxiv
qu7dgv,0,should i re use valid data in incremental learning in incremental learning the dataset is split into multiple segments and the model is trained over each segment so the model can be trained over a large dataset using limited ram as each segment has its own train valid and test is it ok to reuse the valid data in the training data of the next segment how about the test data
rtw69p,1,can i use features of type float in a linear regression machine learning model i am currently working on a basic machine learning project which is revolved around predicting house prices given several different features about the house some of these features are of type float instead of type int examples of this are bathrooms can be 1 5 floors there are houses with 1 5 and 2 5 floors in this dataset and bedrooms can be 1 5 2 5 in this dataset x200b i was looking at other similar projects online and came across this remember that it is essential to change float types to integer types because linear regression is supported only on integer type variables it can be converted using the â€˜astypeâ€™ function in python x200b finally my question is do i have to convert floats to integers for a linear regression machine learning model or can i use floats i want to use floats because i feel like by converting to int type i will lose a lot of important data ex 1 bedroom house and 1 5 bedroom house will be the same after the conversion
ql0a61,0,why isn t converting ml models to plain code trivial i ve only done across one project m2cgen for converting ml models to plain code given that even complex models can be broken down to a series of nested functions why is this not more commonly done yes training is very complex but inference is just passing the input through it s nothing dynamic sure the performance will suffer but for non streaming applications it should be fine even a complex classification network isn t going to take long to run inference the frameworks already parse the graph or pipeline or whatever it is they use to matrix multiplications so why not export a plain code version i know it s not the same but it s probably much easier for the framework devs to implement this rather than someone external it ll take a bit of doing but having completely portable computer code with no hosts model serialisation etc seems like a good thing as you might imagine i m thinking of how to make portable models for integrating with local software a game engine with as little hassle as possible
rgahri,0,virtual mlops round table given the turnout at our last two events and the great feedback we ve received we ve decided to hold another virtual mlops round table on december 16th at 5 pm pst we ll follow the same format of forming breakout groups of 5 7 people and letting the peer learning discussion flow from there there is absolutely no selling or pitching the focus is pure peer learning you can sign up here if you re interested let me know if you have any ideas thoughts or feedback
r7cvi6,0,how important are publications in research scientist interviews sorry if this question is rather dumb but i just want to have hear some first hand account on how important are publications at top venues i e iclr neurips icml respective top conferences in cv nlp in research scientist including internship interviews at industrial labs how important are these compared to for e g leetcode behaviourial qns so on i understand the first foremost thing is probably whether there is a team fit let s assume there exists a team doing things roughly in the same area of the interviewee any comments are highly appreciated thanks
re1s36,1,tutorial on using image super resolution without photoshop video used github project
qoa1tv,0,a rare real example of a true time series anomaly discovered by an algorithm in spite of all the academic work on anomaly detection in time series it is almost impossible to find a real example of a true anomaly captured in the wild here i present such an example a group from texas a m usc has released a very nice large dataset relating to electric grids most of the data is measured temp voltage etc but some solar zenith angle etc is computed as a sanity check upon downloading the data i ran the matrix profile a to look for any anomalies in the data it found the highly significant anomaly shown in the attached figure can you guess what it isâ€¦ spoiler below it took me a few seconds but i guessed it might be a leap year bug in the data generator and indeed after i reported it i found that this was the case moral of the story check your data and the matrix profile is a very useful tool more examples of time series anomalies at a and b a www cs ucr edu eamonn matrixprofile html b c
r6pedp,1,resource to learn time series analysis to prepare for quantitative research take home assessment hi i ve received an offer for a 24 hour coding assessment for the role of quantitative researcher for a trading company the main component seems to be to statistically analyze large scale tick by tick financial data to extract alpha patterns i guess i m supposed to analyze large time series data and detect alpha patterns i have python numpy and pandas background but have not dealt with any professional time series analysis and presentations any resources eg tutorials sample presentations articles books youtube resources on how to prepare for this further clarification on what to expect would be great as well thanks
rgosh5,0,the future of artificial intelligence is self organizing and self assembling blog post by sebastian risi excerpt this is the first post in a series i plan to write on the work from our group and others that combines ideas from deep learning with ideas from self organization and collective systems in this first post weâ€™ll look at some of the developed approaches and the domains they have been applied to ranging from growing soft robots and minecraft machines to self assembling modular robots and creating more resilient and adaptive reinforcement learning agents the merger of these ideas could ultimately allow our ai systems to escape their current limitations such as being brittle rigid and not being able to deal with novel situations however the combination of these methods also poses new challenges and requires novel ways of training to work as efficiently as possible one of the most fascinating aspects of nature is that groups with millions or even trillions of elements can self assemble into complex forms based only on local interactions and display what is called a collective type of intelligence for example ants can join to create bridges or rafts to navigate difficult terrain termites can build nests several meters high without an externally imposed plan and thousands of bees work together as an integrated whole to make accurate decisions on when to search for food or a new nest surprisingly achieving these incredible abilities is a result of following relatively simple behavioral rules and through a process of self organization which camazine et al 2001 define as â€œas a process in which pattern at the global level of a system emerges solely from numerous interactions among the lower level components of the system moreover the rules specifying interactions among the systemâ€™s components are executed using only local information without reference to the global pattern in short pattern is an emergent property of the system rather than being imposed on the system by an external ordering influence â€œ self organizing systems are made out of many components that are highly interconnected the absence of any centralized control allows them to quickly adjust to new stimuli and changing environmental conditions additionally because these collective intelligence systems are made of many simpler individuals they have in built redundancy with a high degree of resilience and robustness individuals in this collective system can fail without the overall system breaking down multicellular organisms learned to exploit self organizational principles to self assemble starting from a single egg cell and only through the process of local cell interaction during embryonic development similar to the robustness of swarms of organisms the self organization of cell populations is remarkably robust to perturbations in some animals this goes as far as being able to regenerate complete body parts such as a salamanderâ€™s tail this type of self repair is a common feature of self organizing systems and interestingly does not involve any additional processes â€œthe same self organization process that built the initial pattern can operate to repair the pattern â€ â€” camazine et al 2001 rest of the blog
r0g2o2,0,ai safety needs great engineers top line if you think you could write a substantial pull request for a major machine learning library then major ai safety labs want to interview you today i work for anthropic an industrial ai research lab focussed on safety we are bottlenecked on aligned engineering talent specifically engineering talent while we d always like more ops folk and more researchers our safety work is limited by a shortage of great engineers i ve spoken to several other ai safety research organisations who feel the same why engineers may last year openai released gpt 3 a system that did surprisingly well at a surprisingly broad range of tasks while limited in many important ways a lot of ai safety folk sat up and noticed systems like gpt 3 might not themselves be the existential threat that many of us are worried about but it s plausible that some of the issues that will be found in such future systems might already be present in gpt 3 and it s plausible to think solving those issues in gpt 3 will help us solve equivalent issues in those future systems that we are worried about as such ai safety has suddenly developed an empirical subfield while before we could only make predictions about what might go wrong and how we might fix those things now we can actually run experiments experiments are not and should never be the entirety of the field but it s a new and promising direction that leverages a different skill set to more classic ai safety in particular the different skill set it leverages is engineering running experiments on a real if weak ai system requires a substantial stack of custom software with projects running from hundreds of thousands to millions of lines of code dealing with these projects is not a skillset that many folks in ai safety had invested in prior to the last 18 months and it shows in our recruitment what kind of engineers looking at the engineers at anthropic right now every one of them was a great software engineer prior to joining ai safety every one of them is also easy to get on with beyond that common traits are experience with distributed systems experience with numerical systems caring about and thinking a lot about about ai safety comfortable reading contemporary ml research papers expertise in security infrastructure data numerics social science or one of a dozen other hard to find specialities this is not a requirements list though based on the people working here already great software engineer and easy to get on with are hard requirements but the things in the list above are very much nice to haves with several folks having just one or none of them right now our job listings are bucketed into security engineer infrastructure engineer research engineer and the like because these are the noun phrases that a lot of the people we like identify themselves with but what we re actually most concerned about are generally great software engineers who ideally have some extra bit of deep experience that we lack how does engineering compare to research at anthropic there is no hard distinction between researchers and engineers some other organisations retain the distinction but the increasing reliance of research on substantial custom infrastructure is dissolving the boundary at every industrial lab i m familiar with this might be hard to believe i think the archetypal research and engineering organisation is one where the researchers come up with the fun prototypes and then toss them over the wall to the engineers to clean up and implement i think the archetype is common enough that it dissuades a lot of engineers from applying to engineering roles instead applying to research positions where they when evaluated on a different set of metrics than the ones they re best at underperform what s changed in modern ai safety is that the prototypes now require serious engineering and so prototyping and experimenting is now an engineering problem from the get go a thousand line nested for loop does not carry research as far as it once did i think this might be a hard sell to folks who have endured those older kinds of research organisations so here are some anecdotes the first two authors on gpt 3 are both engineers some of the most pure engineers at anthropic spend weeks staring at learning curves and experimenting with architectural variants one of the most pure researchers at anthropic has spent a week rewriting an rpc protocol the most excited i ve ever seen anthropic folk for a new hire was for an engineer who builds academic clusters as a hobby should i apply it s hard to judge sight unseen whether a specific person would suit ai safety engineering but a good litmus test is the one given at the top of this post with a few weeks work could you hypothetically write a new feature or fix a serious bug in a major ml library are you already there could you get there with a month or two of effort i like this as a litmus test because it s very close to what my colleagues and i do all day if you re a strong enough engineer to make a successful pull request to pytorch you re likely a strong enough engineer to make a successful pull request to our internal repos actually the litmus test above is only one half of the actual litmus test i give folk that i meet out and about the other half is tell me your thoughts on ai and the future with a pass being a nuanced well thought out response should i skill up this post is aimed at folks who already can pass the litmus test i originally intended to pair it with another post on skilling up to the point of being able to pass the test but that has turned out to be a much more difficult topic than i expected for now i d recommend starting with 80k s software engineering guide take homes we want more great engineers if you could write a pull request for a major ml library you should apply to anthropic if that s not you but you know one or more great engineers ask them if they could write a pull request for a major ml library if yes tell them to apply to anthropic if that s not you but you d like it to be watch this space we re working on skilling up advice this is a twinned version of this post on lesswrong
r9za5a,1,scene detection using opencv how can i make this project if for example the 2 scenes which my model can differentiate between are 1 an excercise scene 2 a cooking kitchen scene also where can i aquire the datasets for these any help will be helpful
rrxzyy,0,machine learning use cases in telecom industry apart from churn prediction customer segmentation and anomaly detection how is data science used in the telecommunications sector are you aware of industry research use cases in that field
qsi0u2,0,prune then quantize or quantize then prune for post training optimization of computer vision models as a student that is interested in improving the efficiency model size throughput energy of pre trained deep learning models in the computer vision domain i was wondering if there was a clear winning approach on in what order we should prune and quantize a pre trained model so basically if you were given a pre trained deep learning model e g resnet would one of the following approaches lead to better solutions in terms of the accuracy to efficiency ratio e g model size throughput energy consumption quantize then prune with finetuning retraining after every stage prune then quantize with finetuning retraining after every stage i have trouble finding related works that answer this kind of question to me it seems to be valid question but maybe i m missing something obvious any input would be appreciated
rh0j05,1,variational autoencoder tf2 typeerror i am trying to implement a vae for mnist using convolutional layers using tensorflow 2 6 and python 3 9 the code i have is data pre processing steps input image dimensions img rows img cols 28 28 load mnist dataset x train y train x test y test mnist load data if tf keras backend image data format channels first x train x train reshape x train shape 0 1 img rows img cols x test x test reshape x test shape 0 1 img rows img cols input shape 1 img rows img cols else x train x train reshape x train shape 0 img rows img cols 1 x test x test reshape x test shape 0 img rows img cols 1 input shape img rows img cols 1 print f ninput shape to be used input shape input shape to be used 28 28 1 specify hyper parameters batch size 64 num classes 10 num epochs 200 convert datasets to floating point types x train x train astype float32 x test x test astype float32 normalize the training and testing datasets x train 255 0 x test 255 0 convert class vectors target to binary class matrices or one hot encoded values y train tf keras utils to categorical y train num classes y test tf keras utils to categorical y test num classes print ndimensions of training and testing sets are print f x train shape x train shape y train shape y train shape print f x test shape x test shape y test shape y test shape dimensions of training and testing sets are x train shape 60000 28 28 1 y train shape 60000 10 x test shape 10000 28 28 1 y test shape 10000 10 x200b specify latent space dimensions latent space dim 3 define encoder encoder input input shape 28 28 1 x conv2d filters 32 kernel size 3 strides 2 padding same encoder input x leakyrelu x x conv2d filters 64 kernel size 3 strides 2 padding same x x leakyrelu x x conv2d filters 64 kernel size 3 strides 1 padding same x x leakyrelu x x conv2d filters 64 kernel size 3 strides 1 padding same x x leakyrelu x shape before flattening k int shape x 1 x flatten x instead of connecting the flattened layer directly to the 3 d latent space we connect it to layers mu and log var mu dense units latent space dim x log var dense units latent space dim x the keras model that outputs the values of mu log var for a given input image encoder mu log model encoder input mu log var print f shape before flattening shape before flattening shape before flattening 7 7 64 def sampling args mu log var args epsilon k random normal shape k shape mu mean 0 0 stddev 1 0 return mu k exp log var 2 epsilon this lambda layer samples a point z in the latent space from the normal distribution defined by the parameters mu and log var encoder output lambda sampling mu log var the keras model that defines the encoder â€” a model that takes an input image and encodes it into the 2d latent space by sampling a point from the multivariate normal distribution defined by mu and log var encoder model encoder input encoder output decoder input input shape latent space dim x dense np prod shape before flattening decoder input x reshape shape before flattening x x conv2dtranspose filters 64 kernel size 3 3 strides 1 1 padding same x x leakyrelu x x conv2dtranspose filters 64 kernel size 3 3 strides 2 2 padding same x x leakyrelu x x conv2dtranspose filters 32 kernel size 3 3 strides 2 2 padding same x x leakyrelu x x conv2dtranspose filters 1 kernel size 3 3 strides 1 1 padding same x x activation sigmoid x decoder output x decoder model decoder input decoder output the complete autoencoder the input to the autoencoder is the same as the input to the encoder model input encoder input the output from the autoencoder is the output from the encoder passed through the decoder model output decoder encoder output the keras model that defines the full autoencoderâ€”a model that takes an image and passes it through the encoder and back out through the decoder to generate a reconstruction of the original image model model model input model output the loss function is defined as follows weight the reconstruction loss r loss factor to ensure that it is well balanced with the kl divergence loss r loss factor 1000 def vae r loss y true y pred reconstruction loss r loss k mean k square y true y pred axis 1 2 3 return r loss factor r loss def vae kl loss y true y pred kl divergence loss kl loss 0 5 k sum 1 log var k square mu k exp log var axis 1 return kl loss def vae loss y true y pred vae loss reconstruction loss kl divergence loss r loss vae r loss y true y pred kl loss vae kl loss y true y pred return r loss kl loss compile model model compile optimizer tf keras optimizers adam learning rate 0 003 loss vae loss metrics vae r loss vae kl loss train autoencoder training hist model fit x x train y x train batch size batch size shuffle true validation data x test x test epochs num epochs which gives the error typeerror traceback most recent call last appdata local temp ipykernel 11960 995477119 py in module 1 train autoencoder 2 training hist model fit 3 x x train y x train 4 batch size batch size shuffle true 5 validation data x test x test anaconda3 envs tf cpu lib site packages tensorflow python keras engine training py in fit self x y batch size epochs verbose callbacks validation split validation data shuffle class weight sample weight initial epoch steps per epoch validation steps validation batch size validation freq max queue size workers use multiprocessing 1191 r 1 1192 callbacks on train batch begin step 1193 tmp logs self train function iterator 1194 if data handler should sync 1195 context async wait anaconda3 envs tf cpu lib site packages tensorflow python eager def function py in call self args kwds 883 884 with optionalxlacontext self jit compile 885 result self call args kwds 886 887 new tracing count self experimental get tracing count anaconda3 envs tf cpu lib site packages tensorflow python eager def function py in call self args kwds 931 this is the first call of call so we have to initialize 932 initializers 933 self initialize args kwds add initializers to initializers 934 finally 935 at this point we know that the initialization is complete or less anaconda3 envs tf cpu lib site packages tensorflow python eager def function py in initialize self args kwds add initializers to 757 self graph deleter functiondeleter self lifted initializer graph 758 self concrete stateful fn 759 self stateful fn get concrete function internal garbage collected pylint disable protected access 760 args kwds 761 anaconda3 envs tf cpu lib site packages tensorflow python eager function py in get concrete function internal garbage collected self args kwargs 3064 args kwargs none none 3065 with self lock 3066 graph function self maybe define function args kwargs 3067 return graph function 3068 anaconda3 envs tf cpu lib site packages tensorflow python eager function py in maybe define function self args kwargs 3461 3462 self function cache missed add call context key 3463 graph function self create graph function args kwargs 3464 self function cache primary cache key graph function 3465 anaconda3 envs tf cpu lib site packages tensorflow python eager function py in create graph function self args kwargs override flat arg shapes 3296 arg names base arg names missing arg names 3297 graph function concretefunction 3298 func graph module func graph from py func 3299 self name 3300 self python function anaconda3 envs tf cpu lib site packages tensorflow python framework func graph py in func graph from py func name python func args kwargs signature func graph autograph autograph options add control dependencies arg names op return value collections capture by value override flat arg shapes acd record initial resource uses 1005 original func tf decorator unwrap python func 1006 1007 func outputs python func func args func kwargs 1008 1009 invariant func outputs contains only tensors compositetensors anaconda3 envs tf cpu lib site packages tensorflow python eager def function py in wrapped fn args kwds 666 the function a weak reference to itself to avoid a reference cycle 667 with optionalxlacontext compile with xla 668 out weak wrapped fn wrapped args kwds 669 return out 670 anaconda3 envs tf cpu lib site packages tensorflow python framework func graph py in wrapper args kwargs 992 except exception as e pylint disable broad except 993 if hasattr e ag error metadata 994 raise e ag error metadata to exception e 995 else 996 raise typeerror in user code c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras engine training py 862 train function return step function self iterator c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras engine training py 852 step function outputs model distribute strategy run run step args data c users arjun anaconda3 envs tf cpu lib site packages tensorflow python distribute distribute lib py 1286 run return self extended call for each replica fn args args kwargs kwargs c users arjun anaconda3 envs tf cpu lib site packages tensorflow python distribute distribute lib py 2849 call for each replica return self call for each replica fn args kwargs c users arjun anaconda3 envs tf cpu lib site packages tensorflow python distribute distribute lib py 3632 call for each replica return fn args kwargs c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras engine training py 845 run step outputs model train step data c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras engine training py 803 train step loss self compiled loss c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras engine compile utils py 242 call self loss metric update state c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras utils metrics utils py 88 decorated update op update state fn args kwargs c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras metrics py 171 update state fn return ag update state args kwargs c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras metrics py 403 update state sample weight weights broadcast ops broadcast weights c users arjun anaconda3 envs tf cpu lib site packages tensorflow python ops weights broadcast ops py 157 broadcast weights values ops convert to tensor values name values c users arjun anaconda3 envs tf cpu lib site packages tensorflow python profiler trace py 163 wrapped return func args kwargs c users arjun anaconda3 envs tf cpu lib site packages tensorflow python framework ops py 1566 convert to tensor ret conversion func value dtype dtype name name as ref as ref c users arjun anaconda3 envs tf cpu lib site packages tensorflow python framework constant op py 346 constant tensor conversion function return constant v dtype dtype name name c users arjun anaconda3 envs tf cpu lib site packages tensorflow python framework constant op py 271 constant return constant impl value dtype shape name verify shape false c users arjun anaconda3 envs tf cpu lib site packages tensorflow python framework constant op py 288 constant impl tensor util make tensor proto c users arjun anaconda3 envs tf cpu lib site packages tensorflow python framework tensor util py 435 make tensor proto values np asarray values c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras engine keras tensor py 254 array raise typeerror typeerror cannot convert a symbolic keras input output to a numpy array this error may indicate that you re trying to pass a symbolic value to a numpy call which is not supported or you may be trying to pass keras symbolic inputs outputs to a tf api that does not register dispatching preventing keras from automatically converting the api call to a lambda layer in the functional model â€‹ x200b help
rwr9kb,1,blog on knowledge distillation my first attempt at writing an ml blog any comments suggestions are welcomed
rcfb33,1,requirements for an entry level job hi there i m curious about what are the requirements for every level job regarding machine learning i m not originally from computer sciences i m from food sciences engineering but i do have long experience in creativity and problem solving 3 years as r d i ve recently learned python and learning machine learning data mining and scraping since last month today i m starting to apply for jobs what do you think should be requirements of an entry level jobs in machine learning
r4h993,1,sentiment analysis ideas hi all i m working on a final project for one of my classes involving sentiment analysis on a data set of imdb movie reviews data set courtesy of keras it s a fairly straightforward binary classification classify the review as positive or negative the thing is i m a little short on ideas with regards to how to accomplish this i ve already utilized a few models but i feel like they re a little simple and in the interest of getting into the spirit of things i was wondering if there were more advanced techniques that a novice like me could still use for what i ve already done all inputs are word embeddings of the dataset imported from keras 1 cnn 2 lstm 3 transformer 4 cnn lstm 5 lstm svm a few things i ve been looking at but not sure about implementing 1 generative discriminative models the generator would perform feature extraction on the data 2 data augmentation techniques to use on the data and then feed into the aforementioned models thus far they ve been fairly simple like swapping in synonyms or antonyms randomly deleting or adding words etc any recommended course of action source for the two ideas i ve been looking at and do you have any other suggestions that i could feasibly implement i m using google colab got a premium membership and the data set isn t very large 25 000 training samples so computational expense should not be a concern i d appreciate any suggestions you guys have
qww285,0,anyone regret coming to this field if yes which path would you have taken edit since y all giving me helpful awards and this post has blown up i would appreciate a gold because i am a broke college student who don t want to pay for reddit premium
r2kiya,0,cross validation leave one out and early stopping doubt hi i m using 10 f cross validation to evaluate the performance of my model on the whole dataset due to its size limit same thing with leave one group out which use as validation fold only images coming from the same source to evaluate if having images from the same source in the train or the test introduces a bias with this in mind i m using tensorflow and the early stopping with restore best weights set to true looking at the results i m getting a huge improvement on average performances using loo and reasoning on it i can understand why early stopping focuses on reducing overfitting in each fold being the folds in loo composed mainly by one to four images early stopping restore weights when my model is able to correctly classify that images when only one image is in the validation fold it stops whenever it s able to correctly classify that image my question is can i use cross validation leave one out along with early stopping does it falsificates the results i can t find any argument about this in the literature or anywere
qzoad5,0,dataprofiler scaleable sensitive data detection analysis on structured unstructured files hello all we created a library to be the one stop shop for data exploration and monitoring the project had two objectives 1 quickly and accurate cheaply identify sensitive data pii npi in datasets 2 generate data profiles which can be utilized in downstream ml applications regarding sensitive data detection we published a workshop paper on the model within the library sensitive data detection with high throughput neural network models for financial institutions in addition to sensitive data detection the library also calculates statistical features and general characteristics of a dataset this has helped our team quickly evaluate datasets but also enabled the profiles use in downstream applications some nifty features the community may be interested in load files with a one command data dp data filename profile data with a single command profile dp load data save load profiles profile save dp profiler load filename merge profiles profile1 profile2 compare profiles profile1 diff profile2 extending the current entity detection model with transfer learning is easy and takes only a few lines of code or retrain from scratch it s possible though a tad rough to add a new custom model for entity detection generally we are looking for feedback and curious what the community thinks of the project
regev9,0,automating the process of finding and fixing bugs in ml code hi everyone i noticed that i regularly spend 30 50 of my time debugging my machine learning models most of the time ml models will work even with bugs so you have to be really careful and not introduce them in the first place in an attempt to speed up the debugging process i tried using static linters pyflakes pylint and it looks like they work for syntax errors only and are very basic as a result i decided to take the matter into my own hands and build an ml model that fixes bugs in ml models please let me know what you think would really appreciate any feedback
rij20m,0,internship after ml phd hello everyone i recently submitted my phd thesis focused on optimization and rl at a university in europe since my advisor was against internships and my funding didn t allow for one i graduated without any internship experience and it is difficult to land a full time job i applied for many full time roles but i got rejections almost all the time in my case does it make sense to apply for internships at big companies i see that faang companies are hiring a lot of interns nowadays do you have any suggestions thanks a lot for your help
ruudv3,1,how to save augmented images hi iâ€™m currently using data augmentation for training my model this is the code i use for augmentation def augment image image label flips the image randomly image tf image random flip left right image image tf image random flip up down image increase the image size then randomly crop it down to the original dimensions resize factor random uniform 1 1 2 new height math floor resize factor input shape 0 new width math floor resize factor input shape 1 image tf image resize with crop or pad image new height new width image tf image random crop image size input shape image tfa image translate image hshift tf random uniform shape minval 1 maxval 1 vshift tf random uniform shape minval 1 maxval 1 interpolation nearest fill mode nearest vary the brightness of the image image tf image random brightness image max delta 0 2 image tfa image rotate image max rot angle tf random uniform dtype tf float32 interpolation nearest fill mode nearest return image label train dataset train dataset map augment image tf data experimental autotune i want to visualize the data augmentation which is going through iâ€™m have used imagedatagenerator method of augmentation there will be option to save in a dir in flow api or flow from directory api but currently i couldn t use that method of augmentation in my scenario is there way to save the augmented images using this method thanks and regards ramson jehu k
rly1g0,1,new to r i am planning to start the google data analytics professional certificate course on coursera i have also started watching a bunch of r youtube videos and practicing with r studio am i on the right path or is there a better place to learn r
rv91cx,1,how to make an ai similar to that of jarvis from iron man i just started coding but the eventual goal is to make an ai that could be similar to jarvis from iron man i know that there s a lot of stuff i need to download so i was wondering if anyone knew what i needed to and the safest ways to do it also i m using python sorry if this isn t really what the sub is meant for if there s a better place please send me there
ramg5l,1,gradient checking implementation not working most likely because of algorithm i am trying to implement gradient checking in java currently used with linear regression but it doesn t seem to work i want to check if my algorithm logistically makes sense because after checking via debugging all other methods seem to work but the actual result is wrong i ran a test case to find actual vs predicted values when running gradient check using x values of 1 2 3 and y values of 2 3 4 i got 2 84 calculated this by hand as well which means the algorithm i am using is wrong as the approximate gradient for the first weight theta0 which would be b in ax b the result i got from linear regression was 9 which is significantly different however i am more inclined to believe the linear regression is corrected as linear regression performs fine on the data but using the approximated values that gradient checking comes up with gives an inaccurate result while debugging my code i found no errors in the methods used so i am inclined to believe that it is an error with my algorithm the algorithm i am using is essentially the following i take my weights theta and iterate through them i add a value epsilon to the ith weight while keeping the rest of them the same next i calculate the cost with this new slightly changed weight vector and do the same thing except subtracting epsilon i then take the the two values and subtract them and divide by 2 epsilon these are my gradient approximations i add this to an array and repeat the process for all weights in the weight vector afterwards i calculate the euclidian distance between my gradient approximations and the gradients via gradient descent and divide that by the sum of the euclidian lengths of the approximation gradients and the true gradients this is the value i return i am posting a link to a gist which has my linear regression class that has the gradient check method this is the part that isn t working a few more notes dapprox is what i am calling my approximated gradient values and dtheta is the real gradient values traininputs is a matrix of the training data with a column of 1s attached before it trainoutputs is simply the outputs as a m 1 matrix where m is the number of data points in this case it is simply 2 3 4
rvui3a,1,subreddit for algotraders with ml methods in use hello guys i m myself a ml engineer and trying to use ml on trading even when a lot of people say it s not possible i know people who are doing it with high profits since this subreddit includes a lot of topics i just created a subreddit for only algotraders who are using or learning to use ml on stock trading here is the subreddit
r1h9ld,1,stackoverflow analysis hello everyone i am a cloud and data science noob as a personal project i want to do some analysis on stackoverflow posts topic modeling and then hopefully identifying useful trends and how they have evolved since 2015 i see something called sotorrent hosted on bigquery i would want to query all the posts and store them somewhere 100gb i cannot store it locally as i donâ€™t have enough storage on my laptop so my question is where could i store it such that it is accessible to my colab notebook where i will run some python code for analysis any ideas pointers over even suggestion for alternatives to the described flow would be super helpful
r5rkbv,1,making a bonding discord group for everyone hey everyone ive been wanting to make a bonding group of ppl in this discipline the thing is study groups often dont work as we sometimes work on very different stuff plus there is no incentive for most of us as we tend to treat it like a burden id just like to have a group where we can work mutually and independently chill and have someone to distract from the boredom of coding studying researching throughout the day the server will have a voice chat and a general text chat nothing more thanks guys
qt4y6g,0,paper overview mae masked autoencoders are scalable vision learners video paper abstract this paper shows that masked autoencoders mae are scalable self supervised learners for computer vision our mae approach is simple we mask random patches of the input image and reconstruct the missing pixels it is based on two core designs first we develop an asymmetric encoder decoder architecture with an encoder that operates only on the visible subset of patches without mask tokens along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens second we find that masking a high proportion of the input image e g 75 yields a nontrivial and meaningful self supervisory task coupling these two designs enables us to train large models efficiently and effectively we accelerate training by 3x or more and improve accuracy our scalable approach allows for learning high capacity models that generalize well e g a vanilla vit huge model achieves the best accuracy 87 8 among methods that use only imagenet 1k data transfer performance in downstream tasks outperforms supervised pre training and shows promising scaling behavior
qvd8ay,0,what are some must read papers on video segmentation and summarisation i m looking to read up on the major advances and contributions in video segmentation and summarisation both static keyframes extraction and dynamic video skimming to this end what are some important and enlightening papers on this topic that you d recommend to read up on i m currently reading this survey paper by apostolidis et al but i was wondering if the community has any recommendations on must definitely read papers on the topic please feel free to even suggest any papers that don t directly address the task of video segmentation summarisation but introduce techniques architectures that have subsequently been applied extensively for the same
r2075f,0,not sure how to formulate this correctly has there been work done on learning what perception constitutes a reward and than using only that perception for learning let s say you have an agent in a maze searching for apples everytime it s viewpoint passes through an apple it gets a reward has there been an attempt to make the agent first learn the association between passing through apple and reward signal and then using only that perception for further training i feel one of the big problems we have is that we still can t give very detailed feedback to our agents and the reason for that is that the reward signal is subsymbolic so to speak so the agent cannot communicate or reason over it if an agent can learn to recognise a reward in it s perception we could communciate more effectively i m sure someone must have thought of something like this already but i don t know the term to google for
rqfx72,1,video s explaining technical side of algorithms hello everyone in a few weeks i have an exam coming up about machine learning and inductive inference in this course we look at the more technical aspects of learning techniques like naive bayes support vector machines theta subsumption etc and the mathematical concepts behind them however the class was rather vaguely explained my question for you is are there any good websites where i can find videos on these more technical details aside from youtube thanks
ric7vd,0,why vit does not beat cnn in the field of deep generative model recently vit beats the cnn in many field proving that vit is superior backbone network than cnn there are some papers who tried to use vit as a discriminator architecture in gan but their results simply do not look so good why vit does not beat cnn in the field of deep generative model
r7lc1e,0,unsupervised outlier detection advise requested hi all i am working on a strategy to detect outliers mostly multivariate data using unsupervised methods currently i am using dbscan optics in one group kmeans finding points that are 3 std from the mean of each group should be similar to centroid 3 standard deviations for a second group and lastly isolation forest and copod for a third group some of the output from each group could overlap with other groups but not all since each method finds different outliers in part of the spectrum each model is executed with point in time data in order to find outliers for that moment with respect to the values present at that time and not trained in a first step and then the model applied to the incoming data since i don t have regular outlier values and want to consider that point in time with incoming values regardless of what might have happened in the past since values might be affected in time by a number of factors is this a sensible approach would you suggest something different would you add any method either in parallel to the ones i mentioned or at the end like voting or anything else thanks in advance
r2przt,1,where can i get data related to company s cost of equity capital i have a list of company identifiers gvkeys sic permno etc and need the associated cost of equity capital i don t mind which method is used to estimate the cost of equity capital i do have access to standard databases like compustat crsp ibes auditanalytics eventus etc
rnp2vk,1,tips tricks of deploying deep learning webapp on heroku cloud kdnuggets hi guys i wrote this blog because i was facing a lot of problems in deployment so i created guide for deploying deep learning models on heroku i hope you guys can avoid making common mistakes and please like and share it will mean a lot
qwn3fz,0,dagyard dvc x mlflow x colab x gdrive automatically configured hey r ml dvc and mlflow tracking are two of the most preferred open source tools for managing ml projects they are heavily used by data scientists widely adopted in the industry with a great community team behind them if you re not familiar with them then in a nutshell dvc data version control acts as an extension to git and enables version control of large scale files mlflow tracking automates the logging process of experiments and sends live information to a local or remote server while the training is still running our aspiration at dagshub is to help data scientists to use great open source tools by lowering the entry bar my recent project the dagyard notebook automates the process of configuring google colab with dvc and mlflow by simply checking some boxes filling your user s details and well that s it you re all set to clone the repo and train your model your colab will be fully configured with dagshub dvc mllflow and based on your use case also github and google drive on top of those i spiced it up with additional capabilities like using google drive as dvc s cache directory and avoid pulling the same file twice copy files from gdrive to dvc storage initialize dvc clone a specific branch and more check it out here ðŸ‘‰ if you have any ideas on how i can improve the notebook or what additional capabilities you re missing here i d love to hear about it
qjx4k3,0,iccv2021 oral neural tmdlayer modeling instantaneous flow of features via sde generators with video explanation our tmdlayer is inspired by stochastic differential equation sde and aims to model the stochastic flow of features in principle it can be easily added on top of any dnn layer to bring the benefits in addition it immediately enables transductive inference once inserted into the model welcome to check out our video for a quick and easy understanding video paper code x200b our paper abstract we study how stochastic differential equation sde based ideas can inspire new modifications to existing algorithms for a set of problems in computer vision loosely speaking our formulation is related to both explicit and implicit strategies for data augmentation and group equivariance but is derived from new results in the sde literature on estimating infinitesimal generators of a class of stochastic processes if and when there is nominal agreement between the needs of an application task and the inherent properties and behavior of the types of processes that we can efficiently handle we obtain a very simple and efficient plug in layer that can be incorporated within any existing network architecture with minimal modification and only a few additional parameters we show promising experiments on a number of vision tasks including few shot learning point cloud transformers and deep variational segmentation obtaining efficiency or performance improvements
qmw9lr,0,survey study examining practices in nlg evaluation do you work or do research on natural language generation nlg if yes we are interested in your participation in a 20 minute survey about practices when evaluating nlg systems or models the participants should have experience with working with or on any type of natural language generation nlg systems and tasks the purpose of this research is to uncover unnamed practices and assumptions made during the evaluation of nlg systems applications and tasks we hope that by understanding such practices and assumptions we will be able to better unpack the ways they could lead to unintended consequences related to fairness and inclusion if you are interested please fill out this form here thank you so much for your consideration and help
r09639,0,deepsquare news deepnews is our weekly update where we bring the latest news from the deepsquare project and our quest to develop sustainable high performance computing as a service ecosystem last week we talked about artificial intelligence and high performance computing with dr florin dzeladini today we will delve into deepsquareâ€™s heart and soul sustainability by the hand of one of our board members frÃ©dÃ©ric juillard if you have been following the project for a while you will have realized our strong focus in sustainable hpc and energy reuse and that is because we truly mean it in deepsquare we are determined to take the next step towards sustainable hpc as a service and offer the best environmentally friendly alternative to the current status quo through the creation of a decentralized infrastructure backed by a professional team and cutting edge technology we have managed to create a growing community whose values align with our vision project update we are happy to be able to share the news that deepsquare supported our friends in alan analytics in delivering a project they worked on together with artist agnieszka kuran t the project resulted in ai generated â€œnewâ€ cave art based on a collection of data documented by paleoanthropologist genevieve von petzinger we used our test environment and csquare application to enable ai generated cave paintings it was really interesting to combine hpc ai and art and merge it into this fantastic project meet the team frÃ©dÃ©ric juillard is one of deepsquareâ€™s board directors and a serial entrepreneur with an engineering background and sustainable deep tech having previously been involved in the foundation of other high tech companies he naturally fits within the character of the deepsquare association and his day to day work includes supporting the business and administration team and leading partnerships frÃ©dÃ©ric is above all a team player and believes sustainability to be â€œa key factorâ€ of the project and the bedrock around which to gather a strong community â€œmy favorite aspect is to be a part of this shared dream that we all have in the project and that more people believe in every day to see our community grow and on board with the same vision is something that i truly appreciate this social aspect interacting with people from very different backgrounds cultures and nationalities is something that perfectly aligns with the core values of deepsquare and the world of todayâ€ the community is indeed a key point for the future of sustainable hpc as a service every day more people are aware of the challenges coming ahead for the hpc industry which is already consuming a vast amount of the energy produced on the planet and will continue to grow in the next decade frÃ©dÃ©ric understands that only a shared effort rooted in a â€œtrustful philosophy and backed by an extremely well prepared teamâ€ can face these future demands â€œthe compute power that our world needs today is something absolutely crazy and these gross numbers are only going to keep growing society will shortly need to meet the demand for new technologies such as 5g autonomous cars smart cities and ever increasing connectivity sustainability is something that every innovative business should integrate there are clearly a lot of things to improve in hpc in terms of sustainability and i am certain deepsquare is part of the solutionâ€ to overcome these challenges the project has the latest hardware in the industry and a conscious plan to regularly update it to match the industryâ€™s demands always with sustainability in mind frÃ©dÃ©ric believes it is â€œextremely important to find the right channel so this hardware can have a second lifeâ€ and thus mitigate the damaging consequences of thoughtless it hardware disposal deepsquare clusters are thus designed with a green mindset but also tackle the growing needs of compute power coming down the line â€œstandard hardware is already extremely efficient when converting a watt of electricity into a watt of heat and by using an immersion cooling system along with a system to reintroduce that heat into the building we have the best way to make a good and ecological use of otherwise wasted energy thatâ€™s clearly the most important aspect where the project can tackle the environmental problem and why in deepsquare we deliver sustainable high performance computing as a serviceâ€ with the support of our community we are every day closer to democratizing high performance computing with better execution cost and social responsibility to learn more about deepsquare check out our website or if you want to connect with the team and the community follow us on twitter linkedin or join our telegram group
r8w4s7,1,feeling weirdly motivated and demotivated at the same time i am taking a class with all beginners in ml we got a dataset and out of 200 students i am one of the lowest even though i tried pretty hard i have ran a full searchgridcv for a random forest tree and can t get it above 68 even though more than 60 of the students have an accuracy over 70 i have also tried the same for xgboosting same story stuck at 67 due to covid the course started offline and i don t really know anyone so i can t ask them it s the middle of the weekend and it s killing me the dataset is pretty clean already but i removed the features of low importance that gave a small boost but didn t help much somehow it demotivates me but at the same time i feel obsessed trying to get to that 70 and learn more about the algorithms has anybody ever been in the same spot and has some tips for when you feel like this
r75uix,0,the new library to make clip guided image generation simpler there are different ways to generate images by their text descriptions but one of the most powerful approaches to generate synthetic art is clip guided image generation we provide a new python library that incapsulates the whole logic of the clip guided loss into one pytorch primitive with a simple api we provide clip guided loss using different clip models such as original clip models by openai and ruclip model by sberai multiple prompts texts or images as targets for optimization and automatic detection and translation of the input texts also we provide our tiny implementation of the vqgan clip based on our library and vqvae by sberai in my opinion this is the best version of the vqgan that is publicly available to make text to image our library is all you need to integrate text powered losses into your image synthesis pipelines by adding a few lines of code you can find our library here pypi package is available
rcdhkt,0,learning multiple gaits of quadruped robot using hierarchical reinforcement learning hello we share our results of learning multiple gaits of quadruped robot using hierarchical reinforcement learning we simply parameterized the policy output considering the periodic features of different gaits although currently there are some limitations we hope the proposed simple method could give insights to other researchers in related fields if you are curious of the methods and results in detail check the paper slides and code linked below enjoy title learning multiple gaits of quadruped robot using hierarchical reinforcement learning abstract there is a growing interest in learning a velocity command tracking controller of quadruped robot using reinforcement learning due to its robustness and scalability however a single policy trained end to end usually shows a single gait regardless of the command velocity this could be a suboptimal solution considering the existence of optimal gait according to the velocity for quadruped animals in this work we propose a hierarchical controller for quadruped robot that could generate multiple gaits i e pace trot bound while tracking velocity command our controller is composed of two policies each working as a central pattern generator and local feedback controller and trained with hierarchical reinforcement learning experiment results show 1 the existence of optimal gait for specific velocity range 2 the efficiency of our hierarchical controller compared to a controller composed of a single policy which usually shows a single gait codes are publicly available paper slides code contact awesomericky snu ac kr mailto awesomericky snu ac kr
r4wd3a,1,how to illustrate in a fairly simple manner how roberta gets fine tuned on a downstream task my thesis defense is coming up next week and i wanted to have your take on an issue i m currently facing one of my thesis contributions is adapting roberta to the task of rumor detection on twitter i want to explain to the jury how roberta can adjust its weights based on the dataset that i fine tune it on in simple terms i fed roberta a variety of datasets describing the task of rumor detection on twitter while altering the class distribution in the datasets to see how it influences the embedding that roberta produces i evaluated the quality of the embeddings by feeding them to a set of classifiers random forest decision tree svm to see how they perform i used standard metrics precision recall and f1 score focusing on the model performance in recognizing the class rumor i was considering explaining it this way roberta takes in a tweet with a label rumor non rumor then it weighs the words and their impact on the class in question and words that occur often in a class are the ones that are potentially correlated to it but i feel like that s too much watering down and even an insult to the intricacy involved in roberta s inner workings so for you out there with much more knowledge and expertise than me will you please indulge my request and enlighten me on how one can explain the details of fine tuning pre trained language models on a downstream task
rc4yiw,0,imputation methods for missing data maybe it is simple question but i want to see your opinions on this i have small dataset of about 2000 3000 samples that were third party collected since physicians were collecting most of that data they most of time think like â€œ oh i forgot to ask this question to patient or to do this analysis who caresâ€ problem is that 5 10 of data is missing but in different categories which makes it impossible to remove so i wanted to check your opinion about imputation methods
rqjpre,1,does the negative of the gradient always go downhill in gradient descent or sgd is it always the case that the negative of the gradient goes in the direction of steepest descent
qnh7w4,0,why jupyter notebook doesnt store requirements require packages in ipynb file the ipynb file is a json file list with required packages can be easily added there why there is a separate file for this
rkkpcg,0,how much can a single 3d model replace a dataset example for the sake of argument i love my epoxy encased hotdog more than life itself he likes to travel around the world and i want to make sure he s safe i have every camera in existence to try to track his location at all times of the day or night but i can t collect data from them for training before he left i took a perfect 3d scan of him so i can simulate angles lighting conditions resolutions occlusions and optical parameters so no matter what camera i m using i can find him i have no tolerance for false negatives in case my baby ever needs my help x200b to put it more seriously as 3d ml techniques evolve and datasets like objectron and co3d help us isolate objects do simulated datasets begin to play a bigger part as well are there any good controlled experiments of detection between training with really good renders vs real world data are there companies focusing on custom data augmentation right now
rjqgrc,0,one to many mapping using probabilistic unet hi i have been working on this project for over a month now however i couldnâ€™t make it work yet so i need help it is basically applying the probabilistic unet paper on the leaf segmentation dataset given an image i want to output one instance at a time by sampling from a distribution the probabilistic unet architecture is shown below x200b x200b the architecture has two loss components the kl divergence and reconstruction loss when i train it the kl loss goes to zero however the reconstruction loss works differently that what i want what i want 1 sample a noise 2 get one instance x200b x200b what actually happens is that the noise is ignored it acts like a semantic segmentation basically the rgb image and the different instances are mapped to the same latent space i have tried beta scheduling kl and even removed the kl term but the noise is still ignored x200b x200b x200b can someone be kind enough to review my code here or explain why this approach might not work thank you
rbj8vg,1,interpreting clustering results hi all beginner here i did kmeans clustering for customer data and added the cluster labels back to the original data which will then be exported as a csv cluster labels in an additional column wondering how do we do the analysis now for the customer personna ideally we want to be able to say eg cluster 1 are the well to do men in their 40s cluster 2 are the women in technology in their 20s etc do we do pivot tables and averages in the excel or do we intepret from the centroid table in jupyter notebook below here is an example of the centroid table output from jupyter notebook different from actual but how would one interpret this would i say for example cluster 1 is the group of people with lowest cooling tower frequency and relatively low humidity etc cluster 4 contains people with the highest cooling tower frequency and highest relative humidity thanks so much in advance
rkv8zg,1,advice on multiple linear regression with sklearn so i am new to machine learning and i want your thoughts if i m doing this right it s probably not the best for prediction but i m doing this to combine a love of sports and to practice on things that i m learning through school the dataset i m using is 5 years worth of english premier league data in short i ve created a program to get the standings of a given year through x many games that i want it includes w l d points gf and ga in the end dataset i have 5 years worth of data for the teams up to the xth game of the season using sklearn i built a multiple linear regression model that uses the gf and ga to predict points for whatever x amount of games played that i wanted the end dataset that i just previously mentioned was what i used to do the training and testing with so now with that model being trained and tested i use the current season s standings through the x amount of games to predict how many points the team should have i think it s doing the job well as most of the points predicted are pretty close to the actual points now my big question say this season gets finished should i add this season to my training and testing dataset and rerun the model so that i maintain accuracy for next season s predictions
r9lzpr,1,project advanced methodology of robust optimal design on the base of agent oriented approach by means of reinforced learning i am addressing to you with hope to set the business contacts in the areas of machine learning methods in multi objective problems of robust optimal design and intellectual diagnostics of systems under uncertainty the goals of robust optimal design rod are to achieve maximum efficiency of the system being developed and or reduction of rejects in the manufacture of a batch of products algorithm for the numerical solution of the multicriteria multidisciplinary rod problem on the base of agent oriented approach by means of reinforced learning 1 the values of the confidence intervals for the design parameters of the system manufacturing accuracy based on the available technological equipment the confidence intervals of the regime variables measurement accuracy for the system prototype are known 2 a training sample of alternatives analogs of the designed system is generated on a set of design parameters and regime variables each alternative can include different subsets of values design parameters regime variables objective functions the values of the objective functions can be obtained by cfd modeling of processes in analogs of the designed system we supplement the training set with data on the prototype 3 searching of the robust metamodels of processes in the form of analytical dependences of objective functions on their variables using discrete data on analogs and prototype 4 the choice of the target values of the average values of the objective functions and the corresponding of them confidence intervals for the designed system assuming that a batch of n systems will be manufactured 5 reducing the dimension of the space of design parameters based on the analysis of the informativeness of the metamodel variables sensitivity analyzes 6 searching of the solutions of multicriteria optimization problem in a deterministic formulation 7 determination of the values of the average values of the objective functions and the corresponding of them confidence intervals for the prototype and the optimal variant according to paragraph 6 assuming that a batch of n systems will be manufactured monte carlo analyzes 8 searching of the solutions of multicriteria optimization problem in a stochastic formulation the results of solving this problem will be rational values of the average values of the design parameters which need to be found of the system as well as the corresponding of them confidence intervals the required manufacturing accuracy ensuring the conditions according to paragraph 4 thus solutions of the direct problem of calculating dimensional design chains can be obtained to determine the rational values of the average values of the design parameters and control variables of the system as well as the corresponding confidence intervals of the quantities which need to be found according to the definitions accepted in mathematics the elements of the pareto set or normal solutions will be obtained for the given target values of the average values of the objective functions and the corresponding of them confidence intervals 9 cfd modeling of processes in the optimal variant of system the design parameters and control variables of which are obtained in paragraph 8 calculation verification determination of the accuracy of the numerical solutions obtained using the proposed numerical model paragraphs 3 8 in comparison with the solutions using the original cfd models 10 reinforcement learning we supplement the training set point 2 with the new alternatives based on the results of calculations of processes in the optimal variant of system the design parameters and control variables of which were obtained in paragraph 8 11 we continue the process of finding solutions of the multicriteria multidisciplinary rod problem according paragraphs 2 9 look figure based on agent oriented approach by means of reinforced learning until the specified solution accuracy is achieved the given statements are reliable as they had confirmed by the experience of using â€œrod idsâ€ software in different fields of activity look please on articles and presentations of our results in the section achievements publications www linkedin com in mykhaylo ugryumov 63148313b we have the right to consider ourselves as specialists in the development of effective machine learning methods mlmâ€™s for solving â€œrod idsâ€ problems we offer you our software in order to resolve specific problems or we can do calculations on our own for you in addition we are always open to have contacts and discuss how our methodology computational methods and it realization theirs in form of interactive compute decision making support software system â€œrod idsâ€ can be helpful for your company we hope that your use of our mlmâ€™s will increase customer demand for new versions of your software
r73sq8,1,multi task learning how s that done hi guys i ve recently heard about the concept of multi task learning i ll call it mtl i ve read some articles online and have watched some youtube videos on the topic however there are some aspects that i don t understand assume that we have a project that we want to do mtl on 3 seperate tasks what i know is there is a base model to extract features and then there are three models on top of that to do the tasks what i don t understand is how do we train these models is it done simultanously if yes there will be three set of gradients backpropagated for each task to update the base model then ho do we handle that if no then how do we train the base model
rdtaiy,0,discussion quick reminder servers go up to 24tb of ram i ve seen this sub lamenting a bunch of times about large companies having models only large companies can run and use the latest such is a bunch of comments in the deepmind thread maybe i am missing something but i think there is a chance that people here have an outdated view of computers so i want to just state a few facts and understand if given these limitations there would still be any problem for a normal research lab to run large models currently outside of super computers and the latest 2021 hardware the most ram a motherboard can fit is 24 5tb running with 8x intel xeons and available on e g aws good intel xeons are very fast i e probably faster than an old gen cuda 11 compatible gpu even for gpu optimized libraries ala torch much faster than all but a few gpus with cpu optimized optimization algorithms hash map based are used server motherboards can also fit up to 8gpus i think this number should actually be 32 but i ve never seen an offer with more than 8 so eh most cloud providers have 16gb gpus with up to 8gpus per server the largest gpus run 80gb of ram though they are not widely available the largest gpu optimized servers boast 1 3t of gpu ram x200b price wise buying said gpu servers are hard to pin down but seem to be in the 200 600k range i assume they are or will soon be available to rent and that price will translate to 200 600 hr for expensive providers and maybe as low as 50 100 for cheap ones with older models but this is an uneducated estimate based on cpu server pricing compared to buy price prices for the 6 2tb server are 50 hr and presumably the 24 5tb version is 200 hr it seems to me like both the cpu and gpu routes here should allow for training and certainly running very big model more so than the biggest models from either google or openai the price point is high but operating at 200 hr or putting down half a million for hardware every 1 3 years seems more than reasonable for a small time well funded lab given that most researcher salaries are 1 4th to 1x that amount given that often enough you can do research work on models that are smaller and extract valid heuristics for models 10 100x the size this is not necessary obviously this is prohibitive for most people i can t run it on my laptop but it seems unfair to say that large text or image models are limited to a few giant corporations with their server farms it seems that a bloke with no knowledge of clustering computers and half a million worth of funding could easily work with models the size of gpt 3 not only for inference but also for testing this seems to increase the availability of such models to something like a few dozen thousand companies and a few hundred or even thousands of well funded academic labs
qriz01,0,landmark annotations in blender i am building a synthetic dataset of images for a landmark prediction task and i m using blender having looked through the main data generation libraries available for blender on github vision blender blenderproc zpy i can t find any that support landmarks before i go and implement this myself does anyone have any pointers that i m missing thanks update the following script will write out the coordinates of vertices in a rendered image import bpy scene bpy data scenes scene camera bpy data objects camera obj bpy data objects cube matrix camera matrix world normalized inverted create a new mesh data block using the inverse transform matrix to undo any transformations mesh obj to mesh preserve all data layers true mesh transform obj matrix world mesh transform matrix get the world coordinates for the camera frame bounding box before any transformations frame v for v in camera data view frame scene scene 3 lx ly for v in mesh vertices co local v co z co local z if z 0 0 vertex is behind the camera ignore it continue else perspective division frame v v z z for v in frame min x max x frame 1 x frame 2 x min y max y frame 0 y frame 1 y x co local x min x max x min x y co local y min y max y min y lx append x ly append y coords f x y n for x y in list zip lx ly with open log txt w as f f writelines coords
rsq1s1,1,how do load deep learning trained model pt into cluster does anyone give some understanding of trained model file pt to load into a cluster like apache spark or airflow for job scheduling if you have any resources repo to share will be highly appreciated
r515bm,1,imerit ml dataops summit â€“ techcrunch hear from top ai and ml leaders from facebook ai microsoft cruise ge healthcare and others revealing the latest trends in successfully deploying machine learning data operations at the imerit ml dataops summit co hosted by techcrunch ð‘ðžð ð¢ð¬ð­ðžð« ðŸð¨ð« ð…ð«ðžðž join 1800 attendees on dec 2nd and gain insights on ðŸ‘‰ why human in the loop data labeling is the critical path to achieving widespread production and adoption of ai applications ðŸ‘‰ overcoming edge cases and leveraging high quality proprietary data ðŸ‘‰ how leading ai companies are scaling their data pipeline ðŸ‘‰ the data labeling ecosystem and its future ð‚ð¡ðžðœð¤ ð¨ð®ð­ ð¨ð®ð« ð¬ð©ðžðšð¤ðžð«ð¬ ðšð§ð ð«ðžð ð¢ð¬ð­ðžð« ð­ð¨ððšð² mldataopssummit machinelearning robotics ai ml artificialintelligence
rk1rql,0,simplefeature a tiny python package with almost no dependencies that lets you extract deep local features check it out a tiny python package that requires almost no dependencies and lets you extract generalized deep features from any image built off of some older but still solid work from fg2011 anyone here have a cool use for something like this
rb8x7l,0,breast cancer analysis using opencv and ml dl i m working on a project and it is on the early stages it is a simple machine learning application i guess but i cannot find anything similar on kaggle to get references i have some macro breast cancer images you can find an example here and i want to first get the shape of the meat doing measurements simple algebra can do that and then get the cancer the white thing and measure how close it is to the border for the first part i can use canny edge detection get the largest contour and that s it but the last part is driving me crazy any suggestions i ve thought about using mask r cnn but i dont know how viable it is
rwxt8r,0,legal use of functions in pytorch or tensorflow does anybody know is it legal to use dropout or batchnorm from pytorch and tensorflow due to google s patents of these two functions did some library avoided patent infringement in its implementation of those functions
radt2v,1,how to pick a high level dl api in 2021 after years of resisting the push to move on from keras i decided it is time to find a new high level deep learning api for myself before its too late when looking up the best alternatives haiku and flax caught my attention i found it weird though that google would have to libraries with such similar approaches does anyone has experience working with both what should one take into consideration when picking one in addition to the aforementioned couple elegy also looks really interesting i must mention though the reason why i stayed with keras for so long is that there is just so much material out there for learning so many good tutorials and examples past projects my initial impression is that newer promising frameworks are still lacking on that front maybe i should change level of abstraction and develop everything on pytorch what do you guys think what frameworks do you currently use for dl
rhh5zr,0,project determining what a classifier thinks a rabbit looks like how it works trying to see what classifiers thought different classes looked like i ended up generating pictures for every class in imagenet those pictures did not look like what i was expecting though i should probably know better at this point lol i used an imagenet pretrained vgg 16 model for the classifier fun results of not rabbits evolving the generated picture from a gray image to something the classifier is a 100 sure is thing x200b x200b x200b i m surprised by the lack of the color orange in the generated image
r5uu53,1,introductory tensorflow course with a focus on computer vision hello there you might have seen my daily doses of ml posts today i want to share with you a free introductory tensorflow course that i put together it focuses on building convolutional neural networks for computer vision tasks check the course curriculum below before you decide maybe it s too basic for you you will also be able to ask me any question if you re stuck i hope to see you in class course link x200b course curriculum
rudaf0,1,how to encode sequence using convulutions hi i m working on a personal project where i m trying to encode fasta sequences and input them into a neural network however i m having trouble figuring out how exactly i can encode them first of all the length of these sequences is varying a typical sequence may look like ashhhhhhsytwtgalitpcaaeesklpinalsnsllrhhnmvyattsrsaglrqkkvtfdrlqvlddhyrdvlkemkakastvkakllsveeackltpphsakskfgygakdvrnlsskavnhihsvwkdlledtvtpidttimaknevfcvqpekggrkparlivfpdlgvrvcekmalydvvstlpqvvmgssygfqyspgqrveflvntwkskknpmgfsydtrcfdstvtendirveesiyqccdlapearqaikslterlyiggpltnskgqncgyrrcrasgvlttscgntltcylkasaacraaklqdctmlvngddlvvicesagvqedaaslrafteamtrysappgdppqpeydlelitscssnvsvahdasgkrvyyltrdpttplaraawetarhtpvnswlgniimyaptlwarmilmthffsillaqeqlekaldcqiygacysiepldlpqiierlhglsafslhsyspgeinrvasclrklgvpplrvwrhrarsvrarllsqggraatcgkylfnwavktklkltpipaasqldlsgwfvagysggdiyhslsrarpr x200b it s essentially a ton of letters where each letter is representing some sort of amino acid x200b to encode these i was considering using a series of convolutions however i don t know if this is the most ideal way to do so x200b what is the most ideal method to encode a sequence like this so it can be passed into a neural network more specifically im trying to use it for a reinforcement learning agent x200b also since the amount of text in a fasta sequence may vary i m having trouble figuring out how exactly i should deal with that
rm34a3,1,andrew ng ml course or fast ai i have heard that a lot of people recommend andrew ng ml course and others recommend fast ai as for as i know fast ai is more practical and nothing theoretical on the other hand andrew ng is more theoretical less practical i am new in ml journey and confused i want to choose between two what to choose if any other more beneficial resources you know please share that also
r9i0a8,1,categorical features in image classification i m training a cnn with tensorflow on a large dataset of images in order to predict a number of classes there are some obvious distinct groups of images in the training set which in a tabular data setting i would have included in training in the form of a categorical variable feature is there a way to do this with image data an obvious workaround would be to train two separate models one per group assuming 2 groups but is that really the best way i may be having more categorical features soon so having a separate model for all their combinations won t be practical any suggestions
r60cet,0,dask dataframe plotting without using too much ram same with pyspark can anyone give me a solution how to actually plot the whole dataset in pyspark and dask if they definitely don t fit in to memory i am interested in solutions that not just dump the whole thing in to numpy array but actually very light on ram ideally something that can plot by smaller batches and that keep the results in the ssd in dask i know the following df x y resample 24h mean compute plot
