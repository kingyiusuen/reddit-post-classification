label,text
2,help me understand how vaes assign probability density r e posteriour collapse i am trying to get a better understanding of how vaes model data in abstract and how this relates to posteriour collapse can you tell me which of the following statements are true or not 0 the vae optimizes a lower bound on p x int p x z p z dz typically the derivation starts from log p x but i m not asking about the elbo derivation let s assume that the lower bound works and suppose the bound is fairly tight 1 the integral int p x dx over all p x is 1 where p x means the marginal density assigned by the model but this is true only if the bound is tight in fact it will be below 1 due to the gap but hopefully not too far below 2 now suppose the model is flexible and we overtrain it so that it assigns as much probability as possible to the given data no early stopping or evaluation on held out data the training should produce spikes around the datapoints 3 if we integrate the density in a small radius around each spike this should capture most of the probability density we might expect that with n datapoints the probability density around each point would be about 1 n although random weight initialization could produce some other result 4a in concept a flexible decoder network can model this density of n spikes each with 1 n probability regardless of whether it makes use of the encoder thus posteriour collapse can happen 4b however this is not possible if the decoder produces a gaussian probability distribution at its output since the gaussian is unimodal and cannot model the n spikes without referring to the input i e posteriour collapse does not happen q z x p z and the decoder makes use of z q z x to produce a conditional p x z that changes for each datapoint 5 unlike a vae a plain autoencoder can be interpreted as assigning an overall probability density larger than 1 to the data using the equivalence between gaussian density and least squares reconstruction loss imagine an autoencoder that produces near perfect reconstruction maybe up to floating point roundoff e g perhaps it is a linear autoencoder and the data is generated from the subspace spanned by the decoder this produces arbitrary high density however the gaussian normalization results in the overall density integrating to 1 for that datapoint when summed over all datapoints the total probability density will be approximately n rather than 1 edit comment and clarification the overall question is about how the vae assigns probability to the data x rather than probability on the latent space the vae decoder ends by generating a density which is usually just set to a independent predicted mean and constent variance with no correlation across dimensions i m thinking about how the probability implied by this density is assigned across the training data my main confusion is considering the case without and with posterior collapse a without collapse if the model is working well it should assign high p x z to that particular datapoint and low density to other datapoints but p x still has to integrate to one across all the data so i think the probability assigned in the region of that datapoint integrating around its spike should be 1 n b in the case of posteriour collapse it the model has to assign probability 1 n to each spike so is the probability density assigned by the model the same regardless of whether posteriour collapse happens
0,brainstorming a ds case study from a big pharma company hello all not sure if this is allowed here or not but i received a ds case study from a big pharma company in usa i just wanted to see if there is anyone experienced in doing case studies and might want to help me brainstorm the problem and discuss possible methodologies and aligning the solution
2,prood provably robust detection of out of distribution data almost for free abstract when applying machine learning in safety critical systems a reliable assessment of the uncertainy of a classifier is required however deep neural networks are known to produce highly overconfident predictions on out of distribution ood data and even if trained to be non confident on ood data one can still adversarially manipulate ood data so that the classifer again assigns high confidence to the manipulated samples in this paper we propose a novel method where from first principles we combine a certifiable ood detector with a standard classifier into an ood aware classifier in this way we achieve the best of two worlds certifiably adversarially robust ood detection even for ood samples close to the in distribution without loss in prediction accuracy and close to state of the art ood detection performance for non manipulated ood data moreover due to the particular construction our classifier provably avoids the asymptotic overconfidence problem of standard neural networks code
1,how important is taking a course on sampling during a masters program i am currently in a m s in statistics program and have a scheduling dilemma i am most interested in the biostatistics and machine learning data science electives but unfortunately i need to get certain prereqs out of the way before i can enroll in them so i am having to decide whether or not to take a foundational course along with the sampling elective or to only take the foundational course and potentially be in the program for an extra term in an effort to take one of those previously stated electives if i choose to not take sampling and stay longer it is going to have a pretty significant impact on my roi for the program but i want to make sure i get the most out of the education because i really do love the subject it is a top 20 program globally so i want to soak up as much knowledge as possible career wise i am most interested in using statistics and machine learning to help people through the world of medicine as my dream would be to work as a data scientist for a health tech company where i can make a real impact on peoples lives my intuition tells me that having a strong foundation in sampling theory would be beneficial for any medical trials i may be working on still any input is very much welcomed
1,how to forecast human population x200b hello everyone the final work i m doing for my economics degree is about rural depopulation in a tiny region of central spain i m writing a chapter about future perspectives on demography in that chapter i want to put a forecast of the population on the region so which kind of model or technique do you recommend to me for it i ve thought on an arima model or a exponential smoothing one because i only have data about the population sorry about my english thank you for your attention and have a good day
1,missing subject index in mathematical statistics vol ii by bickel and doksum the ebook for the following book that i read misses the entire subject index and author index from p443 to p465 mathematical statistics volume ii by peter j bickel kjell a doksum 2016 isbn 9781498722681 i was wondering if someone by chance could scan the following four pages in subject index for subject index from c to e for subject index from g to for subject index from to l for subject index from n to r i have tried to find out if the index parts are freely available by the publisher or the author websites thank you so much in advance
2,can i use group normalization on variable sized inputs i am working on a project which involves processing raw audio i basically have a few dilated 1d conv networks i am using to build an auto encoder the issue i am hitting is the memory on my gpu which is forcing me to use small batch sizes my model currently does not work well without batch normalization i would like to try using either group or instance normalization the problem i see with these is that they depend on the length of the inputs right now i am training on audio samples of length 2 14 sampled at 22050 hz however in practice i will be applying this network to much longer sequences than it was trained on i think this would cause the group instance normalization to be much less noisy than in training which could alter the results any thoughts maybe i could come up with a way to apply group norm in chunks over the time axis
1,transforming interaction to proportions i have a question about interactions imagine x is only marginally insignificant at m 1sd p 051 and strongly significant at m 1sd can i roughly transform this to proportions according to a normal distributions in other words the effect is significant for roughly 85 of the sample i m asking so i can more easily translate an interaction into words for practitioners
2,augly a new multimodal data augmentation lib from fb research fb research just released a new data augmentation library it supports audio image video and text with over 100 augmentations it was developed with near duplicate detection use case in mind and features unique augmentations like one of our augmentations takes an image or video and overlays it onto a social media interface to make it look like the image or video was screenshotted by a user on a social network like facebook and then reshared post code you can find docs for each domain in respective dirs readme
1,job market nearing the end of my degree in math and stats i found myslef wondering what kind of job opportunities exist while i know that stats can be applied to virtually any field i do not really know what job titles to look for i am aware of biostatisticians but it is not a career i want to pursue i haven t seen a lot of statistician job offering and hardly any statistics internships per say on the more lucrative side i am aware of quantitative finance and data science however these are very hot and hard to get into what job titles should i look for
0,how do you version control datasets i am curious to know how everyone version controls their datasets in projects and research projects
2,nist proposes method for evaluating user trust in artificial intelligence systems how do we humans decide whether or not to trust a machine’s recommendations this is the question that a new draft publication from the national institute of standards and technology nist poses with the goal of stimulating a discussion about how humans trust ai systems the document artificial intelligence and user trust nistir 8332 is open for public comment until july 30 2021 learn about the publication and broader nist effort to help advance trustworthy ai systems
0,anyone interested on getting together to focus on personal projects i have a couple projects i’d like to work on but i’m terrible at holding myself accountable to making progress on projects i’d like to get together with a handful of people to work on our own projects but we’d meet every couple weeks to give updates and feedback if anyone else is in the chicago area i’d love to meet in person i’ve spent enough time cooped up over the past year if you’re interested pm me edit wow thanks everyone for the interest we started a discord server for the group i don t want to post it directly on the sub but if you re interested send me a pm and i ll respond with the discord link i m logging off for the night so i may not get back to you until tomorrow
2,how would you simulate online learning in keras i would like to train a classification network per sample after being pre trained on an early part of the dataset how would you go about this in python tensorflow keras code i m currently calling the fit method in a for loop using sgd optimizer at a 0 001 learning rate i ve heard that adam might reset in some way when the fit method is called while sgd is somewhat static
1,how to interpret the drop1 function and how to use akaike s criterion when using the drop1 function it produces this single term deletions model volume factor implantaat factor year factor implantaat factor year 1 fret df aic none 1360 1 factor implantaat factor year 4 1359 2 can someone explain what akaike s criterion does and how to use it to determine if the interaction can be dropped
2,mixed precision training tips recently upgraded to a 3000 series card and have been playing around with mixed precision training i notice some model architectures train just fine while others become unstable and collapse outside of a couple basic tutorials to implement mixed precision training i haven t been able to find any general tips on how to keep training stable things to change or avoid about a model or training loop when switching to mixed precision does anyone know of any good references like these
1,effect size calculation for lm robust my data fail tests of normality and are also skewed i m using lm robust thus summary gpa lmro call lm robust formula mean power abs condition data mont gpa standard error type hc2 coefficients estimate std error t value pr t ci lower ci upper df intercept 0 09681 0 006705 14 437 1 201e 45 0 109955 0 083659 2839 conditionto 0 01441 0 009206 1 565 1 176e 01 0 032460 0 003641 2839 conditionvf 0 02495 0 009326 2 676 7 502e 03 0 006666 0 043237 2839 multiple r squared 0 006129 adjusted r squared 0 005428 f statistic 9 643 on 2 and 2839 df p value 6 706e 05 condition has three levels nf vf and to for the same dataset i relevel and make vf the base level to get the vf to comparison there is an r squared and adjusted r squared but for the overall model i e i don t know the r squared of nf vf which i could use to calculate f 2 is there a function that will calculate effect sizes for each combination of the three condition levels should i use something like yuen effect ci for the significantly different conditions as determined by lm robust would i then adjust alpha if so would i divide by the total number of possible comparisons there are three nf vf nf to vf to or the number of comparisons that i actually do for instance if nf vf is significantly different and that is the only comparison that is significantly different would i use 0 05 1 0 05 then if nf vf and nf to are significantly different would i use 0 05 2 0 025 thanks for any advice
0,how can someone measure quantify friction faced by visitors on a website while performing a transaction action this transaction could be anything depending on a website like in my case financial website it could range from opening a bank account linking an account etc etc also the problem with using common metrics bounce rate exit rate is that it completely disregard the intention of the visitors maybe the person is just there for explorative purpose and if he doesn t perform an action that not necessarily means it s a friction point given the intention
1,confused on how to compare two predictor variables in a logistic regression model which are of completely different units for some context i’m doing a linguistics study which in a nutshell seeks to predict when a complement clause will be full or empty e g ‘i think i’m sick’ vs ‘i think that i’m sick’ the ‘that’ being the binary outcome variable logistic regression is typically used in these types of linguistics studies so this is the correct method in this case for this particular notion called an ‘alternation’ there are several predictors for clarity and succinctness i’ll refer to them as word information and dependency length word information is given in bits so anything from 0 to around 3 5 but these numbers tend to have many more decimals e g 2 3462538 dependency length is the distance between the verb in the sentence and the following clause so something like ‘i realised on monday last weekend in the park that i was sick’ would have a long dependency 9 units or so depending how you count in this case higher values for each of these independent variables are theoretically known to correlate with a greater probability of a full clause e g using the word ‘that’ the problem is whilst these can be used in a logistic model and the coefficients interpreted accordingly it’s difficult to compare them given the different units how can a meaningful comparison be made is this a common problem
0,looking yo hire a data science mentor for career advice next year i will start my mba and after that i am starting a msc in data science i am 31 years old and have worked as marketing manager and ecommerce manager all of these positions involve lots of data analysis and data visualization but none of data science prediction models machine learning etc i have experience using tableau powerbi google data studio analytics ads i have worked mostly on analysis and visualization my biggest weakness would be the math side of data science my ultimate goal is to work at google it has always been my dream company other options are big companies such as amazon facebook netflix didi uber or any other big company i don t know if i am the right thing by looking for a data science mentor or if i should be looking for a different person like a general career counselor any tips
2,any reviews about darts hello i m working on automl and have been discovering some of its fields over the last two months i ve learnt some stuffs about nas and hpo and techniques to perform them such as bayesian optimization mostly for hpo but applicable for nas or evolution based nas or more simple grid random search but i ve learnt about another techniques which i find interesting gradient based nas the first article that i know which deals with it is darts which tackles the problem of nas by introducing a differentiable model darts there are also some derivatives of darts such as pdarts darts snas x200b what do you think about it guys for those who know about gradient based nas some people think this is b llshit but i don t see why by the way i don t find any more recent alternative from 2020 to 2021 if you guys find any of them i would glad to hear about it thanks for your answers
1,is it possible to sample from any probability distribution i was going through some materials about generative models and it was said that we cannot compute the likelihood for all the generative models i was wondering if it was possible to sample from any probability distribution thanks
0,data science at a big 4 sorry if this is not allowed in this sub was wondering if anyone had experience working as a data scientist at a big 4 what should i expect
1,how to deal with reports of mother s and fathers statistically hi everyone i have reports of teens on how they percieve their mothers and fathers what i am finding in the correlation matrix is that the mother and father variables generally display the same relationship with variables and are correlated with one another by about 45 to 68 would it be possible to combine reports on mother s and fathers into a single parent variable if so what would be the best practice to do so in spss i have heard of people combining mother and father variables and taking the mean of the two together as an option is that a good practice or a better alternative
2,help me to decide conference to submit my paper corl etc hello i recently submitted to icml and i got 2a and 2r and my paper got rejected my paper is on rl and has mujuco experiments i am looking to submit to corl i don t have too much experience on the robotic domain do you know if corl must contain robotic component
2,bits per character bpc misuse hi all i developed my own simple character level language model if i use natural log i get the loss of 1 for english however literature reports 1 bit per character which is far ahead of mine the natural log and log2 differs at a factor of ln2 my question double check is it possible that the literature acturally mis reported natural log rather than log2 for example i see mingtp reports natural log loss
2,relation between number of layers in graph neural networks and l hop neighborhood after working with gnns for a short amount of time now i have come across a little bit of confusion some papers that i have read state something along the lines of the number of layers l in a gnn determine the l hop neighborhood of the computational graph of a certain node i am very confused by this statement wouldn t the number of training iterations determine the neighborhood of a node for example after 3 iteration of training one layer of gnn aren t we basically gathering information from the 3 hop neighborhood of each node isn t the number of gnn layers irrelevant to the neighborhood of the node since information from further nodes can still be aggregated with a single gnn layer and enough training iterations
1,is a likelihood function a sufficient statistic in casella and berger s statistical inference there are two principles very similar to each other the sufficiency principle p293 and 272 and the likelihood principle p293 294 and 290 a likelihood function can be viewed as a function valued statistic is a likelihood function a sufficient statistic so that the likelihood principle is a special case of the sufficiency principle
1,question coin flipping probability hello i m interested in statistics i want to understand the basics if i flip a coin for 20 times and got heads 83 of the times this means something or nothing is there anyway to say there is an 40 chance wich the coin will got heads the 83 or something like that and how to do the basics calculations any recommended book for a noob thank you
1,how to model bimodal distribution hey guys i have some data i am analyzing not homework that appears to yield a bimodal distribution in other words it looks like two normal distributions squished together two unimodal normal distributions added together closely they merge in the middle a bit so they aren t fully distinct i did a lag plot and my data is strongly linear and a run sequence plot did not look sinusoidal knowing the data i expect it to give me two unimodal distributions but i am unsure how to go about modeling this especially since they overlap in the middle any tips edit solved
0,tableau vs amazon quicksights i know tableau very well and is my preferred tool but i noticed there s a cheaper option from amazon any input from your experience is appreciated
1,correlation in non independent data setting longitudinal study is the second time i have been asked to compute correlation between two variables in a correlated data setting so is time to solve this out let us assume that we want to calculate “correlation” between variables x and y where we have repeated measures of both variables for each subject in the study and there are several subjects in the study more than 10 the number of repeated measures is not constant but can vary from 2 to 10 i have been thinking on how a mixed effect model can be used to address this something like y x time subject could be a possibility but not sure
0,anyone interested in being a mod obviously you’ll need post comment history here post in this thread and we’ll check you out
0,for most of the problems i try to solve using data science the biggest challenge surprisingly isn’t really the “science” part but the “data” part when you start a project with a problem and try to work towards a solution which is what you should do to make sure your work is actually useful then you arrive at this hurdle where you have the problem and an idea for the solution at hand and they are your only lead to finding the specific data you need to train you models sometimes this data can be really hard to find using these search parameters no matter how much i search i don t find what i’m looking for the data is probably out there and there is probably some search term that would make google put this data right at the top for you to see but i ve often found that the problem and prospective solution i have on hand is generally not it datasets online simply aren t indexed by their applications they are probably most often indexed by their source and that is something that i in my experience can’t really use to engineer a search term that gives good results if the data even exists online i was wondering if you all had the same problem and whether you agreed with this idea is it the same case in your experience or am i just doing it wrong
2,searching for paper for regression loss variant i have used in my code a certain mix of l1 and l2 loss function like this l2 1 exp kappa c l1 with l1 l1 loss and l2 l2 loss computed over a batch i know that i have this from a paper in the paper they chose specific kappa and c i think i modified them but i cannot find it anymore i think it was to penalize higher difference but be more benevolent to smaller differences does anyone know where this comes from thanks in advance
0,job hopping do it i consistently see hear people advice others especially early in their careers not to job hop it will look bad on your resume no one will want to hire you no one becomes vp if they can t show they can stay at one place that may apply in other careers and honestly i don t think it does but whatever but especially in data science it s just terrible terrible advice unless your current company is giving you 8 yearly raises and 20 comp increases with each promotion you should be looking for another job within 2 years edit to be clear if you re not getting 20 increases in comp you should be looking for opportunities that will offer you at least 20 increases in comp i m not saying you should take lateral moves just because you re not getting big enough increases make every move count and you should be able to there are 4 core reasons why experienced data science talent is in incredibly high demand if you have 1 years of legit data science experience you likely won t have trouble finding companies to offer you a job i was trying to hire a guy recently who ended up with 4 offers on the table including one that was 50 higher than the one i gave him that is what the world is looking like right now which means that even if you re flagged as a job hopper the reality is that most companies don t have the candidate pool to get picky enough to reject you they need people they don t have leverage the fastest way to get a higher ranking role is to change companies this is just basic probability the previous point your company may or may not have a higher role for you to move into and that s assuming they would even be interested in promoting you in contrast there are 100s of companies out there with openings at levels higher than you waiting for a role to open up at your company is just a bad gamble when i look at people that joined my first job at the same time as i did and mind you people i thought were just as if not more capable than me i am now 2 3 titles above them and it s all because i was able to get bigger titles as i switched jobs the job hopper red flag is self correcting say you jump jobs every 6 months 3 times in a row you would surely become a job hopper do not hire candidate after that until you weren t see if no one hires you for a new job for 2 3 4 years eventually someone is going to look at your resume and say hey they job hopped a bunch for a bit but now they ve been at job x for like 3 years we should give this person a chance for every hiring manager there is a number y of consecutive years at one job that will undo whatever previous job hopping behavior there was prior internal compensation increases are lower than external ones 99 of the time listen if your company is giving you 10 yearly raises and 25 promotion raises by all means stay that s a great company if you re like the rest of us getting 2 4 raises and 10 15 comp increases when you get a promotion and you re getting promoted every 2 5 years on average or less then the math just doesn t work out here s some very real data from my 8 year career average total comp increase without promotion 2 average total comp increase with promotion 12 average time till promotion 2 years average total comp increase changing jobs 28 if i had stayed at one job for 8 years my current comp would probably be somewhere around 50 60 of what it is now and that is in large part because i took a new job for a 25 raise after being with one company for only 6 months was it risky in that some employers may look at that 6 month tenure as a problem probably but it hasn t stopped at least some substantial number of companies to continue to pursue me now let s pause here some of you will say well i applied to one job once and didn t get it so i know that no one wants to hire me i had a roommate in college that gave me great life advice disguised as terrible dating advice his dating advice was listen the reason i get so many girls is that i go out and i hit on 30 40 girls every night and every one of them but one will tell me no but as long as one of them says yes that s all you need so i never followed that advice for dating but you should 100 follow it for job searching there are 100s of jobs out there for which you re qualified if you re currently employed and in no rush to leave your job continuously keep applying to any job that you would consider an upgrade over your current one obviously make sure the effort is worth the payout don t go doing week long take home assignments for jobs that suck but pick and choose jobs that are good fits pay well look cool and put some effort into applying for them you will get told no a lot at first and then you ll get better at cleaning up your resume better at interviewing better at take homes etc because the only way to get better at those things is to practice and then eventually you ll start making it to the final rounds of some interview processes but probably still get rejected and then one day you ll get an offer that represents a 40 increase in comp and you ll be sitting there thinking wait what the hell tl dr job hop aggressively especially if early in your career don t let blanket advice about how job hopping is bad deter you from doing what is going to almost surely be substantial leaps in your career and comp edit if it wasn t obvious and maybe it wasn t there are certainly cases where this advice won t apply u michaelkamprath pointed out that this may not apply in tech where you can climb the ladder as an individual contributor and deep expertise may be much more valuable than climbing the management ladder and i think that is a fair assessment however one must also recognize this is not the case outside of tech where the big salaries are going to come from management roles i think it s also fair to say that if compensation isn t a big driver for you and the quality of the work matters much more then this may not apply either
1,terminology question wald tests i have a question about what is meant by wald tests based on the contexts of where i have read it as well as some notes like the wikipedia entry my understanding is any test that is formulated by a measure of distance between the estimated value and hypothesized value is a wald test so the distribution of the test is not relevant just if it is a distance it can be called a wald test so a t test f test z test chi square etc are all types of wald tests that have a particular distribution
2,potential test for ai ethicists before i get to the meat of the matter i m going to clarify my stance on ai ethics it is an incredibly important domain that will help define the future we seem to be hurtling into courtesy of ai generically speaking i just wish that the loudest voices weren t well what they currently are willing to use poor methods obsessed primarily with impact in the first world and incapable of taking criticism right so this post primarily stems from the brief period i spent working on debiasing large language models and becoming cynical about the field of ai ethics as it currently exists it has become a very easy way to demand respect at least on ai social media by being unnuanced and vicious and calling yourself an ai ethicist so i propose a small test to see which ai ethicists actually care about ethics and are not trying to ride a wave for clout academic and social or are fanatics did they oppose students and early career researchers being blacklisted in december by dr a as loudly as they seem to oppose every other tiny injustice or did they make no comment thereby tacitly approving unbelievably vicious behaviour by someone arguably in their clique which would be deeply unethical behaviour on their part or did they support it which would be pretty vile how can those who didn t oppose the blacklist claim to be representing any ethical norm in ai when they are unwilling to apply one of the simplest ethical principles stand against tyrannical behaviour in a context where it was trivially easy to do so it is as likely as not that i am missing some nuance here if so i will be glad to learn cheers
0,salaries in spain yet another post like this hi all i m interested in salaries in spain for the mid career to most senior positions including managing head of data science kind of positions in data science machine learning engineer etc specifically for spain any information is welcome and yes i know they are much lower than in the us and the post is not about that i m asking specifically about mid career positions and beyond because i get offers for more junior positions from time to time in linkedin so i have that salary range covered but for more senior positions i don t have that information first hand and i don t trust the numbers i see on linkedin salaries or glassdoor they seem outdated and more importantly biased to people willing to answer kudos if you have information specifically about barcelona and madrid where the salaries seem to be highest best
2,strategies and tactics for regression on imbalanced data hello everyone happy to share our new work on tackling regression problems on imbalanced data this work was accepted at icml 2021 as a long oral presentation data imbalance is ubiquitous and inherent in the real world under the classic problem of data imbalance this work explores a very practical but rarely studied problem imbalanced regression most of the existing methods for dealing with imbalanced data are only for classification problems — that is the target value is a discrete index of different categories however many practical tasks involve continuous and sometimes even infinite target values this work promotes the paradigm of the traditional imbalanced classification problems and extends the data imbalance problem from discrete targets to continuous ones such imbalanced problem in the continuous domain exists in both linear and deep models it is even more serious in the deep model why is this because neural network predictions are often over confident and this data imbalance issue is severely magnified so in this work we formally define and investigate deep imbalanced regression dir arising in real world settings we develop two simple effective and interpretable algorithms for addressing dir label distribution smoothing lds and feature distribution smoothing fds which exploit the similarity between nearby targets in both label and feature spaces the proposed algorithms are easy to implement and could serve as a good starting point for this new problem to support practical evaluation of imbalanced regression methods we also curate large scale dir datasets for common real world tasks in computer vision natural language processing and healthcare they range from single value prediction such as age text similarity score health condition score to dense value prediction such as depth the new datasets could support practical evaluation and facilitate future research on imbalanced regression check out the links below for more details paper code dataset models blog post check out for in depth details towardsdatascience x200b deep imbalanced regression dir aims to learn from imbalanced data with continuous targets tackle potential missing data for certain regions and generalize to the entire target range let me know if you have any questions or comments
2,is it financially viable to do a phd in the uk as a canadian international looking at oxford cambridge ucl websites the tuition for international phd is 20 30k pounds per year whereas domestic fees are only 5 8k are international students able to break even on the tuition from scholarships and teaching assistantship should i even bother applying if i don t want to get a 100k loan to do a phd
1,7 years since norm matloff s blog post statistics losing ground to cs losing image among students how has the statistics vs cs situation evolved statistics losing ground to cs losing image among students mad data scientist wordpress com i will quote the blog post below statistics losing ground to cs losing image among students the american statistical association asa leadership and many in statistics academia have been undergoing a period of angst the last few years they worry that the field of statistics is headed for a future of reduced national influence and importance with the feeling that the field is to a large extent being usurped by other disciplines notably computer science cs efforts to make the field attractive to students have largely been unsuccessful i had been aware of these issues for quite a while and thus was pleasantly surprised last year to see then asa president marie davidson write a plaintive editorial titled “aren’t we data science ” good the asa is taking action i thought but even then i was startled to learn during jsm 2014 a conference tellingly titled “statistics global impact past present and future” that the asa leadership is so concerned about these problems that it has now retained a pr firm this is probably a wise move–most large institutions engage in extensive pr in one way or another–but it is a sad statement about how complacent the profession has become indeed it can be argued that the action is long overdue as a friend of mine put it “they the statistical profession lost the pr war because they never fought it ” in this post i’ll tell you the rest of the story as i see it viewing events as a statistician computer scientist and r enthusiasist cs vs statistics let’s consider the cs issue first recently a number of new terms have arisen such as data science big data and analytics and the popularity of the term machine learning has grown rapidly to many of us though this is just “old wine in new bottles ” with the “wine” being statistics but the new “bottles” are disciplines outside of statistics–especially cs i have a foot in both the statistics and cs camps i’ve spent most of my career in the computer science dept at the university of california davis but i began my career in statistics at that institution my mathematics doctoral thesis at ucla was in probability theory and my first years on the faculty at davis focused on statistical methodology i was one of the seven charter members the department of statistics though my departmental affiliation later changed to cs i never left statistics as a field and most of my research in computer science has been statistical in nature with such “dual loyalties ” i’ll refer to people in both professions via third person pronouns not first and i will be critical of both groups a friend who read a draft of this post joked it should be titled “j’accuse” but of course this is not my intention however in keeping with the theme of the asa’s recent actions my essay will be stat centric what is poor statistics to do well then how did cs come to annex the stat field the primary cause i believe came from the cs subfield of artificial intelligence ai though there always had been some probabilistic analysis in ai in recent years the interest has been almost exclusively in predictive analysis–a core area of statistics that switch in ai was due largely to the emergence of big data no one really knows what the term means but people “know it when they see it ” and they see it quite often these days typical data sets range from large to huge to astronomical sometimes literally the latter as cosmology is one of the application fields necessitating that one pay key attention to the computational aspects hence the term data science combining quantitative methods with speedy computation and hence another reason for cs to become involved involvement is one thing but usurpation is another though not a deliberate action by any means cs is eclipsing stat in many of stat’s central areas this is dramatically demonstrated by statements that are made like “with machine learning methods you don’t need statistics”–a punch in the gut for statisticians who realize that machine learning really is statistics ml goes into great detail in certain aspects e g text mining but in essence it consists of parametric and nonparametric curve estimation methods from statistics such as logistic regression lasso nearest neighbor classification random forests the em algorithm and so on though the stat leaders seem to regard all this as something of an existential threat to the well being of their profession i view it as much worse than that the problem is not that cs people are doing statistics but rather that they are doing it poorly generally the quality of cs work in stat is weak it is not a problem of quality of the researchers themselves indeed many of them are very highly talented instead there are a number of systemic reasons for this structural problems with the cs research “business model” cs having grown out of research on fast changing software and hardware systems became accustomed to the “24 hour news cycle” –very rapid publication rates with the venue of choice being refereed frequent conferences rather than slow journals this leads to research work being less thoroughly conducted and less thoroughly reviewed resulting in poorer quality work the fact that some prestigious conferences have acceptance rates in the teens or even lower doesn’t negate these realities because cs depts at research universities tend to be housed in colleges of engineering there is heavy pressure to bring in lots of research funding and produce lots of phd student s large amounts of time is spent on trips to schmooze funding agencies and industrial sponsors writing grants meeting conference deadlines and managing a small army of doctoral students–instead of time spent in careful deep long term contemplation about the problems at hand this is made even worse by the rapid change in the fashionable research topic de jour making it difficult to go into a topic in any real depth offloading the actual research onto a large team of grad students can result in faculty not fully applying the talents they were hired for i’ve seen too many cases in which the thesis adviser is not sufficiently aware of what his her students are doing there is rampant “reinventing the wheel ” the above mentioned lack of “adult supervision” and lack of long term commitment to research topics results in weak knowledge of the literature this is especially true for knowledge of the stat literature which even the “adults” tend to have very little awareness of for instance consider a paper on the use of mixed labeled and unlabeled training data in classification i’ll omit names one of the two authors is one of the most prominent names in the machine learning field and the paper has been cited over 3 000 times yet the paper cites nothing in the extensive stat literature on this topic consisting of a long stream of papers from 1981 to the present again for historical reasons cs research is largely empirical experimental in nature this causes what in my view is one of the most serious problems plaguing cs research in stat–lack of rigor mind you i am not saying that every paper should consist of theorems and proofs or be overly abstract data and or simulation based studies are fine but there is no substitute for precise thinking and in my experience many nominally successful cs researchers in stat do not have a solid understanding of the fundamentals underlying the problems they work on for example a recent paper in a top cs conference incorrectly stated that the logistic classification model cannot handle non monotonic relations between the predictors and response variable the paper really stressed this point yet actually one can add quadratic terms and so on to model this this “engineering style” research model causes a cavalier attitude towards underlying models and assumptions most empirical work in cs doesn’t have any models to worry about that’s entirely appropriate but in my observation it creates a mentality that inappropriately carries over when cs researchers do stat work a few years ago for instance i attended a talk by a machine learning specialist who had just earned her phd at one of the very top cs departments in the world she had taken a bayesian approach to the problem she worked on and i asked her why she had chosen that specific prior distribution she couldn’t answer–she had just blindly used what her thesis adviser had given her–and moreover she was baffled as to why anyone would want to know why that prior was chosen again due to the history of the field cs people tend to have grand starry eyed ambitions–laudable but a double edged sword on the one hand this is a huge plus leading to highly impressive feats such as recognizing faces in a crowd but this mentality leads to an oversimplified view of things with everything being viewed as a paradigm shift neural networks epitomize this problem enticing phrasing such as “neural networks work like the human brain” blinds many researchers to the fact that neural nets are not fundamentally different from other parametric and nonparametric methods for regression and classification recently i was pleased to discover–“learn ” if you must–that the famous book by hastie tibshirani and friedman complains about what they call “hype” over neural networks sadly theirs is a rare voice on this matter among cs folks there is often a failure to understand that the celebrated accomplishments of “machine learning” have been mainly the result of applying a lot of money a lot of people time a lot of computational power and prodigious amounts of tweaking to the given problem–not because fundamentally new technology has been invented all this matters–a lot in my opinion the above factors result in highly lamentable opportunity costs clearly i’m not saying that people in cs should stay out of stat research but the sad truth is that the usurpation process is causing precious resources–research funding faculty slots the best potential grad students attention from government policymakers even attention from the press–to go quite disproportionately to cs even though statistics is arguably better equipped to make use of them this is not a cs vs stat issue statistics is important to the nation and to the world and if scarce resources aren’t being used well it’s everyone’s loss making statistics attractive to students this of course is an age old problem in stat let’s face it–the very word statistics sounds hopelessly dull but i would argue that a more modern development is making the problem a lot worse–the advanced placement ap statistics courses in high schools professor xiao li meng has written extensively about the destructive nature of ap stat he observed “among harvard undergraduates i asked the most frequent reason for not considering a statistical major was a ‘turn off’ experience in an ap statistics course ” that says it all doesn’t it and though meng’s views predictably sparked defensive replies some quarters i’ve had exactly the same experiences as meng in my own interactions with students no wonder students would rather major in a field like cs and study machine learning–without realizing it is statistics it is especially troubling that statistics may be losing the “best and brightest” students one of the major problems is that ap stat is usually taught by people who lack depth in the subject matter a typical example is that a student complained to me that even though he had attended a top quality high school in the heart of silicon valley his ap stat teacher could not answer his question as to why it is customary to use n 1 rather than n in the denominator of s2 but even that lapse is really minor compared to the lack among the ap teachers of the broad overview typically possessed by stat professors teaching university courses in terms of what can be done with stat what the philosophy is what the concepts really mean and so on ap courses are ostensibly college level but the students are not getting college level instruction the “teach to the test” syndrome that pervades ap courses in general exacerbates this problem the most exasperating part of all this is that ap stat officially relies on ti 83 pocket calculators as its computational vehicle the machines are expensive and after all we are living in an age in which r is free moreover the calculators don’t have the capabilities of dazzling graphics and analyzing of nontrivial data sets that r provides–exactly the kinds of things that motivate young people so unlike the “cs usurpation problem ” whose solution is unclear here is something that actually can be fixed reasonably simply if i had my druthers i would simply ban ap stat and actually i am one of those people who would do away with the entire ap program obviously there are too many deeply entrenched interests for this to happen but one thing that can be done for ap stat is to switch its computational vehicle to r as noted r is free and is multi platform with outstanding graphical capabilities there is no end to the number of data sets teenagers would find attractive for r use say the million song data set as to a textbook there are many introductions to statistics that use r such as michael crawley’s statistics an introduction using r and peter dalgaard’s introductory statistics using r but to really do it right i would suggest that a group of stat professors collaboratively write an open source text as has been done for instance for chemistry examples of interest to high schoolers should be used say this engaging analysis on ok cupid this is not a complete solution by any means there still is the issue of ap stat being taught by people who lack depth in the field and so on and even switching to r would meet with resistance from various interests such as the college board and especially the ap stat teachers themselves but given all these weighty problems it certainly would be nice to do something right switching to r would be doable–and should be done
1,how to choose threshold between absolute and squared errors in robust huber loss function the huber loss function for residuals e is l e 0 5 e 2 for e d l e d e 0 5 d for e d it is quadratic in e up to value d i want to write a program to estimate the location of data using huber loss but what is a good default choice for d
0,starting a job as a data scientist in a month and freaking out i ve been a data analyst with some data science ish work for years and now have finally secured a job as a data scientist and i don t want to f it up i m freaking out everyone on the team seems nice but it s also small so i assume i ll need to be pretty independent i m going to have a few weeks off before starting and wondering if anyone has any advice on things i can do to refresh my skills in this area so i don t seem like a complete idiot when i start it s been a while since i did any academic ds work tia
0,i am self learning data science i asked this question on every platform i can think of but still didn t get an answer please help me out if you know the answer should i remove features such as gender and birth month before drawing the heatmap because they are categorical i am working on a dataset that has both categorical and numerical continuous and discrete features 26 columns 30244 rows target is categorical 1 2 3 and i am performing eda on this dataset my dataset is regarding hotel reservation status not cancelled 1 cancelled 2 no show 3 of customers in the span of 3 years 2015 2016 2017 given data of the customer my task is to predict if the customer will either cancel not cancel or no show for his reservation the categorical features with numerical values ex gender has values 0 and 1 are also considered when taking the heatmap with seaborn as per my knowledge the heatmap is drawn to check the correlation between continuous numerical features right correct me if i am wrong should i remove such features before taking the heatmap the book in date expected check in date expected check out date are given in the dataset i extracted month and year for each feature separately these month columns are also categorical right as they only have values between 1 12 i took screenshots of month distribution plots and uploaded them here should i do a test like the chi square test on those features
1,what range of schools are realistic for a phd program in statistics long story short i wasn t a really serious student until later in my undergraduate career and i found math my senior year i m currently in a msc program in a top 100 math program and am finding myself intrigued in statistics and applied math and wish to pursue further in a phd setting what range of phd programs would be accessible also is it necessary to take the math gre below is a break down of my profile x200b student type domestic student applying for statistics phd fall 2022 undergrad relatively unknown liberal arts major biochemistry and economics minor mathematics gpa 3 7 biochemistry 3 8 economics 3 85 master s mathematics gpa 3 99 over 11 courses for the past school year summer undergrad math courses calc 2 3 a a linear algebra a intro statistics a intro to proof a abstract algebra 1 a complex analysis a abstract linear algebra a real analysis 1 a real analysis 2 a topology a numerical analysis a numerical linear algebra a probability theory a mathematical statistics a differential geometry a mathematics of data science a topological data analysis a graduate math courses abstract algebra 1 a measure theory a graduate complex analysis a uncertainty quantification a markov chains and mixing times a remark the a are quirks of my current institution they do not impact gpa but serve as indicators of maintaining above a 98 strength in coursework taking in summer algorithms intro to ml taking in fall graduate algorithms graduate numerical linear algebra graduate numerical pde pde 1 research starting this summer part of master s thesis focused on differential equations on fractals gre 170q 164v 6w x200b interests use of harmonic analysis for nonparameteric estimation techniques machine learning on manifolds general stochastic processes i m very interested on the theory side in general
1,question help me 50 randomly selected couples interviewed for family planning 31 use some sort of family planning what would be the expected magnitude of variation in the proportion of practising couples if another sample of same size is drawn from the same population
1,metrics for quantifying the diversity of a partition of data as the title suggests i am looking for metrics that quantify a partition s diversity consider a set of observations y y1 y2 yn and a partition of pi of y for example pi a1 a2 am for m n i am looking for metrics f that map from pi the set of all pis into 0 1 such that f y 1 ie the trivial partition and f singleton y minimizes f i ue singleton y to denote the singleton partition of y
1,applications of copulas in predictive modelling recently i came across copulas in statistics these seem very interesting is the main application of copulas the ability to simulate multivariate correlated data for instance suppose i have data collected for 1000 individuals for each of these individuals i have their weight height and age it is very likely that these 3 variables are correlated with each other thus just using this data would it be possible to use copulas to simulate data for new individuals would this require to explicitly assume marginal joint and conditional distributions for these variables or would the copula be able to do this by itself how are copulas usually used in statistics and predictive modelling what are their main applications and advantages has anyone here ever used them in their studies or work i would be very interested to hear about your experiences and results using copulas
2,summarising feature importances best way hi i am running a binary classifier and i have a pipeline with 5 or more different type of classifiers such as random forest xgboost easyensenmble etc i also collect the feature importance of each classifier what would be the most valid way to summarise feature importance can i simply average the feature importance value for each of the classifiers and then plot it what if my classifiers have different performances would that still work with feature importance x200b this is how i do it now thank you
2,ml to assign words to eeg numbers it didn t train on i have a csv file made of two columns one column is the eeg raw data numbers the other one is words i assigned to the eeg numbers if a ml algorithm trains on this csv file can it assign words to eeg numbers even if the eeg numbers is a new number it didn t train on if i add only the eeg column can it generate the words column based on how it trained previously it has to detect how words are assigned to numbers and assign words to numbers it didn t train about how many words should the training file have on it
2,project looking for a music machine learning program sorry i don t know if this type of post is accepted here but i was looking hoping to find a program that creates music preferably with words based on multiple samples it is fed i m trying to create a simulation program of music style differentiation over time based on this so it would have to be either open source or accessable from command line so i can use it through other code let me know if theres anything that fits that description or if not if you think it would be a good idea to try to make one
1,spss split file i want to compare learning approach have 2 scale 4 sub scale between 2 university and for each university i want to find the different between their demographic such as gender academic lvl etc i wonder if i could use split file function in spss analyses them as different set of data or do you guys have any other way
1,bootstrapping confidence intervals from data that is also simulated then repeating is that a thing this is confusing me since i m essentially doing simulations of simulations which feels incorrect right off the bat what i m trying to do figure out how many samples i will need to collect so that the bootstrapped confidence intervals of my metric of interest i ll call moi are within a certain lower limit no formula exists to calculate this directly what i ve done 1 run a data simulation for some n patients where a score predicts whether they are sick in this case i simulate it to have an auc of 0 92 with a massive sample size the moi is 10 so i m considering that the true value 2 run the boot package in r to bootstrap the ci for for moi using 1000 replicates i focus on the bias corrected and accelerated ci i want to see what initial n sample size i need so that my confidence interval lower bound is reliably above a certain value 5 the upper bound doesn t matter with n 200 i get an estimate of 9 and the boot package gives me a ci of 5 17 however this of course depends on the seed the next run gives 29 12 93 the next 10 4 8 19 with n 500 i run the script many times and the lower limit never dips below 5 but how do i eloquently do this i e repeatedly simulating the data bootstrapping the data then checking the distribution of results until i find an n where 5 of these fake studies have a lower ci for moi that is 5 is this ludicrous edit initial post had some weird numbers fixed
2,has anyone ever used the drwhy package in r for explainable ai has anyone ever used this package in r before for trying to explain blackbox machine learning models this package drwhy seems like a comprehensive collection of different algorithms e g lime shap meant for explaining blackbox models e g neural networks has anyone ever used this package before how have your experiences been did it prove to be useful were the results reliable
0,anyone know of a good textbook example for multi task learning i am in the process of preparing a course on neural networks it s a broad strokes walk through both basics and various different topics i have decided that one week will be dedicated to transfer learning and multi task learning together since there are some interesting transfer learning approaches which leverage mtl as part of the course i want the students to solve a small exercise where they improve some machine learning task by combining it with another one however it is really hard to come up with a good task like that regular fine tuning was easy enough i decided to lean on the pytorch tutorial for that one so far i have not been able to find a similar toy example that can both be run in google colab i can not assume that my students have access to a gpu of their own and where the benefit is tangible my best bet so far which is not working out yet is to combine age and gender identification on a subset of the cropped version of the wiki face dataset i m hoping that with a sufficiently small subset i ll have a task which doesn t quite work on its own but where real improvement can be seen when each image is analyzed in two different tasks but perhaps someone knows of a better problem
0,full data engineering data science pipeline on stock market idea hey guys hope you are doing good today i am a python developer and i have very little experience in ml and big data i am also a professional stock trader so i want some technical advice or an idea from the redditors here i wanna a build a full stack data science project something related to the stock market when i say a full stack i mean right from the start from getting data or extracting data and going through all the big data pipelines and building a predictive model and deploying it i mean i have some ideas but i am not able to technically produce it in a way like you guys could do i am asking you because with experience you know something compared to me i want answers like scrap financial data from a website live stream process it using spark and setup in airflow and then take the data do a reinforcement learning algorithm and deploy it showing something in the frontend forgive me if i am demanding too much but please do your best it would be helpful for others too
1,insignificant significant regression is it possible to say run a linear regression with x1 as the explanatory variable and y as the response when the regression is ran x1 is insignificant with a high p value then you run a multiple regression with x1 and x2 and y as the response and x1 now is significant with a low p value is this scenario possible
1,do successful models defy the bias variance tradeoff in statistics we are always warned about the bias variance tradeoff simple statistical models are reliable but are generally unable to sufficiently capture the complexity within the data i e high bias low variance complex statistical models are able to capture complexity within the data but are generally not as reliable when generalizing to new data i e high variance low bias this leads me to my questions 1 are successful statistical models able to defy the bias variance tradeoff as a simple example consider the famous iris dataset kaggle competitions have shown us that statistical models can be made that perform well on both the training data as well as the test data are these statistical models defying the bias variance tradeoff now let s imagine a far more complicated problem and dataset but suppose that we are still able to create a statistical model that performs well on both the training data as well as the test data are we again defying the bias variance tradeoff 2 i have seen proofs that show how the mse mean squared error can be decomposed into a bias term and a variance term thus for a given statistical model for a fixed value of this model s mse if the variance is high then the bias must be low in order to compensate and vice versa my question relates to the following when people discuss the variance in the bias variance tradeoff they are generally interested in the variance of a statistical model s performance when dealing with unseen data since this unseen data might not even exist at the moment how is the bias variance tradeoff able to make claims about unseen data is the bias variance tradeoff a general idea with some theoretical foundations or is it mainly empirical 3 finally how does the bias variance tradeoff apply to real world models such as the self driving car alpha go and computers playing tetris or in the case of reinforcement learning models the bias variance tradeoff does not apply the same way it does in supervised learning models thanks
1,promax varimax rotation w ml fa is there a minimum threshold of item correlation at which i should use promax rotation i ve been reading every piece of literature i can find on this topic today and i still don t have a citeable justification either way i m doing a confirmatory factor analysis using max likelihood and a couple of others on data that is moderately correlated 50 ish inter item
2,chinese ai lab challenges google openai with a model of 1 75 trillion parameters link here tl dr the beijing academy of artificial intelligence styled as baai and known in chinese as 北京智源人工智能研究院 launched the latest version of wudao 悟道 a pre trained deep learning model that the lab dubbed as “china’s first ” and “the world’s largest ever ” with a whopping 1 75 trillion parameters and the corresponding twitter thread what s interesting here is baai is funded in part by the china’s ministry of science and technology which is china s equivalent of the nsf the equivalent of this in the us would be for the nsf allocating billions of dollars a year only to train models
2,need dataset images of lines recognizing if lines are parallel for my masterthesis i need image classification images of 2 lines and need to recognize if the lines are parallel any help appreciated thanks in advance
1,what is the number of cards needed to complete a full pack if i m randomly picking them off the street how many playing cards would i need to find to ensure completion of a full deck of cards assuming i randomly pick up cards off the street some context guy on twitter says he has completed a full deck of cards since the beginning of the pandemic by picking them up every time he saw them on the streets by my very poor reckoning the calculation would go something like this first card chances are 52 52 would work for the collection second card 51 52 because one would be a duplicate third card 50 52 and so on so is the probability the product series of n 52 for n 1 to 52 stick that into wolfram you get an absurdly low number the exponent is 22 so the number of cards needed is 1 over this figure this is probably more than all the playing cards ever printed anywhere this silly post has me plugging numbers into wolfram and i need closure please help thanks
2,how many ideas do you try out before finding something that s actually pursuable to something that s publishable as a master s student who s just now starting to take on research projects on my own i ve always heard that one of the most important qualities to doing research in machine learning is to try ideas out many people have given me this advice and have told me that it s the same advice their supervisors or seniors gave them i d follow accordingly and think of ideas that i could try e g performing error analysis and trying out a new modeling approach but for the past 4 5 months it s usually been ending up in results that are not pursuable performance is mediocre and or there aren t any surprising interesting points to analyze and definitely not publishable i m starting to wonder if i m even approaching research correctly btw before anyone says it i have been going to my pi for help many of the ideas that i ve tried out were his but he s more of an entrepreneur than a researcher and has been out of touch with the current research landscape for a while any tips or opinions are appreciated thanks
0,any good personal blogs that has quality data science content do you have any favourite bloggers that are professionals and have high quality content that you love reading would love to add them to my reading list cheers
2,thoughts and comments of on concrete autoencoder i have been doing some experiments with concrete autoencoder which uses gumbel softmax distribution in one of its encoder layer to select a subset of features what i found was this was not robust at all giving me different subset of features at different runs the paper is here i also have another question about existence of a sota in deep learning based feature selection methods please do suggest
2,miscellaneous theorems used in machine learning i am trying to learn more about the background of machine learning and came across the following theorems 1 the universal consistency theorem i have heard of the universal aproximation theorem but not of the universal consistency theorem 2 the cover heart theorem i have heard of something called cover s effect which talks about how non linearly sepperable patterns tend to become linearly sepperable when projected into higher dimensions is the cover heart theorem related 3 stone s theorem i have heard of something called the weistrass stone theorem which talks about how polynomials can be used to aproximate almost any function is the weistrass stone theorem related to this can someone please help me better understand these thanks
0,40k time series i have 40k time series from roughly the same distribution daily product sales over time and i need to forecast how each will behave how would you approach this retrain a really basic model 40k times or have a huge model that is trained once with all the data how fast would the prediction work i mean if it takes 10 seconds for each of the time series 400k seconds is a few days
2,tips for quick image tagging hi i m currently using vott to tag objects for an image detection model obviously the more images the better and it s better if they re more accurate i m currently using a laptop touch pad for this i ve also tried using the touch screen a wacom tablet with pen and an ipad but settled on the touch pad as the most efficient and simply tagging with rectangles does anyone have any suggestions for how to further improve efficiency is it worth using a pen on my laptop screen or ipad screen also when does it become worth it to use polygons instead of rectangles most of my objects are rectangular but some are triangles or other polygons any other tips are appreciated
1,effect of underreporting on estimator in linear regresion hi sorry as this might be a dumb question i m taking an introductory level econometrics course that uses statistics particularly linear and multilinear regression i m having trouble understanding a particular concept in how the underreporting of data affects the estimator given that the rest of the terms are unbiased and that the regression formula is y i beta 0 beta 1 x i u i why does an underreporting of the x variable by 20 cause the beta 1 estimator to be biased upwards by 20 i have assumed that the beta 1 estimator would be biased upwards by 25 instead since it would need to fulfil hat beta 1 0 8 x i beta 1 x i i have already asked my professor but he redirected me to the textbook that gave me a rather unconvincing argument that had no explanation thank you
0,some tips on how to go from academia to a data science position i read a lot of posts about how to land a data science position and what the biggest differences are between doing research in academia and working as a data scientist since i did the same journey myself a couple of years ago i thought it might be helpful if i summarised my experience and my thoughts about how to successfully do the transfer my first post on the subject is here x200b let me know what you think thanks
2,ml optimizers from scratch using jax github link includes a link to a kaggle notebook to run it directly shreyansh26 ml optimizers jax implementations of some popular optimizers from scratch for a simple model like linear regression the goal of this project was to understand how these optimizers work under the hood and try to do a toy implementation myself i also use a bit of jax magic to perform the differentiation of the loss function w r t to the weights and the bias without explicitly writing their derivatives as a separate function this can serve as an excellent tutorial for beginners who want to explore optimization algorithms in more detail
2,cv model compression for real time inference on low powered computers hi i m planning a research project for my university that will focus on compressing a sota cv model using quantization pruning knowledge distillation etc to allow for it to run on lower powered computers like a rpi and such i have a few queries how does one effectively structure an ml research project of the aforementioned nature are there any resources for what i m trying to do except tinyml what is the possibility of achieving my goal if the model takes continuous video input what challenges might i face and how can i best mitigate them tia
1,phd programs which lean more towards modern applied methods vs theory heavy hello all i’m currently a rising junior statistics major at my university i have really come to love my stats classes and have really liked getting the classical foundation of probability inference and regression i’m also looking forward to my bayesian stats and time series courses and enjoyed math in general as i like math while i do like the classical stats i want to go more into the data science side as this is my interest for the future while i’ve enjoyed classical regression analysis and see it as a great foundation for me i’ve really seen myself get interested in statistical learning and more into predictive modeling with modern methods ie machine learning islr has also been a fascinating read thus far aside from my classes i’ve been fortunate enough to have professors have trust in me to work alongside them in research and have been on two projects in the text analytics space doing sentiment analyses and lda topic modeling in my research i’ve realized i really like working with unstructured text and use statistical learning methods to make sense of unstructured data and to quantify text i like unsupervised learning a lot as well through research i’ve realized i want to pursue graduate school and maybe even a phd but i keep reading how theory based some of these stats phd programs are are there phd programs in stats which lean more on the data science applied stats side where i could do work with nlp or is every stats phd program tons of theory and working on super abstract mathematical topics like likelihood theory etc i don’t want to do a data science program because i’ve heard many data science grad programs can be just titles and have watered down courses i want to do a stats phd or ms at least but don’t want to delve too much into theory any suggestions
1,research are my statistical methods sound apologies if this is basic or if my explanation doesn t make sense this is my first analysis so i m still learnign the ropes i have a study with a two factor design basically my participants are completing two tasks one control one cognitive in two conditions one condition is a control i m working with 4 d imaging data 3d imaging data collected at 2 second intervals and am using a program to compute the statistics that essentially has a drop down list where you select your statistical tests i want to find out the effect of the condition on the cognitive task to do this i have to subtract the control task from the cognitive task filter out baseline activity and then subtract the control condition from the condition of interest so that i am sure the activity i am looking at reflects only changes due to the condition of interest i know that in order to get the difference between the conditions i want a paired t test i thought i would also need to used a paired t test to determine the difference between the two tasks so i first separated my data by condition and within each condition did a paired t test to determine the difference between cognitive and control tasks then i took the two results and did another paired t test to see the difference between conditions however i got some weird results that are telling me i did something wrong is nesting paired t tests something that is ok to do if not what is another way to go about this thanks i really hope this made sense
2,possible strategies for language vae using transformers could you suggest possible strategies for language vae using transformer language vae with rnns uses the encoder output as an input with gaussian noise into the decoder rnn 1 is it effective to follow a similar strategy for transformers as well by passing latent vector as the start token 2 use outputs of the encoder with gaussian noise as the attention weights into the decoder if you find better strategies please do suggest any
0,when to update models suppose you have a predictive model that has been used for a few years at what point do people decide to make changes to the model e g add new variables retrain the model with new data is there a standard procedure for doing this are old and new models often run in parallel i would be curious to hear how this problem is being handled across the industry thanks
1,bootstrapping for standardized rates using complex survey data when analyzing complex survey data e g cchs nhanes are bootstrap weights necessary when estimating cis for age standardized prevalence the formula for the standard error of the age adjusted rate only needs the count population number and number of individuals in the strata for the standard population none of these would change whether or not you use the bootstrap weights
1,quick check is the coefficient in logistic regression for a binary variable synonymous with the log odds ratio i have been suggested a paper that lets me convert cohen s d to a log odds ratio am i right that this latter term is synonymous with the coefficient in a logistic regression when the predictor is binary
0,what are methods that explain aggregated time series anomalies for example what feature values contributes to number of covid 19 cases at a given time interval let’s take covid 19 global number of positive cases for each day the global daily number is known by aggregating each country number and each country cases can be further grouped into different variables like by virus variant by country state etc if someone looks at the global time series it’s hard to know which variables are contributing to the number of cases without cutting and slicing the data are there methods that allow us to surface the variable values that contribute to the number or cases for a given time interval this sounds somewhat easy like i can calculate the contribution percentage of each variable value and sort them though i may also be interested in the contributing trend as well which can be positively contributing to the global number of cases and negatively contributing to the global number of cases from my cursory reading i found topics such as aberration detection or generally anomaly detection but these topics almost never explain what are the variable values that contribute to the anomaly it max explain anomalous points due to trends and seasonalities but not so much on what are the explanatory variables given that we have aggregated time series
2,are pre trained convolutions better than pre trained transformers – paper explained paper reference dehghani et al 2021 are pre trained convolutions better than pre trained transformers reading this paper with wrong expectations made ms coffee bean wonder what it takes for a transformer like architecture to be named transformer when does it become something else e g a cnn let s discuss outline 00 00 are you tired of transformers 01 12 what makes transformers so good 05 13 cnn vs transformers 09 53 what makes a transformer a transformer discussion
0,record linkage supervised vs unsupervised hey all i ve been tasked with figuring out the best way to implement record linkage between multiple data sources at my job my suspicion is that supervised approaches will increase accuracy over unsupervised approaches as long as we are willing to do a clerical review to create a training data set that is large enough to be representative admittedly this would be very time consuming but i think it might be worthwhile in the long run after reviewing many papers blogs etc i can t seem to find many comparisons of current supervised vs unsupervised algorithms has anyone seen any work on this any links or guidance is appreciated or just general record linkage insight is appreciated also thanks
1,accurate statistics textbooks for biomedical scientists i know that this question is asked quite regular but please hear me out what statistics textbook do you recommend that are clear comprehensive and accurate on the following topics x200b confidence intervals p values and hypothesis testing multiple comparisons experimental design x200b i really appreciate accuracy because i am not a statistician myself and have heard of several textbooks with misconceptions especially about confidence interval interpretation or recipes that i know statisticians really disaprove i m planning to use these books to teach other scientists but i m not sure all the textbooks i have used for myself over the years are up to date and accurate i m always looking for books written by statisticians or mathematicians to avoid basic mistakes but i also appreciate abstraction without dumbing down concepts too much x200b i have some candidates below if it helps x200b confidence intervals p values and hypothesis testing 1 zar biostatistical analysis 5th ed isbn 0131008463 2 rosner fundamentals of biostatistics isbn 978 0538733496 3 larry wasserman all of statistics harder x200b multiple comparison 1 jason hsu multiple comparisons theory and methods the first book from next section also has a chapter about this as well as milliken analysis of messy data volume 1 designed experiments experiment design 1 scott maxwell designing experiments and analyzing data a model comparison perspective 2 casella statistical design 3 box hunter hunter statistics for experimenters design inovation and discovery
0,cdo of the future background in data science or data governance i am basically at a crossroads where i can continue growing in ds and mlops and ds management or focus more on data governance i m in my late twenties and my career goal is to one day be cdo and i have been thinking a lot recently about which pathway will provide me with the best learning and development opportunities and will best equip me for that role on one hand data governance is about people documentation process driving a data oriented culture and doing it right and that is a great path to be on if your plan is to one day be cdo on the other hand i do like data science particularly the maths stats and mlops part of it and i have been successful at it so far at my company would i be pigeon hole ing myself too much if i stayed on the ds track shall i seek a more data management governance oriented track would you say that someone with a background in dg would have a competitive advantage thank you
0,working on a project analyzing my etsy sales data i m not sure if this is the right place to ask but here it goes i started my etsy sales dataset in the daily format then i grouped them by year month i ended up with the following for months where i don t have any sales for that specific listing id i want to have the month and zero next to it for example listing 902533496 from the last screenshot i want to add these rows 2021 01 0 2021 02 0 2021 05 0 anybody has an idea on how to do this thank you
2,neural networks with memory hi all i am a beginner in ml i have previously used feedforward nn to for regression and it works great i would like to know if there are neural networks that retain memory for example i trained nn using data 1 and obtained model 1 i have another set new data called data 2 is it possible to use model 1 and train using data 2 and obtain new model 2 which remembers data 1 and data 2 in some sense to save time currently i have been stacking together data 1 and 2 and training using feedforward nn however i get data in multiple batches and stacking the all the data for n batches would be really large and would be computationally expensive looking at alternative that train the nn on the go thanks a lot in advance
1,propensity score matching validity hello fellow statisticians i would like to perform a propensity score matching procedure in order to appraise an intervention for the sake of discussion lets assume this is a government grant robustness my series of questions if i follow the standard procedure of using a pairing algorithm and then compare the treatment and control groups across their respective observable characteristics what are the signs that the pairing algorithm has produced a robust comparison frame model specification does it make sense to include categorical variables in the model specification for the psm procedure i also have a variable that scores grant recipients against selection criteria a continuous variable this is an imperfect proxy for treatment but correlates very well should i include this in the specification for the psm model generalities any general mistakes that newbies often make regarding internal validity if you will how can i insure that the procedure is robust your help is greatly appreciated thanks
2,what platforms do ml professionals use to express themselves for entertainment you have tons tiktok instagram etc where can one find a more meaningful place for knowledge exchange view poll
0,python courses for experienced r programmer i m a mid career data scientist and have somehow managed to stick to r the past 7 years i m going to be looking for a new job in the next few months and would like to be able to put python on my resume basically just have it down enough that i can still work if i get hired by a python shop i feel like once i have to use it i ll become proficient pretty quickly i progressed halfway through a coursera course only to find the rest of it is using ibm watson studio since this is not industry standard it seems like a waste to put energy into can anyone suggest some other python for data science courses
2,looking for case studies on churn and or risk rating models i ve been trying to locate some industry papers blogs on churn and customer risk rating but all of the results that seem to turn up are mckinsey esque in that they only address higher level concerns i m hoping to find guidance on the engineering aspects of these applications think systems design model architecture best practices for database maintenance etc put another way pretend i m starting a company and want to learn all there is about how machine learning and mle intersects with churn and risk rating models thanks
1,no research internship experience for phd application i am confident this question has been asked many times in the last 40 years but i don t see any answers on reddit specifically for phd and only one amstat article from 2014 saying research experience is rare pardon my ignorance i transferred from a community college to a top university and it looks like i will have no research or internship experience before the coming fall application cycle if i have a 3 5 gpa including proofs based linear algebra real analysis 1 numerical analysis 1 and probability mathematical statistics and good gre scores am i likely to be admitted to any decent not top 30 statistics phd program or should i just do a thesis based master s first the most my professors can say about me is went to class and office hours and i will take complex analysis mathematical optimization stochastic processes and linear models in the fall semester but i don t know if i will be able to submit those grades in time thank you for your honesty
1,creating an index with pca principal component analysis hello everyone i have run a pca in stata with 4 components of these 4 components only the first 2 have eigenvalues 1 and their cumulative variance explained is 0 72 i want to create an index using these two components but i am not sure how to determine their weights i was thinking of weighing each component by the variance explained so that index pc1 0 52 0 72 pc2 0 20 0 72 is it correct are there more precise ways to weight the principal components edit do i have to “rotate the components” i didn’t understand what this procedure implies
1,i’m applying for a job in biostatistics what topics should i brush up on as the title says i’m applying for a job internally where i currently work for a biostatistician position i was recommended for this but haven’t brushed up on statistics topics since i graduated last year any suggestions for topics to refresh for the possibility i land an interview all help is welcome thank you
2,architecture search in practice what s your go to there s been tons of papers on nas but in my experience most suffer from being either a very slow b not easily applied to existing code or c focused on one application mostly image recognition also there s a big difference between looking good in a paper and being a useful and versatile library if you re using a particular library i would love to hear your experience the good and the bad pytorch based ones in particular
2,is it possible for a model to increase overfitting when seeing new training examples for the first time so i was running a cnn over a large image dataset 94k images and i was concerned about overfitting so i implemented early stopping but if i set 1 epoch to be all the training data i found it overfit a lot before early stopping had a chance to stop it so i reduced the epoch steps to about 10k so it would check after each time i found that even after 3 epochs the early stopping trigger when it measured that the validation mse was increasing even with a patience of 2 epochs but 3 epochs is only 30k samples so the model hasn t even seen all the data points it s being given new data points and some how the validation mse is increasing even as the training mse is decreasing if i were training over the same training data many times i understand why the model would overfit but i don t see how giving it new training examples is reducing its ability to generalize my guess is one of two things is happening 1 the model overfitting isn t increasing and it just got lucky on the first run somehow 2 the model is overfitting due to some other purposes the validation set i m using is the same each epoch so that shouldn t be an issue has anyone experienced this before update i just did another test and the validation mae is way better than the training mae in the first epoch but stays roughly consistent and the training mae eventually overtakes it similar situation with the mse
1,how to interpret bidirectional causality in the case of granger causality hi all im currently writing a report that utilises granger causality tests to check the influence of numerous factors on the price returns of bitcoin in the case of the volume traded i ve found bidirectional causality by this i mean y granger causes f x p value 0 00005 f x granger causes y p value 0 05 i would regard these as both significant and therefore bidirectional i am unsure though of how to even comment on this what can be said and what cannot the two other cases where one value granger causes another or no causal relationship are relatively straightforward conceptually i am not sure how to interpret this result any advice guidance is majorly appreciated edit sorry forgot to add the p values are the result of an f test there is also significant terms from the exogenous variable in var analysis
1,is a most powerful invariant test ump over all level alpha invariant tests given a testing problem when talking about a ump test is it correct the test is ump over all level alpha tests for some specific alpha when talking about a most powerful invariant test as in sec 6 3 in lehmann s tsp is the test ump over all level alpha invariant tests or just ump over all invariant tests without requirement to meet a level note that a ump unbiased test is ump over all level alpha unbiased tests so i was wondering if a most powerful invariant test is also similar thanks
0,anyone working in a music analytics streaming company like spotify hi there i m an aspiring data scientist hoping to one day work at a music analytics streaming company such as spotify pandora or tidal i would like to learn more about what data scientists do at such companies and how they analyse user data to eventually drive company decisions if there s anyone here working in a similar field or even have similar interests i would love to have a chat with you to learn more about what you do if interested please send me a message or comment on this post so that i can get in touch with you thanks
1,ucla vs ucsb vs sdsu can someone please help me decide between these schools ucla ms biostats ucsb ma statistics sdsu ms statistics i am still planning on getting a phd after completion of masters not partial to biostats despite extensive life science research background my biggest worry is if i go to a lower ranked ms like sdsu will i be shooting myself in the foot if i want get into an elite phd program afterwards ucsb pros funded uc system strong faculty focus in financial statistics but still diverse faculty in terms of other research interests degree is in stats rather than biostats so more opportunity to go into a biostats or stats phd ucsb cons small town 100k ma instead of an ms not sure if this matters ucla pros will likely be funded highly regarded school in uc system and might help application when applying to phd programs located in la less theoretical program than ucsb ucla cons located in la biostats in instead of plain stats sdsu pros located in sd where i live and would ideally like stay likely funded and or very cheap option to concentrate if biostats if i want but still an ms in stats located in massive biotech scene sdsu cons not well known state school and i want to go to a good phd program afterwards very small faculty but they are diverse in research interests
1,what is burning data from rishi narang s inside the black box the world finds new and interesting ways to confound our understanding as such to test our current best thinking against competition that existed in the past is a form of wishful thinking this is a subtle and nefarious form of look‐ahead bias which is a critical problem in research as researchers become more and more familiar with the out‐of‐sample periods they use to test their ideas’ validity it becomes more likely that they are implicitly assuming they would have known more about the future than in fact they would have known had they been asking the same questions historically this practice is called burning data by some quants x200b i m unclear what this means does it just mean that every time you iterrate your model based off the oos test results you are engaging in sort of lookahead bias x200b an earlier paragraph states this by learning from the out‐of‐sample data and using that information to train the model anew he has effectively used up his out‐of‐sample data and has caused them effectively to become part of the in‐sample data set so basically everytime we learn from our results and make some adjustment to our initial model are we effectively using a form of data creep
0,selecting a limited number of features due to restrictions from the software development team i need to restrict the number of features variables for my model to within a maximum number say 10 or less i am wondering if anyone here has run into such a constraint and how you went about handling it assuming the restriction above is non negotiable and trust me we have tried negotiating with the software folks i have tried a number of selection methods to go about doing this and selected the top 10 features for instance as sorted by the following 1 feature importance xgboost 2 boruta 3 shap some drop in performance was obviously expected relative to model with all features available but the drop in performance has been been much sharper than i am comfortable with using either of the three methods listed above therefore i am hoping folks here have experience with better ways to go about doing this i would very much appreciate any feedback
2,why is batch norm becoming so unpopular i read a few papers recently that stress that the architecture is batch norm free and know that there are recent advancements by deepmind and with the vision transformers that do not need it why is it so advantageous not to have batch norm the only thing i think i read is that calibration of nn output gets better when not using batch norm
1,pearson s r in psych does anyone have an objective interpretation of correlation coefficients for psych social science the only citation i can track down is the textbook by dancey and ready and i d like a non textbook source
2,parametric spectral filters for fast converging scalable neural networks this is my first primary author publication i d appreciate any feedback suggestions or thoughts x200b paper ieee xplore implementation
1,pursuing a degree in stats even though i haven’t always had the best relationship with calculus i have a lot of questions and i would appreciate your help 1 i took a look at the courses i have to take and there are only two calculus courses how important is calculus in order to understand statistics anyways 2 how much studying did you do when you were getting your degree 3 would really appreciate if you answer this what would you do differently if you had the chance to go back in time and do your degree all over again any subjects you would focus more less on any courses you wish you took hadn’t 4 have you ever met someone who didn’t have the strongest background in mathematics yet still did very well in this field thanks to anyone who takes the time to read or answer appreciate it
0,are bi and data analyst roles so similar that one could use the same resume for both positions i m asking because i am six months away from going back into the job market after graduating i am starting my search now to get feedback and hone my resume i will be targeting both bi and data analyst roles i was wondering for those in the field i could not notice a vast difference between bi and data analyst roles to warrant creating a separate resume for both is this valid or is there something i am missing
2,d inspecting neural networks with canonical correlation analysis cka svcca video canonical correlation analysis is one of the methods used to explore deep neural networks methods like cka and svcca reveal to us insights into how a neural network processes its inputs this is often done by using cka and svcca as a similarity measure for different activation matrices in this video we look at a number of papers that compare different neural networks together we also look at papers that compare the representations of the various layers of a neural network x200b papers covered svcca singular vector canonical correlation analysis for deep learning dynamics and interpretability understanding learning dynamics of language models with svcca insights on representational similarity in neural networks with canonical correlation bert is not an interlingua and the bias of tokenization similarity of neural network representations revisited similarity analysis of contextual word representation models do wide and deep networks learn the same things uncovering how neural network representations vary with width and depth
2,how to get tensorflow model to run on jetson nano hello i am relatively new to ml al went through andrew ng s course a few years ago and i find myself in a bit of a battle in getting a tensorflow model to work on the jetson nano i built a model for anpr using tensorflow and easyocr over the past week or so getting tensorflow to install on the jetson nano has been next to impossible tons of issues with it some are documented and overall i found one person that was able to get it running well which took over 50hrs to install on the jetson nano this said there has to be a better way to get a tensorflow model to run on a jetson nano possibly i should try to convert and use with onnx
0,i got a ds related internship at hp hello all so recently i started working at hp as a product management intern i know that’s marketing not data science but i’ve spent the past two weeks learning all about it and i find it very interesting so for context i’m a senior and this is my first paid internship i’ve done 5 unpaid internships over the course of 3 semesters before this in hopes it would land me a real one and it actually did i spoke to a hp recruiter which gave me a contact at another company after 3 months of speaking to several people at the other company just to result in my intern application being rejected i returned to said hp recruiter their website had no internship that fit me so i spoke to her directly which led to two interviews and an offer and after viewing my work profile i saw that said internship was meant for an mba candidate not an undergrad therefore i’m guessing my experience made up for what level of education they were looking for thus i signed on to be a marketing manager product management intern for the z by hp division and z makes computers for data scientists like high gigs of ram more core processors and intel gold and probably some other stuff but my tech knowledge isn’t super expansive and while working with them i’ve interviewed a lot of ds professionals about their computer needs and what type they use also about what sw stacks they like and what language they code in and there has been quite a consistency amongst their answers therefore if you are a ds undergrad graduate looking for a job or internship i have some advice for you 1 diversify your skill set most people i spoke to didn’t have a ds degree rather they started their careers something went wrong and they made a career shift to ds it was easy to switch to because from what i can tell ds is growing all companies are dependent on some sort of cloud and python is their preferred code language so with the influx of people entering ds if you started in ds learn for than one coding language and make sure you can use a variety of sw stacks so that you can separate yourself from the competition 2 take an unpaid internship only if it is worth it like i said i’ve had 5 but i quit one early the reason is if you aren’t going to pay me i’m not going to perform work i believe a paid person should which i know is technically any work but just watch where i’m going with this i quit one because there was no way i was going to be at the stadium 3 hours before the game starts run around setting every event table up and it’s accessories miss the entire game because i have to help people the entire time not have a break get off at 11 pm not have a parking pass and on top of that only be fed a mini jimmy john’s sandwhich but there were others where i liked what i was doing and they only required 10 hours or less work a week i’d never advise anyone to work 40 hours a week for free i wouldn’t advise even 15 hours a week for free just a few internships to pad the resumé but most importantly actually provide you with some knowledge that will be applicable to your next job 3 apply even if you are not qualified this applies to both internships and scholarships because i’ve gotten my way with both these things provide too much for you not to try i almost didn’t apply to the scholarship that takes care of all my tuition for my last two years of college but i did i almost reach back out to the recruiter that led to my internship now at a company that will pay off the few student loans i do have hell i almost didn’t apply to transfer to the university in at now after my freshman year at one i didn’t like but was the epitome of “safe” so believe in yourself and pull the trigger this saying be be shot to hell but it’s true you only live once unless you believe in reincarnation but you still only live this life once
2,visualizing latent feature spaces throughout the layers using umap we always imagine how classes get more and more clustered throughout the network in this simple demonstration i took a trained vgg13 on cifar10 and show a umap on the unraveled features at every latent space throughout the layers umap of selected layers of a model trained on mnist umap of layers of trained vgg13 on cifar10 full post
1,question about standard deviation with respect to the summands of a sample mean i m teaching myself statistical learning and ran into something i don t really understand im using the book introduction to statistical learning with applications in r on the last sentence page of 65 it says we have the well known formula var mu hat sigma 2 n where sigma is the standard deviation of each of the relatzions of y i of y i understand the formula and how it s derived as i have taking calculus based probability and statistical inference before but what i don t understand or remember is how does a single y i have a standard deviation isn t y i just one of the summands in the sample mean y bar in other words i thought y i is just a data point so how does it have variance or standard deviation
0,what is the percentage breakdown of your workday what is your job title and how many years of experience do you have
2,any issues with ubuntu with dual boot and rocm currently working using a friend s old build with a ryzen rx 5700 xt the gpu market being what it is right now the support for deep learning is nonexistant for amd rocm on windows 10 so i plan on creating a dual boot to ubuntu and using a docker as my setup as i am quite unfamiliar with such an environment i wanted to ask if anyone has ever encountered problems with such a setup before
0,am i wasting my time learning r hey guys outside of one statistics class for stem majors that i took which involves python jupyter notebooks and the likes the rest of my statistics courses have been using r as the main programming language for our homework and projects i m a senior majoring in applied mathematics i constantly see here that python is the now and r is being utilized less is this true should i just derp my way through the rest of these classes without much thought to learn it better so i can focus on getting better at python
2,how to track down elusive paper materials case study wasserstein dependency measure for representation learning here s my problem i would like to investigate claims made in a paper afaik the following is true 1 paper has no official code repo 2 paper has no presentation explanation demo video the paper in question is wasserstein dependency measure for representation learning i was grateful to find at least one open source implementation but i m not confident it s enough to reproduce the results in the paper because the focus of the experiments are different has anyone reading this post tried to implement this paper do i pretend like the paper never happened because the results can t be replicated generally speaking what do you do when you find yourself in this situation what do
2,how to improve my object detection model hi i m an experienced software dev but have limited experience with ml i m currently trying to train a model to detect objects in the output of a 3d mobile game here is a couple of example images the first is just an example the second is the result of my model being applied and objects being tagged there is only one object tagged though bottom left when clearly there s lots of other objects that should be how should i best go about improving my model i suspect the difference in orientation and background is throwing it off so are more inputs and training the answer
1,question 1461 poem combinations hello i have a rather special question i would like to write a set of sonnets a poem with 14 sentences that would have as many poems as there are days in four years so 1461 poems each poem would be a unique combination taken from a group of sentences the first line would be chosen from x sentences the second from y other sentences some lines might not change in short i would like to know how many sentences i would have to write the minimum possible to reach the 1461 poems i hope my question is clear english is not my first language sorry i forgot about the flair thank you in advance for your help
1,finding relationships when making crypto predictions question hi i only know basic stats currently but will be learning more via this i am looking at tests to find relationships the data will be a time series for each token coin i am planning to use granger causality dickey fuller for stationary of the series maybe a usual chi squared test maybe i need one looking at correlation after performing these tests and possibly more i will use machine learning and see what that has to say this will involve regression i am here to ask what other tests may be useful and what methods could help for time series forecasting thank you
0,mc2 a platform for secure analytics and machine learning disclaimer i m not affiliated with the project mc2 is a project that aims at helping data scientists and engineers collaborate and process collective data without sharing the content of the data with one another not even the cloud provider i think the project is very interesting and i d like about the use cases and what you will do with it
2,model playground machine vision ml ops tool hey peeps tristan here hasty ai cofounder check out our modelplayground at – we d love to hear your thoughts and feedback aim we are building a smooth transition from our annotation tool to mlops for data centric ml as you cannot know a priori where improvements in performance will come from we believe in building a good process for ml dev rather than chasing sota and we hate the ml frankensuite problem when starting with hasty no one talked about data centric ml we had to learn our lessons the hard way kostya alex—my co founders— and i worked on several projects in the german industrial space and we felt the burn given our domain standard approaches to jump start learning with unsupervised or semi supervised learning synthetic data gans pre trained models inset random technique here etc weren t cutting it so we ended up spending evenings labeling data there had to be a better way solution first we build an annotation tool that scales up custom trained models for annotation automation and qa – that is working well now we are launching mp for you to be able to experiment prep and deploy models bringing the worlds of ml ops and annotation together
2,multivariate forecasting with facebook prophet a small open source package for using fbprophet for multivariate forecasting blog post github repo x200b let me know your thoughts all feedback and prs are welcomed
1,how sensitive are statistical models to the richness of information within the data i spent the last hour thinking and creating an example that illustrates my question my question indirectly relates to exploratory data analytics and feature selection for statistical models suppose you have some variables let s assume they are categorical variables for this example when you make a histogram for these variables they appear extremely skewed on first glance you would not want to include heavily skewed variables as inputs for a statistical model e g if 99 of the variable is a single value how informative and useful could it be to a statistical model but how do you know that these heavily skewed variables don t contain some very useful information in the 1 that might really help you in making future predictions sometimes using the context of the problem e g if you working on a biology problem consult with the biologists you might be able to gain some insights other times you can try to use some logic to figure out if these heavily skewed variables are in fact useful for the model but when you have big and complex data how is it possible i posted an example above that illustrates this problem i would be curious to know if any of you have dealt with similar problems in the past thanks
1,spearman s rank correlation for qualitative data 1 how can we use the spearman s rank correlation in case of qualitative data are there any advantages to this 2 also what other correlation techniques could be used for qualitative data
0,realize i don t want to be hardcore stats guy i like statistics and the theory behind a lot of the models implemented in data science but i don t think i can be learning this stuff for the rest of my career i feel like the more i learn the more i realize how much more there is i m not sure if all subjects just keep getting deeper and deeper or if data science is just really hard to master due to the combination of cs and stats right now it s the linear algebra foundation of pca i can obviously do the matrix multiplication but i don t understand how it reaches the result and feels like magic to me but i have so many subjects to go through not looking forward to data structures and how computers work i feel like i can keep doing this for 4 more years or so but i m worried it s never going to end due to the field evolving i really like presenting to upper management and applying business domain to problems and just plain old thinking about problems i m not sure what this realization means for me i guess i ll keep up with data science for now but where do data scientist go after they say enough math or am i just being a wuss
1,how to explain the change in significance due to the addition of an interaction variable i am currently working on my dissertation for my master in finance put simply i am running panel data regressions to analyze the relationship between a firm s valuation and their esg score environmental social governance score in stata the simple regression form without an interaction variable looks like this value b1 esg controls using country and year fixed effects clustering for firm standard errors the esg coefficient seems to be strongly insignificant when i add an interaction effect with a dummy variable of a control variable that represents a specific industry my regression looks as follow value b1 esg industry x esg controls using the same country year fixed effects and clustering the results indicate that both the esg variable p 0 016 and the interaction variable p 0 004 turn out to be significant how can it be explained that without this interaction the esg variable seems to be insignificant while the esg variable becomes significant with the addition of the interaction variable thank you in advance
2,wyze cam and ring cam campfire discussion for computer vision purposes basically i m doing a cv system with deep neural nets that is monitoring specific outdoor images of natural environment it would be nice to use the consumer grade cams from ring or wyze these have cloud subscriptions and some under 100 dollar cams i have already acquired and paid for cloud and cam for both systems hoping one would work well for cv data capture i expect that an rdtp server on cam might help since i expect i then can write code for my own custom client and save any video stills that i find interesting in the past 2008 i made pre machine learning vision app with cheap pc webcams to automatically measure the speed of car traffic and it was semi successful as you can guess special cases began to overwhelm my rule driven analysis code and machine learning will work much much better i expect the current apps i am working on are not vehicle speed measurers my findings so far for wyze cams outdoor and v3 as of early 2021 must click in the ui to manually download pre motion detected segments one at a time even for paid cloud subscribers this is a problem no programmatic api even for paid cloud subscribers this is a problem for some not all cams they sell there is an rdtp server firmware you can put on the cam to replace the original firmware they call it beta release level not production level with all that implies rdtp would enable my code to read all video all the time so it will not miss anything the provided motion detection too often misses my subjects that move in my manual tests i guess it s pir driven not ai driven in the provided motion detection and my subjects are not different enough in temperature than the background this means that their provided motion detection probably is more trouble than benefit i will have to do my own motion detection which is fine really as i am confident i can do this myself wyze has a mode provided in the standard cam firmware to do so called time lapse which is basically taking a still photo at a chosen fixed frequency like once per minute during a defined time range like 6 hours this might help me reduce the file sizes that need to be saved to the local sd memory card it would effectively be sampling instead of continuous time recording thus i would miss some interesting events but on the other hand i will still get some interesting events the wifi that is built in makes them very easy to install in good locations for my projects as for the ring cams slightly more expensive slightly less useful and flexible basically all of the same problems as wyze cams no rdtp that i could find let me know if i overlooked to summarize i might have to go back to pc web cams at least i can do everything with the images it s not a closed proprietary cloud holding my data it would be a shame to go back to pc web cams because the wifi in ring and wyze cams make them very easy to install in good locations for my projects also these new cams have greatly improved resolution and color versus my old web cams from 10 years ago i feel like approaching these companies to ask for api to be installed and published so scientists can use their cams i feel like anyone who pays a subscription to their cloud should ethically and morally be allowed by these companies to use the api without restriction to read download their own image data from the cloud in your own cv projects what have you found that works for a cam system i expect a phone cams is another possibility esp older phones that have no other uses and just sit around in a drawer is there an app for rdtp serving on android phones thanks for reading and sharing
2,custom image data pipeline using tensorflow while using convolutional nets to perform image classification i encountered several difficulties importing image data using the native tensorflow functions so i created my own custom image data pipeline which gives the user finer control and more flexibility when importing image data i hope it is useful to you guys as well let me know what you think
2,bruno maisonnier ceo and founder another brain previously aldebaran robotics we ieee soft robotics podcast are going to have bruno maisonnier the founder ceo of another brain and previously founder of aldebaran robotics acquired by softbank group if you have any questions x200b
0,for data scientists and bi analysts at companies large and small how do you deal with customer privacy when building models and or dashboards i m a data scientist at a medium sized financial company lately we ve gotten quite concerned about customer privacy in data science and analytics and while i have a high level of personal ethics on the topic i m not familiar with what is typical at other companies in how the privacy of customers is handled while doing data science tasks for internal data science projects how do you reconcile the need to restrict access to some personally identifiable information with the need for as much data as possible in a model do you have names addresses hidden to most by default with an exception process for when this information is needed how is consent handled normally does a data scientist assume that there are processes in place to ensure that whatever data is available in the warehouse pipeline whatever has adequate consent for the purpose you re using it for do you need to do a check on consent before any development can begin thanks for any discussion on the topic
1,help needed correlation plant diversity urbanization degree i couldn t find my answer online so i hope someone here on good ol reddit can help me out at the moment i m working on my applied biology thesis the study is about whether there is a difference in plant diversity in same plots green verges in different urbanization degrees 0 20 is extremely low urbanization 20 40 is low urbanization 40 60 moderate 60 80 high ans 80 100 extremely high this is based on the surroundings i want to find out if there is a correlation between the amount of species found in the green verges and the amount of urbanization in the surroundings does anyone know what i can do a pearson won t do because i dont have precise percentages of urbanization per area i divided each location into the 5 groups mentioned above
0,what does it mean when changing the seed changes the predictability of a model i have a glm model that outputs a percentage of likelihood in predicting a binary outcome variable whenever i change the seed the prediction value for all elements is different every time i ran some loops with hundreds of different seeds and saved the results and the majority are in the same ball park but some are way off you could say if you get many different results it’s a bad model but if that’s the case i don’t get how some seeds are super predictive the sample size is too large to be considered random chance any guidance here would be appreciated
0,any of you learn web dev or app dev for a change i know many of you have jobs and don’t really have the time to learn new things but do any of you get bored or tired of just doing “data” related coding whether it be training models data cleaning data visualization and just try and learn how to build a website or an app i’m a student and ive just gotten bored of doing sklearn tidyverse pandas tidymodels all day or doing data stuff with python or r and just feel like learning react or something random like that for a change of pace i don’t actually intend on getting into web development i want to be a ds but sometimes too much data stuff bores me anyone feel the same way and if so what did you learn
0,how do you document your datasets sometimes i fell 90 of my time is spent in data wrangling munging is on data exploration and most of that is spent trying to make sense of fields created by some long lost business logic or trying to discover how that dataset came to be as a consultant i cry tears of join when i see a team that keeps a data dictionary or a well organized catalog even if on excel what is the best documentation practice you ve seen
1,what does it mean when the prediction interval is too wide to the point it s useless hello i am using quantile random forest to get the mean prediction and 95 prediction interval i used quantile 0 025 and 0 975 to get the lower and upper prediction interval the mean prediction that can be like a point estimation is close to accurate but the prediction interval especially the upper limit is way too high to the point that it s useless if i just get the 0 5 quantile the model seems like it s performing well but for my project i need prediction interval as well can you please tell me what can cause such a wide prediction interval and is there a way to fix that thanks
1,why is convolution a meaningful mathematical operation i was watching an introductory video on convolutional neural networks and the convolution operation itself seems so interesting it seems that convolutional neural networks take a picture and via convolutional layers repeatedly shrink the original picture to a small matrix of numbers this small matrix of number is associated with a label e g dog or cat and then the neural network can learn to recognize similar pictures i had the following questions question 1 why is the convolution process a meaningful operation how are convolutions able to capture and filter meaningful information from pictures i know i am wrong but from a layman s perspective the convolution process i e taking the algebraic dot product between filter weights and the pixel values of a picture seems somewhat arbitrary i just find it very interesting that the dot product between these two entities is able to capture and filter meaningful information about the pictures just a guess does this have to do with the universal approximation theorem i e the universal approximation theorem also applies to cnn s question 2 i have often heard claims about neural network based approaches being intended for non tabular data such as pictures the argument being neural networks work better when there is a higher level of richness within the data e g pictures containing 786 pixels and each of these pixels usually being non empty this is usually contrasted with tabular data such as data that can be easily placed into excel spreadsheets e g typical of finance data apparently tabular data is more prone to containing less informative data compared to pictures potentially making the performance of neural networks worse when dealing with tabular data it is said that boosting and bagging based techniques e g random forest are more likely to yield stronger results i was just wondering why aren t pictures normally considered as tabular data couldn t you just take the individual pixel values and collectively store them into matrices i e tabular form
0,what’s the deal with python becoming more popular in job ads i graduated undergrad in late 2000’s grad school in early 2010’s i’m a statistician focused mostly in the social science medical health insurance and public health spheres around 2010 2012 all i heard was “you gotta learn r” so i did and i’m pretty proficient with it now then i heard you gotta learn sql and now i’m an intermediate sql user i mostly just use it in sas commands it’s great with large datasets then in the mid 2010’s you gotta learn tableau check easy enough then the tidyverse package which has a lot of overlap with sql skills anyways then other packages then a few job ads wanted spss amos and or mplus now almost every job posting for statistical data analytics type roles i’ve come across wants me to know python i’m sure i’ll pick it up eventually but i can do most of what i need to do in r and other tools hiring committees can’t explain why they want me to know python i think they just stick it on there because it sounds cool wow snake programming language what’s up with these committees and job ads and their affinity towards python yet they don’t know the value or applicability or necessity of it in these roles
0,i have a large dataset 100 mil rows in russian i want to translate it into english i was using googletranslate api which was showing error coz of a high number of requests is there anything else i can do
0,what are some exciting new tools libraries in 2021 hi everyone i am an industry data scientist one of the problems that i find is that while working at a large company there is some adoption lag with some new tools libraries could anyone help point me in the right direction for software tools libraries that are picking up steam this year i remember hearing stuff about the julia programming language a couple of years ago but not sure if that has risen in popularity
0,respected data science bootcamps focusing on cloud databases big data frameworks and statistics i m currently employed as a full time data scientist but functionally i m more of a machine learning engineer ml is fun hot and all that but i m more interested in taking a higher level approach to inferring insights from data via ml or otherwise also despite my title i haven t ever built a data base or used cloud computing it s been mostly local computations csv files pandas etc so i d love to patch up those holes so i can do more properly data science work given that are there any somewhat well known respected boot camps or moocs out there that would help me patch those holes i have a bs in physics so something more mathy wouldn t be a huge issue so far i ve been looking at datacamp and springboard but i d love some of your thoughts x200b update i read all your comments and i really appreciated the feedback it seems like the best move is to hit some books and maybe take some one off courses in specific libraries or technologies that would be useful next steps include identify a solid statistics course textbook preferably something with rigor as i like thinking from the ground up a one off course in sql a one off course in aws azure and a one off course on hadoop or spark i think that ll cover all my bases
1,if you plot two gaussian random variables against one another the slope has a certain distribution nassim taleb argues that the distribution of the slope allows scientists to game it what does this distribution tell us over and above the well known fact that p hacking is easy for context the goal of nassim taleb s video was to criticize psychologists for making a big deal out of small but significant correlations in the video he develops the characteristic function of the slope of two iid random variables plotted against one another x y are iid gaussians and he derived the chf of beta in y beta x i am not sure i understand what this distribution tells us is it just telling us that the slope has some distribution and that it s therefore not guaranteed to be zero even when the variables are independent or is there something more
0,any data scientists and data engineers who work in hospitals care to discuss their experience i have a similar post on r cscareerquestions to get information from the software engineering side my company wants us back in the office asap and my wife got a new job offer in nyc the job market in nyc is still kind of rough at my level of experience but was thinking of looking at non profits and hospitals too nyc seems to have a huge amount of big hospital systems including columbia nyu cornell and mount sinai so it seems like a big area to work in can anyone describe the interview process salary average workday and potential to switch back into the tech industry afterwords
2,why is lstm gru not mentioned in time series classification state of the art review i m reading up on state of the art of time series classification and i just read deep learning for time series classification a review fawaz et al 2019 which summarizes and compares different modern deep learning approaches however it doesn t mention lstm or gru which surprises me a lot since they would be among the first approaches you d read about in any recent introductory textbook or course on sequence modelling i cite the only section that mentions rnn another popular type of architectures for deep learning models is the recurrent neural network rnn apart from time series forecasting we found that these neural networks were rarely applied for time series classification which is mainly due to three factors 1 the type of this architecture is designed mainly to predict an output for each element time stamp in the time series längkvist et al 2014 2 rnns typically suffer from the vanishing gradient problem due to training on long time series pascanu et al 2012 3 rnns are considered hard to train and parallelize which led the researchers to avoid using them for computational reasons pascanu et al 2013 do you agree and do you think lstm and gru are becoming obsolete with the emergence of convolutional approaches
2,accumulated local effects i recently came across a newer technique called accumulated local effects that attempts to explain the effect of predictor variables on the response variable has anyone tried using this method on real data did you find it useful any stories anecdotes experiences comments reviews you would be willing to share regaeding this method
2,paper explained decision transformer reinforcement learning via sequence modeling full video analysis proper credit assignment over long timespans is a fundamental problem in reinforcement learning even methods designed to combat this problem such as td learning quickly reach their limits when rewards are sparse or noisy this paper reframes offline reinforcement learning as a pure sequence modeling problem with the actions being sampled conditioned on the given history and desired future rewards this allows the authors to use recent advances in sequence modeling using transformers and achieve competitive results in offline rl benchmarks x200b outline 0 00 intro overview 4 15 offline reinforcement learning 10 10 transformers in rl 14 25 value functions and temporal difference learning 20 25 sequence modeling and reward to go 27 20 why this is ideal for offline rl 31 30 the context length problem 34 35 toy example shortest path from random walks 41 00 discount factors 45 50 experimental results 49 25 do you need to know the best possible reward 52 15 key to door toy experiment 56 00 comments conclusion x200b paper website code
0,recommender system advice i have a problem statement of designing a recommender system for a pharmaceutical company which also produces healthcare products vitamins body care products etc but they don t have an e commerce website similar to amazon etc therefore data collection is incomplete and dodgy to say the least their current approach involves placing adverts on social media platforms like facebook instagram etc and then getting gathering user data from such platform by either using platform specific apis and or web scraping to design a recommender system for this company can you suggest 1 what data features i might need 2 which system to use collaborative content based filtering i am new to recommender system domain and this will be my pilot project thanks
1,bio stats phds i have been hearing that a lot of bio stats jobs do way less real and interesting works than data science roles i was wondering if this was still the case for people with phds i am interested in bio stats because of the ability it gives to help the world such as work related to pandemics but don’t want to just be trying to get drugs past the fda also is a bio stats phd just as rigorous and respected as a normal stats phd when applying to industry or academic roles thank you
0,periodic snapshots for jupyter notebook i come from a business intelligence background and i m looking for a way to create data snapshots using a jupyter notebook long story short they told me during the interview process they had a data warehouse and that turns out to be not quite true i probably won t be there long but i ve been asked to create something that sounds a lot like something that would be supported by that kind of data structure etl process what i m looking to do is to be able to run a sql query and then dump it into a month file with the snapshot date in the file name and the snapshot date in the file the same thing would happen the next month and so on and i would load the files in and append to perform the analysis has anyone had to do anything like this before or can provide any useful resources references thanks
0,anyone know of this type of ordinal encoding in python i use my own home brewed feature engineering package in r but haven t had time to translate it to python i learned from a mentor a while ago about this strategy for encoding ordinal variables which is useful for certain coefficient based regression models the idea is similar to one hot encoding but each level adds on to the effect of the prior lower level so let s say you have the following data customer salary score a 1 b 2 c 3 d 4 you only have the score of the salary which is essentially a rank needs to be encoded somehow to use in a nn what i personally call ordinal encoding in my personal r package would transform this to customer ss 1 ss 2 ss 3 ss 4 a 1 0 0 0 b 1 1 0 0 c 1 1 1 0 d 1 1 1 1 does anyone know of an implementation of this in python
2,most valuable skills for data scientists and machine learning engineering roles hi i have almost finished my master degree in cs with a special curriculum on data science and machine learning the majority of my knowledge are theoretical and at academic level since i want to be prepared for my future jobs what are the most valuable skills for data scientists and ml engineering roles that are not taught or underestimated or what are those skills and knowledge that you would have learned before starting to work i have two months off this summer and i want to use this time at the best
1,how to calculate effect size i am trying to calculate the effect size of exercise on depression so i have 2 groups control and intervention in an rct setting i have both mean values and standard deviation of groups at t0 baseline and tf final the number of participants in each group is different due to dropouts etc hence the variations my question is how should i calculate an effect size out of this setting intuitively i feel like i need to standardize every mean value first calculate smd standardized mean difference between tf and t0 for each group and compare take smd again this time for the difference between groups however i am not sure how should i take the variance into account should i just use the variance to standardized each value of the group but how i am going to estimate the variance of the effect size
0,is anyone trying to switch out of data science and if so what jobs are you applying for i m pretty interested in software engineering and i m actually looking into more client facing sales roles i was wondering if anyone has had a similar experience
2,make a model do binary classification on a user specified class in a multi class scenario title sounds confusing but let me clarify each image can only contain 1 class let s say you have 3 classes 0 1 2 but you only want to see if class 0 is in an image model image 0 outputs binary classification of class 0 with sigmoid threshold this is not the same as straight up the model outputting logits for all 3 classes and then softmax argmax because then the model predicts either the most likely class even if the probability of the 2nd most likely class is high x200b i feel like that this has been done before like using said inputted class as part of a loss function to limit the output to 1 class what is the name of this procedure if it has one or are there any relevant papers that do something like this x200b maybe i m overthinking it idk thanks
2,analyzing model response for peaks and values optimization hello everybody i studied biomechanics and i m kind of new to the world of machine learning nevertheless it would be great to get some feedback from some experts what is possible and what is not possible with ml so i would like to know if it s somehow possible to detect problem spots in a model response and with problem spots i mean the peaks and valleys that can be seen in the attached picture in this picture a model response is shown depending on the variation of two model input parameters here i would like to know are there any kind of tools or algorithms that can be used to find this kind of problem spots in a model response and if so what can be recommended or literature that i should have a look to x200b maybe some information about the background i m working with high non linear explicit finite element simulations approx 1 million nodes by generating a lot of data from these fe simulations i apply different surrogate modeling techniques to this data to finally predict a model response scalar responses or vector responses by using for example gaussian processes neural networks etc another option is to first apply a dimensional reduction method like pca and then use a meta modeling approach like regression neural networks etc after all this model reduction steps i would like to further investigate the model response especially the peaks and valleys that is predicted by the surrogate model and is shown as a simple example in the attached picture i hope it s clear what i m asking for feel free to ask and i m happy about every feedback thanks in advance for your help x200b kind regards chiaburr x200b x200b
1,how to determine which variable drives change when you don t have a dependent variable i have a list of all shops that closed by 5 brands 1000 shops in total i also have some numerical variables where the answer should lie as to why those brands decided to close those shops how do i find out which variable was the most important for each brand if i also had a list of shops that are still open i think that would be quite easily done by logistic regression but here my dependent variable has the same value all throughout closed the variables i have are all continuous and mostly demographic how old and how well off the people who shopped there were so i have those dependent numerical variables and then another column that is brand any ideas
0,is there a name for this simple machine learning method or a similar one hello everyone for a university project we were given the following simple algorithm and i was wondering if it has a name or is similar to an existing method i d be glad if you could leave some comments so we have data points with 2 features in 3 evenly distributed categories one can easily see 3 clusters but there is quite some overlap the suggested method wants us to determine 3 centres one for each category and categorize data points by determining the closest centre these centres should be optimized so that the number of wrong categorizations is minimized any optimization methods recommended there seem to be a lot of local minima so we used scipys dual annealing and most likely found the optimum but maybe there is a faster way so is there any name for this categorization method or a very similar one all the methodes we looked at sklearn are much more sohphisticated we wanted to also use knn as an alternative method and compare the two using cross validation if you have any other ideas i d be very happy to hear them thanks for reading and have a nice day p s cool to see this subreddit exists subscribed
1,survey analysis using chi squared and logistic regression i have survey results assessing interest in a healthcare product the overall goal of my analysis is to profile respondents to better understand which factors correlate with those who at the end of the survey indicate that they interested in the product vs those who indicate that they are not interested essentially uncover the differences in these 2 groups i ve begun by splitting respondents into these 2 distinct groups based on their indicated interest interested vs not interested from there i have several categorical and ordinal likert type variables which i will use to profile compare and contrast these 2 distinct groups examples of categorical variables available are gender race employment status examples of likert scale ordinal variables available are how often do you exercise coded from 1 never exercise to 5 daily exercise how likely are you to follow a diet if recommended by your doctor coded from 1 very unlikely to 5 extremely likely i plan to use a chi squared test to identify if there are relationships between each of the categorical variable and the respondent group interested vs not interested following up each significant test with further chi squared tests to identify where the significant differences are between categories for example if education level is significant is college vs high school degree attained significant is college degree vs masters significant my understanding is that this where tests come back as significant will allow me to say things such as respondents with a masters degree are more likely to be interested in the product than those with a college degree for the likert scale data i plan to run a series of simple logistic regression models a separate model for each likert scale variable to identify independent relationships my understanding is that where relationships turn out to be significant this will allow me to say thing s such as the more often a respondent exercises the more likely they are to be interested in our product my main question is does this approach make sense is there a better way to approach this am i misunderstanding how to interpret any of the outputs described
2,i think all vision researchers should be using event cameras in their research event camera the motivation of this post is based on driving the adoption and manufacturing of smaller high quality and cheaper event cameras which seem to offer much better data for high quality and high framerate applications this post probably seems obvious to a lot of researchers as it s covered in abstracts survey papers blogs and talks from 2014 onward explaining event cameras and their benefits the main benefits being getting intensity changes per pixel and not having to consider under overexposure nor motion blur in data augmentation pipelines all of these benefits usually results in less computation required the first paper i d point to is event based near eye gaze tracking beyond 10 000 hz one of the hardware requirements of high quality vr ar is eye tracking over 250hz for foveated rendering that paper creates what appears to be a perfect foundation for eye tracking it s not hard to imagine a miniaturized cellphone scale event camera with maybe an asic tracking the eye at extremely high quality as far as i m aware it s not possible to get that quality with an ir camera and it would use a lot more energy processing the frame data this page has recent projects including one on calibrating event cameras it also has a paper on slowing down time which also has a lot of neat applications for segmentation which others have looked into there s also a paper on there for monocular depth estimation which seems like a perfect application for event cameras another research area is super resolution algorithms granted most try to work on existing color video but the ability to capture changing intensity values in theory offers much higher quality results for future event based cameras there s a number of papers on this topic reminds me of how our own human vision works with hyperacuity i m fascinated with the concept of pointing a camera at something shaking it a bit and getting incredibly high resolution images i think one of the most important uses for event cameras is in the application of low powered slam that uzh page above has a few slam projects and 3d reconstruction papers but there are newer papers the main idea though is event cameras can offer unparalleled sample rates and stability compared to standard camera based approaches they handle fast rapid motion much better as they don t have to deal with motion blur as mentioned this has been discussed since probably before 2014 so it s well known but limited by the availability of event cameras there s also applications like optical flow also things like tracking fiducial markers turns out tracking high contrast images with an event camera works really well seems like multiple people have applied it to yolo type algorithms also which is impressive there s honestly so many vision applications though that could be researched or improved upon simply taking non event camera research and applying event cameras seems to generally give better results i was actually kind of surprised google hasn t converted mediapipe to use event cameras yet being able to do pose detection with rapid motion is huge i think photogrammetry might be the largest open area of research for such cameras there s reconstruction papers and the super resolution stuff but i don t think anyone has put it all together yet in theory one should be able to scan objects with very high resolution with such a camera a moving camera at that since it wouldn t have the motion blur issues i could see a company utilizing this approach out competing current company techniques vr ar and event cameras let me paint a picture of how i think vr ar might work later a vr headset would use 2 event cameras on the front left and right edges with overlap in their fov the headset would use these two cameras for slam tracking and high sample rate hand tracking the headset would also have two event cameras for eye tracking the controllers would have their own wide angle event cameras on the top and bottom such that each would perform their own slam tracking independent of the headset the headset could still track the controllers for extra accuracy but it wouldn t be necessary in this setup the controllers essentially never lose tracking for full body tracking there s a few approaches the controllers i described would have a huge fov and could in theory do pose tracking but it s possible to place the controllers such that they can t see the hips legs to remedy that one can imagine a small puck with a wide angle event camera on each foot with the ability to do pose tracking and slam and combined with pose tracking on the controllers they d have only a few edge cases for pose reconstruction so 10 event cameras total for the whole system ar would have a similar 4 camera headset design for tracking and eye tracking one of the issues with ar is that cameras can t track the user s hands fast enough to use them in the 240hz rendering and get perfect hand occlusion you want fingers to be in front of floating menus and realistically clip them this involves calculating pixel perfect masks with near zero latency there s basically always artifacts or a ghosting effect as the sensors aren t fast enough for ar where you re looking at your real hands tof sensors might be fast enough later conclusion i understand event cameras can be costly or time consuming to work with there are simulators for them though which can make them approachable as far as i know they take in high framerate intensity renders from like blender unreal etc and output the intensity pixel change events with the advantages of event cameras i see them taking over a lot of use cases for conventional cameras i could even see films being recorded with event cameras not sure how likely that is but it seems powerful to be able to capture a whole hdr film with no motion blur for cgi editing purposes would be able to extract all the markers in a scene with much higher quality i digress but if someone could push a hardware company to produce a miniature event camera that would be amazing i know intel funds research that uses them but i m honestly surprised they haven t made their own to replace their t 265 slam device that thing can t handle any sudden movement at all a company that can produce an affordable small sensor could market it to vr ar motion capture drones robotics and cellphones an event camera on a cellphone would probably make so many things lower powered like ocr i digress again this post culminated from watching a ton of event camera talks online and reading about the potential everyone sees in them
0,what would be the best workflow for this image dataset use case hi all the project that i’m working right now is about image recognition for the data pipeline we are using the following pipeline take photos with our cellphones divide them into batches of 100 called tasks upload them into cvat hosted in a aws machine and label them download the images labels to our local machines add some additional metadata to the labels and upload them to a aws s3 bucket this has some manual labor as there is label happening all the time and it would be desirable that the s3 bucket would have the updated information without much of an hassle what would you suggest in this case what tools that you would think to automate this process i think that the bottleneck here is cvat which only works with the creation of “tasks” and each one of them can’t have a lot of images since it will make the application slow much thanks
2,looking for data labelling tool for volumetric multifeature datasets dear all we are currently working on 3d medical imagery with 6 layers of different information a sample has a size of around 120 gb we are looking for a way to label these conveniently according to the following criteria extraction of individual z sections of the xyz volume we have this covered labelling of one of these z sections with the ability to turn on and off the individual 6 layers while there is always a merged picture of all the activated layers visible adjusting gamme histogram of these layers individually later on we would like to do this with volumetric data the same way labelling structures with a volume based on several z sections of the volume we couldn t find a standard solution and are evaluating we should adjust one of the open source solutions out there does anyone know if any other data labelling tool has some of the above mentioned features
1,how can i find a research topic hello all i’m a rising junior at my university who is majoring in statistics at the moment my goal is to do a phd in the field of statistical machine learning specifically with unsupervised learning i don’t know what but one day i want my thesis to be centered around clustering or dimensionality reduction problem is i don’t know what math i need for that or what is even already out there for me to come up up with an idea as a contribution to the field is the field of ml too advanced that by the time i get there i will run out of ideas does anyone think it makes sense or it’s possible for my to eventually do a thesis in that later on i’ve been trying to learn the basics of unsupervised methods right now i know i’m getting ahead of myself here but i have so much time right now on my hands that i’d rather dig into it a little bit if it’s a topic of interest to me
2,research evaluating a convolutional neural network on an imbalanced academic dataset i have trained a posture analysis network to classify in a video of humans recorded in public places if there is a shake hand between two humans b standing close together that their hands touch each other but not shake hand and c no interaction at all there are multiple labels to identify different parts of a human the labels are done to train the network to spot hand shaking in a large dataset of videos of humans recorded in public as you can guess this leads to an imbalanced dataset to train i sampled data such that 60 of my input contained handshaking images and the rest contained different images than hand shaking in this network we are not looking at just labels but also the relative position of individual labels wrt to one another we have an algorithm that can then classify them into the three classes x200b i am stuck on how to evaluate the performance of this network i have a large dataset and it is not labeled so i have decided to pick 25 from class a and b and 50 from class c to create a small test dataset with labels to show the performance of the network and to run the network on the large dataset without labels but because classes a and b are quite rare events i would be able to individually access the accuracy of the network prediction of true positive and false positive cases x200b is this a sound way to evaluate can anyone having experience or opinion share their input on this how else can i evaluate this
2,ijcai 2020 flow based intrinsic curiosity module ficm is a flow based intrinsic curiosity module to encourage a drl agent to explore the environment the rapidly changing parta or moving pattern between agent’s consecutive observations can be interpreted as an important indicator of information when exploring an environment ficm evaluates novelty and generates an intrinsic reward based on the motion features extracted from two consecutive observations advanced detail please visit paper download github link presentation link elsa lab is a research laboratory focusing on deep reinforcement learning intelligent robotics and computer vision please visit our website artificialintelligence reinforcementlearning machinelearning computervision
2,how to geo cluster houses in a real estate dataset i have a fairly large portfolio of houses think thousands that i want to cluster based on proximity to neighboring houses and some house types fuel source detached apartment the goal is to create clusters based on the distance to other houses and the types e g cluster of 5 houses max 50 meters from each other which are all on the same fuel source and are detached luckily in my dataset it is most likely that houses next to each other will also be of the same type do you have any tips on algorithms approaches for this job i am proficient in python r thank you
2,how to stay relevant for work after 1 year of break in ml hello guys unfortunately being a male my country requires me to serve the military for the next 1 year that means a year with minimal or zero exposure to any code or machine learning i had a job as a data scientist after college for 1 year that i had to stop recently because of that i know my career will have to take a big hit but there is no way around it unfortunately i m a bit worried now how easy it will be 1 year from now to get a decent job having a 1 year gap on my resume plus the big influx of data scientists coming each year my goal is to increase my odds of getting hired as soon as possible after i finish my serving some good news are that for the next 2 months i m relatively free to study technologies that will boost my cv before i join the military and help me land a job faster once i finish furthermore i might have some afternoons free inside the military during the year to learn stuff or keep my memory fresh for the interviews nothing that would require a laptop though only books or a smartphone with internet i know predicting what will be hot in a year might not be that easy especially since big advancements in ml and dl happen within the span of a few months but i wanna min max all my lost time so given my situation above what would you recommend me to focus on for a the next 2 months where i will have access to laptop and free time b the next 10 months where i will have access only to books smarthphone and little to no time aws gcp docker thanks in advance
0,this one thing could be holding back your career in data science ever thought to yourself that even though you are perfectly capable at your job or maybe even the most knowledgable person you are often underestimated in your role or worse you find someone else even though less capable technically making the decisions and telling you how the project should be handled you want ask and expect a new opportunity or an exciting project from your leadership but you simply watch them go to someone else in quiet despair you feel under appreciated ignored and mostly misunderstood you probably have several great ideas for your company but if only someone were to listen to you i work with several people in tech it and data science and this is such a common problem that i encounter the reason for this is ‘perception” see there’s a big difference between being an expert vs being perceived as one being an expert will help you excel in the opportunities you land in but how people perceive you is what will land you in opportunities in the first place almost everyone is focused on honing their technical skills which by the way is a great pursuit but most ignore the art of basic persuasion and charm which keeps them from getting truly satisfying roles and opportunities so how does one gets perceived the right way… it has to do with the way you communicate experts have a way of talking that automatically demands compliance respect and conviction from others think of your last visit to a doctor did you argue with the doctor or just wondered in your head that they probably know nothing or simply dismissed what they asked you to do instead you complied with whatever you said effective persuasion is a learnable skill once you make a few key changes to the way you communicate the environment around you and the way people treat you changes dramatically you will command respect and unquestionable trust that will get you elite opportunities in your industry so you can later prove it through your capabilities when you present something people will just tune in to your reality as you speak in fact they literally will come to help you with your work even when you don’t ask them all of this happens subconsciously when you know basic charisma and persuasion
1,role of stochastic process e g martingale residual in survival analysis i am trying to better understand why certain concepts from stochastic process are used in survival analysis for example there is a popular model in survival analysis called the cox proportional hazards regression model this model is used to study the survival rates and the hazard rates of observations groups within the data this model uses the predictor variables associated with each individual to model the hazard it seems like a popular method to check whether a trained cox proportional hazards regression model is a good fit for a given dataset is to evaluate the martingale residuals for this model i e checking the proportional hazards assumption if i understood correctly the martingale residual predicted value subtracted from a martingale term follows a certain probability distribution for each predictor variable you can simulate thousands of paths corresponding to the theoretical distribution of the martingale residual then you can see how the actual martingale residual calculated using the data compares to all the theoretical simulations if the actual martingale residual fits somewhere between all the simulations we can say that this is a reasonable behavior and the model assumptions are valid can someone please help me 1 did i correctly understand the role of the martingale residual in survival analysis 2 can someone please help me understand the motivation for bringing martingales and stochastic process into survival analysis why is this necessary why is the martingale residual useful for validating the proportional hazard assumption i e the contribution of each variable to the overall hazard does not depend on time why is the proportional hazards assumption important to begin with i tried researching this online but all the material i found was either to vague beginner or too complicated can someone please help me understand this thanks
2,project building detection model i am given a task to develop a building detection model based on satellite images i ve been given coordinates of buildings there is not many api services available for getting images based on coordinates i am using this one which does not have high resolution so you can t send request for each building so i have to build a bounding box around multiple nearby buildings and send a request based on it i ve used k means to cluster nearby buildings and got 165 clusters which i put a bounding box around it then wrote a parser and got images for those bounding boxes now i have to build the model however i have my suspicions that the model won t perform great because of low quality reference data each image contains multiple buildings roads and etc so i am not sure what can i do should i proceed cleaning this dataset masking interpolation and etc or it doesn t worth the effort most images look like this i have nearly 100 of them but i can increase size of dataset by requesting more building coordinates any advice would help
2,ucsd researchers develop an artificial neuron device that could reduce energy use and size of neural network hardware researchers at the university of california san diego developed a novel artificial neuron device with the help of which training neural networks to perform tasks like image recognition or self driving car navigation could require less computer power and hardware the gadget uses 100 to 1000 times less energy and space than current cmos based technology to perform neural network computations the work has been published in a paper in nature nanotechnology the basic idea behind neural networks is that each layer’s output is fed as the input to the next layer and to generate those inputs a non linear activation function is required however because this function entails transmitting data back and forth between two different units – the memory and an external processor – it necessitates a significant amount of computational power and circuitry paper summary paper
1,does having a higher drop out rate than what was accounted for cause a threat to your sample size power calculation for example in the study i am reviewing they calculated a 90 power w alpha 5 they had a strict drop out non compliance rate of 15 after analysis they had an 18 21 drop out rate does this cause a threat to the external validity and make their 90 power calculation negligible thank you
1,pca principle components what are they and when are they useful hello please let me know if the below makes no sense i m trying to get my head around pca principle components pcs i d like to know how pcs work and how to predict whether pca will be appropriate for a dataset i ve had a look at a source that has a nice description of pcs quoted at the bottom and a graphic linked in main text from the source s graphic i can understand how to get the pca for a graph of two variables it is just the purple line a link to the graphic is here i m now trying to visualise how this graphic would translate to having more variables e g 3 if i had a three axis graph 1 axis per variable axes are x y z would the first pc axis represent maximal variance for all three variables so like the plot in the graphic but with a z axis and the red lines in the graphic being applied in 3d space and so the first pc axis represents the axis where maximal variance is achieved across every x y z axis for the data set and then the next pc axis is the orthogonal axis to the first pc axis whilst still being in the x y z axis 3d plot with nx pc axis being found where n is the number of variables if that s correct then i would expect a dataset whose points are not correlating in a specific direction i e a cloud of points to have large degrees of variance along multiple axis is this the kind of data that would result in a bad scree plot where the first couple of pcs don t capture all of the variance could you also get this if you have a cloud of weakly correlated points with respect to one of the three variables but the remining two show correlation x200b geometrically speaking principal components represent the directions of the data that explain a maximal amount of variance that is to say the lines that capture most information of the data the relationship between variance and information here is that the larger the variance carried by a line the larger the dispersion of the data points along it and the larger the dispersion along a line the more the information it has to put all this simply just think of principal components as new axes that provide the best angle to see and evaluate the data so that the differences between the observations are better visible source a step by step explanation of principal component analysis pca built in x200b edit this has nice visualisations principal components analysis okstate edu
1,is the 16pf test correlated to any work place attributes i don t know if this is a good place to post this or not if not sorry i wanted to know if the 16pf test was correlated with anything specifically work productivity or satisfaction are any personality test results well correlated with anything at all
2,cvpr 21 best paper giraffe representing scenes as compositional generative neural feature fields michael niemeyer s work giraffe representing scenes as compositional generative neural feature fields has just been given the best paper award at cvpr 2021 10k submissions but you made it congrats michael and andreas it s an honor to work with you abstract deep generative models allow for photorealistic image synthesis at high resolutions but for many applications this is not enough content creation also needs to be controllable while several recent works investigate how to disentangle underlying factors of variation in the data most of them operate in 2d and hence ignore that our world is three dimensional further only few works consider the compositional nature of scenes our key hypothesis is that incorporating a compositional 3d scene representation into the generative model leads to more controllable image synthesis representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects shapes and appearances while learning from unstructured and unposed image collections without any additional supervision combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model as evidenced by our experiments our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose x200b project page paper twitter
0,interpretation result from market basket analysis mba this is the first time i did analysis in mba i got the result something like support confidence lift rule 1 85 1 2 a b does this mean if a is being bought then 85 confidence c will be being purchased too if the lift 1 2 in the output then 20 more likely b is to be purchased if a is purchased however based on mathematical formula i feel high confidence and support does not imply cause and effect it s possible that without a are more confident likely to have b this also means lift should be the main metrics lift 1 to indicate the intention correlation for support and confidence we just simply need those numbers surpass some small enough threshold is that corrected what can we say to business guys with this rule
1,are the beta and bernoulli processes normalized random measures with independent increments hi r statistics i have a quick question about stochastic processes bayesian nonparametrics random measures james et al 2009 introduced the notion of normalized random measures with independent increments does anyone know whether the beta process and or the bernoulli process belong to this family
1,how do i calculate p significance of a simple regression plot i need to figure out if my regression plots are significant or not the issue is that since i am plotting regressions over such a large amount of plots i plotted the regression manually in python to iterate over each plot i successfully plotted a regression for each set of data correctly however i am not generating a stats summary or returning a p value is there a way to manually calculate the significance of a regression line any help is greatly appreciated i used polyfit from numpy to calculate and plot my simple linear regressions
1,is facebook prophet good for forecasting time series data to predict stock price i am a novice when it comes to data science but do have a data engineering background so i do understand data science concepts at a very high level when it comes to forecasting time series data like predicting stock prices it seems like there are two broad main methods 1 deep learning rnn cnn lstm 2 ‘traditional’ time series methods univariate fb prophet arima sarima exponential smoothing multivariate vector auto regression i know it’s ignorant to ask what the ‘best’ method is in forecasting time series stock prices that’s a very subjective question i’m more interested in what the general sentiment is towards each method to ultimate forecast and predict stock price data also the trades offs to each i am less inclined to the traditional approaches and more interested in deep learning but i don’t know if that’s even appropriate for what i’m trying to accomplish thanks for any input in advance
0,what kind of freelancing jobs can data scientist do hi all i m building my project portfolios aiming for banks advertisement firms i have a serious question what s our competitive advantage as data scientists comparing to those software engineers web developer this question has been wandering back of my head for se they can find jobs quite quickly even freelancing jobs more demands for it in the job markets for ds you have to be really competitive across mathematics statistics software engineering the domain knowledge fairly saying researchers could be data scientists my daily frustration is there are more demands for senior ds rather than juniors is there any freelancing jobs for ds cheers
1,is this the equivalent of the what came first the chicken or the egg in statistics i am learning about this concept in statistical learning called probably approximately correct pac although the wording of this can seem complicated and technical if i understand correctly the essence of pac is to show that if a target concept e g the set of all possible input points corresponding to a certain output then the error of a machine learning algorithm can be probabilistically bounded with a certain range i think all this is intended to show that a machine learning algorithm is useful for making predictions instead of basing your predictors off which color socks your neighbor is wearing 1 pac framework was developed in 1984 yet prior to this many statistical models were being used for making predictions e g regression models once pac was developed did researchers have to examine all statistical models they were using prior to this and confirm if these models were compatible with pac framework 2 now the same question for modern models when newer machine learning models are developed e g lstm models developed long after pac framework was established are these new algorithms tested to make sure they are compatible with pac framework 3 can someone please confirm if my understanding of pac framework is correct source a hypothesis in the context of pac seems to be a general term for a machine learning algorithm the hypothesis space is the space of all possible such algorithms e g a linear regression model with specific beta parameters is a individual hypothesis and all possible linear regression models are the hypothesis space d is the distribution of the data and a concept class but im still confused about the differences between target concept and concept class can someone please clarify this
0,is the occupation of a data analyst and database analyst same or are they different under what occupational group does a data analyst fall into non native english speaker here what is the difference between a data analyst and a database analyst is the occupation of a data analyst part of occupation unit group 2172 database analysts and data administrators if not which occupational unit does it fall into in the canadian national occupational classification system
0,coffee chats what s your preferred way of requesting networking connects linkedin email lunchclub tried lunchclub horrible experience didn t have a single conversation of 10 scheduled chats considering reaching out to folks on linkedin but would love to know if there s a smarter way to do this like some active discord channels p s with all the reform talks going on would be worth opening a discord server or slack channel for the sub as well
1,in a regression discontinuity design why can t you just make two least squared regression lines centered around the cutoff point and subtract their y intercepts i am trying to understand regression discontinuities for a project i am working on and when reading papers of others doing the same test they always define some binary variable for when the x values are above below the point why can t you just take two data sets centered around the cutoff point find a regression line for each set and subtract their cutoff points whenever i do that simple subtraction method like in excel i get a different number from when i do the analysis in a programming language i hope this question makes sense and thank you to anyone that responds
1,which of these statistical tests is the most powerful matched pairs t test independent samples t test anova regression mann whitney u test kolmogorov smirnoff test it is going to be the one that has each individual serve as their own control so would it be the matched pairs t test
0,working at consultancies does anyone have experience working at a consultancy mainly doing government work and or making small products in a ‘startup’ sort of environment what was the work life a balance like did you have suitable people to mentor you was the tech stack modern share your experience
0,what is the difference between a data engineer s job and data scientist s job i have googled this but i d like to know from experience what the primary differences are do the interview questions for these positions also vary how detailed is one s knowledge of ml and dl expected to be for data engineering positions are these names often used interchangeably
2,details behind lamda the conversation technology mentioned at google i o 2021 it is based heavily on the meena bot which came out back in january 2020 one must agree the demo was very very impressive google ai blog google ai blog towards a conversational agent that can chat about…anything googleblog com paper
2,question about dropout variants hello so dropout is very binary in nature neurons are either dropped or kept but is there a variant that scales down some incoming neurons by a factor and allows the other nneurons to pass through unaffected i ve tried to find something like that couldn t find anything any help papers appreciated thanks
0,modern time series models does anyone know what are the most modern statistical models being used for time series analysis i have heard of transformer and attention mechanisms models that are used for modelling sequential data but these seem to be more relevant for modelling data from the nlp domain when it comes to classical time series modelling e g a vector of temperature measurements does anyone know what are some of the more modern models being used for this i did some searching online it seems like arima style models were some of the first ones followed by state space models hidden markov and the more recent ones being rnn and lstm are lstm and rnn the most modern models that are being used for classical time series problems thanks
1,is there anything weird with this coding scheme it feels strange to me but i m not sure if it s just an approach that i m less familiar with lets say it s a study where people look at pictures and rate their pleasantness the pictures are coded for the objects they contain imagine these are the predictors picture contains only a tree 0 1 picture contains only a mountain 0 1 picture contains a tree and a mountain 0 1 my instinct would have been to just have two variables here one coding for the presence of a tree one the presence of a mountain but this study has done the above instead is this a common approach am i right that there is something strange with it edit let s also say there are also the following predictors picture contains only a river 0 1 picture contains a tree and a river 0 1 picture contains a river and a mountain 0 1
0,populating e commerce tables with products and variants hey folks i m in the process of building a fashion aggregation website as a portfolio piece to obtain the data for this site i have multiple scrapers for many of the largest fashion retailers in north america to describe the data one product can have many variants e g summer dress that comes in 3 colours and 8 sizes as such this product would have 24 variant products the generic being the parent product each variant may have different pricing availability sizing colour etc my question concerns table construction my scrapers output every variant of every product with their associated data which can be in csv or pandas dataframe format can any of you tell me how i would go about populating two database tables from this data 1 table with the parent product with foreign keys to each of its variants 2 all variant products of a parent product linked to that product such that on my eventual website they can all be found through searching the foreign keys of the parent product i hope that makes sense in a bit of a jam here i m not sure what to google even if you can t provide an answer any points on even what to research would be of great help tldr how do i populate 2 database tables with one csv file one table for the generic product and one for all of its variant s fields
2,implementation of ai into healthcare systems pros and cons i am working on a school project centred around the effect of ai on the healthcare system specifically the nhs but can be viewed in a more general context i wanted to explore different opinions on this do you think ai could potentially lead to a poorer quality of healthcare there is no doubt that the use of ai in these systems will only increase but are we ready for this i have been researching a bit about issues with ai eg in the context of a diagnostics tool and have found a bit about biased algorithms which arise from a lack of input data please correct me if this is wrong what sorts of repercussions would an issue like this have on peoples health what do you think the greatest danger that ai poses on patients and clinicians is and are there certain groups of people who are more at risk to these problems any opinions and discussions would be greatly appreciated to help me gain more insight into this topic thank you
1,how to interpret a lcl that is under 0 when it is realistically not possible so i am doing a rate it is realistically impossible for a rate to go below 0 but when doing a ucl lcl the lcl is below a 0 how do i interpret that if the lcl ends up below a 0 would the lcl be set at 0
0,relaxed work environments or alternatives for ds could be quarantine or zoom fatigue setting in but lately it s felt like i just don t want to put in so much effort any more granted i m in a senior position and maybe i just don t want the responsibility at a startup and the majority of my career has been in startups i find myself waking up everyday just waiting for the weekend i m thinking that in the next year or two i would really like to take a slower pace where i wouldn t have to juggle so much or have so much riding on my projects i ve done a whole lot of studying and improving on the side for years in stats ml but also development and i kind of just want to relax a bit originally i was going to shoot for management roles down the line but i don t think it s for me anyway i know the first step would probably be to get out of startups generally i ve read government and banking are a couple of more relaxed choices how are the big tech companies in this regard i know some of the open ds positions are more analytics which is slightly concerning given that i like the development side a more but i might be okay with it just want to collect a paycheck and feel more at ease alternatively there could be paths that aren t entirely data science but data related i did do some sem work years ago that i find interesting so that could be something anyways generally feeling unmotivated to continue advancing in the field and want to pull back a bit and take a breather anyone have experience with transitioning to a more relaxed industry role
1,best way to normalize data hello i m a new data analyst and wanted some insight on a particular problem i have a dataset that has the sum of hits related to businesses in each county in the us on an hourly daily weekly etc basis some counties are more populated than others for instance somewhere like la county will naturally have a lot more hits than in some random county in kansas i need to normalize this data what s the best way to normalize this sort of data how can i tell which way is best i ve researched various transformations but figured someone here has encountered this sort of problem before and may be able to steer me a particular way i ve read about min max z score creating some sort of index by dividing by the population and multiplying etc
1,has anyone ever used the drwhy package in r for explainable ai has anyone ever used this package in r before for trying to explain blackbox machine learning models this package drwhy seems like a comprehensive collection of different algorithms e g lime shap meant for explaining blackbox models e g neural networks has anyone ever used this package before how have your experiences been did it prove to be useful were the results reliable
0,on the field jobs with datascience wildlife nature conservation also referring to anyone working in nature conservation wildlife deforestation and also working on the field would like to elaborate how much is beneficial to have a background in data science do you use ds frequently or manage team making use of ds how much is valuable for engaging in outdoor activities and negotiate opportunities to go on the field for part of the job what kind of activities do you do possibly i d like to hear about opportunities of fieldwork in the eu and possibly if only volunteering pops up at least volunteering activities offering full cover of costs and relocation my situation is that i d like to gain a relevant background for phd making use of tech e g ml and in perspective aim for jobs where i can live outdoor part of the time in order to apply for a phd i need to get a msc first at least in the eu and due to my background it may be easier to get a msc in a tech domain rather than a knowledge related domain e g biology natural science ethology that would require prior related academic background in these disciplines i d like to hear from other experiences to think about appropriate expectations and tips to consider in perspective i don t want to stay 130 of my time in front of a computer while i find very motivating the possibility to match direct observations and outdoor activities with the part for processing in perspectives i think tech will open up for new opportunities also in this field and again i wonder how a tech background would be considered and leveraged vs knowledge domain backgrounds or direct on field experience
2,prediction logger for model serving we’ve recently launched a prediction logging and monitoring library for model serving applications i e long running servers but also periodic batch jobs it will automatically compute statistics for prediction time windows or batches and send them along with input and output data samples and outliers to our cloud works with free account one of the goals is to make root cause analysis possible in case of issues with data or model by associating issue related prediction input and output data instances with data metrics and errors i’m pretty sure many of you are aware of data related issues in production which are really hard to troubleshoot without prediction samples and context graphsignal visualizes and automatically monitors multiple data metrics for each feature class for data schema validity and consistency anomalies sudden input and output data drift and more here is a 2 minute screencast i hope some of you who deploy maintain models in production find it useful i’ll be happy for any feedback
0,how do you decide how much of historical data is needed to build a predictive model it s going to be a vague question and but i just need some insights especially who works with real estate data we recently started moving all our data to a data lake gcp to store all the historical data now we also have access to years of listings data and some sort of other information from different vendors and we are struggling to decide how many years of listings historical data we want to pull from their servers our plan is to build some predictive models in future but not sure at the moment we just want to make sure we have the data available when the management want us to build a product i know the more data it is it s good but we need a cutoff i looked into different places for example in realtors com you can see the listing price for last 5 years while zillow shows you for last 10 years i am not sure for building predictive models how much data we needed how do you guys decide that so far we have decided to go with 5 years current year but if anyone here has any ideas it would be helpful thanks
0,career path options as a current data analyst hi all apologies in advance if i m breaking any rules or if this is more suited to the weekly thread i ve been working as a data analyst for a healthcare company for the past year a lot of my work surrounds creating queries to track certain metrics then building dashboards to create insights using visualization tools i do like the field but unsure of where to move next i do enjoy the coding aspect of my position lots of sql don t like excel as much as well as hacking away at a problem and figuring out how to fix certain issues with the code we have however i hate coming up with insights and solutions i don t mind creating the dashboards but i don t like the proactive and analytical work that comes with it oh that s a good find maybe we should look into this next i enjoy more when there s a problem to fix then i fix that problem does data engineering fit more into the interests i ve mentioned above i imagine a role as a data scientist would be more similar to what i m currently doing thanks in advance
2,how are computational neuroscience and machine learning overalapping hi i am an undergrad with a background in neuroscience and math i have been very much interested in the problem of agi how the human mind even exists and how the brain fundamentally works i think computational neuroscience is making a lot of headwinds on these questions except agi recently i have been perusing some ml labs that have been working on the problems within cognitive neuroscience as well i was wondering how these fields interact if i do a phd in comp neuro is there a possibility for me to work in the ml and ai field if teach myself a lot of these concepts and do research that uses these concepts
1,prediction interval for arima models in multi step forecast i have an arima model i want to make a forecast over the next 6 time steps not only do i want the predictions but i also want a prediction interval with say 95 probability i m able to generate a confidence interval but this isn t what i want as this is just the confidence of the fit parameters not the residual error please feel free to correct me if i m mistaken if i only care about a one step prediction i believe i can just take the standard deviation of the residual errors and make a prediction interval out of that but in my case i want more than just the one step how can i make this 95 prediction interval for multiple steps into the future i imagine it s some combination of my confidence interval and the standard deviation of my residuals but i can t figure out
0,any advice as to how to best transition into a more advanced data science role so i m a data scientist with 2 years of experience but i work only with traditional ml i e multiple regression logistics regressions regularized regression and clustering algorithms kmeans and hierarchical for the most part i also don t have the opportunity to work with big data since that is not going to change any time soon i ve decided to try to look for a better less limited ds opportunity however i m struggling to find a role that doesn t require 5 10 years of experience using dl ai and big data spec i m just wondering if anyone here has gone through a similar process and if so what advice would you give to someone in a similar position to add a bit to my background i am currently completing my masters in computer science with a specialization in machine learning have a lot of personal experience working with random forest with boosting svms and some personal experience in dl i am also currently completing the mlops coursera course which hopefully would give me some more experience in working in deploying ml models
0,what is your definition of data science i’m sorry if this post is breaking any rules on this subreddit i just wanted to know how each one of you data scientists would define this field
1,question interval estimate for proportional and optimal allocation hi my assignment is to interval estimate the share of sd voters in the parliamentary elections for the region which consist of halland county skåne county etc i get to decide the degree of confidence and precision the data i have is the number of people entitled to vote in each county and its municipalities and the share of voters voting sd party in an preliminary examination for each county how should i by using the data above calculate how big of a sample size i need to interval estimate using proportional and optimal allocation
0,what opportunities exist for data scientists in engineering and product development i’ve come to realize that i’m less interested in providing insights to decision makers than i am in contributing to the development of the actual product i realize that there are plenty of applications for machine learning data science in software products but what about in hardware products i really would like to work alongside mechanical electrical engineers and scientists to develop new technology currently working on a statistics ms with a concentration in data science machine learning any thoughts
2,synthetic data engineering platform open for beta rendered ai is looking for synthetic data engineers and computer vision engineers to beta test our synthetic data engineering infrastructure the rendered ai engine has demonstrated improved outcomes for computer vision algorithm performance through the use of synthetic data as measured by ap scores through rapid creation and modification of synthetic worlds our platform is for synthetic data engineers and computer vision engineers to create modify and experiment with synthetic data and is available as a selected beta drop us a note at www rendered ai
1,any intuitive resources for understanding fixed random and mixed effects models and how to decide which is appropriate in a given context interactive websites videos or text less preferred would be really helpful thank you
1,question about statistical significance with an ongoing customer satisfaction survey with monthly reporting i originally asked this in r surveymonkeypros and was advised to try my luck here hi i send out the customer satisfaction surveys for our clients via survey monkey i don t have much information available to me on statistics i fell into this role because i handle email for our company i ve read the general info for statistical significance on an individual survey but i m unsure how to apply it to my population as i send out invites almost daily automated and report on results monthly my overall population size is 10k people these are active clients who can stay with us for up to five years as they work through our program i don t invite all 10k to the survey each month i rotate the invites so our clients receive them two to three times a year at the moment i don t tie the invitation to a specific interaction or occurance that takes place with the client there are different surveys for that purpose my response rate is really low like 40 respondents per month out of 180 invites sent i can tweak my automation and increase the number of invites i send but i m curious if my population size is 10k and my goal is a 5 margin of error and i report on my survey results monthly do i need 385 respondents per month
0,need some help on best practices to build up a small scale solution hey i haven t been getting long solid responses from this sub in overall but i m gonna try again hope someone can shed some pointers anyway so we don t have a system in place for data and i m tasked with setting it all up there s a few mariadb servers that have to be piped to a centralised data store for bi there s also data from facebook marketing api that i d ideally like a pre built connector for several criteria are no vendor lock in if possible low volumes of data but needed in real time and incrementally sync ed not using cloud db s since our volumes don t justify the need for it plus prefer to keep things local what s the best way to go about doing this some of the options i ve considered use a pipeline like stitch but that s rather expensive for this use case and so i ve considered airbyte but this open source software is still very immature despite some help from the nice people over there tried using clickhouse replication but it doesn t work for mariadb considering something hybrid with airflow or the newer prefect io to schedule and pipe data this probably also means using some manually coded connectors data warehouse db is not selected yet either i m thinking for this use case something simple like clickhouse or postgres though this one isn t exactly an olap preferably i d use pre built solutions since code is not my forte but i am open to considering simple easily debuggable code solutions i have many more little big questions like these so if anyone is willing to share more directly with me i m very happy to connect
1,how do bayesian statistics work in the case of consumer reviews suppose there are 2 restaurants restaurant a with a 4 5 star rating and 5 total reviews restaurant b with a 4 0 star rating and 100 total reviews how could a bayesian formula help show that restaurant b s rating of 4 0 is actually more legit than restaurant a s rating of 4 5 because of the sheer quantity of total reviews apologies if this is vague
1,bayesian methods for sheep ancestry parentage my friend is a sheep farmer and the maintainer of a database of information for a particular breed that goes back in the us perhaps 40 years she has a particular sheep from about 15 years ago that is crucial to the lineage but whose parentage is in question genetics is not my area but i’ve been listening to the learning bayesian statistics podcast and doing some reading i know that bayesian methods can be used to make predictions but can they also be used to make “postdictions ” that is given the thousands of sheep in the database many descended from the sheep in question and others descended from other sheep possibly descendants of mystery sheep’s parents of that era and the interbreeding that has occurred since would it be possible to determine probabilities of the mystery sheep s parentage
0,what industry do you work in from my last post i learned that data analysts do work in education thinking about it now—duh of course they do because how can we improve education without analyzing data i do have some experience in education as a substitute teacher and academic tutor but now work in clinical research i love reading people’s stories about how they entered the field so can y’all tell me what industry you work in why do you like working in that industry how do you feel you’ve made an impact also how’d you get into the industry you work in
1,how do i determine a cutoff value within the answers of likert type items hello i am working on a problem analysis for a medical instrument and use the median and interquartile range of likert type items to determine the central tendency and variation i want to create a system to determine whether the answers indicate that a specific part or feature of the instrument is deemed problematic or not since recommendations to fix the problem will be made it is necesarry that there is a cutoff between problematic or not problematic my current system is simple a median 4 indicates that a problem is present and thus recommendations will be made i however doubt this method it seems quite unprofessional and is not based on literature keeping things simple might however be the way to go does anyone have any ideas or experience regarding this subject
0,linkedin blind this sub is not real life not sure if this is relevant but seeing so many posts about people feeling like they aren t good enough smart enough successful enough enough because they see others on linkedin blind twitter or even reddit posting about their sky high compensation and amazing accomplishments keep in mind that the folks who post on these forums are not a representative sample it naturally skews towards people who are drawn to high compensation level prestige even sources like levels fyi only show the compensations of people who choose to share it which again isn t a representative sample if compensation level prestige is what you re after by all means go for it and work for it but comparing yourself to people who humble brag on social media does nothing good for your mental health studies have shown that instagram is bad for teens mental health comparing yourself to the humble braggers on linkedin blind other cs ds focused social media would likely have a similar impact on your mental health too also keep in mind that on average 65 000 cs graduates graduate every year in the us if you include china india and russia the number is more like 460 000 graduates per year of which 45 000 are considered elite my source is this research article assuming a 15 annual growth rate that means 3 5 million cs graduates just in the last 10 years 5 5 million in the last 20 years of these only about 10 just napkin math based on number of employees in amazon apple alphabet facebook microsoft and assuming only 50 of them are tech roles can ever work in big n and of the 10 there only about another 10 make it to staff levels which is where you see compensations of 500k some seniors can make it too but it s more reliably available at staff levels and these salaries too are only common in sf bay area seattle nyc and maybe austin so you re comparing against 1 of an industry that is already on average better paid than most other industries so take a deep breath stop comparing yourself against humble braggers and know that for the most part you will be ok
1,what are relations and differences between anova and n sample problems and independence problems is one way anova with a n level factor basically an n sample problems is two way or m way anova basically similar to independence testing problems thanks
0,review statistical rethinking by richard mcelreath i noticed a lot of folks recommending this book so i followed the crowd and got a copy it s oriented toward researchers in natural and social sciences so i wrote up my thoughts about the book from the perspective of an industry data scientist
2,mlp mixer pytorch pytorch reimplementation of google s mlp mixer model that close to sota using only mlp in image classification task google s mlp mixer didn t use cnn and transformer but only using mlp showing close performance to sota in the image classification task the mlp mixer will soon be merged into the vision transformer repo and has been reimplemented to simplify the use of the official model with the pytorch version to verify reproducibility i will be comparing the results on the cifar 10 dataset x200b mlp mixer github paper
0,super basic question filter grouping tool for mac hi there i have a super basic amateur question and apologies for it being elementary but i figured that this group would be the right set of professionals to ask ps did i use the right flair i have some basic grouping sums and filtering needs for a large csv data set 500k plus records excel on the mac is abysmal for such tasks even though a basic pivot table would accomplish what i need is there any tool out there could be web based or local that fits the bill and sits between excel and a full fledged database i’d rather not import to sql and query it back out any thoughts suggestions would be appreciated
1,probability of a binary outcome averaging to a different value after a certain number of trials hello i took statistics in college but only the baby course they give to engineers i had a discussion with a co worker about how unlikely some event might be the idea is imagine we are flipping a coin which has a 40 chance of landing heads and 60 of landing tails i m interested in how i d go about finding the probability that after x maybe 100 flips that the total number of heads is more than the number of tails this is a super easy problem but i can t seem to remember how i d go about this thank you
2,open source best practices for ethical and responsible machine learning the foundation for best practices in machine learning released the first versions of their organisation and technical best practices the best practices are free and open source the best practices are designed to be easily accessible to anyone working on or interested in machine learning they can be used as a way to get started with implementing ethical and responsible machine learning in products a common language between data scientists engineers managers and governance compliance professionals a repository for your responsible machine learning questions a point of reference for your machine learning audits policies governance and regulations they are also open source and they are looking for contributors through their wiki portal find the best practices on their website or find them on linkedin
1,how can i interpret the substantive significance of a likelihood if i run a linear regression and get an r squared it has a substantive interpretation this is how much of the variance in the sample the model explained if i add a term and see an increase in r squared that also has a substantive interpretation an increase from r squared from 0 1 to 0 5 is substantively important an increase from 0 1 to 0 105 probably isn t even if it is highly significant many models don t have a natural r squared so instead we look at the log likelihood is there a natural way to understand the size of a log likelihood when we compare two log likelihoods we might do a likelihood ratio test that tests whether one model is significantly better than the other but is there a way to understand the substantive importance of the improvement as there is for the r squared i thought that log likelihood divided by n might work since it gives the per person probability of the data given the model but i m not sure
0,binary classification with pick and choose threshold let s say i have a binary classification model where one of the discrete variable has 3 values for example male female unidentified does it make sense to use different probability thresholds for each value it seems intuitive to me but i m trying to think of potential downfalls to give more context our model generates a lot of false positives but performs well on class 0 tn fn does it make sense to have one threshold optimizing for class 0 prediction then accept class 1 prediction only if the probability score is high since high here means class 1 precision if i want to fix this at say 75 each gender would have a different probability threshold the rest of the predictions will have prob score above class 0 threshold but lower than precision 75 threshold these will trigger manual intervention and we won t rely on model prediction
0,common mistakes in the industry to everyone in the industry what are some of the most common or surprising data science related mistakes you ve encounters at the workplace for example mistaking correlation for causation only looking at mean numbers without checking for outliers
0,should i take this data scientist job offer i am offered a data scientist job at a budding health startup as per the ceo job is not just about prediction but helping them to make a better product at this new start up i would be involved in deciding which product would fit vertically horizontally with the current product finding and discussing with right stakeholders out side the company finding buying data making models i sense the ceo is keen to license other models to integrate within the product i am fine with product development part but i would like to build model myself i am afraid if the ceo just lincense other model then i would be end up setting infrastructure to use the models within current products another key point is i am currently working in non mathematics physics academic lab where i do not do any modeling and would like to transition to data science learned data science through bootcamp personal projects my concern is would i get stuck within product infrastructure without making my hands dirty edited for clarity
1,what does model mean in training a model from a set of training data in statistics a model is a set of probability distributions there are parametric and nonparametric models in training a model from a set of training data does model still have that meaning does model here mean a probability distribution in a model thanks
2,should i create a label of group of many small adjacent objects in object detection i m working on object detection models and my dataset sometimes has a lot of small objects stay far from the seen overlapping and nearby which is really annoying in annotating it s too small and hard to draw bounding boxes for each of them separately so should i create one more or more class and name it group of small a beside a because i think if i make a few bb and forget those little it s gonna make evaluation down but if i overdraw bb which also not good low resolution and it becomes worse after resize before input to the model context i m using one stage dnn detection model
0,scraping twitter with twint questions from a total newbie hey guys if anyone of you has experience with twint i would greatly appreciate a few tipps here i tried for like 2 days now to get this working trial and error youtube vids and medium articles mostly i knew nothing about coding before and i might add that i still have no clue i have this written down it describes my main problems command ran import twint import pandas import nest asyncio nest asyncio apply c twint config c search euro2020 c since 2021 06 19 c until 2021 06 20 c custom time username tweet likes link this is the problem line c store csv true c output test em2020 twint run search c description of issue i am very new to all of this lets just say that as a disclaimer i want to scrape all tweets that happen during specific euro 2020 football games i tried my best to get everything working but i have no idea what i am doing to be honest the pandas and asyncio line i got form a youtube channel it doesn t seem to be working if i leave them out so i kept them the general scraping seems to work fine but there are several things that i want to do that give me error messages x200b 1 when i try to slim the csv down by only scraping for the stuff i need not much see my c custom i get the message critical root twint output output csv error list indices must be integers or slices not str list indices must be integers or slices not str i have no idea what that means but i am confused because i am not asking anything new by my wanted parameters they are in the csv when i do not filter so it doesn t make sense to me why i get an error message when i look for less attributes 2 i really want to keep only englisch and german messages but the c lang de line doesn t seem to be doing anything same as the englisch one i also do not know how i can combine the two so looking only for en and de 3 right now what i still need to do is go into excel and re format the first column with the seperators that does work but is an extra step that i am shure can be automated somehow i just don t know how any help on this would be greatly appreciated i don t know if this is the right place to ask but i found no discussion forum or something of the sort thanks for reading anyways environment details windows 10 anaconda and jupyter notebook
0,so being fired after 4 month so i come from different industry 5 years experience working in energy and then move to the new company in ecommerce since february so technically i m like a grad in ecommerce i m the first data scientist in the company so there is no team and i likely spend a lot of time working with business guys before that the company hired external consultants for analytics and the best i can have from them it s only excel file code without any official business or technical documentation so the only way to find out the logic calculation is to look at the code the business lead i usually reported to also take a maternity leave since april the company are in rush to produce annual customer report on the my first day which even after 4 months i hardly knows anything today this morning the top manager reviewed my performance and they are not quite pleased with what i m delivering then they decide not to extend my contract they expect a data scientist to lead them to the place they want coming up with ideas questions and making decision this is in my opinion a little bit more for business analysts who are already in the company for years and also a little bit unrealisitc if the company do not have a proper technical team the good things is that the company uses sas and if they terminate then i can come back with python i wonder what s your thought in this situation usually do you think 4 6 months are good time enough to onboard data scientists if he didn t have any relevant domain knowledge before
1,question evaluating critically appraising diagnostic tests without a reference i may have the opportunity to consult on a project involving a new first to market diagnostic assay what makes this different than projects i ve typically worked on is that with this i don t believe there is a gold standard method with which to compare performance most of the time i am comparing to some reference through equivalence tests anovas and the like so first question in the absence of a gold standard what statistical measures should i be considering for evaluating and critiquing the performance of this new diagnostic assay sensitivity and specificity come to mind but what are some others second question i know i m going to be asked what constitutes an adequate sample size for our experiments and similar to above i typically calculate sample size using an effect size related to a reference i e how small a change do we want to detect using the new method but in the absence of a reference standard what am i comparing it to i feel like only a subject matter expert in the field of the assay s target would know anything about a proper effect size very much looking forward to seeing what the community thinks
0,i m a senior data scientist at disney and i m hosting another data science q a session this thursday 5 30 pm pst i ll be joined by an applied scientist at amazon disclaimer this is completely free and not sponsored in any way i really just enjoy helping students get started and potentially transition into data science as the title mentions i m a senior data scientist at disney and i m going to host another data science q a this thursday at 5 30 pm pst this time i ll have krishna rao join me susan is an applied scientist at amazon and is responsible for building state of the art advertising recommendation systems krishna has had a slightly unconventional path to get to this point his background is in civil engineering and he was first a data science consultant before joining amazon i m looking forward to having him share his journey and the tips he picked up along the way the last session was an absolute blast with over 250 people who attended from all over the world i hope you see you all there register here verification my photo my linkedin feel free to connect krishna’s linkedin
1,how thin is the line in terms of work between statisticians and data scientists how do these two figures differ and what do they have in common it both work with data i am a statistics student from italy and what i did notice is that the job market of positions which require data handling skills here is still at its first phases many companies are looking for statisticians when what they have to do is work with machine learning algorhythms and many search for data scientists when what they re looking for is someone able to focus on exploratory analysis or clinical tests etc are these two figure almost identical or are the job recruiters which in most cases don t know who they need to looking for i am aware that these two figures share similar patterns but tend to have different approaches would like to hear what you think on this matter and if where you live during your job search you stumbled upon similar situations
1,coefficients in logistic regression and odds ratio this seems really simple as no books or example sites are explaining this but i am wondering how to interpret the log odds and odds ratio of coeffecients in logistic regression models most examples i have seen have easily interpretable coefficients such as in this one example exp 1563404 1 1692241 so we can say for a one unit increase in math score we expect to see about 17 increase in the odds of being in an honors class this 17 of increase does not depend on the value that math is held at obviously this 17 comes from the 169 at the end of the 1 but in my logistic regression model i made with the glm function in r one of my coefficients is 4 9996 which exp 4 9996 148 354 so i don t know how to turn that into a percentage in other words what is the formula for turning the odds ratio into a percent change per unit increase is it 1 the number for instance 1 1 169 169 after rounding is 17 17 like in the example is the percent increase per unit change for my coeffecient 1 148 147 14 700 increase
2,high performance speech recognition with no supervision at all paper blog claims to get good performance while just using audio unaligned text and a gan pretty incredible
2,albumentations 1 0 is released a python library for image augmentation image augmentation is used in deep learning and computer vision tasks to increase the quality of trained models the purpose of image augmentation is to create new training samples from the existing data x200b examples of new augmentations in albumentations 1 0 release highlights albumentations no longer uses the imgaug library by default all previous imgaug augmentations in the library are reimplemented in albumentations with the same api but you can still install albumentations with imgaug if you need the old augmentations new augmentations saferotate safely rotate images without cropping someof transform that applies n augmentations from a list generalizing of oneof randomtonecurve see a notebook for examples of this augmentation other changes bug fixes improved serialization that is fully backward compatible totensor is fully removed in favor of totensorv2 logic in setup py that detects existing installations of opencv now also looks for opencv contrib python and opencv contrib python headless more info about the library to install albumentations from pypi run pip install u albumentations full release notes documentation is available at try the online demo at
2,wygpt improved small gpt model in c from scratch dear all x200b i coded a small gpt model for cpu from scratch in c see the following links x200b x200b the major improvement compared to mingpt is that the mlp layer increase to 3 hidden layers x200b 2012 14913 says the mlp layer is a key value map 2010 14075 says three hidden layer is enough thus now we have a enough key value map x200b also sin activation is used as suggested by x200b have funs
2,mlp mixer paper explained mlp mixers is a recent paper from google brain team which shows vanila neural networks designed wisely can perform as good as convolutional neural networks or transformers we think the idea is quite promising so we have made a video on mlp mixer hope its useful
2,regularizing a loss function while training a neural network you find out that there are some examples that are hard for it to learn while some are easy to learn how would you work on modifying the loss function so that the learned weights give more importance to the harder examples instead of giving all the importance to easier examples would l2 regularization be enough or is it possible to write a custom loss function for that particular dataset
0,data science for computer networks where should i start my boss wants me to do computer network analysis however i have never done it before and i only have a math background i have a good command of r and statistics please help me with ideas and learning material i just bought the book network security through data analysis from data to action by michael collins you can check the index with amazon or google if you go to the index of the book part 3 is analytics starts on page 199 the index shows a list of all possible analytics for computer networks my only experience was with delay analysis jitter analysis udp vs tcp in other words not too much i would say that the general question that we are trying to answer is if our computer network is healthy i have time to learn and i am planning to subscribe to o reilly that give you access to millions of books and videos i have seen that they have an area called cybersecurity analyst please let me know if edx coursera or any other mooc that can help me one example about how to do it with r would be the hrbrmstr crafter tools to analyze and visualize network packet capture pcap files here is the link in advance thank you for your help
1,what mathematical concepts applications should i review before starting graduate level statistics i was recently admitted into an ms statistics program for this fall i have some time to brush up on some skills before my classes start i have taken calc i iii linear algebra and elementary de thank you
1,bs in statistics i recently graduated with a bs in statistics i’m curious if anyone here has the same degree and if so what do you do now
0,when you ve created a finalised dataframe do any of you convert it into an excel document to help you visualise your data or am i being inefficient in doing this
2,the effect of fully connected layers in cnns hello everyone i have been working in this field since 2 years in this whole time i have been experimenting several stuffs with cnns and i personally think that fully connected layers are of no use and sometimes adversely affects the cnn i am looking for certain experiments that provide a base for this or maybe a base for something opposite to this can you guys recommend some good reads and or resources for this or maybe we can start a discussion here i would like to contribute my views ideas thank you
