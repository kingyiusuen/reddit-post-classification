{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":4,"colab":{"name":"Copy of 03-neural_networks.ipynb","provenance":[{"file_id":"https://github.com/kingyiusuen/reddit-post-classification/blob/master/notebooks/03-neural_networks.ipynb","timestamp":1624768934244}]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YVW1_6Y2Qi5K"},"source":["# Neural Network Training in Google Colab\n","\n","This notebook is used for training the neural networks in Google Colab."]},{"cell_type":"markdown","metadata":{"id":"AXXiFnVUQi5K"},"source":["Choose `Runtime` > `Change Runtime type` > `GPU` from the menu. Run the following cell to confirm that your notebook has been connected to a GPU."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iwHtvLSAQi5L","executionInfo":{"status":"ok","timestamp":1624760299524,"user_tz":300,"elapsed":9,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"a471dc17-f82e-4fcb-f9a7-91e9c354ebc8"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Sun Jun 27 02:18:18 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    32W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iUqaeR2CQi5L"},"source":["Clone the repository and `cd` into it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-U524_KQi5M","executionInfo":{"status":"ok","timestamp":1624760324289,"user_tz":300,"elapsed":2132,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"0cb04c4f-d9c3-4c58-86ca-41683055519c"},"source":["!git clone https://github.com/kingyiusuen/reddit-post-classification.git\n","%cd reddit-post-classification\n","%env PYTHONPATH=.:$PYTHONPATH"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'reddit-post-classification'...\n","remote: Enumerating objects: 141, done.\u001b[K\n","remote: Counting objects: 100% (141/141), done.\u001b[K\n","remote: Compressing objects: 100% (107/107), done.\u001b[K\n","remote: Total 141 (delta 34), reused 133 (delta 26), pack-reused 0\u001b[K\n","Receiving objects: 100% (141/141), 1.90 MiB | 2.36 MiB/s, done.\n","Resolving deltas: 100% (34/34), done.\n","/content/reddit-post-classification\n","env: PYTHONPATH=.:$PYTHONPATH\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x8doIb-uQi5M"},"source":["Install dependencies."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RHVC9-p1Qi5M","executionInfo":{"status":"ok","timestamp":1624760452062,"user_tz":300,"elapsed":38680,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"1b229421-db79-4e1b-bfef-4035209ee944"},"source":["!make install"],"execution_count":3,"outputs":[{"output_type":"stream","text":["pip install -e . --no-cache-dir\n","Obtaining file:///content/reddit-post-classification\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting fastapi==0.65.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/a8/a6be420593c4061c086e6d2ba47db46401d9af2b98b6cd33d35284f706d3/fastapi-0.65.2-py3-none-any.whl (51kB)\n","\u001b[K     |████████████████████████████████| 51kB 14.6MB/s \n","\u001b[?25hCollecting scikit-learn==0.24.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n","\u001b[K     |████████████████████████████████| 22.3MB 1.3MB/s \n","\u001b[?25hCollecting hydra-core==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/cd/85aa2e3a8babc36feac99df785e54abf99afbc4acc20488630f3ef46980a/hydra_core-1.1.0-py3-none-any.whl (144kB)\n","\u001b[K     |████████████████████████████████| 153kB 61.6MB/s \n","\u001b[?25hCollecting pandas==1.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/0a/90da8840e044c329a0271fb0244ff40a68a2615bc360c296a3dc5e326ab6/pandas-1.2.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (9.9MB)\n","\u001b[K     |████████████████████████████████| 9.9MB 36.7MB/s \n","\u001b[?25hCollecting pytorch-lightning==1.3.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/6a/20d0bf3b967ab62333efea36fe922aaa252d1762555b4a7afb2be5bbdcbf/pytorch_lightning-1.3.5-py3-none-any.whl (808kB)\n","\u001b[K     |████████████████████████████████| 808kB 46.1MB/s \n","\u001b[?25hCollecting uvicorn==0.14.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/fe/a41994c92897b162c0c83e8ef10bec54ebdefbce3f3725b530d2091492ac/uvicorn-0.14.0-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 53.2MB/s \n","\u001b[?25hCollecting hydra-optuna-sweeper==1.1.0\n","  Downloading https://files.pythonhosted.org/packages/6a/6e/a5afcd542744b3a4fc44f7925aa03065b22d578d7626386538dc97ff3284/hydra_optuna_sweeper-1.1.0-py3-none-any.whl\n","Collecting wandb==0.10.32\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/b4/9d92953d8cddc8450c859be12e3dbdd4c7754fb8def94c28b3b351c6ee4e/wandb-0.10.32-py2.py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 41.8MB/s \n","\u001b[?25hCollecting praw==7.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/3f/14c92b8a33564aaab1725162240ca081dc28d84e384bb1c08ada83485ccf/praw-7.3.0-py3-none-any.whl (165kB)\n","\u001b[K     |████████████████████████████████| 174kB 63.6MB/s \n","\u001b[?25hCollecting starlette==0.14.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/34/db1890f442a1cd3a2c761f4109a0eb4e63503218d70a8c8e97faa09a5500/starlette-0.14.2-py3-none-any.whl (60kB)\n","\u001b[K     |████████████████████████████████| 61kB 47.5MB/s \n","\u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f2/2d5425efe57f6c4e06cbe5e587c1fd16929dcf0eb90bd4d3d1e1c97d1151/pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n","\u001b[K     |████████████████████████████████| 10.1MB 40.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->reddit-post-classification==0.1.0) (1.19.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->reddit-post-classification==0.1.0) (1.0.1)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->reddit-post-classification==0.1.0) (1.4.1)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Collecting omegaconf==2.1.*\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/96/1966b48bfe6ca64bfadfa7bcc9a8d73c5d83b4be769321fcc5d617abeb0c/omegaconf-2.1.0-py3-none-any.whl (74kB)\n","\u001b[K     |████████████████████████████████| 81kB 59.1MB/s \n","\u001b[?25hCollecting antlr4-python3-runtime==4.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n","\u001b[K     |████████████████████████████████| 112kB 65.6MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core==1.1.0->reddit-post-classification==0.1.0) (5.1.4)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.5->reddit-post-classification==0.1.0) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.5->reddit-post-classification==0.1.0) (2.8.1)\n","Collecting tensorboard!=2.5.0,>=2.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/21/eebd23060763fedeefb78bc2b286e00fa1d8abda6f70efa2ee08c28af0d4/tensorboard-2.4.1-py3-none-any.whl (10.6MB)\n","\u001b[K     |████████████████████████████████| 10.6MB 41.0MB/s \n","\u001b[?25hCollecting future>=0.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n","\u001b[K     |████████████████████████████████| 829kB 47.1MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (4.41.1)\n","Collecting PyYAML<=5.4.1,>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n","\u001b[K     |████████████████████████████████| 645kB 48.1MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (20.9)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.9.0+cu102)\n","Collecting torchmetrics>=0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/e8/513cd9d0b1c83dc14cd8f788d05cd6a34758d4fd7e4f9e5ecd5d7d599c95/torchmetrics-0.3.2-py3-none-any.whl (274kB)\n","\u001b[K     |████████████████████████████████| 276kB 65.6MB/s \n","\u001b[?25hCollecting pyDeprecate==0.3.0\n","  Downloading https://files.pythonhosted.org/packages/14/52/aa227a0884df71ed1957649085adf2b8bc2a1816d037c2f18b3078854516/pyDeprecate-0.3.0-py3-none-any.whl\n","Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3a/666e63625a19883ae8e1674099e631f9737bd5478c4790e5ad49c5ac5261/fsspec-2021.6.1-py3-none-any.whl (115kB)\n","\u001b[K     |████████████████████████████████| 122kB 65.9MB/s \n","\u001b[?25hCollecting asgiref>=3.3.4\n","  Downloading https://files.pythonhosted.org/packages/17/8b/05e225d11154b8f5358e6a6d277679c9741ec0339d1e451c9cef687a9170/asgiref-3.3.4-py3-none-any.whl\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.14.0->reddit-post-classification==0.1.0) (3.7.4.3)\n","Requirement already satisfied: click>=7.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.14.0->reddit-post-classification==0.1.0) (7.1.2)\n","Collecting h11>=0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/0f/7a0eeea938eaf61074f29fed9717f2010e8d0e0905d36b38d3275a1e4622/h11-0.12.0-py3-none-any.whl (54kB)\n","\u001b[K     |████████████████████████████████| 61kB 29.2MB/s \n","\u001b[?25hCollecting optuna<2.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/b4/a1a80252cef3d8f5a0acdf6e678d6dc07e2e6964ee46d0453a2ae1af1ecb/optuna-2.4.0-py3-none-any.whl (282kB)\n","\u001b[K     |████████████████████████████████| 286kB 62.2MB/s \n","\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.10.32->reddit-post-classification==0.1.0) (2.23.0)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.10.32->reddit-post-classification==0.1.0) (3.12.4)\n","Collecting GitPython>=1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB)\n","\u001b[K     |████████████████████████████████| 174kB 64.6MB/s \n","\u001b[?25hCollecting docker-pycreds>=0.4.0\n","  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n","Collecting shortuuid>=0.5.0\n","  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n","Collecting subprocess32>=3.5.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n","\u001b[K     |████████████████████████████████| 102kB 64.7MB/s \n","\u001b[?25hCollecting configparser>=3.8.1\n","  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.10.32->reddit-post-classification==0.1.0) (2.3)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.10.32->reddit-post-classification==0.1.0) (1.15.0)\n","Collecting pathtools\n","  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.10.32->reddit-post-classification==0.1.0) (5.4.8)\n","Collecting sentry-sdk>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 61.1MB/s \n","\u001b[?25hCollecting update-checker>=0.18\n","  Downloading https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl\n","Collecting websocket-client>=0.54.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/5f/3c211d168b2e9f9342cfb53bcfc26aab0eac63b998015e7af7bcae66119d/websocket_client-1.1.0-py2.py3-none-any.whl (68kB)\n","\u001b[K     |████████████████████████████████| 71kB 54.1MB/s \n","\u001b[?25hCollecting prawcore<3,>=2.1\n","  Downloading https://files.pythonhosted.org/packages/d7/27/e5ca770e299ed4e94646eb630a8c2dfbb202eefc4d17980550f4ec817e5a/prawcore-2.2.0-py3-none-any.whl\n","Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core==1.1.0->reddit-post-classification==0.1.0) (3.4.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.8.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (0.36.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (0.4.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.34.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (3.3.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (0.12.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.31.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (57.0.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (2.4.7)\n","Collecting aiohttp; extra == \"http\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 41.9MB/s \n","\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna<2.5.0->hydra-optuna-sweeper==1.1.0->reddit-post-classification==0.1.0) (1.4.18)\n","Collecting cliff\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/11/aea1cacbd4cf8262809c4d6f95dcb3f2802594de1f51c5bd454d69bf15c5/cliff-3.8.0-py3-none-any.whl (80kB)\n","\u001b[K     |████████████████████████████████| 81kB 61.2MB/s \n","\u001b[?25hCollecting cmaes>=0.6.0\n","  Downloading https://files.pythonhosted.org/packages/01/1f/43b01223a0366171f474320c6e966c39a11587287f098a5f09809b45e05f/cmaes-0.8.2-py3-none-any.whl\n","Collecting colorlog\n","  Downloading https://files.pythonhosted.org/packages/32/e6/e9ddc6fa1104fda718338b341e4b3dc31cd8039ab29e52fc73b508515361/colorlog-5.0.1-py2.py3-none-any.whl\n","Collecting alembic\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/80/ef186e599a57d0e4cb78fc76e0bfc2e6953fa9716b2a5cf2de0117ed8eb5/alembic-1.6.5-py2.py3-none-any.whl (164kB)\n","\u001b[K     |████████████████████████████████| 174kB 59.7MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb==0.10.32->reddit-post-classification==0.1.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb==0.10.32->reddit-post-classification==0.1.0) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb==0.10.32->reddit-post-classification==0.1.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb==0.10.32->reddit-post-classification==0.1.0) (2.10)\n","Collecting gitdb<5,>=4.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n","\u001b[K     |████████████████████████████████| 71kB 62.9MB/s \n","\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (4.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (4.7.2)\n","Collecting multidict<7.0,>=4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n","\u001b[K     |████████████████████████████████| 143kB 61.0MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n","\u001b[K     |████████████████████████████████| 296kB 59.4MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (21.2.0)\n","Collecting async-timeout<4.0,>=3.0\n","  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n","Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna<2.5.0->hydra-optuna-sweeper==1.1.0->reddit-post-classification==0.1.0) (1.1.0)\n","Collecting pbr!=2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/e0/1d4702dd81121d04a477c272d47ee5b6bc970d1a0990b11befa275c55cf2/pbr-5.6.0-py2.py3-none-any.whl (111kB)\n","\u001b[K     |████████████████████████████████| 112kB 62.9MB/s \n","\u001b[?25hCollecting stevedore>=2.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n","\u001b[K     |████████████████████████████████| 51kB 57.7MB/s \n","\u001b[?25hCollecting cmd2>=1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/6a/e929ec70ca05c5962f6541ef29fb9c207dd41f0f2333680fa39f44fa4357/cmd2-2.1.1-py3-none-any.whl (140kB)\n","\u001b[K     |████████████████████████████████| 143kB 58.3MB/s \n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna<2.5.0->hydra-optuna-sweeper==1.1.0->reddit-post-classification==0.1.0) (2.1.0)\n","Collecting Mako\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/54/dbc07fbb20865d3b78fdb7cf7fa713e2cba4f87f71100074ef2dc9f9d1f7/Mako-1.1.4-py2.py3-none-any.whl (75kB)\n","\u001b[K     |████████████████████████████████| 81kB 60.6MB/s \n","\u001b[?25hCollecting python-editor>=0.3\n","  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n","Collecting smmap<5,>=3.0.1\n","  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (3.1.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (0.4.8)\n","Collecting pyperclip>=1.6\n","  Downloading https://files.pythonhosted.org/packages/a7/2c/4c64579f847bd5d539803c8b909e54ba087a79d01bb3aba433a95879a6c5/pyperclip-1.8.2.tar.gz\n","Collecting colorama>=0.3.7\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna<2.5.0->hydra-optuna-sweeper==1.1.0->reddit-post-classification==0.1.0) (0.2.5)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna<2.5.0->hydra-optuna-sweeper==1.1.0->reddit-post-classification==0.1.0) (2.0.1)\n","Building wheels for collected packages: antlr4-python3-runtime, future, subprocess32, pathtools, pyperclip\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=cbe01759d485eecbdbde96ad9458c829854f9107a8789821eec7918005d65221\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2hqk8k1f/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491070 sha256=9182f7df48d2f6c36e37185522bd86a3a6e8e5249be610e84cdc8c2d876dc9a7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2hqk8k1f/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=496f84c93ff1c632405e4ce6a2c806d15457bba556c11e6becb5c8044d101d20\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2hqk8k1f/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=82ad2e7d35114ca6542b674a37414cf011e326ef22b725674c8610add2d493dc\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2hqk8k1f/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-cp37-none-any.whl size=11136 sha256=862a57018696c4b21fe4d5ac2a49e7fc62a816686f57950ca8d24ab069d367fe\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2hqk8k1f/wheels/25/af/b8/3407109267803f4015e1ee2ff23be0c8c19ce4008665931ee1\n","Successfully built antlr4-python3-runtime future subprocess32 pathtools pyperclip\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 2.4.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.5 which is incompatible.\u001b[0m\n","Installing collected packages: starlette, pydantic, fastapi, threadpoolctl, scikit-learn, PyYAML, antlr4-python3-runtime, omegaconf, hydra-core, pandas, tensorboard, future, torchmetrics, pyDeprecate, multidict, yarl, async-timeout, aiohttp, fsspec, pytorch-lightning, asgiref, h11, uvicorn, pbr, stevedore, pyperclip, colorama, cmd2, cliff, cmaes, colorlog, Mako, python-editor, alembic, optuna, hydra-optuna-sweeper, smmap, gitdb, GitPython, docker-pycreds, shortuuid, subprocess32, configparser, pathtools, sentry-sdk, wandb, update-checker, websocket-client, prawcore, praw, reddit-post-classification\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Found existing installation: pandas 1.1.5\n","    Uninstalling pandas-1.1.5:\n","      Successfully uninstalled pandas-1.1.5\n","  Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","  Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Running setup.py develop for reddit-post-classification\n","Successfully installed GitPython-3.1.18 Mako-1.1.4 PyYAML-5.4.1 aiohttp-3.7.4.post0 alembic-1.6.5 antlr4-python3-runtime-4.8 asgiref-3.3.4 async-timeout-3.0.1 cliff-3.8.0 cmaes-0.8.2 cmd2-2.1.1 colorama-0.4.4 colorlog-5.0.1 configparser-5.0.2 docker-pycreds-0.4.0 fastapi-0.65.2 fsspec-2021.6.1 future-0.18.2 gitdb-4.0.7 h11-0.12.0 hydra-core-1.1.0 hydra-optuna-sweeper-1.1.0 multidict-5.1.0 omegaconf-2.1.0 optuna-2.4.0 pandas-1.2.5 pathtools-0.1.2 pbr-5.6.0 praw-7.3.0 prawcore-2.2.0 pyDeprecate-0.3.0 pydantic-1.8.2 pyperclip-1.8.2 python-editor-1.0.4 pytorch-lightning-1.3.5 reddit-post-classification scikit-learn-0.24.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 starlette-0.14.2 stevedore-3.3.0 subprocess32-3.5.4 tensorboard-2.4.1 threadpoolctl-2.1.0 torchmetrics-0.3.2 update-checker-0.18.0 uvicorn-0.14.0 wandb-0.10.32 websocket-client-1.1.0 yarl-1.6.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EL4NissxQi5M"},"source":["Run the following cells to start training and run hyperparameter optimization."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_kfD639Qi5N","executionInfo":{"status":"ok","timestamp":1624760870093,"user_tz":300,"elapsed":346513,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"8efa7b94-215f-4f62-d3bd-3a8a394b42e5"},"source":["!python scripts/train.py -m model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna hydra.sweeper.n_trials=10"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 2021-06-27 02:22:09,136]\u001b[0m A new study created in memory with name: no-name-69a67eb6-5bd4-4388-b5e6-36fe3bf33377\u001b[0m\n","[2021-06-27 02:22:09,136][HYDRA] Study name: no-name-69a67eb6-5bd4-4388-b5e6-36fe3bf33377\n","[2021-06-27 02:22:09,136][HYDRA] Storage: None\n","[2021-06-27 02:22:09,136][HYDRA] Sampler: TPESampler\n","[2021-06-27 02:22:09,136][HYDRA] Directions: ['maximize']\n","[2021-06-27 02:22:09,140][HYDRA] Launching 1 jobs locally\n","[2021-06-27 02:22:09,140][HYDRA] \t#0 : optimizer.lr=0.009303199318889763 model.num_kernels=128 model.embedding_dim=128 model.dropout=0.1 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n","[2021-06-27 02:22:09,375][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 02:22:09,388][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","2021-06-27 02:22:24.206980: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33molive-glade-32\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/367x897i\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_022223-367x897i\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | CNN              | 888 K \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","888 K     Trainable params\n","0         Non-trainable params\n","888 K     Total params\n","3.554     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:01<00:00,  8.58it/s, loss=7.78, v_num=897i]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 20/20 [00:02<00:00,  9.15it/s, loss=7.78, v_num=897i, val/acc=0.537, val/loss=1.760]\n","Epoch 1:  90% 18/20 [00:01<00:00,  9.54it/s, loss=1.29, v_num=897i, val/acc=0.537, val/loss=1.760]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:02<00:00,  9.81it/s, loss=1.29, v_num=897i, val/acc=0.657, val/loss=0.825]\n","Epoch 2:  90% 18/20 [00:01<00:00,  9.49it/s, loss=0.633, v_num=897i, val/acc=0.657, val/loss=0.825]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:02<00:00,  9.77it/s, loss=0.633, v_num=897i, val/acc=0.761, val/loss=0.662]\n","Epoch 3:  90% 18/20 [00:01<00:00,  9.53it/s, loss=0.441, v_num=897i, val/acc=0.761, val/loss=0.662]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:02<00:00,  9.78it/s, loss=0.441, v_num=897i, val/acc=0.761, val/loss=0.641]\n","Epoch 4:  90% 18/20 [00:01<00:00,  9.74it/s, loss=0.311, v_num=897i, val/acc=0.761, val/loss=0.641]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:02<00:00, 10.00it/s, loss=0.311, v_num=897i, val/acc=0.776, val/loss=0.613]\n","Epoch 5:  90% 18/20 [00:01<00:00,  9.40it/s, loss=0.248, v_num=897i, val/acc=0.776, val/loss=0.613]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:02<00:00,  9.66it/s, loss=0.248, v_num=897i, val/acc=0.765, val/loss=0.647]\n","Epoch 6:  90% 18/20 [00:01<00:00,  9.45it/s, loss=0.23, v_num=897i, val/acc=0.765, val/loss=0.647]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:02<00:00,  9.71it/s, loss=0.23, v_num=897i, val/acc=0.795, val/loss=0.630]\n","Epoch 7:  90% 18/20 [00:01<00:00,  9.68it/s, loss=0.221, v_num=897i, val/acc=0.795, val/loss=0.630]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:02<00:00,  9.93it/s, loss=0.221, v_num=897i, val/acc=0.776, val/loss=0.647]\n","Epoch 8:  90% 18/20 [00:01<00:00,  9.69it/s, loss=0.217, v_num=897i, val/acc=0.776, val/loss=0.647]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:02<00:00,  9.84it/s, loss=0.217, v_num=897i, val/acc=0.769, val/loss=0.645]\n","Epoch 9:  90% 18/20 [00:01<00:00,  9.59it/s, loss=0.207, v_num=897i, val/acc=0.769, val/loss=0.645]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:02<00:00,  9.84it/s, loss=0.207, v_num=897i, val/acc=0.772, val/loss=0.641]\n","Epoch 9: 100% 20/20 [00:02<00:00,  9.82it/s, loss=0.207, v_num=897i, val/acc=0.772, val/loss=0.641]\n","[2021-06-27 02:22:51,985][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00, 14.57it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.7851239442825317, 'test/loss': 0.5197669267654419}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 240\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_022223-367x897i/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_022223-367x897i/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.20713\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 29\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624760572\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.77239\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.64121\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.78512\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.51977\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▄▇▇▇▇█▇▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▂▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33molive-glade-32\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/367x897i\u001b[0m\n","[2021-06-27 02:22:58,257][HYDRA] Launching 1 jobs locally\n","[2021-06-27 02:22:58,257][HYDRA] \t#1 : optimizer.lr=0.0021251467576750933 model.num_kernels=128 model.embedding_dim=256 model.dropout=0.30000000000000004 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n","[2021-06-27 02:22:58,490][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 02:22:58,496][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkingyiusuen\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-06-27 02:22:59.758179: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfancy-smoke-33\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/7hx5f7kz\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_022258-7hx5f7kz\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | CNN              | 1.8 M \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","1.8 M     Trainable params\n","0         Non-trainable params\n","1.8 M     Total params\n","7.098     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:02<00:00,  6.55it/s, loss=3.87, v_num=f7kz]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 20/20 [00:02<00:00,  7.17it/s, loss=3.87, v_num=f7kz, val/acc=0.534, val/loss=1.520]\n","Epoch 1:  90% 18/20 [00:02<00:00,  7.01it/s, loss=1.16, v_num=f7kz, val/acc=0.534, val/loss=1.520]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:02<00:00,  7.27it/s, loss=1.16, v_num=f7kz, val/acc=0.701, val/loss=0.770]\n","Epoch 2:  90% 18/20 [00:02<00:00,  6.96it/s, loss=0.749, v_num=f7kz, val/acc=0.701, val/loss=0.770]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:02<00:00,  7.24it/s, loss=0.749, v_num=f7kz, val/acc=0.769, val/loss=0.618]\n","Epoch 3:  90% 18/20 [00:02<00:00,  6.97it/s, loss=0.567, v_num=f7kz, val/acc=0.769, val/loss=0.618]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:02<00:00,  7.22it/s, loss=0.567, v_num=f7kz, val/acc=0.787, val/loss=0.604]\n","Epoch 4:  90% 18/20 [00:02<00:00,  6.93it/s, loss=0.455, v_num=f7kz, val/acc=0.787, val/loss=0.604]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:02<00:00,  7.20it/s, loss=0.455, v_num=f7kz, val/acc=0.795, val/loss=0.569]\n","Epoch 5:  90% 18/20 [00:02<00:00,  6.96it/s, loss=0.401, v_num=f7kz, val/acc=0.795, val/loss=0.569]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:02<00:00,  7.19it/s, loss=0.401, v_num=f7kz, val/acc=0.799, val/loss=0.565]\n","Epoch 6:  90% 18/20 [00:02<00:00,  7.04it/s, loss=0.375, v_num=f7kz, val/acc=0.799, val/loss=0.565]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:02<00:00,  7.33it/s, loss=0.375, v_num=f7kz, val/acc=0.806, val/loss=0.558]\n","Epoch 7:  90% 18/20 [00:02<00:00,  6.89it/s, loss=0.375, v_num=f7kz, val/acc=0.806, val/loss=0.558]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:02<00:00,  7.17it/s, loss=0.375, v_num=f7kz, val/acc=0.806, val/loss=0.560]\n","Epoch 8:  90% 18/20 [00:02<00:00,  7.01it/s, loss=0.375, v_num=f7kz, val/acc=0.806, val/loss=0.560]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:02<00:00,  7.25it/s, loss=0.375, v_num=f7kz, val/acc=0.806, val/loss=0.562]\n","Epoch 9:  90% 18/20 [00:02<00:00,  7.00it/s, loss=0.376, v_num=f7kz, val/acc=0.806, val/loss=0.562]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:02<00:00,  7.26it/s, loss=0.376, v_num=f7kz, val/acc=0.810, val/loss=0.559]\n","Epoch 9: 100% 20/20 [00:02<00:00,  7.18it/s, loss=0.376, v_num=f7kz, val/acc=0.810, val/loss=0.559]\n","[2021-06-27 02:23:28,832][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00, 12.76it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.85537189245224, 'test/loss': 0.44427648186683655}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 292\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_022258-7hx5f7kz/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_022258-7hx5f7kz/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.38189\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 31\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624760609\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.8097\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.55947\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.85537\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.44428\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▂▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▅▇▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▃▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfancy-smoke-33\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/7hx5f7kz\u001b[0m\n","[2021-06-27 02:23:34,659][HYDRA] Launching 1 jobs locally\n","[2021-06-27 02:23:34,659][HYDRA] \t#2 : optimizer.lr=0.009648693745382655 model.num_kernels=256 model.embedding_dim=128 model.dropout=0.2 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n","[2021-06-27 02:23:34,889][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 02:23:34,895][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 02:23:36.875773: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33melectric-serenity-34\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/xuh3dswy\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_022334-xuh3dswy\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | CNN              | 1.4 M \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","1.4 M     Trainable params\n","0         Non-trainable params\n","1.4 M     Total params\n","5.531     Total estimated model params size (MB)\n","Epoch 0:  90% 18/20 [00:02<00:00,  7.23it/s, loss=12.6, v_num=dswy]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 20/20 [00:02<00:00,  7.47it/s, loss=12.6, v_num=dswy]\n","Epoch 0: 100% 20/20 [00:02<00:00,  7.41it/s, loss=12.6, v_num=dswy, val/acc=0.381, val/loss=1.590]\n","Epoch 1:  90% 18/20 [00:02<00:00,  7.15it/s, loss=1.07, v_num=dswy, val/acc=0.381, val/loss=1.590]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:02<00:00,  7.41it/s, loss=1.07, v_num=dswy, val/acc=0.683, val/loss=0.845]\n","Epoch 2:  90% 18/20 [00:02<00:00,  7.20it/s, loss=0.734, v_num=dswy, val/acc=0.683, val/loss=0.845]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:02<00:00,  7.39it/s, loss=0.734, v_num=dswy, val/acc=0.672, val/loss=0.740]\n","Epoch 3:  90% 18/20 [00:02<00:00,  7.16it/s, loss=0.566, v_num=dswy, val/acc=0.672, val/loss=0.740]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:02<00:00,  7.33it/s, loss=0.566, v_num=dswy, val/acc=0.761, val/loss=0.651]\n","Epoch 4:  90% 18/20 [00:02<00:00,  7.06it/s, loss=0.403, v_num=dswy, val/acc=0.761, val/loss=0.651]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:02<00:00,  7.29it/s, loss=0.403, v_num=dswy, val/acc=0.761, val/loss=0.651]\n","Epoch 4: 100% 20/20 [00:02<00:00,  7.25it/s, loss=0.403, v_num=dswy, val/acc=0.757, val/loss=0.637]\n","Epoch 5:  90% 18/20 [00:02<00:00,  7.15it/s, loss=0.357, v_num=dswy, val/acc=0.757, val/loss=0.637]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:02<00:00,  7.39it/s, loss=0.357, v_num=dswy, val/acc=0.761, val/loss=0.642]\n","Epoch 6:  90% 18/20 [00:02<00:00,  7.13it/s, loss=0.339, v_num=dswy, val/acc=0.761, val/loss=0.642]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:02<00:00,  7.31it/s, loss=0.339, v_num=dswy, val/acc=0.765, val/loss=0.621]\n","Epoch 7:  90% 18/20 [00:02<00:00,  7.14it/s, loss=0.312, v_num=dswy, val/acc=0.765, val/loss=0.621]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:02<00:00,  7.40it/s, loss=0.312, v_num=dswy, val/acc=0.769, val/loss=0.621]\n","Epoch 8:  90% 18/20 [00:02<00:00,  7.13it/s, loss=0.306, v_num=dswy, val/acc=0.769, val/loss=0.621]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:02<00:00,  7.32it/s, loss=0.306, v_num=dswy, val/acc=0.772, val/loss=0.619]\n","Epoch 9:  90% 18/20 [00:02<00:00,  7.22it/s, loss=0.299, v_num=dswy, val/acc=0.772, val/loss=0.619]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:02<00:00,  7.47it/s, loss=0.299, v_num=dswy, val/acc=0.776, val/loss=0.620]\n","Epoch 9: 100% 20/20 [00:02<00:00,  7.40it/s, loss=0.299, v_num=dswy, val/acc=0.776, val/loss=0.620]\n","[2021-06-27 02:24:05,390][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00, 11.03it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.7603305578231812, 'test/loss': 0.6020679473876953}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 344\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_022334-xuh3dswy/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_022334-xuh3dswy/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.29896\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 31\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624760645\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.77612\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.61956\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.76033\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.60207\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▆▆███████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▃▂▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33melectric-serenity-34\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/xuh3dswy\u001b[0m\n","[2021-06-27 02:24:10,920][HYDRA] Launching 1 jobs locally\n","[2021-06-27 02:24:10,920][HYDRA] \t#3 : optimizer.lr=0.0016342362247584504 model.num_kernels=256 model.embedding_dim=128 model.dropout=0.1 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n","[2021-06-27 02:24:11,144][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 02:24:11,153][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 02:24:12.407974: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdaily-dust-35\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2jyl6axw\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_022411-2jyl6axw\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | CNN              | 1.4 M \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","1.4 M     Trainable params\n","0         Non-trainable params\n","1.4 M     Total params\n","5.531     Total estimated model params size (MB)\n","Epoch 0:  90% 18/20 [00:02<00:00,  7.05it/s, loss=3.45, v_num=6axw]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 20/20 [00:02<00:00,  7.24it/s, loss=3.45, v_num=6axw, val/acc=0.470, val/loss=1.600]\n","Epoch 1:  90% 18/20 [00:02<00:00,  7.20it/s, loss=0.984, v_num=6axw, val/acc=0.470, val/loss=1.600]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:02<00:00,  7.44it/s, loss=0.984, v_num=6axw, val/acc=0.612, val/loss=0.896]\n","Epoch 2:  90% 18/20 [00:02<00:00,  7.20it/s, loss=0.555, v_num=6axw, val/acc=0.612, val/loss=0.896]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:02<00:00,  7.43it/s, loss=0.555, v_num=6axw, val/acc=0.765, val/loss=0.612]\n","Epoch 3:  90% 18/20 [00:02<00:00,  7.11it/s, loss=0.426, v_num=6axw, val/acc=0.765, val/loss=0.612]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:02<00:00,  7.34it/s, loss=0.426, v_num=6axw, val/acc=0.765, val/loss=0.612]\n","Epoch 3: 100% 20/20 [00:02<00:00,  7.29it/s, loss=0.426, v_num=6axw, val/acc=0.787, val/loss=0.550]\n","Epoch 4:  90% 18/20 [00:02<00:00,  7.23it/s, loss=0.339, v_num=6axw, val/acc=0.787, val/loss=0.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:02<00:00,  7.48it/s, loss=0.339, v_num=6axw, val/acc=0.795, val/loss=0.574]\n","Epoch 5:  90% 18/20 [00:02<00:00,  7.20it/s, loss=0.323, v_num=6axw, val/acc=0.795, val/loss=0.574]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:02<00:00,  7.41it/s, loss=0.323, v_num=6axw, val/acc=0.791, val/loss=0.567]\n","Epoch 6:  90% 18/20 [00:02<00:00,  7.22it/s, loss=0.306, v_num=6axw, val/acc=0.791, val/loss=0.567]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:02<00:00,  7.42it/s, loss=0.306, v_num=6axw, val/acc=0.802, val/loss=0.546]\n","Epoch 7:  90% 18/20 [00:02<00:00,  7.16it/s, loss=0.299, v_num=6axw, val/acc=0.802, val/loss=0.546]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:02<00:00,  7.37it/s, loss=0.299, v_num=6axw, val/acc=0.806, val/loss=0.547]\n","Epoch 8:  90% 18/20 [00:02<00:00,  7.23it/s, loss=0.289, v_num=6axw, val/acc=0.806, val/loss=0.547]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:02<00:00,  7.44it/s, loss=0.289, v_num=6axw, val/acc=0.795, val/loss=0.551]\n","Epoch 9:  90% 18/20 [00:02<00:00,  7.19it/s, loss=0.293, v_num=6axw, val/acc=0.795, val/loss=0.551]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:02<00:00,  7.45it/s, loss=0.293, v_num=6axw, val/acc=0.795, val/loss=0.550]\n","Epoch 9: 100% 20/20 [00:02<00:00,  7.44it/s, loss=0.293, v_num=6axw, val/acc=0.795, val/loss=0.550]\n","[2021-06-27 02:24:40,827][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00, 11.27it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.8471074104309082, 'test/loss': 0.4518361985683441}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 392\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_022411-2jyl6axw/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_022411-2jyl6axw/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.29653\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 30\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624760681\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.79478\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.54955\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.84711\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.45184\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▂▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▄▇███████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▃▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdaily-dust-35\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2jyl6axw\u001b[0m\n","[2021-06-27 02:24:46,358][HYDRA] Launching 1 jobs locally\n","[2021-06-27 02:24:46,358][HYDRA] \t#4 : optimizer.lr=0.002985873632533843 model.num_kernels=256 model.embedding_dim=256 model.dropout=0.5 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n","[2021-06-27 02:24:46,594][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 02:24:46,599][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 02:24:47.890083: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlilac-sponge-36\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2cmpvuem\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_022446-2cmpvuem\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | CNN              | 2.8 M \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","2.8 M     Trainable params\n","0         Non-trainable params\n","2.8 M     Total params\n","11.041    Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:03<00:00,  4.82it/s, loss=8.33, v_num=vuem]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:03<00:00,  5.23it/s, loss=8.33, v_num=vuem]\n","Epoch 0: 100% 20/20 [00:03<00:00,  5.33it/s, loss=8.33, v_num=vuem, val/acc=0.369, val/loss=3.070]\n","Epoch 1:  90% 18/20 [00:03<00:00,  5.12it/s, loss=2.26, v_num=vuem, val/acc=0.369, val/loss=3.070]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  9.41it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:03<00:00,  5.34it/s, loss=2.26, v_num=vuem, val/acc=0.787, val/loss=0.608]\n","Epoch 2:  90% 18/20 [00:03<00:00,  5.17it/s, loss=0.817, v_num=vuem, val/acc=0.787, val/loss=0.608]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:03<00:00,  5.40it/s, loss=0.817, v_num=vuem, val/acc=0.802, val/loss=0.574]\n","Epoch 3:  90% 18/20 [00:03<00:00,  5.09it/s, loss=0.634, v_num=vuem, val/acc=0.802, val/loss=0.574]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  9.15it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:03<00:00,  5.31it/s, loss=0.634, v_num=vuem, val/acc=0.701, val/loss=0.724]\n","Epoch 4:  90% 18/20 [00:03<00:00,  5.11it/s, loss=0.506, v_num=vuem, val/acc=0.701, val/loss=0.724]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  9.60it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:03<00:00,  5.34it/s, loss=0.506, v_num=vuem, val/acc=0.802, val/loss=0.562]\n","Epoch 5:  90% 18/20 [00:03<00:00,  5.15it/s, loss=0.454, v_num=vuem, val/acc=0.802, val/loss=0.562]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:03<00:00,  5.40it/s, loss=0.454, v_num=vuem, val/acc=0.802, val/loss=0.562]\n","Epoch 5: 100% 20/20 [00:03<00:00,  5.37it/s, loss=0.454, v_num=vuem, val/acc=0.791, val/loss=0.580]\n","Epoch 6:  90% 18/20 [00:03<00:00,  5.20it/s, loss=0.437, v_num=vuem, val/acc=0.791, val/loss=0.580]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  9.38it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:03<00:00,  5.42it/s, loss=0.437, v_num=vuem, val/acc=0.802, val/loss=0.578]\n","Epoch 7:  90% 18/20 [00:03<00:00,  5.18it/s, loss=0.413, v_num=vuem, val/acc=0.802, val/loss=0.578]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  9.98it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:03<00:00,  5.41it/s, loss=0.413, v_num=vuem, val/acc=0.799, val/loss=0.563]\n","Epoch 7: 100% 20/20 [00:03<00:00,  5.40it/s, loss=0.413, v_num=vuem, val/acc=0.799, val/loss=0.563]\n","[2021-06-27 02:25:19,050][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00,  9.98it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.8305785059928894, 'test/loss': 0.46510425209999084}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 436\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_022446-2cmpvuem/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_022446-2cmpvuem/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.42255\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 7\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 136\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 33\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624760719\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 16\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.79851\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.56266\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.83058\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.4651\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▄▄▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▄▄▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▄▄▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁██▆████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mlilac-sponge-36\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2cmpvuem\u001b[0m\n","[2021-06-27 02:25:24,817][HYDRA] Launching 1 jobs locally\n","[2021-06-27 02:25:24,817][HYDRA] \t#5 : optimizer.lr=0.0063082338371660135 model.num_kernels=128 model.embedding_dim=64 model.dropout=0.30000000000000004 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n","[2021-06-27 02:25:25,043][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 02:25:25,048][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 02:25:26.271833: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdrawn-dust-37\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/363ajfdl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_022525-363ajfdl\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | CNN              | 445 K \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","445 K     Trainable params\n","0         Non-trainable params\n","445 K     Total params\n","1.782     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:01<00:00, 10.67it/s, loss=3.47, v_num=jfdl]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 20/20 [00:01<00:00, 11.47it/s, loss=3.47, v_num=jfdl, val/acc=0.377, val/loss=1.100]\n","Epoch 1:  90% 18/20 [00:01<00:00, 11.49it/s, loss=0.944, v_num=jfdl, val/acc=0.377, val/loss=1.100]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:01<00:00, 11.73it/s, loss=0.944, v_num=jfdl, val/acc=0.646, val/loss=0.822]\n","Epoch 2:  90% 18/20 [00:01<00:00, 11.65it/s, loss=0.652, v_num=jfdl, val/acc=0.646, val/loss=0.822]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:01<00:00, 11.85it/s, loss=0.652, v_num=jfdl, val/acc=0.795, val/loss=0.605]\n","Epoch 3:  90% 18/20 [00:01<00:00, 11.56it/s, loss=0.524, v_num=jfdl, val/acc=0.795, val/loss=0.605]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:01<00:00, 11.64it/s, loss=0.524, v_num=jfdl, val/acc=0.806, val/loss=0.548]\n","Epoch 4:  90% 18/20 [00:01<00:00, 11.21it/s, loss=0.428, v_num=jfdl, val/acc=0.806, val/loss=0.548]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:01<00:00, 11.41it/s, loss=0.428, v_num=jfdl, val/acc=0.825, val/loss=0.543]\n","Epoch 5:  90% 18/20 [00:01<00:00, 11.31it/s, loss=0.387, v_num=jfdl, val/acc=0.825, val/loss=0.543]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:01<00:00, 11.53it/s, loss=0.387, v_num=jfdl, val/acc=0.806, val/loss=0.542]\n","Epoch 6:  90% 18/20 [00:01<00:00, 11.61it/s, loss=0.371, v_num=jfdl, val/acc=0.806, val/loss=0.542]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:01<00:00, 11.77it/s, loss=0.371, v_num=jfdl, val/acc=0.795, val/loss=0.550]\n","Epoch 7:  90% 18/20 [00:01<00:00, 11.39it/s, loss=0.34, v_num=jfdl, val/acc=0.795, val/loss=0.550]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:01<00:00, 11.56it/s, loss=0.34, v_num=jfdl, val/acc=0.806, val/loss=0.545]\n","Epoch 8:  90% 18/20 [00:01<00:00, 11.50it/s, loss=0.347, v_num=jfdl, val/acc=0.806, val/loss=0.545]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:01<00:00, 11.74it/s, loss=0.347, v_num=jfdl, val/acc=0.802, val/loss=0.544]\n","Epoch 9:  90% 18/20 [00:01<00:00, 11.63it/s, loss=0.349, v_num=jfdl, val/acc=0.802, val/loss=0.544]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:01<00:00, 11.80it/s, loss=0.349, v_num=jfdl, val/acc=0.810, val/loss=0.543]\n","Epoch 9: 100% 20/20 [00:01<00:00, 11.78it/s, loss=0.349, v_num=jfdl, val/acc=0.810, val/loss=0.543]\n","[2021-06-27 02:25:44,648][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00, 16.63it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.8264462947845459, 'test/loss': 0.46288299560546875}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 486\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_022525-363ajfdl/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_022525-363ajfdl/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.35291\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 19\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624760744\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.8097\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.54257\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.82645\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.46288\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▂▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▅████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▅▂▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdrawn-dust-37\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/363ajfdl\u001b[0m\n","[2021-06-27 02:25:50,188][HYDRA] Launching 1 jobs locally\n","[2021-06-27 02:25:50,188][HYDRA] \t#6 : optimizer.lr=0.00841520059603814 model.num_kernels=64 model.embedding_dim=64 model.dropout=0.5 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n","[2021-06-27 02:25:50,427][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 02:25:50,432][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 02:25:51.699515: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglowing-water-38\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1ceozy88\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_022550-1ceozy88\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | CNN              | 321 K \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","321 K     Trainable params\n","0         Non-trainable params\n","321 K     Total params\n","1.286     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:01<00:00, 12.56it/s, loss=2.35, v_num=zy88]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 20/20 [00:01<00:00, 13.41it/s, loss=2.35, v_num=zy88, val/acc=0.604, val/loss=0.966]\n","Epoch 1:  90% 18/20 [00:01<00:00, 13.56it/s, loss=1, v_num=zy88, val/acc=0.604, val/loss=0.966]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:01<00:00, 13.68it/s, loss=1, v_num=zy88, val/acc=0.739, val/loss=0.757]\n","Epoch 2:  90% 18/20 [00:01<00:00, 13.54it/s, loss=0.805, v_num=zy88, val/acc=0.739, val/loss=0.757]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:01<00:00, 13.61it/s, loss=0.805, v_num=zy88, val/acc=0.720, val/loss=0.679]\n","Epoch 3:  90% 18/20 [00:01<00:00, 13.42it/s, loss=0.698, v_num=zy88, val/acc=0.720, val/loss=0.679]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:01<00:00, 13.51it/s, loss=0.698, v_num=zy88, val/acc=0.795, val/loss=0.623]\n","Epoch 4:  90% 18/20 [00:01<00:00, 13.21it/s, loss=0.571, v_num=zy88, val/acc=0.795, val/loss=0.623]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:01<00:00, 13.36it/s, loss=0.571, v_num=zy88, val/acc=0.799, val/loss=0.573]\n","Epoch 5:  90% 18/20 [00:01<00:00, 13.14it/s, loss=0.522, v_num=zy88, val/acc=0.799, val/loss=0.573]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:01<00:00, 13.04it/s, loss=0.522, v_num=zy88, val/acc=0.795, val/loss=0.565]\n","Epoch 6:  90% 18/20 [00:01<00:00, 13.19it/s, loss=0.479, v_num=zy88, val/acc=0.795, val/loss=0.565]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:01<00:00, 13.30it/s, loss=0.479, v_num=zy88, val/acc=0.799, val/loss=0.559]\n","Epoch 7:  90% 18/20 [00:01<00:00, 13.15it/s, loss=0.484, v_num=zy88, val/acc=0.799, val/loss=0.559]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:01<00:00, 13.33it/s, loss=0.484, v_num=zy88, val/acc=0.799, val/loss=0.558]\n","Epoch 8:  90% 18/20 [00:01<00:00, 13.26it/s, loss=0.47, v_num=zy88, val/acc=0.799, val/loss=0.558]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:01<00:00, 13.45it/s, loss=0.47, v_num=zy88, val/acc=0.799, val/loss=0.559]\n","Epoch 9:  90% 18/20 [00:01<00:00, 13.21it/s, loss=0.462, v_num=zy88, val/acc=0.799, val/loss=0.559]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:01<00:00, 13.27it/s, loss=0.462, v_num=zy88, val/acc=0.799, val/loss=0.558]\n","Epoch 9: 100% 20/20 [00:01<00:00, 13.25it/s, loss=0.462, v_num=zy88, val/acc=0.799, val/loss=0.558]\n","[2021-06-27 02:26:07,811][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00, 14.43it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.8223140239715576, 'test/loss': 0.4989605247974396}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 536\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_022550-1ceozy88/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_022550-1ceozy88/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.46274\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 17\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624760767\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.79851\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.55837\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.82231\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.49896\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▃▂▂▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▆▅███████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▄▃▂▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mglowing-water-38\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1ceozy88\u001b[0m\n","[2021-06-27 02:26:13,626][HYDRA] Launching 1 jobs locally\n","[2021-06-27 02:26:13,626][HYDRA] \t#7 : optimizer.lr=0.0037038928547746554 model.num_kernels=128 model.embedding_dim=256 model.dropout=0.5 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n","[2021-06-27 02:26:13,855][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 02:26:13,861][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 02:26:15.110722: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33myouthful-field-39\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2pzfr3t3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_022613-2pzfr3t3\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | CNN              | 1.8 M \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","1.8 M     Trainable params\n","0         Non-trainable params\n","1.8 M     Total params\n","7.098     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:02<00:00,  6.56it/s, loss=4.43, v_num=r3t3]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:02<00:00,  7.06it/s, loss=4.43, v_num=r3t3]\n","Epoch 0: 100% 20/20 [00:02<00:00,  7.16it/s, loss=4.43, v_num=r3t3, val/acc=0.560, val/loss=2.310]\n","Epoch 1:  90% 18/20 [00:02<00:00,  6.99it/s, loss=1.63, v_num=r3t3, val/acc=0.560, val/loss=2.310]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:02<00:00,  7.25it/s, loss=1.63, v_num=r3t3, val/acc=0.739, val/loss=0.781]\n","Epoch 2:  90% 18/20 [00:02<00:00,  6.88it/s, loss=0.876, v_num=r3t3, val/acc=0.739, val/loss=0.781]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:02<00:00,  7.12it/s, loss=0.876, v_num=r3t3, val/acc=0.731, val/loss=0.648]\n","Epoch 3:  90% 18/20 [00:02<00:00,  6.94it/s, loss=0.671, v_num=r3t3, val/acc=0.731, val/loss=0.648]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:02<00:00,  7.19it/s, loss=0.671, v_num=r3t3, val/acc=0.795, val/loss=0.564]\n","Epoch 4:  90% 18/20 [00:02<00:00,  6.90it/s, loss=0.515, v_num=r3t3, val/acc=0.795, val/loss=0.564]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:02<00:00,  7.19it/s, loss=0.515, v_num=r3t3, val/acc=0.817, val/loss=0.531]\n","Epoch 5:  90% 18/20 [00:02<00:00,  6.83it/s, loss=0.493, v_num=r3t3, val/acc=0.817, val/loss=0.531]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:02<00:00,  7.09it/s, loss=0.493, v_num=r3t3, val/acc=0.799, val/loss=0.520]\n","Epoch 6:  90% 18/20 [00:02<00:00,  6.99it/s, loss=0.464, v_num=r3t3, val/acc=0.799, val/loss=0.520]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:02<00:00,  7.24it/s, loss=0.464, v_num=r3t3, val/acc=0.787, val/loss=0.538]\n","Epoch 7:  90% 18/20 [00:02<00:00,  6.99it/s, loss=0.429, v_num=r3t3, val/acc=0.787, val/loss=0.538]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:02<00:00,  7.26it/s, loss=0.429, v_num=r3t3, val/acc=0.787, val/loss=0.530]\n","Epoch 8:  90% 18/20 [00:02<00:00,  7.04it/s, loss=0.417, v_num=r3t3, val/acc=0.787, val/loss=0.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:02<00:00,  7.27it/s, loss=0.417, v_num=r3t3, val/acc=0.802, val/loss=0.530]\n","Epoch 9:  90% 18/20 [00:02<00:00,  6.94it/s, loss=0.432, v_num=r3t3, val/acc=0.802, val/loss=0.530]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:02<00:00,  7.24it/s, loss=0.432, v_num=r3t3, val/acc=0.802, val/loss=0.527]\n","Epoch 9: 100% 20/20 [00:02<00:00,  7.23it/s, loss=0.432, v_num=r3t3, val/acc=0.802, val/loss=0.527]\n","[2021-06-27 02:26:44,198][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00, 12.82it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.8719007968902588, 'test/loss': 0.41568201780319214}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 582\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_022613-2pzfr3t3/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_022613-2pzfr3t3/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.43514\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 31\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624760804\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.80224\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.52703\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.8719\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.41568\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▃▂▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▆▆▇█▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▂▂▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33myouthful-field-39\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2pzfr3t3\u001b[0m\n","[2021-06-27 02:26:50,042][HYDRA] Launching 1 jobs locally\n","[2021-06-27 02:26:50,042][HYDRA] \t#8 : optimizer.lr=0.006801049746560324 model.num_kernels=256 model.embedding_dim=128 model.dropout=0.1 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n","[2021-06-27 02:26:50,271][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 02:26:50,276][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 02:26:51.524587: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbumbling-snowflake-40\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3uksanbo\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_022650-3uksanbo\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | CNN              | 1.4 M \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","1.4 M     Trainable params\n","0         Non-trainable params\n","1.4 M     Total params\n","5.531     Total estimated model params size (MB)\n","Epoch 0:  90% 18/20 [00:02<00:00,  7.18it/s, loss=7.96, v_num=anbo]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0: 100% 20/20 [00:02<00:00,  7.40it/s, loss=7.96, v_num=anbo, val/acc=0.369, val/loss=2.310]\n","Epoch 1:  90% 18/20 [00:02<00:00,  7.17it/s, loss=1.18, v_num=anbo, val/acc=0.369, val/loss=2.310]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:02<00:00,  7.38it/s, loss=1.18, v_num=anbo, val/acc=0.601, val/loss=1.020]\n","Epoch 2:  90% 18/20 [00:02<00:00,  7.16it/s, loss=0.576, v_num=anbo, val/acc=0.601, val/loss=1.020]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:02<00:00,  7.37it/s, loss=0.576, v_num=anbo, val/acc=0.746, val/loss=0.690]\n","Epoch 3:  90% 18/20 [00:02<00:00,  7.07it/s, loss=0.439, v_num=anbo, val/acc=0.746, val/loss=0.690]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:02<00:00,  7.29it/s, loss=0.439, v_num=anbo, val/acc=0.780, val/loss=0.617]\n","Epoch 4:  90% 18/20 [00:02<00:00,  7.16it/s, loss=0.299, v_num=anbo, val/acc=0.780, val/loss=0.617]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:02<00:00,  7.40it/s, loss=0.299, v_num=anbo, val/acc=0.780, val/loss=0.648]\n","Epoch 5:  90% 18/20 [00:02<00:00,  7.10it/s, loss=0.252, v_num=anbo, val/acc=0.780, val/loss=0.648]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:02<00:00,  7.35it/s, loss=0.252, v_num=anbo, val/acc=0.769, val/loss=0.642]\n","Epoch 6:  90% 18/20 [00:02<00:00,  7.11it/s, loss=0.231, v_num=anbo, val/acc=0.769, val/loss=0.642]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:02<00:00,  7.32it/s, loss=0.231, v_num=anbo, val/acc=0.772, val/loss=0.648]\n","Epoch 7:  90% 18/20 [00:02<00:00,  7.07it/s, loss=0.211, v_num=anbo, val/acc=0.772, val/loss=0.648]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:02<00:00,  7.29it/s, loss=0.211, v_num=anbo, val/acc=0.776, val/loss=0.632]\n","Epoch 8:  90% 18/20 [00:02<00:00,  7.12it/s, loss=0.21, v_num=anbo, val/acc=0.776, val/loss=0.632]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:02<00:00,  7.35it/s, loss=0.21, v_num=anbo, val/acc=0.776, val/loss=0.632]\n","Epoch 8: 100% 20/20 [00:02<00:00,  7.30it/s, loss=0.21, v_num=anbo, val/acc=0.780, val/loss=0.626]\n","Epoch 8: 100% 20/20 [00:02<00:00,  7.29it/s, loss=0.21, v_num=anbo, val/acc=0.780, val/loss=0.626]\n","[2021-06-27 02:27:17,327][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00, 12.14it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.8099173307418823, 'test/loss': 0.4920751452445984}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 630\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_022650-3uksanbo/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_022650-3uksanbo/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.20767\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 8\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 153\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 27\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624760837\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 18\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.77985\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.62644\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.80992\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.49208\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▄▄▄▄▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▅▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▃▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mbumbling-snowflake-40\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3uksanbo\u001b[0m\n","[2021-06-27 02:27:23,414][HYDRA] Launching 1 jobs locally\n","[2021-06-27 02:27:23,415][HYDRA] \t#9 : optimizer.lr=0.0003658078303203621 model.num_kernels=64 model.embedding_dim=128 model.dropout=0.5 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n","[2021-06-27 02:27:23,637][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 02:27:23,642][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 02:27:24.902001: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcopper-haze-41\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2jhgjru5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_022723-2jhgjru5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | CNN              | 641 K \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","641 K     Trainable params\n","0         Non-trainable params\n","641 K     Total params\n","2.566     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:01<00:00, 11.06it/s, loss=1.27, v_num=jru5]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:01<00:00, 11.85it/s, loss=1.27, v_num=jru5]\n","Epoch 0: 100% 20/20 [00:01<00:00, 11.86it/s, loss=1.27, v_num=jru5, val/acc=0.619, val/loss=0.953]\n","Epoch 1:  90% 18/20 [00:01<00:00, 11.84it/s, loss=1.1, v_num=jru5, val/acc=0.619, val/loss=0.953]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:01<00:00, 11.97it/s, loss=1.1, v_num=jru5, val/acc=0.735, val/loss=0.833]\n","Epoch 2:  90% 18/20 [00:01<00:00, 11.85it/s, loss=0.947, v_num=jru5, val/acc=0.735, val/loss=0.833]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:01<00:00, 11.97it/s, loss=0.947, v_num=jru5, val/acc=0.675, val/loss=0.775]\n","Epoch 3:  90% 18/20 [00:01<00:00, 11.38it/s, loss=0.856, v_num=jru5, val/acc=0.675, val/loss=0.775]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:01<00:00, 11.50it/s, loss=0.856, v_num=jru5, val/acc=0.780, val/loss=0.697]\n","Epoch 4:  90% 18/20 [00:01<00:00, 11.43it/s, loss=0.831, v_num=jru5, val/acc=0.780, val/loss=0.697]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:01<00:00, 11.60it/s, loss=0.831, v_num=jru5, val/acc=0.791, val/loss=0.692]\n","Epoch 5:  90% 18/20 [00:01<00:00, 11.88it/s, loss=0.774, v_num=jru5, val/acc=0.791, val/loss=0.692]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:01<00:00, 12.00it/s, loss=0.774, v_num=jru5, val/acc=0.795, val/loss=0.689]\n","Epoch 6:  90% 18/20 [00:01<00:00, 11.65it/s, loss=0.773, v_num=jru5, val/acc=0.795, val/loss=0.689]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:01<00:00, 11.87it/s, loss=0.773, v_num=jru5, val/acc=0.791, val/loss=0.688]\n","Epoch 7:  90% 18/20 [00:01<00:00, 11.76it/s, loss=0.781, v_num=jru5, val/acc=0.791, val/loss=0.688]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:01<00:00, 11.96it/s, loss=0.781, v_num=jru5, val/acc=0.795, val/loss=0.687]\n","Epoch 8:  90% 18/20 [00:01<00:00, 11.26it/s, loss=0.787, v_num=jru5, val/acc=0.795, val/loss=0.687]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:01<00:00, 11.32it/s, loss=0.787, v_num=jru5, val/acc=0.795, val/loss=0.686]\n","Epoch 9:  90% 18/20 [00:01<00:00, 11.44it/s, loss=0.783, v_num=jru5, val/acc=0.795, val/loss=0.686]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:01<00:00, 11.59it/s, loss=0.783, v_num=jru5, val/acc=0.795, val/loss=0.685]\n","Epoch 9: 100% 20/20 [00:01<00:00, 11.57it/s, loss=0.783, v_num=jru5, val/acc=0.795, val/loss=0.685]\n","[2021-06-27 02:27:43,102][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00, 15.18it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.8264462947845459, 'test/loss': 0.6218676567077637}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 676\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_022723-2jhgjru5/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_022723-2jhgjru5/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.78183\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624760863\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.79478\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.68459\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.82645\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.62187\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▅▃▂▂▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▃▃▃▄▄▅▅▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▃▃▃▄▄▅▅▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▆▃▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▅▃▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcopper-haze-41\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2jhgjru5\u001b[0m\n","[2021-06-27 02:27:48,815][HYDRA] Best parameters: {'optimizer.lr': 0.0021251467576750933, 'model.num_kernels': 128, 'model.embedding_dim': 256, 'model.dropout': 0.30000000000000004}\n","[2021-06-27 02:27:48,815][HYDRA] Best value: 0.8097015023231506\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HRHDx7xtT9Lw","executionInfo":{"status":"ok","timestamp":1624768924304,"user_tz":300,"elapsed":1285222,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"f2c66972-1697-4a08-bcd1-ec642c7097ac"},"source":["!python scripts/train.py -m model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna hydra.sweeper.n_trials=10"],"execution_count":15,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 2021-06-27 04:20:41,898]\u001b[0m A new study created in memory with name: no-name-89869a2c-3391-4804-878b-b7bcf0d92d9f\u001b[0m\n","[2021-06-27 04:20:41,898][HYDRA] Study name: no-name-89869a2c-3391-4804-878b-b7bcf0d92d9f\n","[2021-06-27 04:20:41,898][HYDRA] Storage: None\n","[2021-06-27 04:20:41,899][HYDRA] Sampler: TPESampler\n","[2021-06-27 04:20:41,899][HYDRA] Directions: ['maximize']\n","[2021-06-27 04:20:41,902][HYDRA] Launching 1 jobs locally\n","[2021-06-27 04:20:41,902][HYDRA] \t#0 : optimizer.lr=0.009303199318889763 model.embedding_dim=128 model.rnn_type=GRU model.rnn_hidden_dim=64 model.rnn_dropout=0.1 model.rnn_num_layers=4 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n","[2021-06-27 04:20:42,133][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 04:20:42,141][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkingyiusuen\u001b[0m (use `wandb login --relogin` to force relogin)\n","2021-06-27 04:20:43.662153: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mblooming-violet-58\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/tj01mav6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_042042-tj01mav6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | RNN              | 692 K \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","692 K     Trainable params\n","0         Non-trainable params\n","692 K     Total params\n","2.771     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:08<00:01,  1.97it/s, loss=1.13, v_num=mav6]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:08<00:00,  2.17it/s, loss=1.13, v_num=mav6]\n","Epoch 0: 100% 20/20 [00:08<00:00,  2.23it/s, loss=1.13, v_num=mav6, val/acc=0.448, val/loss=1.050]\n","Epoch 1:  90% 18/20 [00:08<00:00,  2.10it/s, loss=1.04, v_num=mav6, val/acc=0.448, val/loss=1.050]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.13it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:08<00:00,  2.25it/s, loss=1.04, v_num=mav6, val/acc=0.504, val/loss=1.020]\n","Epoch 2:  90% 18/20 [00:08<00:00,  2.08it/s, loss=0.937, v_num=mav6, val/acc=0.504, val/loss=1.020]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.46it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:08<00:00,  2.23it/s, loss=0.937, v_num=mav6, val/acc=0.623, val/loss=0.875]\n","Epoch 3:  90% 18/20 [00:08<00:00,  2.11it/s, loss=0.761, v_num=mav6, val/acc=0.623, val/loss=0.875]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.32it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:08<00:00,  2.26it/s, loss=0.761, v_num=mav6, val/acc=0.623, val/loss=0.792]\n","Epoch 4:  90% 18/20 [00:08<00:00,  2.10it/s, loss=0.586, v_num=mav6, val/acc=0.623, val/loss=0.792]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  7.28it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:08<00:00,  2.24it/s, loss=0.586, v_num=mav6, val/acc=0.672, val/loss=0.762]\n","Epoch 5:  90% 18/20 [00:08<00:00,  2.10it/s, loss=0.458, v_num=mav6, val/acc=0.672, val/loss=0.762]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  7.72it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:08<00:00,  2.25it/s, loss=0.458, v_num=mav6, val/acc=0.690, val/loss=0.777]\n","Epoch 6:  90% 18/20 [00:08<00:00,  2.11it/s, loss=0.412, v_num=mav6, val/acc=0.690, val/loss=0.777]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.28it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:08<00:00,  2.27it/s, loss=0.412, v_num=mav6, val/acc=0.705, val/loss=0.754]\n","Epoch 7:  90% 18/20 [00:08<00:00,  2.11it/s, loss=0.346, v_num=mav6, val/acc=0.705, val/loss=0.754]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.16it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:08<00:00,  2.26it/s, loss=0.346, v_num=mav6, val/acc=0.713, val/loss=0.756]\n","Epoch 8:  90% 18/20 [00:08<00:00,  2.11it/s, loss=0.334, v_num=mav6, val/acc=0.713, val/loss=0.756]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  7.57it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:08<00:00,  2.25it/s, loss=0.334, v_num=mav6, val/acc=0.701, val/loss=0.757]\n","Epoch 9:  90% 18/20 [00:08<00:00,  2.08it/s, loss=0.329, v_num=mav6, val/acc=0.701, val/loss=0.757]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  7.48it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:08<00:00,  2.23it/s, loss=0.329, v_num=mav6, val/acc=0.709, val/loss=0.761]\n","Epoch 9: 100% 20/20 [00:08<00:00,  2.23it/s, loss=0.329, v_num=mav6, val/acc=0.709, val/loss=0.761]\n","[2021-06-27 04:22:16,254][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00,  8.41it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.7190082669258118, 'test/loss': 0.7262842059135437}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3488\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_042042-tj01mav6/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_042042-tj01mav6/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.32834\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 94\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624767736\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.70896\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.76103\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.71901\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.72628\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▆▅▃▂▂▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▂▆▆▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▇▄▂▁▂▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mblooming-violet-58\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/tj01mav6\u001b[0m\n","[2021-06-27 04:22:21,544][HYDRA] Launching 1 jobs locally\n","[2021-06-27 04:22:21,544][HYDRA] \t#1 : optimizer.lr=0.005720477787908698 model.embedding_dim=128 model.rnn_type=GRU model.rnn_hidden_dim=256 model.rnn_dropout=0.30000000000000004 model.rnn_num_layers=4 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n","[2021-06-27 04:22:21,770][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 04:22:21,775][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 04:22:23.050769: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvital-galaxy-59\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2xqdfmhq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_042221-2xqdfmhq\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | RNN              | 4.5 M \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","4.5 M     Trainable params\n","0         Non-trainable params\n","4.5 M     Total params\n","18.148    Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:17<00:03,  1.06s/it, loss=1.49, v_num=fmhq]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:18<00:00,  1.04it/s, loss=1.49, v_num=fmhq]\n","Validating:  67% 2/3 [00:00<00:00,  3.38it/s]\u001b[A\n","Epoch 0: 100% 20/20 [00:18<00:00,  1.07it/s, loss=1.49, v_num=fmhq, val/acc=0.310, val/loss=1.190]\n","Epoch 1:  90% 18/20 [00:17<00:01,  1.01it/s, loss=1.14, v_num=fmhq, val/acc=0.310, val/loss=1.190]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.49it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.14, v_num=fmhq, val/acc=0.310, val/loss=1.190]\n","Epoch 1: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.14, v_num=fmhq, val/acc=0.336, val/loss=1.130]\n","Epoch 2:  90% 18/20 [00:17<00:01,  1.00it/s, loss=1.12, v_num=fmhq, val/acc=0.336, val/loss=1.130]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.59it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.12, v_num=fmhq, val/acc=0.336, val/loss=1.130]\n","Epoch 2: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.12, v_num=fmhq, val/acc=0.381, val/loss=1.130]\n","Epoch 3:  90% 18/20 [00:17<00:01,  1.00it/s, loss=1.11, v_num=fmhq, val/acc=0.381, val/loss=1.130]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.49it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.11, v_num=fmhq, val/acc=0.381, val/loss=1.130]\n","Epoch 3: 100% 20/20 [00:18<00:00,  1.07it/s, loss=1.11, v_num=fmhq, val/acc=0.381, val/loss=1.100]\n","Epoch 4:  90% 18/20 [00:17<00:01,  1.00it/s, loss=1.09, v_num=fmhq, val/acc=0.381, val/loss=1.100]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.44it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.09, v_num=fmhq, val/acc=0.381, val/loss=1.100]\n","Epoch 4: 100% 20/20 [00:18<00:00,  1.07it/s, loss=1.09, v_num=fmhq, val/acc=0.358, val/loss=1.080]\n","Epoch 5:  90% 18/20 [00:17<00:01,  1.01it/s, loss=1.06, v_num=fmhq, val/acc=0.358, val/loss=1.080]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.59it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.06, v_num=fmhq, val/acc=0.358, val/loss=1.080]\n","Epoch 5: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.06, v_num=fmhq, val/acc=0.396, val/loss=1.070]\n","Epoch 6:  90% 18/20 [00:17<00:01,  1.01it/s, loss=1.05, v_num=fmhq, val/acc=0.396, val/loss=1.070]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.59it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:18<00:00,  1.09it/s, loss=1.05, v_num=fmhq, val/acc=0.396, val/loss=1.070]\n","Epoch 6: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.05, v_num=fmhq, val/acc=0.396, val/loss=1.060]\n","Epoch 7:  90% 18/20 [00:17<00:01,  1.01it/s, loss=1.05, v_num=fmhq, val/acc=0.396, val/loss=1.060]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.59it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.05, v_num=fmhq, val/acc=0.396, val/loss=1.060]\n","Epoch 7: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.05, v_num=fmhq, val/acc=0.388, val/loss=1.050]\n","Epoch 8:  90% 18/20 [00:17<00:01,  1.00it/s, loss=1.04, v_num=fmhq, val/acc=0.388, val/loss=1.050]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.58it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.04, v_num=fmhq, val/acc=0.388, val/loss=1.050]\n","Epoch 8: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.04, v_num=fmhq, val/acc=0.399, val/loss=1.050]\n","Epoch 9:  90% 18/20 [00:17<00:01,  1.01it/s, loss=1.04, v_num=fmhq, val/acc=0.399, val/loss=1.050]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.50it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.04, v_num=fmhq, val/acc=0.399, val/loss=1.050]\n","Epoch 9: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.04, v_num=fmhq, val/acc=0.381, val/loss=1.050]\n","Epoch 9: 100% 20/20 [00:18<00:00,  1.08it/s, loss=1.04, v_num=fmhq, val/acc=0.381, val/loss=1.050]\n","[2021-06-27 04:25:30,455][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00,  3.53it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.38842976093292236, 'test/loss': 1.0831655263900757}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3534\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_042221-2xqdfmhq/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_042221-2xqdfmhq/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 1.03926\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 190\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624767931\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.3806\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 1.05298\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.38843\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 1.08317\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▂▂▂▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▃▇▇▅██▇█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▅▅▃▂▂▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mvital-galaxy-59\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2xqdfmhq\u001b[0m\n","[2021-06-27 04:25:37,043][HYDRA] Launching 1 jobs locally\n","[2021-06-27 04:25:37,043][HYDRA] \t#2 : optimizer.lr=0.007514175711585728 model.embedding_dim=256 model.rnn_type=RNN model.rnn_hidden_dim=256 model.rnn_dropout=0.5 model.rnn_num_layers=3 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n","[2021-06-27 04:25:37,275][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 04:25:37,281][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 04:25:38.530693: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrobust-valley-60\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3i8q66yv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_042537-3i8q66yv\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | RNN              | 1.8 M \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","1.8 M     Trainable params\n","0         Non-trainable params\n","1.8 M     Total params\n","7.369     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:09<00:01,  1.82it/s, loss=1.29, v_num=66yv]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:09<00:00,  2.00it/s, loss=1.29, v_num=66yv]\n","Epoch 0: 100% 20/20 [00:09<00:00,  2.06it/s, loss=1.29, v_num=66yv, val/acc=0.373, val/loss=1.180]\n","Epoch 1:  90% 18/20 [00:09<00:01,  1.93it/s, loss=1.13, v_num=66yv, val/acc=0.373, val/loss=1.180]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  5.75it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:09<00:00,  2.06it/s, loss=1.13, v_num=66yv, val/acc=0.437, val/loss=1.050]\n","Epoch 2:  90% 18/20 [00:09<00:01,  1.93it/s, loss=1.08, v_num=66yv, val/acc=0.437, val/loss=1.050]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  5.98it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:09<00:00,  2.06it/s, loss=1.08, v_num=66yv, val/acc=0.381, val/loss=1.120]\n","Epoch 3:  90% 18/20 [00:09<00:01,  1.93it/s, loss=1.09, v_num=66yv, val/acc=0.381, val/loss=1.120]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  5.92it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:09<00:00,  2.05it/s, loss=1.09, v_num=66yv, val/acc=0.448, val/loss=1.050]\n","Epoch 4:  90% 18/20 [00:09<00:01,  1.93it/s, loss=1.02, v_num=66yv, val/acc=0.448, val/loss=1.050]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  5.86it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:09<00:00,  2.06it/s, loss=1.02, v_num=66yv, val/acc=0.470, val/loss=0.998]\n","Epoch 5:  90% 18/20 [00:09<00:01,  1.92it/s, loss=0.97, v_num=66yv, val/acc=0.470, val/loss=0.998]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  5.93it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:09<00:00,  2.05it/s, loss=0.97, v_num=66yv, val/acc=0.481, val/loss=0.985]\n","Epoch 6:  90% 18/20 [00:09<00:01,  1.92it/s, loss=0.953, v_num=66yv, val/acc=0.481, val/loss=0.985]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  6.19it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:09<00:00,  2.05it/s, loss=0.953, v_num=66yv, val/acc=0.448, val/loss=0.983]\n","Epoch 7:  90% 18/20 [00:09<00:01,  1.94it/s, loss=0.942, v_num=66yv, val/acc=0.448, val/loss=0.983]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  5.99it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:09<00:00,  2.07it/s, loss=0.942, v_num=66yv, val/acc=0.448, val/loss=0.983]\n","Epoch 8:  90% 18/20 [00:09<00:01,  1.93it/s, loss=0.94, v_num=66yv, val/acc=0.448, val/loss=0.983]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  5.61it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:09<00:00,  2.05it/s, loss=0.94, v_num=66yv, val/acc=0.474, val/loss=0.980]\n","Epoch 9:  90% 18/20 [00:09<00:01,  1.93it/s, loss=0.927, v_num=66yv, val/acc=0.474, val/loss=0.980]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  5.90it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:09<00:00,  2.06it/s, loss=0.927, v_num=66yv, val/acc=0.481, val/loss=0.980]\n","Epoch 9: 100% 20/20 [00:09<00:00,  2.06it/s, loss=0.927, v_num=66yv, val/acc=0.481, val/loss=0.980]\n","[2021-06-27 04:27:17,143][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00,  6.24it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.39256197214126587, 'test/loss': 1.1038929224014282}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3580\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_042537-3i8q66yv/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_042537-3i8q66yv/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.92762\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 100\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624768037\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.48134\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.9801\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.39256\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 1.10389\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▅▄▄▂▂▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▅▁▆▇█▆▆██\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▃▆▃▂▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mrobust-valley-60\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3i8q66yv\u001b[0m\n","[2021-06-27 04:27:23,508][HYDRA] Launching 1 jobs locally\n","[2021-06-27 04:27:23,508][HYDRA] \t#3 : optimizer.lr=0.002985873632533843 model.embedding_dim=256 model.rnn_type=LSTM model.rnn_hidden_dim=128 model.rnn_dropout=0.5 model.rnn_num_layers=5 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n","[2021-06-27 04:27:23,736][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 04:27:23,741][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 04:27:24.986166: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwinter-grass-61\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/m13836ag\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_042723-m13836ag\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | RNN              | 2.8 M \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","2.8 M     Trainable params\n","0         Non-trainable params\n","2.8 M     Total params\n","11.064    Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:15<00:02,  1.07it/s, loss=1.09, v_num=36ag]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:16<00:00,  1.18it/s, loss=1.09, v_num=36ag]\n","Epoch 0: 100% 20/20 [00:16<00:00,  1.21it/s, loss=1.09, v_num=36ag, val/acc=0.399, val/loss=1.070]\n","Epoch 1:  90% 18/20 [00:15<00:01,  1.13it/s, loss=1.05, v_num=36ag, val/acc=0.399, val/loss=1.070]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.58it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:16<00:00,  1.21it/s, loss=1.05, v_num=36ag, val/acc=0.537, val/loss=0.966]\n","Epoch 2:  90% 18/20 [00:15<00:01,  1.14it/s, loss=1.07, v_num=36ag, val/acc=0.537, val/loss=0.966]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.67it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:16<00:00,  1.21it/s, loss=1.07, v_num=36ag, val/acc=0.403, val/loss=1.120]\n","Epoch 3:  90% 18/20 [00:15<00:01,  1.13it/s, loss=1.09, v_num=36ag, val/acc=0.403, val/loss=1.120]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.63it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:16<00:00,  1.21it/s, loss=1.09, v_num=36ag, val/acc=0.381, val/loss=1.100]\n","Epoch 4:  90% 18/20 [00:15<00:01,  1.14it/s, loss=1.07, v_num=36ag, val/acc=0.381, val/loss=1.100]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.66it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:16<00:00,  1.21it/s, loss=1.07, v_num=36ag, val/acc=0.418, val/loss=1.080]\n","Epoch 5:  90% 18/20 [00:15<00:01,  1.13it/s, loss=1.03, v_num=36ag, val/acc=0.418, val/loss=1.080]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.73it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:16<00:00,  1.21it/s, loss=1.03, v_num=36ag, val/acc=0.470, val/loss=0.990]\n","Epoch 6:  90% 18/20 [00:15<00:01,  1.14it/s, loss=0.984, v_num=36ag, val/acc=0.470, val/loss=0.990]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.49it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:16<00:00,  1.21it/s, loss=0.984, v_num=36ag, val/acc=0.500, val/loss=0.969]\n","Epoch 6: 100% 20/20 [00:16<00:00,  1.21it/s, loss=0.984, v_num=36ag, val/acc=0.500, val/loss=0.969]\n","[2021-06-27 04:29:21,846][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00,  3.69it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.5454545617103577, 'test/loss': 0.9539663791656494}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3624\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_042723-m13836ag/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_042723-m13836ag/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.97562\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 6\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 119\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 119\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624768162\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 14\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.96892\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.54545\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.95397\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▅▇█▆▄▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▄▄▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▄▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▄▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▂█▂▁▃▅▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss ▆▁█▇▆▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mwinter-grass-61\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/m13836ag\u001b[0m\n","[2021-06-27 04:29:27,984][HYDRA] Launching 1 jobs locally\n","[2021-06-27 04:29:27,984][HYDRA] \t#4 : optimizer.lr=0.00841520059603814 model.embedding_dim=64 model.rnn_type=RNN model.rnn_hidden_dim=64 model.rnn_dropout=0.2 model.rnn_num_layers=5 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n","[2021-06-27 04:29:28,213][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 04:29:28,218][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 04:29:29.457057: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33measy-spaceship-62\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/30hictuv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_042928-30hictuv\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | RNN              | 313 K \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","313 K     Trainable params\n","0         Non-trainable params\n","313 K     Total params\n","1.254     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:10<00:01,  1.69it/s, loss=1.13, v_num=ctuv]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:10<00:00,  1.86it/s, loss=1.13, v_num=ctuv]\n","Epoch 0: 100% 20/20 [00:10<00:00,  1.91it/s, loss=1.13, v_num=ctuv, val/acc=0.388, val/loss=1.070]\n","Epoch 1:  90% 18/20 [00:10<00:01,  1.78it/s, loss=1.07, v_num=ctuv, val/acc=0.388, val/loss=1.070]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  6.28it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:10<00:00,  1.91it/s, loss=1.07, v_num=ctuv, val/acc=0.444, val/loss=1.060]\n","Epoch 2:  90% 18/20 [00:10<00:01,  1.78it/s, loss=1.05, v_num=ctuv, val/acc=0.444, val/loss=1.060]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  7.12it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:10<00:00,  1.90it/s, loss=1.05, v_num=ctuv, val/acc=0.440, val/loss=1.070]\n","Epoch 3:  90% 18/20 [00:10<00:01,  1.77it/s, loss=1.04, v_num=ctuv, val/acc=0.440, val/loss=1.070]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  6.73it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:10<00:00,  1.90it/s, loss=1.04, v_num=ctuv, val/acc=0.459, val/loss=1.040]\n","Epoch 4:  90% 18/20 [00:10<00:01,  1.77it/s, loss=1, v_num=ctuv, val/acc=0.459, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  7.06it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:10<00:00,  1.92it/s, loss=1, v_num=ctuv, val/acc=0.459, val/loss=1.040]\n","Epoch 4: 100% 20/20 [00:10<00:00,  1.90it/s, loss=1, v_num=ctuv, val/acc=0.500, val/loss=1.020]\n","Epoch 5:  90% 18/20 [00:10<00:01,  1.77it/s, loss=0.958, v_num=ctuv, val/acc=0.500, val/loss=1.020]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  6.44it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:10<00:00,  1.89it/s, loss=0.958, v_num=ctuv, val/acc=0.455, val/loss=1.020]\n","Epoch 6:  90% 18/20 [00:10<00:01,  1.78it/s, loss=0.938, v_num=ctuv, val/acc=0.455, val/loss=1.020]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  6.97it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:10<00:00,  1.90it/s, loss=0.938, v_num=ctuv, val/acc=0.481, val/loss=1.030]\n","Epoch 7:  90% 18/20 [00:10<00:01,  1.78it/s, loss=0.923, v_num=ctuv, val/acc=0.481, val/loss=1.030]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  7.12it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:10<00:00,  1.91it/s, loss=0.923, v_num=ctuv, val/acc=0.474, val/loss=1.020]\n","Epoch 8:  90% 18/20 [00:10<00:01,  1.79it/s, loss=0.926, v_num=ctuv, val/acc=0.474, val/loss=1.020]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  6.23it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:10<00:00,  1.91it/s, loss=0.926, v_num=ctuv, val/acc=0.470, val/loss=1.030]\n","Epoch 9:  90% 18/20 [00:10<00:01,  1.79it/s, loss=0.913, v_num=ctuv, val/acc=0.470, val/loss=1.030]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  6.81it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:10<00:00,  1.91it/s, loss=0.913, v_num=ctuv, val/acc=0.470, val/loss=1.030]\n","Epoch 9: 100% 20/20 [00:10<00:00,  1.91it/s, loss=0.913, v_num=ctuv, val/acc=0.470, val/loss=1.030]\n","[2021-06-27 04:31:15,701][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00,  6.82it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.44214877486228943, 'test/loss': 1.1120139360427856}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3668\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_042928-30hictuv/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_042928-30hictuv/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.91295\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 108\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624768276\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.47015\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 1.02838\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.44215\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 1.11201\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▆▅▅▄▂▂▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▄▄▅█▅▇▆▆▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▆█▃▁▂▂▂▂▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33measy-spaceship-62\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/30hictuv\u001b[0m\n","[2021-06-27 04:31:21,435][HYDRA] Launching 1 jobs locally\n","[2021-06-27 04:31:21,435][HYDRA] \t#5 : optimizer.lr=0.0013802142390900991 model.embedding_dim=256 model.rnn_type=GRU model.rnn_hidden_dim=128 model.rnn_dropout=0.4 model.rnn_num_layers=5 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n","[2021-06-27 04:31:21,668][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 04:31:21,673][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 04:31:22.927325: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtreasured-leaf-63\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1ufl63j1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_043121-1ufl63j1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | RNN              | 2.3 M \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","2.3 M     Trainable params\n","0         Non-trainable params\n","2.3 M     Total params\n","9.088     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:14<00:02,  1.15it/s, loss=1.09, v_num=63j1]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:15<00:00,  1.27it/s, loss=1.09, v_num=63j1]\n","Validating:  67% 2/3 [00:00<00:00,  3.85it/s]\u001b[A\n","Epoch 0: 100% 20/20 [00:15<00:00,  1.30it/s, loss=1.09, v_num=63j1, val/acc=0.451, val/loss=1.060]\n","Epoch 1:  90% 18/20 [00:14<00:01,  1.22it/s, loss=1.04, v_num=63j1, val/acc=0.451, val/loss=1.060]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.71it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:15<00:00,  1.30it/s, loss=1.04, v_num=63j1, val/acc=0.440, val/loss=1.050]\n","Epoch 2:  90% 18/20 [00:14<00:01,  1.22it/s, loss=1.01, v_num=63j1, val/acc=0.440, val/loss=1.050]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.88it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:15<00:00,  1.30it/s, loss=1.01, v_num=63j1, val/acc=0.474, val/loss=1.060]\n","Epoch 3:  90% 18/20 [00:14<00:01,  1.22it/s, loss=0.947, v_num=63j1, val/acc=0.474, val/loss=1.060]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.92it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:15<00:00,  1.30it/s, loss=0.947, v_num=63j1, val/acc=0.478, val/loss=1.040]\n","Epoch 4:  90% 18/20 [00:14<00:01,  1.22it/s, loss=0.891, v_num=63j1, val/acc=0.478, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.78it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:15<00:00,  1.30it/s, loss=0.891, v_num=63j1, val/acc=0.537, val/loss=1.000]\n","Epoch 5:  90% 18/20 [00:14<00:01,  1.22it/s, loss=0.832, v_num=63j1, val/acc=0.537, val/loss=1.000]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  4.00it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.832, v_num=63j1, val/acc=0.575, val/loss=0.994]\n","Epoch 6:  90% 18/20 [00:14<00:01,  1.22it/s, loss=0.725, v_num=63j1, val/acc=0.575, val/loss=0.994]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.92it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:15<00:00,  1.30it/s, loss=0.725, v_num=63j1, val/acc=0.608, val/loss=0.932]\n","Epoch 7:  90% 18/20 [00:14<00:01,  1.22it/s, loss=0.658, v_num=63j1, val/acc=0.608, val/loss=0.932]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.88it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:15<00:00,  1.30it/s, loss=0.658, v_num=63j1, val/acc=0.597, val/loss=0.945]\n","Epoch 8:  90% 18/20 [00:14<00:01,  1.22it/s, loss=0.634, v_num=63j1, val/acc=0.597, val/loss=0.945]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.84it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.634, v_num=63j1, val/acc=0.604, val/loss=0.942]\n","Epoch 9:  90% 18/20 [00:14<00:01,  1.22it/s, loss=0.607, v_num=63j1, val/acc=0.604, val/loss=0.942]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  3.99it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:15<00:00,  1.30it/s, loss=0.607, v_num=63j1, val/acc=0.604, val/loss=0.937]\n","Epoch 9: 100% 20/20 [00:15<00:00,  1.30it/s, loss=0.607, v_num=63j1, val/acc=0.604, val/loss=0.937]\n","[2021-06-27 04:33:57,776][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00,  3.86it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.6033057570457458, 'test/loss': 0.9239904284477234}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3712\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_043121-1ufl63j1/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_043121-1ufl63j1/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.60182\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 157\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624768438\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.60448\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.93707\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.60331\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.92399\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▇▆▅▄▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▁▂▃▅▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss ███▇▅▄▁▂▂▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtreasured-leaf-63\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1ufl63j1\u001b[0m\n","[2021-06-27 04:34:03,928][HYDRA] Launching 1 jobs locally\n","[2021-06-27 04:34:03,928][HYDRA] \t#6 : optimizer.lr=0.0037006977384225113 model.embedding_dim=256 model.rnn_type=RNN model.rnn_hidden_dim=128 model.rnn_dropout=0.5 model.rnn_num_layers=2 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n","[2021-06-27 04:34:04,155][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 04:34:04,164][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 04:34:05.424554: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfancy-forest-64\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/287k5qse\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_043404-287k5qse\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | RNN              | 987 K \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","987 K     Trainable params\n","0         Non-trainable params\n","987 K     Total params\n","3.950     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:04<00:00,  3.95it/s, loss=1.11, v_num=5qse]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:04<00:00,  4.30it/s, loss=1.11, v_num=5qse]\n","Epoch 0: 100% 20/20 [00:04<00:00,  4.39it/s, loss=1.11, v_num=5qse, val/acc=0.444, val/loss=1.050]\n","Epoch 1:  90% 18/20 [00:04<00:00,  4.23it/s, loss=1.04, v_num=5qse, val/acc=0.444, val/loss=1.050]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:04<00:00,  4.46it/s, loss=1.04, v_num=5qse, val/acc=0.481, val/loss=1.040]\n","Epoch 2:  90% 18/20 [00:04<00:00,  4.27it/s, loss=1.01, v_num=5qse, val/acc=0.481, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:04<00:00,  4.54it/s, loss=1.01, v_num=5qse, val/acc=0.481, val/loss=1.040]\n","Epoch 2: 100% 20/20 [00:04<00:00,  4.50it/s, loss=1.01, v_num=5qse, val/acc=0.451, val/loss=1.040]\n","Epoch 3:  90% 18/20 [00:04<00:00,  4.19it/s, loss=0.985, v_num=5qse, val/acc=0.451, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:04<00:00,  4.43it/s, loss=0.985, v_num=5qse, val/acc=0.433, val/loss=1.040]\n","Epoch 4:  90% 18/20 [00:04<00:00,  4.20it/s, loss=0.963, v_num=5qse, val/acc=0.433, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:04<00:00,  4.46it/s, loss=0.963, v_num=5qse, val/acc=0.433, val/loss=1.040]\n","Epoch 4: 100% 20/20 [00:04<00:00,  4.42it/s, loss=0.963, v_num=5qse, val/acc=0.466, val/loss=1.030]\n","Epoch 5:  90% 18/20 [00:04<00:00,  4.18it/s, loss=0.942, v_num=5qse, val/acc=0.466, val/loss=1.030]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:04<00:00,  4.43it/s, loss=0.942, v_num=5qse, val/acc=0.437, val/loss=1.040]\n","Epoch 6:  90% 18/20 [00:04<00:00,  4.23it/s, loss=0.928, v_num=5qse, val/acc=0.437, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:04<00:00,  4.48it/s, loss=0.928, v_num=5qse, val/acc=0.463, val/loss=1.040]\n","Epoch 6: 100% 20/20 [00:04<00:00,  4.48it/s, loss=0.928, v_num=5qse, val/acc=0.463, val/loss=1.040]\n","[2021-06-27 04:34:38,134][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00, 10.07it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.39256197214126587, 'test/loss': 1.1378651857376099}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3762\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_043404-287k5qse/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_043404-287k5qse/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.9341\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 6\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 119\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 34\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624768478\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 14\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.46269\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 1.04107\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.39256\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 1.13787\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▅▄▃▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▅▅▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▄▄▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▅▅▅▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▅▅▅▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▃█▄▁▆▂▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▃▅▆▁▃▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfancy-forest-64\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/287k5qse\u001b[0m\n","[2021-06-27 04:34:43,551][HYDRA] Launching 1 jobs locally\n","[2021-06-27 04:34:43,551][HYDRA] \t#7 : optimizer.lr=0.009004712334784108 model.embedding_dim=64 model.rnn_type=LSTM model.rnn_hidden_dim=64 model.rnn_dropout=0.5 model.rnn_num_layers=3 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n","[2021-06-27 04:34:43,780][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 04:34:43,785][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 04:34:45.031134: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33methereal-pyramid-65\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/q1sz1ddr\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_043443-q1sz1ddr\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | RNN              | 462 K \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","462 K     Trainable params\n","0         Non-trainable params\n","462 K     Total params\n","1.851     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:06<00:01,  2.65it/s, loss=1.07, v_num=1ddr]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:06<00:00,  2.91it/s, loss=1.07, v_num=1ddr]\n","Epoch 0: 100% 20/20 [00:06<00:00,  2.98it/s, loss=1.07, v_num=1ddr, val/acc=0.444, val/loss=1.040]\n","Epoch 1:  90% 18/20 [00:06<00:00,  2.81it/s, loss=1.03, v_num=1ddr, val/acc=0.444, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  9.75it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:06<00:00,  2.99it/s, loss=1.03, v_num=1ddr, val/acc=0.440, val/loss=1.040]\n","Epoch 2:  90% 18/20 [00:06<00:00,  2.79it/s, loss=1.02, v_num=1ddr, val/acc=0.440, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.82it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:06<00:00,  2.97it/s, loss=1.02, v_num=1ddr, val/acc=0.440, val/loss=1.040]\n","Epoch 3:  90% 18/20 [00:06<00:00,  2.80it/s, loss=0.957, v_num=1ddr, val/acc=0.440, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.59it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:06<00:00,  2.98it/s, loss=0.957, v_num=1ddr, val/acc=0.575, val/loss=0.957]\n","Epoch 4:  90% 18/20 [00:06<00:00,  2.78it/s, loss=0.913, v_num=1ddr, val/acc=0.575, val/loss=0.957]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  9.40it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:06<00:00,  2.96it/s, loss=0.913, v_num=1ddr, val/acc=0.627, val/loss=0.875]\n","Epoch 5:  90% 18/20 [00:06<00:00,  2.79it/s, loss=0.835, v_num=1ddr, val/acc=0.627, val/loss=0.875]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.01it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:06<00:00,  2.96it/s, loss=0.835, v_num=1ddr, val/acc=0.597, val/loss=0.872]\n","Epoch 6:  90% 18/20 [00:06<00:00,  2.79it/s, loss=0.784, v_num=1ddr, val/acc=0.597, val/loss=0.872]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.87it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:06<00:00,  2.97it/s, loss=0.784, v_num=1ddr, val/acc=0.616, val/loss=0.869]\n","Epoch 7:  90% 18/20 [00:06<00:00,  2.80it/s, loss=0.764, v_num=1ddr, val/acc=0.616, val/loss=0.869]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  9.57it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:06<00:00,  2.98it/s, loss=0.764, v_num=1ddr, val/acc=0.604, val/loss=0.876]\n","Epoch 8:  90% 18/20 [00:06<00:00,  2.80it/s, loss=0.753, v_num=1ddr, val/acc=0.604, val/loss=0.876]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.61it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:06<00:00,  2.97it/s, loss=0.753, v_num=1ddr, val/acc=0.627, val/loss=0.870]\n","Epoch 9:  90% 18/20 [00:06<00:00,  2.81it/s, loss=0.741, v_num=1ddr, val/acc=0.627, val/loss=0.870]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  9.41it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:06<00:00,  2.99it/s, loss=0.741, v_num=1ddr, val/acc=0.623, val/loss=0.869]\n","Epoch 9: 100% 20/20 [00:06<00:00,  2.99it/s, loss=0.741, v_num=1ddr, val/acc=0.623, val/loss=0.869]\n","[2021-06-27 04:35:53,429][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00,  8.82it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.6611570119857788, 'test/loss': 0.7846356630325317}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3806\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_043443-q1sz1ddr/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_043443-q1sz1ddr/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.73501\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 70\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624768553\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.62313\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.8692\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.66116\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.78464\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▇▆▅▃▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▁▁▆█▇█▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss ███▅▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33methereal-pyramid-65\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/q1sz1ddr\u001b[0m\n","[2021-06-27 04:35:59,008][HYDRA] Launching 1 jobs locally\n","[2021-06-27 04:35:59,008][HYDRA] \t#8 : optimizer.lr=0.00896138632773382 model.embedding_dim=256 model.rnn_type=RNN model.rnn_hidden_dim=64 model.rnn_dropout=0.2 model.rnn_num_layers=4 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n","[2021-06-27 04:35:59,241][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 04:35:59,247][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 04:36:00.488776: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrich-glitter-66\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/30exppwn\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_043559-30exppwn\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | RNN              | 905 K \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","905 K     Trainable params\n","0         Non-trainable params\n","905 K     Total params\n","3.620     Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:07<00:01,  2.14it/s, loss=1.11, v_num=ppwn]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:08<00:00,  2.34it/s, loss=1.11, v_num=ppwn]\n","Epoch 0: 100% 20/20 [00:08<00:00,  2.40it/s, loss=1.11, v_num=ppwn, val/acc=0.392, val/loss=1.060]\n","Epoch 1:  90% 18/20 [00:07<00:00,  2.26it/s, loss=1.07, v_num=ppwn, val/acc=0.392, val/loss=1.060]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.07it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:08<00:00,  2.42it/s, loss=1.07, v_num=ppwn, val/acc=0.444, val/loss=1.040]\n","Epoch 2:  90% 18/20 [00:07<00:00,  2.27it/s, loss=1.03, v_num=ppwn, val/acc=0.444, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.00it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:08<00:00,  2.42it/s, loss=1.03, v_num=ppwn, val/acc=0.470, val/loss=1.040]\n","Epoch 3:  90% 18/20 [00:07<00:00,  2.27it/s, loss=1, v_num=ppwn, val/acc=0.470, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  7.57it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:08<00:00,  2.41it/s, loss=1, v_num=ppwn, val/acc=0.466, val/loss=1.050]\n","Epoch 4:  90% 18/20 [00:07<00:00,  2.26it/s, loss=0.953, v_num=ppwn, val/acc=0.466, val/loss=1.050]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  7.31it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:08<00:00,  2.40it/s, loss=0.953, v_num=ppwn, val/acc=0.463, val/loss=1.040]\n","Epoch 5:  90% 18/20 [00:07<00:00,  2.26it/s, loss=0.926, v_num=ppwn, val/acc=0.463, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.24it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:08<00:00,  2.42it/s, loss=0.926, v_num=ppwn, val/acc=0.455, val/loss=1.020]\n","Epoch 6:  90% 18/20 [00:07<00:00,  2.26it/s, loss=0.905, v_num=ppwn, val/acc=0.455, val/loss=1.020]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.20it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:08<00:00,  2.42it/s, loss=0.905, v_num=ppwn, val/acc=0.474, val/loss=1.030]\n","Epoch 7:  90% 18/20 [00:08<00:00,  2.25it/s, loss=0.877, v_num=ppwn, val/acc=0.474, val/loss=1.030]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.08it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:08<00:00,  2.40it/s, loss=0.877, v_num=ppwn, val/acc=0.466, val/loss=1.030]\n","Epoch 8:  90% 18/20 [00:07<00:00,  2.26it/s, loss=0.882, v_num=ppwn, val/acc=0.466, val/loss=1.030]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  7.99it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:08<00:00,  2.41it/s, loss=0.882, v_num=ppwn, val/acc=0.466, val/loss=1.030]\n","Epoch 9:  90% 18/20 [00:07<00:00,  2.27it/s, loss=0.892, v_num=ppwn, val/acc=0.466, val/loss=1.030]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  8.19it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:08<00:00,  2.43it/s, loss=0.892, v_num=ppwn, val/acc=0.481, val/loss=1.030]\n","Epoch 9: 100% 20/20 [00:08<00:00,  2.42it/s, loss=0.892, v_num=ppwn, val/acc=0.481, val/loss=1.030]\n","[2021-06-27 04:37:24,667][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00,  7.64it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.42561984062194824, 'test/loss': 1.1458896398544312}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3852\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_043559-30exppwn/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_043559-30exppwn/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.8859\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 85\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624768644\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.48134\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 1.03494\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.42562\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 1.14589\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▆▅▄▃▂▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▅▇▇▇▆▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▃▄▆▃▁▂▂▃▃\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mrich-glitter-66\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/30exppwn\u001b[0m\n","[2021-06-27 04:37:30,449][HYDRA] Launching 1 jobs locally\n","[2021-06-27 04:37:30,449][HYDRA] \t#9 : optimizer.lr=0.008120875147152222 model.embedding_dim=128 model.rnn_type=LSTM model.rnn_hidden_dim=256 model.rnn_dropout=0.30000000000000004 model.rnn_num_layers=5 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n","[2021-06-27 04:37:30,856][reddit_post_classification.utils.python_logger][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n","[2021-06-27 04:37:30,862][reddit_post_classification.data.reddit_datamodule][INFO] - Loading train and val data...\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","2021-06-27 04:37:32.151828: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33matomic-elevator-67\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3af8xf8o\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210627_043730-3af8xf8o\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n","\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name    | Type             | Params\n","---------------------------------------------\n","0 | model   | RNN              | 7.5 M \n","1 | loss_fn | CrossEntropyLoss | 0     \n","2 | metrics | ModuleDict       | 0     \n","---------------------------------------------\n","7.5 M     Trainable params\n","0         Non-trainable params\n","7.5 M     Total params\n","29.978    Total estimated model params size (MB)\n","Epoch 0:  85% 17/20 [00:25<00:04,  1.49s/it, loss=1.16, v_num=xf8o]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  95% 19/20 [00:25<00:01,  1.36s/it, loss=1.16, v_num=xf8o]\n","Validating:  67% 2/3 [00:00<00:00,  2.72it/s]\u001b[A\n","Epoch 0: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.16, v_num=xf8o, val/acc=0.362, val/loss=1.120]\n","Epoch 1:  90% 18/20 [00:25<00:02,  1.41s/it, loss=1.1, v_num=xf8o, val/acc=0.362, val/loss=1.120]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  2.70it/s]\u001b[A\n","Epoch 1: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.1, v_num=xf8o, val/acc=0.362, val/loss=1.120]\n","Epoch 1: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.1, v_num=xf8o, val/acc=0.433, val/loss=1.080]\n","Epoch 2:  90% 18/20 [00:25<00:02,  1.41s/it, loss=1.09, v_num=xf8o, val/acc=0.433, val/loss=1.080]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  2.74it/s]\u001b[A\n","Epoch 2: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.09, v_num=xf8o, val/acc=0.433, val/loss=1.080]\n","Epoch 2: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.09, v_num=xf8o, val/acc=0.381, val/loss=1.070]\n","Epoch 3:  90% 18/20 [00:25<00:02,  1.41s/it, loss=1.06, v_num=xf8o, val/acc=0.381, val/loss=1.070]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  2.67it/s]\u001b[A\n","Epoch 3: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.06, v_num=xf8o, val/acc=0.381, val/loss=1.070]\n","Epoch 3: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.06, v_num=xf8o, val/acc=0.429, val/loss=1.060]\n","Epoch 4:  90% 18/20 [00:25<00:02,  1.41s/it, loss=1.04, v_num=xf8o, val/acc=0.429, val/loss=1.060]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  2.72it/s]\u001b[A\n","Epoch 4: 100% 20/20 [00:26<00:00,  1.30s/it, loss=1.04, v_num=xf8o, val/acc=0.429, val/loss=1.060]\n","Epoch 4: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.04, v_num=xf8o, val/acc=0.481, val/loss=1.040]\n","Epoch 5:  90% 18/20 [00:25<00:02,  1.41s/it, loss=1.02, v_num=xf8o, val/acc=0.481, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  2.72it/s]\u001b[A\n","Epoch 5: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.02, v_num=xf8o, val/acc=0.481, val/loss=1.040]\n","Epoch 5: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.02, v_num=xf8o, val/acc=0.474, val/loss=1.040]\n","Epoch 6:  90% 18/20 [00:25<00:02,  1.41s/it, loss=0.994, v_num=xf8o, val/acc=0.474, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  2.71it/s]\u001b[A\n","Epoch 6: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.994, v_num=xf8o, val/acc=0.474, val/loss=1.040]\n","Epoch 6: 100% 20/20 [00:26<00:00,  1.32s/it, loss=0.994, v_num=xf8o, val/acc=0.463, val/loss=1.050]\n","Epoch 7:  90% 18/20 [00:25<00:02,  1.41s/it, loss=0.985, v_num=xf8o, val/acc=0.463, val/loss=1.050]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  2.69it/s]\u001b[A\n","Epoch 7: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.985, v_num=xf8o, val/acc=0.463, val/loss=1.050]\n","Epoch 7: 100% 20/20 [00:26<00:00,  1.32s/it, loss=0.985, v_num=xf8o, val/acc=0.463, val/loss=1.040]\n","Epoch 8:  90% 18/20 [00:25<00:02,  1.41s/it, loss=0.976, v_num=xf8o, val/acc=0.463, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  2.74it/s]\u001b[A\n","Epoch 8: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.976, v_num=xf8o, val/acc=0.463, val/loss=1.040]\n","Epoch 8: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.976, v_num=xf8o, val/acc=0.466, val/loss=1.040]\n","Epoch 9:  90% 18/20 [00:25<00:02,  1.41s/it, loss=0.969, v_num=xf8o, val/acc=0.466, val/loss=1.040]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n","Validating:  33% 1/3 [00:00<00:00,  2.71it/s]\u001b[A\n","Epoch 9: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.969, v_num=xf8o, val/acc=0.466, val/loss=1.040]\n","Epoch 9: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.969, v_num=xf8o, val/acc=0.448, val/loss=1.040]\n","Epoch 9: 100% 20/20 [00:26<00:00,  1.32s/it, loss=0.969, v_num=xf8o, val/acc=0.448, val/loss=1.040]\n","[2021-06-27 04:41:56,512][reddit_post_classification.data.reddit_datamodule][INFO] - Loading test data...\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Testing: 100% 2/2 [00:00<00:00,  2.69it/s]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'test/acc': 0.42561984062194824, 'test/loss': 1.0493454933166504}\n","--------------------------------------------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n","\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3898\n","\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210627_043730-3af8xf8o/logs/debug.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210627_043730-3af8xf8o/logs/debug-internal.log\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.96697\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 266\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1624768917\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.44776\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 1.0417\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.42562\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 1.04935\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▆▅▄▄▃▂▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▅▂▅██▇▇▇▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▅▃▃▁▁▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33matomic-elevator-67\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3af8xf8o\u001b[0m\n","[2021-06-27 04:42:03,186][HYDRA] Best parameters: {'optimizer.lr': 0.009303199318889763, 'model.embedding_dim': 128, 'model.rnn_type': 'GRU', 'model.rnn_hidden_dim': 64, 'model.rnn_dropout': 0.1, 'model.rnn_num_layers': 4}\n","[2021-06-27 04:42:03,186][HYDRA] Best value: 0.7089552283287048\n"],"name":"stdout"}]},{"source":["The performance of RNN seemed to depend a lot on the hyperparameters, but in general is worse than that of CNN. For performance boost, I could use pretrained word embeddings or more advanced models such as BERT, but I preferred a smaller model as I wanted to deploy it to the Web."],"cell_type":"markdown","metadata":{}}]}