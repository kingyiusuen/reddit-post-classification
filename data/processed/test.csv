label,text
0,question answering ai hey people i m working on my personal project which will be quite a challenge one of its features is that the user can interact with an open domain question answering chatbot which will be trained on the data i provide it i want the model to resemble a specific person group and it will be fed everything that that person group wrote said and etc have in mind that the model can answer questions in 1 4 sentences and it doesn t need to be based on pure facts this means that the user won t ask the model questions like what is the capital of france but more something along the lines of existential questions what is the meaning of thing here are the questions i have as i didn t dabble into the nlp world of ai at all 1 are there any pre trained or prebuilt models out there that i could use for this i ve found that the open source pavlov ai library has some interesting ones 2 which models would suit this task the best 3 are there any features i should watch out for or provide more information on the biggest part of the job will be to collect relevant data on the group i want the model to resemble what would be some of the best practices when making the data as informative as it can be also if i want there to be 4 groups that the model can resemble do i need to train 4 models or can i filter what a model learned into 4 categories thanks for all replies and questions in advance if some of you are interested more in the project feel free to send a dm and we could even collaborate on this part of the project to make the model great
1,pluralsight statistics course recommendations for machine learning projects i m beginner who s signed up in plural sight and i m keen to learn r enough statistics to work on data analysis projects in kaggle etc i ve used python before in university looking forward to your replies thanks in advance
2,are machine learning models theoretically designed to make predictions about individuals are statistical models in theory able to make predictions about individuals suppose you have an individual with observed covariate information x a y b z c in theory can a regression model trained well on some data predict the expected value of this individual s response variable i heard today that statistical models are not designed to make predictions about individuals they are only designed to predict the average behavior of a large group of individuals and in theory should not be used to make predictions about individuals is this correct does this mean that any time statistical models are used to make individual predictions this is going against the intended use of statistical models if i understand correctly this means that when a statistical model makes a prediction about an individual with observed covariate information x a y b z c it s making a prediction for the behavior of all individuals in the universe with observed covariate information x a y b z c is this correct does this mean by definition the idea of making predictions for individuals is a fallacy thanks
2,about the neurips 2021 author survey neurips is in a really interesting mood this year first we had the checklist now we have this author survey all authors are asked to estimate the probability that each of their papers will be accepted in the neurips 2021 review process authors who submitted more than one paper will additionally be asked a second question to rank their papers in terms of their own perception of the papers scientific contributions to the neurips community according to the survey invite the purpose of this survey is to evaluate how well authors expectations and perceptions of the review process agree with reviewing outcomes i suppose it s interesting to get an idea of authors confidence in their work but it feels weird i also question what sort of genuine insight they can get from it as for me i got stuck in a dark fantasy of reading the rejection email along with an attachment of my acceptance estimate just to twist the knife a bit now i am extremely confident in the work that i submitted but i am much less confident in its chances as it goes through the increasingly random process of peer review so i had to be realistic when i filled out my estimate which showed it being little better than a coin flip to get in how are you handling it
2,identifying products based on their weights i m trying to develop a system that can classify a product based on its weight to double check if the operators are using the right setup on the balances we use in our production lines how is this where i work we measure the final product s wight to control the consumption of our raw material and quality control the balance s operator set a certain product id based on the production schedule and this product is measured and its weight is pointed on the system the problem is the operator can input product a while we are manufacturing product b and thus we will have a divergence between what is the expected weight and what is being measured i m wondering if i can use some i a to learn what product it is based on its weight to avoid this type of problem the thing is my only input are the measured weights but every product tend to be normally distributed
2,what matters in adversarial imitation learning google brain study reveals valuable insights a research team from google brain conducts a comprehensive empirical study on more than fifty choices in a generic adversarial imitation learning framework and explores their impacts on large scale 500k trained agents continuous control tasks to provide practical insights and recommendations for designing novel and effective ail algorithms here is a quick read what matters in adversarial imitation learning google brain study reveals valuable insights the paper what matters for adversarial imitation learning is on arxiv
2,image similarity challenge facebook ai x200b welcome to the image similarity challenge in this competition you will be building models that help detect whether a given query image is derived from any of the images in a large reference set content tracing is a crucial component on all social media platforms today used for such tasks as flagging misinformation and manipulative advertising preventing uploads of graphic violence and enforcing copyright protections but when dealing with the billions of new images generated every day on sites like facebook manual content moderation just doesn t scale they depend on algorithms to help automatically flag or remove bad content this competition allows you to test your skills in building a key part of that content tracing system and in so doing contribute to making social media more trustworthy and safe for the people who use it
2,twitter bot that tweets trending ml papers hey everyone i created a twitter bot that tweets trending papers in the ai ml category cs ai cs cl cs cv cs lg and stat ml on arxiv org arxivaiml tweets are based on the engagement score of feedly the algorithm is simple and naive implementation if you have any idea to improve the bot please let me know
0,how to extend a text classification ml model to work with more than one language we are using in production an ml for text classification we trained our model using some custom english text corpus currently the model is working acceptable level of accuracy for our purpose now we want to extend it to handle french language as well we are planning to investigate the following two approaches 1 we have a french language corpus therefore we would like to train a new model for handling french text 2 use the same model trained with english corpus but use a third party language translation service such as google translator to translate french text to english before inputting it into the ml model so i would like to know your thoughts regarding these two approaches
1,what s your favorite concept rule theorem in statistics and why what idea s in statistics really speak to you or you think are just the coolest things why what should everyone know about them
2,five very informative machine learning newsletters 1 the batch this is a newsletter published by deeplearning ai led by prof andrew ng it contains about 5 topics such as applications and research and for each topic there is a commentary by an expert on what s new why it s important and their thoughts 2 deep learning weekly about 3 6 topics are provided for each theme such as industrial application edge device relation research code etc very large amount of contents 3 ai weekly about 3 topics are provided for each theme such as application examples ethics robotics and research it is valuable because there are few summaries on ethics 4 papers with code papers with code is a newsletter from papers with code that introduces papers and code datasets and trends that are being discussed on github 5 akira s machine learning news a collection of articles on machine learning including practical examples technical articles and papers the basic commentary is in a few words but the weekly hot topics are explained in a bit longer mostly in the field of imaging
1,what s the difference between a qq plot and a normal probability plot
2,relative robustness on adversarial attacks while testing several defensive models against carlini wagner cw attack i am wondering what would be a good way to measure relative robustness of defenses let s say that i have defensive models a and b when they are attacked by cw attack both models accuracy end up becomes 0 which means the attack is so effective that the defenses are not useful however is there any way that measure relative robustness between a and b against cw in this case although a and b are of no use against cw attack there can be difference on robustness providing information that a is relatively more robust than b or in other way in case of pgd attack amount of perturbation epsilon can be controlled therefore although a defense becomes useless when epsilon is large we can still see and compare robustness of defenses with smaller epsilon but for cw attack i don t think limiting the number of iterations or other parameters are valid ways to limit the degree of cw attack i wonder what you think about this
0,choosing a combined title e g data scientist engineer if given some say in setting a job title for someone who does both and will be expected to do both in the role would you go for a merged title like data scientist engineer or just pick one context data scientist roles are already on the resume so goal is to highlight the additional skill responsibility if appropriate
0,so you trained a model now what the courses at university teach me how to understand and build a model however we do not learn what to do with the model once it s done like how to put it into production for a company i would like to understand this aspect a bit more as i understand it simple models can be saved and stored on a cloud server and accessed through api by the end application to make predictions based on new data is this realistic how do you deploy models in your work environment
1,book recommendation on snps i work as a statistician but need to know a lot about biology current i need to know about snps variants does anyone have any suggestions for books articles or something i can use to study
1,question calculating conditional sensitivity hey all consider a population that i am attempting to create a cohort from the population is described by 2 binary variables a and b my cohort will be composed of people who a 1 and b 1 i have 2 imperfect models a and b which may have correlated errors which i will use to select my cohort i can measure the sensitivity of each prediction individually i e sensitivity of a and sensitivity of b my question is how would you go about measuring how the performance of one model affects the performance of the other model when selecting the cohort x200b some thoughts i currently have 1 sensitivity of a conditioned on b 1 and b 1 i e sensitivity of a among people who should be selected into the cohort this approach however seems to ignore incorrect classification of b 2 sensitivity of a conditioned on b 1 i e sensitivity of a among people selected into the cohort this approach does not account for the correctness of b classification 3 sensitivity of a condition on b 1 i e sensitivity of a among people who should be selected into the cohort this approach doesn t account for the correctness of b as well and does not describe the cohort of interest a 1 and b 1 x200b perhaps i am approaching this with the wrong tool so i am certainly open to other suggestions thanks
2,graph neural network fails at generalizing on unseen graph topologies hi everyone x200b i m using pytorchgeometric to train a graph convolutional network for regression over nodes problem the graph models physical phenomena in the network of sensors in the training dataset there graphs with different topologies i e different edge index tensors and each of which has input and label tensors which consist of float values for each node in the graph the training curves look good the loss curve is converging to a small value and there are no exploding nor vanishing gradients x200b there are 1000 different graph topologies in the training set and around 2000 training samples so when the trained model is tested on graphs whose topology occurs 2 or 3 times in the training set the results are great almost the same as the test sample labels for each node the input values of nodes are different only the topology is already seen when the trained model is tested on graphs whose topology occurs one in the training set the results are slightly worse but when the model is tested on the unseen but similar graph topology the results are completely wrong since the graph models physical phenomena in the network of sensors i would expect that the gnn should be able to learn how sensor information impacts the neighboring variables even for the unseen graphs i ve tried going deeper into the graph and adding the convolutional layers x200b did someone have a similar problem are there some gnn models that are better at generalizing on unseen graph topologies x200b cheers
0,is there any difference in operational definitions between predicting vs forecasting is one a subset of another other or are they fundamentally different i have heard forecasting is different than predicting primarily due to forecasting having a time component but i am not sure if this is true
0,is this a common problem i tried fitting a glm style regression model to some data and it resulted in all the regression coefficients being estimated as 0 i e model failed yet when i tried a random forest model on the same data the model worked well and i was even able to get 70 accuracy on the test set my dataset has continuous and categorical variables as well as a lot of naturally occurring zeros i was just wondering is this a common problem i spent a whole day trying to tweak the regression model to work but the random forest instantly outperformed it thanks
0,what are some good software engineering practices that all data scientists must know we all know that a significant portion of the value added by data science comes from having good data infrastructure and model deploying which are more related to software engineering than math statistics if one wants to be as well rounded as possible what are some of the best software engineering practices things like version control that are a must
0,how hard is powerbi to learn coming from plotly dash so i’m looking at this job and they mention wanting tableau or powerbi prefer powerbi so i feel like that’s the big one how long would it take to pick up enough power bi to feel comfortable listing it in on a resume tableau took all of a few hours to pick up the basics and i’ve heard powerbi is even easier edit should probably mention i’m a ds with 6 years xp
1,distributions for parametric ph and aft models in survival analysis when estimating fully parameterised proportional hazard ph or accelerated failure time aft models we select a distribution according to our assumptions made on the dependent variable and then model the covariates effect on either the hazard or the expected duration i get that some distributions for example the weibull can be used in both cases but others for example the log normal cannot be used in ph models why is that i suspect that it has to do with whether the relevant hazard function is monotonic or not is that correct my text book cameron trivedi 2005 doesn t really explain why
0,marketing analytics how do i track where customers are coming from and how far on the website they go to this is a little confusing so i ll add all the details that i know so far we have facebook ads running for a product that is displayed on amazon but my manager wants me to create a website for this product and then analyze which ad people are coming from at the same time there s influencer marketing for this product as well unfortunately adobe analytics would be an easy way to track where people are coming from but my company doesn t use that there are a lot of clicks on the ads but no one is buying the product so i m trying to see exactly where on the website they stop and how to optimize the site layout my manager really wants me to create a website in order to track all of the information instead of using amazon and i m not so sure if that step is needed at all the product started out as really successful but now almost no one is buying it so it seems like a weird project the main issue is that i m not sure how to even start collecting data after i m able to collect the data i can then analyze it and make a model but i don t have a background in marketing or marketing analytics so i m pretty lost
0,should i take this data scientist job offer i am offered a data scientist job at a budding health startup as per the ceo job is not just about prediction but helping them to make a better product at this new start up i would be involved in deciding which product would fit vertically horizontally with the current product finding and discussing with right stakeholders out side the company finding buying data making models i sense the ceo is keen to license other models to integrate within the product i am fine with product development part but i would like to build model myself i am afraid if the ceo just lincense other model then i would be end up setting infrastructure to use the models within current products another key point is i am currently working in non mathematics physics academic lab where i do not do any modeling and would like to transition to data science learned data science through bootcamp personal projects my concern is would i get stuck within product infrastructure without making my hands dirty edited for clarity
2,are ml phd programs toxic i ve heard grad schools can be really toxic but it also varies by field i am an incoming freshman in computer science and i am interested in studying ml in grad school after i graduate but i ve come across these two posts and it seems the ml phd programs can be really toxic x200b x200b x200b are ml phd programs really that toxic i ve seen many people advising me to stay away from grad school but at the same time there are a lot of people who claim that phd degree is a bare minimum for research scientists and one needs to have at least a masters degree to be a ml engineer for those people who pursued grad school in ml do you think it was worth it also was the environment toxic
1,does anyone have a citation for the equivalence of an interaction regression model between binary regressors and using dummy variables for each combination between the binary regressors it s pretty easy to show on paper that the two models must be equivalent but my professor who isn t the greatest at stats wants a citation anyway the problem is this is such a trivial result that i m having trouble finding an academic source that actually spells it out any recommendations on where to look
2,would it make sense to use a rnn to approximate a linear system i ll start with a very simple example let s say i want to compute something similar to exponential moving average of time series data but with only n previous input values being averaged i can make a vector of exponentially decaying weights and compute dot product of this vector and n previous inputs getting the result i want but of course it s inefficient and for large enough n it doesn t give much of a difference from a standard exponential moving average computed using feedback so i can just use ema as a good approximation of my desired result in some sense i approximated a large memoryless linear system using a small linear system with memory i was wondering about a more general version of this idea what if my weights were different but with magnitudes still decaying exponentially and i was only interested in approximation being good on a small subset of possible inputs an example of this could be an artificial reverberation which also works by taking a large weighted average of shifted copies of an audio signal to me this seems similar to the way rnns work i e taking advantage of internal state and restricted input space to find a good approximation of a non trivial transform of time series data but since they re typically nonlinear i m not sure if it s a good idea to use them here on the other hand i couldn t find much information on rnns with purely linear activation functions has anybody here seen something similar to this are there issues or complications that i m missing thanks
1,post hoc testing for two way anova with many group levels in r how to do it i m currently working on a statistics project where i m performing a two way anova on a dataset with two categorical independent variables one continuous dependent variable the dependent variable is a relative growth rate measurement and the independent ones are a temperature condition 2 levels and a bacterial strain condition 49 levels i think you can see the complication here the assumptions are met and the analysis on a full linear model growth strain temperature strain temperature returns a significant interaction variable this is good news because it confirms our general expectations however things get messy when we want to do some post hoc testing to for instance assess whether growth is significantly different between temperatures for each bacterial strain condition i ve not been able to find a way to do this that doesn t bury me in a thousands of entries long pairwise tests table that compares every combination of strain and temperature with every other combination of these two does anyone know what methods might be useful to assess such data in r thanks in advance
2,help with multi class segmentation images hi guys i need some help with how to transform my images to create a multi class segmentation using a u net my question is regarding the target images they are 1d with three colors 0 to background 128 to my first class and 255 for my second class when i m calling the images they get transformed mask mask 128 1 mask mask 255 2 i m using 3 out channels for the last part of the u net and i m using cross entropy as my loss function is this correct because i m feeling that i might need to one hot encode my target masks
2,convolutional neural network for cell images hi i m not sure if this is the right subreddit for this but i wanted some advice on using a convolutional neural network i m doing a project where i want to detect and count cells in clusters within an image and was wondering the best way to do it in terms of frameworks or libraries are there any examples online that do something similar thanks for any advice
2,data validation and model testing strategy on training vs serving data problem at hand once you identify the model degration problem in the serving environement 1 how can i evaluate new incoming data of serving environment to understand the following 1 drift and skew in the training vs serving data 2 test models and decide the next steps i believe the strategy would highly differ based on the domain volume of data or the complexity of model but are there any generic frameworks available that tries to addresses these issues potential solutions framework i could find 1 drifter ml 2 tensorflow data validation
0,just failed an interview but i have a feeling that the interviewer is wrong so i had a technical take home challenge due to having to do machine learning on a laptop and having 100 million records i took a random sample of the data or more accurately only 1 because that s all my laptop can handle i proceeded to do eda train data and fit a few models that looked well fitting this is retail data and my interviewer immediately told me that my random sample approach is wrong he said that i should have taken a few stores at random and then used all their data as in full data for all the stores picked to train the models according to him you can t train the model unless you have every single data point for a store i think that he doesn t seem to understand the concept of random sampling i actually think both approaches are reasonable but that his claim of needing every single data point for a store or you are not getting the full picture is incorrect i failed the challenge due to this issue and that was literally the only thing that was wrong with my solution according to feedback i asked for to add data set contained 100000 stores in the same chain the goal was to fit a model that will predict total sales for those 100000 stores
0,are there any 3rd party tech data sci recruiting firms worth their salt i swear every time i decide to work with a 3rd party i m reminded of why i never want to work with 3rd parties even if they say they specifically place data science folks anyone here had luck working with a 3rd party recruitment firm and if so which one s
0,anyone has experience on working with ‘fully remote team’ i know that we are all working remotely to an extent now but does anyone here have experience with working on a team which is fully remote i am in talk with a recruiter for an exciting position in a fully remote company well funded startup and recruiter promises a good work life balance i have had colleagues in the same location in all the places i worked before i very much enjoyed the social aspect of office so being in a fully remote team is something new for me and i am being a bit cautious
2,neural networks inside an unconstrained optimization problem suppose there is a black box unconstrained optimization problem the objective it to minimize a given function f which is a scalar function several inputs one output by black box i mean that it is difficult to compute the gradient or even impossible of the function and every evaluation of this function is quite costly inside this black box function there is a neural network n that serves as a parametrization in a specific section of the computation of the black box function the idea is to find the weights of the neural network that can minimize this black box function unfortunately there is no data set that could be used to train the neural network in this case the idea is just to adjust the weights such that the optimization problem is solved i have some questions 1 is this feasible if it is what is the best way to approach such a problem 2 maybe neural networks are not the best way to approach such problem instead of a neural network can any other machine learning method approximate the parametrization inside the black box function such that the unconstrained optimization is solved 3 is there a research field which i could look into for similar problems
1,how can i show that anova is better than t test for comparing group means preface this is a a homework question where i had to choose a topic form a question and answer it and so i felt that it was appropriate for this sub since it is a topic of discussion for me if it isn t please tell me a sub where i can ask this from what i understand t test is used to compare the group means for two groups with n 30 whereas anova is used to compare more than 3 groups together i want to show why anova is preferred for multiple groups rather than multiple t tests according to this multiple t tests increase the error every time you conduct a t test there is a chance that you will make a type i error this error is usually 5 by running two t tests on the same data you will have increased your chance of making a mistake to 10 the formula for determining the new error rate for multiple t tests is not as simple as multiplying 5 by the number of tests however if you are only making a few multiple comparisons the results are very similar if you do as such three t tests would be 15 actually 14 3 and so on i want to prove this either with an example or mathematically are there any examples of this or any resources i can look into i was thinking of taking an example in r where i perform multiple t tests however i don t know how to get the error from it and moreover how should i perform the multiple t tests would every group be tested against every other group say if i have n groups would i need to run n 1 tests
2,do es the target s need to distribute normally before training i have built many classification models now i am working on a continuous target that like many other cases is heavily skewed to the right value from 0 and up that becomes less frequent as the value increases i have read that it is necessary to first have a transformation to make the target distribute normally and invert it upon predictions i have also read that residuals have to distribute normally is this necessary does it also depend on which model you are using i have been playing with a random forest regressor thanks
2,make a model do binary classification on a user specified class in a multi class scenario title sounds confusing but let me clarify each image can only contain 1 class let s say you have 3 classes 0 1 2 but you only want to see if class 0 is in an image model image 0 outputs binary classification of class 0 with sigmoid threshold this is not the same as straight up the model outputting logits for all 3 classes and then softmax argmax because then the model predicts either the most likely class even if the probability of the 2nd most likely class is high x200b i feel like that this has been done before like using said inputted class as part of a loss function to limit the output to 1 class what is the name of this procedure if it has one or are there any relevant papers that do something like this x200b maybe i m overthinking it idk thanks
2,metaheuristics derivative free methods are any groups working in this after reading through the excellent essentials of metaheuristics i wondered if there was any conferences or research networks that actually focus on these sorts of methods eu me looks like it’s pretty dead based on its website are any of you working with developing these type of methods explicitly and if so where are you communicating with other researchers cheers
1,incoming freshman in college about to take statistics major in actuarial science just for some context i just got my college results and i got into the university and program i wanted now that i’m sure i’m going to be taking statistics as my course i want to be productive with my free time and be ahead of the game if anyone has any resources for me that i should check out and learn let me know also if anyone has any advice for me would love to hear them as well
2,how does google s showing results for work if i search i love to eate my food on google then google will show results for i love to eat my food how does this algorithm work
1,with possibly too few data points post intervention for an interrupted time series study should i try a different test hello currently i am analyzing the change in a trend line pre and post the covid 19 lockdowns april 2020 i have two separate datasets one with monthly data points from 2013 q4 2021 q3 dfa and the other from 2015 q4 2021 q3 dfb on each of these datasets i have different points plotted on the same graph ex monthly energy used for the top 10 vs bottom 10 of income for dfa i have plotted both monthly and yearly averages of the data i want to analyze if there was a change pre and post april 2020 between these two groups if being in the bottom 10 caused this group to use more less energy compared to the top 10 but am scared i do not have enough data points to confidently come to a conclusion with the interrupted time series study especially based on this paper zhang f wagner ak ross degnan d simulation based power calculation for designing interrupted time series analyses of health policy interventions j clin epidemiol 2011 64 1252 61 which says you ideally should have close to an even amount of points before and after the intervention so my question is does anyone here possibly have an idea of what other study i could use to analyze this difference or how i could alter the interrupted time series equation of y b0 b1t b2d b3p e to fit my analysis better any help is greatly appreciated and if anything confuses you on this long winded explanation i would be happy to delve in further thank you in advance
1,propensity score matching validity hello fellow statisticians i would like to perform a propensity score matching procedure in order to appraise an intervention for the sake of discussion lets assume this is a government grant robustness my series of questions if i follow the standard procedure of using a pairing algorithm and then compare the treatment and control groups across their respective observable characteristics what are the signs that the pairing algorithm has produced a robust comparison frame model specification does it make sense to include categorical variables in the model specification for the psm procedure i also have a variable that scores grant recipients against selection criteria a continuous variable this is an imperfect proxy for treatment but correlates very well should i include this in the specification for the psm model generalities any general mistakes that newbies often make regarding internal validity if you will how can i insure that the procedure is robust your help is greatly appreciated thanks
2,dataset of the political alignment of subreddits hi as a part of a course on network science i m currently taking i m doing a project where i want to analyze the political divide in the us and how it carries into the digital landscape in particular reddit i d like to see the relationships between subreddits of differing political alignments i d like to use this dataset which includes 6000 subreddits which links subreddits where a user posted a link from one subreddit to another my current issue is that i don t have any way to find out the political orientation of the subreddits well at least not in a matter which takes a reasonable amount of time does anyone know of a dataset of subreddits that includes the political orientation of each otherwise if any of you thinks up an idea of how to perform my analysis without massively enriching the dataset perhaps by only using a handful of political subreddits i d very like to hear what you have to say
1,statistics question ancova or linear mixed model we have a dv depression measured at 3 time points two ivs one is a binary iv based on therapy time using random assignment the other iv is a continuous variable measured only once called alliance score this measures how well the patient and therapist worked together we want to know whether depression is significantly related to the two ivs again we have 3 measurements for depression baseline t0 3 months t1 and 6 months t2 i am curious what analysis most people would run here i have two thoughts 1 a two way ancova method with no interaction where the dv would be depression in t2 and the ivs would be treatment alliance score and depression at t0 the idea is to see if there is a treatment effect or effect of alliance controlling for baseline t0 depression this would mean not using t1 or possibly doing 2 models where depression at t1 is a dv as well 2 a linear mixed model where time is nested with each person this way uses t0 t1 and t3 for depression as the dv and then the ivs would be treatment alliance score and time an interaction between time and treatment would tell us whether the two groups experienced different rates of change in depression over time i would prefer to the run the first given its simplicity i think either analysis could be justified and want to know what some experienced researchers think
0,surprise 45 minute technical assessment in late stage interview with several of company s team members i m in the process of interviewing for a data scientist role i had taken a two hour python sql technical evaluation and passed it one week later i m in a late stage 90 minute interview with several team members with 45 minutes left they suddenly had me do a screen share with everyone and bombarded me with sql python questions this effectively left no time for me to ask questions i had for the team it was stressful and in no way reflects a typical coding environment that i ve been in i didn t botch the surprise technical assessment but didn t ace it either certainly wasn t an environment to do my best work i ll be honest it was a huge turn off i know data science is technical oriented but i felt that i had little opportunity to absorb ask about team and company culture bit of a rant but also curious if anyone else has experienced this and what your experience was like
0,research or internship in prep for phd applications hello all i’m currently an undergraduate stats major who will be a junior in the fall my goals is to apply to phd programs in senior fall if i wanted to look at opportunities for the summer prior to it should i look into doing research within say the stats dept or should i be trying to look for an actual internship at a company my research interests are within statistical learning so i was thinking a research role would be better suited for me than some data analytics position at a company i feel that it will be hard for me to get on any papers or do research because i won’t have a ton of theory knowledge but i’m hoping i can get on something more applied so what do you think i feel like trying to get a research role would be better when applying then doing sql all day at a company for a summer chances are even if i expressed my case as i want to do some more data science and quantitative sort of internship it wouldn’t be as good as if i did research with a prof i will be applying to stats phd programs
2,ai researchers from mit lincoln lab developed rio reconnaissance of influence operations system that would counter the spread of disinformation by making use of machine learning disinformation manipulates accurate information deliberately to mislead the masses and the spread of such information is not new it has been in practice for quite some time right from the imperial war propaganda and now in this digitalized world social media has brought with it its perils and one of them is its usage to spread false information it has the power to change opinions altogether for example the entire dynamics of the public elections however it is now being claimed that artificial intelligence systems could efficiently detect and simultaneously counter the spread of this disinformation on digital platforms the reconnaissance of influence operations rio program built at the mit lincoln laboratory promises to do just the same it would automatically detect and analyze all the social media accounts used to spread disinformation across a network paper
2,unable to use emojis for masked language modelling using bert i am new to hugging face and masked language modelling mlm and i was wondering how to include emojis when doing such a task i have a dataset with tweets with each tweet containing an emoji at the end here is a sample of my data id tweet 1 looking good today 😎 2 weather is so hot lol ☀️ 3 i hate you 🤬 at the moment i have fully trained my masked language model using my dataset but when i predict something it does not output or predict the emojis it just predicts words this is my desired input from using my dataset for mlm you look great mask this is my desired output from using my dataset for mlm score 0 26041436195373535 sequence you look great 😎 token 72 token str score 0 1813151091337204 sequence you look great 💯 token 2901 token str score 0 14516998827457428 sequence you look great 👌 token 328 token str however this is what i am actually getting from my output score 0 26041436195373535 sequence you look great token 72 token str score 0 1813151091337204 sequence you look great token 2901 token str score 0 14516998827457428 sequence you look great token 328 token str i know it is possible to do this but how do i do it i am close but not very likewise i have my model fully trained on my dataset but it just does not seem to output emojis even though i have included them in the training does something need to be included to accept emoji if so what thanks i would really appreciate the help
2,red flags that indicate that an elderly ml researcher has a hopelessly outdated mindset say you re considering a distinguished but elderly ai ml researcher as your future doctoral advisor you ve checked their twitter and decided against it because they wrote an opinion about ai ml that is so outdated there is no hope that their advise on your long term research directions will be of any use what opinion would be such red flag for you
2,cornell ntt’s physical neural networks a “radical alternative for implementing deep neural networks” that enables arbitrary physical systems training a team from cornell university and ntt research proposes physical neural networks pnns a universal framework that leverages a backpropagation algorithm to train arbitrary real physical systems to execute deep neural networks here is a quick read cornell ntt’s physical neural networks a “radical alternative for implementing deep neural networks” that enables arbitrary physical systems training the paper deep physical neural networks enabled by a backpropagation algorithm for arbitrary physical systems is on arxiv
2,where the ml research is headed and a thank you to the community that made it possible mark writes i increasingly feel like conferences matter less than twitter github by the time a paper hits conference the state of the art is two papers further acceptance by u wightmanr into timm and hitting u karpathy’s arxiv sanity trending is arguably more impactful than a neurips talk x200b andrej karpathy writes there’s a few other prestigious venues like ykilcher youtube paperswithcode ak92501 al tweet streams etc but yes i rather like the emerging hybrid model where the new cheap low latency async distributed consensus layer coexists with the legacy “layer 1 chain” pubs x200b arguably this subreddit is also playing a role in this trend without being part of the traditional academic community i feel like i have learned more about ai and ml without the need to do a ph d or attend a conference and i am deeply grateful this is possible thank you what are the thoughts of others on this trend
0,agent based modeling vs statistical learning approaches hello i’m currently a sophomore at my university whose in a undergraduate data science club we had a speaker come talk about the use of “agent based” models network models feedback models spatial models etc which the way he described it as was simulation based approaches as a statistics major and only being familiar with the statistical learning approach to modeling this was very different from the usual tree based models clustering models that i’m used to hearing about can anyone go into a bit more depth of what those types of models are where are they used are agent based models part of reinforcement learning
0,how do you decide how much of historical data is needed to build a predictive model it s going to be a vague question and but i just need some insights especially who works with real estate data we recently started moving all our data to a data lake gcp to store all the historical data now we also have access to years of listings data and some sort of other information from different vendors and we are struggling to decide how many years of listings historical data we want to pull from their servers our plan is to build some predictive models in future but not sure at the moment we just want to make sure we have the data available when the management want us to build a product i know the more data it is it s good but we need a cutoff i looked into different places for example in realtors com you can see the listing price for last 5 years while zillow shows you for last 10 years i am not sure for building predictive models how much data we needed how do you guys decide that so far we have decided to go with 5 years current year but if anyone here has any ideas it would be helpful thanks
1,trouble with understanding mixtures hello everyone i ve been studying gaussian mixtures for a project of mine and i can not for the life of me understand what they represent so far i assumed a gaussian mixture with 5 components was essentially a weighted 5 dimensional normal distribution however in some papers i ve read the number of dimensions of an observation is bigger than the number of mixtures is there a transformation of variables that i m missing or did i completely misunderstand the subject
0,how do i deal with ml models taking soooo long to train when i have to optimize results noob data scientist here this is a bit of a general question but i ll ask it nonetheless i ve lately been involved in two projects at a company that wasn t very interested in the quality of my development approach and code just the results and i think that i have not really learned best practices regarding model training for example at the last project i regularly changed the architecture of a neural network and ran the usual tests and went to do something else using jupyter notebooks also probably didn t help since i always had to come back to run another test manually however there was no real method to how i did it and it sometimes just felt like a stroke of luck when i found better results apart from feeling like a huge waste of time when the model underperformed so i was thinking if you could point out some things that may help me to structure my code and model development
1,what is the number of cards needed to complete a full pack if i m randomly picking them off the street how many playing cards would i need to find to ensure completion of a full deck of cards assuming i randomly pick up cards off the street some context guy on twitter says he has completed a full deck of cards since the beginning of the pandemic by picking them up every time he saw them on the streets by my very poor reckoning the calculation would go something like this first card chances are 52 52 would work for the collection second card 51 52 because one would be a duplicate third card 50 52 and so on so is the probability the product series of n 52 for n 1 to 52 stick that into wolfram you get an absurdly low number the exponent is 22 so the number of cards needed is 1 over this figure this is probably more than all the playing cards ever printed anywhere this silly post has me plugging numbers into wolfram and i need closure please help thanks
2,taxonomy issue what is the opposite of learning based methods hi i work in robotics unmanned vehicles etc and i have the following doubt while i am preparing a presentation how would you call the class of systems which is not using any learning for their functioning an example to understand what i am talking about is computer vision there are ways to detect edges in an image which are based on image moments and stuff like that other methods are entirely based on cnns and received a lot of attention what i am trying to say in my presentation is i know that to solve problem x we could use learning based methods but i won t discard looking into and using methods if i find that they are more effective i am thinking about the following analytical methods not sure it is correct formal methods maybe too narrow classical methods too broad some other ideas thanks
0,thank you r datascience r dataisbeautiful you guys helped me get my dream job ❤️ context i used to love working with technology when i was younger i did computer science at school worked at apple at 17 had work experience at toshiba research europe everything was going great until i got my gcse grades back and realised my coursework was terrible it wasn’t my fault but rather the teacher had taught us the complete wrong thing to do and only 1 person managed to pass he was fired but when it came to a levels i didn’t end up picking computer science as much as i wanted to i was anxiety riddled as a teenager and i didn’t believe in myself to do it i ended up going to university dropping out because of severe depression going into bookkeeping then lockdown happened i had so much free time that i ended up doing programming for fun i got reddit to try and find fixes to syntax errors when i’m programming but reddit recommended me this subreddit data is beautiful and i would check it everyday just because i found it interesting it was the perfect blend between number crunching and technology leading me to learn python get better with excel fast forward to a few days ago and i manage to get an interview with an amazing employer to work as a junior data analyst i was really worried because i didn’t know who or what the competition was but i did my best i mentioned that i followed these pages on reddit turns out they only interviewed one other person and i had the edge as i used reddit taught myself in my spare time showing huge enthusiasm thank you to everyone on this page you are all legends ❤️❤️❤️ tldr i fucked up computer science when i was a teen even though i loved it so much taught myself over lockdown and got a job partly because i read these subreddits in my spare time
2,a practical guide to counterfactual estimators for causal inference with time series cross sectional data this paper introduces a unified framework of counterfactual estimation for time series cross sectional data which estimates the average treatment effect on the treated by directly imputing treated counterfactuals its special cases include several newly developed methods such as the fixed effects counterfactual estimator interactive fixed effects counterfactual estimator and matrix completion estimator these estimators provide more reliable causal estimates than conventional two way fixed effects models when the treatment effects are heterogeneous or unobserved time varying confounders exist under this framework we propose two sets of diagnostic tests tests for no pre trend and placebo tests accompanied by visualization tools to help researchers gauge the validity of the no time varying confounder assumption we illustrate these methods with two political economy examples and develop an open source package fect in both r and stata to facilitate implementation liu licheng and wang ye and xu yiqing a practical guide to counterfactual estimators for causal inference with time series cross sectional data june 21 2020 available at ssrn or
1,how do i create a zscore of variable a based on mean sd of variable b in spss
1,can i use logistic regression in a case control analysis i m watching a linked in learning course that says that a general rule of thumb is that a cross sectional analysis logistic regression is not appropriate if less than 10 of your rows have a 1 in the dependent variable column in that case you should do a case control analysis instead my understanding is that in cross sectional studies the entire subpopulation is used and those w the outcome are compared to those w o the outcome but in case control analysis since the outcome is rare 10 you take all those with the outcome and only a portion of those w o it so you get a more balanced dataset a 1 1 up to 1 4 case to control ratio so why can t you just do logistic regression on that dataset just like you would in a cross sectional analysis or is it okay to do that you just interpret the results differently in cross sectional the odds ratio represents how strongly the exposure is related to the outcome whereas in case control it represents how strongly having the outcome is related to also having the exposure just want to make sure i m understanding things properly this is all very confusing
0,accumulated local effects i recently came across a newer technique called accumulated local effects that attempts to explain the effect of predictor variables on the response variable has anyone tried using this method on real data did you find it useful any stories anecdotes experiences comments reviews you would be willing to share regaeding this method
0,what jobs and sectors can i get with a data science degree hey guys so i’m undecided yet on what i want to major in but data science has caught my eye i was just wondering what specific entry level jobs could i get out of college is it so in demand that you can work in pretty much any sector is there such thing as an associates in data science im not very good at math but i plan on taking remedial math in college assuming they have that will i be fine what kind of internships can benefit me to set myself up for success
0,what are some industry standard codex for data pipelining i recently moved from it governance audit in which we have codex like cobit 5 to follow i wonder if there is something like that for data pipelining
1,do you agree with this statement i came across the following statement when reading a paper various researchers have provided an indication how to interpret bayes factors e g jeffreys 1961 raftery 1995 for completeness we provided the guidelines that were given by raftery 1995 these guidelines are helpful for researchers who are new to bayes factors we do not recommend to use these guidelines as strict rules because researchers should decide by himself or herself when he or she feels that the bayes factor indicates strong evidence do you agree with that last sentence because from my understanding until you choose a loss function you can t say anything about the difference between two bayes factor i know that it is not always feasible but if this choice belongs to the researcher then it is subjective and arbitrary like the 0 05 or 0 01 thresholds when using p values
1,question coin flipping probability hello i m interested in statistics i want to understand the basics if i flip a coin for 20 times and got heads 83 of the times this means something or nothing is there anyway to say there is an 40 chance wich the coin will got heads the 83 or something like that and how to do the basics calculations any recommended book for a noob thank you
1,are the beta and bernoulli processes normalized random measures with independent increments hi r statistics i have a quick question about stochastic processes bayesian nonparametrics random measures james et al 2009 introduced the notion of normalized random measures with independent increments does anyone know whether the beta process and or the bernoulli process belong to this family
1,does using an anova test make t tests unnecessary sorry if the question sounds stupid statistics is not my strong point i m writing my master s thesis that concerns 2 independent variables and 2 dependent variables subjects can be in one of four groups exposed to both iv s exposed to only the first iv exposed to only the second and exposed to neither concerning each dv i want to use an anova test to see differences in them between the four iv categories will using an anova essentially get all the same results as conducting a bunch of t tests between two categories at a time would is there any information i would be missing out on by not conducting t tests and just using the anova also is there a difference between the results i would get conducting a two way anova to assess both ivs at the same time vs conducting an anova for each iv
0,how can someone measure quantify friction faced by visitors on a website while performing a transaction action this transaction could be anything depending on a website like in my case financial website it could range from opening a bank account linking an account etc etc also the problem with using common metrics bounce rate exit rate is that it completely disregard the intention of the visitors maybe the person is just there for explorative purpose and if he doesn t perform an action that not necessarily means it s a friction point given the intention
1,modelling baseline for longitudinal research hi have a question about study design i d hope brighter minds than mine can help with let s say i m trying to run a linear mixed model lmer in r for example with a single repeated measurement random slope and random intercept i have a cognitive measurement as dv the researchers have measured baseline three times over three days prior to intervention let s say for example we treat times 0 1 and 2 coded as time 0 for a repeated measure how would this effect the covariance structure would this just merely provide more variance for the baseline intercept would this bias the slope given the increased n for baseline what would be the benefit of this method over lets say averaging the scores for 0 1 2 as a baseline score i could imagine that having some knowledge of the individuals variability for cognitive score may be useful would there be a third option
0,data literacy at your company i was wondering if some companies obliges most of their employees to learn to code basic skills or for example dashboarding advanced data analysis skills with excel or through training periods one week of full training and no actual work the purpose would be to make everyone data literate so that everyone at the company can provide the data team with good datasets or so that they can handle their dashboards once they are published how to filter refresh data if so please share your experience and what are the best ways to discuss that with the managers
0,eastern university ms in data science my review so i m currently enrolled in eastern university s online ms in data science i haven t finished the degree yet but i m far enough into it that i can give an honest assessment for those interested in enrolling first the good stuff eastern university puts their students first the professors administrators and admissions staff at eu have it together when it comes to solving problems and interacting with students they usually respond to my emails within an hour and give helpful feedback eu is super military friendly i m using the post 9 11 gi bill to fund my studies if you have 100 gi bill then you get a degree for free and pocket at least 15k in stipends even if you only have 60 gi bill like i do you ll still net around 6k at eastern you only need to take one class at a time to qualify as a full time student so you can collect that 894 month stipend while only enrolled in a single class they also offer a 30 per credit discount for veterans and active duty personnel if you re using tuition assistance while serving the whole program will only cost you 500 out of pocket which is an insanely good deal for a graduate degree everything is self directed no weekly modules if you burn the midnight oil you can finish an entire seven week course in just a week or two that s quite convenient if you re a full time working professional with an unpredictable schedule the professor s videos give a clear description on how to use programming languages i enrolled with zero programming experience but they hold your hand through the whole thing and explain in detail how it all works at a theoretical level i usually have the instructional videos open in one window and my python editor open in another so i can pause and play around with it it s an effective way to learn and there are a few drawbacks academically it s not super challenging for example all the assignments can be retaken until you get the grade you want also as an online program it s possible for an unscrupulous student to cheat their way through although i have no idea why someone would spend all this money on a degree to learn data science and then just coast through the program if you want to gain proficiency with python r statistics etc you ll have to practice writing code and crunching numbers on your own in addition to the class assignments they do however provide you with all the learning materials you need to gain proficiency the student account website isn t the most user friendly simply signing up for a new class is a drawn out cumbersome process that took me a while to figure out the learning management system they use brightspace isn t the most efficient either the bottom line i know this sounds cliche but you ll get out of this program what you put into it if you only do the bare minimum to pass each course you can pay all this money just to get a fancy diploma but lack the data science knowledge to succeed in the real world if you go the extra mile and take time to master the material on your own you ll have a strong enough foundation in data science to get into the industry gain experience and build a successful career in a lucrative field eu s program is a convenient and affordable option for working professionals who want to branch out into data science and have the motivation to work for it and now i ll answer a few frequently asked questions how much time per week would i need to put in as a student it varies depending on how much time you need to practice and how many classes you take each course is seven weeks long i ve been taking one course per term while working full time as a civil engineer part time as a national guard officer and taking care of my 9 month old daughter and i still have had plenty of time to finish each class with a few weeks to spare if you plan ahead each week it s possible to balance this program with a full time job eastern university isn t as prestigious high ranking as for example ut austin or georgia tech how will employers judge a degree from eu i wouldn t worry too much about university rankings once you re in the industry and have experience no one cares where you got your degree you ll also find that many data scientists are completely self taught and have degrees in unrelated subjects some of them don t have degrees at all for example my brother attended a bottom tier open enrollment university and studied computer science for a few semesters before dropping out even though he never finished his degree he became proficient at writing code and now makes six figures as a software engineer all you have to do is get good enough at writing the code to pass a technical interview to make yourself marketable do i even need a degree for this career field isn t it possible to learn data science simply by watching youtube videos yes it is possible to learn data science on your own but the structure of a degree program makes it easier to keep a consistent study schedule it also gives you something palpable to list on a resume when you start applying for jobs as i ve stated earlier many private sector tech companies are loose on degree requirements however government and military jobs stipulate certain degrees for their applicants i currently work for dod as an engineer and every time i apply for a new position within our department i have to submit my transcript along with it an ms in data science can help you land a federal job as a data engineer operations research analyst statistician etc if you have further questions don t hesitate to ask
0,what s your best use of automl
0,ways to make money using tech data skills outside of a regular job outside of a job working for a company or freelancing online as an employee or contractor what other options are possible for a software developer to make money using their programming computer science skills data science background any of you using your technology data skills to make money outside of your regular job
0,job frustrations data science analysis x200b so i have been working overall for around 6 months as a data analyst the first 4 months were a full time internship at a big4 and the last 2 months have been a full time position at a 6 years old startup the experience at both firms is amazing and enjoyable however the tasks themselves can sometimes be frustrating problem the frustration comes mainly from hard to clean datasets data inconsistency or tasks that are not at my level of experience yet aka challenges we also receive in some cases datasets that are slightly modified for no reason which causes the script to give an error context we receive generally sales data from retailers we clean them and push them to the database so we can later on visualize analyze create predictive models discussion i would like to initiate a discussion on your personal experiences with similar problems and how you dealt with them stackoverflow asked your supervisor i wrote a small blog here about my thoughts and stories so far a 2 mins read however i would be much more interested in the real stories in the comment section than the traffic to my blog because it will help me deal with the situations in a better way and with less frustrations on the long run
0,has anyone ever worked on a machine learning model for queues has anyone ever worked on a machine learning model for queues suppose there is a bakery the bakery has has n people working m people in line and q orders that they are currently working on the bakery is interested in making a machine learning model that predicts how long a customer will have to wait before the customer s order is ready and how long will the next customer have to wait before they can place an order has anyone ever come across a machine learning model which can predict waiting and processing times i have seen examples online where people try fitting exponential distributions to historical waiting times and see how well they fit as well as trying different m m k combinations but has anyone ever come across an instance where machine learning algorithms e g random forest neural networks are used to predict waiting times i saw something like this but there was no python or r code for this paper can anyone recommend some source blog github website book youtube lectures etc which show and provide computer code for analyzing queues using machine learning models thanks
0,what data science specializations do you think are worth looking into in the future i wanted to go down a route that may not be traditional a field where people can stand out what fields niches do you think those might be and how could one enter and also how can one gain these skills
2,can someone suggest an open free face recognition dataset i m looking for a facial recognition dataset which is free public to use for research purpose i need a dataset which includes a relatively high number of images per identity 200 images per identity for at least 30 identities with labels do you know any
2,is knowledge about probabilistic graphical models a core competence in ml i have read a thread a while ago where the question was whether one should take deep learning as a course in university after taking a ml course lot of people said that it is better to focus on the foundations and to learn more about ml in general deep leanring isn t the universal remedy people are making it out be stick to the basics and learn them very well and if your job wants you to do something with dl than you can begin to learn it is the same true for pgms
0,documenting code in data science my team does not do much documentation they tend to think it s not worth the time to add docstrings or other documentation and figure the code is readable and anyone can just figure out what the code is doing by looking at it we all tend to work on different models but often we need to understand how other models work and i think it s worth the time to have better documentation since what seems obvious to the person writing code might not be clear to another team member i m just wondering how other teams think about enforcing documentation i don t think it s a waste of time and there will most likely be someone taking over someone s codebase eventually and it s painful to try to understand some functions when that person isn t around to ask questions to
2,minimum number of devices for a federated learning environment hi all i am currently researching federated learning on tinyml i would like to know the minimum amount of devices you would suggest i have for researching i m currently working with 2 devices would it suffice if not would you suggest i emulate a few raspberry pis or purchase a few extras
1,randomized tournament style survey my team is looking to create a survey that identifies people s preferences amongst a set of words by making direct comparisons between two options for example let s pretend that we were asking people to determine their favorite ice cream flavor out of chocolate vanilla rocky road strawberry and mint they would be presented with a randomly generated choice between two of the options chocolate or strawberry and asked to select their preference chocolate being the clear winner here in my heavily biased opinion at this point either both options are replaced or the loser is replaced chocolate or vanilla this process continues for either a set or arbitrary number of times two questions is there a name for this style of survey i have seen them and participated in them before but cannot seem to be able to find it are there a particular set of techniques utilized to analyze the output of these surveys are there tools that people have already built to administer this type of survey what are they we ve discussed building a quick r shiny app to do this for us but i d really prefer to utilize a tool that someone else has already built please let me know if there is a better place to post this
0,lowballed for faang ds contracting as new grad advice needed hi everyone i had an unusual situation happen in the past few days and i d like some advice a staffing agency in the bay area offered me the opportunity to interview for a ds role on a faang team that would directly impact a product that is popular worldwide think 100m users i like the role but am hesitant about it being a contract position considering i have a full time job lined up post ms in the bay area that is paying 135k 150k if you include benefits 170k if equity options aren t worth crap with a team i like though at a much smaller scale more relatively unknown company with far fewer ds to learn from the staffing agency told me the team wants to bring me in for an additional 7 interviews testing me on everything statistics ml product sense python sql behavioral but that the position would only be paying 120k i told her that is ridiculous since this is just a contract position and it would need to pay at least 180k for me to waste my time preparing and interviewing for the role considering i have a full time offer already i was told today they would match the 180k was i being extremely low balled initially the staffing agency is well known and i ve heard decent things about it for context this team has been looking for nearly a year for someone and i m the only person to make it to the final stage as far as i know do you think it is worthwhile to continue the interview process would you any advice is appreciated thanks edit 1 i d be a w2 employee of the staffing agency they d have the contract with the faang
0,how long do non competes last i am a partner in a data science company as well going to work in another company both in data science non compete says it s lifetime is this viable
0,should i feel bad for stepping on other people projects hi i m kinda new at the company 7 months since i ve started working i noticed that there are a lot of tools and programs that simply are poorly done and don t work pretty well the thing is i have tons of ideas on how to improve these tools or re made them and i also have the ability to do so but i m afraid of what the people that made them would feel like if i step on the programs that they ve worked so hard on how should i approach this should i just get used to how things work here
2,how do you handle cases when little relevant data is produced in document related ml systems in production for example when extracting data from an unstructured document such as an order confirmation document if only a few fields like date of order and name of the supplier are retrieved but details such as the amount and the items are not how is this handled what are the fallback mechanisms used
1,interpreting ols coefficients to a percentage chance hey all i m gonna illustrate my question with an example of my output of an ols regression model independent variable coeff 0 02 constant term 0 05 is it safe to assume this implies a relative decrease of 40 compared to the model s baseline probability
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
2,ml to assign words to eeg numbers it didn t train on i have a csv file made of two columns one column is the eeg raw data numbers the other one is words i assigned to the eeg numbers if a ml algorithm trains on this csv file can it assign words to eeg numbers even if the eeg numbers is a new number it didn t train on if i add only the eeg column can it generate the words column based on how it trained previously it has to detect how words are assigned to numbers and assign words to numbers it didn t train about how many words should the training file have on it
2,can someone please explain what the white color shades mean in this picture these pictures are supposed to show the decision boundaries of different machine learning algorithms on a binary classification task there are two classes for the response variable red and blue shouldn t all the decision boundaries either be fully red or fully blue what do the shades of white mean does this mean an overlapping decision boundary thanks
1,probability of choosing an expired pill i have n pills in a bottle the pills expire after t days where n is less than t i get a new bottle of n pills at some number of days that is less than n which is a proportion of n called p when i get the new bottle of n pills i fill that bottle with the p n pills from the last bottle what is the probability that i pick an expired pill from the new bottle how does that probability change if i repeat the process assume the bottles are infinite size what if p is variable over time i m most interested in how you approached this problem so please share your thought process as well as any solutions
1,how do i interpret these results spearmans rho 0 107 p value 0 378 what do they show about the correlation between the two variables i need to interpret these for an academic report
2,improving topic modeling hi all i’m trying to implement some topic modeling for employee training survey responses i have 200 different training courses with between 10 and 400 survey comments each i want to use lda to extract topics and then i’d like to make a word cloud with the actual topic titles colored by the average sentiment is there any way to automatically infer the topic labels see idea below new training are being developed each week that have new topics like excel or providing feedback to the people you manage additionally does anyone know of any way to have a variable k number of topics auto selected by course some courses are a single topic one hour course while others are a week long with several training topics idea for auto tagging i’m thinking about extracting the nouns from my lemmatized text since i’m truly looking for topics thinking i could then create a noun list and label topics by using the most frequent noun in each topic
1,casella berger in pdf does anyone has casella and berger statistic inference as pdf without being images i would like to use the search tool since it s a 600 page book i didn t read anything about asking for book in rules so i hope this doesn t goes against them
0,need some advice on tracking specific words on a website over time hi guys hope you can help me out with this one i m looking to do some research i ve never done before to track visualize how many times a word has been mentioned on a website and on what dates they were mentioned so for example lets say i want to get data on how many times the word covid 19 has been mentioned on a specific news website on each date over the past 2 years i don t have a clue on how to attach a date to each mention how could i do this thanks in advance
1,what’re some good free cheap online resources for learning stats specially regression methods multivariate analysis and bayesian stats p s i have a strong mathematical and comp sci background in case that’s relevant
1,how to calculate the average time to a place within a country for a paper on international trade i d like to calculate the average distance to a port within a country is there a way to do this ideally it would be the average distance by road and adjusted for population but that seems like it d be much harder
1,are there any careers in data science and plant research i’m a junior in high school and i’m think of pursuing a career in statistics data analytics i’m passionate about plants and permaculture but it’s not really feasible career wise so i’m wondering if there’s a way to merge the two into one career are there any of you who works with data and plants if so what led you to choose this pathway and how did you enter what are the pros and cons of your job what do you recommend to someone trying to enter this field thank you
1,looking for statistics about the comprehension of sexual violence sorry if this isn t the right sub but i m looking for statistics about the comprehension of sexual violence whose participant isn t grouped by gender what s your understanding about sexual violence what do you regard as sexual violence do you know sexual violence is kind of items thank you
1,bayesian methods for sheep ancestry parentage my friend is a sheep farmer and the maintainer of a database of information for a particular breed that goes back in the us perhaps 40 years she has a particular sheep from about 15 years ago that is crucial to the lineage but whose parentage is in question genetics is not my area but i’ve been listening to the learning bayesian statistics podcast and doing some reading i know that bayesian methods can be used to make predictions but can they also be used to make “postdictions ” that is given the thousands of sheep in the database many descended from the sheep in question and others descended from other sheep possibly descendants of mystery sheep’s parents of that era and the interbreeding that has occurred since would it be possible to determine probabilities of the mystery sheep s parentage
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
1,expected value of throwing 3 heads in a row i know a solution to this problem is as follows a t b ht c hht d hhh and then you just say e x where x is 3 heads e x a p a e x d p d my issue is not understanding why we choose those events could someone please assist me with a detailed and nice explanation i know we need independent events that are exhaustive but i would never think to use those thank you
1,discord servers for statistics statistics related topics does anyone know of can anyone recommend good discord servers related to statistics and similar topics maybe there s one for this subreddit that i can t manage to find for example thank you
1,how do i set up my data for linear regression if i have multiple observations for one sample i have a dataset that i need to do linear regression with to find the residuals it is gene expression data for multiple genes for a cohort of people i am using their health data as covariates the problem i am running into is there is 1 point of data per person for lets say sex m 1 f 0 there are multiple observations for each person for gene expression do i repeat the sexfor the patient as many times as there are observations if patient a is male and there are 3 different genes expressed that i have data for does my data look like this x200b patient gene expression sex a z 5 1 a x 6 1 a c 5 5 1 x200b thanks in advance
2,on the role of knowledge graphs in explainable ai very interesting paper on the role of knowledge graphs in explainable ai by freddy lecue the current hype of artificial intelligence ai mostly refers to the success of machine learning and its sub domain of deep learning however ai is also about other areas such as knowledge representation and reasoning or distributed ai i e areas that need to be combined to reach the level of intelligence initially envisioned in the 1950s explainable ai xai now refers to the core backup for industry to apply ai in products at scale particularly for industries operating with critical systems this paper reviews xai not only from a machine learning perspective but also from the other ai research areas such as ai planning or constraint satisfaction and search we expose the xai challenges of ai fields their existing approaches limitations and opportunities for knowledge graphs and their underlying technologies x200b link to paper
2,pornhub uses machine learning to re colour 20 historic erotic films 1890 to 1940 even some by thomas eddison as a data scientist got to say it was pretty interesting to read about the use of machine learning to train an ai with 100 000 nudey videos and images to help it know how to colour films that were never in colour in the first place safe for work non porhub link
1,introduction to computational statistics using pymc3 by srijith rajamohan the purpose of this series of courses is to teach the basics of computational statistics for the purpose of performing inference to aspiring or new data scientists learn the basics of pymc3 for various bayesian modeling including linear regression hierarchical regression classification robust models and assessing the quality of models source the enrollment is free
2,wu dao 2 0 a new 1 75 trillion parameter multi modal mixture of experts model from china s baai lab with 10x the parameters of gpt 3 it reportedly achieves sota on a number of benchmarks across several domains this is a link to the least politicized article i could find on the topic china s beijing artificial intelligence lab released a statement this past week about their wu dao 2 0 model i haven t been able to find any demos or examples but from the handful of articles i ve seen it s apparently everything you d expect from a model that size
0,comparing a data analyst intern bank offer with it project analyst consultancy offer hi all i have two offers one is from a big4bank for a data analyst internship pays less but i believe they use state of art technology and there is a lot to learn the work is all for internal stuff the second offer is to work as a project analyst in which i will be working with clients the company is big but not in the top10 or anything the advantage of this company is that i’ll be working with clients which means more exposure and connections they also use near top tech i m not sure which one will provide most growth any advice appreciated
2,life expectancy of a 1080ti buying decision used 1080ti or rtx 2060 it goes without saying that gpu prices are insane right now but sometimes we still have to spend i m setting up a machine to train cnn and lstm models in keras and need to buy a gpu but budget constraints limit me to about 600 and i m a bit torn between getting a new rtx 2060 or a used gtx 1080ti speed wise i don t think there s any question that the 1080 is the better choice but aside from being more energetically expensive i am also worried about how much more life i am able to get out of a used card physically i d imagine that a large number of 1080ti s in the used market may have come from cryptominers who decide to retire these cards for whatever reason and while capacitors and fans can last a while are used miner cards going to be a significant problem in a machine that will inevitably also power the cards nearly 24 hours a day 7 days a week i guess i can avoid the issue of getting a secondhand miner card by only looking for private sellers and going by their words instead of going through big box stores but then i lose the safety net of being able to return the card if it s a clunker although honestly it s not like i d be able to tell if a card is going to die suddenly within a week right that and the aspect of not having tensor cores make me worry about how future proof the 1080ti will be well what would you do i guess buying a used rtx 2060 is an option but retailers are slowly restocking their shelves and new 2060s can go for less than used ones
1,what test would i use to compare means of answers from two different groups on a survey hey all i’m doing a survey for a class where i’m trying to compare answers for four questions based on whether or not people listen to a certain type of music for x amount of time what test would i use to compare the means of the answers for both of the groups it’s been a while since i took stats but i think the correct test would be a paired sample t test but i’m not sure i’m familiar with using spss thanks
0,new sub for honest feedback on your data analysis r destroymyanalysis r destroymyanalysis was created to help data scientists and students find pitfalls in their analyses i ve found it extremely helpful to have someone else look over my work and provide honest feedback for my sources methods conclusions etc so i thought it might be a good idea to create a sub just for that purpose anyone can post a link to colab binder github blog etc to get ideas for improvement find pitfalls and anything else to help you improve and make your future analyses more robust
1,question about testing i have three distributions of variables that are more or less normal these variables measure whether i think someone is cheating at a video game for var1 and var2 the lower the value the more likely i think you are to be a cheater for var3 the higher the value the more likely i think you are to be a cheater no measure alone is evidence but if you do really poorly on either both var1 and var2 or on all three metrics i think that person s probably a cheater i want some sort of statistical test using the three of them to identify the cheaters with some degree of confidence like if i can say 5 of the population is cheating with 95 confidence or these 10 players we are 99 sure are cheaters that would be amazing does anyone have a good idea about how to approach this is there a way to say like “this person is 2 standard deviations below on all three metrics and there’s only a 5 chance that’s a statistical coincidence” or something like that if this is relevant i have var1 and var3 data for all players but only var2 data for about 1 3 or 1 2 of the players
0,how much of your time do you spend with boring data tasks because your colleagues cannot code hey when talking to other professional python r users i sometimes hear them complaining that they have to spend a lot of time answering basic data questions for their colleagues just because they cannot code i am wondering what s your perception about this do you have the feeling that you are hired for your data science skills where you are actually working on interesting and challenging tasks or do you spend a lot of your time just bridging the gap for colleagues who cannot code
2,pytorch wrapper of attention free transformer aft layer hello folks recently came across the paper titled an attention free transformer by zhai et al from apple inc here s my pytorch wrapper around the aft full layer you can pip install it as well more info in readme it s a plug and play module with existing attention based networks without major tweaks any prs suggestions amends appreciated the paper wasn t too clear for some implementation details so i had to essentially pull certain things out of thin air and somehow it still works tbf cheers
0,data science job postings asking for both python and r i m seeing this quite a lot is this normal or are they just throwing in some buzz words they ve seen for data science skills i ve always been under the impression combing both is largely unnecessary and their use depends on the business or the individual data scientist s preference
0,looking for general advice how do you guys handle conflicts in data science projects with respect to peers managers and stakeholders
2,what left field approaches to ai do you know of over the years i ve come across the occasional alternative approach to ai huge efforts creating comprehensive hardcoded domain knowledge alternatives to neutral nets etc but i regret not bookmarking them so the above descriptions is all i remember do you guys know of interesting quaint but serious efforts of doing things really differently it would be nice to eventually get an overview of all the wierdness out there edit thanks everyone interesting stuff so far does anyone know the two examples i was referring to i d love to find them again one was a professor that as a alternative to gpt like nlp was handcrafting a huge database of concepts and how they related to eachother i read about this 2 3 years ago and the effort was ongoing the other one was a machine learning alternative to neutral nets i think it had a 3 letter acronym with an m and a c in it it also had one lone professor flagbearer can t remember much about it but it didn t fit into anything i had heard of i can t remember the general premise not svm on anything that common it was mid 2000 s tech
1,level of agreement finding a way to justify why only a certain level of mean was analyzed hey people i m currently writing my first ever thesis and i m a bit overwhelmed by the statistical part in my research i asked students to fill out a survey in which they had to indicate how important 34 different attributes were to them based on a 5 point likert scale i did descriptive statistics in spss to see the mean and std deviation of all 34 attributes the outcome was a table that included all 34 attributes and i was able to sort them from highest to lowest based on the mean my problem is now that i don t wanna analyze all 34 attributes but i have no idea how can i justify that i only focused for example on the ten most important attributes or on the 5 most important attributes my question is now if there is some way to calculate some sort of highest level of agreement that i can use to justify that i only looked at a certain amount of attributes for example saying that 12 attributes are the most important ones m 4 10
0,currently a data scientist want to increase my skillset to expand into data engineering any great resources courses etc that you guys can recommend thanks
1,question what r 2 values are typical in engineering design of experiments developing a model of a physical process think how a material property changes with respect to temperature time etc and i m trying to find what range of r 2 values i should expect to be reasonable for this field i know alone it s not a good indication of fit so i m looking at s too but i don t even have a gauge for what a reasonable r 2 value looks like 0 4 0 8 edit brain fart moment changed to proper values
2,weka dl4j image iteration i posted this in r learnmachinelearning first but didnt get any responses so trying here i am hoping someone is familiar with image classification using the dl4j library in weka using the gui i can train and save my model just fine but i am having trouble figuring out how to point the model to new data to classify my training arff and newdata arff are in abc123 jpg class format for training the model i set the image iterator location to the folder with all of my training images however when i try to run the model on the newdata arff it seems to want to look in the test folder for images and i am failing to understand where i can set the filepath for the newdata arff i m positive that i am overlooking a simple setting but frustration has set in and any help would be greatly appreciated
2,how are computational neuroscience and machine learning overalapping hi i am an undergrad with a background in neuroscience and math i have been very much interested in the problem of agi how the human mind even exists and how the brain fundamentally works i think computational neuroscience is making a lot of headwinds on these questions except agi recently i have been perusing some ml labs that have been working on the problems within cognitive neuroscience as well i was wondering how these fields interact if i do a phd in comp neuro is there a possibility for me to work in the ml and ai field if teach myself a lot of these concepts and do research that uses these concepts
0,can someone please explain what the white color shades mean in this picture these pictures are supposed to show the decision boundaries of different machine learning algorithms on a binary classification task there are two classes for the response variable red and blue shouldn t all the decision boundaries either be fully red or fully blue what do the shades of white mean does this mean an overlapping decision boundary thanks
0,everything wrong with zindi data science competition platform warning rant coming i want to to share my unfortunate experience with zindi platform it is a data science competition platform same as kaggle but the bounty doesn t usually exceed 2000 and it is geared more toward african countries i participated in a competition there hoping that the company hosting it would hire me if i win after few weeks i snatched the second place on the leaderboard i kept slightly improving it for the span of of what s left on the competition then one week before the deadline i got my account banned i opened my email thinking it was some sort of a mistake i found an email sent by them stating that they banned me under the pretext of collaboration outside of team i responded explaining to them that i single handedly worked on the solution of my problem telling them i m ready to provide proof if they want they didn t respond then today out of sheer luck i discovered that the team that took my place on the leaderboard when i got banned work as a data scientist for zindi which is quite preposterous to say the least how can they work in the company and be allowed to participate meaning it s in his advantage to ban people who are topping the leaderboard they eliminate any competition and they get the money this explains the very empty devoid of any logic explanation provided by zindi as the reason on why they banned me the such of collaboration outside of team without the willingness to elaborate any further or give sufficient proof even if my solution out performs theirs it s just insane i would say stay out of zindi it is an unfair community chances are not equal and they are not professional zindi epitomizes everything wrong with african countries conflict of interest lack of respect to people rentier state and corruption ps i come from an african country
0,effective sql for data science these last couple of years i ve spent a lot of time writing sql i put together some lessons learned to use it effectively for data science projects small things like using ctes auto formatting and jinja have made a huge difference for me what other recommendations you have to master sql for data science
1,solve collinearity in a fixed effects model using multi level regression as well dear reader first and foremost i feel obliged to tell straight that i m not a statistician or econometrician when trying to look for solutions i often find it hard to put into words exactly what i am looking for i am not looking for someone to solve my homework i just want to understand this concept ill try to keep it as clean as possible and hope that someone can confirm what i want to do is possible or correct me if there is an easier more applicable obvious solution i have panel data from 30 different stores for 90 days time the dependent variable is the amount of a certain good sold per store location per day i am currently using a fixed effects model to differentiate between store specific differences x200b in my fixed effects model i use 3 explanatory variables the indicator variable day of the week different weekdays have different amounts of traffic i dayofweek dummy var for whether or not there is a sale going on in that particular store that day the sale is always on a friday but not active for every store location at once discountday anticipation variable object of my study i want to know if customers buy less on days prior to the sale if they know that there will be a sale later this takes on the value 4 to 4 where 1 is on the thursday prior to the sale 2 is on the wednesday prior to the sale etc this is also an identifier variable i anticipation for all observations not within four days of a discount day this value returns blank this value is not the same at a date for each station since different stations have discounts on different weeks this leads me to the following fixed effects regression the daily sales amount for each station regressed against the the dayoftheweek the discountday dummy and i anticipation as can be expected there is a lot of collinearity here the discountday always coincides with a friday and each value of i anticipation can only coincide with one particular day of the week still i need all this information for my model i have no idea how to solve this here is what i am currently thinking i am looking to make a fixed effects model that first applies the day of the week to the aforementioned system so i find regression coefficients for each day of the week under no discounts and no anticipation effects i want to lock those values and then apply the anticipation values and the dummy is this possible using a multi level model i am not familiar with this is there another solution that is more suited for this problem i am not that good at econometrics so i prefer to keep it as simple as possible all i need to find is a good estimate of the anticipation effects per day i use stata so if you can share a stata function that is used in your solution then i would also appreciate that a lot whatever the case any help is much appreciated i am immensely helpful for anyone willing to put time into this and will always take the time to reply to you to thank you for your help x200b bhta
1,reasons to discourage using linear regression on big data are there any mathematical justifications as to why obvious question we shouldn t use linear regression on big data for example is there a math formula that shows the more data points rows and columns you have the probability for these points to become non linearly sepperable increases is there a mathematical formula that explicitly shows that linear regression models can not model non linearly sepperable data is there a mathematical formula that shows when you have more data points the standard error of a linear regression model e g the beta parameter estimates are more likely to rapidly increases is there any mathematical formula that shows linear regression models for big data are unstable e g for two inputs that are slightly different the same linear regression model would produce notably different outputs thanks
2,advanced python nlp introduction course we are publicly releasing all class material for our advanced python nlp introduction course at the budapest university of technology and economics feel free to share it all feedback is welcome course page on github
1,could you give me suggestions on how to get better at statistics this is kind of a broad question i m not in the stats field but i need statistics for my degree social science i wanna improve in interpreting data that includes making sense of the math and stats but i m not good at either of those i ve been recommended some textbooks on statistics e g gravetter levine etc i don t have a copy of the books yet before i get them i wanna ask do you have any recommendations for textbooks or even online courses coursera etc for stats recommendations for math stuff like calculus if that s helpful or necessary would be good too
1,life table vs survival analysis i was looking at this article on how actuaries use life tables is this not closely related to the kaplan meier method from survival analysis
2,torchsr image superresolution for pytorch hi all i started torchsr a package for super resolution networks written in pytorch it s inspired by torchvision and should feel familiar to torchvision users check it out low resolution image super resolution x4 and ground truth at the moment i implemented many datasets the most popular models edsr rcan and a number of network improvements and data augmentation method plus the training script for people who want to develop their own next steps gan training and multiscale networks github repo python package
1,question can you measure interaction in a chi squared test of independence i have statistical data where i want to look at differences of frequencies with a chi squared test of independence my main hypothesis includes 2 independent variables gender marital status and i want to add another variable country of origin and look at its effect if this were a continuous variable i would use anova and check for a statistically significant interaction but since i am looking at frequencies i use chi squared is there a way to measure interaction in chi squared tests or should i use another test thanks
1,decline of traditional state space models it seems that recurrent neural networks have overtaken traditional state space models for time series models is this because traditional state space models require the analyst to make certain assumptions about how the system transitions between different states whereas a recurrent neural network can consider a wide combination of states through hidden layers and deep architecture
0,i m scared for my future i m a college student with a data science major and accounting minor and i m frightened for my future recently i ve been reading about how competitive the market is and i m afraid that when i m finished with my degree then i won t be able to get a job furthermore i struggle with coding i know coding is such a large part of this job so it just hurts me to know that i suck my professor says i m doing the right things going to office hours almost every single day but i feel like i get the concepts but don t know how to create the code i hate the feeling of writing code but it fails but when i get it right it’s like the best feeling ever generally learning comes easy to me i hate to say it but time flew by in high school school came to me easy i graduated with a really high gpa and perfect attendance what a nerd i m not saying i never worked hard at all in school because when it came to dual classes i worked my ass off i feel like i have imposter syndrome and that i m not learning anything i love data and stats but i love the business side of the career even more i like the concept of being able to explain the models and have an impact on the company would the best course of action be to take online python courses in the summer and stick it through also in my course we have three cognates which are inferential thinking business intelligence and analytics and machine learning which would be the best inferential thinking contains mostly statistic classes business intelligence and analytics contains bia and infs classes and machine learning contains a bunch of cs classes thanks ds guys this year has been rough on me mentally this is my 2nd semester and it s been hard after this semester i will have 51 credit hours and i feel like life is moving so quick for me i barely get to hang out with friends anymore and i am pledging for a fraternity mainly for networking so this semester has been my hardest any tips or advice would be awesome edit i haven’t started on my accointing minor at all and i have decided on switching to business administration for my minor to be more educated on the business side of things
1,does anyone here study game theory i have always been curious about game theory but have been to intimidated to try to learn it this week i have decided to try and shake my fear and begin to learn about some basic principles in game theory x200b i am looking at this basic problem in game theory that asks how should the owner of a company split the profits between her and her workers x200b x200b i am having some difficulty in understanding the notations x200b 1 formally a coalitional game is defined as there is a set n of n players and a function and a function v that maps subsets of players to the real numbers v 2 n r where v empty set 0 x200b a why does v 2 n is the point of this expression to show all the possible interactions that can exist between any groups of players x200b b when considering the value function i e v why does v empty set 0 i guess this is an obvious statement but is it supposed to mean that the potential value of the interaction between no players i e the empty set is 0 x200b 2 now the problem where the owner of a company split the profits between her and her workers x200b the owner is represented by the symbol o there are m number of workers w1 w2 w3 wm each worker contributes an amount p to the profit x200b i am having difficulty understanding how the value function for this problem is determined x200b the value function for the coalition of players the coalition is denoted by s is x200b v s mp if the owner is included in the coalition v s 0 if the owner is not included in the coalition x200b my question how are v s mp and v s 0 initially determined or are these values assumed x200b 3 in this question m is defined both as the number of workers as well as the cardinality of s o x200b i have generally heard cardinality referred to as the size of a set what exactly does cardinality mean in this question are you supposed to be able to compute s o why does s o m x200b 4 can someone please walk me through the calculations why according to the shapley values should the profit be distributed such that the owner receive mp 2 and each worker receive p 2 x200b 5 lastly why is this question important is the point of this question to show the fairest distribution of profits according to contributions x200b thanks x200b x200b source
2,who said ai art has no soul a summed up history of progress in the ai art world x200b medium ai art when i started experimenting with a i art back in 2018 the compute power was barely enough to generate blobs it wasn t too different for the art stars of those days who produced fairly appalling aesthetics edmond de belamy anyone the issue with this is that it stuck in the art commentators mind as gimmicky and far from the real thing the art created by trained artists but we had to start somewhere a lot of water has passed under the bridge since then and today it is difficult to tell the origin of an ai work of art was it created by a human was it created by a machine next step in this movement is the merging of a i art blockchain and the delivery of a i art as nfts one day my real hope is that a few human artists will draw what the a i has helped them imagine it is an amazing tool to enhance imagination the medium article illustrates those aspirations
2,how can machine learning help improve seo can machine learning models help improve seo performance in any way for example google analytics is able to track data without cookies with the aid of machine learning any other ways in which seo can be revolutionized by machine learning
2,how to do multi task learning intelligently we have a new article out how to do multi task learning intelligently that may be of interest to you it covers the concept of multi task learning and provides a summary of some cool recent papers about it adashare learning what to share for efficient deep multi task learning end to end multi task learning with attention and which tasks should be learned together in multi task learning would love feedback
0,if you made a huge discovery to improve ai e g self driving cars what would you do next i ve sometimes wondered what i would actually do if i came across a discovery like this it s not like i can just send elon musk an email what would you do
2,styler style factor modeling with rapidity and robustness via speech decomposition for expressive and controllable neural text to speech new publication from interspeech 2021 we introduced styler which is non autoregressive based style modeling tts model paper demo code
1,which family function to use for my glmer hello i have plotted out my response variable and i m not sure whether i should use poisson negative binomial or a gamma function to model this response variable it is a discrete count variable i actually filtered the response variable when it was equal to 0 because i was interested in only those who have received one or more promotions otherwise the response looks like this and maintains the same pattern i will be using the glmer package because there are random effects in my model
0,julia vs r python recently i found a programming language called julia that is used for data science statistical analysis and machine learning does anyone actively use this software for their work how have your experiences been with julia does it offer anything that r and python can not is it worth learning julia e g some companies still have a loyal sas user group does the same happen with julia
2,dino pytorch implementation hi all i created a video where i implemented dino emerging properties in self supervised vision transformers from scratch i took the official code the authors open sourced made multiple modifications and finally used it to train a model the video is relatively long so feel free to use the timestamps i provide in the description lastly the video also contains two short tutorials on weight normalization and buffers in pytorch hope some of you find it helpful
1,where to download gmhdif software from
0,hired at a small company my job is shaky first ds job working remotely started about 3 months ago it s a very small company in an industry that i was unfamiliar with so there was a distinct learning curve in getting acquainted there was also no onboarding as is to be expected for very small companies i m not sure they needed a data scientist and now i m kind of scrambling to try and show value along with that the part of the business that i m working on does not generate any revenue and has very little data about 3000 sparse data points maybe 1200 good ones very few updates maybe 5 per week and very few insights that i am being asked to run on these actual data points with that i m being involved in the business development side of things higher ups do not know how this current 6 year old project should generate revenue and every week my task changes usually it s about finding open source datasets but my boss has very little focus patience so each week is different i struggle to maintain focus in my day to day data work as it becomes clear that what my boss wants is usually not doable within a reasonable time frame i e investigate a causal question that would be great for an entire econometric paper and will likely not generate revenue anytime soon are there any other ds folk who have been hired into small places with very little data being the only ds how did you handle the situation how long does it take you to do open source data collection i don t mind the field the work nor wearing many hats but i m worried that i won t be generating enough value to justify staying on the team
1,explanation on normal probability plots p p q q plots hello r statistics i m doing some classwork that pertains to normal distribution plots and i m having a bit of a hard time understanding the meaning of the plot axes i understand that probability plots indicate how well a set of data matches a normal distribution or some other distribution however scipy probplot labels the axes as ordered values and theoretical quantiles see the plot generated from my data set here i ve looked into quantiles a bit more and i don t quite understand what the x axis and y axis ranges really mean i don t quite understand what a 3 quantile is nor how i would take a particular data point and map it to these coordinates can anyone help shed some light on this online descriptions aren t very intuitive to me but that s probably because i am a bit new to this depth of statistic and this kind of jargon
0,why do so many of us suck at basic programming it s honestly unbelievable and frustrating how many data scientists suck at writing good code it s like many of us never learned basic modularity concepts proper documentation writing skills nor sometimes basic data structure and algorithms especially when you re going into production how the hell do you expect to meet deadlines especially when some poor engineer has to refactor your entire spaghetti of a codebase written in some jupyter notebook if i m ever at a position to hire data scientists i m definitely asking basic modularity questions rant end edit i should say basic oop and modular way of thinking i ve read too many codes with way too many interdependencies each function should do 1 particular thing colpletely not partly do 20 different things edit 2 okay so great many of you don t have production needs but guess what great many of us have production needs when you re resource constrained and engineers can t figure out what to do with your code because it s a gigantic spaghetti mess you re time to market gets delayed by months who knows spending an hour a day cleaning up your code while doing your r d could save months in the long term that s literally it great many of you are clearly super prejudiced and have very entrenched beliefs have fun meeting deadlines when pushing things to production
1,does increasing sample size increase test size as well as power power and test size of a test procedure are computed in the same way based on the probability of rejection under each parameter value increasing sample size can increase power does that also increase test size and therefore increase the risk of test size exceeding the level of significance how do people manage to increase power without risking test size exceeding level of significance thanks
1,glm regression canonical or non canonical link function hi guys it s been a while since i studied glms and their use and despite having a ton of experience with logistic regression i never worked so far with other distributions except normal i have some data where the response has a very skewed distribution so i was thinking to use a glm with gamma or exponential still evaluating if very small values have to be removed for external factors so i don t really know how the final distribution will look at the end this might be good practical learning however as i said i lack practical experience here i remember everything about residual analysis how to practically fit the model i work in r and how to perform variable selection however as far as i remember all these steps are quite straightforward when using the canonical link function things get complicated when using a non canonical one given that these analyses have to be presented to people that probably don t know anything more than the simple linear regression i have a great focus on the interpretation of the results at the end of the day they don t care about the methodology they want to know how x1 make y change that s where a non identity link function makes me struggle with the logistic regression i m quite able to read the estimated parameters as odds ratios and make it easier to understand for non statisticians what to do in this case my doubts are 1 if i use the canonical link in my case negative inverse how do i make understandable the findings if the estimated coefficient of x1 is 2 3 well how much is it increasing y in practice 2 if i opt for a non canonical link function identity in this case might fit well how all the steps for testing and variable selection are affected is it necessary to bootstrap to get the s e on the parameters and the p value of an f test any r package suggested thanks for the help guys i google a bit around but this seems a bit too deep to get easy answers from google
2,self supervised learning in vision recent papers dino barlow twins paws etc video interview dr ishan misra is a research scientist at facebook ai research where he works on computer vision and machine learning his main research interest is reducing the need for human supervision and indeed human knowledge in visual learning systems he finished his phd at the robotics institute at carnegie mellon he has done stints at microsoft research inria and yale today though we will be focusing an exciting cluster of recent papers around unsupervised representation learning for computer vision released from fair these are dino emerging properties in self supervised vision transformers barlow twins self supervised learning via redundancy reduction and paws semi supervised learning of visual features by non parametrically predicting view assignments with support samples all of these papers are hot off the press just being officially released in the last month or so many of you will remember pirl self supervised learning of pretext invariant representations which ishan was the primary author of in 2019 youtube pod
2,what are the active fields of research in bayesian ml i have only a vague idea that variational autoencoders are quite mature already and there is a lot happening about making variational inference usable for larger datasets can you give me some more detailed and broader view of the topic thanks a lot
2,transformer encoder and temporal sequence encoding is there any way of including temporal data instead of positional data as the input for a transformer encoder
0,steps when working with a new database what steps would you take to make sense of a large new dataset that has been sent to you had the above question pop up in an interview so from a data science perspective what would you do
1,question variational inference under the mean field assumption requires expressing kl q j θ j exp ln p y θ but exp ln p y θ isn t a probability distribution so how is this possible i am trying to understand variational inference under the mean field assumption in particular i am confused about the last two steps on this image kl divergence between q p is defined as the integral of qlog q p dθ where q and p are probability distributions in this case the author considers p to be exp ln p y θ where the expectation is over all qi θi except qj θj note q is chosen from a family of distributions that factorize into independent partitions this is the mean field assumption i have two questions 1 how can the kl divergence be defined between q p if p is not a proper probability distribution 2 in this case p is not even a function it is a constant it is just e raised to an expectation of some value under some distribution can kl divergence be defined between a probability distribution and a constant
2,paperfella where we learn from research papers together hello everyone want to join the place where people learn from research papers while talking to each other paperfella is an app for that it currently has two main functionalities 1 it creates a smart real time chat per research paper 2 it improves the research papers so you can read it in vertical mode in your mobile without zooming and you can also break down a word or expression to its most basic meaning by just touching the word or expression cambridge dictionary s api it also applies better styles on the typography and math symbols i want to talk to some people who d like to help me with this idea or are interested in it
2,what are suitable computer vision projects that can be implemented in office environments i was brainstorming with a bunch of friends we were wondering what are different use cases computer vision tasks in an office environment 1 social distance maintenance through video analytics can also be used for monitoring cigarettes alcohol etc in cafeterias 2 facial recognition application what are some other useful applications of computer vision tasks in an office environment looking forward to hearing from the community
0,weekly entering transitioning thread 18 apr 2021 25 apr 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
2,convect instant serverless deployment of ml models i’ve recently launched convect and would love for folks to try it out convect deploys machine learning ml models to instantly callable serverless api endpoints using convect jupyter notebook users can deploy trained models from their notebooks and share them with the world in seconds no web development or infrastructure experience is needed convect simplifies the process by being more opinionated than other model deployment workflows to give convect a try visit you can also try out models without signing into an account on the demo page i would love your feedback some background context deploying ml models to be used in production entails a different set of skills than training models in a sandbox environment and can get pretty complicated depending on what you’re trying to do for many data scientists this “sandbox” environment is a jupyter notebook one common approach to “deploying to production” that i’ve seen is turning a model trained with scikit learn in a notebook into an api endpoint from my experience there are a few ways to deploy a model to an api endpoint and all of them involve a nontrivial level of effort and time examples of some of the steps in the process include pickling a model and uploading it to cloud storage dockerizing a model’s prediction code and environment deploying a flask app or getting set up with an ml framework e g mlflow or platform e g sagemaker so you can use the deployment feature in their sdk while complex workflows make sense for deploying complex models i haven’t seen any dead simple deployment solutions for simple models and that’s what i am working on building with convect in this case simplicity comes at the cost of flexibility i e you give up the ability to customize your infrastructure and runtime environment in exchange for a simple one click workflow my hypothesis is that this tradeoff is worth it in many situations and i’m curious to see what people are enabled to build when this aspect of the ml workflow is drastically simplified under the hood convect creates two artifacts by serializing 1 the model prediction code and 2 all the variables that are in scope in the python session at deployment time i’ve made this part of the deployment code public here these artifacts are then loaded and executed in an aws lambda function upon invocation by an api gateway endpoint i’ve talked with 50 data scientists at small to medium size companies 2 300 employees and many have identified deployment as a pain point in their workflows i’ve also spoken with a few data scientists who have indicated that this would help save time on their after work weekend side projects i’d love for people to try out convect and to hear about how you use it or how i can improve it to make it useful i’ve also put together a gallery of examples for training and deploying models to make it easy to quickly get started and provided example endpoints that you can use query or even build ml powered apps on top of find out more at thanks for having a look
0,things i can do daily for improvement what are some things that i can do daily to become better in data science i don t have any formal background in data science because i come from a psychology background but i did enjoy a lot of my research and statistics courses which got me into data things that i am currently doing include watching tons of youtube videos programming in python and r for practice reading as much textbooks on python machine learning and even math i e precalculus linear algebra refreshing statistics but a lot of this stuff can be taxing so i m just wondering what little things i can do that could go a long way and can compound for the future any advice would be great thanks
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
1,what is this r computed from t statistic and df in the last row in the table in r sqrt t 2 t 2 df what is r is it pearson correlation coefficient r why does the relation hold is there some book explaining that thanks
0,similarity between datasets suppose you have two datasets each dataset contains continuous variables x y z and 1000 rows let s say the first dataset is from a hospital in california and the second dataset is from a hospital in new york are there any common ways to measure how similar the two datasets are another application if you train an ml algorithm on a dataset and then the you get new data then if the new data is really similar to the old data you can be more confident about the performance of the ml algorithm on the new data thanks
2,fb prophet to use or not to use hey team i m currently reviewing some projects at work and need some articles on fb prophet do you guys have any articles or resources i can use for their api
0,handling huge data sets with python so i m working with some huge datasets in python we re talking hundreds of columns and millions of rows python keeps crashing and giving me memory errors i tried increasing the virtual memory in windows and it helps somewhat but it still crashes some times there s got to be a better way does anyone have any suggestions the data sets are stored as csvs that are being imported into python i also have access to sas but it doesn t do me any good because sas has a limit to the number of rows it ll import which is two orders of magnitude below the size of the datasets
1,role of stochastic process e g martingale residual in survival analysis i am trying to better understand why certain concepts from stochastic process are used in survival analysis for example there is a popular model in survival analysis called the cox proportional hazards regression model this model is used to study the survival rates and the hazard rates of observations groups within the data this model uses the predictor variables associated with each individual to model the hazard it seems like a popular method to check whether a trained cox proportional hazards regression model is a good fit for a given dataset is to evaluate the martingale residuals for this model i e checking the proportional hazards assumption if i understood correctly the martingale residual predicted value subtracted from a martingale term follows a certain probability distribution for each predictor variable you can simulate thousands of paths corresponding to the theoretical distribution of the martingale residual then you can see how the actual martingale residual calculated using the data compares to all the theoretical simulations if the actual martingale residual fits somewhere between all the simulations we can say that this is a reasonable behavior and the model assumptions are valid can someone please help me 1 did i correctly understand the role of the martingale residual in survival analysis 2 can someone please help me understand the motivation for bringing martingales and stochastic process into survival analysis why is this necessary why is the martingale residual useful for validating the proportional hazard assumption i e the contribution of each variable to the overall hazard does not depend on time why is the proportional hazards assumption important to begin with i tried researching this online but all the material i found was either to vague beginner or too complicated can someone please help me understand this thanks
1,discussion opinions on nassim nicholas taleb i m coming to realize that people in the statistics community either seem to love or hate nassim nicholas taleb in this sub i ve noticed a propensity for the latter personally i ve enjoyed some of his writing but it s perhaps me being naturally attracted to his cynicism i have a decent grip on basic statistics but i would definitely not consider myself a statistician with my somewhat limited depth in statistical understanding it s hard for me to come up with counter points to some of the arguments he puts forth so i worry sometimes that i m being grifted on the other hand i think cynicism in moderation is healthy and can promote discourse barring taleb s abrasive communication style which can be unhealthy at times my question 1 if you like nassim nicholas taleb what specific ideas of his do you find interesting or truthful 2 if you don t like nassim nicholas taleb what arguments does he make that you find to be uninformed untruthful or perhaps even disingenuous
0,maybe this question has been asked before but how important is it to have a github account with your independent out of work projects i am in the market right now and i see that many companies are asking for github url information i m assuming it is so that the interviewee can provide some work that they have done on the side in ml outside of work requirements so how important is that because i enjoy ml and data analytics as my job but after work i enjoy other activities not necessarily doing data analytics and projects in ml so is not having a github full of ml projects something frowned upon by employers now
2,classification of natural language search vs keyword search for search engines in the past microsoft was recommending users to continue using keyword search however end users didn t get the message and they like searching using natural language also known as semantic search the truth is that technology has improved and maybe we can return better results using natural language queries nlq yet some people still prefer using keyword queries kq if you have 2 different internal search engines one optimized for kq and another optimized for nlq how do i inspect the query received and make a decision about what internal search engine to send the query to obviously if you send a kq to an nlq optimized search engine you will get bad results the same will happen if you send an nlq to an kq optimized search engine therefore an query type detector is necessary however i haven t found any relevant information about solving this problem do you know any prior work on it if now how to you suggest approaching it
1,discussion decision theoretic reasons for cluster analysis cluster analysis seems to be relatively popular for purposes such as separating out different market segments to aim for with different strategies however i have found it a bit difficult to find a reason in decision theory economic logic that this makes sense i was wondering if anyone could precisely articulate logic where this would be the analysis that really tells you what you want to know even under limited circumstances going back to the example of market segmentation one thing that would maybe make a bit of sense to me is if the clusters were actually the result of some latent categorical variable that you can t measure directly and you had a reason to expect due to other research that people with different values on this latent variable would respond optimally to different messages in this case maybe you would perform a cluster analysis on predictor variables to decide the cutoff for using different messaging strategies it seems like maybe this would be something close to optimal if your cluster analysis was based on sound assumptions about the distribution of the latent variable and how it relates to your observed predictor maybe on the other hand it seems like classification would be more useful than clustering in this situation at least if you had the right data to make a classification model alternatively without the assumption of some latent categorical variable maybe the point of the cluster analysis would be to find the coordinates of the center of the cluster the idea here is that people with similar characteristics on your predictor variables would respond similarly to messaging and a message optimized for the archetypical member of the cluster would perform pretty well for a lot a people since after all there is a cluster of people centered around that point i m not fully sure of my reasoning on either of these examples does someone have a better example is there a more logical statistical analysis to use for basically the same purpose
2,before i re attempt docker gpu pass through on windows promising signs from wsl 2 gpu support for docker on windows is tensorflow gpu acceleration now possible running in windows based docker containers before i revisit this and sink huge amounts of time trying to do the impossible again has anyone had success with this the learning experience was ‘fun’ the first time but probably much less so the second
2,how to geo cluster houses in a real estate dataset i have a fairly large portfolio of houses think thousands that i want to cluster based on proximity to neighboring houses and some house types fuel source detached apartment the goal is to create clusters based on the distance to other houses and the types e g cluster of 5 houses max 50 meters from each other which are all on the same fuel source and are detached luckily in my dataset it is most likely that houses next to each other will also be of the same type do you have any tips on algorithms approaches for this job i am proficient in python r thank you
0,has anyone ever applied an unsupervised learning method or reinforcement learning method to define ‘roles’ at the company think permissions ad groups and on boarding
1,can someone please explain the representer theorem in simpler term can someone please try to explain the representer theorem in simpler terms why is it considered important in the realm of machine learning thanks
1,can i use one way anova here hi for different sets of data 4 groups i have been asked if assumptions of variance equality were met since i used one way anova to test for statistical significance i am not an expert on this at all so would appreciate any insight you might have for me for some analyses variances were unequal based on bartlett s test using graphpad prism based on my research and limited understanding this may not be an issue and i could still use one way anova if the samples have the same size is that correct does same size mean that samples have the exact same number in my experiments the sample sizes are roughly the same as in 45 54 or 30 45 so can i still use one way anova or would i have to change my statistical test
2,text to image gans text rendering not being interpreted why when working with text to image gans e g clip and biggan does text from the prompt sometimes render in the image as text instead of interpreting that text is the word not understood is the prompt too short or is there another reason
1,inevitable manual work required in statistics projects i have feeling that not many people are willing to admit but ultimately is a significant part of many data mining projects e g checking data quality parsing through data etc still done manually for example here is an example i just made up relating to supervised nlp natural language processing classification suppose i have 1000 medical reports of patients containing unstructured text made by a doctor during a hospital visit for a given patient each report contains all the text notes that the doctor made for that patient for visits between 2010 and 2020 these reports make mention of the patients bio data e g age gender medical history etc and the details of the symptoms that the patient is experiencing over a long period of time e g let s say that these reports are 2000 words on average the problem is different doctors have different styles of writing each of these 1000 reports is different from another if a human were to read the report the human could figure out what happened to the patient did the patient have a serious condition let s call this class 1 or a non serious condition let s call this class 0 this is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients the problem is there is no clear and fast way not that i know of to take the 1000 medical reports that are available and label each report as class 1 or class 0 for example for class 0 one of the doctors could clearly write at the end of a report all medical tests were conducted and the results and were all negative and another doctor could end the report by saying the patient should seriously consider changing their lifestyle and eat healthier food benign in this example how would someone assign labels to all these 1000 cases without manually reading them and deciding if the information in the report corresponds to a serious condition or a non serious condition i was thinking of using something like sentiment analysis to capture the mood of these reports and use sentiment analysis a method to informally gauge if the tone of the report is dark serious condition or light non serious condition but i am not sure if this is the best way to approach this problem is there a way to do this without reading all the reports and manually deciding labels in the end this is what i am interested in doing suppose a new patient comes in and on the first visit the doctor makes some quick notes e g patient is male 30 years old 180 cm 100 kg non smoker frequently complains of chest pains no high blood pressure works a construction worker and takes daily medicine for acid reflex just based on these quick notes and the 1000 reports available note i am trying to illustrate a point here that the medical notes for the new patient and the 1000 reports do not have the same format can a researcher predict supervised classification e g decision tree if this patient will have a serious or a non serious condition ps suppose the doctors have a very detailed medical encyclopedia on their computers can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results
2,open catalyst challenge using ai to find catalysts for renewable energy storage x200b the open catalyst project is a collaborative research effort between facebook ai research fair and carnegie mellon university’s cmu department of chemical engineering the aim is to use ai to model and discover new catalysts for use in renewable energy storage to help in addressing climate change scalable and cost effective solutions to renewable energy storage are essential to addressing the world’s rising energy needs while reducing climate change as we increase our reliance on renewable energy sources such as wind and solar which produce intermittent power storage is needed to transfer power from times of peak generation to peak demand this may require the storage of power for hours days or months one solution that offers the potential of scaling to nation sized grids is the conversion of renewable energy to other fuels such as hydrogen to be widely adopted this process requires cost effective solutions to running chemical reactions an open challenge is finding low cost catalysts to drive these reactions at high rates through the use of quantum mechanical simulations density functional theory new catalyst structures can be tested and evaluated unfortunately the high computational cost of these simulations limits the number of structures that may be tested the use of ai or machine learning may provide a method to efficiently approximate these calculations leading to new approaches in finding effective catalysts to enable the broader research community to participate in this important project we are releasing the open catalyst dataset for training ml models the dataset contains 1 2 million molecular relaxations with results from over 250 million dft calculations in addition to the data baseline models and code are provided on our github page view the leaderboard to see the latest results and to submit your own to the evaluation server join the discuss forum to join the discussion with the community and ask any questions x200b
1,different coefficient signs with cross sectional regression vs pooled ols regression panel variant hello i have a data set with stock returns for a 21 day time period of 229 firms when i run a pooled ols regression on the data set i get significant positive variables where when i run a cross sectional regression on the sum of the returns cars the signs of almost all variables in the regression change what is the intuition behind this
1,can anyone remember a youtube tutorial for bayesian statistics that used measurement of the length of a football field as its teaching scenario i have a vague recollection that the video had a black background with neon coloured virtual pen being used by the presenter i watched it a few years ago and cannot find it anywhere
0,job hopping do it i consistently see hear people advice others especially early in their careers not to job hop it will look bad on your resume no one will want to hire you no one becomes vp if they can t show they can stay at one place that may apply in other careers and honestly i don t think it does but whatever but especially in data science it s just terrible terrible advice unless your current company is giving you 8 yearly raises and 20 comp increases with each promotion you should be looking for another job within 2 years edit to be clear if you re not getting 20 increases in comp you should be looking for opportunities that will offer you at least 20 increases in comp i m not saying you should take lateral moves just because you re not getting big enough increases make every move count and you should be able to there are 4 core reasons why experienced data science talent is in incredibly high demand if you have 1 years of legit data science experience you likely won t have trouble finding companies to offer you a job i was trying to hire a guy recently who ended up with 4 offers on the table including one that was 50 higher than the one i gave him that is what the world is looking like right now which means that even if you re flagged as a job hopper the reality is that most companies don t have the candidate pool to get picky enough to reject you they need people they don t have leverage the fastest way to get a higher ranking role is to change companies this is just basic probability the previous point your company may or may not have a higher role for you to move into and that s assuming they would even be interested in promoting you in contrast there are 100s of companies out there with openings at levels higher than you waiting for a role to open up at your company is just a bad gamble when i look at people that joined my first job at the same time as i did and mind you people i thought were just as if not more capable than me i am now 2 3 titles above them and it s all because i was able to get bigger titles as i switched jobs the job hopper red flag is self correcting say you jump jobs every 6 months 3 times in a row you would surely become a job hopper do not hire candidate after that until you weren t see if no one hires you for a new job for 2 3 4 years eventually someone is going to look at your resume and say hey they job hopped a bunch for a bit but now they ve been at job x for like 3 years we should give this person a chance for every hiring manager there is a number y of consecutive years at one job that will undo whatever previous job hopping behavior there was prior internal compensation increases are lower than external ones 99 of the time listen if your company is giving you 10 yearly raises and 25 promotion raises by all means stay that s a great company if you re like the rest of us getting 2 4 raises and 10 15 comp increases when you get a promotion and you re getting promoted every 2 5 years on average or less then the math just doesn t work out here s some very real data from my 8 year career average total comp increase without promotion 2 average total comp increase with promotion 12 average time till promotion 2 years average total comp increase changing jobs 28 if i had stayed at one job for 8 years my current comp would probably be somewhere around 50 60 of what it is now and that is in large part because i took a new job for a 25 raise after being with one company for only 6 months was it risky in that some employers may look at that 6 month tenure as a problem probably but it hasn t stopped at least some substantial number of companies to continue to pursue me now let s pause here some of you will say well i applied to one job once and didn t get it so i know that no one wants to hire me i had a roommate in college that gave me great life advice disguised as terrible dating advice his dating advice was listen the reason i get so many girls is that i go out and i hit on 30 40 girls every night and every one of them but one will tell me no but as long as one of them says yes that s all you need so i never followed that advice for dating but you should 100 follow it for job searching there are 100s of jobs out there for which you re qualified if you re currently employed and in no rush to leave your job continuously keep applying to any job that you would consider an upgrade over your current one obviously make sure the effort is worth the payout don t go doing week long take home assignments for jobs that suck but pick and choose jobs that are good fits pay well look cool and put some effort into applying for them you will get told no a lot at first and then you ll get better at cleaning up your resume better at interviewing better at take homes etc because the only way to get better at those things is to practice and then eventually you ll start making it to the final rounds of some interview processes but probably still get rejected and then one day you ll get an offer that represents a 40 increase in comp and you ll be sitting there thinking wait what the hell tl dr job hop aggressively especially if early in your career don t let blanket advice about how job hopping is bad deter you from doing what is going to almost surely be substantial leaps in your career and comp edit if it wasn t obvious and maybe it wasn t there are certainly cases where this advice won t apply u michaelkamprath pointed out that this may not apply in tech where you can climb the ladder as an individual contributor and deep expertise may be much more valuable than climbing the management ladder and i think that is a fair assessment however one must also recognize this is not the case outside of tech where the big salaries are going to come from management roles i think it s also fair to say that if compensation isn t a big driver for you and the quality of the work matters much more then this may not apply either
1,how to learn about weibull distribution sampling i m a biologist and i have some code i run to sample from a dataset of genes where most genes have skewed gene lengths most being very small but i can sample an equal number of genes across their gene lengths with this code but i am stuck on understanding statistically how it is doing the equal sampling i do this in r with input data view gene length gene1 5 gene2 6 gene3 400000 gene4 1000 gene5 25000 gene6 10 gene7 50 gene8 4 gene9 100 gene10 2000 classes df order df length classes density dweibull 1 nrow df shape 0 1 scale 1 classes gene length density 1 gene8 4 3 678794e 01 2 gene1 5 1 353353e 01 3 gene2 6 4 978707e 02 4 gene6 10 1 831564e 02 5 gene7 50 6 737947e 03 6 gene9 100 2 478752e 03 7 gene4 1000 9 118820e 04 8 gene10 2000 3 354626e 04 9 gene5 25000 1 234098e 04 10 gene3 400000 4 539993e 05 dfrep classes rep 1 nrow classes classes density 100000 classes table dfrep gene density calc classes sum classes dfrep density calc density calc match dfrep gene names density calc density prob 1 dfrep density calc gene sample data frame sample dfrep gene size 100 prob 1 dfrep density calc i haven t been able to find any statistics resources to learn specific about sampling from a weibull density especially those that are not maths intensive if that is possible for this in particular i don t understand the use of the word classes in this code and how this code over represents the short genes when creating dfrep and that somehow leads to equal sampling of both the short and long genes i would ve assumed increasing the number of short gene representation would increase the probability of those genes being sampled in sample but i can see it doesn t do that somehow are there any resources i could find for learning about sampling methods like this
2,highest resolution gan available hi everyone for a private film production experiment am seeking for the highest quality resolution image generation gan available for public i am searching for those that are trained on landscape and architectural datasets not those for human faces alternatively if there is a hi res gan that could be retrained it would also be appreciated generation consistency is also a big factor for me as plan to animate a lot of generated frames what providers are out there that can do this my manual search has only pointed me to tech papers andl am an artist not a ml scientist thank you very much for your help 3
2,data challenge by oak ridge national laboratory news if you are interested to play with large scientific data generated by various divisions of ornl please visit
1,what are some of the ways to model a dataset consist just two variables like actual sales and predicted sales in glm
0,how many of you are in a sales position but doing data science for your company i m head of sales for a relatively small company 20m annual sales and do all of the sales data analysis via power bi and i m learning python just curious how many other people in similar positions and how you re dealing with it any insight or guidance is appreciated i love working with data but have 20 years of sales specific experience previously wrote programs whatever you d like to call it in excel vba currently pulling from sql into power bi realizing i can t do what i want without python seaborn etc
2,is there a difference between composite class prediction or merging multiple classes into one hi all so i’m not sure if it’s the right place to ask this but i don’t know any better places for it either might be a bit silly as i’m not a machine learning engineer myself we are currently annotating data and building models for object detection from images and our solution at the moment is that first we would want to predict the object itself and then its different properties like size color material etc depending on the domain of the objects and we recently had a bit of a disagreement which one would be more accurate 1 having the objects’ classes as high level and generic as possible like “trousers” and “dresses” for garments or “tables” and “chairs” for furniture and then have “style type” as a separate property with values like “suit pants” and “jeans” and “sundress” or “office table” “dining table” “coffee table” 2 having the objects already more specific so basically already defining the style type in the first level of labels by just having “dining table” and “coffee table” as object labels and then predicting properties like material and leg count etc one valid argument for the second option was that object detection happens based on the whole image but as this process also selects the area of the object the property prediction only works with that area so for example if you can’t see from the table itself that whether it’s an office or a dining table it shouldn’t be a property but those should be separate objects because during object detection the model is working with the whole image and can consider the context like the table being inside the kitchen for example although i wouldn’t know how the model would know about this context if this isn’t present in the training data in any shape or form but other than that from the annotation point of view it made more sense to me to have the object classes as generic as possible and separate everything that somehow describes this specific object in a separate property i was told this composite prediction might grant lower accuracy because you need to do 2 predictions instead of one but for me this sounds like bs because we would still have different properties that need second prediction otherwise we’d just create all combinations of properties to form a single level set of classes for example merge “gender” and “material” as well while we’re at it “male cotton suit pants” “female cotton suit pants” etc i don’t want to believe this separation if objects and properties would somehow cause lower accuracy during prediction any professional experienced feedback on this topic
1,how do you get the z score out of only a percentage let s say we re planning on constructing a confidence interval we haven t done any research yet so we don t have the mean the standard deviation anything we want to set the confidence level at 95 2 5 tail probabilities without the use of a calculator or a z score chart how would we get the z score out of 95 i m guessing there s an equation for this if so what is that equation a solution i see is rearranging the equation for the bell curve to isolate x but i don t see how i could do it since i m not good enough at math to do that i m sorry if somebody has posted this question before but i can t find anything on this anywhere on the internet
0,i’m a pm a and need data science advice lead product manager here who recently joined a new company i have about ten years of experience but this is the first time i’m working on a heavy data science product with a junior data science team 😔 i am new to the org and on my second day the lead of the data science team put in his two weeks but didn’t stick around to provide institutional knowledge or any time for me to understand what’s been going on the data science is a small team of 4 and they are very very junior with that said i’m really trying to get them involved in our daily meetings so they can understand the vision and product we are building so ultimately be a individual contributor and ultimately point out where data science work is needed the problem is they don’t say a word it’s been 8 weeks and the data science really does not give any input at all not even on our designs which is super important and when asked to give feedback they stall and take it off line but i really think they just don’t have proper leadership i just can’t get them to give any input so i am not able to understand backend implications and such i imagine at a certain point the data science team will have to build “something” to power be efforts for example but if they don’t tell me that up front when discussing designs or requirements i can’t plan or gather solid data science requirements which is nuts what can i do to get more data science input to fuel our mission 80 percent of requirements have a data science implication from what i see and i need the data science team to step up more what can i do to make sure we have proper data science requirements how can i set up a successful data science strategy
0,advice on working on personal projects while maintaining a regular 9 5 day job i ask this as i m currently in a career switch and finally coming to realisation that i m only left with few hours after work how do you create time for your personal work outside of your day work optimizing sleep to gain more hours waking early to have uninterrupted focus
0,how to encoding nan values with meaning the two most common cases i ve seen nan values appear in datasets are either because the data was simply not collected and or is just missing for no meaningful reason or that a response is not applicable to a feature due to the nature of that specific data point as an example id has arthritis arthritis limits ability to work 1 yes yes 2 yes no 3 yes nan 4 no nan in the table above the values are missing in rows 3 and 4 for different reasons seemingly the value in row 3 is missing because most likely the data was not collected however in row 4 the missing value is due to the feature not being relevant i e we do not need to ask if a patient s arthritis limits their ability to work if they do not have arthritis it would thus seem that in order to make the most accurate model we should not treat these two cases the same what are some methods for dealing with these types of situations
1,assumptions of k means clustering i have often seen blog posts like this which show that k means clustering algorithm is unable to handle complicated data and can only recognize clusters within spherical clusters i have often seen this shown empirically like in these blogs but are there any mathematical justifications that explain why k means is unable to recognize clusters in more complicated data e g concentric circles crescents etc why is k means only good for specifically spheres beyond empirical demonstrations are there any reasons why k means receives a lot of criticism
2,game on mit allen ai microsoft open source a suite of ai programming puzzles a research team from mit allen institute for ai and microsoft research open sources python programming puzzles p3 a novel programming challenge suite that captures the essence of puzzles and can be used to teach and evaluate an ai s programming proficiency here is a quick read game on mit allen ai microsoft open source a suite of ai programming puzzles the paper programming puzzles is on arxiv
1,introduction to bootstrapping book giveaway dear all a new book about statistical bootstrapping has been published aimed at students practitioners and researchers the book provides an introduction to the technique besides the theoretical foundations practical examples are given in python and stata this book was written for very beginners and only the very basics stats i maybe ii are required to understand the content i am happy give away a few digital copies pdf i would very much appreciate a short review online personal blog website webshop etc bootstrapping an integrated approach with python and stata isbn 978 3110694406 please pm me with your email and a very short summary of your status student researcher etc many thanks edit due to the large interest i will send out the book about saturday evening european time thanks for the interest edit2 done if you pmed me but did not receive an email please check your spam folder as well message me again and please make sure to include your email thanks a lot edit3 i just realized many people contacted my in the messenger in reddit i did not see this until now my apologies
0,visualizing graph2vec i’m a data science n00b and was recently asked to look into graph2vec i ran the ex program and it outputs a csv file with 128 dimensions is their anyway to visualize this so that one can roughly see what is happening thanks
2,reconnaissance blind chess join our neurips competition create a bot for our neurips 2021 competition in reconnaissance blind chess reconnaissance blind chess is a chess variant designed for new research in artificial intelligence rbc includes imperfect information long term strategy explicit observations and almost no common knowledge these features appear in real world scenarios and challenge even state of the art algorithms each player of rbc controls traditional chess pieces but cannot directly see the locations of her opponent s pieces rather she learns partial information each turn by privately sensing a 3x3 area of the board rbc s foundation in traditional chess makes it familiar and entertaining to human players too there is no cost to enter this tournament winners will receive a small monetary prize and authors of the best ais will be invited talk about their bots at neurips the world s largest ai conference reconnaissance blind chess is now also a part of the new hidden information games competition higc being organized by deepmind and the czech technical university in prague learn more play a game of rbc yourself and join our research community at x200b organized by johns hopkins university applied physics laboratory with ashley j llorens microsoft research todd w neller gettysburg college raman arora johns hopkins university bo li university of illinois mykel j kochenderfer stanford university
2,what platforms do ml professionals use to express themselves for entertainment you have tons tiktok instagram etc where can one find a more meaningful place for knowledge exchange view poll
1,probability question i have someone who is trying to pass a multiple choice exam for state licensing each question has 4 choices and only 1 correct answer the exam question bank is roughly 300 questions the exam is 120 questions passing grade is 80 the person in question has taken and failed the exam 16 times if one were to randomly select answers how many times would it take to achieve a passing score
1,how to interpret a lcl that is under 0 when it is realistically not possible so i am doing a rate it is realistically impossible for a rate to go below 0 but when doing a ucl lcl the lcl is below a 0 how do i interpret that if the lcl ends up below a 0 would the lcl be set at 0
0,analysts scientists devs which dbms do you prefer view poll
0,do successful models defy the bias variance tradeoff in statistics we are always warned about the bias variance tradeoff simple statistical models are reliable but are generally unable to sufficiently capture the complexity within the data i e high bias low variance complex statistical models are able to capture complexity within the data but are generally not as reliable when generalizing to new data i e high variance low bias this leads me to my questions 1 are successful statistical models able to defy the bias variance tradeoff as a simple example consider the famous iris dataset kaggle competitions have shown us that statistical models can be made that perform well on both the training data as well as the test data are these statistical models defying the bias variance tradeoff now let s imagine a far more complicated problem and dataset but suppose that we are still able to create a statistical model that performs well on both the training data as well as the test data are we again defying the bias variance tradeoff 2 i have seen proofs that show how the mse mean squared error can be decomposed into a bias term and a variance term thus for a given statistical model for a fixed value of this model s mse if the variance is high then the bias must be low in order to compensate and vice versa my question relates to the following when people discuss the variance in the bias variance tradeoff they are generally interested in the variance of a statistical model s performance when dealing with unseen data since this unseen data might not even exist at the moment how is the bias variance tradeoff able to make claims about unseen data is the bias variance tradeoff a general idea with some theoretical foundations or is it mainly empirical 3 finally how does the bias variance tradeoff apply to real world models such as the self driving car alpha go and computers playing tetris or in the case of reinforcement learning models the bias variance tradeoff does not apply the same way it does in supervised learning models thanks
2,do successful models defy the bias variance tradeoff in statistics we are always warned about the bias variance tradeoff simple statistical models are reliable but are generally unable to sufficiently capture the complexity within the data i e high bias low variance complex statistical models are able to capture complexity within the data but are generally not as reliable when generalizing to new data i e high variance low bias this leads me to my questions 1 are successful statistical models able to defy the bias variance tradeoff as a simple example consider the famous iris dataset kaggle competitions have shown us that statistical models can be made that perform well on both the training data as well as the test data are these statistical models defying the bias variance tradeoff now let s imagine a far more complicated problem and dataset but suppose that we are still able to create a statistical model that performs well on both the training data as well as the test data are we again defying the bias variance tradeoff 2 i have seen proofs that show how the mse mean squared error can be decomposed into a bias term and a variance term thus for a given statistical model for a fixed value of this model s mse if the variance is high then the bias must be low in order to compensate and vice versa my question relates to the following when people discuss the variance in the bias variance tradeoff they are generally interested in the variance of a statistical model s performance when dealing with unseen data since this unseen data might not even exist at the moment how is the bias variance tradeoff able to make claims about unseen data is the bias variance tradeoff a general idea with some theoretical foundations or is it mainly empirical 3 finally how does the bias variance tradeoff apply to real world models such as the self driving car alpha go and computers playing tetris or in the case of reinforcement learning models the bias variance tradeoff does not apply the same way it does in supervised learning models thanks
2,got ml role but dislike ml any advice hi all before hand i want to preface that i understand the field is very competitive and many people would be very glad to trade for my position so i m sorry if i look entitled i ve recently been very fortunate to get a really high paying swe role that is very relevant to ml have to read and implement ml algo and tune parameters the amount of pay and level is what drives me to take this position however i have been secretly developing a sense of dislike and despair toward learning ml related stuff even though my resume clearly show that i only do ml stuff undergrad courses internship and job exp i mainly think that its a waste of my time in long run and feel like the materials are too detached from systems level thinking i have realized that i couldn t care less about the next best paper in ml or any new algorithm to tune model what i trully care about in long run is mostly software systems distributed system and devops how long do you think i will last in ml field if i have no passion for it or any advice from other who went through the same phase
2,has anyone heard of zaslavsk s theorem of hyperplanes has anyone heard of zaslavsky s theorem on hyperplane arrangement supposedly it says that there are only a finite number of ways that hyperplanes can be arranged does anyone know why this is important apparently it has implications to decision boundaries of machine learning classifiers thanks
1,how can i take notes and understand statistics in a college level course next semester i will be taking an economics statistics class that is supposed to be quite difficult i did some pre reading over the break in order to familiarize myself with the concepts before the semester and i find myself rewriting 50 of the content in the textbook and partially understanding what s going on there are about 2 or 3 formulas per page explaining the derivations and one or two practice problems one of my primary concerns is that i won t be able to memorize all the formulas to apply them on the exam or understand the concept presented i was wondering if anyone could give me study tips understanding the concepts and how to take effective notes instead of just re writing half of the content in the textbook
2,what ensemble techniques should i use hey guys my project is based on ensemble learning i have used 4 pre trained models to obtain my predictions and they are stored in 4 separate csv files i am doing some research on different ensemble techniques but the ones that i have seen so far use raw data train and test models get predictions and apply ensemble methods altogether i already have my predictions so i am trying to look for ensemble methods that simply take in predictions and don t carry out the training and testing models bit p s i am quite new to coding and ml thanks in advance h
1,how to appropriately deal with a single outlier than can t be deleted normal distribution hello everyone i am currently writing my thesis and have encountered the following problem i have sample of 150 countries with their respective scores for further tests and z score calculation i ll need to check whether the data is normal distributed the problem is that north korea s score is an outlier which leads to an excessive kurtosis how do i deal with this appropriately i have no legitimate argument for excluding the data point from the sample do i transform my entire dataset via log transformation or what is the usual approach here any help or advice appreciated thanks in advance
1,question indicating the difference between missing and censored data so i m working with a dataset let s say i have a variable time to fever clearance or tfc there are missingness markers for censored stuff like 7 discharged before clearance or 8 died before clearance and then there is blank for actual missing i m going to be doing regression on this data so i can t really keep values of 8 and such so how do i indicate the difference between actual missing vs censored without keeping values like 8 in my dataset should i just make them all as missing should i learn to work around the negative indicators or should i make a new variable just for indicators and mark the rest as missing in the original variable how would i take that into account in my regression thank you in advance for any help also i m sorry if this is a dumb question maybe i m overthinking it
0,i just got offered a data science internship with amazon i ve been lurking on the sub for 3 years and just wanted to thank the folks who put together stats ml cheat sheets this sub really motivated me to take my undergraduate degree in biomathematics statistics and turn it into a masters in data science i use to think i wouldn t have the programing background or that i wouldn t have the technical skills people wanted it took a lot of my moving past my imposter syndrome as a woman in stem and working on my skill set but i ve gotten this far thank you all so much edit just came back to this post and saw all the support for any one interested i have been applying since september to internships and have since then applied to 83 positions reworked my resume twice ended up making my own website for my projects just to look better on paper and got 5 interviews at the end of march i have gotten offers so far from every place i interviewed at and used the smaller offers to ask amazon to give me a decision earlier which ended up working i only did 2 interviews with amazon before i got my team and offer which from reading online isn t common as they usually have a 3rd or 4th interview for interns its been a long process and a battle at every stage just 2 weeks ago i was resigned to the idea of a summer with no internship but here we are now
1,prediction interval for arima models in multi step forecast i have an arima model i want to make a forecast over the next 6 time steps not only do i want the predictions but i also want a prediction interval with say 95 probability i m able to generate a confidence interval but this isn t what i want as this is just the confidence of the fit parameters not the residual error please feel free to correct me if i m mistaken if i only care about a one step prediction i believe i can just take the standard deviation of the residual errors and make a prediction interval out of that but in my case i want more than just the one step how can i make this 95 prediction interval for multiple steps into the future i imagine it s some combination of my confidence interval and the standard deviation of my residuals but i can t figure out
2,chinese ai lab challenges google openai with a model of 1 75 trillion parameters link here tl dr the beijing academy of artificial intelligence styled as baai and known in chinese as 北京智源人工智能研究院 launched the latest version of wudao 悟道 a pre trained deep learning model that the lab dubbed as “china’s first ” and “the world’s largest ever ” with a whopping 1 75 trillion parameters and the corresponding twitter thread what s interesting here is baai is funded in part by the china’s ministry of science and technology which is china s equivalent of the nsf the equivalent of this in the us would be for the nsf allocating billions of dollars a year only to train models
0,how do i gracefully exit an interview i am not qualified for i applied for a data science job recently thinking i was ready i ve been a data analyst for a few years now i had an phone interview over the phone and the job is way way over my head i don t know python much just basic syntax and slight use of pandas yet they want me to be able to code there i meet the sql requirements for sure however the recruiter over the phone did not sound confident when i told her i did not expect python to be used so heavily per the job description i also told her i am aware of a models that would be used in the role like linear regression k mean clustering etc but i have never actually coded them still she wanted me to progress to an in person interview next week i really don t think i can do this job some of the things she mentioned i have never even heard of i d love to work for this company in the future but i d like to bow out of the interview gracefully for now tips
1,maximum entropy method with random constraints i m working on a research problem that involves measuring constraints on a function f x c and then finding the distribution p x that has the largest entropy while respecting the constraint the problem is that when i measure c from the data there is uncertainty involved so i don t know exactly the value of c to put it in a bayesian context let s say the best i can determine with total certainty about c is a prior distribution for example it is well known that if f x is the expected value of x then p x is an exponential distribution with mean parameter c now what if all i know about c is that it s a normal random variable with mean mu and standard deviation sigma how would i find the solution p x that meets these criteria if it helps here are some important properties of p x i know with certainty 1 p x is a member of the exponential family 2 1 x 1 3 the sufficient statistic is a set of legendre polynomials 4 the function f x are the expected values of the legendre polynomials 5 the maximum likelihood estimate of the distribution p x is easily obtained from any given value of c 6 the mle obtained from 5 is also the maximum entropy solution because it is unique only one distribution p x satisfies the constraint any thoughts or relevant articles would be appreciated thank you
2,how can c4 5 algorithm split numerical attributr how can c4 5 algorithm split numerical attributes i had encountered one journal paper where the c4 5 decision tree has same attribute with different ranges multiple of time in same rules for example if age 60 and wealth poverty and age 75 then sick can somebody explain
1,have you ever had to derive a new estimator or test in industry i m curious under what circumstances a statistician would have to derive a new estimator or test in practice i would love to get a chance to use what i learned in math stats someday but i can t think of a situation i ve encountered where i d need to derive something new if i have i didn t recognize it and found an existing approach that was acceptable are there certain application areas where this is more likely to happen
1,is it possible to cross correlate rainfall data and flood extent basically i want to find out whether upstream rainfall affects downstream flood extent can i do this using cross correlation i wanted to use cross correlation because i think that upstream rainfall will have an impact but after a few days lag period how should i treat my data before performing cross correlation do i have to have the same number of data points in both time series for cross correlation for example i have rainfall data for 30 days but i have flood extent data for only 7 days so do i need to create a third time series comprising of the same days for which i have flood extent data or can i perform cross correlation on the 30 day time series and the 7 day time series i would also appreciate if anyone could point me toward some resources on cross correlation any help or advice will be highly appreciated kind of walking in the dark right now
1,pca principle components what are they and when are they useful hello please let me know if the below makes no sense i m trying to get my head around pca principle components pcs i d like to know how pcs work and how to predict whether pca will be appropriate for a dataset i ve had a look at a source that has a nice description of pcs quoted at the bottom and a graphic linked in main text from the source s graphic i can understand how to get the pca for a graph of two variables it is just the purple line a link to the graphic is here i m now trying to visualise how this graphic would translate to having more variables e g 3 if i had a three axis graph 1 axis per variable axes are x y z would the first pc axis represent maximal variance for all three variables so like the plot in the graphic but with a z axis and the red lines in the graphic being applied in 3d space and so the first pc axis represents the axis where maximal variance is achieved across every x y z axis for the data set and then the next pc axis is the orthogonal axis to the first pc axis whilst still being in the x y z axis 3d plot with nx pc axis being found where n is the number of variables if that s correct then i would expect a dataset whose points are not correlating in a specific direction i e a cloud of points to have large degrees of variance along multiple axis is this the kind of data that would result in a bad scree plot where the first couple of pcs don t capture all of the variance could you also get this if you have a cloud of weakly correlated points with respect to one of the three variables but the remining two show correlation x200b geometrically speaking principal components represent the directions of the data that explain a maximal amount of variance that is to say the lines that capture most information of the data the relationship between variance and information here is that the larger the variance carried by a line the larger the dispersion of the data points along it and the larger the dispersion along a line the more the information it has to put all this simply just think of principal components as new axes that provide the best angle to see and evaluate the data so that the differences between the observations are better visible source a step by step explanation of principal component analysis pca built in x200b edit this has nice visualisations principal components analysis okstate edu
2,how many ideas do you try out before finding something that s actually pursuable to something that s publishable as a master s student who s just now starting to take on research projects on my own i ve always heard that one of the most important qualities to doing research in machine learning is to try ideas out many people have given me this advice and have told me that it s the same advice their supervisors or seniors gave them i d follow accordingly and think of ideas that i could try e g performing error analysis and trying out a new modeling approach but for the past 4 5 months it s usually been ending up in results that are not pursuable performance is mediocre and or there aren t any surprising interesting points to analyze and definitely not publishable i m starting to wonder if i m even approaching research correctly btw before anyone says it i have been going to my pi for help many of the ideas that i ve tried out were his but he s more of an entrepreneur than a researcher and has been out of touch with the current research landscape for a while any tips or opinions are appreciated thanks
0,any library that can help me with identifying the correct data type for a given column i m looking for anything that can help me with this type of problem i basically need some help identifying the type that a given array is intended to be given its values titles etc for example i would need some code that realizes that this 45321 is intended to be numeric or that a given column is intended to be date even if some values are misformatted any help in the right direction is appreciated
0,is there a sub discipline of data science that focuses on the physical sciences i’m looking to see if there is a data science category that focuses on physical phenomena i e physics math chemistry etc instead of data science methods used when analyzing human behavior like voting tendencies or purchasing habits i know that both areas can use the same machine learning models but i m curious to know if the physical sciences tend to benefit from a certain category of data science methods am i right in assuming that our approach to data science in the physical sciences can be different than our approach to data science in the softer sciences like psychology and sociology if so i would greatly appreciate your thoughts and any potential references to already existing literature that relates to the topic thanks in advance
0,what course book talk etc added the most value to your career as a data scientist
0,why is php nowhere to be found when talking about data science i started intensively learning php a couple months back but still can t figure out why no one wants to use it for complex systems everything seems to line up it s decently fast scalable with a good framework very easy to mantain and develop in yet its popularity is steadily decreasing especially in complex areas like data science
2,which possible approaches for comparison of two data time series transformations without real metrics imagine that there are many time series and the problem of finding significant drops in the level of each series is being solved if there are heavy drops in it we also have two methods for detecting such level of series drops however there is a problem we make the decision about the catastrophic fall in the level of the series on the basis of the series itself but we do not know how much the real business metric corresponds to our finding for one of the methods it is possible to evaluate the business sense of the found falls in the series after the fact and we have a certain amount such assessments how theoretically can such two methods be compared i would like to come to the basic metrics accuracy precision recall errors of i ii types etc but unfortunately the feedback is either completely absent or given delayed and indirectly and also not accurate for example one method signals that a catastrophic fall the level of the series happened on may 1 and in the feedback to it there may be information that according to other data inaccessible to me problems have been observed say since april i think that useful metrics could be confirmation of the happened fall of the series according to the later data of this series for example the level of the series fixed at a lower level or continued to fall for the next n days also other useful metrics could be the qualitative characteristics of the method the conditions in which it can be applied and the restrictions on the data that it imposes but all of these methods of assessment and comparison are designed around the method itself and yet i need to somehow tie to the business metrics but they are clearly not given and they need to be constructed somehow
1,looking for standard statistics probability bayesian book hey i am taking a course in bayesian stats now and the book bayesian data analysis 3 we are using is really good but it is very text heavy i am wondering if there is any statistics book with the format definition theorem proof example that focuses on bayesian statistics or at least that it cover bayesian methods as much as it would cover standard frequentist statistics i seen a lot of recommendations for the book statistical rethinking but it seems to be very text heavy as well
1,history of non parametric models i have been reading about the use of non parametric models in machine learning e g kernel methods like svm kernel regression decision trees gradient boosting random forest and i tried to contextualize the reasons why these methods emerged in my head this is the conclusion i reached 1 parametric models like standard regression models are tricky parametric models require certain assumptions about the data to be true also require the analyst to manually specify interaction terms within variables but the biggest drawback regression models tend to require more beta coefficients to capture more complex patterns within the data a regression model with many beta coefficients behaves similar to a higher order polynomial function and higher order polynomials are notorious for behaving in very unpredictable ways outside the range of observed data runge phenomenon this basically explains why higher order regression models have a bad reputation of overfitting training data and generalizing poorly to new data this is all related to the bias variance tradeoff 2 non parametric models do not require interaction terms between variables to be manually specified e g in a decision tree you don t need to specify this the decision tree will try to recover these interactions by itself and have less stringent assumptions about the data e g choice of kernel the appeal of non parametric methods was an attempt to defy the bias variance tradeoff the idea of trying to make a less complex model with the ability to make predictions comparable to a complex model with the hope that the lack of explicit complexity leading to better generalization on test data 3 the popularity of neural networks a parametric model is due to the fact that researchers found out ways to make these explicitly complex models generalize to unseen data e g effective regularization methods is my interpretation of the history correct thanks
1,interrupted time series analysis with 2 constant variable hello statisticians i am currently working on a study where i am analyzing time series data for 2 groups of people before and after covid using the date cali started lockdowns as the specific date i am wondering if anyone had a statistical test where i could compare these two groups change before and after covid i currently have them graphed on the same graph which hints that there may be a difference but do not know what test to use to definitively say i have tried to use an interrupted time series with the standard equation yt β0 β1t β2xt β3txt but am unsure how to manipulate it to add the two groups as constant variables any help would be amazing thank you
0,how to use training data in python keras sequential metric edit the title says training because i m an idiot reddit doesn t let you edit titles but i meant to say testing i ve got a python keras sequential model that i would like to use early stopping on as soon as the test mse gets out of hand to prevent overfitting but i m not seeing any way to feed keras the test data and tell it to calculate a metric off that
0,not sure where in the spectrum i fall background i was hired as a data analyst a couple of months ago it s a startup with a small data science team the team is really endearing and i absolutely adore them that sounds all good i love the work i do as it s writing code and i love writing code but i m not doing any data analyst work i write etl processes come up with formulas to do certain calculations fix historical data because they were calculated wrong i e fix formulas that are close enough but there are better ways to calculate them and reprocess the entire data and writing sql queries to see things i wanna see the closest to what we can call analysis i do is to verify if the etl process i wrote shows updates the graphs and numbers in the platform and if they re correct so what exactly am i edit grammar
2,gan training aren t the double discriminator calls wasteful i ve noticed that a lot of gan codebases runs the discriminator d on the gen d data once for training the generator g and then again on the same data for training d i guess this is mainly as a convenience because g and d s losses have opposite signs is there another upside or do we accept 50 ish extra processing time just to avoid a small hassle two ways around it i can think of 1 use a minimizer on g and maximizer on d a k a negative learning rate 2 somehow flip the sign of the gradient when backprop ing from d to g
1,what is the best approach to analyzing how normal patient visit counts are i m currently looking at patient visits and no show rates for my clinic and i m wondering what would be the best way to show if the current month quarter patient count is within a normal range i usually just report the descriptive statistics and show the counts by month along with an overall average and then i also show what the last 5 months of patient counts looked like to see how the data trends i m wondering if there is a test out there or a better way of showing if the current report data is within a normal range and if not to flag it for attention in my report only if it s outside of a normal range because right now it s nice getting raw counts but we don t know if we should be concerned or not would it be better to show this data in comparison to the previous year s data or is there a statistical test out there that shows this better it will be mainly used to facilitate a discussion on whether the patient count drops rises requires a discussion from our leaders
1,estimating a transition matrix with irregular time points i have a dataset which has variables id state nr between 1 and 9 say date and nr days the number of days since the first observation for each id the observations were sampled at irregular time intervals i would like to estimate the probability of going from one state to any other state can i estimate the transition matrix by counting the number of transitions between each pair of states and then normalizing these counts i e can i just calculate m ij nr transitions from state i to state j nr of transitions from state i to estimate the proportion of people going from say state 1 to state 7 or would this be a bad idea
0,hi i just expanded the data science cheatsheet to five pages added material on time series statistics and a b testing and landed my first full time job hey all you might remember me from the data science cheatsheet i posted a few months ago here the support from that was incredible and i thought i’d share an update since then i’ve gone through a dozen interviews ranging from fang to startups to mbb and updated the cheatsheet with topics i’ve seen covered in actual interviews improvements include added time series added statistics added a b testing improved distribution section added multi class svm added hmm miscellaneous section and a bunch of other small changes scattered throughout these topics along with the material covered previously are all condensed in a convenient five page data science cheatsheet found here i’ll be heading to a fang company as a ds after graduation and i hope this cheatsheet is helpful to those on the job hunt or just looking to brush up on machine learning concepts feel free to leave any suggestions and star save the repo for reference and future updates cheers aw github repo
1,how to create a score between twitter sentiment scores positive negative neutral and performance statistics for athletes hi this is my first time posting here so i hope this is the correct space for this question i scraped twitter and analyzed a year s worth of posts for their sentiment i made a count for each type of sentiment positive negative and neutral i would like to create a type of index score when cross referenced with a player s performance during that same year the performance will have positive statistics goals 5 points assists 3 points etc and negative statistics owngoals 2 yellow cards 1 etc my goal is to determine which player deserves praise jeers based on their performance for the given year can someone help me out i ve never done statistical analysis before and am not sure where to begin
1,career can you guys recommend me some good statistic textbooks that touch upon the syllabus below preferably by indian authors exploratory analysis design of experiments sampling sampling error sampling bias measures of central tendency and dispersion statistical survey and presentation of data statistical inference confidence intervals correlation formulating null alternate hypothesis type i and type ii errors regression z test t test p value probability basics of probability probability density function pdf and cumulative distribution function cdf standard distributions
0,data science field is overwhelming so i am a beginner in this field and the amount of knowledge and work being done looks very overwhelming in fact my peers too seem like years ahead of me when it comes to knowledge and implementation curious whether anyone out there also felt this way and how did you manage to get out of this confusion to feel a little confident that you know something and can do something or if you still feel this way i want to know your experience
0,is it reasonable to ask for a full kaggle task as part of an interview process for a contract ds role i have been recently asked to do a kaggle task and 40 minute presentation and given a week to do it i think it is an overkill for a 3 6 months role how to politely decline it
1,has anyone ever used the drwhy package in r for explainable ai has anyone ever used this package in r before for trying to explain blackbox machine learning models this package drwhy seems like a comprehensive collection of different algorithms e g lime shap meant for explaining blackbox models e g neural networks has anyone ever used this package before how have your experiences been did it prove to be useful were the results reliable
0,are data science skills transferrable to regular swe roles i might be getting an automation data science python job soon and it s full time and i m still in school full time to me this job would mostly be worth it if the experience here would be sought after by future swe roles that i apply at when i graduate is this the case i love automation with python but am not personally a huge pandas numpy guy myself
2,image generators with conditionally independent pixel synthesis cips by anokhin et al generative models have become synonymous with convolutions and more recently with self attention yet we yes i am the second author of this paper yay 🙌 ask the question are convolutions really necessary to generate state of the art quality images perhaps surprisingly a simple multilayer perceptron mlp with a couple of clever tricks does just as good if not better as specialized convolutional architectures stylegan 2 on 256x256 resolution check out the full paper digest reading time 5 minutes to learn about the architecture of our mlp based generator the two types of positional encoding used to increase the fidelity of generated images and how cips can be used to generate seamless cyclical panoramas without ever training on full panoramic images meanwhile check out the paper summary poster by casual gan papers cips conditionally independent pixel synthesis full explanation post arxiv project page more recent popular computer vision paper breakdowns dall e vqgan decision transformer
1,is there any downloadable or browser program which can make more detailed stats so recently we won a tournament and i want to analyze the data mark up the bad rates what was the distribution etc is there a good site program what not relies on coding to do this rather than using excel
