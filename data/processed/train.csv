id,label,text
rm1u3i,1,linear programming small example lp powerful subject created one optimisation example gurobipy could easy illustrate power lp got new teachers assigned nearest school must align teacher salary expectation budget one constraint original example enjoyed please star like motivation best regards happy holidays
rfk295,1,batch size mlp difference rnn hi confused definition batch size number training examples one forward backward pass sounds trained model using information training examples predict difference rnn mlp batch size 1
ruv7tc,1,understanding model overfitting learning certain classes currently training model classify types bikes trained using resnet50 architecture attached simple output layer trained scratch twice times model failed pick two classes 6 classes total classes different times tried using different batch sizes learning rates image augmentation shuffling around training validation sets improvements seen anyone know something like could happening potential solutions try thanks advance also information required lmk first time posting communities would like provide information needed
rodati,1,looking map skills need pick implement application idea hello currently andrew ng machine learning course close finishing idea little project toying around without going much conceptual information rough idea allow multiple users input textual information website website generate summaries identified objects website detect contradictions provided information example user 1 ravens birds user 2 pets mammals user 3 pet raven called jack whose feathers white user 4 ravens black user able tell application track information detected noun ravens application store information knows ravens parse sort semantic understanding detect second third order contradictions inference application detect flag contradictions facts introduced users flag resolution like identify holes need know c developer nlp new anyone good resources courses particularly applicable could take point right direction thanks
rmdrrj,1,resources deep recurrent q networks currently learning partially observable markov decision processes deep recurrent q networks drqn looked paper massively useful resources available learn drqns
r3h9rp,1,filter conv layers operate previous conv output filter less current hi guys question filter conv layer sake clarity let take look simple model input layers input shape 224 224 3 layer1 layers conv2d 64 kernel size 3 input layer2 layers conv2d 256 kernel size 3 layer1 model keras model input layer2 model summary whose output layer type output shape param input 1 inputlayer none 224 224 3 0 conv2d conv2d none 222 222 64 1792 conv2d 1 conv2d none 220 220 256 147712 total params 149 504 question 1 conv layer 1 output 64 filter image take input 3 channels every filter works 3 channel merge together filters work first channel second network choose assign filters channels 2 layer 1 output 64 channels layer 2 output 256 channels input 64 basically question example channel take account first filter layer 2 e conv2d 1 thanks answers
qviuar,0,post undergrad paths machine learning econ major graduating spring top 40 uni found really like machine learning ai tech savvy particularly good math get really fast learner currently learning python econometrics probably favorite class go graduate soon looking grad programs finance data science etc try find job immediately particular tips break finance world machine learning kinds firms looking jobs would really appreciate advice tips
rjsh06,0,gpu run experiments train models aware many people use cloud computing services like aws gcp curious methods aware edit clarified say gpu meant access privately owned gpu used use lab servers gpus graduating use anymore
rx8l60,1,would finetune gpt neo respond prompt rather predicting comes specifically training titles youtube videos predicting comments
qk9uet,0,makes multi armed bandit problems contextual hi everyone dive straight problem makes multi armed bandit problem contextual read tensorflow agent tutorial agent receives context vector observation every step makes bandit setting contextual isnt every agent bandit setting since mab problem agent needs know machine bandit much knows probability machine contextual mab defer standard mab example extra information ontop example knows wether machine bandit higher probability wether raining second part question currently working stable baselines 3 normal observation function correlating observation function context vector tf using observation every step making contextual couldnt find information sb3 documentation contextual settings work specific extra context mab problem state machine bandit uses state one armed bandit hope isnt beginner question tolerated
r7vue4,0,guys read papers looking read latest research ml know look
ro1ebu,0,redditors work big tech 0 1 improvement matter company seen time time paper borderline acceptance marginal improvement 0 1 0 2 accuracy sort authors always argue marginal consistent improvement large sum revenue companies make billions dollars ml industry simple although new method redeploying whole pipeline time consuming want ask people working big tech improvement really matter companies willing redeploy improvement
rlon3o,1,hackthisai adversarial machine learning ctf challenges couple years ago took one day adversarial machine learning workshop fascinating field trouble finding clear entry learning path prototyping capture flag style challenges provide sort skill development appreciate playtesting thoughts format recommendations etc
rvy222,1,free code ai learn anyone experience using code ai models actually useful versatile cheap free want learn ai without coding plan ai engineer want use everyday life sit learn code python learn ai theory learn statistics experiment tools learn experiment algorithms gonna happen charlie
rbc9fd,1,anyone experience using riot games api machine learning tasks final project would like use riot games api basic machine learning analysis league legends profile x200b following tutorial variables undefined followed tutorial x200b wondering anyone could help understand missing share way used riot api learn something insanely hard anything want basic analysis
rwwjal,0,paper suggests time series anomaly detection papers wrong stumbled nice paper appear aaai 22 title seems much modest show random algorithm achieve apparent sota results domain seems stunning result casts doubt contribution dozens papers reason area time series anomaly detection seems wild west dubious papers sloppy thinking aside benchmark set 250 datasets b evaluated way free flaw post title reflects understanding paper authors may different preferred claim towards rigorous evaluation time series anomaly detection b www cs ucr edu eamonn time series data 2018 ucr timeseriesanomalydatasets2021 zip
qllc70,0,research towards generalization contrastive self supervised learning interesting results contrastive learning theory 1 generalization ability contrastive self supervised learning depends 3 factors strength data augmentations alignment positive samples divergence class centers 2 data augmentation enables self supervised learning good algorithms optimize alignment divergence factors well weak data augmentation still bad performance 3 barlow twins aims decorrelate different vector components representation implicitly optimizes geometry embedding space satisfy alignment divergence factors actually
rb3wh8,0,create self funding decentralized transparent research lab post research ml directly even though started qd algorithm searching interesting lenia creatures always wondered one could create self funding research lab even possible even better lab would transparent ground belief openai first understood state field time started non profit know evolved safe path maybe need radically transparent decentralized ground also optimistic trying start lab would love criticism ideas involvement community cheers
qxiywa,0,spann highly efficient billion scale approximate nearest neighbour search that‚Äôs 2√ó faster sota method research team microsoft peking university tencent baidu proposes spann simple efficient memory disk hybrid vector indexing search system guarantees low latency high recall achieves 2√ó speedup state art nearest neighbour search anns solution retaining recall quality memory cost quick read spann highly efficient billion scale approximate nearest neighbour search that‚Äôs 2√ó faster sota method associated code available project‚Äôs github paper spann highly efficient billion scale approximate nearest neighbor search arxiv
r6lbdj,1,clustering data sets mixed types hi experience best way cluster data sets mixed data types k prototypes agglomorative single average complete linkage thanks
rhpjg6,0,would anyone interested automated object detection data labeling tool hi recently graduated jobless computer engineer currently working tool allows automated photo capture bounding box labeling data augmentation training object detection models yolov5 basically create fully labeled diverse dataset 1 object category less 10 seconds manual work opposed tens even hundreds hours usually required collect label augment dataset using conventional methods main advantage tool fully automated labeling bounding boxes otherwise take long time manually annotate well extremely potent data augmentation training model also automated really anything new take usual 8 30 hours depending gpu hardware main limitation object must smaller approximately 50x50x50cm fit automated photo capture labeling device cannot example label sofa big single red blood cell small also physical hardware cost around 1000usd automated photo capture labeling device would like know cv ml community would find tool useful worth 1000usd capture device would likely cost currently working prototype want know anything similar exists whether worth keep developing maybe developing product future thanks ideas suggestions criticisms
qsaonk,0,adversarial loss understanding total relighting learning relight portraits background replacement paper hi hope get help understanding google total relighting learning relight portraits background replacement paper stuck adversarial loss 4 1 paper says add adversarial loss face region help network learn plausibly remove high frequency shading effects input image maintaining image detail use least squares discriminator mao et al 2017 disc alb add loss crop face ground truth albedo ùê¥crop gt matching crop face predicted albedo ùê¥crop predicted albedo ùê¥crop face crop output u net like network details provided struggling understand setup since paper using ground truth predicted albedo inputs loss imagine two scenarios 1 use loss original lsgan paper train discriminator model training seems counterintuitive 2 use l1 l2 distance pretrained discriminator output ground truth prediction really adversarial loss guess experience discriminator usage therefore choose two come something else reasonable common way use gan discriminator loss calculation non gan networks paper sounds like something require deeper explanation
r9hnkd,0,methods like resume chronotron span train single layer spiking neural networks resume chronotron span use stdp like local learning rules implement training algorithm though approach training differently e g span uses gradient descent via spikes transformed analogue signals convolution alpha kernel whereas resume understanding papers read claim suitable training multi layer spiking neural networks used train single layer networks 0 1 entirely clear getting answer proven difficult use local learning rules propagating weight changes across multiple layers impossible 0 ponulak filip kasi≈Ñski andrzej 2011 introduction spiking neural networks information processing learning applications acta neurobiologiae experimentalis 71 409 33 1 kasabov n k 2018 time space spiking neural networks brain inspired artificial intelligence springer series bio neurosystems 1st springer publishing company incorporated isbn 3662577135
qq644r,0,evaluating effectiveness text generation using gpt3 generate text based q dataset data domain specific based data scrapped various internal company sources challenge facing quality output somewhat subjective makes hard improve model output easily able move beyond outputting gibberish something works reasonably well however finding hard evaluating effectiveness minor model changes e g temperature prompt design tweaks dataset etc considering crowd sourcing input colleagues giving model output various tweaks asking score results however obvious limitations wondering techniques people developed make easier fine tune models output subjective quality
ro1ke3,1,make neural network classified fruits
rd2l00,0,1st ever method perform gpu quantization ü§ó hf transformer models 2x faster inference quantization technique significantly accelerate inference replacing high precision tensors lower precision representation way accuracy kept intact close it‚Äôs quite common cpu inference lot less gpu even performance boost significant end end method also added many benchmarks library repo apache 2 licence give idea latency speed x200b roberta base classification mnli latency benchmark batch 32 seq len 256 afaik 3 methods gpu quantization exist 2018 vanilla bert architecture 2 nvidia 1 microsoft none exist architecture roberta electra distillbert deberta etc limiting benefit gpu quantization old models hope project help generalisation quantization nlp ease nlp big models deployment it‚Äôs big deal quantization rarely used gpu unlike cpu requires nvidia tools well known ml practitioners like tensorrt lib wrapped tools transparent final user result model always several times faster vanilla pytorch gpu batch size seq length transformer flavour small base large xx large etc work based recent model qdqbert 2018 vanilla bert supports quantization added nvidia hugging face transformer library weeks ago next generic method developed also implemented lib new model proof concept qdqroberta roberta identical bert course idea extend process transformer architectures many approaches offer different trade offs generalisation accuracy details notebook many things still experimented instance qdqroberta future qdq models roberta source code almost trivial modification good idea patch onnx files like microsoft build scratch graph directly tensorrt like nvidia leverage new pytorch fx interface approaches theory lead result accuracy speed offer different ease use final user vs ease maintainability library maintainer trade offs gpu quantization discussed known nlp please don‚Äôt hesitate comment ask questions improve tuto democratize gpu quantization
ruja9s,0,machine learning wayr reading week 128 place share machine learning research papers journals articles reading week relates researching means elaborate give us insight otherwise could interesting paper read please try provide insight understanding please post things present wiki preferably link arxiv page pdf easily access pdf summary page way around pertinent links previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 121 130 week 1 11 21 31 41 51 61 71 81 91 101 111 121 week 2 12 22 32 42 52 62 72 82 92 102 112 122 week 3 13 23 33 43 53 63 73 83 93 103 113 123 week 4 14 24 34 44 54 64 74 84 94 104 114 124 week 5 15 25 35 45 55 65 75 85 95 105 115 125 week 6 16 26 36 46 56 66 76 86 96 106 116 126 week 7 17 27 37 47 57 67 77 87 97 107 117 127 week 8 18 28 38 48 58 68 78 88 98 108 118 week 9 19 29 39 49 59 69 79 89 99 109 119 week 10 20 30 40 50 60 70 80 90 100 110 120 upvoted papers two weeks ago u catalyzex code bot paper link u rakshith291 u rakshith291 besides rules fun
ra1vjy,0,artist seeking learn high res image synthesis hey r machinelearning photographer artist preparing small gallery show february experimenting vqgan clip taming transformers well image scraping tools like flickr scraper likes hopes achieve generative images based famous artworks hoping kind folks sub provide little guidance want produce ai generated photo realistic images source images well known photographs tried creating image segmentations feed taming transformers appears handle photographs involving people well landscape images way get somewhat photo realistic images generated ai
qyvuzj,0,daily working routine machine learning engineer hey r machinelearning currently studying business informatics germany next year finish bachelor want master degree study bachelor part time software company already practical experience field interest focused machine learning artificially intelligence general find fascinating computer learn data predict outcomes already 2 semesters data science team company mostly statistically analysis liked tasks general daily work focussed creating presentations business people really technical part currently team wich works natural language understanding problems infrastructure tasks e g docker openshift deploying models building api wich communicate models really like technical part ml related usually last semester work team going work bachelors current plan go team wich solves vision problems e g identification loose screws trains thought position ml engineer would fulfillment recently doubts 1 scared day consists annotating data 2 want think implement different models see wich best 3 want creative e g implement things follow instructions implement model x 4 also scared like tasks change different path ml specialized really liked summary ml researchers positions know hard get research position germany general open minded phd ml field know hard going comparison current study think currently one best year uni really hardest guys experience ml engineer take away fears really hard land position ml researcher go different career path ml fast living
rrxzyy,0,machine learning use cases telecom industry apart churn prediction customer segmentation anomaly detection data science used telecommunications sector aware industry research use cases field
qxz09z,0,embodiment help ml heard question embodiment necessary claim symbol grounding occurs system body therefore needed claim movement real reason brains claim seems view prevalent machine learning community ml practitioners claim statistics data alone get us novel argument favor embodiment specifically aimed ml statistics community claim embodiment allows conducting statistical experiments opposed performing observations creating conducting statistical experiments one control conditions experiments dramatically speeds learning layman terms one hit corner cases might never reveal something observed without ability modify experiment example observing coin smooth building lobby floor people kicking versus ability conduct experiment flipping coin think argument arguments embodiment
rgw4fr,1,class embeddings vit trying implement vision transformer pytorch confusions regarding learnable class embeddings x200b 1 exactly embedding learn corresponding label image 2 pass class embedding related outputs transformer encoder mlp 3 would better get linear projection transformer encoder output input mlp ditch class embedding entirely 4 finally exactly implement seen blog posts using nn parameter class pytorch understand defines random vector weights learnt training proceeds correct way x200b thanks alot
rgxiav,0,state art online deep reinforcement learning algorithm continuous action spaces hi current state art online deep reinforcement learning algorithm continuous action spaces looking particularly one open source implementation download github also anyone know muesli algorithm muesli combining improvements policy optimization hessel et al 2021 open source implementation yet thank
qw2c3p,0,bert future image pretraining bytedance team‚Äôs bert like pretrained vision transformer ibot achieves new sotas research team bytedance johns hopkins university shanghai jiao tong university uc santa cruz seeks apply proven technique masked language modelling training better vision transformers presenting ibot image bert pretraining online tokenizer self supervised framework performs masked prediction online tokenizer quick read bert future image pretraining bytedance team‚Äôs bert like pretrained vision transformer ibot achieves new sotas paper ibot image bert pre training online tokenizer arxiv
r29dv7,1,analysis micro macro data trying run analyses data somewhat peculiar structure hoping someone sub help identify relevant resources nested data approximately 10 000 repeated measurements nested within 400 individuals individuals one observation whereas others hundreds repeated measurements weird part outcome binary outcome variable level individual e 10000 rows data trying predict values varies across 400 unique individuals initial research suggested make sure test train split prevents one individual contributing observations train test data tried random forest using 10000 rows obtain near perfect prediction training set auc near 1 training set auc 60 remained true iterating varied hyperparameter values note creating test train split ignoring dependence observations leads great performance across datasets trust one bit next tried average features data contained one row per person heart breaking see data disappear get random forest performs test training sets auc 60 want continue experimenting models perhaps feature engineering see improve prediction metrics however sure approach prioritize experimenting evaluating whether model generalizes independent cases feels absolutely right would want model used ever rolled real world getting similar performance estimates train test data also make think anything learn tuning generalize training set however wonder repeated data compute means person feature also iccs place features would suggest features much individual level variability capture reliably able find paper suggests latent variable approach data structure anyone sub aware similar ressource pertaining applying machine learning comparable data structure surely must something failing find even reading paper seems successfully deal issue would helpful thank advance oh happy thanksgiving ü¶É
qwwjdv,0,research groups interested work deep learning combinatorial optimization published paper peer reviewed journal titled neural knapsack neural solver knapsack problem article want phd topic europe canada find research group supervisor would interested projects quite good knowledge deep learning applied fields computer vision one help
r0jl0p,0,anyone heard ai foundation nlp work curious nlp works seem like sort language personalization individual level looking virtual beings demo look way advanced big tech companies unless extremely cherry picked example would like believe cherry picked marketing stunt also find papers technical details sure x200b edit omg misspelled title
rbn0fh,0,ivis dimensionality reduction large datasets using siamese networks
qmq7hb,0,way gan generate arbitrary length time series signal hello working using gan generate signals viewed related works found merely sample latent vector distribution fixed size e g latent vector dim 64 upsampling operation get signals fixed window size e g signal 4s 256hz 1024 points want get rid annoying limit fixed window size able generate continuous signal arbitrary length tried code designed gan framework generator takes input arbitrary length vector outputs signal length input upsampling process thrown away generator merely modification input signal instead upsampling discriminator use global average pooling replace linear layers however code failed work think maybe need new ideas come guys help know paper might helpful good idea thanks
qo8wvt,0,necessary manually label desired objects image object detection example dataset thousands images 5 different objects object images already stored different folders train model give image input contains 5 objects want model detect 5 objects image draw bounding boxes around able preparing dataset label desired object image drawing boxes around train model better way manually labelling data thousands images
rhy0x6,1,nlp model document length need advice basics around nlp created model python predict political ideology author based upon tweet 3000 single tweets per author total dataset around 300k model logisticregression utilizing scikit learn doc length effect accuracy trained model e trained utilizing standard length tweet happens feed something much smaller much larger model hold regardless size document excel utilizing similar length trying wrap brain around concept general really related single model thanks
rg4r3l,0,decrease response time meanshift algorithem sklearn currently using m1 macbook pro want use meanshift clustering algorithem segement image takes like 2 3 seconds give output anyway reduce time
qsl5jj,0,opinion recent paper buggy resizing libraries misleading recent paper buggy resizing libraries surprising subtleties fid calculation claims image downsampling methods opencv tensorflow pytorch buggy therefore pil used instead fid estimation corresponding twitter post quite popular last week believe claim main figure paper misleading issue caused aliasing fixed simply setting right parameters functions used x200b main figure x200b reproduction fixed results bilinear bottom image see reproduced antialiased downsampling results frameworks cases single parameter modification enough mitigate issue shared code complete opinion repository though believe discussion whether antialiasing default image libraries valuable opinion none methods buggy paper presents issue sensationalist way edit found another thorough investigation work highly recommend checking blogpost
rtrbso,0,neuron outputs weights
rjde7a,1,ppml series 2 federated optimization algorithms fedsgd fedavg second post privacy preserving machine learning post talks optimization algorithms used train machine learning models federated learning setting fedavg prominent algorithm came 2016 still used today wrote bit earlier twitter thread read ppml series 2 federated optimization algorithms fedsgd fedavg
r1bd7l,1,data problem unsupervised learning tsdae sentence transformers hi put together article video covering tsdae fine tuning sentence transformer models basically use plain unstructured text data fine tune sentence transformer quite data close tsdae paper actually need something like 10 100k sentences fine tune pretrained transformer producing pretty good sentence embeddings achieving stsb evaluation tsdae train bert getting nli labeled dataset trained bert using softmax loss pretty cool imo although reality supervised methods produce better performing models labeled data unsupervised way go really cool learning planning unsupervised sentence transformers future let know think
rjhlot,0,help find book arxiv going insane edit found thanks much u fni19wym definitely better brain book nicely titled algebra topology differential calculus optimization theory computer science machine learning jean gallier jocelyn quaintance adding 1958 pages original post lost hard drive months ago book whose table contents recall liking looking forward read find remember book written mathematician hefty volume least 800 900 pages mathematics machine learning deep learning tbh dont remember one possible titled way havent able find using terms know mathematicians elements analysis algebra study universal approximators noted title posted arxiv published far recall kind book senior professors write summarizing lots stuff book rigorous think starts way back analysis even set theory spends like 500 pages touching ground author french mathematician think although based us know link book posted also hacker news obvious notes professor mr lecun book mml bengio et al one racking brain day nothing maybe one guys provide relief reserve right wrong details know brain tricky bastard thanks
r1sdpa,0,possible work independent contractor ml finishing phd nlp academia work thinking going self employed consultant giving courses etc really value flexibility independence money see 9 6 position know lot people self employed tech mostly web mobile development feasible work independent contractor ml would advise edit apparently looking freelance job independent contractor one
r6trwq,1,python library best counting objects using edge detection wanting create program count number objects touching example number boxes tightly packed together counting box suggestions library use break project would appreciated thinking tensorflow opencv cheers
qydn0p,0,pip package managed ml training aws spot instances deep learning expensive even aws gcp credits quickly run built tool make training cheaper credits longer spotml command line tool automatically manages ml training aws spot instances 3x cheaper lets handle spot interruptions resuming training using latest checkpoint documentation link try looking feedback early testers would ideal candidate side project spending money train acknowledgement spotml built top existing open source library spotty
qkxatt,0,aaai fasttrack 2021 review results good luck everyone results gonna soon aaai 2022
rw50hg,0,deep learning future gaming hey everybody know hard core ai research thinking lot deep learning gaming recently put together little presentation see things unfolding lots cool research featured video go 1 photorealistic neural rendering 2 deepfakes gaming better example obama one used 3 gan theft auto dreaming game engines neural networks 4 large language models building realistic npcs storytelling 5 using openai codex automatically program games really clear deep learning important technology impact gaming since advent 3d graphics would love talk anybody working stuff space
revtrf,1,best youtube channel learn machine learning learned python basics want learn ml please recommend best yt channel course
r2trr7,1,best way learn ai ml help current path hey guys months coding journey know python rather well midway first course fast ai career goal able work develop artificial intelligence however need job soon possible since know master artificial intelligence know likely need get similar field work skills question learn order get job close field possible ai ml pretty sure data science far away ai however sure barrier entry close high ai thinking taking courses sql possibly software engineering resume unsure correct route take know many much experience field computer science whole would thus love hear guys thanks advance
rd5uti,1,resources learning materials learning machine learning
qx5mjd,0,reference letter dossier universities ml phd programs accept 3 references lined applying phd programs ml one references throws wrench plans sending one place says one former supervisors suggests use interfolio reference letter dossier send applying cmu mit stanford mila cornell uoftoronto uofillinois interfolio dossier system sends schools
r86c1r,1,resources explain build neural network scratch need resources assist building neural network scratch
r4eumq,1,unable download casia irisv4 dataset hello everyone project train model iris dataset problem downloading casia iris v4 download seem must sign get error link could someone guide download thanks
r718bt,1,build job recommender scratch using networkx streamlit hey guys check newest project built job recommender scratch using scrapy networkx streamlit looks like public code tutorial github project page huynhnhathao job recommender job recommender system using graph based data representation hybrid recommender system algorithm github com demo link ultimate jobs recommender ¬∑ streamlit job recommender
rovtsh,1,research 2021 looking interesting ml papers read break new year curated list made video explanation short read paper code best ai papers 2021 clear video demo short read paper code depth blog article full list github short recap video
ro7xei,0,paper explained ‚Äì linear algebra transformers would one build transformer solve linear algebra problems numpy linalg paper charton fran√ßois linear algebra transformers video abstract applications transformers mathematics integration theorem proving focus symbolic computation paper show transformers trained perform numerical calculations high accuracy consider problems linear algebra matrix transposition addition multiplication eigenvalues vectors singular value decomposition inversion training small transformers six layers datasets random matrices achieve high accuracies 90 problems also show trained models generalize training distribution domain accuracy greatly improved working diverse datasets particular training matrices non independent identically distributed coefficients finally show shot learning leveraged train models solve larger problems check video find f charton trained transformers solve linear algebra problems transformers work video outline 00 00 linear algebra transformers 00 41 weights biases sponsor 02 21 throwing transformers linear algebra cool 08 08 transformers solve linear algebra 09 50 encoding matrices transformers 11 28 training data results 12 43 generalization 16 05 shot learning
qyy2ps,0,aesthetics algorithmic art idea behind research created thousands images piece taking number unique parameters generate enough votes public begin distinguish parameters make good piece art adjust accordingly put together website artvote allows voting thousands images art considered subjective want find truly case hypothesize must aspect piece tailored increase favorability likeability seen large audiences well individual art see website broken multiple categories although pieces may look similar one unique intentionally designed way would easier evaluate small changes impact scores imagine hypothesis correct exists clear path designing piece art even image could large impacts beyond simply generating art advertising market would especially benefit serve precise ads visually appealing specific user increase click rates
rutxdj,1,machine learning z course udemy enough learn basics machine learning hey guys studying machine learning knowledge regression topics joined udemy course recently offering basic little explanation theory go straight use scikit learn class algorithm regression topics know able join dots lines said previous knowledge believe spend much time math behind feel get explanation youtube video look python class documentation find example docs course enough learn basics need supplement course cover math explanations behind want give negative impressions course completed chapters bit doubtful better supplemented material feeling guy hired course people answer xd
rms1jn,1,check first blog post similar image search built last days using pytorch hi ml community wrote first blog ever describing project built explaining code would appreciate lot could read provide feedback thank
rk77ho,0,build binary classification model imbalanced dataset performs better naive model always predict majority class working building binary classification model imbalanced 95 majority class 0 dataset model ultimately evaluated similarly imbalanced test set goal highest overall accuracy even though understand one typically use accuracy evaluation imbalanced dataset best models far never predict minority class 1 giving 95 accuracy lost make higher given models performing well naive model predicts 0 time training set include minority class e g smote sampling even model ultimately evaluated similarly skewed test set using different model currently logistic regression random forest work best model instead output probabilities set threshold make predictions resources way python change loss function punish model getting minority class wrong
rm3cxf,1,would like ask question approach ai project good morning x200b would like design artificial intelligence able improve performance task another ai tell mistakes x200b concretely scenario would 3 intelligences capable playing chess x200b intelligence 1 would know play chess learn communicating intelligence 3 intelligence 2 would learn way better intelligence 1 intelligence 3 superior intelligence 2 every time finishes game tells intelligence 1 detected wrong x200b ias speak human language project able communicate strip ajdfgjjagfasdf asdfwgasryh tuds works even understand x200b problem know practically nothing ml know angle approach problem although experience programming fact intelligence number 2 would done minmax algorithm x200b point keep mind important thing intelligences end chess masters point design teacher student communication circuit later extrapolated problems felices fiestas
r1667s,1,possible gather user info using javascript changed major information security year teacher gave assignment really know started learning python year assignment gathering user information website like users interests better javascript possible task javascript sorry english bad live europe
r4s0wx,0,aaai 2022 paper results aaai 2022 outcomes released outcomes papers
r6cobf,1,üíäyour daily dose machine learning tensorflow c api series posts post almost daily call ‚Äúyour daily dose machine learning‚Äù last time shared used opencv deploy tensorflow models c environment today want share experience deploying deep learning models using tensorflow c api tensorflow built using c offers api make relatively easier deploy models even train models wish c sounds great start trying implement program uses api developer know documentation important tensorflow‚Äôs c api limited it‚Äôs hard find information you‚Äôre looking reading documentation secondly support windows 32bits system you‚Äôre facing wall it‚Äôs better try looking options great thing api don‚Äôt worry compatibility model trained python c api especially you‚Äôre using version tensorflow python c i‚Äôve personally deployed image classification models object detection models using api apart limiting factors mention models worked exactly expected last option personally tried comes deploying tensorflow models c onnx runtime upcoming post follow favorite social network
rtw69p,1,use features type float linear regression machine learning model currently working basic machine learning project revolved around predicting house prices given several different features house features type float instead type int examples bathrooms 1 5 floors houses 1 5 2 5 floors dataset bedrooms 1 5 2 5 dataset x200b looking similar projects online came across remember essential change float types integer types linear regression supported integer type variables converted using ‚Äòastype‚Äô function python x200b finally question convert floats integers linear regression machine learning model use floats want use floats feel like converting int type lose lot important data ex 1 bedroom house 1 5 bedroom house conversion
rkseqq,1,optimal number cart trees random forest hi guys x200b part trying learn machine learning stumbled upon question determine optimal number cart trees ranging 1000 10000 increments 1000 1000 2000 3000 question part random forest model question hard time figuring thought cart entirely different model trees determine optimal amount cart trees random forest
qvh045,0,risks relying inter rater reliability ml data labeling anyone else feel like irr often relied heavily assessing data quality many ml use cases involve highly subjective tasks like content moderation sentiment analysis etc think need reconsider think relationship irr data quality recently wrote blog subject curious community thoughts matter examples instances reliance irr caused problems road
rwnzi9,1,intutive source probability studied probability college really bad course searched sources learn found tens sources really intuitive apply equation suggestions courses explain going behind scene
raqkuf,1,want learn ml long offline plane flight suggestions hi long plane flight 12 hours coming interested machine learning web developer also familiar python quite bit unfortunately airline flying even offer internet plane makes hard relevant work familiar ml environment know people going use google colab notebooks running ml code guessing going option without internet planning download entire udemy course pytorch tensorflow something prepare everything churn course offline even reasonable thing try anyone ever try far get ml code offline someone suggest good course go materials offline would prepare laptop python relevant libraries m1 mac power outlet good whatever without internet appreciate help tldr download full udemy course pytorch tensorflow go without internet
qr44bi,0,trained discriminator gan used multi class classification thinking using discriminator classifier trained gan idea make discriminator classifier discriminator says fake real leads please
r6dcj3,1,exercises collection think best way learn otherwise stick anywhere online list machine learning exercises ideally example code solutions found far following various textbooks rarely code data hard get sometimes coursera mit courses homework usually 1 2 easy problems per chapter hackerrank many problems community solutions well structured often poorly curated
rfaz1q,0,choose correct lambda values loss function choose correct loss lambda value losses scale lambda selection process try different lambdas using search algorithm random search grid search eth wonder work deep learning models use lambdas know exact intuition
rh1jwm,0,ai generated pokemon made using finetuned rudall e model test end see would happen tried finetune rudall e specific domain relatively low sample size 900 images results much better expected ideal goal see finetuned model could preserve rudall e ability write prompts unfortunately tough balance
ru6fvr,1,trying get ml specific project need help getting started hi worked ml simple classifications particle physics realm want go bit beyond chosen something relating hobby mine hobby 5v5 video game end goal something feed one team model give team best chance winning first start want model classify team1 chances winning based trajectories train trajectories game map label round already sure best build model trajectory data 5 players team1 x z position first 20 seconds game also dont want model care players causes trajectory model treat player 1 trajectory1 player2 trajectory 2 player1 traj 2 player2 trayj 1 would great someone could point ressources allow understand exactly build model first step
rcix6r,1,community databricks longer available access community edition every time try sign get taken towards paid version
r0g2h0,0,microsoft‚Äôs debertav3 uses electra style pretraining gradient disentangled embedding sharing boost deberta performance nlu tasks microsoft releases debertav3 improving original deberta model using electra style pretraining gradient disentangled embedding sharing achieve better pretraining efficiency significant performance jump quick read microsoft‚Äôs debertav3 uses electra style pretraining gradient disentangled embedding sharing boost deberta performance nlu tasks code available project‚Äôs github paper debertav3 improving deberta using electra style pre training gradient disentangled embedding sharing arxiv
qkdfwe,0,tars task aware representation sentences generic text classification paper summary state art approaches text classification leverage transformer architecture linear layer top outputs class distribution given prediction problem effective approach suffers conceptual limitations affect utility shot zero shot transfer learning scenarios üî• paper proposes novel formulation text classification addresses limitations paper
rqnjst,0,guys tune hyperparameters single training run takes long time days weeks training large model example pretraining large bert model take weeks guys hyperparameter tuning scenario
qpk1tt,0,landing ai gets 57 million series build data centric mlops platform news data centric mlops tools take cnbc techcrunch
qk79s9,0,physics informed neural network suggestion recommendation hi guys learning physics informed neural network actual research focussing autoencoder however got new field really want get depth knowledge area would recommend related work papers read thanks lot
rie49f,0,scientific literature review generation v0 2 hello everyone developed recently algorithm automatically generate literature review hopefully could useful phds non phds details algorithm thankful remarks cheers
r515bm,1,imerit ml dataops summit ‚Äì techcrunch hear top ai ml leaders facebook ai microsoft cruise ge healthcare others revealing latest trends successfully deploying machine learning data operations imerit ml dataops summit co hosted techcrunch ùêëùêûùê†ùê¢ùê¨ùê≠ùêûùê´ ùêüùê®ùê´ ùêÖùê´ùêûùêû join 1800 attendees dec 2nd gain insights üëâ human loop data labeling critical path achieving widespread production adoption ai applications üëâ overcoming edge cases leveraging high quality proprietary data üëâ leading ai companies scaling data pipeline üëâ data labeling ecosystem future ùêÇùê°ùêûùêúùê§ ùê®ùêÆùê≠ ùê®ùêÆùê´ ùê¨ùê©ùêûùêöùê§ùêûùê´ùê¨ ùêöùêßùêù ùê´ùêûùê†ùê¢ùê¨ùê≠ùêûùê´ ùê≠ùê®ùêùùêöùê≤ mldataopssummit machinelearning robotics ai ml artificialintelligence
r76igz,0,discussion rant us pretend understand transformers see lot people using concept attention without really knowing going inside architecture works rather others put picture attention intensity word dog attending people slap bert kaggle competitions well easy thanks huggingface without really knowing even abbreviation means ask self proclaimed person linkedin say oh works attention masking refuses explain saying searching eli5 like explanations could get trivial description
qotk5u,0,state ai 2021 forth edition read skimmed important information opinion
qo1sdh,0,gptsd transfer trauma human machine back human via machine learning models x200b gptsd series images text created gpt2 explores transfer trauma human machine back human via machine learning models converting human portraits text gpt2 able recreate new text base portraits dream recollections based dream diary vietnam war veteren
qvd8ay,0,must read papers video segmentation summarisation looking read major advances contributions video segmentation summarisation static keyframes extraction dynamic video skimming end important enlightening papers topic recommend read currently reading survey paper apostolidis et al wondering community recommendations must definitely read papers topic please feel free even suggest papers directly address task video segmentation summarisation introduce techniques architectures subsequently applied extensively
rfr6vf,1,looking resources learn ml techniques models forecasting times series data anyone recommendations provide
rvdjaw,1,metric would use i‚Äôm trying write regression algorithm makes predictions based empirical data goal i‚Äôve given need success that‚Äôs explained ‚Äú95 predictions within 15 actual value‚Äù statistics least strong science far‚Ä¶ best way show explain results based criteria
r7b0mv,0,snowball fight multi agent competitive environment ml agents hello thomas simonini hugging face published snowball fight deep reinforcement learning environment made unity ml agents play game try beat agent prefer train scratch download environment first custom environment unity ml agents publicly available working building ecosystem hugging face deep reinforcement learning researchers enthusiasts uses ml agents would love hear feedback demo project
rp1af8,1,need help deciding approach currently working personal project encountered problem keeping trouble deciding approach take cannot reveal whole problem similar one data varying weights lets say 100 people time need predict time weight majority people would certain threshold suggestions approach would great
rrxtnj,1,question ml approaches hello guys bit context currently searching way estimate sex f age slice kid young adult adult old person based full body picture mask 80 success would enough found many approaches giving sufficient result analyzing face without mask based current situation question result masked people obviously terrible question existing repositories solving particular problem beginner believe problem could solved training model e able get working solution learning whole process works
r4oghg,1,recap whole tv series movies using artificial intelligence working college project decide would something like vague idea started taking ai ml andrew yang classes figure anyone help please
rm7kxf,1,data scientist vs machine learning engineer hi everyone know appropriate forum pose question apologies context received offers 2 companies offer 1 data scientist big oil gas corp job profile involves research process mining offer 2 machine learning engineer popular analytics consulting firm profile involves deploying machine learning deep learning models using kubernetes heroku dask etc options choice location offer 2 also providing fully remote workplace option offer 1 pays better leaving aside pay confused one go ahead
qpw3br,0,recursive ml strategies looking ideas use recursive ml strategies possibly utilizing multiple individual models one model uses output another model make accurate predictions example use two sklearn randomforestclassifier models provide simple signal direction stock market first takes n inputs outputs prediction second takes original n inputs plus output first make new prediction provide earth shattering results appears slightly better using one model random forests also provide ability use bag samples could also used curious established methods papers look etc discuss meta recursive strategies get ml models
rxep30,1,hardware good gpu bad computer bad idea heyo bit short money side especially buying gpu costs liver half wanna upgrade hardware limited due vram limitations graphics card generally great thing possess good desktop pc gaming laptop old desktop one 10 years old buying whole beast bit budget right much bad idea mount recent good gpu old computer video game purposes would plain stupid since lot interaction gpu cpu ml since everything loaded gpu left run thought maybe would ok thx help sorry posted wrong place
rjjaht,1,start working dl rl hello first sorry kind post allowed web developer currently trying start journey machine learning engineer specially deep learning reinforcement learning would like freelancer purposes starting ok employee already done 2 courses one formal one via youtube practices since 2 years right competitions www kaggle com would people already working niche recommend successfully insert guidance would appreciated thanks
qjxfu9,0,machine learning wayr reading week 124 place share machine learning research papers journals articles reading week relates researching means elaborate give us insight otherwise could interesting paper read please try provide insight understanding please post things present wiki preferably link arxiv page pdf easily access pdf summary page way around pertinent links previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 121 130 week 1 11 21 31 41 51 61 71 81 91 101 111 121 week 2 12 22 32 42 52 62 72 82 92 102 112 122 week 3 13 23 33 43 53 63 73 83 93 103 113 123 week 4 14 24 34 44 54 64 74 84 94 104 114 week 5 15 25 35 45 55 65 75 85 95 105 115 week 6 16 26 36 46 56 66 76 86 96 106 116 week 7 17 27 37 47 57 67 77 87 97 107 117 week 8 18 28 38 48 58 68 78 88 98 108 118 week 9 19 29 39 49 59 69 79 89 99 109 119 week 10 20 30 40 50 60 70 80 90 100 110 120 upvoted papers two weeks ago u icko patches need u catalyzex code bot paper link besides rules fun
rva1dk,0,academia tends invest engineering infrastructure tweet jackclarksf asks interesting question good paper explains academia tends invest engineering infrastructure
rpfuh7,1,differential geometry topics like manifolds usually used gnn‚Äôs interested using differential geometry project based ml course i‚Äôm next semester need know place start learning type ml whether place gnn‚Äôs think differential geometry really cool topic math want ml project involves math interest also seems like geometry becoming prevalent ml want start learning course i‚Äôm wondering gnn‚Äôs right method start looking i‚Äôm interested using manifolds ml gnn best tool allow analyze information embedded manifold don‚Äôt know right terminology think using manifolds differential geometry project advanced someone doesn‚Äôt know much yet please tell suggestions start appreciated thank advance help
ramg5l,1,gradient checking implementation working likely algorithm trying implement gradient checking java currently used linear regression seem work want check algorithm logistically makes sense checking via debugging methods seem work actual result wrong ran test case find actual vs predicted values running gradient check using x values 1 2 3 values 2 3 4 got 2 84 calculated hand well means algorithm using wrong approximate gradient first weight theta0 would b ax b result got linear regression 9 significantly different however inclined believe linear regression corrected linear regression performs fine data using approximated values gradient checking comes gives inaccurate result debugging code found errors methods used inclined believe error algorithm algorithm using essentially following take weights theta iterate add value epsilon ith weight keeping rest next calculate cost new slightly changed weight vector thing except subtracting epsilon take two values subtract divide 2 epsilon gradient approximations add array repeat process weights weight vector afterwards calculate euclidian distance gradient approximations gradients via gradient descent divide sum euclidian lengths approximation gradients true gradients value return posting link gist linear regression class gradient check method part working notes dapprox calling approximated gradient values dtheta real gradient values traininputs matrix training data column 1s attached trainoutputs simply outputs 1 matrix number data points case simply 2 3 4
rj5rfb,1,need help hello sure silly posting don‚Äôt know anyone guide help posting started working nlp beginning lockdown almost 2 years ago since love working nlp invest countless hours already good understanding data science learned nlp liked time working full time still giving 3 4 hours everyday learn nlp beginning year 3 months non paid freelancing well learned nlp web scraping eventually stage could say know enough guess look freelance full time nlp job finally started looking freelance work fulltime job realized nlp field people look phd ms holders candidates case even graduate thought okay degree show talents might able find right people started working projects posting different social media sites helpful stage feel like giving nlp trying focus python get something heart still wants work nlp sure please help thanks advance
r5ox5x,0,best pretrained model cleaning excel sheet pretrained model perhaps bert something huggingface pass spreadsheet perhaps csv format guesses might want clean it‚Äôs ok it‚Äôs perfect
r8007v,0,improve vector search results i‚Äôve using use msmarco models sbert models apart simple vector embeddings people improve search results bit context using exact nearest neighbour solution seems like tried across queries results seem suboptimal curious know others improving
rug7ce,1,super harsh guide get machine learning date 4 years want know people still believe information relevant need additional changes link know talking x200b super harsh guide machine learning x200b data scientist interested getting career machine learning engineer wanted know focus efforts
rlg44n,1,grab digital copy tensorflow workshop hurry packt published tensorflow workshop grab digital copy feel interested part marketing activities offering free digital copies book return unbiased feedback form reader review get started tensorflow fundamentals build train deep learning models real world data practical exercises challenging activities learn book 1 get grips tensorflow‚Äôs mathematical operations 2 pre process wide variety tabular sequential image data 3 understand purpose usage different deep learning layers 4 perform hyperparameter tuning prevent overfitting training data 5 use pre trained models speed development learning models 6 generate new data based existing patterns using generative models key features understand fundamentals tensors neural networks deep learning discover implement fine tune deep learning models real world datasets build experience confidence hands exercises activities please comment dm details
rv6xf2,1,add existing classes pretrained tensorflow models projects include new classifications started learning train custom object detector tutorial shows use pretrained model detecting hand signals tutorials show similar pipelines use pretrained model detect new things annotated trained upon let say wanted model detect hand signals people need train model people way switch back detection class people already trained
qxdqt0,0,similar open source long list tf like pytorch ecosystem tools found pretty cool page pytorch contains great many libraries pytorch ecosystem tools find similar page tensor flow know please let know top 10 article medium actual longlist contain wide variety options
ret3mw,1,introduction artificial intelligence like share short article artificial intelligence article supposed short introduction subject comments welcome
rtispn,1,good course ml dl big data hi looking course essentially describes big data shows use ml dl models deep understanding ml dl looking course add big data puzzle piece tools preferably course demonstrate work exercises written python mostly use coursera go since teaches technical mathematical background piece code try type course course cover theoretical practical sides topic welcome
qr0rck,0,avoid cpu bottlenecking pytorch training slowed augmentations data loading hello colleague training models workstations noticing bottlenecks leveraging gpus stopping us reaching full performance curious techniques folks use python pytorch fully make use available cpu cores keep gpus saturated data loading data formatting tricks etc firstly systems 1 amd 3950 ryzen 128 gb ram 3x 3090 fe m2 ssds data sets 1 intel i9 10900k 64 gb ram 2x 3090 fe m2 ssds data sets notice systems take amount time per epoch ie get gains 3 gpus vs 2 gpus frustrating things observing cpus systems spike 100 cpu occasion always utilized disk throughput via iotop shows around 50 55 mb max read way ssd speeds surprisingly low gpu usage spikey image nvtop htop systems things using pytorch 1 10 pillow simd latest nvidia ngc containers also use pytorch lighting training follow best practices setting gradient none instead zero grad performance small improvements setting cu dnn auto benchmark true using distributed data parallel accelerator using pinned memory using num workers 8 see behavior low gpu usage without augmentations reduced batch size experiment see issues lie 1 3rd max possible batch size see maybe 10 20 difference performance things observed get intermittent crashing increase num workers 8 noticed gpu 0 3 gpu system sometimes idle would explain performance differences however unclear us may similar issue guess image loading pre processing appear issue entirely sure diagnosing correctly folks getting around issues like pre processing data set somehow storing optimal format relying pillow simd image reading decoding copying tensors good pragmatic guides optimizing training thank
qm6ieq,0,discussion applied machine learning implementation debate oop approach towards data preprocessing python overkill tl dr trying find ways standardise way solve things data science team setting common workflows conventions illustrate case expose probably engineered oop solution preprocessing data oop proposal neither relevant important happy things differently actually apply functional approach working alone main interest trigger conversations towards proper project software architecture patterns best practices among data science community context working data scientist big company trying hard set best practices protocols standardise way things within team ergo changing extensively spread overused jupyter notebook practices start building proper workflow reusable set tools particular idea define common way things workflow protocol 100s projects implementations anyone jump understand whats going way enforced process definition today every data scientist team follows procedural approach taste making sometimes cumbersome non obvious understand going also often times easily executable hardly replicable seen among community recurrent problem eg opinion many data scientist really crossroad data engineering machine learning engineering analytics software development knowing necessarily mastering unless cs background may understand well ml concepts algorithms know inside scikit learn pytorch doubt sometimes lack software development basics really help building something bigger searching general applied machine learning best practices even tons resources general architectures design patterns many areas found clear agreement case closest thing find cookiecutters define general project structure detailed implementation intention example proposed solution preprocessing sake example would like share potential structured solution processing believe may well 75 job case general dask pandas processing routine huge big data pipes may require sort solutions chance ends something people willing debate together find common framework would happy share examples different processes x200b keep mind proposal could perfectly solved functional approach well idea force team use blueprint follow structure protocol even solution may bit engineered blocks meant replicated many times set common agreement always proceed way forced abstract class imo final abstraction seems clear makes easy understand whats happening order things processed etc transformation main pipe also clear shows steps explicitly typical routine 3 well defined steps read parse data transform data export processed data basically etl process could solved functional way even go extra mile following pipes chained methods brilliantly explained clear pipes approach follows parse‚Üítransform‚Üíexport structure level cohesion shows common pattern could defined abstract class class defines bare minimum requirements pipe course always possible extend functionality instance needed defining base class explicitly force cohesive way defining dataprocesspipe pipe naming convention may substituted block avoid later confusion scikit learn pipelines base class contains parse data export data main pipe process methods short defines formal interface describes process block pipe implementation specific implementation former follow processing base import dataprocesspipebase class pipe1 dataprocesspipebase name clean raw files 1 def init self import path export path params self import path import path self export path export path self params params def parse data self pd dataframe df pd read csv self import path return df def export data self df pd dataframe none df csv os path join self export path index false return none def main pipe self df pd dataframe pd dataframe return df dropnan reset index drop true pipe extract name self params extract pipe time datetime self params dt groupby foo sum reset index drop true def process self none df self parse data df self main pipe df self export data df return none approach ins outs clear could one many cases specify imports exports even middle exports main pipe method interface allows use indistinctly pandas dask library choice needed functionality beyond abstractmethods defined implemented note parameters passed yaml json file complete processing pipelines needed implement many dataprocesspipes required also convenient easily executed follows processing pipes import pipe1 pipe2 pipe3 class dataprocesspipeexecutor def init self sorted pipes dict self pipes sorted pipes dict def execute self pipe pipes items pipe process name main params json loads parameters json pipes dict pipe1 pipe1 input1 csv output1 csv params pipe1 pipe2 pipe2 output1 csv output2 csv params pipe2 pipe3 pipe3 input3 csv output2 csv clean1 csv params pipe3 executor dataprocesspipeexecutor pipes dict executor execute conclusion even approach works would like example opens conversations towards proper project software architecture patterns best practices among data science community happy flush idea away better way proposed highly standardised replicable main questions would makes sense whatsoever particular example approach place resource etc guidance people discussing thanks lot advance ps first post published stackoverflow erased cause see define clear question based facts least end would still love see anyone interested share views
r7dbuf,1,laptop better machine learning view poll
rxf7wq,0,mdli ops ‚Äì free conference help make sense mlops landscape disclaimer conference good friend work awesome job community building thought might interest community well sponsor hey r ml might heard mdli ‚Äì short machine deep learning israel independent israeli community 25k members professionals data science ml conference everyone english 14 days register know sometimes free events tend feel like commercials really check agenda talks going ml teams companies like appsflyer bigpanda explaining built internal stacks ml systems also super interesting talk nvidia team talk building supercomputers train ml models one seems crazy awesome also good opportunity listen offerings cool ml startups might give better understanding compare actually care choosing mlops tools organizers probably comments questions opinion every event community organizes really awesome get learn lot really recommend
r8lq9b,1,need learn image recognition interior design project hi everyone devops engineer studied cs college worked linux 10 years know python obviously developer know code basis related topic idea need learn image recognition need able write program identify surfaces close future measure dimensions room application able identify floor walls ceiling big objects door windows table softa etc later able interact wtih texture change color surface type etc using phone vr headset checked youtube research online etc found opencv tensorflow libraries get job done want start smart want follow route use tools certainly takes right track even project turns big one creating full feature app recommendations fellow redditors technology use start look much time effort needed someone like build simple app like
qoqqp2,0,stl file data annotation first time using reddit anyone know software use annotate point cloud stl medical image file aware several softwares dicom files none seem work stl ideally free software willing pay good
rvz50d,0,neural networks using generic gpu framework personal ml project uses cnns two little problems 1 everyone nvidia gpu home included sadly 2 cnn needs trained every time used photo photo style transfer would good framework implement cnn training targeting desktop thought using opengl know using glsl shaders would good fit
rmrifl,0,multi output generation sparsely fixed output trying use generative deep learning synthetically generate certain biological parameters given experimental constraints using gans successful far next step want fix certain parameters output vector experimentally verified values generate rest parameters output vector based features output vector gans obviously coupled complex non linearities changing fixing one propagate outputs well way already tried researched 1 transfer learning trained generator curated training dataset output features want fix already constrained works gan learns fix values certain output features already fixed training data 2 context encoding gans aware context encoding similar tasks already showcased gans used fill missing regions image based rest image extrapolating case hard make direct comparison case analogous image pixels shown rest image masked need inferred could find literature sparsely fixed output anyone aware literature blog tackles similar problem please let know
rudaf0,1,encode sequence using convulutions hi working personal project trying encode fasta sequences input neural network however trouble figuring exactly encode first length sequences varying typical sequence may look like ashhhhhhsytwtgalitpcaaeesklpinalsnsllrhhnmvyattsrsaglrqkkvtfdrlqvlddhyrdvlkemkakastvkakllsveeackltpphsakskfgygakdvrnlsskavnhihsvwkdlledtvtpidttimaknevfcvqpekggrkparlivfpdlgvrvcekmalydvvstlpqvvmgssygfqyspgqrveflvntwkskknpmgfsydtrcfdstvtendirveesiyqccdlapearqaikslterlyiggpltnskgqncgyrrcrasgvlttscgntltcylkasaacraaklqdctmlvngddlvvicesagvqedaaslrafteamtrysappgdppqpeydlelitscssnvsvahdasgkrvyyltrdpttplaraawetarhtpvnswlgniimyaptlwarmilmthffsillaqeqlekaldcqiygacysiepldlpqiierlhglsafslhsyspgeinrvasclrklgvpplrvwrhrarsvrarllsqggraatcgkylfnwavktklkltpipaasqldlsgwfvagysggdiyhslsrarpr x200b essentially ton letters letter representing sort amino acid x200b encode considering using series convolutions however know ideal way x200b ideal method encode sequence like passed neural network specifically im trying use reinforcement learning agent x200b also since amount text fasta sequence may vary trouble figuring exactly deal
r980d7,0,lot projects machine learning allow international students participate heard projects funded nsf industry usually fine dod doe related projects tend require security clearance heard 70 projects ml cv nlp funded nsf industry less 30 dod doe related thus international students participate ml related projects true seems international students participate nsf funded projects usually dod doe funded projects individual u citizen individual apply grant permanent resident serve principal investigator u institution except nsf fellowships statute made citizens nationals lawfully admitted permanent resident aliens united states generally nationality restrictions nsf program proposing institution us may designate principal investigator anyone believes capable fulfilling role
qub6oz,0,tpu v3 8 train big model momory 1 batch 16gb know tpu v3 8 8 16gb v3 128gb memory want know v3 8 train big model 1 batch size 16gb 128gb memory like integrated memory model parallel situation
qkzi4q,0,icra reviewer see names author submitted paper icra forgot put names paper question reviewer still see author names system
qm5nfs,0,neural program generation modulo static analysis mukherjee et al 2021 neurips spotlight nice paper using program analysis learning signal program synthesis author alas paper twitter 1 10
qm06ct,0,iris open source photos platform powered pytorch submission pytorch annual hackathon 2021 self hosted alternative google photos currently contains basic features built short scope hackathon team continuing work adding new features explore section smart search go check support project links youtube devpost github x200b
qsfiw7,0,causal learn causal discovery python excited release python causal learn package causal discovery see package documentation feedback welcome x200b
rekxvo,1,need help suggestions data preprocessing hi currently started thesis masters computer science work upon data preprocessing dataset dataset contains accelerometer g values x z approx 300 000 rows x z values sampling rate 1600 samples per second data collected 3 mins approach reduce number samples using machine learning python thanks
ruy2vw,1,play something netflix spotify work always wondered technology model behind netflix spotify play something button related conventional recommender system gives top result different thing altogether also technology use rl simple multi arm bandit links would really helpful
r8vicl,1,pca explained basic principles blog post arrived optimisation problem pca using basic principles mathematics click link read
rclejv,1,know good gan tutorials hi learned gan mnist dataset need advanced gan tutorials higher image resolution mnist
rgcrfj,0,zenml extensible open source framework create reproducible machine learning pipelines hey everyone wanted share open source tool building munich called zenml zenml extensible open source mlops framework create production ready machine learning pipelines simple flexible syntax cloud tooling agnostic interfaces abstractions catered towards ml workflows think metaflow project similar goals differences approach constructing pipelines also abstraction layers provided underlying infrastructure could feedback us vision goes something like create pipelines deploy stack code course tools look similar zenml focused ml workflows key advantages pipelines data dependent rather task dependent means artifacts flowing pipelines modeled specific way enable features like caching lineage artifacts flowing pipeline steps standardized adding standard validation deployment step standard data model artifacts steps standardized enable effect enable special features certain steps e g distributed training trainer step zenml materialize read write common objects like pandas dataframes pytorch modules automatically regardless environment pipeline running local cloud data scientist use objects natively always sets zenml apart tools like airflow luigi prefect focused data engineering use cases hard implement ml specific tasks developers data scientists point view application infrastructure even advantages pipelines integrations need work across varied environments infrastructure requirements use case notion mlops stack comes happy feedback looking actively contributors create complex mlops stacks within application simply zenml stack spins infrastructure target choosing addition zenml takes care deploying pipelines relevant stack automatically e g try spinning kubeflow based stack local machine simple command zenml build container create kubeflow pipeline run automatically simple command future hope expand include complex deployments think links github star would appreciated docs
qx479l,0,reinforcement learning traffic light control read interesting paper rl traffic lights reinforcement learning benchmarks traffic signal control already worked rl train scheduling traffic lights new says nutshell many people developed rl approaches traffic light tl control many people claimed sota üòÖ paper introduce well defined benchmark using sumo simulator show things considered recent methods perform well simple independent dqn approach best flavor dqn approach happens described first author previous paper sounds interesting first time cleaned benchmark reveals simpler method best along things surprise independent dqn beats everything else surprises methods use cooperative approaches know marl hard still setting complex city would expect traffic lights able communicate outperform although expect converge slowly convergence speed stability look crazy really solving region scale problem 21 tls 4 2k car trips fewer 200 episodes 360 timesteps sound realistic rl land last point want bring attention amazing intas project guy made realistic traffic scenario ingolstadt germany using actual data traffic lights public transportation shop opening hours etc got relevant offices presentation worth watching curious hear experienced practitioners
qkes8a,0,top 7 books boost data driven outlook post cover best 7 books data analysts data analytics books teach power big data ways harness started career software developer switched data science 8 years ago big data software projects difficult predict risky conduct due large volumes unclassified data many types metrics using machine learning data analysis visualization approaches essential facilitating informed decision making throughout software development testing process mastering data analysis one challenging experiences life wading tons books figure start methods techniques use particular case extremely daunting time consuming studying data analytics time choosing right educational resources crucial launching advancing career within area 1 storytelling data data visualization guide business professionals data analyst aim retrieve data also make intelligible requires able present data certain way however presenting data imply dragging dropping data fields chart entails creating meaningful visual representation data book based real life scenarios give idea difference colorful visualization intelligent visualization explaining closely examine line color visual interface book provides excellent guidance examines criteria presents examples properly deal data 2 mastering tableau 2021 implement advanced business intelligence techniques analytics tableau 3rd edition business analytics practitioner search publications simplify complicated topics manner everyone understand book contains several tips techniques assist understanding utilize particular chart styles data granularity sort presentation end user begin fascinating trip learning essential strategies using sophisticated math tackle challenging situations strategies involve inventive use several sorts computations row level aggregate level others besides get concise instructions using tableau solve practically data visualization problem knowing tool inner workings thinking creatively possibilities expanded capabilities reading book equipped arsenal advanced chart types methods allow display information range audiences effective engaging manner using clear efficient engaging dashboards explanations examples effective inefficient visualization approaches well planned badly created dashboards compromise choices tableau users embrace data visualization expand knowledge tableau get powerful tool 3 machine learning elastic stack second edition book one kind resource users using elastic search actual case focuses substantial growth machine learning technology elastic search providing actual case studies extensive explanation book similar one one conversation subject matter expert need refresh practical skills machine learning book offers examples apply elastic ml environment get valuable insight data turn machine learning static intelligent want understand build tasks also tap underlying models variables machine learning elastic stack ideal option 4 data analytics made easy analyze present data make informed decisions without writing code data literacy important component data driven mindset book excellent resource data science students looking obtain practical information learn apply analytical skills author excellent job introducing readers knime low code data analytics framework allows instantly evaluate data furthermore presentation machine learning user friendly emphasis theoretical knowledge handling variety use cases significantly de mauro assists readers comprehending significance becoming great data presenter vital talent cultivate order influence decision making 5 fundamentals machine learning predictive data analytics second edition algorithms worked examples case studies fundamentals machine learning predictive data analytics detailed analysis important machine learning methods used predictive data analytics encompassing theoretical principles actual implementations technical mathematical knowledge complemented instructional practical examples case studies show models may employed wider business setting following description journey extracting data gaining insights making prediction book delves essential machine learning techniques data based learning correlation based learning probability error based learning strategies starts tech description core principle followed quantitative models algorithms demonstrated extensive practical examples authors discuss procedures straightforward succinct way without referring specific programming frameworks languages fantastic job introducing main concepts diving deeper complexities logic math underpinning algorithms 6 analytics stories using data make good things happen analytics stories make good things happen serious intelligent entertaining look analytics tackle real world problems situations analytics stories fills gap data analytics particular challenges solves topics ranging sports finance politics healthcare commerce author outstanding job conveying notion data storytelling reader develops around 50 business cases topics ranging education sports dr winston mostly utilized ms excel interpret analyze display successfully convey data 7 data pipelines pocket reference moving processing data analytics data science pipeline set procedures transform raw data meaningful business responses data science pipelines streamline data validation extract transform load machine learning modeling revision implementation crucial data analytics success difference data truly deriving value moving data various sources processing create context helpful reference describes common pipeline failures key decision factors like batches vs streaming data input building vs purchasing book delves fundamental concepts apply open source systems consumer applications homegrown solutions well common decisions made experts data pipelines pocket reference precious resource everyday problems activities likely encounter work data analysis related field assist making data driven decisions many years come conclusion thorough grasp data analytics knowing gain actionable data driven insights essential successful career data science anyone interested expanding knowledge data analytics benefit books mentioned article since provide recent industry information illustrated examples best practices
rpctc4,0,crowdsourced evaluation model good idea good idea evaluate model letting users grade model scale might vary problem problem basis correctness believe may subjective cases
r5a6vx,0,paper explained ext5 towards extreme multi task scaling transfer learning video walkthrough t5 model staple nlp research last years size approach formulate nlp tasks prompt based language modeling make convenient choice tackle new challenges provides strong baseline current datasets ext5 pushes t5 limits pre training self supervised mask filling also time 107 different supervised nlp tasks new exmix dataset resulting model compares favorably t5 fine tuned downstream tasks x200b outline 0 00 intro overview 2 15 recap t5 model 3 55 ext5 model task formulations 8 10 exmix dataset 9 35 different tasks help 16 50 tasks include 20 30 pre training vs pre finetuning 23 00 hypotheses going 27 20 much self supervised data use 34 15 experimental results 38 40 conclusion summary x200b paper
r3g7kd,0,looking sponsor functional programming language new ai hardware 20s proceed novel ai hardware replace gpus arrive scene looks things python c seems like dominant combination programming huge pity could better languages created 80s 70s programming hardware created 2020s trying change destiny spiral apart special features control inlining specialization make suitable hardware dynamic memory allocation capability like gpus spiral quite similar languages like f ocaml static typing global type inference first class functions records tuples unions competent functional language would unlike toy languages well done language server want try language simple installing plugin vs code marketplace 3 years full time work went want make backends novel ai chips cost money restricted availability might able get even money various companies position utilizing want something better old poorly designed languages kind work consider sponsoring spiral least need access chips order make backend would really waste skills spent next years things spiral languished background believe made back late 00s would become dominant language programming gpus writing ml libraries background see resume master functional programming researching poker rl agents since early 2021 spiral good enough first release quite good implementing ml papers scratch written whole gpu based deep learning based library past previous iteration spiral making rl agents poker game served well try new language debug pretty much tried failed full holdem make work need significantly larger batch sizes would make training take long gtx 970 matter good enough algorithms enough compute broadly trying make agent gambling game massive parallelism afforded chips could great game ai general could allow simulating games large number independently acting agents beyond deep learning better ml algorithms kind brain uses would also allow approximately storing large amounts procedurally generated world data would infeasible current hardware algos get long ml frameworks like pytorch tensorflow obsolete new ones written current time like try various things chips implement holdem game directly see whether anticipated 100 1000x speedups enough actually enough make current algorithms tractable toy games since current algorithms trash try evolve something better aid chips right would even think trying gpu novel hardware might make kind brutish research tractable dumb go beyond backprop directly might able figure principles given algorithm front unsupervised learning making art music spent 6 5 years unpaid work mostly around ml like create something tangible like game others could play next project definitely tired old approach course would done spiral make work least need create ref counting c backend ai chip probably python backend connects would hard going game development current path order create assets without help ml learning 3d sculpt draw move learning musical composition done hand would leave much regret really working things related ai trying game dev feels much like cope able hack rl still lingering attachment old path offer community thus better way utilizing future hardware interested supporting kind work
qqopdj,0,google automl prices trying understand google automl pricing three questions 1 price forecasting 2 much pay endpoint available 24 7 post data execute previously trained model assuming simple numerical data classification 3 upload model ready predictions thanks
r23jpj,1,generating music aws questions want upload mp3 files receive mp3 outputs looking something different aws deepcomposer creates song output based input melody difference want upload dataset many songs deepcomposer seems keyboard design melody single song point right direction guides tools recommend ai ml models 550 files total size 12 5 gb aws requirement use credits
r5rxbr,0,anybody tried using openai gym automate using gui application something simple like ‚Äúcreate new macro excel ‚Äù pretrained model translate natural language mouse actions image desktop
rj7ivc,0,different results model every time train model parameters following model facing issue results every time run model get different results train test data remains every run give details questions let know note data 1 data 2 correlation reason using x200b
retq7g,1,live nsfw detection possible playing video program running background processing screen every time changes detect every time something nsfw shows possible quick possible would approach task
r7f3h3,0,aaai rejected paper despite accept reviewer metareview scores submitted paper aaai obtained accept weak accept scores metareview also positive recommending acceptance paper despite paper rejected without reason contacted organizers obtained default response borderline weak accept scores typically directly translate accept decision unfortunately great deal inter reviewer variability across papers required calibration negative remarks associated borderline rejects reviewers comparable accepts others meta reviews less informative others failed capture totality reviewer comments ac often weighed spc recommendations associate chairs many cases rebuttal included additional details experiments included paper unfortunately given challenges conference review option ‚Äúaccept subject revision ‚Äù given scale aaai timeline involved possible reconsider decisions hope reviews useful revising paper submission another top tier conference near future unfortunately two scores weak accepts rather full accepts two scores scores weak accepts situtation submit published papers neurips icml iclr past never terrible experience aaai even top tier conference feels extremely shoddily organized
rjwwfi,0,advice training full imagenet project need train full imagenet scratch it‚Äôs taking ages i‚Äôm struggling get good accuracy advice general approach directions follow parameter choices would greatly appreciated setup pytorch vgg16 ilsvrc2012 imagenet batch size 300 optimizer sgd lr 0 001 momentum 0 9 weight decay 0 006 num workers dataloader 12 pin memory dataloader true using nn dataparallel five blocks vgg16 hardware cluster rtx6000 using 4 x200b getting many cuda oom errors hence choice small batch particular num workers highest seemed work generally cluster i‚Äôm seems affect training many jobs running terms parameters i‚Äôve heard adam much better job optimizer compared sgd i‚Äôm going try next also found online pytorch‚Äôs distributeddataparallel seemed like useful difficult implement parallelization option i‚Äôm sure manageable time frame project training looks like moment ps newbie ml obvious already lol
rahy9m,0,mahalanobis distance ood detection reading paper core idea given test sample x set classes c compute score x maximum c negative class specific mahalanobis distances struggle understand compute auroc score using distance ground truth 1 distribution 0 distribution compute auroc x e g 639 2
rl0rmt,1,pruning reinforcement learning hello seminar university witch pruning nns want spezialice pruning reinforcement learning read five important papers found something else witch relevant interesting papers theme
r6ns57,1,colab pro p100s recenetly upgraded colab pro title states received p100s past week since upgraded wondering common busy common issue might well upgraded even though knew getting v100 a100 still garunteed wish recieved unluckiest colab user
rchzpy,0,guys thinks ai dream app u think created tools resources used app take create text video generation app
qqqzmu,0,advice buying pc phd ml decided need better set computationally currently running scripts either locally dell xps 13 gpu colab evidently insufficient budget phd studentship equipment thinking buying desktop ssh laptop anyone experience recommendations e g gpu cpu specs info area research yet set though definitely working generative models though huge models tabular time series data work often involves data preprocessing steps might mainly require cpu power fixed budget though need justify purchase seems buying moderately good gaming pc nvidia gpu financially computationally interesting help appreciated x200b edit budget wise thinking 2000¬£ able justify funding body limiting please let know
rppsrl,0,new cppe 5 dataset paper introduces cppe 5 medical personal protective equipment dataset new challenging image dataset goal allow study subordinate categorization medical ppe gloves masks coveralls face shields goggles unlike existing dataset furthermore easily get started use dataset tutorials data loaders code repository use one among models model zoo dataset supporting links code repository paper
r1h9ld,1,stackoverflow analysis hello everyone cloud data science noob personal project want analysis stackoverflow posts topic modeling hopefully identifying useful trends evolved since 2015 see something called sotorrent hosted bigquery would want query posts store somewhere 100gb cannot store locally don‚Äôt enough storage laptop question could store accessible colab notebook run python code analysis ideas pointers even suggestion alternatives described flow would super helpful
r9io6e,0,simple questions thread please post questions instead creating new thread encourage others create new posts questions post instead thread stay alive next one keep posting date title thanks everyone answering questions previous thread
qquog8,0,good advertising papers using rl dl hey everybody currently working adtech searching innovative products advertising ctr prediction digital inventory pricing online learning costumer segmentation stuff help please suggest good paper comments able find good papers subjects looked fb research arxiv mainly
r5lf8h,1,reward enough hypothesis hi guys video made explaining reward enough paper deepmind david silver posit reinforcement learning right paradigm development general artificial intelligence hope enjoy
rcuyl8,1,good start tts development custom speech based recordings
rjzi1g,1,someone educate machine learning got wrong answer following beam programming concepts also created memory data inputs outputs particular step pipeline 1 pcollections 2 ptransform 3 pipeline 4 transforms b following scripts included train cloud mle 1 distributed training sh 2 hyper tune sh 3 single instance training sh 4 create prediction service sh 5 none c following scripts needed make prediction service using scikit learn 1 export job name 2 export model name 3 export gcs file dir 4 export test file 5 none following bi products partner google cloud ml engine 1 tableau 2 power bi 3 looker 4 none e compute service lets customers supply chunks code get run demand response events infrastructure wholly managed google 1 cloud functions 2 compute engine 3 kubernetes engine 4 app engine point correct answer think also e question thought cloud functions accidentally chose 3 got correct someone enlighten way failed exam would like learn mistakes order better one time exam every time take exam changes question
rg6vp7,1,best plot user flow ok let specific working analysis chatbot data bot dialogs contents represented ids company wants see routes accessed users already information lead data scientist said would nice plot flowchart kind way told look sankey diagram read think best way requires information example id routes x200b acessed ids 7 43 342 78 92 33 7 123 56 73 23 22 7 89 76 125 48 77 7 89 76 125 48 77 see stored lists idea would plot way showed flow user experience sum everything 1 way using sankey diagram 2 better way edit btw using python
qmqhgg,0,stylegan3 wav2lip x200b due limit compute quality suffers bit
r736yo,0,pls help cv problem guys trying make denoising model specific type noise geological data kinda like diffusion waves looks like image right ideas could model noise slap onto synthetic data make denoising model maybe know dataset like open source denoising model tried 2d fft doesnt really give type noise looking optins details noise real look like kind noise wouldnt
rr6l1h,1,conduct features scaling working classification problem wanted normalize variables conducting machine learning classification algorithms training test variables scaled within range 0 1 wrote following code sc x standardscaler x train2 pd dataframe sc x fit transform x train x train2 columns x train columns values x train2 index x train index values x train x train2 x test2 pd dataframe sc x transform x test x test2 columns x test columns values x test2 index x test index values x test x test2 however got error message typeerror float argument must string number pandas libs interval interval advice problem fixed
rx9kzo,1,incorporate normalization inference dear machine learning practitioners intro mentioned currently training neural network small 350 sample size dataset perform regression would like use trained network webapi accessed users samples however samples come later stage question comes question using pytorch sklearn libraries code normalize dataset looks similar x200b normalize true normalize sc minmaxscaler x train sc fit transform x train sc fit transform x200b save minmaxscaler information apply inference data later stage smoothest way thanks pointers help
reosda,1,use python library instead matlab octave course semester numerical computing professor told us would prefer using matlab use python library instead like numpy something
qr1pex,0,mel spectrum useful non speech recognition classification tasks working long term project involving development process monitoring program additive manufacturing process read good deal different techniques used audio classification speech recognition etc understand machine learning models involving audio either use features extracted time frequency representations signal spectrograms inputs looking audio feature extraction noticed common extract frequency domain features mel spectrum obviously mel spectrum useful speech recognition tasks since intended align human ear perceives sound question mel spectrum useful audio classification anomaly detection tasks involve human speech reason would useful say standard frequency scale spectrum non speech recognition task literature references would helpful
rc60ei,1,sagemaker canvas limits pitfalls short comings getting feet wet using sagemaker canvas fell first hole please ensure dataset least 250 rows missing value even train kaggle titanic everyone age cabin limitations short comings going make allowances
rhliul,1,loan credit score beginner model use hello new machine learning intending start project loan credit score project basically using information users produce credit score grade example information categorical data continuous data e g categorical employment status business owners 1 employed 2 homemaker 3 continuous biz last financial year revenue arranging tiers making different categoricals tier 1 0 100 000 tier 2 100 001 200 000 finally want produce credit score grade credit score employment status biz last financial year revenue many variables want use weighted variables example employment status business owners 100 employed 70 homemaker 50 also biz last financial year revenue tier 1 100 tier 2 70 tier 3 50 final credit score context business owners 1 tier 2 credit score 170 100 1 70 2 x200b 1 go using weighted kind linear regressions reference sites 2 since using lot variables many unnecessary variables lasso regression possible tuning getting job done 3 go improving model ml models considered thank beginner
r8t0g2,0,multi input multi output problem hello recently wondering solve machine learning problem would like discuss ideas solve objective predict list products e g grocery given various input conditions e g size type location store performance store etc output list predicted products meet input criteria might consist multiple products list consist even 100 records dataset explanatory variables around 1 000 due one hot encoding categorical variables response variables currently 1 column 1 000 distinct classes e types products text form thinking perform one hot encoding however drawback 1 000 response variables potential solution train cnn relationship products response variables taking consideration one hot encoding response variable multi class classifier
rl5msp,1,book resource provides high level overview deep learning problems tasks applications simple level title says wondering anyone knows kind book resource explains high level different areas deep learning problems general approaches applications example computer vision graph networks generative learning reinforcement learning something like introduction dummies would preferably nontechnical reading one would understand roughly different kinds tasks field applications obviously cover technical details algorithms names rough intuition applications wondering anyone written book anyone came across resource fits bill cheers
rndy6z,1,ml algo takes account derivative timeseries x200b timeseries data corresponding features trying determine features variables include well best algorithm heard lstm algorithm might best application however wondering aspects signal predicts output want machine learning model take account current position past position current slope current 2nd derivative acceleration ml algos like lstm take account slope derivatives need add slope column feature capture behavior thanks
qyszpw,0,pyconverse conversational text transcript analysis library github project link pyconverse conversation analytics plays increasingly important role shaping great customer experiences across various industries like finance contact centres etc primarily gain deeper understanding customers better serve needs library pyconverse attempt provide tools methods used gain understanding conversations multiple perspectives using various nlp techniques called conversational text nlp primarily contact centre data various domains like financial services banking insurance etc past year come across interesting open source tools help understanding conversational texts decided create library provide various tools methods analyse calls help answer important questions compute important metrics usually people want find conversations contact centre data analysis settings x200b things done library 1 emotion identification 2 empathetic statement identification 3 call segmentation 4 topic identification call segments 5 compute various types speaker attributes word counts number words per utterance negations etc identify periods silence interruptions question identification backchannel identification assess overall nature speaker via linguistic attributes tell speaker talkative verbally fluent informal personal social goal oriented forward future looking focused past identify inhibition please give try share feedback
rn27l0,1,android studio tf lite hey guys anyone experienced using tensor flow lite android studio trained model exported tflite getting error uploading studio meta files included suggestions
rhnfm7,0,neural networks tell whether already seen given example want neural network perform following task given training dataset x 1 x 2 x 3 x n network output yes provided one example similar one provided one example similar examples cannot provided training time therefore network would need learn memorize seen answer anything similar looks like distribution detection task familiar literature reference idea would really appreciated
rqvrwx,0,x mlps highly configurable mlp architecture built jax haiku wanted share new project developed x mlps library provides flexible mlp architectural foundation quickly implement mix match test various sota mlp blocks architectures key pattern used throughout x mlps factory function enabling arbitrary blocks created stacked network built jax haiku initial release implements several mlp blocks use box including resmlp mlp mixer gmlp 2 mlp plan implement several well along qol improvements however primary goal x mlps anyone able create blocks rapidly experiment different approaches code pretty well documented enough info repo learn extend let know questions repo tiny attention yet implemented gmlp
rh8s5l,1,beginner integrate ml web page x200b
qlqls4,0,gnns gcns viable graphs node features unique node ids different deepwalk point started dig gnns first time trouble understanding advantages nlp inspired embedding methods like deepwalk node2vec gnns shine node features handle ids giant one hot vectors well usual input gnns consist vector handcrafted features gnns used directly tasks like link prediction embedding generators models appreciate explanations
rsbm0e,1,cool projects college app finished andrew ng beginner ml course coursera wondering guys project ideas enjoyed making think might interesting challenge willing learn lot really struggle project really idea goes üòÉ thanks
ru3ba4,1,nlp algorithm consider dataset small 250 500 data points want perform text classification identify semantic similarity already tried distilbert text classification model got overfit 99 train accuracy 30 test accuracy currently 250 data points dataset supposed increase time much would 1000s wondering algorithm text classification performs better small datasets
qravhd,0,anyone working code generation openai works code generation models like codex openai seen open source version codex variants model
rrnctu,1,anyone azure associate data scientist certificate experience think would boost resume trying break field
qqssgq,0,new open source vector search solution meet new open source vector search solution vektonn offer opportunity product teams data scientists solve problem reliable vector data storage scalability undisturbed availability store embeddings attributes interesting users since use real world objects example identify objects using real identification support changing indexes new data arrives delete change add data index parallel search queries expand multiple indexes single data source vectors attributes seamlessly transition new versions indexes expand different indexes different parameters data work vectors type example use bag words solve word processing problems load appropriate sparse vectors vektonn appreciate feedback suggestions project welcome github stars join course find interesting üôÇ learn see done
rpa2pm,0,tesla create ml models scratch start using another company services find answer googling thought ask brains trust musk seem like kind person want outsource kind task ownership openai would also helped build initial models said many large firms bought ml companies integrated rather build scratch example apple bought siri nuance communications furthermore mercedes automobile companies hope join self driving future building models outsourcing massive task outside firms democratic position makes sense ensure one company monopolizes navigation optimisation standpoint absolutely would thought google would use waymo create operating system new age car would made perfect sense maps android search services
r6k36f,1,finding big data sets hello newbee trying make elearning ehealth website want big data sets implement machine learning ideas website find data sets easier find data elearning ehealth appreciate machine learning suggestions guys
r7xbw1,0,optimize gradient descent small known problem hello hope posting correct community problem part machine learning figured best help comes gradient descent part hobby project mine photometric stereo take images different lighting conditions calculate shape object need minimize cost function depending 64 measured values variables optimize direction vector scalar value albedo luminance object hundreds millions times within reasonable time find fitting papers addressing need exactly typical photometric stereo added fresnel equation polarized light robustness shadows developed algorithm works reasonably well optimizations found less trying different stuff seeing works already added momentum direction vector part directly solve luminance one iteration hyperparameter optimization also list question properly optimize gradient descent algorithm solves small known problem converge little steps possible least learn also appreciate links research papers lecture slides
r98ajp,0,good pixel image upscaler trying upscale image 256 512px 256px issue pixels lose squareness become obtuse acute deform used srcnn whatever using real esrgan good ml upscaler works well pixel images
rp0t4m,1,stacking models trying stack multiple models using stacking regression sklearn accuracy stacked model better individual models way increase accuracy
qse6gc,0,deepmind‚Äôs one pass imagenet new benchmark resource efficiency deep learning deepmind research team presents one pass imagenet opin problem designed study space compute efficiency deep learning streaming setting constrained data storage develop model training systems example passed system quick read deepmind‚Äôs one pass imagenet new benchmark resource efficiency deep learning paper one pass imagenet arxiv
r84bxk,0,shape generation algorithm hello field looking algorithm shape generation field mechanics want see different shapes structure would affect stress envision instead intuitive design ai come interesting shapes dont know already exist would appreciate kind help thanks
r52qrb,1,transform categorical data numerical data using pandas writing python program uses logistic regression predict outcome based survey data csv however running issue survey data non numerical need transform categorical data numerical data without knowing columns categorical many categories per column ahead time able map numerical data onto category labels later suggestions approach sincerely appreciate thoughts example data weight systolic blood pressure diabetes 155 119 210 131 yes 301 143 yes example output weight systolic blood pressure diabetes 155 119 0 210 131 1 301 143 1 diabetes dict 0 1 yes
rkw26f,0,aws visual interactive introduction explanation double descent hope allowed given arxiv link though derivation second article easily thrown one two articles double descent first introduces concept visual manner second provides explanation linear models relation energy natural cubic splines designed quick consumption understanding
rw401v,1,install caffe framework mac m1 working deep learning system c using caffe installed dependencies brew installed caffe compile make get following ld map file errno 22 file usr local lib caffe architecture arm64 clang error linker command failed exit code 1 use v see invocation make build error 1 sure wrong build incompatible mac m1 help appreciated thanks
rbudur,0,distilroberta 15x less used distilbert according huggingface distilroberta base downloaded 287k times last month vs 3m9 times distilbert pure performance perspective roberta seems better bert case limited hardware capacity looking distilled model wondering 15x people chose distilbert last month compared roberta would chosen otherwise thanks
rgahri,0,virtual mlops round table given turnout last two events great feedback received decided hold another virtual mlops round table december 16th 5 pm pst follow format forming breakout groups 5 7 people letting peer learning discussion flow absolutely selling pitching focus pure peer learning sign interested let know ideas thoughts feedback
ragkis,1,grad probability bayesian book recommendations need prob bayes bioinformatician field deals discrete probability quite bit motifs patterns mutational frequencies normalizations also deal non normal distributions often understanding dl nn often wrong tool job broader foundation needed pytorch fast ai way success looking enrolled ms data science startled little probability emphasized especially bayesian probability book recommendations newcomers bayesian stats right track prefer graduate professional level texts considered added statistical rethinking mcelreath crc machine learning probabilistic perspective pml book murphy applied multivariate stat analysis johnson wichern pearson course bayesian data analysis gelman crc reading list based part post level completed sophomore junior level probability stats biostats linear algebra calculus multivariate analysis coursework uni delaware essentially rehashes undergrad probability graduate course little calc theory thrown without real world applications calculations emphasized total crap imo
ragd4u,1,find good resources using nlp techniques comes processing commands requests user
r246im,1,leave columns training model basic classifier dataset includes 40 columns first two id associated name eliminated non numeric columns classifier uses numeric values prediction eliminated id name column want associate predicted data test set ids names excluded classifier training way either leave ids names data set tell python use training associate back prediction use python pandas sklearn etc classifier logistic regression sklearn
r3j9ps,1,simple hosted mlops auto scaling solutions i‚Äôm looking deploy vqgan clip models pretty large scale 30 gpus want explore options kubernetes aws gcp hosted mlops platform simply upload ml service docker flask platform completely take care scaling gpu provisioning essentially outsource scaling handling traffic product service minimal setup
r6eple,1,november updates data science interview book month november christmas month feels like 2021 almost slipped past us month november nlp section updated missing values section added formatting changes statistics section took break obsessively working üòå new section tree based approaches industry application added launched linkedin page interesting plans near future added support dark theme ü§Ø remove breaking lot stuff wait official support added new problems probability python regression sql added temporary datasets time page sql covering ctes regression section extensively updated forget show project ‚ù§Ô∏è support
r8tnzq,1,batches sequential iteration sequential data hello studying rnns d2l ai book part describe reading data sequentially choice made wondering reason example let assume sequential data follows seq 1 2 32 33 34 iterate data get following values first two batches x tensor 0 1 2 3 4 17 18 19 20 21 first batch x tensor 5 6 7 8 9 22 23 24 25 26 second batch question reason iterating batches x tensor 0 1 2 3 4 5 6 7 8 9 first batch x tensor 10 11 12 13 14 15 16 17 18 19 second batch want read chapter talking check corresponding chapter thank
rvqr95,0,play ai detect fake audio hi everybody phd student interested audio spoofs voice recordings faked help ai developed online game play artificial intelligence try distinguish spoofed real audio recordings fun much supports research partificpation e playing game comments suggestions welcome
qqxcgt,0,microsoft india proposes varuna scalable low cost training massive deep learning models microsoft research india team presents varuna system training massive deep learning models commodity networking eliminates need specialized hyperclusters alleviates cost scale resource utilization challenges deep learning model training quick read microsoft india proposes varuna scalable low cost training massive deep learning models varuna code open sourced available project‚Äôs github paper varuna scalable low cost training massive deep learning models arxiv
re1f4s,1,ppml series 1 introduction federated learning started series privacy preserving machine learning wanted quite long time finally decided start first post short introduction federated learning blog post written detailed version twitter thread check ppml series 1 introduction federated learning
rc44cl,0,best style transfer distributions hi everyone working style transfer project inspired recent results toonification projects based stylegan e g 1 stylegan based papers seen use reference image style transfer one hand great flexible hand great reference image much influence output image example toonification case could change color person hair toon version match reference image even though toon dataset may similar hair color within distrubtion know straightforward approach solve style transfer problem 2 distributions something like cyclegan many architectures influenced however seen none seem good results stylegan based ones thanks replies
qmy3ir,0,need random noise z conditional gans obviously need kind input neural net case conditional gans another kind input random noise z serve introduce variety given condition e g many different faces blonde hair didn‚Äôt care variety could without random noise justification need random noise z makes training easier theoretical reason ‚Ä¶
r8tsv6,0,discussion einstein sum notations popular ml changed life recently discovered torch einsum mad every friend mentor acquaintance telling way intuitive handle operations would want tensors elegantly remember way axis 0 remember way dim 1 remembering many numpy torch functions misuse np unsqueeze torch expand dims takes 30 mins learn notation become somewhat proficient sorted life arguments using einstein notations everything writing code others find difficult understand kindly pitch thoughts theories seldom used one size fit
r76otc,1,sanebox clean inbox minutes keep way forever implement sanebox workday never waste time email sanebox simplifies email process using powerful algorithms learn email behavior organize inbox average sanebox customer saves 12 hours month works email provider client device keep email organized sanebox experiencing clean inbox hard imagine life without sanebox average power email user works everywhere check email fits existing workflow customers choose sanebox saves time boosts productivity sanebox pricing overview 14 day free trial miss chance start sanebox pricing starts 7 00 flat rate per month free version
rhyhah,1,looking data science mentees hi software developer goldman sachs looking websites work part time beta tester mentor students hourly basis please let know aware opportunities ex coursera mentor udacity mentor
rin63t,0,best tool drawing u net style diagrams struggling find good tool drawing u net style architectures likes plotneuralnet nn svg seem like excellent tools seem lend drawing u nets correct wrong though since otherwise familiar need anything particularly fancy want something similar standard u net diagram linked
qutuo6,0,resources line machine learning wondering books articles tutorials line machine learning x200b example website nice lecture notes lec16 aspects x200b book x200b x200b x200b seem find much resources trying understand basics read research papers anyone share resources would nice
qu2de6,0,need stochastic environments hello extensive search unable find good stochastic environments train algorithms found toy text based ones since algorithm variant dqn wanted environment huge state space discrete action space help would really appreciated thanks
rp61rj,1,machine learning data scientist role fully remotely location restricion hi ml community experiences working completely remotely outside country example live england work fully remotely portugal similar experiences interested want find job outside country someone similar experiences would nice share stories 1 employer willingness hire way 2 matters related employment contract 3 tax matters 4 important best look positions portals websites
r0ezps,0,discussion biggest computer vision model ever opensourced wondering everyone telling trained multi billion parameter model none opensourced like fbs seer biggest computer vision model ever released ru dalle mind
qxlk34,0,permutation invariant neural networks reinforcement learning link blog post permutation invariant neural network agents presented handle ill defined varying observation spaces agents robust observations contain redundant noisy information observations corrupt incomplete
rnegp0,1,data science machine learning interview participation request good evening among current circumstances hope message finds everyone well current high school senior student state illinois seeking potential data science professionals prospective data scientists willing participate interview ap research course provide general overview institution currently partnering college board ap capstone diploma diploma program develops student‚Äôs skills research analysis evidence based arguments collaboration writing presenting skills based two long year courses ap seminar ap research student currently enrolled ap research course expected requirement tasked year long process exploring individual area interest may academic topic choice idea circumstantial issue year centering research effects traditional mathematics subjects retain minority students academic success primarily latino students students hispanic origin well assessing measure academic success collegiate students professionals attaining post secondary education degree career worth noting state illinois offer data science education within public school districts objective would like implemented community tried establish contact potential participants success therefore decided post objective hopes gain participants though willing take 20 participants interested seeking previously enrolled data science course secondary high school career post secondary interested participating know may interested please hesitate contact information willing set date time either platform zoom google meets address questions concerns thank reading lengthy post happy holidays
qrpx36,0,integrate ml model favorite apps single python file hi many build simple machine learning model predict whether customer churn get nice pandas dataframe customer data however gets really complicated want model deployed integrated production say procedure 1 pull data new customer shopify 2 predict customer whether churn 3 predict churn true 4 send discount code customer mailchimp suddenly code data integrations etl pipelines deploy original machine learning solution spin http server etc huge pain indeed building framework takes care exactly boring stuff described really believe bridge gap research real world ml super excited share please let know comments feedback
rdyyvl,1,algorithms could potentially help improve logistic regression model whilst maintaining original loss function original model looking bagging realize bagging particularly effective unstable high variance machine learning models much stable high variance algorithms would therefore good improving logistic regression model mainting original loss function
rui1c7,1,sklearn pipeline breaks using functiontransformer hey learning use pipelines look clean im working tabular playground competition kaggle tryna follow pretty simple pipeline use functiontransformer add new column dataframe ordinal encoding finally fit data linearregression model code code breaks first step functiontransformer gives following error assemble mappings requires least year month day specified day month year missing weird since print inside function executed shows datetime format even transform x train date functiontransformer works intended seem work steps joined help appreciated thanks
r4fep0,0,news get code ml ai papers anywhere internet google arxiv twitter scholar sites ‚ù§Ô∏è x200b browser extension chrome browser extension firefox
rdccjw,1,question finished aur√©lien g√©ron hands ml right currently chapter 17 done exercises etc problem 3 chapters preprocessing data tf ch 13 tfrecrods processing sequences using rnn cnn ch 15 nlp rnn attention ch 16 3 chapters specifically even handle programming exercises like killing exaggerating ofc help advice guidance whether videos topic feel 3 chapters programming exercises hard need answers github repo
rp7kc6,1,seeking internship hey everyone hope fine ahmed abbassi studying computer science electrical engineering bilkent university spring semester 2021 exchange student wanted ask could help get internship anywhere even remotely field ai wanted give brief introduction tunisia pre engineering studies math physics ranked 49 2000 students nationwide exam allowed enroll one top public engineering schools tunisia higher school communications tunis worked hard get selected top students exchange program opportunities chose turkey destination aside worked numerous projects writing machine learning algorithms scratch way building full scale models even scraping data needed need cv click x200b thank 3
r1w2hr,1,45 worked examples machine learning energy medicine banking retail physics finance
qzj8qo,0,training batch size 1 working virtual tryone problem thing really cant use batch size 1 training due gpu memory limitations network really benefits batchnorm however batch size 1 really giving good results using pytorch help appreciated use instead batchnorm tried instancenorm doesnt work well try playing momentum batchnorm
rp849m,0,research paper figure drawing hi novice researcher one question drawing great quality figure scientific research paper would like suggest mostly researcher draw interesting figure many customization deep learning based papers could suggest great tools make drawing much easier express add also latex source code website customize drawing video series talked demonstrate draw figure research paper thank time suggestions regarding
qsc6y3,0,data scientists still make better predictive model using talents evidence script kiddies increasingly take great software ai well studied applied data science machine learning since 2014 startled high quality free tools coming online past week month year two tried good talking automatic model selection automatic tuning nlp breaking away old limitations old ways projects see youtube lately watching data scientists posting videos run software microsoft google made call day even change default settings add nothing talents results pretty good golly thing automatic feature engineering one hallmarks top deep neural networks people spent careers linguistics computer vision hand making parts hand curating mathematical techniques often completely bypassed today ml techniques baked free software one example many trivial question data science making predictive model canned fixed public dataset like iris titanic jewellery even higgs boson also mlops exploratory data analysis study design visualization data wrangling data quality assurance data life cycle many areas data science wide scope many specialties predictive model making 5 percent whole ml ds job according presentation saw yesterday modeling might fully automated soon ml engineer data scientist bring wide deep talents bear actually show make model better script kiddie predictive performance well known open dataset making predictive models fully automated task
r17z2y,0,deepmind google brain world chess champion explore alphazero learns chess knowledge deepmind google brain researchers former world chess champion vladimir kramnik explore human knowledge acquired chess concepts represented alphazero neural network via concept probing behavioural analysis examination activations quick read deepmind google brain world chess champion explore alphazero learns chess knowledge paper acquisition chess knowledge alphazero arxiv
r6xzz7,1,machine learning methods classification categorical variables start like say little experience machine learning statistics computer science general interested list models use classify binary dependent response output variable non ordered categorical independent explanatory input x variables know list super helpful tell models use quantitative ordered variables quantitative output variable used randomforest neural network model great success like find models play learn x200b edit case anyone curious models used far single decision tree model rpart r random forest classification neural network classification using one hot encoding predictors done nnet package naive bayes classification done e1071 package naivebayes package issues
r2gm3v,1,separate steps learn stable baselines3 working project two agents train simultaneously agent sometimes needs make decision possible code follows roughly following structure model a2c mlppolicy env verbose 1 learning rate 0 0005 obs env reset range 2000000 action states model predict obs obs rewards dones info env step action model update experience obs action reward type function exist also married stable baselines way another library would also greatly appreciated thanks
r05n9v,0,scaling law recommendation models towards general purpose user representations
rh1viq,0,layer wise relevance propagation pytorch hi set basic implementation layer wise relevance propagation lrp pytorch comes additional relevance filter method much crisper heatmaps happy lrp implementations available pytorch implementation easy understand extensible without much effort small preview repository found thanks
ru91o8,0,paper explained author interview player games games one algorithm video walkthrough special guest first author martin schmid games used throughout research testbeds ai algorithms reinforcement learning agents however different types games usually require different solution approaches alphazero go chess counterfactual regret minimization cfr poker player games bridges gap perfect imperfect information games delivers single algorithm uses tree search public information states trained via self play resulting algorithm play go chess poker scotland yard many games well non game environments x200b outline 0 00 introduction 2 50 games player games trained 4 00 tree search algorithms alphazero 8 00 different imperfect information games 15 40 counterfactual value policy networks 18 50 player games search procedure 28 30 train network 34 40 experimental results 47 20 discussion outlook x200b paper
qzostq,0,question regarding bounding boxes dataset hi relatively new ml looking training model use part license plate anpr program dataset generating random characters font uk plate rectangle placing random backgrounds sun dataset see examples x200b bounding boxes images use exact cooridnates used generation fake plates need use typical box mean give co ordinates may draw irregular rectangle rather style seeing packages labelimg would use black box could use green style training x200b
ropcvb,0,researchers university chicago tel aviv university introduce ‚Äòtext2mesh‚Äô novel framework alter color geometry 3d meshes according textual target recent years neural based generative models center attention exceptional capability creating aesthetically attractive graphical content seemingly nowhere recent solutions kind like vqgan general derivations generative adversarial networks combination deep learning techniques like clip openai joint image text embedding model led amazing results using complex powerful generative techniques advent nfts application transformer based techniques computer graphics videogames hype built recent years generative models might finally lead ai generated art meet growing market demand field entertainment main perk generative models versatility learning latent representations given datasets comes cost higher complexity lower success rate training experiments researchers university chicago tel aviv university introduce the‚Äô text2mesh‚Äô model text2mesh model tries avoid problem proposing non generative method alter appearance 3d meshes using ‚Äú neural style field ‚Äù nsf learned neural techniques maps vertices input mesh rgb color local displacement along normal direction based text prompt determines style appearance result model powered clip joint text image embedding space appropriate regularization avoid degenerate solutions paper summary paper github project page x200b
rk117l,0,come machine learning hi would like know according experience next hot topic people might say machine learning data science never die would like know trend next couple years would quatum computing machine learning topic ml dl thank advance
rngpmh,1,utilize machine learning model website trying make chatbot website process making model model want implement model users able interact wiith chatbot interface made react enable use pickle pickle model load django make endpoints conversations
r78jko,0,platforms python shelf genetic algorithms aco hello somewhat new machine intelligence subsideries apologies silly question currently tasked finding implementing python code using shelf done research nothing jumped applicable task wondering anyone knows best sites platforms finding things like unlucky thus far also worth noting required reference anything find depth better thank
rja4ma,0,project introducing fastshap quick kernel model explanations purpose fastshap make calculation shap values lightweight fast possible accomplished two batching routines keeps process inside vectorized operations often also intelligent numpy slicing involved info works determine optimal batch sizes github boston dataset using 506 rows background set runs 26 seconds original shap package kernelexplainer takes 11 minutes difference grows pronounced larger datasets notes 1 package really designed used tabular data features grouped e superpixels feature added near future though 2 kernel explainer implemented calculates shap values arbitrary model model specific methods e treeexplainer still much faster 3 automatically handle pandas dataframes numpy arrays 4 background dataset stratifying methods available 5 calculate shap values 1 dimensional outputs n dimensions coming soon 6 linear model get shap values coefficients models sklearn linear model module available pypi coming soon conda forge
r3esd8,0,paper overview florence new foundation model computer vision video paper abstract automated visual understanding diverse open world demands computer vision models generalize well minimal customization specific tasks similar human vision computer vision foundation models trained diverse large scale dataset adapted wide range downstream tasks critical mission solve real world computer vision applications existing vision foundation models clip align wu dao 2 0 focus mainly mapping images textual representations cross modal shared representation introduce new computer vision foundation model florence expand representations coarse scene fine object static images dynamic videos rgb multiple modalities caption depth incorporating universal visual language representations web scale image text data florence model easily adapted various computer vision tasks classification retrieval object detection vqa image caption video retrieval action recognition moreover florence demonstrates outstanding performance many types transfer learning fully sampled fine tuning linear probing shot transfer zero shot transfer novel images objects properties critical vision foundation model serve general purpose vision tasks florence achieves new state art results majority 44 representative benchmarks e g imagenet 1k zero shot classification top 1 accuracy 83 74 top 5 accuracy 97 18 62 4 map coco fine tuning 80 36 vqa 87 8 kinetics 600
rs65ei,0,adapting class activation maps object detection semantic segmentation hi r machinelearning project comprehensive collection pixel attribution methods pytorch like package name grad cam original algorithm implemented class activation maps help diagnose properties model predictions like model see cat image many requests added support object detection semantic segmentation wanted share find detailed notebook tutorials tutorial using class activation maps object detection tutorial using class activation maps semantic segmentation x200b computing cam object detection computing cam semantic segmentation problem class activation maps usually researched applied classification models repeating request repository also object detection projects add support grad cam object detection one challenge object detection frameworks typically output tensors back propagate compute gradients typically output dictionaries bounding boxes labels etc lot processing expose way compute gradients respect detections want compute cams typically dive code object detection packages create solutions work generic tool works adapted new object detection models solution gradient free methods class activation map methods depend computing gradients examples eigencam computes pca activations returns first principle component fast since requires single forward pass good enough class discrimination case might several different objects bounding box ablationcam ablates individual activations measures output score drops sota method class discrimination much slower since requires many forward passes ablations object detection networks already heavy ablating activations makes slower practice many activations contain useful information identify skip ablating tried coming heuristic computes binary mask objects might based comparing eigencam low threshold scoring activations according much pixels values fall inside mask control ratio activations actually want ablate lower ratio makes faster seems give good results dramatic run time reductions use methods applied activations feature pyramid network object detection networks adapt object detection custom cam target functions object detection segmentation cam computed target property image like parts image important dog category adapt object detection targeting properties like parts image important get high iou original bounding box detections score high categories similarly adapt segmentation asking questions like pixels image important predicting car pixels x200b hope find useful good starting point towards applying cam methods practice production monitoring diagnostics vision models
rae9ui,0,strategies handling large conference proceedings hi r machinelearning happy neurips week time year around large major conferences field always exciting times waiting see new work break throughs colleagues community members might released personally work applied signal processing space always task find papers might provide insight outline new method learning might able adapt research however look absolutely massive list accepted papers even filtering papers titles become incredibly time consuming daunting wanted pick brains insight manage massive amount work grudge wait community self select go top papers terms awards wait papers become relevant enough pop background reading subject guess one personal struggles fomo something highly relevant well done slips past hive mind community
qmsyf8,0,pruning self supervised speech recognition mit news paper neurips 2021
r0428v,0,regarding phd admissions ml much first authored publication hold back title question graduating master cs first authored publication undergraduate cs pretty much learned actually code started master managed get name 3 conference proceedings 1 preprint also helped lab team win award large ml competition recently alas first authored publication recently attended conference talked phd students many said pretty much red flag day age phd applicant first authored paper wondering much truth obviously one would best scenario much would one hold back thanks
ri2cpj,0,thousands billions overview methods scaling graph neural networks graph neural networks gnns become popular recent years early approaches easily handled small graphs thousand nodes scaled poorly large graphs millions nodes several approaches proposed scaling gnns large graphs recently published overview core approaches gnn scalability read scalable graph representation learning graph neural networks welcome constructive feedback cheers
rlmr8s,0,hand picked selection best python ml libraries 2021 hi compiling list top python libraries released popularized 2021 hoping find good ones might missed year related data science machine learning workflows decided expand post listing many 10 libraries although several main picks related ml awkward array jupytext gradio augly skweak evidently jina finetuner hub full list expanded descriptions available case curious many listed extra picks relevant 2021 late 2020 think picks miss good ones
rfmxi5,0,find closely related paper mistakes hello guys new master student ml currently writing paper found closely related paper submitted arxiv several months ago main proof paper wrong certain contacted author heard response treat paper course ignore cite point mistake paper make paper dangerous review process conference sorry silly question worried first paper
rthi58,1,customer survival analysis saas company hello redditors start survival analysis project using python thesis also since use data company working thought would nice idea create end end survival analysis project company also searching everywhere understand survival analysis difference non parametric parametric analysis use ml models survival analysis many main question would super useful main understanding result outcome customer survival analysis questions answered analysis meaningful results company make use result analysis thank time
r66avr,1,really noob question way look clip database manually think neat see images used train clip
r4aio0,1,model predict sales different customers hi would like build model predicting sales certain products approx 100 different customers predict montly sales important customers total volume sold month time series volumes sold month group customer past 10 years show seasonal pattern unfortunately many customers became past years place orders month timeseries many zeros initially thought adding autoregressive terms maybe considering exponential decay categorical seasonality also tried cluster group customer according correlation timeseries use clusters categorical variables dendrogram shows many timeseries clustered low correlation making clustering almost useless really know suggest build model
rj628g,0,land pattern annotation hi would like train semantic segmentation model detect chunks soil shadows annotation without helper tools time consuming sample image far annotation used mostly cvat labelme patterns however would take much time annotate single image would thankful anyone recommended helpful tool approach annotate images like one
rhh5zr,0,project determining classifier thinks rabbit looks like works trying see classifiers thought different classes looked like ended generating pictures every class imagenet pictures look like expecting though probably know better point lol used imagenet pretrained vgg 16 model classifier fun results rabbits evolving generated picture gray image something classifier 100 sure thing x200b x200b x200b surprised lack color orange generated image
rv3yty,0,bringing serverless ml stateful arbitrary dependency serverless ml serverless infrastructure yet practical use ml think could bring lots benefits friend decided make serverless easy ml building platform solve main issues find serverless ml stateful don¬¥t want reload whole model every time user calls model predict arbritary dependecies normal python code package dependencies use love many many times parallel scale scale scale ease auto shutdown keep resources consumptionyou visit webpage try demo request early access use platform webpage happy receive questions comments building
rew9uj,0,ml community grown big edit probably titled ml conferences grown big lot conferences relevant machine learning community general conferences like neurips iclr icml possibly approaching carrying capacity also seems 2022 bring even new conferences colla conference lifelong learning agents clear causal learning reasoning automl today new journal tmlr transactions machine learning research also announced transformers seem consolidating field least terms model architecture also clearly observe branching community around specific subfields address next wave problems solve conferences nlp computer vision robotics ml theory already exist well thoughts divisions community think future progress supported specialized conferences think neurips iclr icml hold place top venues research move away published elsewhere
rf95gl,0,useful data summary statistics image classification hello x200b image classification tensorflow learning purposes splitting data 5 folds would like get useful summary statistics validation sets could useful shape validation sets
rwehvc,0,scalar reward enoguh create agi check paper discusses idea scalar reward enough create agi thoughts
rx732h,1,come novel nlp project third year undergrad starting 10 week course deep learning nlp expected complete nlp project final really want something novel improve upon state art papers possible struggling coming novel idea topic would appreciate pointers ideas looked projects cs 224n really impressed topics novel actually beat state art results areas interest tts contrastive learning
rciodi,1,model perform better consider unimportant feature hey everyone currently part kaggle competion need predict probability default particular dataset currently trains csv performed feature engenering one hot encode categorical values dealt missing values outliers test csv test performance model considering unseen data good implemented logistic regression got appropriate metrics notice something odd forgot drop variable customer id went back removed model performed worse using predict function nonly predicitng 0s single 1 however consider variable model performs much better able predict 0s 1s made submission file kaggle customer id column risk predictions got good score understanding even possible prediciting 0s consider variable notebook someone need look specific part thank time
r3ldxf,1,embedding ml model microcontroller enlarge functionality thermal station hi guys recently built sensor station monitors thermal comfort indoor spaces monitoring station 3 sensors 1 microcontroller type esp32 would like enlarge functionality station ml since beginner kind confused suggestions go thanks advance
quv04i,0,paper explained gradients need full video walkthrough systems made differentiable means accurate gradients systems dynamics computed exactly development led lot advances also distinct situations backpropagation bad idea paper characterizes systems domain iterated dynamical systems often including source stochasticity resulting chaotic behavior systems often better use black box estimators gradients computing exactly x200b outline 0 00 foreword 1 15 intro overview 3 40 backpropagation iterated systems 12 10 connection spectrum jacobian 15 35 reparameterization trick 21 30 problems reparameterization 26 35 example 1 policy learning simulation 33 05 example 2 meta learning optimizers 36 15 example 3 disk packing 37 45 analysis jacobians 40 20 done 45 40 use black box methods x200b paper
r1p73d,1,time series forecasting daily temperature dataset contains monthly record temperature still predict daily temperature even monthly record temperature
rnhena,0,dimensionality reduction geometrical data hi dataset data set geometrical 3d data points example one data could x1 y1 x2 y2 x3 y3 x4 y4 xn yn x200b looking dimensionality reduction techniques could fit data know techniques could apply thanks advance
rv2j9k,0,illustrated retrieval transformer gpt3 performance 4 size hi r machinelearning spent time wrapping head around deepmind retro transformer visualizing works hope find useful feedback welcome
rcgkhi,0,owns rights images produced ai thought using ai art creator used text image ai like starryai neuralblender owns rights image creates since made prompt ai developer since made ai since ai trained pre existing images owners intial images rights
rwju9t,0,minimum corpus size word embedding extraction dear smallish 100mb corpus historical text non mainstream language want apply word embedding enough shall consider frequent words frequent help preprocessing stemming etc choose parameters especially embedding dimensionality libraries recommended language agnostic unsupervised ways evaluate embeddings x200b thanks
rghuvg,1,4 date techniques image data augmentation wrote brief introduction selected data augmentation techniques published top conferences recent years find mysterious yet working particularly well feel free look
rvw32g,1,beginner learning path question python c developer starting learn ml question regarding path convenient porpuses project mind automatic placement labels cad following certain rules see image time consuming task specially thousands objects example image finished ng course confused learn next learn ml reinforced learning start directly reinforced learning want skip stage would like learn progress project x200b thanks
qtsomm,0,reverse language reconstructing consensus priority months collecting data becoming one next stage project 70 000 rows x 12 columns working reconstruct words classic latin currently lemmatization 9000 words unknown english many unable lemmatized however translating latin every language evolved latin able fill pieces instance errat english errat could find within dictionaries assume means follow line process latin german english latin spanish english latin catalan english forth going list err irren err wrong err wander err shine rail err errar make mistake made mistake fout wrong conclude means error obvious err would perfectly acceptable english however go line line especially tried create natural language consensus row work issue getting right data tried pattern 1 1 1 1 1 1 2 2 3 1 2 1 however spit gibberish instead choosing appropriate column almost impossible tell machine fout wrong made mistake synonyms unless feed massive thesaurus also easy one word supporter journey wandering traveler repairer refresher none illustrate unique definition see main subject translated word explorer indirect subject someone supports fixes refreshes repairs know back ancient times rich repairmen wagons tents anything traveling would likely person word unknown best could expect machine would summarize two main ideas definitions could super simple overthinking complex underthinking would love suggestions opinions guess could simple summarization row
rey855,1,make dataset progressing along journey wandering people come datasets know image recognition could simple collecting images certain class want let say something like style gan people go creating dataset people know dataset work
qmxns9,0,sagemaker linear learner hi everyone wondering anyone know corresponding library cran scikit learn linear learner aws sage maker access aws tell whether interface different regressions something sophisticated enjoy day fella
rucjmx,0,simple questions thread please post questions instead creating new thread encourage others create new posts questions post instead thread stay alive next one keep posting date title thanks everyone answering questions previous thread
rv4u2w,0,paper mathematically proves gradient descent achieve zero training error think well known paper able find interested paper mathematically proves neural nets fit set datapoints far found mostly papers show empirically something related like one appreciate help edit u new scientist shared paper looking also apologize vague description paper shown hope clear future readers kind results meant case case wondering question conditions neural network achieve zero training error particular interested papers mathematical even without empirical results
ru7k5y,0,machine learning research hi everyone compiled trusted sources ideation based top tier conferences machine learning deep learning worldwide repository includes datasets tasks state art repository github
ruhkky,1,good colab notebook large batches text hey interesting using gpt 3 gpt 2 generate things like film scripts social media posts like deep leffen bot twitter like train large amount text anyone know model best good colab notebooks specific task feeding large batches text instead small prompts
qo5fhz,0,problems neural networks output order magnitude working project trying train airplane control 2d setting neural network acts airplane pilot 2 outputs related control airplane altitude angle attack throttle familiar unfortunately network outputing numbers completely wrong order magnitude tried writing activation function limit values simply getting max min allowable value since actual output scale way control order magnitude outputs tried ton things intitializing weights extremely small normalization input
qniktz,0,structure cv made first cv working company two years worked multiple projects interested looking bit reworking cv really know way display projects skills senior ai related engineers want share cvs suggestions
rqycm4,0,elon musk talking lex fridman ai tesla third interview interesting points manage training inference neural nets tesla points repeated talks like dot products way repeatedly saying vector space high level topics term hydranet multi headed prediction network used new type architecture large focus extensive code written c c reduce inference time especially device point mentioned many many times risk jitter volatility inference time affect robustness overall system retraining networks based raw image data instead processed image data also small comment networks use built old hardware process updating new versions networks rely multiple layers neural networks instead heuristics implied move closer version 11 large redesign distilling giant bags points outputs models create inferences multiple outcomes back something digestible
r1lspw,1,best way plot soft clustering models python hi guys machine learning course assignment hard vs soft clustering algorithms particular k means vs fuzzy c means point want plot clusters k means easy either part cluster set colors data points matplotlib accordingly however c means possible clusters overlap could assume part cluster largest weight feel like removing possibly important information things clustered would end producing similar plot k means good ways plotting soft clustering algorithms python
qngw0u,0,buying pc training make sense 2021 work text data love put 500 machine fine tune largest gpt 2 instance largest two 774m 1 5b parameters still make sense buy computer spent hundred dollars aws credits knowing throttled cost limits experiments run would love hear bought machines decided would probably look used one ebay though know little purchasing pc
rbj8vg,1,interpreting clustering results hi beginner kmeans clustering customer data added cluster labels back original data exported csv cluster labels additional column wondering analysis customer personna ideally want able say eg cluster 1 well men 40s cluster 2 women technology 20s etc pivot tables averages excel intepret centroid table jupyter notebook example centroid table output jupyter notebook different actual would one interpret would say example cluster 1 group people lowest cooling tower frequency relatively low humidity etc cluster 4 contains people highest cooling tower frequency highest relative humidity thanks much advance
r60cet,0,dask dataframe plotting without using much ram pyspark anyone give solution actually plot whole dataset pyspark dask definitely fit memory interested solutions dump whole thing numpy array actually light ram ideally something plot smaller batches keep results ssd dask know following df x resample 24h mean compute plot
r75y47,1,üíäyour daily dose machine learning onnx runtime deep learning c series posts post almost daily call ‚Äúyour daily dose machine learning‚Äù week shared experience deploying tensorflow models using c shared used opencv dnn module tensorflow c api today want share experience deploying tensorflow models c environment using onnx runtime onnx stands open neural networks exchange it‚Äôs whole ecosystem aims standardize representation machine learning models it‚Äôs developed microsoft onnx aims make easier deploy kind machine learning model coming type ml framework including tensorflow deploy tensorflow models using onnx c need 2 things convert tensorflow model onnx format open source tool called tf2onnx use onnx runtime deploy converted model i‚Äôve personally tested approach many deep learning models works great example converted almost models tensorflow object detection api onnx format able run inference problem fell love tool suggested friend seeing capabilities future posts might go details capabilities tool also post almost daily linkedin twitter follow
r1uz2q,1,rtx 3080ti 12gb quadro m6000 24gb deep learning
r61gwu,0,much reviewers check proofs papers reviewers much time energy spend following proof presented paper suppose understand paper idea makes sense head rigorous comes proof know many different factors e g reputation journal better journals might strict rigorous reviewers curious general sense got curious found paper looks interesting presents theorem fully digest proof even though spent know reviewers probably better understanding still
rsv2o4,0,machine learning alternative mcmc nested sampling work regularly position fit complex models datasets part involves defining kind likelihood function merit function uses certain set parameters find distance model data currently standard methods field usually mcmc variants nested sampling mcmcs ns robust safe way find global minimum maximun high dimensional parameter spaces become slow right running possibly month long ns chain exploratory test 32 cores pc month long run could ok ish definitive result feasible exploratory exercises really one time tweak thing fast efficient alternative mcmc nested sampling something might safe nested sampling could give good estimate best set parameters significantly shorter run
rwqq81,1,savitzky golay filter data denoising smoothing important part data related problems require preprocess data first one step data denoising smoothing people many times high spikes dat overfit model give bizzare results one underrated smoothing technique utilised savitzky golay filter low pass filter mostly used signal processing filter signal fluctuations could well used smoothing data well filter tries approximate original function removing useless fluctuations noise data well misguide model take look works python implementation would love hear constructive criticism writing subject thank
qysmyc,0,webapp monitoring gpu machines hi created webapp monitoring gpu machines working maybe useful someone else app aggregates output gpustat across machines displays single page using app working well extensively tested encouter problems setting app hesitate open issue github
r65cbg,1,mlb came today say 2 different types baseballs used last season type clustering algorithms would use determine pitch sequence contained type hi wanted share question thought would interesting discuss subreddit might accomplished unsupervised learning problem much tougher definitive answer number clusters least mostly small sized varying length pitch sequence make determination variables account pitchers stadium
r2fjja,1,cheese cake doge x200b implement optimization based clip styletransfer open replicate github
rhsefd,0,anyone else suspicious concerned spread data science degrees along ms programs artificial intelligence rather ms cs stats focus ai could understand say cmu wanted ms ai would probably pretty good prep phd subject example yeshiva university ms ai despite far tell literally two full members faculty working area somewhat concerned rise degree programs ml ai ds specifically seem really specialized way undergraduate professional degrees probably always offered departments trust deliver appropriate instruction would way appropriate students degree math statistics cs pick coursework research narrower specialty rather potentially left holding bag hype dies
qrhh89,0,advances encryption new topic understand easy ai find valid result predictable algorithm like basic letter letter encryption small dataset saw example lecture complicated encryption algorithms 16bits 32bits 256bits tls big would dataset get able easily predict correct unencrypted text encrypted text ai used test algorithm actually safe solving without really knowing exactly data encrypted example encryption standard looks safe experts unknown mathematical shortcut ai could find possible find topic
rf7fxo,1,questions regarding cnn lstm model understanding video data must converted individual frames converted sort array data passing cnn don‚Äôt understand last step converting array data could someone please assist
rf59gu,0,jina open source framework build scalable deep learning applications mins jina neural search framework empowers anyone build sota scalable deep learning search applications minutes think building image search video search semantic search quickly ‚è±Ô∏è save time design pattern neural search systems native support pytorch keras onnx paddle build solutions minutes üåå data types process index query understand videos images long short text audio source code pdfs etc üå©Ô∏è local cloud friendly distributed architecture scalable cloud native day one developer experience local cloud üç± stack keep end end stack ownership solution avoid integration pitfalls get fragmented multi vendor generic legacy tools get started pip install jina checkout jina github design pattern get speed quickly make good choices next deep learning app seeking feedback find much opinionated much abstract
r6789x,1,autusave resume training google colab shutting brutly new google colab bought colab pro member found run around 24 36 hours brutly shut time know retrain gan sometimes would download pkl file manually upload retrain looks stupid way autosave pkl files google drive gan training snapcount thanks lot
r9qmhc,1,machine learning equivalent deep learning book goodfellow came across goodfellow et al deep learning book wondering machine learning equivalent book suggestions thanks advance
rb29js,0,managing ml experiments code git dvc experiment tracking tools log experiments central database show dashboard makes easy share teammates compare however active experimentation phase may create hundreds experiments team members may overwhelmed loose ability effectively share experiments team members following article shows dvc tool push experiments like git branches giving flexibility share experiment choose track ml experiments version dvc experiments run stored local repo best experiments promoted central repo github example share teammates distributed experiments shared people code repo traditional experiment tracking tools log ml experiments central database show dashboard makes easy share teammates compare however active experimentation phase may create hundreds experiments team members may overwhelmed loose ability effectively share experiments team members dvc experiment versioning treats experiments code saves metrics hyperparameters artifact information text files versioned git need centralized database online services git becomes store experiment meta information dvc data versioning backs artifacts anywhere
rt6zny,1,would approach book deep learning ian goodfellow research perspective context undergrad student currently senior year prior experience ml dl also currently pursuing couple research projects couple professors also want something side wondering would best way get started read book mentioned order get ideas try something else thanks
re71yb,1,choose hyperparameters gaussian process hello working project model plots using gaussian regressions would like use grid search make exhaustive search many hyperparameters kernels possible said really experience gp know hyperparameters combinations kernels testing tried look examples either examples specific transfered case found complicated math explanations really help concretely help would welcome thanks
r2njr1,1,model use simple animation generator getting computer vision recently got idea make simplest possible video generator like one still thinking make like 1 input frame output 3 sec animation predict next frame also know example gans look good complex may need really sure use gan stylegan biggan pix2pix autoencoder vae something else
r9u2e2,0,secret santa stylegan hi guys created quick notebook weekend uses stylegan create face morph comic strips x200b gag gift print comic custom mug costs ¬£6 well within secret santa budgets x200b let know give notebook try artistic suggestions make comics better similar description found github affiliated mug company profit lol
r9latm,0,thoughts hands machine learning scikit learn keras tensorflow geron early portion data science masters wanting learn much purchased book title noticed code progressed chapters 2 3 exactly work book expects common problem throughout book instance chapter 3 goes sdg classifiers kept getting errors code used pages 86 88 able figure correct kind perplexing book errors
rpum3a,0,article suggestions graphs meet transformers hey everyone trying gather articles graph network use transformer like architecture uses attention mechanisms used biological medicinal purposes open read articles fields also trying come review article disease drug therapy triad use ml dl course feel like studies field undervalued comment take look thanks advance cheers
r51g2v,1,github googlecloudplatform mlops vertex ai end end example mlops google cloud using tensorflow tfx vertex ai check repository google cloud find useful end end example ùêåùêãùêéùê©ùê¨ ùê®ùêß ùêÜùê®ùê®ùê†ùê•ùêû ùêÇùê•ùê®ùêÆùêù using ùêìùêûùêßùê¨ùê®ùê´ùêÖùê•ùê®ùê∞ ùêìùêÖùêó ùêöùêßùêù ùêïùêûùê´ùê≠ùêûùê± ùêÄùêà 1 performing exploratory data analysis data ùêÅùê¢ùê†ùêêùêÆùêûùê´ùê≤ 2 creating vertex ai dataset resource using python sdk 3 generating schema raw data using ùêìùêûùêßùê¨ùê®ùê´ùêÖùê•ùê®ùê∞ ùêÉùêöùê≠ùêö ùêïùêöùê•ùê¢ùêùùêöùê≠ùê¢ùê®ùêß 4 preparing data using ùêÉùêöùê≠ùêöùêüùê•ùê®ùê∞ 5 implementing ùêäùêûùê´ùêöùê¨ ùêúùê•ùêöùê¨ùê¨ùê¢ùêüùê¢ùêúùêöùê≠ùê¢ùê®ùêß ùê¶ùê®ùêùùêûùê• 6 training keras model vertex ai using pre built container 7 upload exported model ùêÇùê•ùê®ùêÆùêù ùêíùê≠ùê®ùê´ùêöùê†ùêû ùê≠ùê® ùêïùêûùê´ùê≠ùêûùê± ùêÄùêà 8 extract visualize experiment parameters vertex ai metadata 9 use vertex ai ùê°ùê≤ùê©ùêûùê´ùê©ùêöùê´ùêöùê¶ùêûùê≠ùêûùê´ ùê≠ùêÆùêßùê¢ùêßùê† 10 clone repository built environment 11 run unit tests 12 run local ùêûùüêùêû ùê≠ùêûùê¨ùê≠ ùê®ùêü ùê≠ùê°ùêû ùêìùêÖùêó ùê©ùê¢ùê©ùêûùê•ùê¢ùêßùêû 13 build ùêåùêã ùêúùê®ùêßùê≠ùêöùê¢ùêßùêûùê´ ùê¢ùê¶ùêöùê†ùêû pipeline steps 14 ùêÇùê®ùê¶ùê©ùê¢ùê•ùêû pipeline 15 upload pipeline cloud storage 16 creating ùêÇùê•ùê®ùêÆùêù ùêèùêÆùêõ ùêíùêÆùêõ ùê≠ùê®ùê©ùê¢ùêú 17 deploying ùêÇùê•ùê®ùêÆùêù ùêÖùêÆùêßùêúùê≠ùê¢ùê®ùêß 18 triggering pipeline 19 receive hyper parameters using ùê°ùê≤ùê©ùêûùê´ùê©ùêöùê´ùêöùê¶ ùê†ùêûùêß custom python component 20 extract data bigquery using ùêÅùê¢ùê†ùêêùêÆùêûùê´ùê≤ùêÑùê±ùêöùê¶ùê©ùê•ùêûùêÜùêûùêß component 21 validate raw data using ùêíùê≠ùêöùê≠ùê¢ùê¨ùê≠ùê¢ùêúùê¨ùêÜùêûùêß ùêöùêßùêù ùêÑùê±ùêöùê¶ùê©ùê•ùêûùêïùêöùê•ùê¢ùêùùêöùê≠ùê®ùê´ ùêúùê®ùê¶ùê©ùê®ùêßùêûùêßùê≠ 22 process data using ùêÉùêöùê≠ùêöùêüùê•ùê®ùê∞ ùêìùê´ùêöùêßùê¨ùêüùê®ùê´ùê¶ ùêúùê®ùê¶ùê©ùê®ùêßùêûùêßùê≠ 23 train custom model vertex ai using ùêìùê´ùêöùê¢ùêßùêûùê´ ùêúùê®ùê¶ùê©ùê®ùêßùêûùêßùê≠ 24 evaluate validate custom model using ùêåùê®ùêùùêûùê•ùêÑùêØùêöùê•ùêÆùêöùê≠ùê®ùê´ ùêúùê®ùê¶ùê©ùê®ùêßùêûùêßùê≠ 25 save blessed model registry location cloud storage using ùêèùêÆùê¨ùê°ùêûùê´ ùêúùê®ùê¶ùê©ùê®ùêßùêûùêßùê≠ 26 upload model vertex ai using ùêØùêûùê´ùê≠ùêûùê± ùê¶ùê®ùêùùêûùê• ùê©ùêÆùê¨ùê°ùêûùê´ custom python component 27 model deployment 28 test model interface 29 create ùêûùêßùêùùê©ùê®ùê¢ùêßùê≠ ùê¢ùêß ùêïùêûùê´ùê≠ùêûùê± ùêÄùêà 30 deploy model endpoint 31 test vertex ai endpoint 32 use vertex ai endpoint online prediction 33 use vertex ai uploaded model batch prediction 34 run batch prediction using ùêïùêûùê´ùê≠ùêûùê± ùêèùê¢ùê©ùêûùê•ùê¢ùêßùêûùê¨ 35 set ùê¨ùê§ùêûùê∞ ùêöùêßùêù ùêùùê´ùê¢ùêüùê≠ ùê≠ùê°ùê´ùêûùê¨ùê°ùê®ùê•ùêù 36 create monitoring job models endpoint 37 list monitoring jobs 38 list artifacts produced monitoring job 39 pause delete monitoring job 40 ùêåùêûùê≠ùêöùêùùêöùê≠ùêö ùê≠ùê´ùêöùêúùê§ùê¢ùêßùê† gcp console
r9iqi3,0,discussion machine learning eventually make high school math curriculum eager young people interested math someday distant future begin clamoring classes ml high school level mathematicians ai experts increasingly using machine learning solve previously intractable math problems getting foundation ml young age expedite trend
rh3nj8,1,rnn predict getting 13 outputs number columns x train set using model predict function using keras need one output can‚Äôt find anything online issue model code regressor sequential regressor add lstm units 50 return sequences true input shape x train shape 1 1 regressor add dropout 0 2 regressor add dense units 1 regressor compile optimizer adam loss mean squared error history regressor fit x train train epochs 10 batch size 32 predict code train predict regressor predict x train test predict regressor predict xtest test predict shape 48 2142 13 1
ris9cv,1,ml engineers work company research subfield work key resources used learn please specific possible new comers good idea diverse fields good resources fields
rnnx7b,1,sound event detection using machine learning video presentation want learn deep learning used detect sounds practical introduction topic given presentation europython 2021 know sound event detection used track progress beer fermentation learn following video sound event detection using machine learning youtube happy take questions
r928oo,1,books online courses start ml like study machine learning could start degree physics bachelor information technology mid 1990s 25 years ago high school mathematics teacher like things mathematical also enjoy coding c sometimes javascript sometimes r sometimes visual basic whatever pretty much level consider math good currently high school level think calculus college algebra level etc beyond forgotten learned like remember name fourier series quite remember remember well karnaugh maps really remember 25 years ago could fine folks recommend look machine learning comp sci mathematics courses investigate mean look course linear algebra sort algorithm course find appropriate books online courses like hard describe hope provided enough push get ball rolling thanks muchly
ruz0nc,0,causes feature collapse unfamiliar feature collapse train model classification model ends mapping distribution data data different classes close proximity multi dimensional space example model learns cluster speak cat test projects dog center cluster classifies cat ways sort deal cv double gradient penalty spectral norm resnet blocks causes feature collapse
qksrhl,0,tools exist determine useful type perdiction messed around ibm google automl frameworks remember output report data helpful predictions using correct terminology sorry basically train ai model data car steering angle gyro acceleramotor speed etc ground truth precise car steering angle want figure data types useful good prediction way feed whole lot data train model know data sources irrelevant tools exist
qxo4ym,0,search machine learning phd internships effectively machine learning phd student uk university interested research internships big tech companies questions like ask reddit community best strategy land offers interviews submitted cv filled application forms many heard back since except summary reject deepmind took 3 days effective ways progress worrying bit early stage positions based us found many positions available us north america know 2020 21 cycles internships virtual location might mattered much world starts open commonplace us based positions hire someone say europe actually prefer uk european positions find many online looking propective interns found job descriptions rather vague rumours leetcode worry bit since hardcore cs undergraduate initial experience leetcode medium seems hardest realistic interview setup without hours put practice starting 3rd year one top uk universities uk phds nominally last 3 years observation people tend slightly longer research scientist internship big tech company level faang decent someone referred position urgently needed someone back guess get full interview experience also expect positions top tier companies could competitive interview experience might representative terms publications 3 first co first authored papers top conferences e icml iclr neurips couple review 2 first authored papers lower tier conferences journals non first authored papers otherwise insights suggestions welcomed many thanks help
rjdy4s,0,discussion new ide built ground ml engineers practioners lot plugins things like vscode folks use pycharm like even found interesting early projects like focused making ml engineering easier curious hear community might ide built purely ml engineers beginning look like something like exist someone working space many tools keep coming back idea creating ide space
r5cvb6,0,research using chaos theory study deep neural network know reading book emergence chaos order studied simple shadow neural network research using chaos theory study deep neural network
rd3hwy,0,wavenets comprehensive presentation code studying wavenets time published experiences presentation code wavenets exceptionally efficient capturing distant relationships parameters think concepts immediately clear everyone property wavenets generative models allows us use model generate new data continue existing data progressively classify observations shown illustration approach wavenets applied sound usual random fourier series mnist images image shows progressive classification mnist images wavenet trained p x image class classification performed via bayes rule would happy presentation helps understand wavenets better maybe use next project
ra70fe,1,bayesian linear regression tasked regression specific dataset built decision tree regressor model random forest regression model compare two using r squared value test dataset also computed third model using bayesian linear regression however idea determine good model bayesian linear regression computes far aware r squared metric bayesian linear regression sampled posterior distributions unsure next step essentially
r1ba74,0,gans transformer sota compositional generator compositional transformers scene generation explained 5 minute summary casual gan papers several attempts mix together transformers gans last year one impressive approaches gansformer featuring novel duplex attention mechanism deal high memory requirements typically imposed image transformers six months releasing original model authors deliver solid follow builds ideas transformer powered compositional scene generation introduced original paper considerably improving image quality enabling explicit control styles locations objects composed scene could model dethrone spade full summary blog post gansformer2 arxiv code subscribe casual gan papers follow twitter weekly ai paper summaries
r3rzms,1,determining loss function cnn autoencoder making autoencoder compress audio using librosa stft make images chose adadelta optimizer binary crossentropy loss based tutorial tensorflow train loss becomes pretty big negative number somewhere 100ks know something wrong sure fix even happening included relevant model code info class autoencoder model def init self super autoencoder self init self encoder tf keras sequential layers input shape 1024 1024 1 layers conv2d 16 3 3 activation relu padding strides 2 layers conv2d 8 3 3 activation relu padding strides 2 layers conv2d 4 3 3 activation relu padding strides 2 self decoder tf keras sequential layers conv2dtranspose 4 kernel size 3 strides 2 activation relu padding layers conv2dtranspose 8 kernel size 3 strides 2 activation relu padding layers conv2dtranspose 16 kernel size 3 strides 2 activation relu padding layers conv2d 1 kernel size 3 3 activation sigmoid padding def call self x encoded self encoder x decoded self decoder encoded return decoded autoencoder autoencoder autoencoder compile optimizer adadelta loss binary crossentropy
rawaur,1,want get top k important words sentence using infersent model exploring infersent model developed facebook researchers model lot methods functions available interested visualize method showing importance words sentence help bar graph getting clue extract words visualise methods checked official github page infersent model find clue get done totally clueless case x200b infersent model visualize method output need moving canada business meet define k 4 k top number words
qshm5b,0,questions regarding self supervised learning music dino moco hello everyone first pretty inactive reddit hope right place post computer science graduate student focusing machine learning working interdisciplinary research project regarding analysis music tl dr try extract general descriptive music features various different levels self supervised methods recommend limited resource capacities data augmentations use music general tips tricks ssl music good idea use pretrained backbone order get away small dataset compute e g openai jukebox vq vae dino setup converging x200b general idea rather solving specific mir task e g genre classification goal extract generic interpretable descriptive music features words want model ‚Äúperceives‚Äù ‚Äúunderstands‚Äù music general without giving specific goal would analyze extracted features e g find relations human music perception use downstream tasks ideally outcome model describe music various different levels e g beat rhythm harmony example features extracted different neural network layers shallow low level deeper higher level know far easy task worth investigating possibilities limitations opinion self supervised learning method research self supervised learning ssl seems way go ssl research area gained momentum last two years multiple proposed methods however applied mostly computer vision area images additionally ssl methods seem rather data hungry might imagine resources quite limited gtx 1060 available locally also willing pay order train model gpu cloud services e g lambda gpu since budget low i‚Äôd like testing locally free alternatives colab order find right method hyperparameters etc actual training payed server idea using pretrained jukebox backbone idea use pretrained vq vae openai‚Äôs jukebox see backbone hope extracts useful intermediate features forward comparably small model hope get away small dataset 60 hours music relatively low resource usage still achieving reasonable results anyone experience setups ssl method ignoring generative models one hand contrastive methods cpc simclr less resource hungry moco hand methods explicitly formulate contrastive loss byol dino especially latter one seems interesting unfortunately experience training models wanted ask community tips feedback suited problem preferable methods require much hyperparameter tuning time compute limitations much compute training e g large batch sizes simclr still achieve good results dino working properly currently testing dino method limited setup locally dataset consists small portion fma dataset 90 minutes music sample size 4 seconds batch size 14 using lars optimizer additionally adopted audio augmentation strategy paper applies simclr music clmr see happy ideas better music augmentations problem though however model seem converge properly loss decreases quite quickly epochs however model seems collapse loss increases rapidly staying high remaining epochs figure something momentum hyperparameter e much student‚Äôs weights get transferred teacher iteration increasing number collapse effect minimized non existent large enough loss really decrease much either logged everything computed stats several epochs anyone interested splitting loss teacher entropy kl divergence student teacher one see collapse caused entropy part anyone experience dino similar methods might clue happening mentioned give details hyperparameters logs statistics etc possible phenomenon due small dataset batch size local machine would diminish cloud larger dataset batch size though mentioned paper successfully tested method even batch size 8 know lengthy post wanted share much detail possible goal resulting problems hope anyone might give feedback papers jukebox clmr dino
rwl4pe,1,learn ml python basics grade 11 junior pretty good understanding math concepts like functions algebra basics trig ive never done calc get started learning ml learn things web development flask django going onto advanced things im new programming ive exploring quite time learning basics python please help thank
qse2ov,0,cnn kernel weights reach high values recently read bunch literature network pruning common criteria field select kernels removed l1 magnitude e g 1 2 heuristic apparently catches relevant kernels quite well often time cnns trainied form weight decay imho intended regularize model prevent single kernel weights dominating entire set parameters distribute relevance across multiple channels also normal add batchnorm architecture stabilizes training procedure argue 1 relevant thing detecting pattern relative weight kernel weights absolute value matter change value range cnn kernels matrix multiplication therefore single scaling factor would scaling entire thing 1 kernel followed batchnorm values get automatically scaled zero meaned getting scaled shifted less reason using conv bn relu ordering 2 weight penalty lead continuously slowly decreasing values kernel weights except batchnorms way penalty efficient kernel reason could think one run numeric stability issues one approaches minimum resolution data format e float16 32 maybe introduces sort noise optimization process however evidence shows get higher weights wrong would happy get shown thought process breaks 1 comparing rewinding fine tuning neural network pruning 2 learning efficient convolutional networks network slimming
razw0l,1,requesting help beginner ml working tuberculosis tb prediction project binary classification using tensorflow using chest radiography images dataset cnn 3500 normal chest x ray cxr images 700 tb infected images need perform data sampling find better dataset optimizers activation function many convolutional layers dense layers use use data augmentation looking suggestions
rlj83w,0,important numerical analysis machine learning i‚Äôm already signed numerical methods class cs department application focused originally planning sequence graduate numerical analysis math department next year theory focused i‚Äôm wondering i‚Äôm better taking class instead sequence ie graduate algorithms class graduate real analysis class good idea upper division undergrad numerical methods class sufficient p i‚Äôm aiming phd statistics undergrad
rga91a,0,using pytorch tensorflow going 2022 pytorch tensorflow ecosystems developing quickly thought time take another look stack one another analysis frameworks compare found pretty interesting results pytorch still research framework tensorflow still industry framework majority papers papers code use pytorch job listings seek users tensorflow thorough analysis relevant differences two frameworks read interested framework using going 2022 think jax haiku compete pytorch tensorflow coming years love hear thoughts
rp7wfs,0,anyone know projects involving generating objects coco segmentation dataset using gans know intended purpose dataset seems pretty good given size dataset object bounds masks
rbgeq0,1,someone help derive equation used paper nmf x200b reading paper used nmf data analysis course already read paper already gotten insights concepts mean majority concepts still vague terms unfamiliar biological sciences major someone explain following concepts like five 1 language python want learn utilize tool take crash course python would able understand initialize convergence mean 2 mean initialize related return h 3 convergence mean 4 ‚Üê mean 5 someone derive formula 1 formula 2 formulas b c 6 formula one v wh mean divide matrix w h 7 formula one right side component wise multiplication written matrix w √ó matrix v √ó matrix w 1 √ó matrix h 1 aware matrix cross product order multiplication matters know order multiplication provide correct order cross product 1 lastly formula 2 four vertical bars hg mean
r7cvi6,0,important publications research scientist interviews sorry question rather dumb want hear first hand account important publications top venues e iclr neurips icml respective top conferences cv nlp research scientist including internship interviews industrial labs important compared e g leetcode behaviourial qns understand first foremost thing probably whether team fit let assume exists team things roughly area interviewee comments highly appreciated thanks
rolzia,0,segmentation based shape identification need help making deep learning classifier want right algorithm splits image equal squares classifies segment counts many want use mask finds shapes objects first classifier work within individual shapes finds instead classifying equal segments classify pixels located within strange shapes found see drawing sometimes mask find shapes contour lines certain parts image areas like segment grid classify way see pic 3 drawing hope makes sense someone point right direction python
qphreq,0,intuition meaning behind magnitude covariance covariance matrices pretty essential many ml algorithms probabilistic models two variables positive covariance correlated negative covariance inversely correlated covariance zero correlated however degree correlation cannot read magnitude covariance value question follows well read magnitude mean two variables large covariance value opposed small one
r0ht8v,0,text paint connecting neural painters clip goal paint telling machine natural language paint neural painters paint using strokes colors rather generating images pixel level working text paint together diavlex ai art collaboration idea specify prompt natural language ant tell machine paint using strokes pixel level human artist would overview implementation links notebook try note notebook hosted kaggle colab part kaggle community probably better option given currently kaggle offers nvidia tesla p100 notebook sessions whereas using google colab might get less powerful gpu session notebook kaggle notebook colab tl dr following steps capture essence idea note use pseudocode based python necessary reflect models api please look notebook actual code methods used 1 specify paint e g prompt black sheep 2 encode text using clip language portion obtain text features text features clip model encode text prompt 3 initialize list brushstrokes actions ask neural painter paint canvas beginning canvas look random canvas neural painter paint actions 4 use vision portion clip model extract image features initial canvas image features clip model encode image canvas 5 goal teach neural painter modify strokes e actions depending different painting initial text request prompt example perfect case scenario cosine similarity text image feature vectors 1 0 using intuition use loss guide optimization process cosine distance measure different vectors cosine distance case corresponds loss 1 0 cos text features image features 6 minimize loss adapting neural painter actions end produce canvas close possible original request enjoy neural painting x200b example prompt black sheep showing evolution optimization 13 strokes used x200b x200b paint prompt black sheep last canvas painted stroke stroke x200b paint stroke stroke prompt black canvas using 13 strokes fin
r10kwg,1,questions regarding machine learning ai calculation power locally cloud hello everyone university internship research formulate solution current problems around machine learning artificial intelligence calculation power internet find plenty resources regarding aws azure make one main things pay attention costs setting system finding one suits university best considering local solution cloud solution viable options found trouble finding correct resources point right direction regarding local solutions treads posts blogs found far 4 5 years old info could helpful much rather chance see updated opinion information main question would hear everyone opinion different solutions regarding machine learning calculation power cloud based locally thanks advance replies
rj4ju4,1,making project guys think idea hello need help voting program ‚Äúdoctor box‚Äù dib analyzes medical data artificial intelligence dib suggests patient eat health problem increase program presents medical reports simplified way understandable average people achieve healthier life reduce need regular hospital visits dib also provides reminder medications timing adding dib act lifesaver case emergency dib automatically contact hospital one relatives event loss consciousness illness voting link
rkdxfs,0,100x faster nerf explained plenoxels radiance fields without neural networks 5 minute summary casual gan papers every comes along idea pertinent makes alternatives look drab uninteresting even consider nerf 3d neural rendering phenomenon last year one idea‚Ä¶ yet despite hype around alex yu sara fridovich keil team uc berkley chose another approach focus perhaps surprisingly without neural networks yes still reading blog ai papers even surprisingly approach coined plenoxels works really well authors replace core component nerf color density predicting mlp sparse 3d grid spherical harmonics result learning plenoxels scenes two orders magnitude 100x faster optimizing nerf noticeable drop quality whatsoever crazy yeah let‚Äôs learn full summary blog post plenoxels 100x faster nerf arxiv code subscribe casual gan papers follow twitter weekly ai paper summaries
qka32i,0,2d models 3d tasks convolutions simple replace 2d tasks enjoy vast backing successful models reused convolutions one simply replace 2d ops 3d counterparts inherit benefits extra steps improve transition interested unrolling 3d input along channels pubs code help
r2gl7w,0,separate steps learn stable baselines3 working project two agents train simultaneously agent sometimes needs make decision possible code follows roughly following structure model a2c mlppolicy env verbose 1 learning rate 0 0005 obs env reset range 2000000 action states model predict obs obs rewards dones info env step action model update experience obs action reward type function exist also married stable baselines way another library would also greatly appreciated thanks
rbabt8,1,deal multiples dataframes structure different values years trying learn data science basic courses took past bachelor degree computer science mainly worked already strucured datasets found various places always cleaned datasets pandas applied supervised semi supervised methods manipulating variables names structure dataframes always example x200b id gender country age 0 242 brazil 40 n 815 f canada 38 x200b decided challenge building dataset indicators years found different free databases first batch headaches data processing data cleaning finally managed obtain two dataframes structure years index names column headers except values different one indicator per df example first dataframe weight name 1 name 2 name name n year 1 138 129 185 130 year year n 174 155 134 220 second dataframe size name 1 name 2 name name n year 1 49 51 49 55 year year n 62 61 59 64 x200b impression merge concat possible since almost 10 000 individuals 50 years names columns labels individuals block obtain kind structure first exemple order apply ml methods usual turned problem around several directions must made mistake somewhere without managing exactly yet dare assume problem must common data science unless wrong perform analysis python case
qy8gfg,0,paper explained learning rate grafting transferability optimizer tuning w rant reviewer 2 last years deep learning research given rise plethora different optimization algorithms sgd adagrad adam lars lamb etc claim special peculiarities advantages general algorithms modify two major things implicit learning rate schedule correction gradient direction paper introduces grafting allows transfer induced learning rate schedule one optimizer another one paper shows much benefits adaptive methods e g adam actually due schedule necessarily gradient direction correction grafting allows fundamental research differences commonalities optimizers derived version makes possible computes static learning rate corrections sgd potentially allows large savings gpu memory x200b outline 0 00 rant reviewer 2 6 25 intro overview 12 25 adaptive optimization methods 20 15 grafting algorithm 26 45 experimental results 31 35 static transfer learning rate ratios 35 25 conclusion discussion x200b paper openreview old paper arxiv
r2p2vo,1,derived computed inputs bad cnns building cnn wondering inputting derived computed inputs generally bad effectiveness cnns nns general derived computed values mean data raw instead computed based raw data example simple form using timeseries data raw data computing 30 day sma derived computed value another input bad practice boosting networks effectiveness bad practice tips kind computed values someone consider adding new inputs goal nn building predictions timeseries data sorry newb question indeed new ml
rfty6i,0,favor volume quality bert based text classifier ill train binary classifier yes samples make 5 percent samples multiple persons labelling pairwise alpha 0 65 scenario label sentence every 10 sentence workers check reliability resulting 52000 single vote samples plus 6000 multiple vote samples together 3000 positive labels scenario 2 tripple label everything resulting 20000 samples majority vote 1000 positive labels experience better quality samples worth volume
rqtl0z,0,create question answering system potentially large corpus text hello guys wondering would go creating query answering system based potentially large corpus text decent exposure nlp realize could use transformer say answering questions squad format reference text small pass together question transformer get answer however would answer questions based say large corpus text possible pass whole text neural network time ask question one option see going text looking similar words sentences however might costly question ideally would make sense create kind knowledge base based provided corpus use get answers entirely sure done guys references best practices thanks much advance
rulr8h,1,model consistently forecasting working time series data trying forecasting next 90 days model ensemble prophet arima model pretty good job terms accuracy almost always forecasting consistently suggestions would fix client expecting sort ups downs actual forecast
rg69um,0,state art methods identifying dag parameters say written directed acyclic graph causal model dozen variables specified functional form corresponding posterior pdfs moreover dataset observations many though variables simplicity let us assume variables categorical methods state art identifying model parameters posteriors graph
qrwuvy,0,must every phd ml know graduating 3rd year phd finally finished program requirements classes etc fully focused research question things average ml phd good point know subjective depends research field common things well versed
r7seqs,1,python interview questions greetings made android app called python interview questions intended python software developers world helpful job interview situations also refreshing many aspects python programming language normal working schedule provides 140 python questions answers code examples knowledge divided 8 categories including data types operators classes oop numpy pandas also random questions game try test knowledge
qoa1tv,0,rare real example true time series anomaly discovered algorithm spite academic work anomaly detection time series almost impossible find real example true anomaly captured wild present example group texas usc released nice large dataset relating electric grids data measured temp voltage etc solar zenith angle etc computed sanity check upon downloading data ran matrix profile look anomalies data found highly significant anomaly shown attached figure guess is‚Ä¶ spoiler took seconds guessed might leap year bug data generator indeed reported found case moral story check data matrix profile useful tool examples time series anomalies b www cs ucr edu eamonn matrixprofile html b c
qt90rt,0,algorithms correlation events issues generally application software co exists multiple software problem one software cascading effect software somewhere else stack e g 1 deploy application pod kubernetes orchestration software 2 pod container running sometime ec2 machine virtual machine pod scheduled run issues 3 ec2 machine vm issues autoscaling software supposed manage vms properly working correctly 4 autoscaling software working correctly dependent system job 5 basically chain issues one issue significant cascading effect many dependent software systems apply logic many places imagine application working properly load balancer working properly load balancer vms networking issues datacenter level failures etc logs files spread across stack reports issues independently usually takes manual effort correlate figure root cause typically user sees symptoms high level stack e g application working properly start debugging finally figure somewhere stack something wrong usually takes specific expertise sre takes time arrive root cause basically happening chain events cascading effect ways catch using monitoring dashboards problem usually monitoring dashboard setup static way manually setup typically maintenance problems long term e change one software version wants change dashboard accordingly might forget modify monitoring dashboard accordingly etc also setting monitoring dashboards specific problem means need setup various different kinds dashboards different systems scenarios want apply ai problem possible want come well trained ai based system tell actual root cause issue give 10 different issues happened around specific time interval e g model detects various issues anomaly detection based software already list issues happened across stack around certain time interval lets say 10 issues let say 10 different issues happens around particular time interval 1 5 mins would like send 10 issues ai based system want tell 10 issues 1 2 issues probably root cause could caused issues ai based system basically correlate multiple issues tell us root cause e basically issues likely caused issues useful possible thoughts guidance greatly appreciated typical algorithms approaches people apply kind problem imagine use case systems send bunch alerts coming assume alert relevant data issue attached ai system process segregate alerts way informs user alert actual issue causing would mean user quickly resolve getting rid alerts many use cases like
rsq1s1,1,load deep learning trained model pt cluster anyone give understanding trained model file pt load cluster like apache spark airflow job scheduling resources repo share highly appreciated
rlaltq,0,know tools libraries frameworks intuitive enough teenagers practical introduction ai hello computer science student trying set workshop high school students give first hands introduction ai specifically computer vision wondering know frameworks tools libraries would raise awareness ai give first incentive teenagers help would much appreciated cheers
r74ejm,0,data search engine machinelearning hi work ds constantly solve problems increasing quality models achieving best result obvious way increase quality add new meaningful data search data ml tasks
rlvyvl,0,architectures exist time series map prediction given problem generating map predicted dust clouds imagine n grid know amount dust cell also know much new dust produced cell intuitively amount dust cell time 1 depend amount dust nearby cells time amount dust produced nearby cells time factor wind simplicity lets ignore win curious architectures exist sort problem initial idea n 2 input shape n 1 output input would current dust map first layer map newly produced dust second layer output would dust map one time step case convolutional autoencoder seems like simplest solution technique used paper models dust exist researching weather prediction models similar problem seen techniques use gans although discussions forum seemed doubts effectiveness briefly looked r cnn methods think would applicable object detection feel free tell otherwise also considered grid small enough could probably get away using convolutions could run straight rnn problem might require lot data converge missing adjacency context convolutions provide ideas much appreciated
raly1j,1,use mathematics machine learning book one stop shop covering math essentials question mostly title aware book omits proofs rigorous math textbook hold relevant material asking think covers essentials sufficiently get one going find knowing looking search general information courses different domains mathematics gets confused good enough map math point trying determine single pill take would suffice full question think topics covered plus reading portions chapter sufficient terms teach getting good map math needed sources oriented ml would recommend address deficiencies thank
rdw0zo,1,deep forest diversity short tutorial hi folks released quick tutorial newcomers junior mleng new framework 2017 deep forest ‚úÖ feel free share thoughts üî¨ cheers
ri7m95,0,discussion important graduate degree always suspect graduate degree needed machine learning industry basically scraped 500 job posts mle basically filter education level key words results want share everyone statistics 500 job posts mle ones mention education level job posts mention education level 71 mention bs 58 ms 44 phd guess takeaway compared specialties ml tend favor graduate degree lot compare swe devops etc necessity u guys agree x200b source
rk4vk0,1,many doubts hints solve problems x200b 1st problem 1 approach something wrong idk take trigonometric functions please tell wrong right thing
r5zkt2,1,giving reviews rating based existing reviews big project working issue run data review rating column missing would like make algorithm take provided reviews ratings csv file make predictions missing ratings question simple tutorial nlp involve deep learning algorithms
r4vums,0,linking decision tree nearest neighbors classifiers hello looking opinions paper collapsing decision tree concurrent data predictor variant nearest neighbors algorithm derived flattening decision tree algorithm particular would interested getting opinions two aspects morphing decision tree nearest neighbors variant evaluating one attribute time b fact predictions decision tree nearest neighbors converge optimum amount training data increases please note focus data categorical attributes anyone else exploring subjects something similar past thanks
rlpls7,1,start ml hey guys undergrad informations technology student go final semester university last semester submit final year project topic chose using machine learning analyze procurement data proposal defense figured would something spend analytics procurement data x200b last semester balancing university full time customer service job time study much topic fairly new machine learning whole would best place start path follow also anyone experience procurement data would great advice spend analytics really way go something cooler would really cool tell source procurement data searched kaggle find helpful data
rssbiv,1,right hardware decision mobile observation robot outside sure question suits subreddit generally wanted build observation mars rover like robot able object detection csi port nightvision camera already got shouldnt also include opportunity expand knowledge go dont train model home laptop already heard intel neural compute stick 2 could used raspberry pi 4 8gb ram also question would neural compute stick enough mobile model training purpose get hardware
r5kjoq,0,oversquashing bottlenecks gnns graph ricci curvature x200b squashing common plight gnns occurring message passing fails propagate information efficiently graph new blog post discuss phenomenon understood remedied concept ricci curvature see paper details second installment post discuss whether diffusion improves graph learning analyzing popular digl rewiring method klicpera et al post part new series graph neural networks lens differential geometry algebraic topology
rvr3lk,1,funtion np c confused np c np array 1 2 3 np array 4 5 6 np c np array 1 2 3 0 0 np array 4 5 6 gives different results
r44r7q,0,combinatorial optimization panoptic segmentation fully differentiable approach recent interest incorporating algorithms layer neural network recent work cops neurips 21 tackle similar task following questions 1 possible train pipeline containing neural networks combinatorial optimization 2 pipeline extendible real world large scale tasks 3 using 1 2 create fully differentiable approach panoptic segmentation answer questions yes show benefits insights training hybrid pipelines contribution 1 backprop combinatorial optimization co layers much recent work direction methods previously applied large scale tasks b non optimal co solvers extend previous work 1 compute better gradient estimates obtain faster convergence contribution 2 transformation backward pass previous works gradient estimation co x argmin x c c x apply loss x perturb costs c incoming loss gradient x case panoptic segmentation scenario x z argmin x c z x c x loss applied z need perturb costs associated z exist remedy problem solve different co problem backward pass compute gradients w r c contribution 3 show differentiable surrogate panoptic quality metric tldr 1 backprop possible co large scale tasks even non optimal co solver 2 achieve 1 smooth gradients backward pass 3 solve another co problem backward pass optimizing variables appearing objective 4 propose panoptic segmentation approach fewer hyperparams better results comparable approaches code available 1 black box backprop
rkmiay,1,need background deegre ml hi question learning python going course ml standford university 3 years biochemistry degree changed career computer science year possible learn get job ml without degree
qsigso,0,discussion data quality suffering first mile reliability problem data product fueled tens hundreds external data sources may relevant schema changes volume anomalies late deliveries plague first mile go infect downstream warehouse tables business processes reliability data sources questionable cascade points failure data team‚Äôs control awareness like learn improve data first mile reliability check latest blog post
rrrjrz,0,spectral clustering still useful impression deep learning revolution spectral clustering incredibly powerful tool explosion unsupervised deep learning techniques wondering anyone still uses spectral clustering settings would expect spectral clustering state art
rbo3c7,1,recursive feature elimination cross validation imbalanced dataset currently new machine learning learned feature selection project dataset 89 majority class 11 minority class also 24 features opted use recursive feature elimination cross validation rfecv scikit learn package find optimal number features dataset also set scoring parameter f1 since dealing imbalanced dataset furthermore estimator used random forest classifier fitting data around 12 features f1 score 0 94 using rfecv appropriate imbalanced datasets
r7t0nu,0,generate read summary research paper twitter using rax bot rax bot twitter bot help read research papers effectively trigger bot replying research paper tweet want summarize mention summarize keyword bot send summary link back twitter bot also follows accounts share research papers regularly automatically reply summary link examples
r5rn66,0,handle advisor hi know title might sound offensive mean deal advisor ph student machine learning last year find thesis topic whenever go advisor responds need wait data supposed come company started looking different projects datasets online work advisor spend time reading many papers keeping updated field thought would find way noticed bring ideas takes master students involves bad thing problem comes asks lead although big novel ideas still way progress research develop strong profile recently shared idea called students work completely removed scene want hear ph students handle situations course want troubles advisor talking help seems aggressive times want suggestions guys manage type situations thanks
rpg57g,1,yolo v5 inference speed slower exported onnx model compared pytorch model hi everyone using official pytorch yolov5 repo perform object detection task trained model using custom dataset saved weights pt file also exported weights onnx model well using export py repo x200b running detect py using pt weights inference speed 0 012 seconds per image however onnx weights require 0 2 seconds per image installed onnxruntime gpu sure gpu utilized run onnx model x200b slower inference speed known behaviour onnx models
rdb1uw,0,uttt ai alphazero like solution playing ultimate tic tac toe browser tl dr developed ai solution mcts nn inspired alphazero playing ultimate tic tac toe game browser try ever since started working machine learning 5 years ago always wanted cool project portfolio reading scientific papers gave plenty ideas read alphazero preprint knew alphazero third paper alphago alphago zero alphazero muzero sequence alphazero paper deepmind generalizes previous work ai learn self play master go also chess shogi read previous papers alphazero specifically sparked imagination probably love simple elegant engineering solutions alphazero mostly discovered ultimate tic tac toe implemented alphazero early 2018 weeks work realized going easy ride two major problems essentially made forget project long time firstly although ultimate tic tac toe uttt looks easier chess go still quite challenging game average length uttt game somewhere 40 50 plies average number legal actions per position somewhere around 7 estimate self play data difficult setup side project one key factors enabling alphazero success massive computing power 5000 tpu v1 self play 64 tpu v2 training figure much cheaper way develop interestingly good ai personal budget secondly envisioned deploying alphazero browser zero knowledge web development frontend general meant find time learn easy already full time job stuff going life decided put whole project hold said maybe one day better time fast forward 2021 left job decided spend year career break pursuing interests realized finally enough time resources conquer project learned basics web browsers html css javascript react bought desktop pc managed incrementally redesign alphazero self play training something executable computer evaluated ai confirmed superior comparison already existing implementations access online websites mobile apps built react app tested finally deployed week differences original alphazero much smaller policy value network architecture designed specifically playing ultimate tic tac toe browser 5 million parameters 20 mb source code total separation self play data generation process policy value network training offline rl crucial change way could succeed single script implements online rl runs 10 weeks desktop broken manageable stages mcts simulations per position training self play data quality quantity initial self play dataset generated pure mcts simulations random playouts faster better random policy value network predictions search simulations synchronous single threaded sequential enabled data augmentation flipping board policy value network training value target mse loss function defined root mean state value rather game outcome masked kl divergence loss policy head instead cross entropy loss auxiliary policy head loss predicting action values next action logits external benchmark compare solution came evaluation setup details main selling point final policy value network checkpoint 1k simulations much better faster mcts random playouts 10m simulations 4 order magnitude difference words policy value network learned useful information ultimate tic tac toe enabling better faster evaluations found publicly available ai ultimate tic tac toe beat best version online found implements mcts random rollouts custom modifications source code keeps first 10 15 moves uttt ai eventually mistakes losses game sometimes draw various technical details takeways project build using react onnxruntime deployed azure static web app shout microsoft providing great service policy value network running browser device using webassembly backend utilizes cpu wanted use webgl backend enables gpu access support convtranspose2d layer yet rewrite retrain policy value network without convtranspose2d learn web dev frontend read entire course twice watched plenty deved videos implemented many small throwaway projects computing hardware developing project intel i7 10700k 8 cores x 3 80ghz 2 x rtx 2080 ti 64 gb ram onnx format great load pytorch model javascript via easily torch jit nad libtorch brilliant tools using pytorch model c works best desktop gaming laptops desktop 75 simulations sec laptop 70 sims sec phone 2 5 sim sec created video showing ai self play 100 000 simulations think recording another video games nmcts2 10k vs mcts 10m evaluation show mcts dominated nmcts2 know ultimate tic tac toe game rules strategy playing ultimate tic tac toe learned ai follows start center square center subgame undoubtedly best move unless want surprise opponent something weird response push corner subgame let next 8 moves played corner subgames breaks corner subgames jump side subgames least useful take still one careful mess maintain overall balance board wait opponent mistake game marathon sprint think twice sending opponent finished subgame able choose move unfinished subgames powerful source code twitter thread
rbvn1s,1,cannot load data file weka download data set type data file open notepad content contains lines like 18 0 8 307 0 130 0 3504 12 0 70 1 chevrolet chevelle malibu convert arff using link type change weka cannot recognise file sry dumb question first time using weka edit anyone similar problem friend mine gave article follow solve itl
ro1s0p,1,minimal requirement classify neural network instance im classifying oranges could use one neural network image need thousands
qr5per,0,paper explained autoregressive diffusion models full video walkthrough diffusion models made large advances recent months new type generative models paper introduces autoregressive diffusion models ardms mix autoregressive generative models diffusion models ardms trained agnostic order autoregressive decoding give user dynamic tradeoff speed performance decoding time paper applies ardms text image data extension models also used perform lossless compression x200b outline 0 00 intro overview 3 15 decoding order autoregressive models 6 15 autoregressive diffusion models 8 35 dependent independent sampling 14 25 application character level language models 18 15 sampling training works 26 05 extension 1 parallel sampling 29 20 extension 2 depth upscaling 33 10 conclusion comments x200b paper
rwuhlg,1,specify nodes feature map get applied different filters layers tensorflow example say wanted apply 1d convolution fft raw time series data first layer say first 400 nodes example use simple feed forward network 1d statistical features remaining say 20 nodes x200b mostly used adding layer able interact node previous layer x200b help appreciated
r5ynbw,1,tsne back original feature space hello assume dataset whose number features 256 apply tsne dataset 2 components 2 dimensions right perform operations 2 dimensional space come data point want expand point original feature size 256 meant flow 256 tsne 2 magic 256 looking magic actually way
rhui36,0,made pytorch modules agent systems starting little evolutionary algorithms project know bit frowned upon noticed working deep neural networks need instantiate separately iterate network forward pass slow even gpu reason made little package pytorch modules main class widelinear behaves family linear layers different running fully parallel single forward pass time even works gpu application outside evolutionary algorithms mostly still agent based systems gradients work expected brief documentation github available pip
qlbye5,0,text image models rudall e kandinsky xxl 12 billion parameters rudall e malevich xl 1 3 billion parameters demo latter available technical report russian technical report translated english google translate demo rudall e malevich xl github repo rudall e malevich xl links post
r5uu53,1,introductory tensorflow course focus computer vision hello might seen daily doses ml posts today want share free introductory tensorflow course put together focuses building convolutional neural networks computer vision tasks check course curriculum decide maybe basic also able ask question stuck hope see class course link x200b course curriculum
rfcrr3,0,difficult easy learn nlp experience cv hi 3 years industry experience different cv deep learning tasks ml engineer recently started scratch surface nlp educational purposes honest quite interesting far could notice nlp cv share concepts
qkfgxj,0,tensorflow perform m1 pro max basically title apple claims tensorflow optimized native m1 chips actually perform
rfkcdy,0,deep learning 2021 year review predictions 2022 hey everyone wondering people thoughts deep learning 2022 little recap predictions little background work data scientist python working supervised deep learning models nlp tasks personal endeavors recently started looking audio models fun played many computer vision image models like clip dall e meaning cool art see luckily next year python library pytorch tensorflow jax pytorch definitely made big moves year research tools like speech brain released audio models like eleutherai gpt neox large transformer models using pytorch also beating tensorflow google trends tensorflow still reigns supreme github 3x stars industry tensor board tfx pipelining really powerful stuff google used papers like bert also huge built anything jax yet looks like got great potential wait try 2022 understanding able run code either gpus tpus brings next prediction cloud platforms gcp aws azure ibm cloud background mainly use gcp biased used aws sage maker lot aws sage maker industry basically unmatched till year gcp released vertex ai service well works way new many people adapt yet additionally also released fully managed kubernetes clusters first time working kubernetes speak talking people say amazing experience azure ibm cloud please fill prediction jax lead huge rise models use tpu google plans release tpuv4 amazon trainium still early access 2022 deep learning might go gcp got tpu kubernetes amazon course still ml industry chokehold everything containerized switching services well still hassle working large deep learning models think might consider moving models gcp shout cool organizations dl ml space hugging face growing like crazy transformers library weights biases always tracking models nvidia flexing hard stylegan3 megatron open ai gpt 3 bought hype bit solved language eleutherai gpt j neo speech brain org cool toolkit released year groups awesome stuff love way much stuff keep track would love broad overview year review 2021 please comment thoughts year think attention transformer models biggest thing year want hear crazy image models really wanna hear people unsupervised learning libraries used 2021 favorite used anyone use python cloud provider use ml switch want use azure ibm cloud hows watson predictions people care much tpu slaughter bots coming one thing sure going hope keeping track cup
r6axta,0,continue training using vgg16 higher resolution dataset hello i‚Äôd like use popular trained model vgg16 trained fixed size 224x224 rgb images continue training dataset thing dataset size 896x896 rgb images still use vgg16 model tweaks need
r0mq3l,0,openai miles brundage ai misuse trustworthy ai might find new gradient interview miles brundage interesting papers touched technology make work better everyone economic possibilities children artificial intelligence future work education leisure taking superintelligence seriously malicious use artificial intelligence forecasting prevention mitigation release strategies social impact language models news that‚Äôs fit fabricate ai generated text tool media misinformation toward trustworthy ai development mechanisms supporting verifiable claims timeline 00 00 intro 01 05 get started ai 07 05 writing ai slate 09 20 start phd 13 00 ai end scarcity 18 12 malicious uses ai 28 00 gpt 2 publication norms 33 30 ai generated text misinformation 37 05 state ai misinformation 41 30 trustworthy ai 48 50 openai policy research team 53 15 outro
r3n2zl,0,inherent limitations gpt 3 wrote little editorial titled inherent limitations gpt 3 negative towards gpt 3 hope rather lays basic facts architectural constraints mainly anyone worried take job lead agi find hopefully relax would love feedback especially corrections
rtgxqy,1,majoring statistics computer science subjects need learn happy new year everyone second year university student majoring statistics interested ml ai possible want apply master degree ml school programming courses covered statistics dept 1 python computer modelling scientists 2 database sql 3 math modelling currently self studying data structures algorithms using c considering learning 1 computer architecture 2 computer network 3 operating systems may ask subjects good fit need please subjects need take adequate knowledge coding part ml thank nice day edit formatting
r5pjtp,1,started learning machine learning today enrolled andrew ng course approach course get get theory projects particular sources etc beginner
rsqktc,0,recent breakthroughs generative models art hey everyone interested recent developments models area neural style transfer also interested applications art neural style transfer feel free point well mostly used models area nowadays point papers thanks lot
qjx4k3,0,iccv2021 oral neural tmdlayer modeling instantaneous flow features via sde generators video explanation tmdlayer inspired stochastic differential equation sde aims model stochastic flow features principle easily added top dnn layer bring benefits addition immediately enables transductive inference inserted model welcome check video quick easy understanding video paper code x200b paper abstract study stochastic differential equation sde based ideas inspire new modifications existing algorithms set problems computer vision loosely speaking formulation related explicit implicit strategies data augmentation group equivariance derived new results sde literature estimating infinitesimal generators class stochastic processes nominal agreement needs application task inherent properties behavior types processes efficiently handle obtain simple efficient plug layer incorporated within existing network architecture minimal modification additional parameters show promising experiments number vision tasks including shot learning point cloud transformers deep variational segmentation obtaining efficiency performance improvements
qka6p0,0,cuda latest version support version pytorch tensorflow greetings sorry could think better place ask question get response regarding pytorch tensorflow cuda version want know install cuda 11 5 support lower version tensorflow torch packages tensorflow 2 4 pytorch 1 71 supported 11 0 cuda version actually want install tensorflow pytorch best option install cuda 11 0 10 1 want know install latest version cuda whether support frame works
rn7w3e,1,os machine learning tasks os use recommend machine learning app software package distros use data management database building conda spread everybody use built pc gpu running ubuntu conda package spyder code writing copy paste kaggle wonder experts say
r9i0a8,1,categorical features image classification training cnn tensorflow large dataset images order predict number classes obvious distinct groups images training set tabular data setting would included training form categorical variable feature way image data obvious workaround would train two separate models one per group assuming 2 groups really best way may categorical features soon separate model combinations practical suggestions
roaey3,1,getting overwhelmed wanted ask guys whether normal get overwhelmed learning machine learning studying since almost last 6 months feel like getting nowhere andrew ng machine learning course also course university based linear regression sci kit learn cnns tensorflow inspite really feel lost read lot stuff subreddit also kenjee discord almost understand everything sometimes things understand worry far behind ever get point confident projects etc
qnbrji,0,acl rolling review work hi folks going acl rolling review process doubts submitting paper open review website 1 accepted reviews comments pdf open review 2 rejected papers rejected withdraw article open review going along rejected reviews open review website main concern rejected submitted another conference plagiarism paper already open review website along reviews affect submission
qrpwsn,0,research wav dataset morse code anyone know obtain dataset containing morse code wav files checked kaggle competition data longer available research
r9k1u5,0,using singular value decomposition compact operators latest paper dynamic mode decompositions something trouble finding standard textbooks operator theory functional analysis including conway course functional analysis pedersen analysis honestly surprising really lines past theorems texts really located discussions bunch pdf online hand even though core data science also appear textbooks either probably due idea real world data finite dimensional almost follows argument save infinite dimensional considerations singular value decomposition compact operators tool come use work overlaps operator theory data science namely convergence theories concerning dynamic mode decompositions decompositions rely strongly finite rank approximation infinite dimensional operator group managed show convergence routines using svd compact operators one papers published past week find linked available freely link limited time publisher paper linked introduced scaled liouville operator compact particular rhks interesting compact subspace workspace get point wise approximation unbounded liouville operator compact operator past certain threshold difference indistinguishable particular data set help students burn memory put together video discussing svd compact operators older video perspective dmd leverage quite strongly liouville operators occupation kernels find sheldon axler steve brunton discuss regular svd done channel think first video youtube extending discussion compact operators though happy shown otherwise let know think presentation cheers paper link
rwcehg,1,best way fuse metadata cnn working gan cnn producing good results apart one area network takes photograph input produces stylised output supervised training model ground truth network optimising toward ground truth different lighting input image need way informing network far tried two approaches first write column pixels input image column colour contains needed lighting data drawback approach causes artifacting output network stumbles around long time learning colour coding second concatenate input tensor forward passes additional channel whose brightness contains needed lighting data might appropriate solution although concerned waste convolution filters would appreciate insight provided particularly best practices thanks
qoqpv2,0,measure distance two domains transfer learning know distances defined minimise domain adaption want know exist distance measurement measure difficulty performing domain adaption source domain different target domainsÔºü
ruwchh,0,spotted post lesswrong anyone verify rather fantastic claims made writing red flags looks interesting enough trouble gpu drivers run right
r6v5dx,1,logistic regression log odds linearity assumption hello trying understand assumption means drew picture think means believe assumption true data separated split correctly straight line left image assumption violated boundary classes highly non linear mixed together right image looking confirmation really thanks follow question true okay fit model simply evaluate validity assumption model accuracy get low accuracy probably means assumption violated high accuracy assumption probably valid
qrygiv,0,phd postdoc positions ut austin ml complex systems chaotic time series cellular automata fluid dynamics hi i‚Äôm looking phd students interested intersection machine learning physics particularly chaos fluid dynamics also informally looking postdocs official ad coming soon based physics department ut austin affiliated oden institute computational engineering sciences link lab website projects pretty flexible based curiosity mutual interest there‚Äôs room algorithm focused time series mining projects well pencil paper dynamical systems control theory problems far applications go we‚Äôre particularly interested projects eventually used biological data fluid dynamics we‚Äôre super open applicants uncommon academic personal backgrounds recent examples ‚Äúchaos interpretable benchmark forecasting data driven modelling‚Äù neurips 2021 ‚Äúdeep reconstruction strange attractors time series‚Äù neurips 2020 ‚Äúcellular automata convolutional neural networks‚Äù phys rev e 2019 applying grad students feel free apply grad programs ut austin physics department due 12 1 oden csem program due 12 15 departments cs ee probably possible postdocs please reach informally physics phd program require physics gre normal gre physics undergrad degree four core courses previously students undergrads cs engineering bioinformatics etc quals research talks written exams sounds interesting feel free email dm chat neurips aps
r18rjz,0,long term career goals 10 years hello guys trying figure long term goals feel reached plateau senior ml engineer wondering people field long term career goals
rbq46a,0,knowing features affect target variables neural network currently working blood glucose forecasting using lstms course insulin decreases blood glucose carbohydrates increase blood glucose way feed network using keras simply inputting insulin carbs array network know certain feature affect forecast e negative positive correlation incorporate network sure specific problem ml prediction forecasting problems would know particular feature increases decreases target variable utilise information
rd2ngm,1,complete production example anyone know great complete ml project example don‚Äôt mean saved model data pipeline working app etc
rv5yj2,1,nn vs lookup table hi assuming one collected 24 pairs input output datasets target system x200b one create simple lookup table describe input output behavior utilize controller one also train dnn model learn relationship benefit using dnn case opinion dnn one store whole dataset lookup table one gives new input value included training dataset trained dnn would perform better since case lookup table predicted output extrapolated value previously known output benefits justify using dnn
rb4c2w,0,go creating data driven model trying get bead people optimize models create ml models scratch particular problem say given dataset told predict attribute approach initial data visualization mining gathered seems like googling around model worked previously similar model modifying works bit better use case seems common practice wondering would strategy models available similarly decently performing model optimize last 2 3 increase performance without overfitting
r3t02h,0,unsupervised topic segmentation meetings bert embeddings research paper walkthrough paper authors propose bert based unsupervised topic segmentation method task dividing multi person meeting transcripts topic blocks üî• finally online meeting recordings useful paper walkthrough paper
r73sq8,1,multi task learning done hi guys recently heard concept multi task learning call mtl read articles online watched youtube videos topic however aspects understand assume project want mtl 3 seperate tasks know base model extract features three models top tasks understand train models done simultanously yes three set gradients backpropagated task update base model ho handle train base model
rdff22,1,model used document extraction camscanner microsoft lens etc want start small project create model would extract document picture rescale something like camscanner microsoft lens apps gathered small dataset prototype concept sure might best approach label data 1 using bounding boxes might work best locate document would bring noise since picture might angle document could held hand etc might require processing eliminate background noise 2 using mask r cnn probably good job isolate document guess would tricky reshape center later since possible get irregularly shaped mask example someone holding hand finger holding document might get excluded mask extrapolation needed probably 3 idea use keypoints like pose estimation models keypoints would edges document would connected straight line isolate document center anyone worked type problem idea apps mentioned handling probably approaches used unaware
r5o8q0,1,parameter tuning method called work hello work meteorological service canada suppose want tune parameters prediction system however heavy prediction system like weather forecasting supercomputer takes hour finish one run get score result good prediction ten parameters tune parameters try next training nn even examples model parameters output scores use nn try thousands combinations parameters next set parameters run hour real best parameters according nn guess hour run retrain nn new example repeat name work well example articles discussions better way thanks
qp8897,0,open nsfw 2 tensorflow 2 implementation yahoo open nsfw model detecting suitable work nsfw images particular pornographic images high demand task computer vision yahoo open nsfw model originally developed caffe framework favourite choice work discontinued caffe also becoming less popular open nsfw 2 project provides tensorflow 2 implementation yahoo model references previous third party tensorflow 1 implementation please take look
qq18tc,0,improving segmentation masks dataset segmentation masks objects better polygons around objects interested quality good polygons around objects correct rough low number edges huge chunks background algorithmic way improve tried grapcut performance good huge chunks background still included stuff like hair done poorly
rkl5ev,0,weather station project get trend symbols powered sklearn rbf trend prediction hello fellows x200b weather station project raspberry pi bosch bme280 temp hum pressure sensor want show trend prediction symbols raw input sensor variies want become touch ml want use sklearn rbf interpolation temp hum pres data already working trend prediction working want say getting warmer etc according model comprae last raw values doesnt work good raw values fluctuating x200b think think calculate second confidence german 2 konfidenz lokale extremwerte know local minimum maximum likely temperature rising sinking x200b weather station uses rbf kernel draw nice smooth line temp hum pres feed numpy array sklearn rbf get result numpy array result based 1 dont know calculate predict im al local extrema therefore change trend symbols x200b 1 x200b think x200b edit fixed typos
rev18h,0,announcing transactions machine learning research announcement new ml research journal post we‚Äôre happy announce founding new journal transactions machine learning research tmlr journal sister journal existing well known journal machine learning research jmlr along proceedings machine learning research pmlr jmlr machine learning open source software mloss however departs jmlr key ways hope complement community‚Äôs publication needs notably tmlr‚Äôs review process hosted openreview therefore open transparent community another differentiation jmlr use double blind reviewing consequence submission previously published research even extension allowed finally intend work hard establishing fast turnaround review process focusing particular shorter form submissions common machine learning conferences
qr6bu6,0,discussion plaforms frameworks backtesting regression testing lots models need test lots models submitted different teams baseline curated datasets also updates new data comes field models may different preprocessing requirements need retrain models updated data evaluate models archive reports models configuration management probably need queued something like slurm along need asynchronously monitor performance multiple dgx servers commercial tools manage testing fleet models data must work offline
r90f3v,0,embedding consecutive video frames close latent space embed consecutive frames video close would mean walk latent space get video instead visually similar frames vae think may prove useful example world models make easier predict next state thanks fact close latent space think idea somebody already try papers topic
ra1vy3,1,artist seeking learn high res image synthesis hey r learnmachinelearning photographer artist preparing small gallery show february experimenting vqgan clip taming transformers well image scraping tools like flickr scraper likes hopes achieve generative images based famous artworks hoping kind folks sub provide little guidance want produce ai generated photo realistic images source images well known photographs tried creating image segmentations feed taming transformers appears handle photographs involving people well landscape images way get somewhat photo realistic images generated ai
r4ra5g,0,patrickstar pytorch based training framework train faster larger ptms github paper last month team tencent open sourced patrickstar pytorch based ptm pretrained model training framework could train larger model better performance hardward environment compares sota training frameworks like deepspeed version 0 4 1 performance 8xv100 node key idea project use chunk based memory manage strategy means group parameters chunks size dynamically move chunks cpu gpu compare deepspeed would determine part model need put cpu memory training dynamic memory scheduling would make better use gpu memory namely params involve computing moment loaded gpu results larger model also designed efficent prefetch sharding mechanism based chunk based method bring us nice performance large ptm becoming must nlp believe patrickstar would help researchers engineers would nice could checkout github repo give us feedback
qr9ndf,0,machine learning tutorial r anybody know good tutorial would help r model predict probability home run given ball play explanation chosen model features visualization model outputs identify home run least likely home run non home run likely home run describe think model classified plays less accurately others advice get started model choose would helpful well want answers want learn far model overall prediction accuracy 98 compared 90 completely random prediction accuracy 63 least one predicted actual result home run 3 accuracy random model models using knn random forest
rvo19z,1,start generating new images i‚Äôm reading gan dcgan msggan bmsggan head spinning set 500 1000 black white photographs i‚Äôve taken want use dataset want output similar looking entirely new images it‚Äôs art project low level coding abilities seems like image size significant constraint i‚Äôm fine begin resizing data set supersampling later still rectangular though seemed problem looking start what‚Äôs right tool i‚Äôd appreciate help guidance edit don‚Äôt want use anything
rj5xak,1,build user user recommender tensorflow recommenders title want make recommendation system specifically recommends profiles people people similar tinder bumble functions possible tfrs far i‚Äôve seen examples user item interactions user user per se please tell there‚Äôs resource could refer thank much
rggtdu,0,exciting new effort develop synthetic data genomic research
rs0vl3,1,decent summer college programs offering machine learning programs high school students i‚Äôm high school student interested computer science specifically ai machine learning currently learning tensorflow although i‚Äôm still beginner next summer wanted go college summer program preferably offers college credit however can‚Äôt find related ai ml offers credit anyone knows suggestions please let know also side note realistic high school student getting medal kaggle
rg8rw2,0,padl unifying open source development framework functional api pytorch better build deep learning models happy announce new opensource project brings functional building models pytorch padl allows easily build pipelines using pytorch layers along normal python functions classes together padl pipeline builder pytorch may used great pytorch functionality used writing layers allows users build pre processing forward passes loss functions post processing pipeline models may arbitrary topologies make use arbitrary packages python ecosystem allows converting standard functions padl components using single keyword transform github notebooks try website pip pip install padl
qrknk3,0,favourite annotation platforms tried annotations platforms redbrick ai v7 labs supervise ly none quite hit sweet spot use case mainly care uploading pre annotations semantic segmentation network double checked human annotators aforementioned platforms offer service involves fiddling respective sdks always work well anyone share favourite annotation platforms
r4fjus,0,discussion anything similar google pathways currently many probably know google creating pathways model many things released details timeline wondering generalizable learners use
rcbvnb,1,deepfashion mmfashion reading getting started page github says python demo test py input input image file somebody explain would get point want create rec system usually go existing projects none online
rhtdrw,0,know make rudalle generate similar images input one optimizing text embedding input image input text elon musk result image colab runs memory disclaimer scientist developer fact industrial designer wanted see make tool would generate variations input pictures case design objects like headphones first tried clip guided image generation make changes images lacking creating particular objects seems retain object features quite well forest input image method optimize text embedding transformer order make output closer input image thing fine tuning optimizing text embeddings instead model weights modify model forward pass make retain gradient sorry messy code aim however take one step achieve text based image modification original example openai without using image prompts also anyone know possible reverse text embedding models guess would interesting see final image accounts
rpl389,0,categorical features image classification training cnn classifier tensorflow large image dataset obvious distinct groups images training set tabular data setting would included training form categorical variable feature way image data obvious workaround would train two separate models one per group assuming 2 groups would way may categorical features soon separate model combinations practical looking algo library takes pixels categorical data input training suggestions
rfczeg,1,happens output two lstm bidirectional lstm implementing blstm confused happens output 2 lstm forward backward supposed 3 words 4 length embedding e input form 4x3 one group people say output two lstm concactenated e 4x3 goes lstm independently suppose output 4 4x3 matrix output lstm result conactenated fully meaning result 12 length vector another group says output two lstms goes activation function independently meaning output two lstm two 4x3 matrices go activation function word time first word fore example 4 4 inputs activation function another group say output concactenated word word basis meaning final output 8x3 matrix 4 forward 4 backward currently using older papers use mix original paper using something closer third option thanks
rweffw,1,finding good dataset diagnosing crop disease project hi decided work machine learning project want build app used diagnose crop diseases would simply snap picture crop would use machine learning predict disease associated picture critical step project finding good dataset could anyone point direction website would dataset use project
r5bkph,0,important initialization could share papers discuss topic initialization theoretical point view feeling topic usually overlooked know justifiably anecdotal experience initialization crucial moreover simplistic experiments small 3 layer nn hard come non trivial initialization allow loss decrease function learn nn identity function one example kind papers thinking one lazy training differentiable programming asking maybe someone knows wants share paper find interesting
rchlwg,1,deepwalk embeddings differentiates nodes degree zero hi using deepwalk publication implementation order embed nodes network latent space mistake unconnected nodes network deepwalk able generate random walks implementation random walk returned would contain node interestingly found model actually learned separate nodes degree 0 ones surprising model also found clusters within nodes different nodes connected anything network different representations thank much input
rbbv4g,0,clip nerf explained zero shot text guided object generation dream fields ajay jain 5 minute summary casual gan papers like generative art love get whole lot crazier ajay jain minds google behind original nerf dropped hot new paper right thought putting together clip nerf actually dream fields possible train view consistent nerf object without images using text prompt dream fields leverages fact object e g apple resemble apple regardless direction look one core features clip basic setup simple render randomly initiated nerf random viewpoint score image text prompt update nerf repeat convergence juicy details well continue reading find full summary blog post dream fields chair shape arxiv code released subscribe casual gan papers follow twitter weekly ai paper summaries
r3j0d7,1,courses books pdfs take action eda done ml gotten pretty rusty started new project totally blanking data response eda conducted could fine people recommend resources cover actionable steps response exploratory data analysis edit lol downvoted eda crucial part learning ml throw data giddy alrighty
r2075f,0,sure formulate correctly work done learning perception constitutes reward using perception learning let say agent maze searching apples everytime viewpoint passes apple gets reward attempt make agent first learn association passing apple reward signal using perception training feel one big problems still give detailed feedback agents reason reward signal subsymbolic speak agent cannot communicate reason agent learn recognise reward perception could communciate effectively sure someone must thought something like already know term google
rb3tfn,0,data augmentation nlp hi like ask think experience data augmentation nlp particularly intent based chat bots know common practice computer vision whether downsides synonym replacement resulting thousands examples within one intent occur dubious ü§î common sense tells help overfitting long keep dialog balanced see downside training time love hear inputs üôÇ
rbue4h,0,us gov launches ml competition predict snow water remote sensing data 500 000 prize pool x200b seasonal mountain snowpack critical water resource throughout western u snowpack acts natural reservoir storing precipitation throughout winter months releasing snowmelt temperatures rise spring summer meltwater becomes runoff serves primary freshwater source major streams rivers reservoirs result snowpack accumulation high elevation mountains significantly influences streamflow well water storage allocation millions people snow water equivalent swe commonly used measurement water forecasts combines information snow depth density swe refers amount liquid water contained snowpack depth water would result column snow completely melted water resource managers use measurements estimates swe support variety water management decisions including managing reservoir storage levels setting water allocations planning extreme weather events past several decades ground based instruments including snow course snowpack telemetry snotel stations used monitor snowpacks ground measures provide accurate swe estimates ground stations tend spatially limited easily installed high elevations recently high resolution satellite imagery strengthened snow monitoring systems providing data otherwise inaccessible areas frequent time intervals given diverse landscape western u shifting climate new improved methods needed accurately measure swe high spatiotemporal resolution inform water management decisions goal challenge estimate snow water equivalent swe high spatiotemporal resolution western u using near real time data sources prizes awarded based accuracy model predictions write ups explaining solutions described getting better swe estimates mountain watersheds headwater catchments help improve runoff water supply forecasts turn help reservoir operators manage limited water supplies improved swe information also help water managers respond extreme weather events floods droughts seasonal mountain snowpack critical water resource throughout western u snowpack acts natural reservoir storing precipitation throughout winter months releasing snowmelt temperatures rise spring summer meltwater becomes runoff serves primary freshwater source major streams rivers reservoirs result snowpack accumulation high elevation mountains significantly influences streamflow well water storage allocation millions people
rvzhnh,0,interpolation extrapolation linearisation prof yann lecun dr randall balestriero special machine learning street talk episode yann lecun thinks specious say neural network models interpolating high dimensions everything extrapolation recently dr randall balestriero dr jerome pesente prof yann lecun released paper learning high dimensions always amounts extrapolation discussion completely changed think neural networks behaviour intro talk spline theory nns interpolation nns curse dimensionality yt pod references learning high dimension always amounts extrapolation randall balestriero jerome pesenti yann lecun spline theory deep learning dr balestriero baraniuk neural decision trees dr balestriero interpolation sparse high dimensional data dr thomas lux
rglz0f,0,reaction beautiful moment ever moments pure glee developing networks encoder decoder type network custom designed intermediate function acting upon output encoder thus far training per batch took north 180 seconds batch 8 number samples took holistic training time north 13 hours got reduced 1 second per batch training never felt happier
r277kg,1,supervised semi supervised unsupervised self supervised learning first began learning machine learning difficulty understanding exactly supervised unsupervised learning wrote article describing understanding addition semi supervised self supervised learning hope like
qmj7tq,0,local latin hypercube refinement multi objective design uncertainty optimization many real world systems consist input features aleatoric e irreducible uncertainties engineering design applications uncertainties may arise production tolerances operational conditions well environmental factors thus distribution features measured extent modified e g moving mean value design uncertainty optimization seeks find distribution parameters input features optimize metrics failure probability variance key performance indicators besides expected objective values since often require uncertainty quantification black box functions whole process computationally quite burdensome work propose using machine learning methods combination sequential sampling reduce required amount computation accelerate uncertainty optimization task due small data setting limit investigation gpr svr argue suitable model exist depending problem could used within proposed framework instead
rngzdq,1,gpt j fine tune create sales copy creates kind result compare fine tune gpt3
rb62hd,1,code ubuntu hi everyone generally use windows python via anaconda jupyter notebook today installed ubuntu laptop order learn well parallely already spent much time understanding basics like install package launch jupyter notebook terminal etc want know anyone recommend brief tutorial ubuntu terminal enough get started
r7kj61,0,neural noise one cool anyone know causes messing around style transfers lot especially ones based illustrations specific type noise x200b good example noise splotchy rgb noise varies lot size x200b gif see developing still see noise really well
rd20n0,1,increasing accuracy textual data analysis corpus 2 billion words soroco ingest 200 million 2 billion words course model training analysis single team workers using scout product blog post talk tips tricks might use increase accuracy models including appropriate processing text purpose leveraging standard techniques machine learning demonstrate showing represent text high dimensional vector space applications toy regression problem
raq8ti,1,anyone help multi part question question 1 suppose working phone company wants predict churn let actual binary random variable meaning customer actually churns let flag binary random variable meaning certain classifier predicts customer churn note classifier makes yes binary predictions base rate churn p actual 1 0 05 similarly p flag 1 fraction customers predicted churn define p actual 1 flag 1 b p actual 0 flag 0 expensive data mining consultant dr zuckerberg claims train classifier achieves b 0 9 explain english meaning dr zuckerberg‚Äôs claim b would classifier useful making decisions c write confusion matrix show obtain p actual 1 p flag 1 terms entries confusion matrix show obtain b terms entries confusion matrix e give precise clear argument shows dr zuckerberg wrong classifier possibly exist f possible dr zuckerberg fooled falling victim leakage
raznry,1,üíäyour daily dose machine learning onnx framework series posts post almost daily call ‚Äúyour daily dose machine learning‚Äù last week posted different approaches use deploy tensorflow model c one approaches onnx runtime week want share tips insights onnx open neural network exchange framework whole ecosystem allows standardization neural networks here‚Äôs problem onnx trying solve many machine learning frameworks libraries pytorch tensorflow scikit learn famous examples one use well answer depends many factors free launch onnx aims make choice easier basically choose whichever framework want train model you‚Äôre happy model transform onnx format keep production code working onnx onnx help achieve 2 things keep options open comes choosing machine learning library training standardize production code since you‚Äôll need maintain one main dependency onnx moreover use onnx models python c languages onnx upcoming days connect favorite social network x200b
rpmcot,1,collection free python courses hi guys collected free python udemy hope help every one start learning python data science ‚Äì great learning introduction python programming learn python 3 scratch python absolute beginners practical python python oop object oriented programming python
qkbfst,0,nlp model chatbot inference 11 gb gpu hello everybody i‚Äôve found amazing huggingface library awesome piece work would like train chatbot existing dataset several datasets e g pile training fine tuning model gpu memory limitations 48 gb gpu available inference gpu 11 gb available inference feasible real time e around 3 seconds model adjustable e source code available change structure model model best taking account requirements probably one best models gpt j think inference needs 11 gb gpu models huggingface library fully customizable e layers etc
rwrv2l,1,cannot install pycaret getting error x200b error failed building wheel scikit learn failed build scikit learn error could build wheels scikit learn required install pyproject toml based projects python version 3 9 5 pip pep date latest versions pretty much tried everything stackoverflow still facing issue anybody faced would idea really appreciate help thank
qriz01,0,landmark annotations blender building synthetic dataset images landmark prediction task using blender looked main data generation libraries available blender github vision blender blenderproc zpy find support landmarks go implement anyone pointers missing thanks update following script write coordinates vertices rendered image import bpy scene bpy data scenes scene camera bpy data objects camera obj bpy data objects cube matrix camera matrix world normalized inverted create new mesh data block using inverse transform matrix undo transformations mesh obj mesh preserve data layers true mesh transform obj matrix world mesh transform matrix get world coordinates camera frame bounding box transformations frame v v camera data view frame scene scene 3 lx ly v mesh vertices co local v co z co local z z 0 0 vertex behind camera ignore continue else perspective division frame v v z z v frame min x max x frame 1 x frame 2 x min max frame 0 frame 1 x co local x min x max x min x co local min max min lx append x ly append coords f x n x list zip lx ly open log txt w f f writelines coords
r75uix,0,new library make clip guided image generation simpler different ways generate images text descriptions one powerful approaches generate synthetic art clip guided image generation provide new python library incapsulates whole logic clip guided loss one pytorch primitive simple api provide clip guided loss using different clip models original clip models openai ruclip model sberai multiple prompts texts images targets optimization automatic detection translation input texts also provide tiny implementation vqgan clip based library vqvae sberai opinion best version vqgan publicly available make text image library need integrate text powered losses image synthesis pipelines adding lines code find library pypi package available
rr95gq,0,semantic search finetune realtime using clip personal project made release clip almost year ago provides web front end search images within local folder supports search text images drag drop think interesting feature like button finetune input query real time still lacks functions like save results export finetuned embedding planning add features next weeks ideas comments welcome
qxlsmm,0,imperial college london researchers propose novel randomly connected neural network self supervised monocular depth estimation computer vision depth estimation one fundamental problems computer vision it‚Äôs essential wide range applications robotic vision surgical navigation various deep learning based approaches developed provide end end solutions depth disparity estimation recent times one method self supervised monocular depth estimation monocular depth estimation process determining scene depth single image disparity estimation bulk models use u net based design although relative depth perceived easily humans task machine proven quite challenging due absence optimal architecture tackle issue complex architectures chosen generate high resolution photometric output hamlyn centre‚Äôs research team imperial college london introduces unique randomly connected encoder decoder architecture self supervised monocular depth estimation model architectural design capable extracting high order features single image loss function imposing solid feature distribution credited idea‚Äôs success quick 5 min read paper imperial blog
qup0fe,0,discussion thoughts manually modifying model output optimistic results hi currently working freelancer delivery company predicts order estimated time arrival eta using machine learning strange information saturated delivery area whether weather traffic etc getting model prediction check saturation add x minutes model predicted eta thus manually modifying model output optimistic results opinion bad practice would take approach
rbpj3s,1,üíäyour daily dose machine learning converting deep learning models onnx format series posts post almost daily call ‚Äúyour daily dose machine learning‚Äù mentioned yesterday post week sharing tips insights onnx tensorflow developer pytorch developer first step use onnx production would transform model onnx format transform tensorflow models onnx format use tool called tf2onnx image shows use tool learn tool github repo you‚Äôre pytorch user transform model torch onnx format directly using pytorch done using module torch onnx example connect favorite social network
r7e49h,1,recommendation getting started studying ml general ai need podcasts work hello ether starting first college level courses ml next semester wanted get head start started taking udamy courses work want though i‚Äôm working listen podcasts maximize time question 1 anyone know good podcast goes z regards learning ml ai subjects related matter starting scratch general understanding technology 2 podcast follow keep date field see what‚Äôs coming pipeline thank help safe travels
qqu6xh,0,rebooting acgan new gan achieves sota results harmonizes various architectures adversarial losses even differentiable augmentations neurips 2021 research team pohang university science technology introduces new type acgan rebooted auxiliary classifier gan reacgan overcome unstable training poor generation performance acgan quick summary paper gradient exploding classifier acgan cause undesirable training collapse simply normalizing feature embeddings resolve problem using normalization technique propose rebooted auxiliary classifier gan reacgan reacgan achieves state arts generation results benchmark datasets reacgan harmonizes various gan architectures dcgan resnet big resnet stylegan2 adversarial losses differentiable augmentations ada diffaugment arxiv
qm6l31,0,paper explained efficientzero mastering atari games limited data full video analysis reinforcement learning methods notoriously data hungry notably muzero learns latent world model scalar feedback reward policy predictions therefore relies scale perform well however rl algorithms fail presented little data efficientzero makes several improvements muzero allows learn astonishingly small amounts data outperform methods large margin low sample setting could staple algorithm future rl research x200b outline 0 00 intro outline 2 30 muzero recap 10 50 efficientzero improvements 14 15 self supervised consistency loss 17 50 end end prediction value prefix 20 40 model based policy correction 25 45 experimental results conclusion x200b paper code note code yet release video
rm1da7,1,much mlops engineers know inner workings algorithms much mlops engineers know inner workings machine learning algorithms think job mlops engineers taking models data scientists deploying production assets integrated within business operations let us know thoughts comments üëáüèª machinelearning datascience mlops view poll
r688iw,1,somebody explain classification reports meaning somebody explain classification report done sgd classifier 79 precision 100 recall 0s 0 precision 0 recall 1s
rlli7q,0,axon deep learning elixir repo hi everyone wanted share passion project mine hacking little bit ask op would bother creating dl framework another programming language use tf pytorch jax insert framework set unrealistic goal overtaking existing tooling really enjoy learning internals modern dl frameworks algorithms etc really like elixir said also convince anybody elixir axon better python existing stack share something think cool disclaimers aside little bit axon built top elixir nx project supports automatic differentiation jit compilation using xla heavily modeled jax keras like model creation api support pretty much anything find modern framework well custom layers implementing anything already optax inspired optimization api creating composing gradient based optimizers pytorch ignite inspired training validation testing api creating instrumenting training loops validation loops testing etc also actively developing library top axon converts onnx library still wip lot features still hacking away multi device support found pretty useable every example implemented far feedback questions always welcome
rak5zr,0,working secret field 1 immensely rich 2 moves breakneck pace ‚Äì spent morning scrolling hundreds new papers published neurips year got added existing queue read arxiv papers sits alongside queue various bits mathematics like understand better eventually get make way backlog research deadlines point feels like never ending mountain things things read things learn ml sense great makes imo stimulating exciting hand exhausting often makes hard set boundaries work rest life feel getting tired less happy less productive yet seemingly loss comes thinking ways spend free time three years ml time seems sucked ability imagine meaningful ways spending time interested hearing kind outlets people community fit every day lives
rr6eb8,1,set questions give end user idea machine learning implemented business case looking build ml service org needed understand use case validation questionnaire answered give us idea implementation ml case valid
rud2m5,0,tensorflow keras implementation vision transformer image worth 16x16 words vit excellent results compared sota cnns requiring fewer computational resources train paper code
qkf6mt,0,anyone else received e mail iclr review got e mail openreview single review paper went openreview website see deleted anyone else similar experience
rjzlf7,0,simple questions thread please post questions instead creating new thread encourage others create new posts questions post instead thread stay alive next one keep posting date title thanks everyone answering questions previous thread
qrte64,0,discretize data use mask prediction representation learning fields vision speech lot recent papers learn representations masking parts input predicting original often refer masked language modeling mlm bert style pre training say must first discretize continuous input discrete tokens e g using vq vae making predictions e classification task possible tokens dictionary one argument predicting discrete tokens allows model learn high level concepts whereas making prediction original input space e g raw pixels force model learn high frequency low level details useful representation learning compression also computationally prohibitive since original input space likely high dimensional however question regression continuous latent space masked positions e g predicting latent representation learned continuous vae masked positions instead classification discrete latent space e g predicting discrete tokens learned vq vae masked positions theoretical advantage using discrete tokens instead continuous latents
reo2yy,1,need help cnn lstm model hi guys i‚Äôm trying train cnn lstm model videos convert video data cnn read previously i‚Äôve used models offered keras like vgg19 wondering way convert without need keras model
rlin1a,1,ml classification supervisedlearning hey everyone looking algorithm helps sort values sensor three classes f e nan 1 2 3 lot trainingdata already declared time value class best one choose supervised learning classification algorithm would suggest find better way solve problem
qtik3a,0,lyric studio artificial intelligence song lyrics created ai generated song lyrics app thought subreddit would love pytorch text generation also live sentiment analysis background shading link feedback improvements
qv1t0h,0,labeling tool id dataset want create id dataset 10 classes however find tools allow id assignment closest thing found cvat allow create bounding box specific id anyone know tools suitable multiclass identification
rick1i,1,take ms computer science ms specialized artificial intelligence deciding masters degree would taking ai degree consists 10 cs courses rest machine learning taking courses andrew ng coursera prepare hand cs degree comprehensive coursework cs like databases theoretical computing software engineering contains machine learnign courses asking want know employers practictioner view one based readings cs degree offers flexibility employmeny ai degree project ml path right i‚Äôm leaning towards ai deg heavily works mulitple algorithms cs degree offer
rai9r9,0,reviews rebuttals rejected icml neurips submissions increase transparency review process hopes might help others shape papers rebuttals sharing reviews rebuttals icml submission baller2vec neurips submission baller2vec neurips submission baller2vec rejected notably received highly negative reviewer three submissions reviewer 2 reviewer 7vsi reviewer grxc respectively bummer particularly clear reviewer negativity caused another reviewer lower score icml neurips submissions baller2vec reviewer claimed expert sport machine learning yet repeatedly mischaracterized misunderstood aspects models going far state methods theory section explained needlessly complicated uninformative manner icml review contrast three icml reviewers said writing clear reviewer 5 quite enjoyed reading paper reviewer 1 writing clear reviewer 4 even ignoring ratings think raises important questions means peer reviewed given probably many hundreds thousands researchers field multi agent spatiotemporal modeling individual given three opportunities judge work behalf community anonymous opinions considered representative additionally found dataset criticisms reviewers incredibly frustrating notably reviewer mention anything additional datasets icml review neurips reviews least two multi agent trajectory modeling papers accepted neurips year evaluated methods single large real world dataset 1 grin generative relation intention network multi agent trajectory prediction evaluated method small simulated dataset 50k training sequences preprocessed nba dataset 100k training sequences 2 collaborative uncertainty multi agent trajectory forecasting evaluated method nuscenes dataset 1 000 scenes argoverse 206k training sequences seem like expectation evenly applied reviewers
rcl0l0,0,quick tips building chatbot gpt 3 gpt j hello realize questions people trying leverage gpt 3 gpt j next chatbot usually questions always 2 things format requests model understands conversational mode model keep history conversation answering 2 points quick article hope help question please hesitate ask
r9yzub,0,opinion areas deep learning explored many questions still remain unanswered tremendous progress different areas loss landscape optimization architectures etc opinion areas problems important received much attention opinion initialization get attention deserves seems people accept standard guassian initialization think lot potential use initialization schemes experience using default initialization schemes pytorch offers sometimes leads neural net bottleneck information propagated forwards backwards usually fiddling initialization works wonderfully opinion deserves attention get
quddpi,0,open accessarticle detection bovine mastitis raw milk using low cost nir spectrometer k nn algorithm recently published applied science paper people community may find interesting open access therefore one problems access abstract among bovine diseases mastitis causes high economic losses dairy production system nowadays detection field conditions mainly performed california mastitis test considered de facto standard however method presents problems slowness expensiveness chemical reactive process deeply dependent expert‚Äôs trained eye consequently highly imprecise aim work propose new method bovine mastitis detection field conditions proposed method uses low cost smartphone connected nir spectrometer solves aforementioned problems slowness expert dependency disposability chemical methods method uses spectra combination two k nearest neighbors models first model used detect presence mastitis second model classifies positive cases weak strong resulting method validated using leave one technique ground truth obtained california mastitis test detection model achieved accuracy 92 4 one classifying severity showed accuracy 95
rksnv1,1,project advanced methodology intelligent diagnosis systems processes goals intelligent diagnostics systems processes ‚Äì increasing level fault tolerance diagnostic object reducing probability incorrectly determining state systems errors third kind classifying state systems monitoring systems obtaining stable effective estimates unknown values design parameters functional elements systems corresponding found state choice rational control law object diagnostics identified basis forecast state algorithm numerical solution problem intelligent diagnostics systems processes base agent oriented approach means reinforced learning 1 data pre processing 1 1 structural analysis diagnostic object construction fault trees corresponding event trees identification probable scenarios system failures whole caused single multiple defects functional elements basis monitoring data state prototypes system operation 1 2 formation subset controlled state variables values quantities check measuring instruments 1 3 training sample alternatives dataset values quantities generated well corresponding confidence intervals measurement accuracy regime control variables controlled state variables based data experimental studies diagnostic object healthy state 1 4 data cleaning anomalous values quantities outliers 1 5 identification complete mathematical model system based solution problem multicriteria optimization stochastic formulation using dataset clauses 1 2 1 3 1 6 search values objective functions corresponding boundaries intervals operating modes diagnostic object healthy state based solution direct analysis problem using complete mathematical model paragraph 1 5 1 7 searching solutions inverse problems interval analysis failure scenario system whole values boundaries intervals design parameters controlled state variables correspond healthy state diagnostic object based solution multicriteria optimization problems stochastic formulation based results obtained database formed contains boundaries intervals acceptable values design parameters controlled state variables scenario failure system whole data used tolerance control states diagnosed object 1 8 total dataset alternatives generated alternative includes subsets design parameter values regime control variables controlled state variables objective functions total dataset includes subsets alternatives corresponding different failure scenarios system whole including healthy state values controlled state variables objective functions alternatives obtained basis modeling characteristics using complete mathematical model well directed experimental studies diagnostic object defects functional elements 1 9 data cleaning anomalous values quantities outliers normalization data 1 10 cluster analysis ‚Äì determination possible number states failure scenarios system whole using regime control controlled state variables paragraph 1 8 number identified clusters coincides specified one distances clusters statistically significant subset monitored state variables considered complete otherwise diagnostic object equipped new measuring instruments see paragraph 1 2 1 11 development robust metamodels using data paragraph 1 8 1 9 multidimensional logistic regression form analytical dependences posterior probabilities different failure scenarios system whole regime control controlled state variables b multidimensional observer model form analytical dependences controlled state variables design parameters regime control variables c multidimensional diagnostic model form analytical dependences objective functions regime control controlled state variables 2 monitoring state diagnostic object operation 2 1 measurements made values regime control monitored state variables current moment operation diagnostic object 2 2 determination scenario maximum posteriori probability realization corresponds observed values quantities paragraph 2 1 based solution classification problem using metamodel 1 11a 2 3 tolerance control diagnostic object uses database paragraph 1 7 containing values boundaries intervals acceptable values monitored state variables scenario defined paragraph 2 2 2 4 robust estimation values design parameters functional elements corresponding scenario defined paragraph 2 2 diagnostic object based observed values regime control controlled state variables paragraph 2 1 search values design parameters carried basis solving problem multicriteria optimization stochastic formulation using metamodel 1 11b 2 5 tolerance control diagnostic object using database paragraph 1 7 containing values boundaries intervals acceptable values design parameters scenario defined paragraph 2 2 2 6 robust estimation values objective functions corresponding scenario defined paragraph 2 2 diagnostic object based observed values regime control controlled state variables paragraph 2 1 using metamodel 1 11 c 2 7 development robust multidimensional models control state diagnostic object form analytical dependencies predicted measured values regime control controlled state variables monitoring process 2 8 forecasting multidimensional time series controlled state variables based multidimensional models control state diagnostic object paragraph 2 7 2 9 reducing dimension space controlled state variables based analysis informativeness variables robust multidimensional models control state diagnostic object sensitivity analyzes estimating rank time series cointegration 2 10 determination state diagnostic object based solution classification problem using metamodel 1 11 predicted values regime control controlled state variables 2 11 synthesis rational control law diagnostic object state identified basis forecast given statements reliable confirmed experience using developed us ‚Äúrod ids‚Äù software different fields activity look please articles presentations results section achievements publications right consider specialists development effective machine learning methods mlm‚Äôs solving ‚Äúrod ids‚Äù problems offer software order resolve specific problems calculations addition always open contacts discuss methodology computational methods realization form interactive compute decision making support software system ‚Äúrod ids‚Äù helpful company hope use mlm‚Äôs increase customer demand new versions software
rg650d,0,time event survival analysis churn prediction telecom hello studying different models churn prediction time event manner came across simpler models like random survival trees support vector regression simple nn rnns wondering anyone research working survival analysis current state art models advanced models time event churn prediction thank
r6l8nh,1,algorithm selection basic music generation looking generate basic sound snippet around 40 50 notes based around small training set 5 different genres new machine learning trouble determining algorithms pursue implement working matlab use frequencies first 40 chord song 5 10 songs per genre create prediction next frequency play based previous frequency e genre indie pop putting song basic form chords first frequency song bflat major chord second chord g minor chord chords would randomly generated knowing part indie genre want predict next 40 chords song rhythms frequencies algorithm suggestions
qqckfw,0,message r mlops announcing first ama hi mod r mlops finally managed book first ama interest 2m members following content sticky sub would please help us grow community mlops enthusiasts burying post ÂÖ´ Ôºæ‚ñ°Ôºæ know remember going amas celebrate fact many us naturally since niche subreddit top tier mlops superheroes lining post ama miracle happened delighted announce alessya visnjic ama thursday spread word let make ama first many successful ones case immersed mlops ecosphere bio alessya visnjic ceo co founder whylabs ai observability company mission build interface ai human operators whylabs alessya cto residence allen institute ai ai2 evaluating commercial potential latest advancements ai research earlier career alessya spent 9 years amazon leading machine learning adoption tooling efforts founding member amazon first ml research center berlin germany alessya also founder rsqrd ai global community 1 000 ai practitioners committed making ai technology robust responsible course always ulterior motive alessya focusing recent announcements whylabs round funding new saas solution called ai observatory personally think corner mlops tooling space super exciting whylabs hard opensource groundwork additionally marketing spammy honor host sub
r015mw,0,death threat cat meme context matters machine learning death threat ‚Äî cat meme post rooting cancer ‚Äî demise examples interested context absolutely crucial building ml models expect machines effectively understand analyze world training isolated data complicated computationally expensive build context inclusive models actively building ml would use case benefit including context would love hear experiences topic
rptzd6,0,project idris xla linear algebra probabilistic modelling w dependent types june announced started work probabilistic modelling library idris post announce first major milestone basic linear algebra idris backed xla right addition ops easy add project mission well evolving roughly seen number impressive numerical computing projects lead pack performance others leverage advanced language features theory expressive apis project hope explore highlights design including user friendliness paramount features come second dependently typed tensor shapes verified compile time expect support gpu future possibly accelerators xla competitive performance see comments detail
rekhev,1,detect tag persons photos software engineer ai ml experience think something interesting play thousands photos taken family onedrive like download store local server meanwhile hand pick train model categorize multiple persons eith output assign names say wife son daughter use input continue run photos end database table photo1 son photo2 wife daughter etc course also edit exif info photo attach tag information later search photos tagged son different way would run photos altegother output would person1 photo1 person 2 3 photo2 etc assign names final output could simple mapping database person1 sure making sense get started
qv5292,0,gan animated gif video clip generation looking prior open source github code generate animations video clips google getting results anime don‚Äôt want work trains style gan bunch video clips animated gifs generates brand new ones
rqideg,1,projects cs undergrads soon cs grad trying get ml engineering job know need projects portfolio know go experience cnns fully connected nns thinking making basic dog cat classifier first project figure employer suspect copy pasted someone elses code since type project typical make differentiated project code neural network numpy show know works
rbzzpz,1,‚Äú‚Äù what‚Äôs best way analyse behavioural activity data get correlations health insights large dataset activity behavioural data date abs time stamp activity took place self reposted mood associated activity geolocation data what‚Äôs best way derive health wellness correlations data based user‚Äôs activities build recommendation engine suggest something less something based user goals activities activities daily living working reading shopping cooking cleaning watching tv etc suggestions get started would helpful
r0y56t,0,python library optimize hugging face transformer inference 0 5 ms latency 2850 infer sec launched new open source python library help optimizing transformer model inference prepare deployment production it‚Äôs follow proof concept shared reddit scripts converted python library apache 2 license used nlp project documentation reworked also added direct tensorrt support provides another boost performance compared ort trt backend usually provide 5x faster inference compared vanilla pytorch 10x specific cases rtx 3090 perf analyzer reports 2800 inferences per second throughput want try üëâ readme includes benchmarks small base large transformer architectures give idea large benefit learn whole process also check article showing open source library beat commercial product hugging face company python library basically tutorials deploy production transformer model tell take fastapi put pytorch inside many reasons it‚Äôs bad idea first inference performance low side spectrum nvidia demos showing us build manually full transformer graph operator operator tensorrt get best performance hardware it‚Äôs reach many nlp practitioners it‚Äôs time consuming debug maintain adapt slightly different architecture tried plus secret optimized model works specific sequence lengths batch sizes truth far improve soon it‚Äôs mainly mlperf benchmark one used compare dl hardware marketing content specialized engineers usual way perform model optimization deployment automatically convert existing pytorch tensorflow model kind graph apply optimizations deploy artefact production ready inference server optimization part project leverages nvidia tensorrt microsoft onnx runtime choose best optimized models benchmark performed optimizations inference server library generate whole configuration nvidia triton inference server triton mature tool api clear documentation covers typical use cases etc features may require ml deployment knowledge nothing complex tensorrt it‚Äôs another story documentation vast sometimes incomplete api evolves rapidly many traps like way setup model precision allocate memory gpu ram found single oss project take random hugging face model simply optimize tensorrt still tool provides best performance hope library help nlp practitioners benefit target hugging face transformer models made experience simple requires single command line whole process tensorrt triton unknown please find 2 slides recent nvidia gtc 2021 conference slides amazon presentation learned amazon search amazon ads aka üí∏üí∞ü§ë generators also built triton inference servers slides still enough convince may benefit check article microsoft learn microsoft bing built nvidia tensorrt got onnx runtime tensorrt triton big guys tools may also help projects let‚Äôs democratize
r4001a,0,vectorizer use building search engine system based embedding retrieval many vectorizers choose one use also curious hear use cases vectorizers others using view poll
rsusfc,1,custom h5 dickpics hey want detect nudity live video stream tried train model detect anything couldnt find already trained model penises anywhere guys maybe know source thank advance
qww285,0,anyone regret coming field yes path would taken edit since giving helpful awards post blown would appreciate gold broke college student want pay reddit premium
qpi381,0,openai gpt 3 cases misusage failures hello everyone name alex 26yo italy currently studying marketing iulm university milan grew quite interest finally gpt 3 came discussing one professors topic bd thesis came long story short gonna talk gpt 3 one topics love cover collection known applications aka use cases found quite decent number less succesful cases find anything also tried google since refer google scholar find reliable sources still never managed find anything asking guys know case company tried use gpt 3 ways whole thing end quite expected thank
rlsa2y,0,discussion hot reloading headless deep learning rig hey folks sure asked plenty experienced ml scripting looking opinions setting hot reloading external headless rig offload rl training employer uses kubernetes based ml platform really neat distributed big data jobs via dask‚Ä¶ hot reloading heavy duty argo wf via tilt across 1 600 nodes gpu 32gb memory‚Ä¶ want dig hardcore deep rl want set personal development environment compatible remotely hot reloading macbook like upon saving changes local scripts updated try run headless linux rig whilst logs returned printed folks got recommendations open source ml development toolkits ideal
ri8f7a,0,work detecting predicting smells seeing electronics detect aspects air like humidity temperature vocs gases ml research detecting smells research sensors would needed make decent predictions thinking detecting specific chemicals along lines nose works lot ml work image detection language modeling senses
r2w8o7,0,discussion federated learning practice hi anyone know detail descriptions surveys fl deployments practice type aggregations people use ensure privacy deployments rely tf federated tried googling around struggling find much information thanks lot
rb8x7l,0,breast cancer analysis using opencv ml dl working project early stages simple machine learning application guess cannot find anything similar kaggle get references macro breast cancer images find example want first get shape meat measurements simple algebra get cancer white thing measure close border first part use canny edge detection get largest contour last part driving crazy suggestions thought using mask r cnn dont know viable
r8v6rk,1,career change programming data 30 ‚Äì options europe hi everyone starting little bit background 30 years old french therefore citizen eu already master degree engineering chemistry materials polymer working 6 years process engineer automotive industry 2 years france 2 years slovakia 2 years germany ended job germany currently learning part time german school pass b2 lot free time 6 7 hours day also speak english necessary consider passing c1 certificate french many months reflexion would like change career field work mainly related studies suit great attraction programming computer science even basic knowledge would like change field totally sure yet would like therefore targeting one following jobs data analyst data scientist probably later job related machine learning ai discovering getting super excited info learning python data science couple months finally questions best learning options employable certified company recognised knowledge order find job data science data analyst example start ones would recommend looking study europe online possible person germany find job germany switzerland austria example fast long free time money invest want realise necessary different options found far self learning couple portfolio projects free difficult prove competences certifications supervision bootcamps online expensive quite fast intense however sure recognized companies really help find job master degree think chance find master online europe starting 2022 knowing don‚Äôt educational background cs master conversion course heard maybe uk totally sure works knowing already master degree probably don‚Äôt start beginning bachelor start first bachelor bachelor enough online would better city country europe schools bootcamps universities something recognized certified might help lot ideas information could help would grateful motivated invest lot project totally sure would best option moment qualification really necessary job thanks advance
ri8nfe,1,convert csv data graph dataset anomaly detection hello x200b working anomaly detection problem using graph neural network however sure best way convert csv data graph data 115 different attributes timestamp find example dataset want test time stamp anomaly best way convert csv data graph data thinking considering attribute node connecting undirected edge since know connection single feature node lot edges specific feature shape 115 1 edge shape 2 6555 sure good way model best method work node embedding graph embedding ps using autoencoder model detect anomalies thanks advance hope clear question vishal
r8ge6o,0,paper overview n√ºwa visual synthesis pre training neural visual world creation video paper code abstract paper presents unified multimodal pre trained model called n√ºwa generate new manipulate existing visual data e images videos various visual synthesis tasks cover language image video time different scenarios 3d transformer encoder decoder framework designed deal videos 3d data also adapt texts images 1d 2d data respectively 3d nearby attention 3dna mechanism also proposed consider nature visual data reduce computational complexity evaluate n√ºwa 8 downstream tasks compared several strong baselines n√ºwa achieves state art results text image generation text video generation video prediction etc furthermore also shows surprisingly good zero shot capabilities text guided image video manipulation tasks
ruyk5i,0,new paper relational tsetlin machine applications natural language understanding x200b relational tsetlin machine paper introduces first relational tsetlinmachine reasons relations variables constants approach based first order logic herbrand semantics taking first steps toward computing power universal turing machine approach take advantage logical structures appearing natural language learn rules represent actions consequences related real world outcome logic program horn clauses bringing structured view unstructured data closed domain question answering first order representation produces 10√ó compact knowledge bases along increase answering accuracy 94 83 99 48 approach robust towards erroneous missing superfluous information distilling aspects text important real world understanding ml ai nlp machinelearning logic relational
rqi1qq,1,online resources explain coding kinda want resource teach machine learning coding explaining cause feel like resources recommended feel like get bored easily
r2lfjf,1,exactly machine learning algorithm hey guys beginning learn machine learning something confused confused usage word algorithm context machine learning learned algorithm means step step instructions perform task takes input produces output machine learning instance linear regression algorithm model linear function fitted dataset graph based understanding called algorithm guys eli5 thanks update found article helped clear confusion check might help
r8n8qx,1,suggestion best use datacamp started using datacamp
rb5r86,1,machine learning ai hey guys im assignment machine learning ai topics cover
rplc4j,1,get stuck linear algebra hi everyone newbie machine learning stuff learning linear algebra machine learning linear algebra takes lots time learn read contents linear algebra useful machine learning guys recommend books course learning linear algebra machine learning p sorry bad english thanks help
r8yq8n,0,assert whether complex high dimensional data predictive power assigned binary classification multi channel time series signal data data collection process expensive give small sample data 15 samples per class check whether data good enough make prediction want check prediction power data retune sensor hoping different signal give task hard sensor good enough capture difference problem know assert rgb image classification task easy since look image see features one complicated hard understand feature simple plot comparison channel class still look similar eyes enough data may able tell different imagine kind eda signal processing extract kind statistics sure extract useful information confident predictive power please suggest ideas anyway guys usually dealing limiting signal complicated dataset general job assert tell easy know unless enough data case costly gonna use deep learning probably try traditional signal feature extraction first like fourier transform feed extracted data simpler model rather let model learn feature
rd7pd2,0,techniques shot domain adaptation trying domain adaptation synthetic real images task anomaly detection usually problem domain adaptation lot target images without labels case target images labels therefore common technique creating pseudo labels target domain useful current idea kind style transfer synthetic images real images cycada contrastive unpaired image image ideas domain adaptation lot source images target images ground truth would glad pointed right direction
r9jd2h,0,go beyond data parallelism model parallelism talking gshard article lists papers gshard presents background information inspiration papers finally evaluates else done improve gshard similar work done oneflow article oneflow paper code paper gshard contains two main parts work one parallel apis one mixture experts former part interesting discuss part contribution parallel apis outlined clearly abstract paper gshard module composed set lightweight annotation apis extension xla compiler gshard paper
r07nms,0,inference server gpt j 6b huggingface optimizations fit g4dn xlarge hi created inference server gpt j 6b huggingface using fastapi main challenge making sure model fits smallest g4dn instance aws 16gb ram vram save server costs check hope find helpful also write explains optimizations applied
rut9hs,0,flow based method treats input data different lengths hello searching researches different size data generated flow based network super resolution task continuous mapping want generate output time aligned scalar data example x200b input noise sampling b x x c output scalar data b x c 1 x200b introducing variational data augmentation technique vflow output high dimensionality concatenate noise vector input output output problem time dimension different data input treat problem x200b p appreciate read flow based research nlp task
rhwua0,1,undersampling r hi guys x200b highly unbalanced dataset 93 accuracy zeror want undersample bit reduce zeror accuracy demonstration purposes data analytics course uni x200b however uncertain go first much remove overrepresented value x200b kinda want cut maybe 30 overrepresented data simply sorting dataframe target true false delete bottom 30 values row number data super jumbled anyways sorting target true deleting bottom values removes relatively random cut target false rows target false overrepresented value also actually remove set amount rows based row number lol
rd3oby,0,yuno ai search engine recommends anime given specific description yuno action x200b yuno search engine working past 6 months working quite time confident search engine usable source code yuno try yuno notebooks ui 1 kaggle notebook recommended notebook 2 colab notebook research yuno basically type kind anime looking yuno analyze compare 0 5 million reviews anime information index return animes might contain qualities looking r animesuggest inspiration search engine people essentially thing favourite part idea pretty simple goes like let says looking romance anime tsundere female mc read every review anime exists internet able determine anime qualities looking n ot framing differently reviews read anime likely decide whether particular anime qualities looking x200b consider section review anime oregairu yahari ore isn‚Äôt first anime tackle anti social protagonist certainly captures perfectly characters deadpan writing it‚Äôs charming funny yet bluntly realistic may go expecting typical rom com instead come lashed harsh views characters reading much review conclude anime 1 anti social protagonist 2 realistic romance comedy read reviews anime find qualities case reviews must contain enough information particular anime satisfy query like mentioned therefore create method reads analyzes different anime reviews train model understand anime reviews without kind labelled dataset question took time solve banging head wall quite sometime managed goes like let x two different anime don‚Äôt share genres among sufficiently large reviews anime x totally different content idea inverse idea web link analysis says hyperlinks web documents indicate content relativity relatedness connectivity among linked article pretty much idea well works x200b fig1 10k reviews plotted 1280d 2d using tsne x200b fig2 reviews zero zero sequel able see fig1 several clusters different reviews fig2 zoomed version fig1 reviews zero sequel close definition never mentioned anime sequel close case every anime sequel close want play check whether case interactive kaggle notebook contains 100k reviews x200b since method use kind handcrafted labelled training data method easily extended different many domains like r booksuggestions r moviesuggestions think pretty cool x200b context indexer favourite indexer coz solve crucial problem mentioned bellow consider query like romance anime medieval setting revenge plot finding review anime difficult review talks thing particular anime eg consider anime like yona dawn anime 1 great character development 2 medieval theme 3 romance theme 4 revenge plot reviews anime mention four things mention review talk romance theme revenge plot means need somehow remember reviews deciding whether anime contains looking talked great detail mention article interested x200b note please avoid two things otherwise search results bad 1 make spelling mistakes query coz auto word correction 2 type nouns query like anime names character names properties looking eg type anime like attack titans type action anime great plot character development yuno watched anime reads reviews know attack titans x200b questions regarding yuno please let know happy help discord id parad√∏x 8587 thank x200b edit 1 added bit context indexer edit 2 added things avoid search yuno
rlfkvk,1,overfitting 2 minute visual guide x200b üîµ overfitting üîµ üßê overfitting common phenomenon machine learning community tries avoid like plague model overfits performs extremely well training data provided performs poorly fails generalize unseen data üíæ imagine overfitting analogy one assumes questions exercise session lecture exactly asked exam end memorizing exam realize rote learning would help answering unseen questions üòÆ overfitting avoided many ways 5 common ways 1 regularization model parameters using l1 l2 norm example see previous posts details 2 gathering training data let model cut noise 3 early stopping monitoring training validation error curves 4 reducing number features selecting better features 5 performing data augmentation first post hopefully first many come studying practicing machine learning computer vision 7 years time passed realized power data driven decision making seeing firsthand ml capable personally felt great inter disciplinary tool automate workflows bring different topics ml form short notes interest existing practitioners fresh enthusiasts alike posts cover topics like statistics linear algebra probability data representation modeling computer vision among things want incremental journey starting basics building complex ideas like content would like steer topics cover feel free suggest topics would like know comments
r46h3o,1,quick introduction pruning apply model hey everyone wrote short article pruning thought might useful community hope find helpful looking forward hearing feedback
qvc3cb,0,create new neural architectures blocks layers keep using models others invent come learn create type layers blocks come squeeze excitation layers efficient net example start
r09639,0,deepsquare news deepnews weekly update bring latest news deepsquare project quest develop sustainable high performance computing service ecosystem last week talked artificial intelligence high performance computing dr florin dzeladini today delve deepsquare‚Äôs heart soul sustainability hand one board members fr√©d√©ric juillard following project realized strong focus sustainable hpc energy reuse truly mean deepsquare determined take next step towards sustainable hpc service offer best environmentally friendly alternative current status quo creation decentralized infrastructure backed professional team cutting edge technology managed create growing community whose values align vision project update happy able share news deepsquare supported friends alan analytics delivering project worked together artist agnieszka kuran project resulted ai generated ‚Äúnew‚Äù cave art based collection data documented paleoanthropologist genevieve von petzinger used test environment csquare application enable ai generated cave paintings really interesting combine hpc ai art merge fantastic project meet team fr√©d√©ric juillard one deepsquare‚Äôs board directors serial entrepreneur engineering background sustainable deep tech previously involved foundation high tech companies naturally fits within character deepsquare association day day work includes supporting business administration team leading partnerships fr√©d√©ric team player believes sustainability ‚Äúa key factor‚Äù project bedrock around gather strong community ‚Äúmy favorite aspect part shared dream project people believe every day see community grow board vision something truly appreciate social aspect interacting people different backgrounds cultures nationalities something perfectly aligns core values deepsquare world today‚Äù community indeed key point future sustainable hpc service every day people aware challenges coming ahead hpc industry already consuming vast amount energy produced planet continue grow next decade fr√©d√©ric understands shared effort rooted ‚Äútrustful philosophy backed extremely well prepared team‚Äù face future demands ‚Äúthe compute power world needs today something absolutely crazy gross numbers going keep growing society shortly need meet demand new technologies 5g autonomous cars smart cities ever increasing connectivity sustainability something every innovative business integrate clearly lot things improve hpc terms sustainability certain deepsquare part solution‚Äù overcome challenges project latest hardware industry conscious plan regularly update match industry‚Äôs demands always sustainability mind fr√©d√©ric believes ‚Äúextremely important find right channel hardware second life‚Äù thus mitigate damaging consequences thoughtless hardware disposal deepsquare clusters thus designed green mindset also tackle growing needs compute power coming line ‚Äústandard hardware already extremely efficient converting watt electricity watt heat using immersion cooling system along system reintroduce heat building best way make good ecological use otherwise wasted energy that‚Äôs clearly important aspect project tackle environmental problem deepsquare deliver sustainable high performance computing service‚Äù support community every day closer democratizing high performance computing better execution cost social responsibility learn deepsquare check website want connect team community follow us twitter linkedin join telegram group
rmws0h,0,mathematical framework transformer circuits link paper x200b paper attempt take initial preliminary steps towards reverse engineering transformers given incredible complexity size modern language models found fruitful start simplest possible models work way aim discover simple algorithmic patterns motifs frameworks subsequently applied larger complex models specifically paper study transformers two layers less attention blocks ‚Äì contrast large modern transformer like gpt 3 96 layers alternates attention blocks mlp blocks
rvw0rv,0,launching dagshub 2 0 ‚Äì git integrated data labeling smart ml discussions tl dr ‚Äì dagshub integrated label studio open datasets git dvc remotes label commit labels back without devops also comment labels bounding boxes file check example project try tutorial comparing annotations hi r ml one creators dagshub help ml practitioners create central repository projects leverage open source tools version datasets models track experiments starting today ‚Äì label data comment anything like github machine learning probably heard mean vision anyone could jump open source data science project contribute code data labeling models experiments via pull request like would open source software project take awesome popular open source tools connect place build data science projects lowering barrier devops work creating coherent workflow makes sense production oriented teams open source community work 1 version code git notebooks also work 2 version data models pipeline dvc 3 git push dagshub repo 4 dvc push free dvc compatible dagshub storage preferred cloud storage support 5 get zero config access controlled mlflow api endpoint log experiments saw labeling big challenge many community projects well teams industry spoke wanted make community‚Äôs life bit simpler since many challenges around integrating labeling rest ml lifecycle that‚Äôs exactly built dagshub annotations label studio integration main additions 1 zero setup labeling ‚Äì work push data dagshub click button fire labeling instance 2 sync data git dvc remote ‚Äì it‚Äôs select data want label waiting workspace something know many wanted 3 commit back git ‚Äì you‚Äôre done version labels created ‚Äúcommit‚Äù button adds annotations git making easy pull model training need preserving project history 4 diff discuss annotations ‚Äì see annotations data check example project discussions way preserves context even comment bounding boxes images committing annotations back git created example project play see actually looks course love hear thoughts feedback answer questions might thank want read launch check launch blog
r9rcka,1,made visualization cost complexity pruning decision trees made animation visualize cost complexity pruning decision trees hope help someone appreciate feedback style code interface etc
rktqol,1,papers recently training sentence classifier discovered something kind interesting calculate distance weights word embeddings start training weights end training distance corresponds important word classification task wondering research seems like would useful model explainability
r13iiq,1,federated learning implementation hi wondering anyone come across implementation federated machine learning system want build one hospital system hardware forte could spin vm cloud system respective hospitals make sure communicate thanks
rh0rb2,0,sota adversarial examples hard time finding whats currently state art adversarial attacks defenses gathered still hard defend cleverly engineered ones impact ur research ur decision use cnns methods svm display weaknesses
ro224d,0,ad implemented jax tensorflow pytorch hi everyone heard two ways implement automatic differentiation source code transformation sct operator overloading oo type jax use question tensorflow pytorch
rjvm5e,0,parametrizing random feature models let say dataset x nxd construct feature dataset f nxp relu xw w random matrix relation p influence train test loss sort bias variance tradeoff
rfyujd,0,meta learning use test samples transfer learning use shot learning 1 far know meta learning transfer learning two common ways adress image classification task shot senario found offen use images train set specified dataset test procedure test set meta learning traditional transfer learning use test dataset deal test task wonder getting wrong understandding accurracy reported meta learning method comparable report transfer learning 2 found word called meta dataset omniglot fungi etc testset dataset use transfer learning strategy deal mata dataset split dataset trainset testset weather accuracy reported transfer learning comparable metalearning strategy thanks
rg86kv,1,recommend website book article thesis used deep learning forecast climate next ten years using historical data 50 years ago example new term machine learning deep learning beside source possible develop model predict future climate using normal laptop ram 4 gb hard bigger use deep learning forecast future climate thank
qqcrbh,0,intel optimized facebook dlrm 8x speedup deep learning recommendation model intel leveraged sigopt hyper parameter optimization platform achieve software speedup dlrm additionally intel leveraged vertical split embedding lamb optimization parallelizable data loaders
qv9ez4,0,tools feature selection deep learning let say access feature store pretty much every imaginable derived feature available tool available specifically testing feature combinations feature scaling upsampling downsampling know weights biases feel like could wrong please chime know mainly hyper parameter optimization would variations data within version would want possibly explore hyperparameter tuning even answer feature selection dump use something like l2 regularization different thresholds things like scaling class imbalance fixes imputing etc create different versions input data good approaches tools track experiments around preprocessing feature selection part
rvmeo6,0,tools helpful annotation videos action recognition team university work ergonomics want action recognition videos approached help work images idea videos dataset want annotate key points frame please tell tools helpful annotation videos
r519uc,1,beginner friendly python machine learning library wan start machine learning searched internet python library ml best library beginners
ru6niq,1,transformers state art every kind task suck tasks well kind tasks suck kind neural net architectures outperform tasks
rwmvn7,0,blogs fundamentals score based diffusion probabilistic models two part blog describes theoretical fundamentals score based models diffusion probabilstic models relationship written coherent documentation theoretical developements new class generative model rigorous mathematical proofs excluded order make readable sharing case anyone finds useful part 1 score base models part 2 diffusion probabilistic models
rxbg0z,1,fun way creating gpt j chatbot found cool article explaining create gpt j chatbot couple steps
r44hwo,0,professional voice ai hi folks looking realistic informative voice changer text speech tool want use producing professional 2d ads based ai really professional cost matter result must proficient guys know reliable platforms projects
qlxl01,0,zero shot models input features nlp classification tasks places like huggingface offer zero shot models pretty decently zero shot classifier easy implement thinking took zero shot models added features fit classifier top let say trying classify toxic tweets might use embedding model verctorizer turn tweet model features fit classifier top try predict label text features add simple zero shot binary features angry model1 sad model1 news model1 etc one models even simply use toxic model1 toxic model2 use predictions input features classifier planning experiment bit wanted get thoughts previous similar work use reference thanks
qr6hhb,0,current state deep learning theory assume many people subreddit interested understanding currently state art models work use understanding underlying princples models create better powerful ones mean sense physics theory classical computer science one develop mathematical framework understand predict subject course difference one assumes laws experiences onr firmly rooted math still use mathematical approach sometimes see papers subreddit look relevant vmatter try read given current math education somewhere practioneer ba student believe truly understand therefore judge experts accepted predictive theory deep learning models
r6hwc4,1,prerequisites cs229 elements statistical learning hi want cs229 course read elements statistical learning sure prerequisites quite advanced level calculus linear algebra understand probability theory enough learn something else statistics best books resources available
rhpymx,1,citable literature hey guys writing chatbot bachelors thesis python using tensorflow already got book chollet deep learning python however first part thesis give overview different techniques reinforcement learning supervised learning etc came wanted ask advise literature techniques need academic journals academic books etc wikipedia online tutorial help appreciated
qnhn4s,0,cvpr policy posting arxiv planning submitting paper cvpr 2022 questions regarding process finding information site bit confusing time frame allowed post preprint versions papers arxiv certain considerations need know thanks advance
r78ctn,0,project predicting group behavior based members behavior hi everyone asked small project completely sure tackle details dataset describes behavior dozen groups group comprises number individuals group score indicates whether group going become aggressive less aggressive based model created asked find certain conditions lead groups becoming aggressive example half group members certain activity means 75 chance group become aggressive two options considering look statistics groups became aggressive say 40 members groups became aggressive activity 1 therefore say 40 group activity 1 group likely become aggressive suspect expected confuses causation correlation fact x group something became aggressive mean groups likely become aggressive plus case sure get probability potentially create large number models proactively try predict groups behavior based certain conditions 25 group activity 1 group likely become aggressive 50 group activity 1 group likely become aggressive 75 group activity 1 group likely become aggressive 25 group activity 2 group likely become aggressive 50 group activity 2 group likely become aggressive 75 group activity 2 group likely become aggressive however many possible permutations think right approach unsure tackle grateful thoughts ideas
r8tdrm,1,logo detection nn outperforms almost 2x logo detection nn one year older missing two nn architectures logo yolo 2020 paper benchmarks osf logo 2021 paper benchmarks nns tested logodet 3k 1000 dataset even though papers one year apart map scores new paper almost double ones older paper take instance cascade r cnn map 48 14 older paper 89 1 newer paper yes cascade r cnn seems different backbones paper make big difference also resnet 101 101 layers r 50 fpn assume 50 layers even realistic one year get backbone architecture way fewer layers way performant previous one model state art logo detection thank
r8ly8r,1,taking coursera nlp specialization understand lecture halfway week 1 material really understand equations think able write python quite long realised specialisation intermediate level read reviews many said class easy deep enough think opposite know anything abt nlp taken python everybody course dr chuck learned python nlp course try instead try andrew ng machine learning course first planning apply nlp language technology master next spring semester thanks much taking time answer going pyhton ml courses first diving nlp
ru70fv,0,raising errors using accelerators hard raise exceptions ml pipeline using gpu example make classic index bounds error libraries like pytorch get generic cuda error see exact error transfer tensors explicitly cpu rerun code think possibility improve future sorry cs related question
r6k9n4,0,detect ai generated images vs authentic photos content generated gans thought would useful detect whether piece content generated ai starting images demo suggestions welcome
r5p9nx,0,build automations powered machine learning single python file hi machine learning potential automate huge portion boring processes often gets stuck jupyter notebook stage never make real world take example case want predict whether customers unsubscribe using basic customer data many sub build simple machine learning model predict whether customer churn get nice pandas dataframe customer data however gets really complicated want model deployed integrated production say procedure 1 pull data new customer shopify 2 predict customer whether churn 3 predict churn true 4 send discount code customer automatically suddenly code api integrations build etl pipelines deploy original machine learning solution onto rest api spin http server etc maintain huge pain indeed building framework takes care exactly boring stuff described really believe bridge gap research real world ml super excited get early feedback please shoot questions comments
rv50th,1,exploratory data analysis hello friends started studying machine learning months back idea process collection data training model want know exploratory data analysis eda part typically done training data could suggest books tutorials learning eda resources providing steps taken eda based particular type ml models decision trees svm etc
r61irj,1,need help theory questions asked research position asked questions assessment research position machine learning intuition solved reference solve would really helpful 1 non uniform weights linear regression given dataset data points denoted xn tn n 1 ¬∑ ¬∑ ¬∑ n data point associated non negative weighting factor gn 0 error function thus modified œÜ ¬∑ representation data find expression solution w ‚àó minimizes error function b give two alternative interpretations weighted sum squares error function terms data dependent noise variance ii replicated data points 2 given dimensional data x x1 x2 ¬∑ ¬∑ ¬∑ xd consider linear model form x200b n data samples corresponding labels xi ti 1 2 ¬∑ ¬∑ ¬∑ n sum squares error mean squared error function given x200b suppose gaussian noise œµk ‚àº n 0 œÉ 2 e zero mean variance œÉ 2 added independently input variables xk find relation minimizing sum squares error averaged noisy data minimizing standard sum squares error averaged noise free input data l2 weight decay regularization term bias parameter w0 omitted regularizer 3 solve problem given hand programming implementation required consider training set test set given tables 1 2 x200b use linear model fŒ∏ x1 x2 Œ∏0 Œ∏1x1 Œ∏2x2 logistic regression function œÉ fŒ∏ x1 x2 consider initial weights Œ∏0 ‚àí1 Œ∏1 1 5 Œ∏2 0 5 learning rate 0 1 gradient descent logistic model p ÀÜy 1 x1 x2 cross entropy error function ii use gradient descent update Œ∏0 Œ∏1 Œ∏2 one iteration write updated logistic regression model calculate report accuracy precision recall evaluate model iteration
qmrr7t,0,pairprogramming pair analysis ml development teams improve harmony hi gonna assemble small team work ml project might lead ml first software wondering whether using pair programming beneficial ml teams especially early stages adventurous analysis exploration experience pair programming ml stages think good strategy etc working remotely another aspect interesting experience create harmony newly assembled team working remotely
rl4nqz,0,deep learning regarded black box people continue insist know works genuinely curious multiple papers read past year make claim clear understanding dnns work work yet achieved perhaps ignorance view claims incredulously dnn giant logistic regressor made smaller logistic regressors fitting curve absolutely magic actually mechanical borderline mundane proliferation magical claims professing ignorance exactly authors referring reference though foundation models based standard deep learning transfer learning scale results new emergent capabilities effectiveness across many tasks incentivizes homogenization homogenization provides powerful leverage demands caution defects foundation model inherited adapted models downstream despite impending widespread deployment foundation models currently lack clear understanding work fail even capable due emergent properties foundational model literally model imagenet zoo used transfer got vast diversity feature representations layers responsible highly fundamental therefore transferable features converged closer effervescent global optima big regressor like mystery stupid
rvvfoa,0,vae used instead autoencoder world models paper hi reading paper wondering want achieve compact version original representation could use traditional autoencoder specific reason vae used thanks
rt139q,1,completed applied statistics basic ml courses college wanting learn go hello last year took applied statistics course intro ml course college taught professor got didn‚Äôt feel like learned retained much information feel lost go terms ml like remember z scores normal distributions confidence intervals couldn‚Äôt tell gamma distribution used it‚Äôs basis review statistics first knowledge good enough jump back ml i‚Äôm winter break wanting visit library read books topic i‚Äôm clueless start could anyone recommend good starting points statistics basic machine learning lot application python i‚Äôm also wanting understand underlying theory i‚Äôm hoping refresh brain went hopefully delve advanced topics feel comfortable thank apologies repeat post answered
rhiqoc,0,project ru dalle diffusion better images ru dalle diffusion decoder link previously posted project uses ddpm instead vqgan decoder layer new model applies idea ru dalle ddpm model produce much detailed images compared default vqgan realesrgan setup decoding exact image embeddings ddpm model trained 256px images 16x16 token dimensions output 512px images given 32x32 tokens ru dalle model architecture took pretrained ddpm models released openai modified middle block take image embeddings input fine tuning pre existing openai model allows train reasonable amount time prosumer ml rig constrains certain architectural decisions specifically middle block must take embeddings 8x8 dims one downscale pass 16x16 embeddings possible get better performance fewer parameters custom ddpm matched image embedding dimensions
rxijit,1,log transform data reading book hands machine learning said skewed data pose problem machine learning algorithms looked internet found log transform data distribution skewed crucial algorithms need data normally distributed
rn6m8o,0,reinforcement learning initial policy hi guys happy holidays understand difference passive rl active rl passive rl we‚Äôre merely learning values states policy feed agent active rl agent learn optimal policy what‚Äôs best course action initial policy think good also want find exists better policy thought approximate q learning might suitable initialize q states policy values 0 q states values 0 set low epsilon encourage testing policy first increasing time see better policies however seems kind backwards usually we‚Äôd want explore first starting ‚Äúblank slate ‚Äù exploit another way going perhaps combining form passive rl
rv4nah,0,nlp hybridization statistical approach expert system hi everyone question context aggregate platform various ai apis market gcp azure etc including nlp apis keyword extraction sentiment analysis ner etc idea developer create accounts different providers one api test compare change whenever wants however many customers ask us mix statistical approach behind apis expert systems achieve hybridization idea thanks
rpqwao,1,federated learning mobile keyboard prediction ever wondered mobile keyboard gives next word suggestions give personalised suggestions time ensuring privacy individuals check blog post federated learning mobile keyboard prediction talks happens privacy preserving manner blog post ppml series 3 federated learning mobile keyboard prediction annotated paper annotated ml papers federated learning mobile keyboard prediction
r5hymj,1,data structures algorithms important become machine learning engineer hi started prepare machine learning roles working maths python required see people say learning data structures algorithms important well suggestions would helpful
rhdofe,0,audio speech ‚Äúharmonization‚Äù task equivalent image harmonization compositing task audio speech music data say want combine clean speech data natural scene background speaker naturally sounds like background scene generally i‚Äôm asking make clean speech data ‚Äúnatural‚Äù given natural sound data reference i‚Äôm familiar sound design sfx either please let know particular task achievable non neural approach
r5ktec,0,one prefer idea already implemented tested pytorch want test architecture publicly available tensorflow would 1 implement idea framework 2 implement framework would change answer want test idea architectures
ria7qt,1,training multiple regressors independent weights dataset pytorch hi like train multiple linear regressors 20000 odd pytorch take input features different true ground truths also like different weights every one predictors architecture supposed use task simple 2 layer neural net stuck come way train classifiers independently thought making list nn module objects invoking one one different however seem find documentation could someone point resource
r3fd4m,1,specifics random forest decision tree prediction step greetings first post subreddit phd wants use ml various tasks future career figured would dig various ml algorithms code say might posting future grateful page exists currently trying code algorithm random forests completing decision tree algorithm trouble wrapping head around ensemble prediction step far understand take data create multiple subsets data create decision trees data simple enough supposed make predictions based decision tree model created question prediction step data supposed test individual decision trees impression make one prediction decision tree na√Øve understanding making think proper way build ensemble model anyone give thorough explanation aggregate decision tree outputs thanks advance
rjvmw0,1,machine learning hobby full time commitment hey asking question today quite torn future studies im thinking double major microbiology biomed cs dont really want put stress time uni prepared heard really needing uni learn computer related stuff self learn things appealing company individual degree know career prospects computer science dont really want start uni 2nd year science student plus dont even know program yet committed want good future getting deep waters swim away advice would appreciated cheers
qpf5t0,0,introducing metaicl language model meta training framework shot context learning research team university washington facebookai research allen institute ai introduces meta training incontext learning metaicl new meta training framework shot learning lm meta trained learn context ‚Äî conditioning training examples recover task make predictions quick read introducing metaicl language model meta training framework show context learning metaicl code data made available project‚Äôs github paper metaicl learning learn context arxiv
qrw5pb,0,article introduction language models nlp hey working intro series language models would love feedback first installment
rbqomm,0,training dataset distributed sorry beginner question suppose building text speech model wondering training dataset realistically distributed e distribution data used uniformly distributed make sure performs well kind sentences thanks insight
rgzppr,0,arxiv worth academic career know topic already discussed case feel free delete thread point right one finished phd postdoc researcher want pursue academic career department uses scopus metrics public competitions used upload work arxiv soon ready always received useful feedback community papers always automatically scraped paperswithcode websites gave nice exposure even paper 12 citations first two months lot received lot emails researchers asking clarifications collaborations general posting preprints arxiv always good experience problem lost lot citations everybody cited arxiv version articles journal one citations appear scopus count public competition point view colleges suggested use researchgate instead arxiv platform control citations think ever faced problem platform suggest gives exposure similar one arxiv gives stop publishing preprints wait journal conference publish articles
rxhkxd,1,closing data loop dagshub annotations label studio hey r learnmachinelearning nir dagshub know working data less attractive building cool model reality though many times improving data leads much better results maybe one reasons don‚Äôt much working data complicates workflow significantly dagshub place build data science projects it‚Äôs like github data models experiments notebooks pipelines code recently launched dagshub annotations lets easily annotate data without needing set anything push data open labeling workspace clicks annotate constructed data labeling workflow smoothly connect gitflow requiring zero adjustments check tutorial wrote beginner friendly get started dagshub annotations spoiler alert get annotate elon musk ü§´ check would love hear thoughts
rm4xps,1,best ml ai courses yt using python tensorflow google collabratory hi beginner ml want good course uses python tensorflow google collab hv searching course like long time really want someone help link best course interactive fun well pls help
rndczs,1,hi hoping predict next year fifa ratings based ratings positions age clubs past 5 years missing data important players played last 5 years ie 17 year olds ml technique could applied case
qvmkjg,0,surprisingly simple sota self supervised pretraining masked autoencoders scalable vision learners kaiming et al explained 5 minute summary casual gan papers simplest solutions often elegant cleverly designed certainly case new model facebook ai research called masked autoencoders mae uses smart yet simple ideas can‚Äôt stop asking ‚Äúhow nobody think try ‚Äù using asymmetric encoder decoder architecture coupled data efficient self supervised training pipeline mae pretrained models outperform strong supervised baselines learning reconstruct input images heavily masked image patches 75 blank patches full summary blog post mae upd originally included wrong links arxiv code subscribe casual gan papers follow twitter weekly ai paper summaries
qpuax4,0,alibaba damo academy creates world‚Äôs largest ai pre training model parameters far exceeding google microsoft 10t parameters according company m6 achieved ultimate low carbon high efficiency industry using 512 gpus train usable 10 trillion model within 10 days compared gpt 3 large model released last year m6 achieves parameter scale consumes 1 energy thoughts pace foundational models starting get scary seems like bigger bigger model pushed every week
qskjq5,0,simulation software would use train custom robot say want use reinforcement learning train custom virtual robot stand simulation software would recommend requirements following good training times billions steps would want step restart robot hit ground hard would like get lots training steps fast possible inputs outputs python observe state simulation take actions every one robots joints position balance velocity observations needed well ability observe visual sensors robot fine construction virtual robot terms size weight components exact positioning components precise force robot motors joint see gazebo mujoco popular sure need option maybe write physics engine
r4v2i1,0,software engineering mle maturity assessment soo maybe basic question apologies advance started getting interested ml recently got chance work side project helping manager create framework assessing mle maturity organisation criteria told look software engineering read several articles ml ops finding hard isolate parts process would considered software engineering articles videos recommend could explain software engineering ties ml
r2rn3d,1,understand tensorflow train output understand batch size parameter hello noticed changing batch size train model status number changes follows dataset batch size 10 epoch 1 10 552 552 9s 11ms step loss 0 3074 val loss 0 0041 batch size 20 epoch 1 10 276 276 6s 12ms step loss 0 3802 val loss 0 0107 x200b far thought train function use 10 20 train sample 10 times seeing numbers 552 276 anti proportional batch size think got wrong reality train model slices train set batch size pieces trains per epoch number gets smaller parallelize better need less steps go
rcke6u,1,computer science student want work ml currently working double diploma computer science engineering management really want work deep learning classes well one class one semester like 10h ish theory 20h ish project though 2month internship deep learning tried recreate simclr based scientific paper contrastive learning well better expected still great results find 6 month internship ml since wondering online courses youtube channels would recommand better foundations ml better chance find internship necessarily want projects theory thanks lot
qkh9jg,0,model performance monitoring production recently introduced model performance metrics graphsignal basically logging label prediction model specific metrics automatically computed visualized monitored graphsignal currently saas free account necessary raw data sent statistics details blog post monitoring model performance production logger repo hope useful need monitor models production want build pipelines continuously computing accuracy metrics implementing alerting etc
rb1rcj,0,uc berkeley‚Äôs sergey levine says combining self supervised offline rl could enable algorithms understand world actions new paper understanding world action uc berkeley assistant professor department electrical engineering computer sciences sergey levine argues general principled powerful framework utilizing unlabelled data derived reinforcement learning enable machine learning systems leveraging large datasets understand real world quick read uc berkeley‚Äôs sergey levine says combining self supervised offline rl could enable algorithms understand world actions paper understanding world action arxiv
rnke9u,1,parametric non parametric statistics 2 minute visual guide x200b üîµ parametric non parametric statistics üîµ üßë‚Äçüíª written code heard parameter function takes input computation return output similarly probability distributions parameters define properties distribution üîî case normal distribution mean standard deviation parameters mean controls position distribution standard deviation controls spread peakiness bell curve ‚õ∑Ô∏è parametric approach model consists finite set parameters characterize big assumption parametric model makes model well task underlying distribution data sampled matches model case model data mismatch hurt performance task care ü§∏ non parametric model rely parametric assumptions generally flexible good choice prior knowledge could good model reflects data distribution histogram good example model points sampled arbitrary distribution üßê however free lunch need enough data points get good approximation otherwise histogram look nothing like underlying distribution studying practicing machine learning computer vision 7 years time passed realized power data driven decision making seeing firsthand ml capable personally felt great inter disciplinary tool automate workflows bring different topics ml form short notes interest existing practitioners fresh enthusiasts alike posts cover topics like statistics linear algebra probability data representation modeling computer vision among things want incremental journey starting basics building complex ideas like content would like steer topics cover feel free suggest topics would like know comments
rn5rlo,0,article architecture used commercially curious know companies ml area work sense say example neural network architecture article architecture used commercially find implementation github article would legal use depending repository license maybe implement
r89r8r,0,neurips authors required purchase ticket register question title first year accepted purchasing ticket regular way
r0um7e,1,opinions books core concepts machine learning hi want buy books christmas currently trying develop machine learning skills wondering though much things tend change jobs like machine learning data science actually worth buying books going become outdated relatively quickly better saving money learning google really want buy core concept books going last long time act foundational reference types book existl anyone recommendation please
r761vg,0,clip vs starspace reading paper accompanying clip form openai well starspace fair meta ai couple years ago clear exact novelty besides prettier presentation even though images considered starspace paper method essentially also works potentially cross modal contrastive embedding learning hype
r76jtx,1,bayes probability network pomegranate package anybody familiar could explain model predict proba method actually suppose calculates conditional probabilities particular input based input data sure made hope good learning example learning also implemented predict method gives integer outcome based probability hope understood well star example like anybody idea improve would grateful
rkm5dv,1,neural network dataset fetch different results different operating systems gpus even used code dataset fitting neural network got different accuracy every epoch local system compared google colab think possible time taken definitely vary different systems accuracy neural network 100 epochs doesn‚Äôt even get close
r0mok4,0,nvidia releases web app gaugan2 generates landscape images via text description inpainting sketch object type segmentation map style image gaugan2 blog post gaugan2 web app links info comment another post examples generated text description trees turning color autumn text description winter mountain landscape near sunset text description stream sketch computed app previous image text description stream sketch previous image style change using one app style images
rmuptq,1,language write conversational ai basically ultimate goal would ai able hold conversation know words mean answer maintaining context even topic clearly stated message also amount memory e remembering names relationships like mom name know languages working knowledge html css javascript think really helpful interested know language learn determined make brainchild reality thank advance
qlujgx,0,loss function penalizes classification errors heavily modify log loss working classification problem predictions good enough actually good enough need model spend time optimizing top 1 90 become 98 99 however really care model makes incorrect decisions built family models different sizes parameter counts using traditional log loss loss function model make catastrophically bad decisions occasion domain catastrophically bad classifications ruin tens thousands good classifications wondering different loss function use penalizes errors even heavily log loss log loss squared natural question ask errors data pipeline model configuration debugged quite bit sure problem domain chess naturally manifold complex twisted many sharp edges saddle points really difficult space work model smaller gpt 3 understandably going trouble
r4okv5,0,ways reduce model gpu vram usage hey guys gan models take 10gb vram try generate 512x512 image need scale stuff plan productionizing kubernetes cluster want optimize first save costs ideally want run multiple inferences machine without facing cuda memory error good ways optimize vram usage
qm5axp,0,adaconv explained adaptive convolutions structure aware style transfer 5 minute summary casual gan papers classical style transfer based adaptive instance normalization limited transferring statistical attributes color distribution textures ignoring local geometric structures image stuff past let introduce adaptive convolutions drop replacement adain proposed prashanth chandran team disney research adaconv able transfer structural styles along colors textures real time full summary blog post adaconv arxiv code subscribe casual gan papers follow twitter weekly ai paper summaries
rw7go4,1,iterating model fit fast forward model think worth using there‚Äôs always little bit randomness involved training model right you‚Äôve tuned validation set you‚Äôre ready try model test set bad practice iterate fit process like 1000 times store model least loss deploy test set asking overfitting corrupt model assume time required perform iteration n loops training process reasonable
qszmuu,0,analysis iclr 2022 review scores analysed relationship iclr 2022 review scores factors social media popularity presence arxiv twitter thread results analysis papers present arxiv higher recommendation scores mean review scores 5 1 papers arxiv 4 7 papers arxiv papers shared twitter 5 likes also higher recommendation scores papers mean review scores 5 4 tweeted 4 8 tweeted papers source code available promised upload soon empty repos also got better reviews mean review scores 5 2 code 4 8 without code scatter plot likes twitter review score small positive correlation twitter likes review scores correlation coefficient 0 2 iclr 2022 submissions sorted review scores found
r24rp7,0,peer review still broken neurips 2021 review experiment yannic kilcher yannic kilcher thoughts 2021 neurips reviewer experiment frankly agree completely review process completely broken arbitrary yes phenomenal papers get accepted given phenomenal papers get traction whether published peer reviewed venue however sheer level randomness regards good phenomenal papers searing condemnation review process kilcher discusses completely invalidates notion publishing metric value respect phd students tenure track professors grant applications etc
rg44wl,1,international data analysis olympiad idao 2022 x200b invite ml students specialists world take part international data analysis olympiad hse university yandex organizing 5th time otkritie bank platinum partner year since it‚Äôs first anniversary decided change format students ml specialists divided two separate divisions students able join main competition ‚Äî student division others join open division participate hors concours interest traditionally first stage‚Äôs task given laboratory methods big data analysis lambda hse university predicting properties two dimensional crystals various configurations task finals provided otkritie bank olympiad includes two stages online stage 1 28 february 2022 ‚Ä¢ track 1 traditional machine learning competition yandex contest platform need make new predictions upload automatic verification system ‚Ä¢ track 2 come solution problem keeping within rigid framework time memory used final 16 17 april 2022 moscow ‚Ä¢ top 30 teams according online stage results invited online final ‚Ä¢ final 36 hours competition participants try train model create full fledged prototype tested terms accuracy performance registration open till february 13
r3cvu4,1,difficulty keeping new research applications ai ml read list gets bigger easy fix difficulty keeping new research applications ai ml read list gets bigger easy fix follow channel blog explain new applications papers short videos articles weekly hope useful let know thoughts content improve better answer need
rx2vov,0,retrieval transformers domains going research behind wondering work like domains say computer vision example
ra9jmh,1,apply feature selection mixed inputs numeric nominal apply feature selection mixed inputs numeric nominal
r9f3nh,1,need help extracting number objects different classes using yolo trying extract number objects yolov4 detect video frame variable use processing find particular method think save variable printing find way thanks
rwtpxv,0,preparing comprehensive exam ml phd student based canada comprehensive exam coming 4 6 months exam nervous since began phd fairly confident actual proposal answering questions related field concerns fundamental background question ml statistics broad plus little older side memory little poor students taken comprehensive exam experience prepare reading making notes textbook good idea preparing list topics reading extensively better option
rvy2u8,1,made tutorial variational autoencoders intuitive explanation code hey everyone variational autoencoders really awesome tool data generation found lack resources simple explanations work made guide find without going heavy mathematics guide goes 1 high level introduction link section 2 ordinary autoencoder recap link section 3 overview variational autoencoders link section 4 intuitively understanding variational autoencoders learn link section 5 building variational autoencoder fashion mnist dataset link section end able create gif like one see vae learning associate different regions plane different types clothing used vaes think vaes compare gans applications generative models interesting today love hear thoughts feedback also please let know interested follow post goes mathematics behind vaes
qzjuvk,0,discussion neurips 2021 finally accepted submissions statistics crawled data rating distribution x200b top keywords x200b consistency experimental results x200b
rmx04g,1,k means clustering 2 minute visual guide x200b x200b üîµ k means clustering üîµ üî± k means algorithm divides n data points k disjoint clusters well known unsupervised learning model means k means algorithm requires data points need corresponding cluster point belongs algorithm figures ‚ú® clusters found allocating points way total variance points within cluster intra cluster variance reduced minimized although iterates quite fast k means algorithm varying cluster formations based initialization widely implemented many software packages scikit learn package python one used often üîÅ common method perform clustering iterative alternates two steps 1 assigning point cluster based point closeness distance cluster center 2 updating cluster center called mean centroid hence name k means one k clusters based points belong particular cluster iterations usually performed centroids stabilize converge consecutive iterations ü§ì k means popular scenarios data known consist multiple groups distributions unknown point belongs group cluster used data analysis splitting data comes multiple distributions image segmentation color quantization among things studying practicing machine learning computer vision 7 years time passed realized power data driven decision making seeing firsthand ml capable personally felt great inter disciplinary tool automate workflows bring different topics ml form short notes interest existing practitioners fresh enthusiasts alike posts cover topics like statistics linear algebra probability data representation modeling computer vision among things want incremental journey starting basics building complex ideas wanted something would personal touch decided hand written notes self drawn figures takes time create post deciding content making concise coming ideas best convey illustrations wanted give something reaches wider audience shot like content would like steer topics cover feel free suggest topics would like know comments
rrfeyx,1,kind technologies programming languages would take build ml ops platform like valohai
qsjngz,0,unity3d ml agents use gridsearch hello trying find way gridsearch hyperparameter tuning reinforcement learning unity3d machine learning platform googled feels like options available right alternative ways please share thank guys
qpuqtk,0,bayesian nonparametric causal inference nice summary paper cutting edge bayesian causal inference link abstract spurred recent successes causal inference competitions bayesian nonparametric high dimensional methods recently seen increased attention causal inference literature paper present comprehensive overview bayesian nonparametric applications causal inference aims introduce fundamental bayesian nonparametric toolkit ii discuss determine tool appropriate given problem iii show avoid common pitfalls applying bayesian nonparametric methods high dimensional settings unlike standard fixed dimensional parametric problems outcome modeling alone sometimes effective argue time necessary model selection outcome processes
rv4nt6,1,nlp hybridization statistical approach expert system hi everyone question context aggregate platform various ai apis market gcp azure etc including nlp apis keyword extraction sentiment analysis ner etc idea developer create accounts different providers one api test compare change whenever wants however many customers ask us mix statistical approach behind apis expert systems achieve hybridization idea thanks
r7wdou,1,need help understanding meaning loss values wgan gradient penalty hey guys currently working training auxiliary classifier wasserstein gan gradient penalty based implementation added auxiliary classifier functionality model trained quite epochs emoji images trying filter bad vice versa good samples set generated examples understand best way automatically utilize trained discriminator loss values evaluate fakeness images order able select ones able fool discriminator understand discriminator using wasserstein distance attempts separate loss values fake samples much possible ones real samples given discriminator output randomly chosen set generated images looks something like table think images maximum minimum values highest lowest graded examples 22 96732 12 37780 23 39248 44 45711 14 15668 11 19169 35 99777 9 65943 16 71531 9 35125 25 98240 4 36232 8 58446 24 78805 7 47653 19 14746 28 33695 30 18404 2 67499 8 63077 mean fake real generated image samples 44 4 2 67 neither outliers particularly real looking look much worse randomly chosen examples see interpret loss values would make sense go median loss value get good looking average loss images would make sense run 2 means clustering algorithm losses try separate realistic fake samples anyways thanks advance help
qvhazn,0,possible generate text based previously generated text huggingface currently gpt 2 based text generator trained shakespeare plays currently generated text looks like first murderer mark antony sirrah pray thee thou wilt fight sword cassius know thee sir cannot tell knew tis well thou art sooner mark antony timon hour tis best make sorry malvolio lord king claudius king henry vi lord talbot florizel ay sir tis better prospero measure shall well come thy mouth come water else thou couldst hear think thy face quick thyself heart new heart could thee come thy heart shall blood fair thy bed death peace thy heart look must hour day death shall however problem text disconnected really coherent really sure huggingface provides api like able x generation somehow take account x 1 feel like would make text lot coherent less disconnected case piece generated text started caps character name anyone knows bit stuff know better way generate realistic large amounts coherent text problem generate 768 tokens time obviously models write entire paragraphs text figured good way use previous output text generation sort initial state future generation also add pass previous input ids model generate method generate text since previous text already eos token
qz4uun,0,edit images gans part 1 digital metaverse avatar tutorial covers intuition behind image inversion gans editability vs reconstruction tradeoff projecting images generator latent space telegram post blog post image edited styleclip subscribe casual gan papers follow twitter weekly ai paper summaries gan tutorials
rvl86n,1,need help assistance completing project decided create application web application actually spotify users kind match fellow friends mates anyone else matching similar spotify blend spotify api gives us ability gather data user‚Äôs listening history etc key data songs listen data might able help create algorithm match really know nothing ml drive project completion wish someone ml experience open learn help drive project end interested could reply post can‚Äôt help direct right path respect things could learn complete task could share thanks
rav2l3,1,starting machine learning natural science standpoint hi currently working phd material science research trying accomodate machine learning work requires quite deep understanding cannot tensorflow way one research papers actually involves creating package scratch ml concepts adapted molecular atomistic calculations trying find good book give 1 good perspective things happening ml space models techniques available used 2 good rigour overly abstract introductions statistics linear algebras used 3 equip skills understand research papers methodology hopefully dissecting source codes made scratch oftenly coded python c fortran language really barrier understanding implementation ml concepts instead good book candidates introduction statistical learning b pattern recognition machine learning c deep learning understanding machine learning theory algorithms e machine learning probabilistic perspective f elements statistical learning know reading everything beneficial thats bit unrealistic especially phd student haha x200b help opinions appreciated thanks
rul8l2,1,padding vector variable sequence vector multivariate lstm regression task aware regular nlp would use padding token regular seq2seq mode would use padding instead sequence tokens variable sequence continuous vectors
r4hhs3,1,looking study partners applied predictive modelling hi aspiring data scientists applied predictive modelling great book learning ml foundations used industry looked books elements statistical learning quite application focus one top rated book r datascience book suggestion thread help keep accountable learning goals discuss content exercises understand content together meet discord server discussion week let know pm link goal find data scientist role great position find work 3 months 12 chapters exercises aim complete 1 chapter per week book happy hook cheers
rav3gq,0,hierarchical topic modelling time hello reddit proud present htmot hierarchical topic modelling time paper proposes novel topic model able extract topic hierarchies also modelling temporality modelling time provide precise topics separating lexically close temporally distinct topics modelling hierarchy provides detailed view content document corpus code easily accessible github working interface provides ability navigate resulting topic tree ease
rnhyeo,0,visualize ground truth boxes images hello guys might guessed topic want visualize ground truth boxes images pascal voc dataset anyone help dilemma trying find ways display ground truth boxes images help would good
rf7mpc,1,take machine learning first semester sophomore year hi currently freshman first semester taking python class take java class next semester along intro stats linear algebra database management sql ap cal high school hence solid base cal ii think want ask would good base learn ml first semester sophomore prepare better appreciate contribution
rhzj3y,1,gis ml newbie seeking help land classification project hello recently started learning bit gis deep learning order learn practice skills trying create dataset open source data training models try recognize various types land classes usage initial thought process project 1 acquire data land usage prediction target acquire real map data 2 combine obtain labeled images 3 use supervised training labeled dataset 4 use trained models new inferences x200b far found interesting gpkg truth labels urban atlas using geopandas currently able open gpkg access several columns data land class example water industrial well geometry sample contains multipolygon objects would like use jp2 band files obtained via copernicus sentinel 2 program merge one tif file containing different bands combine overlap real satellite images land usage map contained previously mentionned gpkg x200b thought would cut square pieces rasters images map save label class filename later train cnn like eurosat dataset using cnns might best correct approach though saw articles mentionning r cnn u net architectures seemed interesting know much use case scenario would interesting use project x200b kind knowledgeable soul guide help figure problem enlighten basic gis ml concepts might seem misunderstand help appreciated x200b adding working python currently using jupyter notebook trying use open source stuff possible staying away using software like qgis
qq5hrq,0,community sourced open audio datasets ‚Äì hacktoberfest 2021 hey r ml month ago posted dagshub supporting hacktoberfest ml datasets wanted something geared towards ml community decided create open source catalog üîä audio datasets response truly amazing received 40 dataset contributions publicly available viewable dagshub cover various tasks languages sizes use projects want check list datasets wait see everyone builds huge thank everyone participated made possible fact hacktoberfest mean continue contributing love see datasets audio domain others
rh942h,1,random variable vs instances ml stats lets say talking dataset used input stats ml algorithm example paper passage x200b dataset considered sequence iid random vectors instances vectors assumed first people stats stackexchange discussion claims 2nd
r3xwru,1,k means high cluster count use high cluster count suggested elbow method kmeans may lead extra data analysis lead mixing data points one cluster another high extent using 200 clusters instead 50 suggested elbow method high data left unclustered
rd8wum,0,make best neurips starting work phd project months already enrolled first months devoted looking supervisor phd project attending neurips would like make best amount papers events etc daunting someone starting recommendations people situation
ru4odm,1,laptop get machine learning getting started machine learning figure matter much type laptop get suggestions would helpful come data science machine learning primary focus study assume early won‚Äôt matter much get want something able use long term improve heard linux best os use don‚Äôt much knowledge get recommendations trying keep 2 500 us edit looking dell precision 7560 workstation processor 11th gen intel core processor i7 11800h 8 core 24mb cache 2 30ghz 4 60ghz 45w operating system ubuntu linux 20 04 graphics card intel¬Æ uhd graphics 11th gen intel¬Æ processors display 15 6 fhd 1920x1080 60hz anti glare non touch 45 ntsc 220 nits cam mic wlan memory 32 gb 4 x 8 gb ddr4 3200mhz non ecc sodimm hard drive 2 2230 256 gb gen 3 pcie x4 nvme solid state drive thoughts
radg0c,0,using reduced size databases alphafold multimer anyone success running alphafold multimer reduced size databases e g small bfd default seem like functionality supported deepmind like second opinion whether something like currently possible
r8do0d,1,cuda cudnn docker container nvidia license distribution question told license allow person use cuda cudnn devel docker image base build opencv cudnn install python program distribute people use license reads 1 license subject terms license nvidia hereby grants non exclusive non transferable license without right sublicense except expressly provided license 2 install use copies container modify create derivative works samples example source code delivered container applicable develop test services applications 3 b deploy container infrastructure lease offer service third parties without distributing container exposing nvidia apis container directly service users 4 c develop extend container create compatible defined derived container includes entire container plus software primary functionality develop compile applications distribute derived container run applications subject distribution requirements indicated license used section ‚Äúcompatible‚Äù means extensions container must adversely affect functionality components container x200b pertinent part non exclusive non transferable wording extend distribute searching seems build image using debian cuda cudnn distribute thats ok using nvidia cuda cudnn container base x200b thanks helping clear confusion
rh25sh,1,i‚Äôve made search engine 5000 quality data science repositories help save time machine learning projects i‚Äôve working data science 15 years years i‚Äôve found many awesome data science github repositories created site make easy explore best ones site 5k resources 60 languages mostly python r c 90 categories allow access detailed stats repository commits number contributors number stars etc filter language topic repository type find repositories match needs hope helps let know feedback website edit link site gitsearcher com
qv3xpt,0,cycada feature level gan loss cycada paper equation 5 paper makes use f however corresponding portion figure 2 orange makes use f additionally f makes sense since input target domain f equation 5 f
qlvjdy,0,using perplexity evaluating language models hello assume training dataset ten sentences used constructing probabilistic language model consisting maximum likelihood estimates different combinations bigrams ten sentences test accuracy bigram language model test dataset consisting twenty sentences used calculating probability test sentences using bigram conditional probabilities calculated using sentences training set evaluate accuracy bigram language model need calculate perplexity measure indicating accuracy probabilistic language model test dataset twenty sentences correct calculate perplexity value twenty sentences using probability values taking average twenty perplexity values calculating overall perplexity value test dataset correct calculate probability twenty sentences find simple average twenty probability values give simple probability average used calculating perplexity value test dataset would procedure apply test dataset number sentences would procedure apply trigram model would procedure apply n gram models like quadrogram model goal always probabilistic language model lowest perplexity value thanks
rg61dp,1,best machine learning resources beginners planning studying machine learning want best resources courses books etc get started andrew ng ml course coursera exception
qqhct3,0,interesting bit info htc keynote nvidia selene 500 node superpod trained gpt3 11 days sure cost run machine imagine represents big cost time reduction vs 1 year ago
r1v2n3,1,help ai assignment hi guys new ai ml need help assignment could someone please walk needs done question assignment understand thing question sports prediction large numbers factors including historical performance teams results matches data players accounted help different stakeholders understand odds winning losing demonstrate following fifa 20 dataset fifa 20 complete player data set collection detailed attributes every player registered latest edition fifa 20 database get data kaggle 1 demonstrate data preparation feature extraction process 2 create feature subsets show maximum correlation dependent variable 3 create train suitable regression machine learning model predicts overall rating score player based attributes 4 measure performance model fine tune process optimisation 5 use data another season used training test good model 6 deploy model simple web page using either heroku streamlite flask upload link video shows application website works
qx0enm,0,bias ml comes biased data post referring bias social sense racism sexism ‚Ä¶ bias strictly mathematical sense obviously train model biased data trained model inherited bias people say main way bias finds it‚Äôs way models however assume imagenet biased benchmark probably vision model architectures developed well imagenet bias could way also inherent resulting architectures learned weights wrong ways besides biased data one aware
rbw8g2,1,help nim game using reinforcement learning recently given assignment already submitted taking get back would like feedback know right track actually understanding content code get reward state float get reward int state int turn float retval 5 state 20 retval 100 turn 0 return retval else return 1 retval get potential current state int get va int state int turn state 20 return 0 turn turn 1 2 float reward1 get reward state 1 turn float reward2 get reward state 2 turn float v1 reward1 gamma get va state 1 turn float v2 reward2 gamma get va state 2 turn multiply potential probability happening 0 5 v1 0 5 v2 return 0 5 v1 v2 decide value play returns either 1 2 int get value int score float one reward get va score 1 0 float two reward get va score 2 0 printf 1 f 2 f n one reward two reward one reward two reward return 1 return 2 thinking get potential move know time inefficient fine play move highest potential time bit skeptical though since assumed 50 probability play rather reality opponent playing best move would love feedback share rest code since professor want get trouble sharing pretty simple prompts user input keeps track score calls get value function computer turn edit found way make work logs make run compiling running says first 20 goes first 1 computer 2 1 0 add 1 2 1 1 3 000000 2 2 000000 1 computer adds 2 3 add 1 2 1 1 3 000000 2 3 000000 4 computer adds 1 5 add 1 2 1 1 1 000000 2 3 000000 6 computer adds 1 7 add 1 2 1 1 4 000000 2 0 000000 8 computer adds 2 10 add 1 2 1 1 8 000000 2 0 000000 11 computer adds 2 13 add 1 2 1 1 17 000000 2 8 000000 14 computer adds 2 16 add 1 2 1 1 25 000000 2 50 000000 17 computer adds 1 18 add 1 2 2 win
r9za5a,1,scene detection using opencv make project example 2 scenes model differentiate 1 excercise scene 2 cooking kitchen scene also aquire datasets help helpful
qm2ov5,0,project discover ongoing ml ai competitions looking feature suggestions mlcontests com two years since posted new project ml contests well received since thought post update ask feedback main page mlcontests com main page lists ongoing competitions also newsletter occasionally send updates competitive ml space separate page compares cloud gpus ml visit site mlcontests com traffic steady newsletter growing ads like figure take next love hear thoughts want site ps want contribute open source
qnu4bg,0,real world challenges agi deepmind either understand article inserted letters g randomly current progress weather prediction plasma control fusion
qnznd4,0,help choosing right data file format storing text data hello working nlp project requires collect data collected data stored individual text file one article single text file future analysis cleaned data converted dict article id article text want store data specific file formate json hd5 etc want recommendation best format store kind data please provide suggestions keeping mind latency loading data drive faster better
rau451,1,code machine learning platform amazon aws sagemaker canvas recently launched sagemaker canvas pretty awesome beginners machine learning like tried hands ml projects like customer churn spam sms detection etc gave predictions almost accurate short course udemy currently free join
rwseha,0,format full paper presentations general ml conferences like ijcai aaai hi first conference season curious author full papers extended abstracts student track invited speeches present conferences like icml aaai ijcai mean instance presentations performed panel 3 5 presenters using slides presented posters authors stay available duration time interested readers show discuss something else
r9kpez,1,anyone explain make predictions test dataset hey everyone fairly new machine learning aplogiws incorrections school project implement logistic regression model see probability customer enter default state teacher provided us train test csv need make submission competion kaggle customer id predictions developed notebook performed eda selected features wanted ploted confusion matriz checked performance metrics know test data set sure suppose procced instance train dataset drop features dealt outliers missing values performed one hot encoding test set know use predict funtion python getting 1 list colimns want use make predictios geting 1 result cant understand sorry long post help really apreciated
r3odcu,1,mobile app machine learning deployment package like streamlit pre made templates mobile application convenient creation machine learning apps please suggest
r4yvl3,1,best beginner book machine learning best beginner book machine learning python something super rigorous also gloss details covers almost everything beginner along implementing machine learning algos
quip0a,0,siraj raval youtube video siraj put interview lex fridman two years ago lex fridman interviews siraj raval lex took video channel siraj asked permission repost video lex reply siraj took upon post anyway
r2rw37,1,visualize humor detection model gain useful insight going bunch 1 1 2 2 3 3 humor detection paper papers include visualizations say graph related model trained thinking train language models like bert gpt xlnet guessing kind interesting visualization aim order gather data training gain sort insight like fine tuning zero one shot learning based models train long involve significant learning scratch somewhat black boxes nothing much visualize 1 2 3
rb7oa7,1,dissecting lobe ai models tensorflow start new machine learning trying hand cnns moment raspberry pi set window take pictures birds visit feeder want make model tensorflow detect save photos birds sometimes car pedestrian passing captured interest x200b models made thus far good two reasons accuracy basically coin toss enough knowledge improve yet came across lobe ai model builder decided reverse engineer models desirable accuracy dataset want see makes tick export model makes tensorflow format clue compare mine interested preprocessing data augmentation used layers number epochs etc possible model made files look x200b could use lobe model last resort since learn anything
rwmsd6,1,bias fully connected deep feed forward neural network coding neural network scratch increase knowledge neural networks trouble bias node bias value number biases number weights one bias value per layer insights would much appreciated
rh4l9j,0,area statistical power curve machine learning field binary classification common metric measuring performance models auroc area receiver operating characteristics curve statistical hypothesis testing power curve turns roc curve plot true false positive rates area roc curve nice interpretation heard anyone talk area power curve also interpretation probability test statistic null higher one alternate see proof interpreting auroc hypothesis testing rohit pandey dec 2021 medium
qxl93a,0,state pytorch mobile comparison tflite anyone experience frameworks depth knowledge current state affairs regarding embedded dl like turn another framework flame war instead general comparison tflite pytorch mobile latter still beta terms ease use mostly much need break model layers actually conversible mobile format etc api stability support mobile ios android terms cross compilation tools hw acceleration support socs fringe devices also contenders missing
rnu744,1,recurrent neural network time series forecasting hi given challenge artificial neural network course university consists forecasting 7 time series using rnn state art architectures try improve performance achieve task thanks lot help
r3ab0x,1,anyone answer manual practice computing using python would much appreciated thanks advance
rewfbp,1,hardware assistance hello far training done cpu since radeon rx 590 want upgrade expensive one want match new gpu properly current cpu motherboard prime a320m k ryzen 7 2700x planning getting zotac gtx 1070 8gb x gaming fisable also really require one 8 pin slot still use old gpu primary dedicate zotac training thanks advance
qsebrh,0,cfp ai design manufacturing workshop adam aaai 2022 deadline extended hello r machinelearning deadline submitting ai design manufacturing workshop adam aaai 2022 extended week due multiple requests working intersection ai design manufacturing scientific computing geometric modeling consider submitting 4 page workshop paper invite paper submissions following related topics new theory fundamentals ai aided design manufacturing novel ai based techniques improve modeling engineering systems integration ai based approaches engineering prototyping manufacturing novel methods learn scarce sparse heterogenous multimodal data novel ml methods computational material physical sciences novel ml accelerated optimization conceptual detailed system design novel ai enabled generative models system design manufacturing ml guided rare event modeling system uncertainty quantification development software libraries benchmark datasets identification key challenges opportunities future research workshop website submission website submission deadline november 19th 2021
r132f4,1,good ai mlcourses youtube channels books beginners hi guys courses youtube channels books recommend absolute beginners learning ai ml
rf4vyo,1,repurpose yolo classify objects like usual also binary classification objects deal want take pretrained yolo model replace last layer slightly different task classes individual people gary vs chris vs stacy etc also want know hand open closed fist dataset would consist bounding boxes class x width height 0 annotations open fist class x width height 1 annotations closed fist know standard yolo algorithm returns b 5 c sized tensor number grid cells per axis b number bounding boxes per grid cell c number classes obviously b 5 means every bounding box 5 outputs namely confidence x width height intuition correct need change output layer yolo output b 6 c tensor would represent bbox confidence x width height hand confidence retrain labeled dataset correct could also double class vector gary open gary closed chris open chris closed guess really bad solution gary open would complete separate class gary closed would confusing model wanted add hand poses example tripling quadrupling worse class size also output layer linear activation function would optimal binary classification problem
r31m4v,1,generating weights file training check accuracy trained model recognize object using yolo model generated weights file validate accuracy model weights file
qus9jr,0,speech text google api something else hi would google asr api considered best thing around others equally good better particularly interested open source freely available code could customized specifically looking building app train voice use proofing audio recordings pre prapared texts hoping work max msp good code seen examples max patches query google api speech recognition believe least one example max object op recognize also speech text far got far thought ask
r6314s,1,feel job recruitment ai survey hey everyone name davis work consulting organization called illinois business consulting we‚Äôre working company ai sector hoping improve recruitment made survey ai recruitment guys wouldn‚Äôt mind filling survey would awesome thank
ru57dw,1,infrastructure requirements deploying nlp model freelancer builds ai models small companies one asked deploy model production environment wanted give infra req related know requirements regarding hardware software integration needed good articles would appreciated one getting google generic need something specific nlp model based distilbert 250mb size tia
qwu7nc,0,synapseml simple multilingual massively parallel machine learning library today we‚Äôre excited announce release synapseml previously mmlspark open source library simplifies creation massively scalable machine learning ml pipelines building production ready distributed ml pipelines difficult even seasoned developer composing tools different ecosystems often requires considerable ‚Äúglue‚Äù code many frameworks aren‚Äôt designed thousand machine elastic clusters mind synapseml resolves challenge unifying several existing ml frameworks new microsoft algorithms single scalable api that‚Äôs usable across python r scala java website paper blog x200b x200b synapsemls api aims unify simplify distributed ml
r51wa6,1,difference deeplearning framework teachable machine learned deeplearning wondering companies didnot use teachable machine already developed difference seeking deeplearning analyist use already developed app new track
r3jz31,0,edit images gans tutorial part 2 need photoshop got gans tutorial learn intuition behind discovering editing directions latent space stylegan 2 generator using directions manipulate generated images meaningful way pipeline exported actual image editor telegram post blog post image source subscribe casual gan papers follow twitter weekly ai paper summaries gan tutorials
qmm6uh,0,ethical concerns ml predict race gender i‚Äôm working data product primarily uses image name classifiers identify race gender means someone buys product able see race gender data associated people companies database use case behind report make decisions improve diversity e investment firm seeking invest underrepresented groups hr company reporting industry trends i‚Äôm looking feedback ethical design quality concerns regards following factors primarily leveraging publicly available training data sets models classifiers gender classification includes male female options use publicly available photo classifying person none data provide self reported product communicate customer yet provide confidence score feedback correction feature race gender sex legally protected classes cases using make decisions product ones generating data customers use discretion i‚Äôm worried enough due diligence intentional choices we‚Äôre making unintended impact may resources designing fair systems especially ones attempt generate rather consume type data would appreciated
ri76tj,0,geo deepfakes far away webapp demonstrating capabilities geo deepfakes
rimqij,0,embed powerbi report jupyternotebook powerbi reports could add new dimension visualization capabilities jupyter notebook tried demonstrate post one access custom powerbi report inside jupyter notebook
qyo2mu,0,implement permute transpose op 6 times faster pytorch article introduce techniques optimize permute kernel oneflow compare pytorch‚Äôs permute native copy operation experiment results show deeply optimized permute operation much faster bandwidth effective pytorch bandwidth utilization close native copy operation
r0zvrb,1,monte carlo simulation approximation pi number number œÄ mathematical constant approximately equal 3 14159 defined euclidean geometry ratio circle circumference diameter also various equivalent definitions number appears many formulas areas mathematics physics earliest known use greek letter œÄ represent ratio circle circumference diameter welsh mathematician william jones 1706 also referred archimedes constant simulation works principle generating random points square page r 1 based distance point center coordinate system possible determine whether point inside outside imaginary quarters circle quarter surface circle frac r 2 pi 4 area square r 2 number pi approximated frac k 4 k number points within quarter circle total number simulated random points number randomly generated points increases possible calculate number pi greater precision expense time efficiency
r4wd3a,1,illustrate fairly simple manner roberta gets fine tuned downstream task thesis defense coming next week wanted take issue currently facing one thesis contributions adapting roberta task rumor detection twitter want explain jury roberta adjust weights based dataset fine tune simple terms fed roberta variety datasets describing task rumor detection twitter altering class distribution datasets see influences embedding roberta produces evaluated quality embeddings feeding set classifiers random forest decision tree svm see perform used standard metrics precision recall f1 score focusing model performance recognizing class rumor considering explaining way roberta takes tweet label rumor non rumor weighs words impact class question words occur often class ones potentially correlated feel like much watering even insult intricacy involved roberta inner workings much knowledge expertise please indulge request enlighten one explain details fine tuning pre trained language models downstream task
r5l11k,1,ai ml model use predicting side effects previous taken decisions hello let know totally newbie ai ml field full stack php developer knowledge javascript html css etc wondering ai ml model capable predict side effect based previous taken actions example let say plant used water every day normal process forget one day water flower leafs turning another day water plant twice leafs getting yellow another day water mistake water petrol inside well wife water chlorine finally plant getting die due petrol see according actions take plant side effect like remains healthy leafs turning leafs getting yellow plan die wandering anything make link side effect actions taken previously like watered plant water petrol chlorine plant died water plant many times leafs became yellow anything mind could possible scenario creating model thank advance
rl5j1e,1,things need concentrate course curriculum hi started master computer science specialization artificial intelligence aspire become machine learning engineer would helpful anyone suggest things course curriculum concentrate thank
r489w8,1,resources sequential pattern mining r looking good resources apply sequential pattern mining app data specifically r common package job apparently arulessequences cspade algorithm found blog post basically looking something similar help appreciated thanks advance
rnios4,1,easy understand cnn tutorials hello trying learn convolutional neural network nice resources got basic ideas part unclear resource every specific detail studied anyone learn topic easily thanks
rlcpx5,1,train custom text handwriting models hi testing aws textract service noticed json file results bounding boxes detected words wondering information used build dataset retrain custom model asking lot errors predictions si correct train new model good model stop using textract thoughts
r7uqrp,1,sentiment analysis api vs custom text classification one choose kdnuggets
r1zsaw,0,graph output ml modelling hi currently thinking tackling rather specific problem using ml kind statistical approach really problem given set nodes names want predict acyclic graph nodes arranged also set meta attributes put graph context also set historical graphs together meta attributes basic idea assume graphs similar meta attributes follow similar pattern also wrt node names ultimately want build model able forecast resulting graph solution could think building model forecast single entries adjacency matrix essentially fitting function f meta node1attributes node2attributes 1 node 1 node 2 else 0 smarter way
rpcifl,1,visual ads data set pinterest visual ad recommendation project hello folks need help currently working pinterest visual ad recommendation project knowing people pins ads categories ads recommend found nice 2million visuals data set users pins however struggling find visual ads category data set would appreciate pieces advice find datasets apis ideas approach search thank advance
r6asy1,1,poll data quality remains top level concern machine learning teams recent study concluded data quality increasingly becoming top level concern among companies implementing ai labeling annotation business cogito tech llc noticed several cases organization approached us labeling work various reasons wanted know larger community think key reasons demand data quality still increasing despite many datasets available much work done view poll
qwbi8e,0,find balance writing code scratch modifying code final year aerospace project want implement ml newbie final year project need something presentable odds things need ml like image recognition someone already written code feels like copying pasting code know modifying degree still think learn much obviously wanna reinvent wheel still one go
qx6giu,0,faang interviewing research interns summer 22 already applied many big tech companies phd research intern positions mostly focusing machine learning deep learning recently got several offers non faang companies initial goal get internship position faang received many opportunities get interviewed interview amazon interview facebook soon faang curious usual hiring interview period intern students perhaps attractive big techs got many interviews wonder gathering applications start interviewing applicants december got offer companies need decide soon whether accept idea hiring process faang almost finished opportunities hard decide 1 experience hiring process research intern positions faang could let know rough dates interviews got offers 2 also first internship phd know consider choosing position positions research intern positions topic projects roughly aligned interests time told specific topic decided start internship discussing mentor hope could get position high possibility could publish paper true many uncertainties could give advice selecting intern position multiple options advice regarding internship helpful thank inadvance
rb6jow,0,news aaai 2022 moves fully online format x200b salty salty
rbno04,1,cnn trained fashion mnist hello would cnn trained fashion mnist perform categorizing mnist thanks
rs6bu8,0,anyone know databases political text labelled classes conservative liberal neutral supervised learning like fine tune google bert natural language processing machine learning model political domain many thanks
rg43yb,0,international data analysis olympiad idao 2022 x200b invite ml students specialists world take part international data analysis olympiad hse university yandex organizing 5th time otkritie bank platinum partner year since it‚Äôs first anniversary decided change format students ml specialists divided two separate divisions students able join main competition ‚Äî student division others join open division participate hors concours interest traditionally first stage‚Äôs task given laboratory methods big data analysis lambda hse university predicting properties two dimensional crystals various configurations task finals provided otkritie bank olympiad includes two stages online stage 1 28 february 2022 ‚Ä¢ track 1 traditional machine learning competition yandex contest platform need make new predictions upload automatic verification system ‚Ä¢ track 2 come solution problem keeping within rigid framework time memory used final 16 17 april 2022 moscow ‚Ä¢ top 30 teams according online stage results invited online final ‚Ä¢ final 36 hours competition participants try train model create full fledged prototype tested terms accuracy performance registration open till february 13
rwr9kb,1,blog knowledge distillation first attempt writing ml blog comments suggestions welcomed
raa9y7,0,arxiv sanity like view neurips 2021 papers image thumbnail paper one favorite features arxiv sanity helps get quick overview whether paper includes nice graphs summarize work convey intuition proposed approach wanted get quick overview top neurips 2021 papers created arxiv sanity like view 2334 papers ordered rating includes links code tldr section meta data available openreview link project yeah people still go publication lists p
rpcwbh,1,questions mitx courses probably statistics interested taking introductory course probability offered mit edx see link also interested taking introductory course statistics following probability course listed see link seems like courses require students familiar single variable multi variable calculus statistics course also requires student familiar vectors matrices taken calculus 1 first year college 10 years ago would prefer get certificates courses afraid might able complete taken 2 courses able well courses without prerequisites thanks advance
rtvf85,1,got list practical ml projects surprisingly done individuals computer necessarily limited enterprise origin
razsj2,0,people ‚Äúread‚Äù many papers possible i‚Äôve got colleagues always claim reading papers way ‚Äúread‚Äù damn superficial example finished fully reading comprehending paper won‚Äôt lie took solid couple days understand everything fully reading things multiple times meanwhile daily meetings mention paper try use components work someone says ‚Äúoh ya read like 15 mins‚Äù decide impromptu discussion jesus christ swear thing read abstract maybe glanced network architecture i‚Äôm sorry turning rant really grates nerves people say read something reality look abstract i‚Äôm firm believe reading comprehending fully understand 1 single ‚Äúkey‚Äù paper whatever field you‚Äôre studying much better investment time skimming 100 regurgitated ideas edit guys clarify believe skimming abstracts looking interesting papers go dozens day you‚Äôd lost otherwise haha take issue though someone claims they‚Äôve ‚Äúread‚Äù something they‚Äôve done gone abstract glanced
r2qf51,0,discussion ratings 837 iclr 2022 submissions changed open discussion x200b updated data 12 01 2021 x200b iclr 2022 statistics available 466 submissions withdrawn 11 09 11 26 submissions 3328 2862 837 2862 submissions ratings changed reviewers top submissions ratings got lifted top submissions ratings got droped unluckily x200b
r8mlwd,0,implement efficient softmax cuda kernel ops computed deep learning frameworks translated cuda kernel functions gpu softmax operations exception softmax widely used op networks efficiency cuda kernel implementation affect final training speed many networks efficient softmax cuda kernel implemented article code article introduce techniques optimizing softmax cuda kernel oneflow experimentally compare softmax operation cudnn results show oneflow‚Äôs deeply optimized softmax utilize memory bandwidth close theoretical upper limit much higher cudnn implementation
rgmgcn,0,10loc pytorch wrapper tokenlearner hey everyone recently built pytorch wrapper tokenlearner tokenfuser paper tokenlearner 8 learned tokens images videos google ai better image video understanding check fully compatible existing pytorch modules layers plug play vision transformers mentioned paper star share find useful
replf5,1,set weight basic simple 2 layer neural network basic question sub can‚Äôt find answer remember studied neural network 15 years ago basic math formula made clearly understand basic neural networks believe it‚Äôs simple exercise useful teach basic node works remember able get pencil piece paper give example simple working 2 layers neural network helped many people understand basic time alien concept learning propose exercise ask sub able create 2 layers nn 4 5 nodes layer network able recognise two different patterns example 1 1 1 0‚Äì0 0 vs 0 1 0 1 0 1 second layer able recreate pattern even input first layer mistake example input 1 1 1 1 0 0 output 1 1 1 0‚Äì0 0 forgot everything need help calculate weight layers remember basic math formula weight
radok8,1,conditional gan image generation w specified attributes hello new world please accept apologies misuse terminology right looking pointers general direction going project look research paper sample code etc large dataset labeled images widgets say image number known attributes size weight color model specify metadata collection would like train model dataset generate new images different specific attributes control tune along amount randomness make new images novel use example w faces would like able specify attributes generated face brown eyes blue hair male etc still new far reading research led things like stylegan2 3 focus totally novel images w random seeds things like dall e focus natural language processing generation unlabeled training data need something complex yet ideas pointers would super helpful thank
rwb444,1,study proofs math learning hello everyone right studying intro probability textbook first undergrad probability course eventually want learn math needed machine learning bother spending time derivations theorems like know important know machine learning algorithms formed hood talking bother stuff like derive induction inclusion exclusion formula n events x200b thank
r6lfdy,0,pytorch dev day going twitter broadcast youtube broadcast
rgdwjy,0,new datasets democratize speech recognition technology hey gradient published new datasets democratize speech recognition technology written folks mlcommons org since datasets seems like would interest folks sub tldr preview last year mlcommons org set create public datasets ease two pressing bottlenecks open source speech recognition resources first prohibitive licensing several free datasets exist sufficient size quality make models truly shine barred commercial use response created people‚Äôs speech massive english language dataset audio transcriptions full sentences see sample 1 30 000 hours speech dataset largest diverse freely available english speech recognition corpus today second datasets heavily english centric also present multilingual spoken words corpus mswc 50 language 6000 hour dataset individual words sample 2 contains random examples ‚Äúhello‚Äù multiple languages single word transcriptions useful training keyword spotting kws models ones used activate google voice assistant alexa siri dataset provides significant leap diversity available keyword spotting datasets together datasets greatly improve upon depth tps breadth mswc speech recognition resources licensed researchers entrepreneurs share adapt
r6t3fo,0,ml audio signal processing conferences journals lot discussion conferences venues core ml dl research little discussion publish ml audio signal processing advise good venues know icassp interspeech others know lot work still submitted journals signal processing due ee influence ieee transactions many
rqfx72,1,video explaining technical side algorithms hello everyone weeks exam coming machine learning inductive inference course look technical aspects learning techniques like naive bayes support vector machines theta subsumption etc mathematical concepts behind however class rather vaguely explained question good websites find videos technical details aside youtube thanks
r6b27d,0,neurips best paper awards link many awards year‚Ä¶ also noticed they‚Äôve released accepted papers datasets benchmarks track hope track becomes permanent part neurips
rim2dg,1,online course recommendations learning computer vision title context finished andrew ng ml course finished one term uni ml module know certain difficulty bracket fine id greatly appreciate beginner intermediate course recommendations guys
rb2a1f,0,discussion areas computing affected machine learning involved machine learning science research seems dominates current mainstream research development efforts computing pattern recognition computer vision data compression techniques audio video synthesis name ml really invasive blind see
rd609b,0,3d printing process parameters optimization hi dissertation make machine learning model optimize 3d printing process parameters 3d printing process probelm seem find data online 2000 records data l pbf process 4 predictor variables largely synthetic trying research actually make ml model based less data help would appreciated p know machine learning subreddit leads regarding 3d printing process parameter dataset would also appreciated thanks
ru06dy,0,coding practices job work ml engineers provide whatever need experiment train test deploy ml models gpu infrastructure distributed training support etc interface code almost always find poorly written little thought given long term stability use code 100 know going production brilliant people far smarter really good matter good enough feel limited experience happy wrong like ml engineers incentivized write poor code metric evaluation seems accuracy loss plots come research understand completely focus lies industry seen many models perform poorly code hard read refactor big issues remained unspotted months together especially befuddling field completely fine spending months get roi single digit increases model performance metrics experimentation phase seem care anything might go wrong production feels like fundamental disconnect since without core ml stuff working perfectly none stuff like value even taught hold code much higher standard critical stuff happy since write production code default weird like vending machines nuclear power plant better engineered reactor common problem localized issue facing
rhjpcu,1,question gated recurrent units mlps hello everyone question personal machine learning project working currently working features extracted cnn learned correctly classify images biological dataset goal use feature vectors learn cluster according ground truth distance matrix successfully done simple mlp uses simple mean squared error loss tries minimize distances learned features ground truth distance matrix simple regression task tries emulate distance matrix question following would appropriate use something like gru instance instead work task understand grus usually used sequence data case seem work slightly better mlps comes making predictions sure reasonable appropriate methods used learn distances feature vectors additional info trying make sense feature vectors find pattern sequences learn ground truth distances thought using grus computer scientist would appreciate advice thanks advance
qs5igm,0,lower standards code quality match research team recently joined lab working research robotics reinforcement learning research intern graduated college recently previous work experience ml engineering active research rather implementing things people already done lab months become clear standards code research lot lower expected one team pushing things like ci pipelines refactoring documentation regression tests etc raise concerns usual response things worth spending time supervisor told job researcher write good code find correct problem formulation describe correctly pass software engineer whose job create high quality implementation emphasised importance rapid iteration idea rather going slowly concerned getting scooped current project true extent small resource limited group whose experiments take hours run working popular field quadruped legged locomotion struggle lot mindset hate looking crappy code written teammates hate debugging trying understand lot time wasted things like checking experimental configuration model trained automated logging aesthetically also dislike high personal standards code quality uncle bob clean code anyone feel like constantly breaking write code work looking advice learn tolerate better similar resolutions problem thanks advance edit thank kind replies far received many responses ever expecting get going difficult reply one best read consider respond general points seen made repeatedly 1 arrogant enough think know everything research coding 3 months lab many rightly pointed need research code live standards long term production code recognize case problem likely lies mostly hence title lower standards question learn tolerate something comfortable seeing popular post sure struggle resonates many others well result hope less cynicism motives discussion far largely productive topic appreciate much staying way 2 give additional context working right part group main project work 1 person also recent graduate supervisor actually look code write far tell care much code seem implicitly trust able correctly implement things according description believe true us weekly meetings present progress form videos graphs slides means teammate solely responsible maintaining quality code current project mentioned philosophical disagreements much code quality matters know necessarily right 3 project working much one experiment main codebase inherited somebody else longer lab iteration open source implementation iterating various elements robot sensor setup addition reinforcement learning algorithm training curriculum well known shelf envs like mujoco pretty happy accepting working intended code write code written people tested lot harder confidence 4 correctly pointed using standard logging tool like w b would easy fix time spent resolving experimental configurations issue particular could resolved easily setting appropriate pipelines would likely net plus team generally could benefit critically examining practices used seeing concrete benefits current workplace propose team 5 strong proponent unit tests unit tests save butt making simple mistakes already incredibly difficult figure experiments fail sometimes least eliminate possible causes unit testing code easy appropriate also fix bug write unit test enforce fix mostly forget mental real estate go things automated easily 6 similarly big fan simple ci pipelines like github actions perhaps term ci comes lot associated bells whistles rightfully used research code ci simply way automate manual running unit tests bit know github ci workflows easy set cookiecutter template save lot time well mental worry like include linters well necessary core functionality
rrlgt6,1,best resource learning reinforcement learning anyone suggest resources learn reinforcement learning preferably python examples
rrh38q,1,sample size justification ml position need write section sample size calculations ml study targeting medical journal basically want justification size final test set thing done anything approximating sample size calculations field task involves using deep learning algorithm predict angle measurement image continuous comparing prediction angles manually measured experts final metric targeting icc prediction expert raters produce metric hold test set images question sample calculations really apply trying really prove effect size thing attempting show whether predicted angle agrees expert measures
r7gt1m,0,data platform generator data solution blueprints hi everyone took inspiration matt turck‚Äôs data ai landscape scott brinker martech landscape generate visual data solution blueprints inspire design process data platform help find alternative combinations data technologies idea simple let take technologies landscapes generate simple visual data solution blueprints inspire data aficionados design process data platform also expose different technologies exist market could good alternatives already better known technologies create generator display one picture time millions combinations discover used first time code applications like bubble airtable launched page today concept site call picture data solution data aficionados fun inspired hope enjoy forget share friends like also appreciate brutal honest feedback
rnpl4x,1,object detection using rcnn hello guys object detection without using pre trained models difficult understand neural architecture rcnn requires pre trained cnn trained cnn network cifar 10 understand plug rcnn architecture guys somebody guide explanation link reference thanks lot reading helping
rhcwxh,1,multivariate grouped time series classification options hi looking ideas handle multivariate time series classification problem specifically dealing many hundreds time series varying length within ballpark let say 30 difference longest shortest features behave similarly end time series binary outcome time series around 5000 observations 32 features challenge want able pass live data model say halfway time series classify likely positive outcome material seen relates forecasting single time series learning characteristics many historical individual time series classify new one midway right approach handle observation independent manual feature engineering use grouped cross validation avoid horribly overfitting fact training multiple instances time series thus outcome using lgbm gives acceptable even good results sure best approach results work lot variance prediction across time series ideally uncertainty start clearer picture starts form right lot jumping around probability score familiar time series modeling imagine deep learning could good approach lstm something like experienced dl either also looking sktime see anything would work box suggestions reading material approaches could use help appreciated cheers
rec0nk,0,karpathy consolidation field i‚Äôve definitely noticed trend papers i‚Äôm seeing year people combining building blocks creative ways wonder far trend go unlock
qqzpf6,0,list iclr 2022 papers review scores iclr 2022 reviews publicly available compiled list papers sorted review scores weighted confidence link mean review score 4 9
qx42a5,0,anyone working explainability foundation models recently researchers stanford came term foundation models huge transformer models claim able solve number natural language even logical problems wondering whether anyone working explainability transformer models thanks
r6nzv3,0,animegan videos would anime look like using gradio demo find recording video webcam applies animeganv2 across frames recorded video stitches back together lets check results try hugging face spaces ü§óüöÄ üëâ demo üëâ announcement thread
ruyjmn,1,purpose cyclic layers network hi currently tensorflow advanced techniques specialization coursera instructor mentions custom resnet architecture actually make make layer cyclic particularly weights illustrated code x200b range 1 4 x self block2 x block2 defined residual layer isnt basically cycling input multiple times layer purpose
rrsjbi,1,list statistics topics need machine learning hello everyone endlessly googling get definite answer someone please tell list topics statistics know get started machine learning statistics learned one learns machine learning one specifically learn stats topics moving onto next phase learning ml thank much
rirflx,1,machine learning book behavior analysis r book interested learning machine learning concepts using r programming language book focuses behavior analysis concepts applied field complete free version book available x200b
r7x490,1,always necessary retrain model entire dataset elaborate question give example let say dataset consisting 1000 datapoints trained model data deploy starts making predictions every week however gather data lets say 250 new datapoints every week improve model want model learn new points necessary retrain model 1250 datapoints someway keep original learning additionally learn 250 points imagine smaller dataset really matter get much bigger datasets costly process retrain model data
ru66do,1,improve performance multi class hi classifying traffic using cic 2020 darknet data set 8 classes data set tried lot different ways sampling sampling complex sampling outlier elimination feature selection model change cat boost rus boost decision tree random forest ovo ovr svm etc however performance improve picture selected features using rfecv used light gbm best performance x200b validation x200b test good way improve performance multi class classification please help
r27jbb,1,use data augmentation online offline hi use data augmentation image based tasks online offline mean augment images time training augment training use final augmented dataset training model one experimenting seems like approaches advantages disadvantages would love hear take especially working projects industry
qzvh6i,0,design patterns actually fairly used production ml hi wondering design patterns used code producation grade ml like wondering maybe going factory pattern builder pattern instantiating model objects might good idea theres also strategy pattern think uses overall well versed patterns production ml experience regard
r91vk3,1,methods like resume chronotron span train single layer spiking neural networks many papers discuss mention primarily suitable training single layer spiking neural networks however clear case use local learning rules
reohjf,1,backpropagation beginners warning faint hearted beginners complain find posts videos backpropagation beginners sometimes idea backpropagation works tutorials comes backpropagation equations shown beginners tutorials directly jump keras pytorch etc time tutorials documentation keras pytorch posts tried explain backpropagation simple language neat codes building neural network backpropagation part 1 backpropagation part 2 posts helpful think helpful others please share join youtube neuralthreads reddit
r7vrf6,0,discussion dot product vs addition followed transformation attention networks consider set vectors v quad 1 k set queries q j quad j 1 q two ways finding attention alpha ij value v 1 addition keys queries bringing appropriate dimension linear projection scalar non linear activation e ji v attn tanh u attn key v w attn q j alpha ji softmax e ji 2 dot product keys queries linear transform shape gives one scalar value pair j simply take softmax along dimension values sum k alpha ji 1 query q j method appropriate usecase general specifically want set fixed length vector conversion number elements set variable thanks lot
r8sfwj,0,impact initialization gans hi wondering important initialization gans heard normal init leads better convergence cannot find paper support claim guys resources topic practical advice x200b thanks
qlwcgx,0,twitter cortex proposes lmsoc socially sensitive pretraining new paper lmsoc approach socially sensitive pretraining twitter cortex research team proposes simple effective approach learning linguistically contextualized socially sensitive representations large scale language models quick read twitter cortex proposes lmsoc socially sensitive pretraining lmsoc code available project‚Äôs github paper lmsoc approach socially sensitive pretraining arxiv
roulqh,0,apple ai residency 2022 hi starting thread everyone applied apple ai residency 2022 applications closed 15th december anyone heard online coding test thank wishing everyone best luck applications
rwumjj,0,vq vae heuristics number embeddings embedding dimension hi r machinelearning anyone experience training vq vaes know good rules thumb embedding size e g given data dimension n use embeddings size p thanks help
rd6vlv,1,best way unet generate masks different image sizes dealing dataset different size images also predict masks different sizes would like automatic way dealing distort prediction downscale images smallest resolution appears dataset problem backtransformation bigger resolutions mask gets distorted bigger images also bigger ones mostly squared problem model uses keras input layer allow dynamic inputs think also output hope anyone help
qq6jgo,0,virtual mlops round table putting together mlops roundtable focused peer learning ml mlops practitioners share utilize mlops automate scale ml processes learn teams structure works like break people small groups 5 7 people based team size working absolutely selling pitching focus pure peer learning sign interested let know ideas thoughts feedback
qojz5g,0,comparing deep models different complexity different accuracies working deep learning based system complexity reduced minimal however impact accuracy ask question somewhat hypothetical scenario hope clear statement say one system uses 1 billion flops network achieve 99 accuracy yet another uses 0 5 billion flops network give 95 equal accuracy could say system lower flops better likewise equal flops would able say first system better direct way like standard kpi compare two systems given scenario short compare two systems different accuracies different flops
rfb7c3,0,long running forex trading machine learning models companies basically already stood test time robust models trading also thoughts forex trading using machine learning long term usage
rahzmf,1,optimize gradient boosting model hi ml noob sorry stupid question machine learning exercise needed train models make decision best model predicting defaults bank loans 5 models tested gradient boosting model came top accuracy 0 66 sensitivity 0 68 specificity 0 60 need optimize model even quite loss exactly due inbalanced dataset decided sensitivity metric focus best way hyperparameter tuning get model working even better x200b thanks advance replies help
rlhwve,1,know python ml code seems meaningless trying learn ml long time know python math every time started course youtube udemy introduction ml tensors things get really confusing like loading data training model understand going recently watching statquest really liked ml videos wanna projects need coding
qv0jg1,0,getting top4 stanford cmu mit berkeley ml phd program requires 3 first author papers top conferences reading post one comments mentioned 3 first author ml papers top conferences 1 spotlight oral paper co organized conference workshop met pis professors applying masters double bachelors major cs statistics math top 20 school candidate satisfy least 3 4 criteria get top 4 ml phd programs true really hard mean person satisfies 4 aforementioned criteria get phd right away also much harder international students compared us students given international student graduated american university
rez90o,0,machine learning wayr reading week 127 place share machine learning research papers journals articles reading week relates researching means elaborate give us insight otherwise could interesting paper read please try provide insight understanding please post things present wiki preferably link arxiv page pdf easily access pdf summary page way around pertinent links previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 121 130 week 1 11 21 31 41 51 61 71 81 91 101 111 121 week 2 12 22 32 42 52 62 72 82 92 102 112 122 week 3 13 23 33 43 53 63 73 83 93 103 113 123 week 4 14 24 34 44 54 64 74 84 94 104 114 124 week 5 15 25 35 45 55 65 75 85 95 105 115 125 week 6 16 26 36 46 56 66 76 86 96 106 116 126 week 7 17 27 37 47 57 67 77 87 97 107 117 week 8 18 28 38 48 58 68 78 88 98 108 118 week 9 19 29 39 49 59 69 79 89 99 109 119 week 10 20 30 40 50 60 70 80 90 100 110 120 upvoted papers two weeks ago u catalyzex code bot paper link u pm profanity besides rules fun
rhgoj3,1,working speech text transcriber projects tutorials tips hi guys working simple speech text transcriber using tensorflow python want look similar projects done earlier research hope achieve 1 imagine video stream input video file want transcribe speaker says text based output 2 want develop model train 3 get text based output 4 furthermore develop model work better scale thanks advance
rlu1bk,1,beginner need advice review pytorch project hi guys new channel python machine learning watched tons videos read stuff ml decided want make tetris ai problem started coding tetris agent model python used pytorch seemed like good choice beginners bot starting pretty strong feels like fixing single decisions later runs e g holding right left spinning permanently etc repo code ignore readme please literally help much appreciated want know whether something completely wrong want learn mistakes greetings thanks advance
rblylk,1,optimizie keras model two losses one losses losses threshold keras model predicting time series want optimize respect two losses one loss want get close certain threshold value one want minimize first loss mae second loss mean error times time series read stackoverflow question quite close question except entirely sure use time series input parameter loss function help much appreciated
rxe28v,0,machine learning engineering conferences wondering anyone could recommend conferences suitable mle‚Äôs maybe focus deploying models i‚Äôve traditional data science focused one anaconda one though wanted see existed closely connected deployment
rh7mf4,1,performance compute power local machine matter 1st year grad student ml 1st term realized main stuff using laptop ml lot web browser tabs lot visual studio code since code running remote gpu also considering getting new laptop desktop wondering step deeper ml planning get phd expected use heavier compute demanding stuff local machine still web broswer ide performance compute power local machine matter
qzsgma,0,hard time finding dataset project need dataset segments images colours classifying whole image one colour hard time finding one especially true need shadows anti shadows light sources also labelled dataset posting go create dataset help appreciated
ril4sy,1,gpt 3 libraries anybody made full fledged library functions powered gpt 3 spacy use gpt 3 methods example thanks
rp3926,1,vehicle detection counting intelligent vehicle detection counting becoming increasingly important field highway management however due different sizes vehicles detection remains challenge directly affects accuracy vehicle counts simple project treats issue using opencv image processing hear cascade used object detection
ruudv3,1,save augmented images hi i‚Äôm currently using data augmentation training model code use augmentation def augment image image label flips image randomly image tf image random flip left right image image tf image random flip image increase image size randomly crop original dimensions resize factor random uniform 1 1 2 new height math floor resize factor input shape 0 new width math floor resize factor input shape 1 image tf image resize crop pad image new height new width image tf image random crop image size input shape image tfa image translate image hshift tf random uniform shape minval 1 maxval 1 vshift tf random uniform shape minval 1 maxval 1 interpolation nearest fill mode nearest vary brightness image image tf image random brightness image max delta 0 2 image tfa image rotate image max rot angle tf random uniform dtype tf float32 interpolation nearest fill mode nearest return image label train dataset train dataset map augment image tf data experimental autotune want visualize data augmentation going i‚Äôm used imagedatagenerator method augmentation option save dir flow api flow directory api currently use method augmentation scenario way save augmented images using method thanks regards ramson jehu k
rflmju,1,best shelf model use question answering internal documents serverless architecture hi many gbs internal documents need build search tool currently using haystack nlp library task would like know another library tools exists make haystack model serverless architecture would like something demand vs turned time computational resource perspective thank advance
rrwjyk,1,training u net model pytorch hi guys trying implement unet model pytorch done tensorflow works fine pytorch implementation doesnt seem work check model train fn model py train fn using nn crossentropyloss loss function error code results pretty bad model kernel size number layers keras works fine
rr17f9,0,4 5 times faster hugging face transformer inference modifying python ast recently ü§ó hugging face people released commercial product called infinity perform inference high performance aka fast compared pytorch fastapi deployment unfortunately it‚Äôs paid product costing 20k one model deployed single machine info price scaling publicly available according product director transformer deploy open source alternative build enterprise grade softwares inference server nvidia triton takes queries passes engine plus adds features useful inference like dynamic batching multi inference engine dispatching inference engines microsoft onnx runtime cpu gpu inference nvidia tensorrt gpu appears without much effort easy match hf infinity public benchmarks still opportunity push inference performances afaik yet leveraged oss project gpu quantization transformer models please find measures roberta base seq len 256 batch 32 mnli dataset classification source code repo performing gpu quantization requires modifying model source code add specific nodes called qdq costly operations like matrix multiplication error prone boring good generator technical debts maintain source code modified model done work manually several models appeared us made automatically patching model module abstract syntax tree aka source code user end performing basic quantization model gpu looks like shown benchmark get model 4 5 times faster vanilla pytorch costs 0 4 accuracy point mnli dataset many cases reasonable tradeoff it‚Äôs also possible lose accuracy speedup around 3 2 faster course exact trade depends model dataset etc gives basic idea it‚Äôs big improvement compared previous version project speedup costing 1 accuracy point behind scene transformer source code parsed ast operators like matmul layernorm wrapped quantizer linear layers replaced quantized versions tensorrt unsupported operators replaced etc new source code replaces ram original one right successfully tested process albert bert including minilm distilbert roberta including camembert xlm r distilroberta etc electra work box little effort transformer model exported onnx format regarding cpu inference quantization easy supported transformer deploy however performance transformer low outside corner cases like batch short sequence distilled model last intel generation cpu based instance like c6 m6 aws quite expensive compared cheap gpu like nvidia t4 say otherwise transformer ok slow inference takes small instance poc instance cpu inference probably good idea
qs98gv,0,create semantic search applications machine learning workflows x200b create semantic search applications machine learning workflows demo shows various nlp pipelines connected together build semantic search application txtai executes machine learning workflows transform data build ai powered semantic search applications txtai support processing unstructured structured data structured tabular data grouped rows columns spreadsheet api call returns json xml even list key value pairs example workflows summarize news articles summarize translate research papers load index data via csv schedule recurring job query api index results semantic search references live demo github article notebook notebook
qrnozu,0,much vram ram need nlp transformer models phd student looking new desktop current personal pc amd gpu train pre trained bert model using cpu takes forever assume using ram online often read transformer models using vram question training transformer models exclusively use vram also need sufficient ram much would needed minimum work models aware google collab suitable work
qpbci5,0,hubert apply bert speech visually explained recently facebook ai released hubert bert like model learning powerful speech representations first glance model looks similar wav2vec 2 0 training process objective actually different made detailed illustrations visually explain pre training process hubert compares wav2vec 2 0 models already available huggingface transformers library hope helpful
qwn3fz,0,dagyard dvc x mlflow x colab x gdrive automatically configured hey r ml dvc mlflow tracking two preferred open source tools managing ml projects heavily used data scientists widely adopted industry great community team behind familiar nutshell dvc data version control acts extension git enables version control large scale files mlflow tracking automates logging process experiments sends live information local remote server training still running aspiration dagshub help data scientists use great open source tools lowering entry bar recent project dagyard notebook automates process configuring google colab dvc mlflow simply checking boxes filling user details well set clone repo train model colab fully configured dagshub dvc mllflow based use case also github google drive top spiced additional capabilities like using google drive dvc cache directory avoid pulling file twice copy files gdrive dvc storage initialize dvc clone specific branch check üëâ ideas improve notebook additional capabilities missing love hear
rvzum7,1,much data need collect create reasonably accurate model really need millions lines hello working project take inputs number biometric sensors heart rate galvanic skin response emg order try detect user close peak sexual arousal orgasm even convinced actually need ml good exercise learning fun well going give go unsurprisingly struggling find existing model kind data prudish society live eh going create model thoughts bring user orgasm via various means record metrics every 5 seconds go boolean whether orgasm reached data recorded csv file think able create model use python pandas problem see even manage keep stimulation orgasm going 30 minutes still 360 data points one data points true orgasm column probably need repeat process dozens times get anything significant far data means close orgasm concerned love know folks recommend around approach go back using load nested statements way get ml improve detection side things time runs based data flows need dataset thousands rows create model project simply big beginner bite got background hardware software engineering worried learning write code generation models struggling thanks advance
rmcp62,0,convolution lead translation equivariance asked similiar question understanding translational equivariance one main properties allows convolution powerful finding patterns images however trouble finding concrete proofs happens anyone papers explains convolution leads translation equivariance preferably linear algebraic perspective secondly anyone papers explains translational equivariance allows convolutional neural networks successful image recognition explains idea translation equivariance relates happening layer thanks
qrvasc,0,handling bound constraints cma es hi apologies question bit naive passing familiarity ml full stack developer trade really looking insight implementation consequences bound constraints cma es optimization come across biedrzycki 2019 discusses topic lot paper findings least come across fairly natural finding lot wondering whether anybody else recommendations reading personal stories potential pitfalls problem dealing specifically coordinates mutant search point vectors need bound within unit interval wanted make sure walking easily preventable mistakes background context currently working personal project inspired popular geijtenbeek et al 2013 paper implemented hill type muscle model rigged humanoid model 200 muscles approximating human skeletal muscle function beginning muscle control experiments inspiration paper wochner et al 2020 except plugging cma es instead bayesian optimization used paper utilizing objective function insights
rfssk6,0,transformers positional embeddings added tokens rather multiplied element wise think token vector describing input word patch etc element wise multiplication positional embedding token would change token relative fashion whereas addition absolutely intuitively would thought relative modulation makes sense thinking addition context
rr4y9y,1,overfitting pass unnoticed small dataset let say small dataset trained tested model training set test set accuracies high similar cross validation accuracy also similar low variance possible overfit dataset small
rjybto,1,regularization techniques beginners l1 l2 penalties dropout layer normalization hope posts helpful someone wants understand regularization techniques work l1 l2 regularization dropout layer normalization part 1 layer normalization part 2 join youtube reddit
rs5cjb,1,introduction machine learning beginners hello group students made introduction machine learning people doesnt know anything teach basics 4 models full explained english lots comments used test online know potential ml also videos spanish explain everything machine learning dangers 4 models provided uses fields like medicine social networks hope find useful encourages enter world
r4rxry,1,looking teammates hi everyone looking teammates participate kaggle competition please dm interested
qwmizo,0,benchmarking deep learning m1 pro gpu metal vs colab gpu tesla p80 kaggle p100 hey r machinelearning someone like wondered m1 pro new tensorflow pluggabledevice metal performs model training compared free gpus made quick comparison nutshell m1 pro 2x faster p80 p100 2x faster m1 pro equal m1 max however transformers seems good optimized apple silicon x200b p pytorch gpu support way
qryaz4,0,p alphafold 2 1 1 without docker want fold monomers multimers using alphafold2 without need docker look fork deepmind alphafold repository removes docker dependencies enjoy alphafold docker
rk9s3c,1,linear regression model best use case type ok data set 200k lines data test data set around 100k lines data gathered later time data three columns might expand future add 4th timestamp column target column column think called feature column ml talk holds data target column mostly dependent csv header line simply datetime usd unit price unit visualized data graphs see price unit values similar movement usd unit values slightly delayed time intervals stock data kind market prediction stuff simply educational data set clear though price units directly linked usd unit time obviously important target column directly impacted usd unit trying build model give say last 30 data points usd unit price unit values predict point price unit trails usd unit movement catch usd unit done reading looks like linear regression model need simple multi something ones talk smoothing weighted averages stuff really understand architect devops engineer machine learning expert mathematician looked examples people type analysis goes head gather python language choice seems 17 different modeling packages frameworks reason everyone starts letting draw freaking graph matplotlib already excel would great someone could guide towards goal please point linear regression beginners tutorial probably already saw either went head work someone willing take time explain heck need point tutorial example actually explain plain simple techie mathematician like linux linked differential equations oh english second language english greatly appreciate sorry bit frustrated partly resources mostly understanding something lot folks say hard swear dumb thanks advance
qmzy8a,0,rudall e model open source sberbank submitted open source rudall e model inspired openai dall¬∑e russian model 1 3 billion parameters available apache 2 0 license pipeline includes image generation ranging results ruclip super resolution large model 12 billion parameters available cloud rudall e biggest neural network project history russia taking 20 000 gpu days nvidia v100 train github model demo russian x200b pictures generated cherry pick authors avocado style malevich cat looks food anime chan trump hides pain mystery forest salvador dali picture beautiful chan pepe frog grand canyon
r9hir9,1,latent space walk custom stylegan2 model hi guys started working ai weeks ago going well far dont coding experience pretty hard sometimes understand going github posts colab notebooks managed train custom model stylegan2 colab trained ai around 6k pictures 3 days want create interpolation video custom model kinda stuck point find proper tutorial easy use colab notebook generate video anyone explain generate video cutsom model functional colab notebook guidance found notebook generates interpolation video anime faces pretrained model believe notebook could work custom model manage edit code properly grant 1000years thankfullness every help 3
ris3un,1,grouped bagging boosting classifiers hey wondering possible classifier relies bagging boosting like random forest lgbm xgboost keep groups observations together bagging example let say observations bunch people throughout teenage years early adulthood want predict whether eventually get married train model observations sample bag may get observations person different point life thus outcome leaking information overfitting data points unique person remain relatively constant problem gets even worse deal far solution reduce feature set keep generic features avoid overfitting know best solution
radt2v,1,pick high level dl api 2021 years resisting push move keras decided time find new high level deep learning api late looking best alternatives haiku flax caught attention found weird though google would libraries similar approaches anyone experience working one take consideration picking one addition aforementioned couple elegy also looks really interesting must mention though reason stayed keras long much material learning many good tutorials examples past projects initial impression newer promising frameworks still lacking front maybe change level abstraction develop everything pytorch guys think frameworks currently use dl
qoqp4w,0,discussion mlops tool image data management exploration hi x200b large part current work developers dedicated manual work data done several stages development 1 beginning get annotated data go thousands images segmentation tasks observe annotated images validate annotation correctness images relevance tasks images may correctly annotated relevant task use case remove 2 check statistics data validate data biased example enough samples class distribution instances sizes e g number pixels segmentation mask within classes reasonable 3 trained model go images model gives poor results try find similarities understand weaknesses model find root causes often leads change data repeating stages 1 2 eventually 3 time consuming tasks looking mlops tools make work efficient optimally tool also take care mlops aspects like experiments orchestration experiment tracking data versioning also combine several tools using one tools preferred open source proprietary paid product possible recommendations thanks
qrbkc7,0,calling authors trajformer paper claiming published code never read paper neurips 2020 titled trajformer trajectory prediction local self attentive contexts autonomous driving found interesting authors claim multiple times paper release code turns never fine thought perhaps future starred repo check later many others raised issues asking update code release never replied finally april update readme say release code last update know common trend ml papers sucks emailed authors grad student pi multiple times asking update never replied paper literally based empirical improvements without working code replicate results word mine strongly think things change believe call waited long enough made significant effort contact authors response mean mind releasing code least claim paper review phase disappear undergrad lab asked take time clean code document release others move next interesting project answer
rjqgrc,0,one many mapping using probabilistic unet hi working project month however couldn‚Äôt make work yet need help basically applying probabilistic unet paper leaf segmentation dataset given image want output one instance time sampling distribution probabilistic unet architecture shown x200b x200b architecture two loss components kl divergence reconstruction loss train kl loss goes zero however reconstruction loss works differently want want 1 sample noise 2 get one instance x200b x200b actually happens noise ignored acts like semantic segmentation basically rgb image different instances mapped latent space tried beta scheduling kl even removed kl term noise still ignored x200b x200b x200b someone kind enough review code explain approach might work thank
re2787,1,personal ml models hey looking best practices training targeted personal models let say 1000 customers using product plan training 1000 individual models make best possible recommendation customer individually go training get best possible outcome first train data 999 customers retrain target customer lower learning rate freeze last couple layers tricks working time series model predicting user wants go next web page medium amount data 50 000 events customer order page visits length time since previous action quick sample 1 customerid pagename seconds 2 123 dashboard 0 3 123 detailspage1 5 9 4 123 detailspage2 3 1 5 123 action5 21 2 learning material best practices keywords help search highly appreciated thanks yall
qwzhbb,0,colab tpu low performance wanted make quick performance comparison gpu tesla k80 tpu v2 8 available google colab pytorch quickly used mnist example pytorch lightning trains simple cnn reason performance tpu even worse cpu gpu 52 tpu 9 cpu 13 colab notebook results legit maybe small conv kernels suitable tpu issue pytorch xla lightning suggestions
royf9n,1,bit confused grocery purchase recommendation approach got assignment make grocery purchase recommendation system maximize sales recommending items popular sellers already cart bit research found problem lies market basket analysis apriori algorithm top recommendation reading paper amazon item item collaborative filtering occurred apriori approach would able suggest items item pairings support level keep support low would go apriori key goal keeping search terms less process faster go approach makes list item popular pairings ensure code least recommend related sale item customer faster time space efficient implementation problem insight related would appreciated
qwsmnn,0,intel‚Äôs prune compression method achieves sota compression accuracy results bert intel research team presents prune prune ofa training method leverages weight pruning model distillation produce pretrained transformer based language models high sparsity ratios applied bert approach achieves state art results compression accuracy ratio quick read intel‚Äôs prune compression method achieves sota compression accuracy results bert paper prune sparse pre trained language models accepted poster session neurips 2021 december 6 14 arxiv
r7ki6k,1,fix multioutput target data supported label binarization error hello currently trying figure use scikit learn try use mlpclassifier spits error multioutput target data supported label binarization really sure means people asked stackoverflow seem get different reasons x train ndarray columns float values except id column simply integers train also ndarray integers columns id class classes ranges 1 10 would really appreciate pointers
r6t6u1,0,compare models reporting standard deviations enough also report significance testing many papers report standard deviations bother reporting significance tests countless papers search example 3k citations article show reporting standard deviations sufficient thanks
rljk7d,1,joining project hi cs ml expert projects would like join team would like work
rgbbc5,0,spring 21 reproducibility challenge results support fall 21 edition tl dr announced support reproducibility challenge awarding 500 per paper reproduced announcing award winners spring 21 edition well support fall 21 edition challenge check awesome papers üëá hey r ml dean dagshub back announced support papers code ml reproducibility challenge award participants 500 per paper reproduced according guidelines align incentives put money mouth today really happy share teams given award projects worked ‚Äì read full blog honestly think full read interesting worth time highlights papers 1 contextual decomposition explanation penalization cdep ‚Äì original paper proposes method reduce chance models learning spurious correlations instead actually important features team reproduced implemented original project tensorflow rewriting functions completely scratch along way made contribution tensorflow addons repo 2 self supervision shot learning ‚Äì name suggests paper tests importance self supervised learning shot learning contexts team reproduced explored different input configurations one proposed article found significantly affects performance 3 ganspace discovering interpretable gan controls ‚Äì proposed method use simple pca create controls gans humanly interpretable computationally efficient team implemented original implementation tensorflow trained model benchmark datasets lot cool examples method report thank everyone took part challenge none could possible without learned lot process next ‚Äì well decided continue support fall 2021 edition reproducibility challenge want host reproduced papers since makes ml field better everyone want take part move field forward reproducibility front check guidelines information take part
rh3hyx,1,recommendations multi terabyte time series machine learning project cleared hurdle dig deeper project wanted learn time series data analysis agriculture mining company data set obviously impacts stock market indicators also taking account accurate mapping predictor variable customers buy truckloads ink spilled want look data done large amount data also going back records 1700 needless say lack data would start end goal understand buying patterns calculated chunks time process trying find isolate methods apply identify time series trends many techniques choose feeling like boiling ocean proxy gpu time ml books reading back older general focus 1 books papers know 2 project discover using methods 3 thinking outside box recommend
r47kvq,0,google research open sources ‚Äòsavi‚Äô object centric architecture extends slot attention mechanism videos multiple distinct things act compositional building blocks processed independently recombined humans‚Äô understanding world foundation high level cognitive abilities like language causal reasoning arithmetic planning compositional model universe therefore it‚Äôs essential generalizing predictable systematic ways machine learning algorithms object centric representations potential dramatically improve sampling efficiency resilience generalization new problems interpretability unsupervised multi object representation learning widely used various applications algorithms learn separate represent objects statistical structure data alone without requirement supervision using object centric inductive biases despite promising outcomes approaches currently constrained two major issues 1 limited toy data moving 2d sprites extremely rudimentary 3d scenes struggle realistic data complex textures 2 training inference clear interact models concept object imprecise task dependent models‚Äô segmentation always correspond tasks interest overcome problem unsupervised weakly supervised multi object segmentation tracking video data new google research introduces sequential extension slot attention called slot attention video savi check github paper read short summary x200b
qp9mnn,0,unified view relational deep learning polypharmacy side effect combination synergy drug drug interaction prediction x200b git paper abstract recent years numerous machine learning models attempt solve polypharmacy side effect identification drug drug interaction prediction combination therapy design tasks proposed present unified theoretical view relational machine learning models address tasks provide fundamental definitions compare existing model architectures discuss performance metrics datasets evaluation protocols addition emphasize possible high impact applications important future research directions domain paper provides unified model drug pair scoring models general architecture design recipe model design comparisons based architecture input modalities evaluation metrics used important papers public datasets relevant evaluation regime designs stratified splits github repo comes paper links implementations links datasets
rofn2t,1,looking direction project like capstone project capstone project semester would like project neural representations interested differential geometry side math recently learning manifolds used ml representations going project using manifolds time series data project using time series data like audio signals eeg meg brain signals going see possible project professor would able help far know seen professors school specialize neural representations using manifolds initial guidance get started something like would extremely helpful example talking paper video neural circuits constructing representations like something like really know start learn quite bit semester would like get basics direction kind machine learning need semester starts least project complex paper something least direction want would anyone willing provide guidance work towards building project like semester advice start learning area resources would give framework start kind ml help would greatly appreciated
rvqyji,1,classifiers greyed weka data trying classify using weka environment classifiers interested greyed know means dataset using compatible weka trying find case try dataset first 200 instances algorithms show point 200 instances 2k instances algorithm stops showing link sample data arff format anyone help spot wrong
rm34a3,1,andrew ng ml course fast ai heard lot people recommend andrew ng ml course others recommend fast ai know fast ai practical nothing theoretical hand andrew ng theoretical less practical new ml journey confused want choose two choose beneficial resources know please share also
rwp07l,1,tflite conversion keras model gives low accuracy hi converting two keras models full integer quantize tflite model one tflite model gives high accuracy 98 another one gives low accuracy 0 4 checked quantized model found scaling factor change significantly come quantization parameters model 98 accuracy tensor index 1 quantization parameters scales 0 0077935 zero points 1 tensor index 2 quantization parameters scales 0 02159962 zero points 0 tensor index 3 quantization parameters scales 0 00016834 zero points 0 come quantization parameters model 0 4 accuracy tensor index 1 quantization parameters scales 0 00770933 zero points 3 tensor index 2 quantization parameters scales 0 00948102 zero points 0 tensor index 3 quantization parameters scales 7 3092306e 05 zero points 0 see scales changed significantly model low accuracy scaling value makes difference model accuracy
qu7dgv,0,use valid data incremental learning incremental learning dataset split multiple segments model trained segment model trained large dataset using limited ram segment train valid test ok reuse valid data training data next segment test data
ruqaoc,1,tensorflow keras implementation vision transformer tensorflow keras implementation image worth 16x16 words vit excellent results compared sota cnns requiring fewer computational resources train paper code
r2o0mu,1,help withml class project hello i‚Äôm currently conducting research study class project i‚Äôm examining employees‚Äô attitudes toward telecommuting e work home use machine learning predict survey take approximately 5 10 minutes complete time spare please help fill google form survey identifiable information collected also know others might interested taking survey please feel free forward survey link thank
rnadmh,1,decoding openimages v6 mask coordinates hi new ml using v6 semantic segmentation segmentations csv mask coordinates header clicks set three coordinates first two coordinates pixel values x last coordinate 0 1 anyone please explain last coordinate stand thanks
ruvgva,1,üíäyour daily dose machine learning opencv series posts post almost daily call ‚Äúyour daily dose machine learning‚Äù you‚Äôve worked image processing projects probably heard opencv it‚Äôs cool framework sorts things related computer vision things opencv include basic image processing techniques applying filters blurring edge enhancement ‚Ä¶ depth estimation using stereo cameras extracting features computing descriptors images classical machine learning linear regression svm bag visual words deep learning image classification object detection although opencv machine learning framework still offers variety tools build powerful ml applications opencv replace tensorflow codebase really you‚Äôre already using opencv codebase it‚Äôs worth taking look it‚Äôs ml capabilities might surprised i‚Äôll share insights opencv upcoming days stay tuned connect favorite social network
rjgbxc,0,memory efficient attention self attention need n 2 memory project unofficial implementation self attention need n 2 memory jax pytorch implementation almost one proposed paper additional masking adding bias compatibility batch dimensions support pytorch implementation computing attention proposed method requires sqrt n memory provided functions used drop replacement attention calculation github
rqfcx3,1,isolation forest question agree outlier fraction priori formulation parameter main drawback iso forest algorithm demands presice knowledge anomaly appearances
qkyini,0,machine learning engineers facebook approached interviewer position hard time grasping work day day basis thanks advance
rj4r42,1,still unfamiliar unsupervised learning clustering questions still need one hot encoding categorical features clustering like supervised classification mixed numerical categorical features still need train test split like supervised learning di still need deal imbalanced data using python btw
r5x0v2,0,ama david bau study structure complex computations learned within deep neural networks david bau believes members subreddit would interested ama note discussion questions answers linked thread one
rur2j3,0,like yolov5 code complexity like yolov5 code complexity deny yolov5 practical open source object detection pipeline however pain begins adding new features new experimental methods code dependencies hard follow makes code difficult maintain wanted try various experimental methods hate write one time code never used worked making object detection pipeline better code structure could continuously improve add new features easy maintain applied ci formating linting unittest ensure code quality docker support development inference docker supports development environment vim code design beginning try various experimental methods fewer efforts features far developed follows 1 easily use trained model another project without code copy paste pytorch requires model code use model build model library builds pytorch model yaml file trained model portable pip install kindle 2 model compression support tensor decomposition pruning 3 export model torchscript onnx tensorrt 4 inference torchscript tensorrt 5 wip c inference torchscript tensorrt 6 auto search nms parameter 7 wip knowledge distillation support 8 wip representation learning support ayolov2 also supports w b model upload load function make trained models easy manage python3 val py weights j marple ayolov2 179awdd1 instance single command line download trained model w b run inference time read might wonder name ayolov2 ayolov2 comes auto yolo v2 initial goal implement auto model architecture search v2 represents v1 v1 go built auto model architecture search based original yolov5 worked pretty nice became unmanageable please stay tuned nas feature coming soon suggestions feedback kind appreciated thank happy new year
rbu0e7,0,visual guide prompt engineering gpt language models hi r machinelearning rise gpt language models making lot people start experiment using text generation general problem solving language tasks guide help get people started thinking prompts hope find useful feedback appreciated x200b
r2bbvl,0,causal machine learning hearing term lot recently advisor supervisor asked look dont get idea isnt causality supposed impact effect actions taken given time impact current decision exactly rl nlp understand new field defined causal machine learning also implications research area theoretical see lot language vision rl models apparently performing well already need causality hype really important concept x200b sorry questions basic simple dont understand causality yet thanks advance
rdv2xw,0,releasing three pokemon reinforcement learning ai tools including computer vision program play pokemon sword autonomously nintendo switch video proof source code available hullo tempest storm background building pokemon ai tools years get researchers news media cover research dumping bunch likely future bots play pokemon shining pearl autonomously using computer vision reason people think lying dump put doubts rest get code current work title pok√©mon ai shining pearl random ai wild battle honorablesaladai talk source code available topics covered 1 pokemon shining pearl random ai demo 2 patent autonomous navigation explanation 3 honorable salad ai model overview 4 pokemon sword random ai demo old videos 2019 2020 let start video proof videos marked two years old showing progression work computer vision building pokemon bots videos formerly private made public recently repos keep mind date version sword capture tool version repo mar 2020 made many changes since update files sake making runnable people tool 1 mock environment pokemon used practice making machine learning models tool 2 transformed pokemon showdown simulator environment could train pokemon ai bots reinforcement learning tool 3 pokemon sword replay capture tool video guide repo presentation working presentation video record end week sent slides powerpoint pro make look nice see draft version interested computer vision create yolov3 custom object detector course conclusion presentation journey bring ai bots nintendo switch hopefully sometime next weekend learn repos
rrlenc,1,pca running naive bayes achieving conditional independence features need independent uncorrelated would applying pca give better results naive bayes model ways make model accurate
qvvd9c,0,benchmarking scaledyolov4 dataset images scaledyolov4 go model object detection decided test well dataset different one trained used citypersons dataset experiment subset popular cityscapes dataset consists person annotations found precision recall values 0 489 0 448 also found object detection dataset pretty good even though classes assigned lacking times checkout details experiment also checkout notebook used experiment
r1v2z9,1,generating confidence scores clustering algorithms dataset requires confidence scores point dataset apart confidence score require point leader follower associated case leader think assigning point closest centroid particular cluster leader sure related confidence score follower points advise appreciated
rwrhfh,1,learn mechine learning beginner hi android developer interest machine learning planning start learning proper manner far know machine learning basically mathematics googling couple days preapred plan wanna share request please give valuable suggestions tips comments 1 complete mathematics machine learning 2 complete 6 041 probabilistic systems analysis applied probability 3 complete stanford cs229 machine learning already know python hands experience machine learning related libarary doubts 1 watch whole lecture 6 041 probabilistic systems analysis applied probability watched till discrete probability e till lecture 7 2 learning statistics x200b request please give valuable suggestions tips comments x200b thanks lot stopping taking time read
rm67i0,1,notebooks release 30 crypto trading notebooks kaggle crypto competition released 30 starter notebooks demonstrating different model method crypto forecasting competition currently running kaggle x200b ongoing project also beginner friendly since highly documented many time series finance related notebooks released future also serve first stop studying time series analysis x200b whole project took lot time develop easy maintain please find value feedback support highly appreciated x200b selected example baselines starter notebooks x200b g research lstm starter notebook üî• x200b g research reinforcement learning starter x200b crypto prediction technical analysis features x200b crypto prediction volatility features x200b purged time series cv lightgbm optuna üî• x200b üî•üî• wavenet starter notebook üî•üî• updated x200b let talk time series validation x200b many many links inside notebooks x200b fork please enjoy
rnp2vk,1,tips tricks deploying deep learning webapp heroku cloud kdnuggets hi guys wrote blog facing lot problems deployment created guide deploying deep learning models heroku hope guys avoid making common mistakes please like share mean lot
r84o2r,1,qualitatively speaking training loss seems decrease strongly beginning training slows time quantitative study lucky proof qualitative observation
qq4v0g,0,deep shallow fusion rnn personalization end end deep learning models speech recognition produce highly accurate transcriptions lot harder personalize paper facebook ai team walks methods help increase accuracy proper nouns rare words end end deep learning models found really interesting made summary paper read link original paper facebook ai found
rcs88b,1,resource releasing three pokemon reinforcement learning ai tools including computer vision program play pokemon sword autonomously nintendo switch video proof source code available hullo tempest storm background building pokemon ai tools years get researchers news media cover research dumping bunch likely future bots play pokemon shining pearl autonomously using computer vision reason people think lying dump put doubts rest get code videos let start video proof videos marked two years old showing progression work computer vision building pokemon bots videos formerly private made public recently repos keep mind date version sword capture tool version repo mar 2020 made many changes since update files sake making runnable people tool 1 mock environment pokemon used practice making machine learning models tool 2 transformed pokemon showdown simulator environment could train pokemon ai bots reinforcement learning tool 3 pokemon sword replay capture tool video guide repo presentation working presentation video record end week sent slides powerpoint pro make look nice see draft version qa people might questions days get slides back use form add qa section video record discord event people interested code want learn run join discord empty years expect things look polished current link identity mystery real name slides well patent linked slides shining pearl bot briefly shown beginning custom object detector video around 1 minute 40 second mark conclusion presentation journey bring ai bots nintendo switch hopefully sometime weekend learn repos
qmln6l,0,washington u google study reveals attention matrices formed encoder decoder architectures new paper understanding encoder decoder architectures attend researchers university washington google blueshift team google brain team propose method decomposing hidden states sequence temporal input driven components revealing attention matrices formed encoder decoder networks quick read washington u google study reveals attention matrices formed encoder decoder architectures paper understanding encoder decoder architectures attend arxiv
rgykys,0,found 1 years worth research already published phd student middle studies year ago idea designing neural network medical image segmentation using shape priors done quick literature review time although admit might thorough enough found one really tried use shape priors especially task wanted use descriptors would fit specific task especially well worked hard implementation designing network architecture writing article understanding necessary mathematical proofs theorems related task submitted article weeks ago word yet today found article arxiv citations published spring basically uses idea task network architecture different mine performance evaluation different main selling point article usage shape priors already published bit devastated point would first 1st author paper really put lot effort thought discover idea already discovered obviously need much thorough literature review next time happen besides know else could mitigate damage done motivation even considering quitting phd moment feel like wasted lot time stupidity anything similar happened advice could cope similar issues career
r5fyxe,1,deep learning optimizers beginners simple language neat python 3 codes learning optimizers may first step beginners deep learning time tutorials codes skipped mathematical notations shown people watching simply jump dl libraries compile optimizer choice sometimes beginners idea happening one example beginners wonder loss increasing training case loss divergence problem posts medium talked widely used optimizers neat codes simple language hope posts prove useful beginners sgd sgd momentum sgd nesterov acceleration adagrad rmsprop adadelta adam amsgrad adamax optimizers racing minima x200b like posts follow medium reddit youtube future posts talk neural networks simple language neat codes
re1si9,0,phd internships forgive better venue r cscareers appear appropriate mostly software engineering discussions question best way land internships phd basically started getting conference publications precisely covid hit able attend conferences meet people zero personal connections cannot get referrals way case times lucky enough actually land interview feedback implicitly overwhelmingly previous internship experience therefore top candidate especially apparent research engineering positions found also true research scientist positions well appears chicken egg problem kind advice context several first author publications several second author etc big conferences neurips icml iclr come top european university phd lab supervisor average somewhat well known definitely far top 10 labs world h index somewhere 5 10 range citation count somewhere 200 500 range research around periphery reinforcement learning may bit niche nothing obvious industry lab able immediately plug done zero internships far able land oh clear supervisor going help helpful choose continue within academia really wish problem job internship hunting whole year expected graduation date coming summer stressing continuation exit plan whatsoever debt piling high stipend well poor point noticed increasingly yearning job able pay bills actually thinking wish work research fill applications every day simply seem go nowhere apologies correct place asking appreciate similar experience knowledge advice
quvv7h,0,deep generative model enables automated structure elucidation novel psychoactive substances past decade illicit drug market reshaped proliferation clandestinely produced designer drugs agents referred new psychoactive substances npss designed mimic physiological actions better known drugs abuse skirting drug control laws public health burden nps abuse obliges toxicological police customs laboratories screen law enforcement seizures biological samples however identification emerging npss challenging due chemical diversity substances fleeting nature appearance illicit market present darknps deep learning enabled approach automatically elucidate structures unidentified designer drugs using mass spectrometric data method employs deep generative model learn statistical probability distribution unobserved structures term structural prior show structural prior allows darknps elucidate exact chemical structure unidentified nps accuracy 51 top 10 accuracy 86 generative approach potential enable de novo structure elucidation types small molecules routinely analyzed mass spectrometry check new paper automatic structure elucidation new ‚Äúlegal highs‚Äù using artificial intelligence mass spectrometry published arxiv
qpzewi,0,ml datasets commercial use hi ton datasets ml researchers stemming different areas looking closely vast majority restrictive licensing allowing used research purposes commercial environment wondering strategies obtaining high quality datasets commercial purposes let say want build car object detection model company one well known detection use cases neither use public datasets allow commercial usage use pre trained models task trained datasets could collect data pay crowd annotators annotate data buy data would specifically interested last point way acquire types datasets market handling companies
qlqqow,0,researchers seoul national university nvidia microsoft release ‚Äòacav100m‚Äô automatically curated video dataset self supervised audio visual learning audio visual av learning defined delivering applying instructional content includes sound visual information natural relationship visual observations accompanying sounds shown strong self supervision signals learning video representations massive amount online videos become valuable source self supervised learning among research communities however due overdubbed audio online videos frequently provide imperfectly aligned audio visual signals therefore models trained uncurated films shown develop poorer representations result misalignment difficulties existing techniques typically rely manually curated datasets predetermined taxonomy semantic ideas audio visual connection highly likely overcome gap researchers seoul national university nvidia microsoft released automatic dataset curation pipeline large video dataset self supervised audio visual learning termed acav100m automatically curated audio visual dataset dataset made massive number uncurated web videos researchers took 140 million full length videos reduced 100 million segments best audio visual correspondence checkout paper codes project microsoft blog video presentation short read us x200b x200b
rftozk,1,advice get started hi everyone first post asking advice get started ml currently math undergraduate strong interests machine learning statistics done part cs231n courses online haven‚Äôt really implemented ‚Äúreal‚Äù model thinking project fun break read blogs online decided go ocr know bit much beginner hoping learning much implementing also it‚Äôs first question easier ways get started second question would good idea read papers look people‚Äôs implementation try suggestions would greatly appreciated thanks much
rfw5vf,0,new open source project model validation toolkit colleagues open sourced project validating monitoring machine learning models model validation toolkit included model validation toolkit modules measuring concept drift specialized performance measures biased data assessing credibility performance measurements taken small samples building interpretable neural networks adaptively setting thresholds maximize performance intelligently checking might missed sensitivity analysis user guides documentation notebook tutorials found project website please feel free reach gitter via github issues questions comments
rgxzhq,1,generate segmentation masks pixel level annotated images got csv file pixel coordinates convert values segmentation mask
qvgc13,0,u fda fellowships regulatory science machine learning systems apply see link anticipated appointment start date soon qualified candidate identified start date flexible location silver spring maryland x200b organization u food drug administration fda center devices radiological health cdrh office science engineering laboratories osel division imaging diagnostics software reliability didsr artificial intelligence machine learning program ai ml x200b assignment continual learning machine learning poised bring changes speed healthcare industry adapt changes patient management research area still nascent stage several recent research publications aiming towards solving plasticity stability dilemma critical time agency develop performance assessment strategies evaluate safety effectiveness continual learning algorithms project goal develop evaluation framework continual learning algorithms specifically segmentation classification tasks research fellow play key role developing evaluating ai ml algorithms x200b job responsibilities guidance mentor participant involved following activities conduct research answer emerging evaluation challenges medical imaging diagnostics systems contribute agency‚Äôs regulatory efforts providing technical expertise collaborate team members stakeholders complete report widely disseminate findings regulatory science tools conferences peer reviewed journal publications x200b candidate qualifications qualified candidate currently pursuing received master doctoral degree one relevant fields degree must received within past five years preferred skills strong background fundamentals eagerness solve technical challenges systematically experimental computational approaches developing analyzing ai ml methods cnn rnn gan etc programming python including scientific stack numpy scipy scikit learn etc deep learning frameworks tensorflow pytorch etc experience image segmentation processing data management
r1bau8,1,model credit payment probability asked work model predit credit payment probability know models predict credit default gives true false outcomes case need number probability model could give
r0fnl3,0,term specifically trained model per object use case train use specific model object building term defines case versus using one model task applied every building
qzsrdw,0,test set glorified validation set everything seems click really well comes training validation sets ideology behind completely held test set never really sat well basic level guess test set cheat least becomes lot nuanced start thinking used let say trained model proud final sanity check test performance test set see well model generalizes unseen data model performs poorly kind decision making inform start tweak model try figure going wrong soon tweak single thing test set instantly become kind glorified validation set thoughts matter interested hearing people know test set affect model otherwise acts validation set thus act bad test result change model test set becomes validation set although involved validation set used early stopping parameter tuning words test set must useless moment useful becomes validation set although precise test set useless probably lowers expectation later performance model production example kaggle competition final set test set since involvement model training however soon final leaderboard announced test set becomes validation set e g affects algorithms later choose e top competitors summary seems time using less involved validation sets double check involved validation sets
qvm71a,0,big tech companies coding interviews ml ph research student scientist intern positions want know need practice leetcode
rcg62g,1,difficulty understanding pix 2 pix discriminator trying understand pix 2 pix gan used image translation paper discriminator looks 70 70 pixel patches evaluates whether real fake takes conditional image synthesised image input according tensorflow tutorial implementation 2 images concatenated together question discriminator looking 70x70 patches time understand 2 images relate check conditional input actually informed image generated help greatly appreciated
r4639a,1,checking understanding neural networks hi wanted check whether actually understood neural network works first initialise network input layer hidden layers output layer nodes right word use perceptron neuron instead input layer takes weighted sum inputs adds bias weights biases usually random first run activation function usually sigmoid represents probability neuron active input fed forward network way error calculated end backpropagation used adjust weights biases input fed forward another error calculated way error reduces time network tested test data accurate description also different cnn dnn thank
rt2vwh,1,math self learn ai ml hello everyone want go cs masters thinking buying textbooks self learn prepare know much ai ml master degree holder would far know calc1 calc2 intro level probability linear algebra 1 undergrad course usually next list learn multivariable calculus maybe textbook maybe khan academy x200b math would need learn know much master student would ai ml x200b thank much
rtjo93,1,yolov4 add new class already trained model without training entire dataset already trained model dataset let‚Äôs say 50 classes it‚Äôs trained recognize letters let‚Äôs say want add newly discovered letter want model also able recognize letter would go would prefer able train existing model dataset containing new letter ‚Äúmake aware of‚Äù new letter appreciate help
ro12hr,0,drawbacks louvain leiden algorithm community detection looking drawbacks louvain algorithm recent leiden algorithm community detection internet search turns almost nothing except louvain lead disconnected communities fixed leiden algorithm however surely leiden algorithm end community detection drawbacks louvain leiden algorithms instances performs poorly instances prefer community detection methods e g spectral come see often classic data clustering one could build knn graph run louvain leiden top
ql5hdb,0,apply batch normalization layers batch normalization basically trying get unit gaussian output initialising data unit gaussian seems good idea network make sense
qr5t70,0,nvidia gtc 2021 check omnisci‚Äôs session nvidia gtc 2021 free learn bidmc dept endocrinology leveraging omnisci‚Äôs gpu accelerated analytics platform explore massive amounts transcriptomic data advanced research processes register
qlq3rt,0,raspberry pi ml frameworks benchmarks training inference hey folks know benchmarks available ml frameworks tf pytorch training toy data data sets inference raspberry pi edge devices
r84reh,1,common uses ml hello trying recap common main real world usage ml things company would need find helpful list moment time series prediction classification image text pattern recognition text summarization anomaly detection voice recognition text keyword extraction chatbot ideas add want see perspectives sure missed thanks consideration
rgmdwn,1,questioning whether use bag words portfolio project odds whether use text feature classification project problem inherently nlp one like sentiment analysis like traditional classification problem uses census urban data neighbourhood profiles make predictions neighbourhoods said text column dataframe help think would make good feature converted bag words versed nlp bag words pca makes sense worried might complicate things though send unnecessary rabbit holes trying learn nlp might able make due without
qvvsuk,0,category orthogonal object features guide information processing recurrent neural networks trained object categorization recurrent neural networks rnns shown perform better feedforward architectures visual object categorization tasks especially challenging conditions cluttered images however little known exact computational role recurrent information flow conditions paper tests rnns trained object categorization hypothesis recurrence iteratively aids object categorization via communication category orthogonal auxiliary variables location orientation scale object using diagnostic linear readouts find information auxiliary variables increases across time network layers b information indeed present recurrent information flow c manipulation significantly affects task performance observations confirm hypothesis category orthogonal auxiliary variable information conveyed recurrent connectivity used optimize category inference cluttered environments preprint code data tweeprint
r8r78p,1,alternative isolation forest requirement finding anomalies detail two files one one cloud platform aso need find anomalies communication tenants access vm sorry give vague information asked give names algorithm currently used algorithm isolation forest wanted know alternative even similar algo finding anomalies comparative study
r23zpp,0,adding tags images stylegan dataset experienced ml read somewhere data set stylegan2 stylegan3 training divided separated classes tags putting images common features separated folders main folder making tfrecord root folder actually improve quality model stability training
rkv8zg,1,advice multiple linear regression sklearn new machine learning want thoughts right probably best prediction combine love sports practice things learning school dataset using 5 years worth english premier league data short created program get standings given year x many games want includes w l points gf ga end dataset 5 years worth data teams xth game season using sklearn built multiple linear regression model uses gf ga predict points whatever x amount games played wanted end dataset previously mentioned used training testing model trained tested use current season standings x amount games predict many points team think job well points predicted pretty close actual points big question say season gets finished add season training testing dataset rerun model maintain accuracy next season predictions
rie4k9,0,machine learning research quantitative finance automation notion api wanted easy way get pulse hundreds quant research insights produced every day share students nyu dashboard helps follow arxiv srrn repec published financial journal papers including important twitter linkedin posts github repos hundreds blogs surely done sub disciplines machine learning attempting multiple solutions past months came tech stack uses notion api gcp functions cloud scheduler python scrapers rss feeds ml quant website let know questions feedback would happy share answer targeted questions
rjy067,0,anyone got hypothesis icing cake method improves accuracy understanding icing cake paper claims higher accuracy saving feature maps generated convolution layers classifier layer training using retrain classifier fully connected layers authors comment idea hypothesize assuming classifier training feature maps given order original training feature maps early training poor cnn trained bad extracting features actually act regularization method classifier much better feature maps end original training converges classifier slightly different point loss landscape seems similar layer wise regularization noise injection perhaps guys thoughts
rqq3xq,1,2 school courses would useful machine learning advanced linear algebra eigenvalues eigenvectors diagonalization orthogonality svd complex matrices infinite dimensional vector spaces vector spaces finite fields probability iii finite markov chains stationary distributions time reversals classification states classical markov chains convergence total variation distance l2 spectral analysis relaxation time monte carlo techniques rejection sampling metropolis hastings gibbs sampler glauber dynamics hill climb simulated annealing harmonic functions martingales markov chains course taken introductory courses lin alg probability topics thanks
rocen2,0,gans probability distributions images training gans either classic loss wasserstein loss try minimize distance probability distribution real data probability distribution generated data case gans images e g trained celeba probability distribution images look like intuitive way understand concept
rg2ba8,1,arima prediction hourly natural gas consumption hello predicting natural gas consumption trying different methods hourly observations data two seasonalities yearly daily tried tbats python takes ages train think many observations training set 24 365 2 17520 need many capture yearly seasonality estimator estimator tbats seasonal periods 24 365 25 24 x200b also tried auto arima python used fourier terms modelling two seasonalities keep running memory decent laptop would try seasonal decompose twice simple arima sure add seasonality back anyone good advice also would even evaluate results realize predict consumption compare actuals done larger scale thanks ideas able offer
qlrxzz,0,learn ai together discord community looking experienced people share projects willing help ai enthusiasts answering questions time time hey everyone looking researchers teachers teacher assistants professionals willing help exchange people learning ai answering questions time time ai focused community 20 000 people members chat ask questions share resources projects share find job offers etc focusing getting experts advanced members join us help us help others everyone else field welcome join well info community join us learn ai together excited chat note paid opportunity asking hours anything help whenever
rqt6w6,1,google colab tpus training fashion mnist model 5 6 times slower cpu reason way make run faster help appreciated thanks solved x200b colab notebooks cpu tpu x200b update thanks nice users kind enough help managed diagnose problem reason model training slower passing enough data easily fixed changing model fit function model fit train images train labels epochs 10 model fit train images train labels epochs 10 steps per epoch 60 scenario thanks people comments helped
rqwf4e,1,classification object detection question built object detector detecting birds bird feeder trained birds general without regard species order classify data running classification model another computer data retrieved database detections x200b wondering counterintuitive trying train object detection model yolov4 running jetson nano also perform object detections respect species point developing classifier powerful pc handle species classification
r5s348,1,given measurement values 20 air pressure sensors collected 3 months pipe system detect location broken parts holes system first thing came mind outlier detection size broken part increases pressure whole system seems drop sensors return less lower values one breaks drops first though therefore detecting moment something started break worked fine several outlier detection techniques localization work anyone got smart idea approach could work better
r8tpdc,1,best way label video object detection hello want label videos contains small fish order detect track looking way turn videos dataset train yolo detect please suggest free tool use besides label studio somehow work videos maybe know use well
rv7w4p,1,convolutional neural network scratch hello fellow learners searching tutorial convolutional neural network scratch please follow link blog gives begginer guide create cnn model like keras scratch
rvmiyr,0,paper summary rethinking segmentation sequence sequence perspective transfromers hi published latest medium article summary scientific paper aims eliminate effect locality one limitations cnns attempt researchers tried reform image semantic segmentation problem operate proposed transformer finally introduce three different decoder architectures please read give feedback find interesting share others interested ml well also find helpful follow medium updated forthcoming articles üôÇ
rlgygp,0,solo learn 1 0 3 new methods support transformer architectures better evaluation improved docs additional results hi reddit solo learn team back interesting news ssl library last months worked hard improve library adding following features 1 new methods nnbyol nnsiam vibcreg 2 support transformer architectures vit swin poolformer keep new research 3 better evaluation k nn object detection detectron2 pascal voc coco 4 improved docs adding tutorials code conduct contribution guidelines 5 additional results moco v2 barlow twins imagenet feel free ask questions contribute github enrico victor
qn1erq,0,theory could make code bridges different music genres know little coding playing vqgan recently idea popped head theory could write ai code gets feed stems instrument track separate audio file two different songs rebuilds first song style second one code would look relevant information first song melodies rhythms structure maybe even allow user feed lyrics vocals would study elements employed second song rebuild first song using second song style code could rebuild rock song rap song instance
r88p0z,0,k means producing different centroids even initialization canopies big data coursework ds masters pretty confused seeing running k means clustering mahout understand generally get different final centroids k means initializes randomly initialized clusters using canopy clustering even seeing wide variance final density values one example x200b run inter cluster density intra cluster density 1 0 430836339 0 590516787 2 0 618821774170647 0 581433137 3 0 4960634 0 567159394 run different k values distance metrics max iteration values behavior similar problematic purpose project compare performance across different parameter values divergent results within parameter set forced run set parameters multiple times take average drastically increasing processing time computation cost expected behavior misunderstand something
rdd5f9,0,automl conf 22 excited announce 8 years automl workshops icml 2022 hold first international conference automl website amazing speakers anima anandkumar jeff clune chelsea finn timnit gebru julie josse alex smola co located icml 3 days currently planned person submission deadline feb 24 2022 great co organizers frank hutter mihaela van der schaar marius lindauer isabelle guyon please spread word submit best work see conference
r7o01i,1,swimming thoughts cool evolve train deep sea creatures literature search openai gym swimmer v2 train three link fish swim direction fish schooling rl navier stokes environment swimming microscale rl nice model details
r6oxjl,0,machine learning factor analysis rule derivation etc considering question wanted know patterns could found question classification wanted know list factors criteria could deduced terminologist often prepared glossary considers phrase term example vaccine instruction manual ‚Äúmrna‚Äù would good term glossary word ‚Äúthere‚Äù would here‚Äôs thing order study factors tended correlate degree e kind model mathematical function structure perhaps linear equation need factors first place generate ample list factors begin like let‚Äôs say answer scenario ‚Äúa good term highly specific important good quality translation high frequency source text ‚Äù possible ai could perform level inductive reasoning observe classified data points context term term given text predict term somehow produce kind structure could observe understand factors found really hope someone help know ideas exist field question thank
qk6h7n,0,ai fit metaverse future popular applications research topics ai relevant vr ar ml engineer interested learning ml topics could useful building vr software hardware meant open ended question kinds opinions perspectives appreciated thanks
rf8vfu,1,useful data summary statistics image classification hello image classification tensorflow learning purposes splitting data 5 folds would like get useful summary statistics validation sets could useful shape validation sets
qvueo6,0,microsoft cmt cvpr 2022 good god dying wtf cannot access cmt official page cvpr 2022 anyone else
rhnnaw,1,approach scheduling problem got problem whereby trying match resources tasks think planning maintenance train need know people go trains understand optimization problem struggling understand frame problem pointers papers topics would appreciated
qxeu1x,0,stress testing models ensure robustness hey wondering could get discussion going regarding stress testing ml models ensure robustness maybe someone ideas paper recommendations know possible test robustness image recognition models example rotating flipping adding noise data wondering scientifically thorough guides also could generative models adversarial attacks used well degree curious see people think topic
qsdnc2,0,invitation participate study labour ethical ai invitation participate study titled ‚Äúthe labour ethical ai‚Äù name james steinhoff postdoctoral fellow institute communication culture information technology university toronto mississauga aim research understand labour goes ethical ai research research intended promote ethical responsible democratic human centered non profit socially beneficial ai organizations work conducted problems facing workers field field connected industry academia government goal study interview people work study intern ethical ai order gain empirical insight working conditions sector experience working sector could provide great insight challenges opportunities facing workers willing participation would involve meeting online interview last approximately one hour ask questions working conditions chose ethical ai sector promises problems facing sector participation project confidential supply potential participants informed consent form outlines greater detail project parameters participation happy answer questions concerns might thank time consideration sincerely james steinhoff phd j steinhoff utoronto ca postdoctoral fellow institute communication culture information technology cct building 3071 3359 mississauga rd mississauga l5l 1c6
r1rghr,1,programming gradient descent scratch fundamental essential part understanding machine learning made purposes learning functions explanations programming gradient descent scratch purposely made linear regression understood linear regression pain fully implement kind neural networks give star motivation like
rqy3kd,1,data science ml development processes would like see automated e python library parts ml projects find really tedious think wrapped python library looking project ideas
r79a3h,0,great r content code computer vision news december 2021 peek computer vision news december 2021 many articles ai deep learning computer vision html5 version recommended pdf version dilbert page 2 free subscription page 52 enjoy
qk5avf,0,mlops done current workplace joined startup recently necessary backend support ml deployment pretty much non existent simple templates ci cd modified designed generic microservices currently takes data scientists least 3 5 working days post r put model production prediction end point logging observability excludes setting necessary data pipelines models backend services whole process take long 2 weeks team looking setting framework automation cut turn around time putting models production trying establish reasonable goals project hope get insight others x200b part production processes automated mlops teams tools much effort tools help save much time currently take put piece r work production
rggzay,1,french beginner needs help important project hello french student citizen project trying set political program comparator french presidential elections 2022 fight abstention affects mainly poorest france beginner ml managed collect survey 10000 answers 10 favorite themes french first step want able classify user two categories left right according answers questions second step ask specific questions match candidate answers expectations problem exploit data obtained algorithm would allow best classify new users two categories thank help
r97cfd,1,buddies watch ml lectures planning watch machine learning lectures beginning today would appreciate buddies keep company learn something new together first plan watch lectures pca svm followed others feel free dm leave comment connect discord
r23juf,1,matching pursuit explanation anybody explain matching pursuit algorithm steps meanings
rcnfgx,1,find feature importance svm rbf kernel bit beside trying find best way evaluate feature importance 5 feature data set svm rbf kernel sklearn python kernel non linear cannot use coeff attribute searching online presented permutation importance anything else ask logistic regression xgboost evaluation methods feature importance would good sanity check method thus far found rbf svm thanks advance
r4ngnz,0,true shot setting described recent papers reasonable understanding concept properly coming across papers shot learning claim paradigm true shot learning inspired paper true shot learning language models perez et al 2021 basic claim current n way k shot setting widely used shot tasks unrealistic due fact models still access many data points classes mention models also usually access development set tune hyperparameters authors claim assumptions unrealistic grounds assuming access classes data points unrealistic access dev set might well use training writing post sure agree claims understanding paper properly would like seek opinions unrealistic access dev set case even test set well logic presented paper test set might well include training furthermore test set actually realistic split dev set well
rs21jj,0,good attempts recognizing asl sign languages beyond simple static signs like numbers letters seen lot projects aimed recognizing asl alphabet lot sign language depends movement current state actual sign language recognition translation able find anything online
rrk8j1,1,interested mentoring 1 person who‚Äôs motivated getting ds work data scientist ai company also finishing msc ds studies
rfvb02,0,advice stabilizing unstable dqn card playing agent attempting train reinforcement learning agent play game sushi go using deep q learning memory recall buffer separate model target networks current form q network takes input 25 element state vector returns 12 element vector action values one hidden layer layers fully connected relu activation functions search tune size hidden layer trying values 15 30 neurons methodology run five trials per network configuration follows starting randomly initialized network run 2 500 training games evaluate resultant performance 10 trials 100 games search though noticed network behavior wildly unstable starting randomly initialized model seed network configuration sometimes reach 90 win rate playing 2 500 games times quickly plateau 10 win rate show improvement tried longer trials 25 000 50 000 games example seen similarly unstable results anybody advice tuning performance tried adding additional hidden layer similar results next best guess start playing around state vector thoughts appreciated
rqeg9k,1,best resource learn implement knowledge graph facebook groups data create knowledge graphs data please recommend best source learn implement
r76sfb,0,discussion applied tracks ml conferences spite good set reviews aaai paper rejected citing appropriate applied tracks ml conferences even though fully agree reviewers since considerable novel approaches rl extremely heart breaking see year worth work get rejected based one opinion time find child mine new home kindy suggest places send paper reputed ai conferences coming next science track work involves graph networks rl something along lines also fine thanks lot reading whose paper got rejected worry find way
qmrglg,0,isomorphic labs unveiled today new alphabet company led deepmind demis hassabis plans tackle drug discovery using ai even insider found idea deepmind offshoot pretty surprising curious folks think odds succeed alphafold even useful drug discovery tweet unveiling company website
r2mi5a,0,research optimizing kernel matrix experimenting kernel perceptron polynomial kernel wondering way optimize kernel matrix representation training set given ordered training set x composed n vectors high dimensional real space kernel matrix size nxn cell index ij contains result k xi xj k polynomial kernel xi xj training examples index j respectively couple observations gram matrix means positive semidefinite computed real values real accessed line line due structure perceptron total size increases square amount training examples access matrix sequential therefore amount memoization suffice storing matrix memory becomes expensive quickly training set 60000 examples occupies 28 8gb 64bit floats given simple structure matrix accessing method think unlikely way compute quickly row without storing everything memory space saving solution would compute line needed efficient algorithm executed different number epochs execution may involved hyperparameter tuning choosing right kernel parameters solution impacts much performance also memory cheaper time way reduce effective size kernel matrix would store half matrix symmetric technically works practice almost efficient storing matrix computing line done vector oriented operations cpu gpu composing line existing data strictly sequential operation generally work well end good middle ground save half space growth rate unchanged computation time line almost storing matrix
rdwanv,1,difference xgboost gradient boosting xgboost enhanced version gradient boosting give better performance anyone explain different mathematically
rvg08w,1,bootcamp graduation project generating new cooking recipes list ingredients models could fit task hi team four working datascience project finished second day pretty depressed models could fit task hand week half left complete project python rather using kaggle dataset english trying make work data scraped french website aiming 20 000 recipes random recipe website interested thinking separating whole thing two tables features decided scrap far table ingredients table recipe name recipe id ingredient one row per ingredient recipe ingredient quantity units measure e g 500g salt 1 spoon oil recipes table recipe id recipe name recipe steps trouble deciding single string list strings two weeks machine deep learning course seen fair amount library tools like scikit learn keras pandas numpy done research clear understanding following models thinking using task would appreciate sort advice source way go next short summarize simplernn lstm gru rnn layers simple generate proper text gathered word2vec different way gan transformers gpt 2 bert albert seems likes answer idea realist us work limited time even start textgenrnn grasp useful could tablegan library appear good generating text x200b long text understanding models would really appreciate advice even without reading x200b x200b deep understanding neural networks works understanding rnn aim predict next future least element sequence mainly time series nlp familiar using simplernn lstm gru layers keras models understand concept vanishing gradient depth lstm remember supposed slower better simplernn told gru faster uses less parameters lstm textgenrnn tablegan never heard two looking works subject cooking recipes python library textgenrnn appeared better choice classic rnn models could used strings different lenght trained whole sequence adding character per character tablegan opposed two models returned tabloid data seemed cluster ingredients could interesting use vectors like think word2vec went back tablegan article said used countvectorizer scikit learn results gave back unusable long lists ingredients word2vec feed large amount processed text tokenize lemmatize methods remember input word short sequences returned dict score key words value closely related often used given string honestly know range application extends much gans transformers basically one lone slide told us advanced models cover course fully understand transformers understand seems function encoders decoders resort rnn use vectors guess need read attention need asap understanding transformers understanding gpt 2 bert albert transformers low moment gpt 2 seems pre trained calculates likelihood word clustering vectors occurring string based full content tried website made seem like one better options might already good feel like student level working might make worse think possible make predict words ingredients recipe steps like tomato curry chop oven instead giving words following sentence found bert albert searching best currents nlp models last one seems lighter version bert training hides random part full input string training avoid fitting also increases performances right know start gan transformers deadline quite soon honest like focus one two better solution x200b would grateful could recommend us models think could used task would help us lot limit field research
r7bns7,1,guys keen learn create ai art go beginner guys question says new machine learning want create ai generate art know long way go vague question ask really keen creating programmer 1 year experience kind help appreciated
r2ruzj,1,properly structure training data hello trying fit neural network training data keep getting errors like reaching fit function valueerror failed convert numpy array tensor unsupported object type list understand get error fitting see official keras documentation fit function accepts lists arrays tried pass always get errors passing lists gathering trainig data model sequential model add embedding dictionnary size 120 input length nr training vectors model add spatialdropout1d 0 4 model add lstm 176 dropout 0 2 recurrent dropout 0 2 model add dense 1 activation sigmoid model compile loss binary crossentropy optimizer adam metrics accuracy print type type traininginput print type 0 type traininginput 0 print type 0 0 type traininginput 0 0 print n print type labels type traininglabels print type labels 0 type traininglabels 0 print type labels 0 0 type traininglabels 0 0 model fit traininginput traininglabels epochs 5 batch size nr trainig vectors verbose auto prints type class list type 0 class list type 0 0 class int type labels class list type labels 0 class list type labels 0 0 class int valueerror failed convert numpy array tensor unsupported object type list tried converting data like well model sequential model add embedding dictionnary size 120 input length nr training vectors model add spatialdropout1d 0 4 model add lstm 176 dropout 0 2 recurrent dropout 0 2 model add dense 1 activation sigmoid model compile loss binary crossentropy optimizer adam metrics accuracy traininginput tf constant traininginput dtype tf float32 traininglabels tf constant traininglabels dtype tf float32 print type type traininginput print type 0 type traininginput 0 print type 0 0 type traininginput 0 0 print n print type labels type traininglabels print type labels 0 type traininglabels 0 print type labels 0 0 type traininglabels 0 0 model fit traininginput traininglabels epochs 5 batch size nr trainig vectors verbose auto leads following error valueerror convert non rectangular python sequence tensor lastly tried giving input training vectors length vectors series integers varying lengths range nr trainig vectors traininginput traininginput 10 model sequential model add embedding dictionnary size 120 input length nr training vectors model add spatialdropout1d 0 4 model add lstm 176 dropout 0 2 recurrent dropout 0 2 model add dense 1 activation sigmoid model compile loss binary crossentropy optimizer adam metrics accuracy print type type traininginput print type 0 type traininginput 0 print type 0 0 type traininginput 0 0 print n print type labels type traininglabels print type labels 0 type traininglabels 0 print type labels 0 0 type traininglabels 0 0 model fit traininginput traininglabels epochs 5 batch size nr trainig vectors verbose auto leads issues type class list type 0 class list type 0 0 class int type labels class list type labels 0 class list type labels 0 0 class int valueerror failed convert numpy array tensor unsupported object type list point quite sure anymore structure vectors order issues point
ruwidq,0,deal huge categorical data dataset already contains 55 columns around 10 columns categorical data onehotencode end column count 300 something advisable people deal huge number columns mean 300 columns big deal would like know opinion thoughts
qpenkt,0,project google movenet real time pose estimation used control nintendo punch hey ai fans hacked original nintendo punch control actual punches boxing video game uses google movenet real time pose estimation track movements detect punches blocks moves sends commands game check full video plenty sweet movenet footage play x200b
rkqfjc,0,shopify applies ml anomaly detection forecasting scale dr ella hilal head data science engineering revenue growth shopify enterprise data ai jan 6 11 30 hi r machinelearning wanted share webinar coming january 2022 enterprise data ai put info website along link interested interested hearing built anomaly detection engine performing featured guest speaker dr ella hilal head data science engineering revenue growth shopify shopify 1 7 million merchants across 175 countries hundreds millions consumers shopping stores we‚Äôre focused leveraging scale data empower shopify create new experiences merchants impossible without data daily operations shopify highly data informed ways we‚Äôre leveraging advanced analytics building anomaly detection engine allows us process hundreds thousands metric segment combinations accessible way talk dr hilal share key tips apply machine learning anomaly detection forecasting scale agenda 11 30 12 30pm featured presentation 12 30 13 00pm q interaction link website
rchy2l,1,guys thinks ai dream app u think created tools resources used app take create text video generation app
qxmw3r,0,monitoring defi activity open source tools detailed post cloudwall capital team uses moonstream python api dagster data pipeline framework monitor defi activity rare see clear writing technical blog post definitely worth read
qz3qtv,0,5 considerations deploying machine learning models production ‚Äì miss wrote post considerations deploying machine learning models production considerations consider know first consideration seems obvious thought worth mentioning 1 use laptop development best practice consider development environment first data scientists ml engineers invariably use laptops development testing debugging code simplicity easy access install latest ml libraries practitioners overwhelmingly prefer laptops clusters development spoiled ides syntax highlighted editors good reason python developers like customize environments match staging environment library dependencies using conda python virtual environments ideally best practice code developed laptop run minimal changes staging production environment cluster immensely improves end end developer productivity consider laptop preferred choice development environment possibility extending syncing code cluster environment cloud consideration number 1 use laptop development best practice 2 training scale tracking model experiments unlike traditional software development cycle model development cycle paradigm different number factors influence ml model‚Äôs success production first outcome model measured metrics acceptable accuracy second achieving accuracy satisfies business goal means experimentation one model ml library many models many ml libraries tracking experiment runs metrics parameters artifacts etc vital accuracy developer‚Äôs choice ml libraries experiment third accuracy directly linked quality acquired data bad data results bad model data preparation feature extractions feature selection standardized normalized features data imputations encoding imperative steps cleansed data lands feature store accessible model training testing phase inference deployment fourth choice programming language familiar data team ‚Äî data analysts data scientists ml engineers ‚Äî also supported many ml libraries employed model experimentation training phases python seems de facto choice alongside choice programming language choice ml framework taming compute intensive ml workloads deep learning distributed training hyperparameter optimization hpo inference ‚Äî horizontal scale ‚Äî laptop single node multiple cores multiple nodes multiple cores finally ability easily deploy models diverse environments scale part web applications inside mobile devices web service cloud etc consideration number 2 consider using model life cycle development management platforms like mlflow dvc valohai weights biases sagemaker studio ray ray tune ray train formerly ray sgd pytorch tensorflow distributed compute intensive deep learning ml workloads 3 managing machine learning features feature stores emerging pivotal components modern machine learning development cycle data scientists engineers work together successfully put models production singular store persist cleaned featurized data becoming increasing necessity part model development cycle feature stores address operational challenges provide consistent set data training inference avoid data skew inadvertent data leakage offer customized capability writing feature transformations batch streaming data feature extraction process training allow request augmentation historical data inference common large fraud anomaly detection deployed models recommendation systems aside challenges considerations putting models production operationalizing ml data equally important model accuracy depends good data feature stores help manage precomputed cleansed features model training production inference model serving consideration number 3 consider feature stores part model development process look feast tecton sagemaker hopsworks databricks feature stores 4 deploying serving inferencing models scale model trained tested confidence met business requirements model accuracy seven crucial requirements scalable model serving frameworks consider framework agnostic model serving elected framework ml framework agnostic deploy common model built common ml frameworks example pytorch tensorflow xgboost scikit learn algorithms model architectures business logic model prediction often requires preprocessing post processing ability augment request data connecting feature store data store validation model serving allow part inference model replication models compute intensive network bound elected framework fan requests model replicas load balancing among replicas support parallel request handling peak traffic request batching models production employed real time serving often models scored large batches requests example deep learning models parallelizing image requests multiple cores taking advantage hardware accelerators expedite batch scoring utilize hardware resources worthy consideration high concurrency low latency models production require real time inference low latency handling bursts heavy traffic requests consideration crucial best user experience receive millisecond responses prediction requests model deployment cli apis ml engineer responsible deploying model able use model server‚Äôs deployment apis command line interfaces cli simply deploy model artifacts production allows model deployment within existing ci cd pipeline workflow patterns models production ml applications increasingly becoming pervasive sectors industry models trained ml applications complex composite range computer vision natural language processing recommendation systems reinforcement learning models don‚Äôt exist isolation predict results singularly instead operate jointly often four model patterns pipeline ensemble business logic online learning pattern purpose merit machine learning engineers adopt two common approaches deploy patterns models production one embed models web server offload external service approach pros cons respect seven considerations consideration number 4 look seldon kfserving ray serve seven requirements 5 observing monitoring model production model monitoring often overlooked stage part model development lifecycle critical model‚Äôs viability post deployment production stage often afterthought ml engineer‚Äôs peril models afterlife viability viable life production needs constant watchful sentinel eye fact monitoring phase simply continuation model serving consider model monitoring number practical reasons stage pivotal let‚Äôs briefly discuss data drifts time mentioned quality accuracy model depends quality data data complex never static meaning original model trained extracted features may important time new features may emerge need taken account example seasonal data changes features drifts data require retraining redeploying model distribution variables longer relevant model concept changes time many practitioners refer model decay model staleness patterns trained models longer hold drifting data model longer valid relationships input features may necessarily produce model‚Äôs expected prediction hence accuracy degrades models fail time models fail inexplicable reasons system failure bad network connection overloaded system bad input corrupted request detecting failures‚Äô root causes early frequency mitigates user bad experience deters mistrust service user receives wrong bogus outcomes systems degrade load constantly vigilant health dedicated model servers services deployed important monitoring health data pipelines transform data entire data infrastructure‚Äôs key components data stores web servers routers cluster nodes‚Äô system health etc collectively aforementioned monitoring model concepts called model observability step acceptable imperative mlops best practices monitoring health data models never afterthought rather ought part parcel model development cycle consideration number 5 model observability look evidently ai arize ai arthur ai fiddler ai whylabs ai thanks excellent pytorch reddit post inspiring formatting post
rsm8st,0,bayesian evidence calculation normalizing flows way calculate evidence marginal likelihood model evidence using normalizing flows usually calculate using nested sampling algorithms wondering ml alternative
rb1urf,1,phd machine learning want phd ml going give importance best topics phd ml
qmq67b,0,feedback idea platform turns code monetized api hi building platform turns code api hosted us infrastructure paid whoever calls service charge exact amount infra costs us choose also monetize service data scientist really excited possibilities easily sharing models capabilities api without dealing dev ops biased love building really want know think using platform mentioning name sure allowed many create ml models know set api monetize want smart people create smart solutions able share easily anyone creating service would free signing free creator earn someone using service thoughts offer
qt1vuy,0,nvidia jetson thoughts anyone used nvidia jetson models production use cases considering using health application heard much developer experience one way alternative would probably running application mobile device
rpirvh,1,machine learning use cases telecom industry apart churn prediction customer segmentation anomaly detection data science used telecommunications sector aware industry research use cases field
racb4q,0,optimal policies tend seek power neurips spotlight summary ways actions like staying alive optimal roughly staying alive lets agent things actions help agents stay alive keep options open provably optimal reward functions upshot might hard design intelligent real world ai systems let us deactivate correct statistically goals don‚Äôt incentivize behavior goals would conflict goals smart ai agents excerpts paper abstract researchers speculate intelligent reinforcement learning rl agents would incentivized seek resources power pursuit objectives researchers point rl agents need human like power seeking instincts clarify discussion develop first formal theory statistical tendencies optimal policies context markov decision processes prove certain environmental symmetries sufficient optimal policies tend seek power environment symmetries exist many environments agent shut destroyed prove environments reward functions make optimal seek power keeping range options available maximizing average reward navigating towards larger sets potential terminal states introduction omohundro 2008 bostrom 2014 russell 2019 hypothesize highly intelligent agents tend seek power pursuit goals power seeking agents might gain power humans marvin minsky imagined agent tasked proving riemann hypothesis might rationally turn planet‚Äîalong everyone it‚Äîinto computational resources russell norvig 2009 however another possibility concerns simply arise anthropomorphization ai systems lecun zador 2019 various 2019 pinker russell 2020 mitchell 2021 clarify discussion grounding claim highly intelligent agents tend seek power section 4 identify optimal policies reasonable formalization highly intelligent agents optimal policies tend take action action optimal reward functions expect future work translate theory optimal policies learned real world policies section 5 defines power ability achieve wide range goals example money power money instrumentally useful many goals conversely harder pursue goals physically restrained physically restrained person little power action seeks power leads states agent higher power make claims large scale ai power seeking behavior could become plausible instead consider theoretical consequences optimal action mdps section 6 shows power seeking tendencies arise anthropomorphism certain graphical symmetries present many mdps symmetries automatically occur many environments agent shut destroyed yielding broad applicability main result theorem 6 13 conclusion many real world environments symmetries produce power seeking incentives particular optimal policies tend seek power agent shut destroyed seeking control environment often involve resisting shutdown perhaps monopolizing resources caution many real world tasks partially observable learned policies rarely optimal results mathematically prove hypothetical superintelligent ai agents seek power however hope work foster thoughtful serious rigorous discussion possibility links paper neurips recorded spotlight presentation neurips poster session tue 7 dec 8 30 pst spot d3 gather town series blog posts line work twitter thread
qr3b6w,0,fast cc taylor transform computer vision potentially train nerf matter minutes hello released latest paper arxiv explore idea fast summation algorithms equivalent fast fourier transform taylor series instead context computer vision terms computational complexity approach disentangles number model parameters n model evaluations e approach n instead nm allows us reduce number flops required training inference 150 200x depending problem unfortunately current implementation flop inefficient flop efficient implementation requires good bit custom cuda code working video abstract paper link let know questions
r1ledn,1,salary swe vs ml oftentimes see posts people either r machinelearning ops talk field making less money vanilla basic swe work
rmu8gx,1,dbscan clustering algorithm interesting article dbscan clustering algorithm hyperparameter tuning
rwots2,0,methods create monolingual language model pretrained multilingual model apart fine tuning pretrained multilingual language model target language anything sophisticated people
rqdpqs,0,ml agriculture hey working software product fruit growers helping organize tasks season better quality fruits wondering potential ml anyone experience field ideas predicting yield field using historical weather data actual yield year actions taken field predicting price fruit variety based total users yield predictions amount searches variety marketplace subpage hoping focus though quality quantity produced fruits many factors like temperature humidity air soil weather stations even sensors make accurate macro micro nutrients unfortunately available year requires lab actions like tree thinning hard digitalize perhaps using image recognition impact yield quality firmness color diameter fungi pest infection risk appreciate ideas opinion
r4k5lf,0,seeing compelling use cases semantic search leveraged scale org highly reliant elastic search currently investigating merit incorporating semantic search component search pipeline like built prototypes leveraging faiss data results impressive come across compelling use cases teams putting use scale want note come research background
rxczzl,0,deepchecks open source tool high standards validations ml models data hey everyone wanted share open source tool building deepchecks open source tool validating testing models data efficiently deepchecks python package implementing validations tests needed order trust ml pipeline contains many built checks verifying data integrity inspecting distributions validating data splits evaluating model comparing different models addition contains test suites similar test suites software programs accompany building blocks ml pipeline development test suite contains checks necessary specific part pipeline suite result looks something like x200b suite result suites checks simple syntax highly customizable want jump right try quick start notebook think i‚Äôll happy hear thoughts feedback
r1owph,1,powerful laptop really needed one ms data science macbook air enough
rprmq3,0,sentencepiece wordpiece bpe tokenizer best one several popular tokenization algorithms frequently encounter byte pair encoding sentencepiece wordpiece less often unigram title formulated somewhat provocatively assume single best algorithm candidates key differences situations one might preferred others
rms9uj,1,deeper learning colu activation
r6otxk,0,cohen kappa ‚Äî useful often see subtle misuses interrater reliability metrics example imagine running search relevance task search raters label query result pairs 5 point scale relevant 2 slightly relevant 1 okay 0 slightly irrelevant 1 irrelevant 2 marking relevant vs slightly relevant big difference relevant vs irrelevant however irr calculations take kind ordering account gets ignored wrote introduction cohen kappa rather simplistic flawed metric good starting point understanding irr hope helps welcome feedback curious hear irr metrics find relying
r8v4fq,0,project dall 3 generate better images fewer tokens clip guided diffusion link images code link diffusion model colab project created combines transformer image generation clip guided diffusion named 3 projects based dalle pytorch clip guided diffusion vqgan btw big thanks lucidrains rivershavewings openai vqgan team work general ddpm gan vae transformer image generators use vqvae decode images better use diffusion model wondering started experimenting different ways decode vector quantized embeddings diffusion model see discussion lot trial error got something works pretty well resulting diffusion model basically absorbs one scaling factor get similar image quality 16x16 tokens diffusion compared 32x32 tokens vqgan trained relatively small dalle pytorch model embeddings put whole thing together github colab terms generalization ability eg avocado armchairs substitute large scale transformer common object categories people food etc small transformer diffusion works surprisingly well kind proof concept conditional diffusion approach provide flat boost quality transformer model including cogview ru dalle vq diffusion openai original dall e parameters architecture mostly dictated pretrained models leveraging lot potential optimizations explored training scratch removing final unet downsampling layer leave someone gpu cluster
rknwid,1,skewed imbalanced feature learned imblearn smote imbalanced label class es dataset skewed categorical feature column pandas balance numerical data box cox way cat feature column need evently counts n subgroup
rtndgm,0,best practices machine learning non profit promotes best practices machine learning specifically responsible ml practices open source cool link think technical best practices seems little stronger organisational ones thoughts linkedin url
riji40,0,gpu access without limit increases hi folks trying get access gpus urgent training jobs looks like cloud providers require 3 business days turnarounds alternatives someone suggest could get started training job right away
r4txgv,1,üíäyour daily dose machine learning deploying tensorflow models c series posts post almost daily call ‚Äúyour daily dose machine learning‚Äù machine learning projects several ways deploy final model could cloud deployment deployment mobile devices deployment embedded systems etc leverage several programming languages depending tech stack you‚Äôre using day job project companies worked needed deploy deep learning models c environment models mostly dealing image classification object detection remember asking question stackoverflow almost 4 years ago deploy tensorflow model c environment received 2 answers 4 years period that‚Äôs first started looking different options deploying tensorflow models c environment i‚Äôve mainly tried 3 options seemed promising 1 deploy models using tensorflow c api 2 deploy models using opencv dnn module 3 deploy models using onnx runtime based experience choose one 3 options go onnx runtime it‚Äôs great tool offers lot flexibility comes models deployment i‚Äôll go little bit details upcoming posts follow favorite social network
rhm5u9,1,ml dataset help derive insights nature un sdgs sharing open dataset building machine learning models related un sustainable development goals sdgs dataset contains tens thousands text excerpts validated 1 000 citizen scientists 100 countries respect sdgs dataset called osdg community dataset osdg cd freely accessible zenodo end using dataset share results us via email mailto community osdg ai love see come
rade21,1,writing inference engine quantization float32 int8 hello question quantization trying write inference thing manually already written working version using float32 image processing network size intermediate layers mostly effected resolution input wanted try use int8 instead float32 see well worked pytorch convert quantized int converts weights bias intuitively think would need weight bias input data int8 run network smaller memory usage intuition true convert bias int8
rcn2ha,0,pretrained models gpt 2 clip etc api endpoints building web apps hi recently built pretrained convect ml i‚Äôve interested potential building web apps top pretrained models gaining popularity machine learning community one foundational piece apps would access predictions models fast response times i‚Äôve made models available via api ai tasks 1 text generation gtp 2 gpt neo 125m pegasus paraphrasing provide text generate text similar style content use models build ai writing assistant even synthesize entire articles 2 computer vision clip measure association sequence images list arbitrary texts use clip app detect vehicles animals trees household appliances physical objects describe words 3 conversation blenderbot 400m distill build ai powered chatbot responds user inputs tbh blenderbot‚Äôs responses don‚Äôt always make sense i‚Äôd careful one 4 article summarization bart large cnn generate summary salient points article use model build tools help people consume information faster 5 text classification bart large multinli measure association sequence words list arbitrary text labels classification model used detect topics e g send customer call center transcripts api detect customers reaching specific topics interest product defects payment discrepancies 6 sentiment analysis distilbert base uncased finetuned sst 2 detect sentiment piece text model example could used measure overall customer approval levels product social media posts many endpoint response times sub second could used applications provide near real time experience also included codepens react models make easy get started building top endpoints handle preflight requests origin applications purely browser based want i‚Äôm first make models available free api hoping make experience getting started easy possible i‚Äôd also love chat anyone curious using models projects there‚Äôs particular model wish readily available api endpoint
r4rd5h,1,object detection handwritten signatures trying run object detection pdf documents recognize signature position know pretrained model recognize signatures
r6ywq0,0,prediction interval confidence index obtained neural network background material science used neural networks good success predict material properties however feel like missing two important ingredients 1 ability output prediction interval opposed single value 2 reliability confidence level indicating whether neural net found close neighbors simple interpolation led high confidence prediction vs prediction made region little data extrapolation needed low confidence 1 maybe simply adding additional output neurone could work one neurone mean highest density one variance would acceptable option 2 idea algorithms output information would grateful someone could share information looking forward feedbacks thank
qvklgh,0,run huge language models transformers hii everyone interested trying t0pp big science project lately appear big models able run machines colab colab pro still able run t0pp wondering experiment big models fit colab
r8r78q,1,alternative isolation forest requirement finding anomalies detail two files one one cloud platform aso need find anomalies communication tenants access vm sorry give vague information asked give names algorithm currently used algorithm isolation forest wanted know alternative even similar algo finding anomalies comparative study
rbp1e6,1,state art approaches product matching high accuracy goal compare invoice product prices price catalog qualify price reasonable information everything usually seen invoice e g product description unit quantity price example product descriptions german kelit kelox windox u press stk aus windox ppsu d20mm xl 2 ig messing messing muffe reduziert nr 3240 ig ig 3 4 x1 2 geberit silent pp rohr mit muffe l√§nge 50cm d40 tried string similarity far however approach seems slow easily fails string length differs much main problem available data used comparison become gradually also tried capture certain attributes product description e g ‚Äúbrand‚Äù ‚Äúactual product‚Äù ‚Äúmaterial product made‚Äù ‚Ä¶ looking phrases present product description consequently comparison done attributes approach also cumbersome phrases need set manually approach could try thank lot
re1hgk,1,forsyth 2019 applied machine learning hi everyone searching pdf book someone help would greatful
rpv5va,0,looking papers prove deep learning cannot solve given problem hey guys looking works proven machine learning cannot solve given problem lack information present input data ps relevant fields involving high stakes decisions like healthcare crime social good etc
rq4wp4,0,anyone got autokeras working sort scale played around autokeras last exactly says tin regards small scale single gpu configurations scale multi gpu setups performance benefit isn‚Äôt documentation says there‚Äôs ostensibly support multi gpu setups via mirrored training strategy doesn‚Äôt seem increase performance rather decrease
r12kku,0,federated learning microblog part 2 annotated paper wrote another twitter thread goes deep math behind federated learning trained well performs twitter thread annotated paper communication efficient learning deep networks decentralized data like feedback let know
rc4yiw,0,imputation methods missing data maybe simple question want see opinions small dataset 2000 3000 samples third party collected since physicians collecting data time think like ‚Äú oh forgot ask question patient analysis cares‚Äù problem 5 10 data missing different categories makes impossible remove wanted check opinion imputation methods
rr0q7h,0,best resources tools draw nicer table comparing different models frameworks performance hi would like suggest share resources draw table compare different deep learning model like minst cifar performance thinking latex way trying exact like looking exactly hence looking suggestions make nicer actually
r1z1ig,0,70 page paper yoshua bengio team gflownet foundations new paper gflownet foundations research team mila university montreal mcgill university stanford university cifar microsoft azure ai builds upon gflownets providing depth formal foundation expansion set theoretical results broad range scenarios especially active learning quick read 70 page paper yoshua bengio team gflownet foundations paper gflownet foundations arxiv
rrapbr,1,need help hello everyone beginner ml tips suggestions start also please suggest good online resources
rntvid,0,make lower upper bound constrained predictions using sci kit learn trying run regression model sports player performance data sum individual predictions falls upper bound lower bound team expected prediction distribution given current covid situation many star players missing games leading many name players getting unprecedented opportunity problem regression model underpredicts almost say know dallas cowboys best offensive line football going worst rushing defense football star running back ezekiel elliot game hypothetically two running backs filling played much averaging 5 yards game 10 yards game respectively regression model predict somewhere ballpark 5 10 players implies dallas cowboys rush 15 yards entire game whole world knows going happen historical data says 95 chance dallas cowboys rush 75 125 rushing yards altogether way group predictions together scikit learn set lower bounds upper bounds team sum almost like multilevel model type thing two agree make logical sense predicting team rushing yards using team statistics would predict somewhere 75 125 yards get individual player predictions sum something makes sense essentially trying combine regression constrained optimization thanks much
rnnube,0,could give transformer long term memory reserving part attention window world vector embeddings inputs transformer merely vectors like every ml architecture vectors usually represent tokens allow transformer generate store world vector embeddings select batch via attention new input would create functional loop access memory right
rucns3,1,using genetic algorithm minimax fitness function recreated 2 player turn based deterministic board game like chess checkers etc thought fun try create simple ai plays game began reading online matter seemed fitting use case use minimax algorithm inspired videos seen genetic algorithms additional reading thought great idea use genetic algorithm create good evaluation function surprise hard time finding examples genetic algorithms used generating fitness function use minimax algorithm also thought less made sense even relate chromosome board state single number output
rlcc92,1,little tip aspiring data scientists ml engineers take coursera udemy course work practical tutorials jupyter notebook use fancy tools collection helpful cheat sheets books multiple certifications well good doesn‚Äôt mean complete grasp subject test skills implementing use case scratch without using ready made resources instructions data collection preparation understanding modeling training optimization robust pipeline able explain interpret realized try visualize describe someone expert several times notice learned fuse something unique valuable excerpt book ai thought book amazon link
rju3nf,1,detect timing anomalies using ml methods hello everyone really new machine learning algorithms right research project desk expected addressed ml methods would like detect timing anomalies power traces embedded system let say sample system voltage rate 1 second exactly moment system powers make scenario simple possible easy start system embedded power behaviors fixed expect voltage patterns run well noises considered done collect 30 traces 20 items training 10 validation training set hoping trained model give prediction timestamp testing phase threshold set alert incoming values deviate far prediction anomaly collected 30 traces would exactly considering random noises different micro architectural states main reference using tutorial select cnn algorithm perform training retrieve prediction waveform shown x200b blue reference trace without timing anomalies red prediction trace looks like cnn captures patterns amplitude seems appear proportional difference squared errors two traces maximum error around 25 x200b square errors reference trace prediction one however compare prediction trace corrupted trace many timing anomalies square errors report deviations timestamps maximum value smaller 25 cannot set static threshold claiming anomalies x200b blue corrupted trace timing anomalies red prediction trace x200b square errors corrupted trace prediction one quick questions might look naive experts 1 cnn correct option go purpose otherwise kind algorithm use project opinion 2 prediction trace make sense amplitude difference occur predictions reference 3 possible set dynamic threshold differentiate timing anomalies latter scenario 4 using pytorch suggestions offer run training procedure faster training took 1 day really appreciate finishing reading long question really looking forward hearing suggestions opinions thank
rv5ufl,1,batch normalization kaiming initialization addressing issue internal covariate shift repost question asked cross validated stackexchange received answers yet reposting visibility hope perhaps someone help body question pasted se legibility original batch norm paper ioffe szegedy 2015 autors define internal covariate shift change distributions internal nodes deep network course training present batch norm solution address issue normalizing layer inputs across mini batch understanding internal covariate shift exact issue typically addressed designing weight initializaiton criteria instance kaiming initialization et al 2015 central idea investigate variance responses layer avoid reducing magnifying magnitudes input signals exponentially far tell also addressing internal covariate shift understanding correct case often make use techniques seems redundant perhaps two solutions better one understanding incorrect please let know thank advance references ioffe sergey christian szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning pmlr 2015 kaiming et al delving deep rectifiers surpassing human level performance imagenet classification proceedings ieee international conference computer vision 2015
r8w4s7,1,feeling weirdly motivated demotivated time taking class beginners ml got dataset 200 students one lowest even though tried pretty hard ran full searchgridcv random forest tree get 68 even though 60 students accuracy 70 also tried xgboosting story stuck 67 due covid course started offline really know anyone ask middle weekend killing dataset pretty clean already removed features low importance gave small boost help much somehow demotivates time feel obsessed trying get 70 learn algorithms anybody ever spot tips feel like
refydd,0,ml community outdone seems gpt associated models dali clip came roughly year ago machine learning community gotten lot quieter terms new stuff get state art results need outperform giant opaque models mean ml solved really think anything look forward seems models successful
rhod30,0,project deepmind perceiver io available hugging face deepmind perceiver io first transformer based neural network works kinds modalities text images audio video point clouds added hf transformers blog post example notebooks x200b wonder someone real life experience working one consideration choosing dedicated models unless use cases involves mix modalities
rv8ubv,1,hugging face pytorch lightning tutorials hi everyone recently graduated top class great uk university also worked nlp lab studies recently switched computer vision researcher role work ml models majority time anyway side started making practical coding ml tutorials favourite way learn moment guides focus creating hugging face language models due main focus university revolved around hugging face transformer models etc studies thought content youtube good always perfect looking hence putting guides people future similar projects put two videos far one basics hugging face library guide multi label classification beats state art dataset knowledge future ideas include masked language modelling task cifar10 cnn guide yes plan make tutorials computer vision future would great get feedback suggestions current videos let know find useful thanks multi label classification hugging face basics x200b x200b
rtvqdl,1,idea could one create local personalized ai pair programmer kind tools algorithms could required title ¬øwhy also considering possible limitations could conditions 1 ai would trained scratch pc user 2 work one programming language maybe stuff missing anyway pleasured read insights personally curious whether one could create program using modern tools like julia work modern complex languages like rust accelerate development personalized way possible
rvp1xq,1,actually difference ds ml deep learning know python tensorflow libraries numpy pandas sklearn matplotlib made projects using sklearn models csv datasets deployed flask backend created cnn models using keras api made neural networks using keras layers like conv layer dense layer maxpool layer activation functions created gan model using tensorflow keras completed andrew ng course deep learning question knowing things category fall beginner intermediate data science beginner intermediate machine learning beginner intermediate deep learning something else
r5wc8j,0,stylegan3 overview tutorial pre trained model post covering stylegan3 discusses architecture improves stylegan2 use also includes pre trained stylegan 3 model
qs0eup,0,fine tuning running gpt j made easy guys seen eleuther‚Äôs gpt j nlp yet feel like it‚Äôs par openai‚Äôs curie pretty good overall generic language generation still need fine tune custom tasks ended putting together project simplify fine tuning deployment production done web interface also added default pre trained gpt j use interface api please check give feedback thanks project
rmimjq,1,give control game ai probably lots ways simplest abstraction let say game league legends bare mind one person feasible even attempt ask curiosity give control ml algorithm input controls let thing
qsi0u2,0,prune quantize quantize prune post training optimization computer vision models student interested improving efficiency model size throughput energy pre trained deep learning models computer vision domain wondering clear winning approach order prune quantize pre trained model basically given pre trained deep learning model e g resnet would one following approaches lead better solutions terms accuracy efficiency ratio e g model size throughput energy consumption quantize prune finetuning retraining every stage prune quantize finetuning retraining every stage trouble finding related works answer kind question seems valid question maybe missing something obvious input would appreciated
r6jee3,1,load 85 6 gb xml data dataframe hello everyone trying load large xml dataset 85 6gb dataframe jupyter notebook python tried 1 pd read xml system crashes high memory usage 2 elementtree system crashes high memory usage 3 vaex opensource library handles xml data well throws error anyone encountered similar challenge kindly share go thanks advance regards mustafa ml student python machine learning datascience
qukghv,0,riemannian manifold intuitively recently studying dimensionality reduction come state art dimensionality reduction algorithm umap understand mathematics part think first obstacle understand understand riemannian manifold watched multiple videos tutorials riemannian manifold still cannot catch idea easily seems like first thing explain already requires complicated maths knowledge would like ask anyone really explain one simple sentence normal computer science student year one undergraduate stem student understand thank much
rnr1vu,1,need guidance computer science couple years 3 precise till wandering trying every field web dev game dev app dev data science etc know want make carrier data science know much math 10th grade prior programming experience python c javascript anyone give good quality free learning resources math currently learning statistics book think stats
rwenkn,1,ml dojo get daily updates latest field artificial intelligence üß† machine learning ü§ñ link ml dojo let‚Äôs start question keep updated latest happenings ai ml really think second moving forward asked question lot ai ml practitioners quick summarization responses includes ‚Äî following top scientists researchers ai labs twitter finding good articles medium recent papers arxiv posts reddit even scanning interesting questions stack exchange complete list means üòä performing one two practices possible sure missing something important skipping rest flip side daunting ‚Äî okay spending hours daily browsing multiple platforms hoping stumble upon something interesting answer ml dojo help üññ context ready answer burning questions ml dojo ml dojo daily report latest upcoming research experiments topics articles papers ‚Ä¶ name field artificial intelligence machine learning ml dojo two main reasons subscribe 1 publish daily yes waiting weekly monthly newsletters 2 cover wide variety platforms make sure don‚Äôt miss anything important keep eye latest feed twitter reddit stack exchange medium coming soon short get best worlds daily üåé see guys üòÄ
rrn7xe,1,data image loading help pytorch hey guys couple months ago posted making image classifier school project far project mainly working albeit pre trained resnet model online need replace custom built model order make project complex i‚Äôve decided caltech256 dataset seems decent size still manageable process laptop i‚Äôm looking load dataset program seems first step i‚Äôm bit dead end really can‚Äôt seem get torchvision class caltech256 work matter try i‚Äôm fairly new python imagine doesn‚Äôt help also look trying manually i‚Äôm sure doable anyone guide right direction would much appreciated would prefer use less libraries possible beggars can‚Äôt choosers torchvision datasets lot beginner friendly that‚Äôs fine thank
rbgcu0,1,efficientnet mobilenet object detection recently seen paper would seem detection problem would require object localization classification researchers say use mobilenet efficientnet would possible mention localization step would scenarios mostly background ie zoomed enough skip localization previously seen software engineers use yolo detection pass cropped bbox mobilenet effnet definitely case possible use classification network detection problem
rl8jrb,0,milvus vector database 2 0 cloud scalable objectives milvus vector database idea milvus vector database first came minds wanted build data infrastructure could help people accelerate ai adoptions organizations milvus ai ml applications development cycle set two crucial objectives milvus project fulfill mission ease use ai ml emerging area new technologies keep coming developers entirely familiar fast growing technologies tools ai developers already consumed energies finding training tuning models hard spend additional efforts handling large amounts embedding vectors generated models mention manipulation large volume data always challenging task thus give ease use high priority since could significantly reduce development cost low running costs one primary hurdles ai production justify return investment would opportunities put ai applications production lower running costs would conducive lifting margin potential benefits design principles milvus 2 0 made start towards goals milvus 1 0 far enough especially scalability availability started development milvus 2 0 improve points principles laid new version include aiming high scalability availability building upon mature cloud infrastructure practice minimum performance compromise cloud words want make milvus database cluster cloud native evolution database clusters vector database new species database handles new types data vectors still shares challenges databases requirements rest article focus learned existing database cluster implementations thinking process designed new milvus group architecture interested implementation details milvus group components please stay top milvus documentation continuously publish technical articles milvus github repo milvus website milvus blog ideal database cluster let first list critical capabilities ideal database cluster 1 concurrency single point failure users connected different group members simultaneously read write access piece data 2 consistency different group members see data 3 scalability add remove group members go honestly capabilities hard acquire together modern implementations database clusters people compromise capabilities people expect perfect database cluster long fit user scenarios however shared everything cluster close ideal database cluster want learn something start key considerations database cluster shared everything cluster extended history compared modern implementations db2 data sharing group oracle rac typical shared everything clusters many people think shared everything means sharing disks far shared everything cluster one kind database member group users could connect one symmetric members access data everything needs shared making work sequence events group first group event sequence crucial resolve potential conflicts caused concurrent access different groups members usually use database log record sequence number represent event sequence time log record sequence number generally generated timestamp thus requirement group event sequence equal need global timer could atomic clock group would fabulous yet milvus open source software project means rely commonly available resources date atomic clock still premium option large companies implemented time synchronization component milvus 2 0 database cluster find link appendix global locking database locking mechanism resolve concurrent access conflicts whether optimistic pessimistic locks similarly need global locking resolve simultaneous access conflicts across different group members global locking means different group members talk negotiate lock requests several vital factors would impact efficiency global lock negotiation process speed inter system connections number group members need participate negotiation process frequency group conflicts typical group size 100 example db2 dsg 32 oracle rac 100 group members placed one server room connected optical fiber minimize transfer latency sometimes called centralized cluster due group size limitation people choose high end servers mainframes minicomputers much capacity cpu memory channels etc consist shared everything clusters hardware presumption dramatically changed modern cloud environment nowadays cloud data centers comprise high dense server rooms full thousands commodity x86 servers tcp ip connections rely x86 servers build database cluster group size increase hundreds even thousands machines business scenarios want hundreds x86 machines spread different regions thus implementing global locking might worth anymore global locking performance good enough milvus 2 0 going implement global locking facility one hand update vector data people rather delete insert instead update need worry multi writer conflicts piece data milvus group sharding arrangement meantime could use mvcc multi version concurrency control lock avoidance concurrency control method resolve reader writer conflicts hand vector data processing consumes much higher memory footprint structured data processing people looking much higher scalability vector databases shared memory data cache briefly divide database engine two parts storage engine computing engine storage engine responsible two critical tasks write data permanent storage durability purposes load data permanent storage memory data cache aka buffer pool place computing engine accesses data database cluster scenario member updated data cached member b could member b know memory data expired classic shared everything cluster buffer cross invalidation mechanism resolve issue buffer cross invalidation mechanism work similarly global locking maintain strong consistency across group members stated practical modern cloud environment decided lower consistency level milvus cloud scalable group eventual consistency manner way buffer cross invalidation mechanism milvus 2 0 asynchronous process shared storage shared storage probably first thing people would think discussing database cluster storage options also significantly changed recent years cloud storage evolution storage attached network san still storage foundation shared everything group cloud environment san database use local disk attached cloud virtual machines using local disk introduces challenge data consistency across group members also worry high availability group members snowflake made great role model cloud databases using cloud shared storage s3 storage inspires milvus 2 0 stated intend rely mature cloud infrastructure could utilize cloud shared storage think couple things first s3 storage cheap reliable designed instant r w access like database scenarios need create data components call data nodes milvus 2 0 bridge local memory disk s3 storage examples like alluxio juicefs etc could learn reason integrate projects directly focus different data granularity alluxio juicefs designed datasets posix files focus data record vector level vector data settled s3 storage answer metadata easy store etcd log data classic implementations log store also based san log files one database group member shared within database cluster failure recovery purposes problem got cloud environment spanner paper google illustrated implemented globally distributed database group paxos consensus algorithm need program database cluster state machine replication group redo log usually state replicated across group redo log replication consensus algorithms powerful tool substantial advantages business scenarios milvus vector database find enough incentives creating state machine replication group whole decided use cloud messaging queue platform apache pulsar apache kafka etc alternative cloud shared storage log store delegating log store messaging platform acquire benefits group event driven means many processes asynchronous improves scalability components loosely coupled making much easier perform online rolling upgrades improves availability operability revisit topic later section far wrapped crucial considerations database cluster jump discussion milvus 2 0 architecture let first explain manage vectors milvus data management performance predictability milvus stores vectors collections collection logical concept equivalent table sql databases collection could multiple physical files keep vectors physical file segment segment physical concept like tablespace file sql databases data volume small save everything single segment physical file nowadays constantly facing big data multiple segments physical files spread data different data partitions although data comes first rather indexes store data way index algorithm prefers make data access efficiently cases frequently used strategy sql databases partition range partitioning key values people usually create clustered index enforce partitioning key overall decent approach sql databases data stored good shape optimized prefetch still defects data skew partitions might much data others distribution real world data simple numeric range access hotspots workload might go data partitions imagine workload goes partitions data need rebalance data across partitions situations occur dba tedious daily life clustered index vectors also create clustered index vectors inverted list index case sql databases index built sql databases efficient access data index less computation less operations vector data far computation operations even index defects mentioned severe impact vector database clusters moreover cost rebalancing vectors across different segments high due data volume computing complexity milvus use strategy partition growth inject data vector collection milvus append new vectors latest segment collection milvus close segment size large enough threshold configurable build index closed segment meantime new segment created store upcoming data simple strategy balanced vector processing vector query process search similar candidates vector collection typical mapreduce procedure example want search top 20 similar results vector collection ten segments search top 20 one segments merge 20 10 results final 20 results since segment amount vectors similar index processing time segment almost identical gives us advantage performance predictability essential planning scale database clusters new paradigms milvus 2 0 milvus 1 0 implemented read write splitting sharding group like sql databases good attempt scaling milvus database cluster problems quite obvious milvus 1 0 sharding cluster milvus 1 0 r w node take total care latest segment including vector appending searching unindexed segment building index etc since collection one writer writer busy data continuously streamed system performance data sharing r w node reader nodes also problem besides must either rely nfs stable premium cloud storage expensive shared data storage existing problems hard tackle milvus 1 0 architecture thus introduced new paradigms milvus 2 0 design resolve issues milvus 2 0 cloud scalable vector database actor model two models program concurrent computation systems shared memory means concurrency control locking synchronous processing actor model aka message passing means message driven asynchronous processing also apply two models distributed database clusters stated high profile distributed databases use method redo log replication consensus algorithms synchronous processing using consensus algorithms build distributed shared memory redo log records different companies venture capitals invested billions bucks technology want comment started work milvus 2 0 many people regard technology way realize distributed database systems annoying say something people might misunderstand reckless distributed database design recent years redo log replication consensus algorithms overestimated database technology two key issues presumption redo log replication better fragile vendors mislead people expectations capability consensus algorithms let say two database nodes source node target node ever beginning exact copy data change operations u sql statements source node want keep target node updated simplest way replay operations target node efficient way thinking running cost u statement divide execution preparation physical work parts execution preparation part includes work sql parser sql optimizer etc matter many data records affected fixed cost cost physical work part depends many data records affected floating cost idea behind redo log replication save fixed cost target node replay redo log physical work target node cost saving percentage reciprocal number redo log records one operation affects one record see significant savings redo log replication 10 000 records worry network reliability one reliable send one operation 10 000 redo log records one million records redo log replication super scenarios like payment systems metadata systems etc scenarios database u operation affects small number records 1 2 hard work intensive workloads like batch jobs vendors always claim consensus algorithms could provide strong consistency database clusters people use consensus algorithms replicate redo log records redo log records consistent different nodes mean data views nodes consistent either merge redo log records actual table records even synchronous processing still get eventual consistency data views use redo log replication consensus algorithms appropriate places metadata system etcd messaging platform e g apache pulsar used milvus 2 0 implemented consensus algorithms said milvus vector database find enough incentives state machine replication group whole milvus 2 0 use actor model organize worker nodes worker nodes lonely talk messaging platform getting commands sending results sounds boring actor model asynchronous suitable scalability availability since worker nodes know impact worker nodes worker nodes join removed separation availability durability milvus 2 0 operation replay rather log replay vector database much difference operation replay log replay update function insert select function also much easier operation replay actor model multiple worker nodes might execute operation messaging platform according responsibility mentioned decided use s3 cloud storage shared storage layer milvus database cluster s3 storage reliable necessary different worker nodes write data shared storage thus designed three roles worker nodes query node maintains memory data view according assignment work query node includes vector search keeping memory data updated need write anything s3 storage memory sensitive node group data node responsible writing new data s3 storage data node need maintain memory data view hardware configuration data node quite different query node index node builds indexes segments closed data node size segments reaches threshold cpu intensive work group three types nodes represent different kinds workload scale independently call separation availability durability learned microsoft socrates cloud database
r332qa,0,iclr rebuttal deadline sorry find info online deadline 29th november anywhere earth specific time
r2x9eg,1,linear regression question code trying implement simple linear regression code explodes independent variable x gets high ranges code works small data sets try scale anything moderately large like 1000 data points x axis weight goes nan actually kind seems like makes sense based math tho let say two samples equal error one x 1 another x 1000 linear model 1 gradient loss equal 2 pred 1 2 1 1 gradient wrt weight wayyy bigger large x e g 1 x 1 1000 x 1000 missing
rboc5r,1,running model slightly different attributes question organize notebooks testing different document representations tf idf doc2vec sometimes make small adjustments model want keep results compare later sure whether open new notebook asking self taught feel like projects seems bit organized framework maybe organize machine learning code
rcy2tq,1,question model stacking working small personal project would like implement model stacking however approach would like proving unintuitive sure possible recommended stumbled across appropriate method yet plan split records one dataframe would good point mention working python contains numerical features second dataframe contains categorical features maybe even keep copy original dataframe course split target variable sharing index run selection models feed predictions meta learner read articles model stacking implementation via sklearn process seems involve initiating stackingregressor stackingclassifier depending task passing estimators final estimator fitting predicting etc typical fashion problem lies disparate dataframes seems able run single x process cannot tell stacker run models b numerical dataset run models c categorical dataset correct perhaps may manual workaround
r11355,1,evaluate model performance labelflow know firsthand important choose right metrics evaluate ai model performance check latest blog article walk evaluate model metrics use take example image classification model machinelearning ai evaluation
rc56gb,1,work data updates daily currently working ml portfolio project data updates daily projects worked date flat datasets data live updating model building pretty vanilla wanted challenge start pulling data api data fact updated example status feature contains values like received review approved appealed closed updates end day note frequency updates small values stay current status months end original plan build model one time deploy streamlit app thinking seems flawed model necessarily account changes data performance might drop time updates made would easiest way handle
rvyuhb,0,interviews usually like ml positions context applying phd level positions expect technical interviews including coding challenges similar swe advice prepping
qo5in1,0,gpt 3 longer game town hey put little article might find interesting gpt 3 longer game town catalogues appearance models akin gpt 3 course 2021 like hyperclova hope enjoy tldr organizations face significant challenges creating model similar openai‚Äôs gpt 3 nevertheless half dozen models big bigger gpt 3 announced course 2021
rl7jfz,0,oslo open source framework large scale transformer optimization x200b oslo framework provides various gpu based optimization features large scale modeling 2021 hugging face transformers considered de facto standard however best fit purposes large scale modeling yet oslo comes oslo designed make easier train large models transformers example fine tune gptj hugging face model hub without many extra efforts using oslo currently gpt2 gptneo gptj supported plan support soon information see
rdj5t7,1,trying implement neural networks scratch gotten class neuralnet init parameters sure continue please dm comment help please thank
r5iyb6,1,need help tutorial hi completely new learning machine learning watching video make first ai python going well ran model fit cell got error idea could issue external file outdated otherwise clue guys girls idea might wrong thanks alot answers good one
qv5nbi,0,interview raquel urtasun waabi best iccv papers computer vision news dear peek computer vision news november many articles ai deep learning computer vision html5 version recommended pdf version dilbert page 2 free subscription page 74 enjoy
rb84aa,1,anyone know increase weka perfomance linux use linux tasks edit dataset readings still use windows run weka permance issues linux linux takes way longer consumes way resources heap size enough program meaning heavy cpu anyone know fix get realy annoying reset pc everytime need boot windows run weka
r7xih4,0,productionize results data analysis project supposed prediction house prices usual started exploratory data analysis eda zeppelin reached point data enriched transformed enough would like add pipeline use ci cd source data going exactly arrive batches weekly basis deal rewrite stuff zeppelin notebook spark application add unit tests run ci cd pipeline correct approach assuming receive new data every week keep training model new data every week kind makes sense eda part double work mean eda taking code notebook application ideas done professionally
qulcqf,0,bayesian models perception action book draft upcoming book wei ji konrad kording daniel goldreich published mit press
r4e8he,0,machine learning wayr reading week 126 place share machine learning research papers journals articles reading week relates researching means elaborate give us insight otherwise could interesting paper read please try provide insight understanding please post things present wiki preferably link arxiv page pdf easily access pdf summary page way around pertinent links previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 121 130 week 1 11 21 31 41 51 61 71 81 91 101 111 121 week 2 12 22 32 42 52 62 72 82 92 102 112 122 week 3 13 23 33 43 53 63 73 83 93 103 113 123 week 4 14 24 34 44 54 64 74 84 94 104 114 124 week 5 15 25 35 45 55 65 75 85 95 105 115 125 week 6 16 26 36 46 56 66 76 86 96 106 116 week 7 17 27 37 47 57 67 77 87 97 107 117 week 8 18 28 38 48 58 68 78 88 98 108 118 week 9 19 29 39 49 59 69 79 89 99 109 119 week 10 20 30 40 50 60 70 80 90 100 110 120 upvoted papers two weeks ago u catalyzex code bot paper link besides rules fun
rgbl0q,1,markov diffusion operator hello reading paper authors mentioned markov diffusion operator
ri4pqb,0,understanding alphazero neural network‚Äôs superhuman chess ability summary paper acquisition chess knowledge alphazero common sometimes proven belief deep learning systems seem learn uninterpretable representations far human understanding recently studies highlighted fact may always applicable networks may able learn human readable representations unfortunately ability could merely come fact networks exposed human generated data demonstrate ability learn like humans simply memorizing human created labels necessary test without label following idea deepmind google brain teams together 14th world chess champion vladimir kramnik studied creature alphazero point view alphazero descendant alphago super neural network beat world champion lee sedol best five go match turning point history deep learning also seen wonderful netflix documentary alphago unlike alphago alphazero trained self play e learns play competing masters go also chess shogi trait makes alphazero perfect case study explore idea moreover given fact performs superhuman level understanding functionality also particularly useful highlighting unknown patterns never discovered chess theorists full paper summary leonardo tanzi paper
r93h2k,0,dealing missing values anomaly detection numerical data small numeric dataset around 400 attributes numeric 200 instances target categorical attribute 4 different categories indicating state one class represents normal condition 3 represent anomaly types instances representing normal state anomaly missing values however instances representing anomalies missing values attributes addition names attributes missing values correlate anomaly type e g instances anomaly class 2 attributes 10 400 missing instances anomaly class 3 attributes 300 400 missing would best way deal missing values case supervised unsupervised case sure methods going use would need cover supervised unsupervised cases
r8geii,1,paper overview n√ºwa visual synthesis pre training neural visual world creation video paper code abstract paper presents unified multimodal pre trained model called n√ºwa generate new manipulate existing visual data e images videos various visual synthesis tasks cover language image video time different scenarios 3d transformer encoder decoder framework designed deal videos 3d data also adapt texts images 1d 2d data respectively 3d nearby attention 3dna mechanism also proposed consider nature visual data reduce computational complexity evaluate n√ºwa 8 downstream tasks compared several strong baselines n√ºwa achieves state art results text image generation text video generation video prediction etc furthermore also shows surprisingly good zero shot capabilities text guided image video manipulation tasks
qplveb,0,maritime grand challenge abu dhabi came across maritime grand challenge thought others might find interesting combines drones robotics ai there‚Äôs 2m first prize 3m overall prize money open universities research institutions companies individual innovators here‚Äôs link video competition development real world solutions illegal fishing piracy smuggling coastline security first deadline initial phase includes white papers registration ‚Äì jan 31 2022 found story
qu1e0q,0,examples humans ml model working together outperform either isolation maybe interpretability explainability methods used ideal world automated explanations model outputs would make possible human expert classifier working together outperform either working isolation like ensemble except instead n models combine way 1 model 1 human chooses whether accept model output reject go opinion instead ‚Äúsuccess‚Äù setup outperforms model human acting alone classic literature paul meehl arguing allow human experts second guess outputs even simple regression based classifers like human expert loop help generally makes things worse intuitively seems like methods explainability interpretability could help human know defer model go opinion example maybe skin cancer classifier predicts particular lesion melanoma dermatologist see saliency map classifier paying attention right part image rightly ignores classifier particular instance whereas different case dermatologist less certain saliency map makes sense defer machine classification research looks whether combo human expert using ml model automated ‚Äòexplanation‚Äô ml model outputs saliency maps lime whatever outperforms human experts ml model isolation feel like super common evaluation paradigm interpretability literature perhaps i‚Äôm using wrong keywords i‚Äôm finding lot would especially interested cases ml model question language model anything realm would helpful edit thanks much great responses really pleasantly surprised sparked much discussion lots dig appreciate
qrm0o7,0,simple projects openai thinking making sql query generator english language ideas welcome thanks
r40m4u,1,i‚Äôm start producing circuitmess batmobile diy robot car teaches kids stem sciences hi everyone may remember name albert 22 year old diy electronics enthusiast also happens huge batman fan ü¶á earlier posted circuitmess batmobile project developed getting license warner bros aimed teach kids technologies future ai machine learning computer vision received really nice feedback people group wanted notify batmobile funded officially start producing kit soon ‚ú® crowdfunding campaign live 5 days meaning still pre order batmobile support project find interesting here‚Äôs link listing also get info batmobile‚Äôs software hardware thank support far
r4it3w,1,examples semantic search leveraged scale research able find resources outside faiss prototypes wondering something used scale examples sources would much appreciated
rvtxxh,1,need help first job ml engineer hello everyone recently graduated master degree cs got first job last month soon started realised even though hired backend engineer tasks would akin machine learning engineer company ad company task predict revenue ad using data last 4 months september december last year took data built feature vector tried basically regression models available scikit learn got r2 score 0 7 using 80 data training set 20 test set problem model works really well instances 4 months try predict revenue ad january fails miserably question deal mean model overfitting never seen example time period thank help
rwc7zy,0,classification imbalanced datasets question working medical classification project imbalanced tabular dataset 3 classes class 44 16 14 rows data respectively train random forest classifier see model predicting dominant class test instances time get around also recommendations give dealing imbalanced datasets thank
qw0rxa,0,organizing ml reproducibility ‚Äì reproducibility scale hey r ml lot said ranted reproducibility ml always seems like everyone agrees reproducibility important still many projects difficult impossible reproduce part reason reproducibility hard quantify different people define differently wrote blog trying provide concrete scale reproducibility practical rating system checklist help project creators maintainers validate reproducibility work scale consists 5 criteria 5 star system 1 code 2 configuration 3 data artifacts 4 environment 5 evaluation specific requirements one tried order growing level complexity 3 star project lower effort less reproducible 5 star project also add format reproducibility md file filled added projects help community assess reproducibility project want use check üëâ know might controversial honestly love open discussion ‚Äì think missing something order different would also love get pr format reproducibility md
rq1cnm,0,diffusion models beat gans image synthesis explained 5 minute paper summary casual gan papers dodging one long enough finally time make paper summary guided diffusion gans dominated conversation around image generation past couple years though new king might arrived diffusion models using several tactical upgrades team openai managed create guided diffusion model outperforms state art gans unstructured datasets imagenet 512x512 resolution among improvements ability explicitly control tradeoff diversity fidelity generated samples gradients pretrained classifier ability guide diffusion process auxiliary model also diffusion models skyrocketed popularity generative art community particularly clip guided diffusion sound good true wrong caveats approach vital grasp intuition works full summary blog post guided diffusion sota generative art model clip arxiv code subscribe casual gan papers follow twitter weekly ai paper summaries
rpnptp,1,product preference dataset looking dataset kind consumer product e g soda lot product features e g color sugar content bottle color etc used predicting personal preference ideas welcome sort consumer product soda seems easiest goal give father hands demonstration power nns observe correlations inane properties
ri4wl9,1,suggestions learning path hello everyone made ml dl learning path ask make comments suggestions learning path please note already finished codecademy data scientist path want keep learning ml dl first say want get hands dirty first weeks path instead spending weeks learn theory intention learn ml dl using ml dl start building portfolio courses specializations 1 practical deep learning coders fast ai 2 part 2 deep learning foundations fast ai 3 deep learning specialization deeplearning ai 4 tensorflow developer professional certificate deeplearning ai 5 aws fundamentals specialization aws next specialization 6 practical data science specialization deeplearning ai aws 7 tensorflow advanced techniques specialization deeplearning ai 8 code first introduction natural language processing fast ai 9 natural language processing specialization deeplearning ai books 1 hands machine learning scikit learn keras tensorflow concepts tools techniques build intelligent systems aurelien geron 2 ai machine learning coders laurence moroney x200b please feel free advise courses books thanks advance
rk1rql,0,simplefeature tiny python package almost dependencies lets extract deep local features check tiny python package requires almost dependencies lets extract generalized deep features image built older still solid work fg2011 anyone cool use something like
rq0b9k,0,using model checkpoints validation sets 5 fold cross validation issue suppose training neural network dataset performing 5 fold cv saving best model weights checkpointing improvements particular metric validation set evaluating later using validation set thought whether model checkpointing like mean theory cross validation give idea generalized performance unseen test set validation sets treated like would treat test set accessing training would appreciate feedback
rav68s,1,tqdm printing info x200b everytime print new progressbar created tried position 0 leave true way stay one progresbar
r47k8g,0,smart solution reduce impractical number iterations time series model hello folks timeseries based ml solution helps anomaly detection finding root cause anomaly currently operates see anomaly metric actual far prediction try examine associated dimensions associate anomaly root cause accordingly example considering covid related deaths across us model senses anomaly steep rise fall try break available data state see state responsible steep change example one dimension state model essence iterates 50 states hence 50 iterations try scale multivariate involves example 5 dimensions already existing state say add gender age group income group health rating number iterations going explode earlier 50 going become 50 states 4 genders 10 income groups 12 health ratings 24 000 iterations model detect anomalies say e commerce data number dimensions going larger hope see model scale least without parallelization guys think solve suggestion greatly appreciated thanks
rqjpre,1,negative gradient always go downhill gradient descent sgd always case negative gradient goes direction steepest descent
rhp5gj,1,learn python specifically ml know python basics started read book ml hands ml scikit codes python understand want learn python ml perspective please suggest resources
rkf0fh,1,yolov4 custom classifier hi starting computer vision project identify images fires mostly worked tensorflow keras try work darknet get yolo working annotated dataset get weights file get predict anything really tried working tutorials really helped people seem use weights coco hoping someone could suggest another resource help would willing connect help troubleshoot cs student would really appreciate help
qrn5qb,0,replacement softmax want activate multiple samples equally softmax used activating normalizing representation way important sample largest value 0 1 rest close 0 wondering want activate multiple samples equally representation lets say vector 1x10 dimension know 5 equally important want weight activation softmax wont work one option use sigmoid use softmax top make sure sum one sure smartest approach wondering people suggestion
r8lljf,0,implemented transformer transformer implementation recent transformer transformer paper uses pixel level attention paired patch level attention
rlewuz,1,face filter project using mediapipe p5 js hey everyone christmas coming tried make small 2d face filter web application using mediapipe p5 js youtube check note please use chrome order run getting css issues firefox good css anyone looking source code also share screenshot filters hope enjoy project cheers
r69q9n,0,check self made deep rl library written tf2 x hi delighted receive feedback deep rl mini library based tensorflow 2 mainly educational comes performance contains clear implementations classic rl algos dqn variants ddpg td3 sac policy gradient a2c ppo lib also dockerized mainly convenience beautiful people could give feedback better present would useful actual practical problems etc would nice
qynzlz,0,high level microblog federated learning wrote high level technical thread federated learning twitter thread found informative let know
qklvfp,0,bert scaled trained massive dataset like gpt3 architectures trained completely unsupervised gpt scaled bert software limitation
ruobs9,1,interesting post deep learning beginners wild cats image classification using deep learning wild cats image classification using deep learning x200b
rrd2tt,1,andrew ng ml course teach octave hello everyone trying ask lazy question tried google find answer quickly learn octave starting andrew ng ml course teach enough course eventually plan move python anyways thank
ql0a61,0,converting ml models plain code trivial done across one project m2cgen converting ml models plain code given even complex models broken series nested functions commonly done yes training complex inference passing input nothing dynamic sure performance suffer non streaming applications fine even complex classification network going take long run inference frameworks already parse graph pipeline whatever use matrix multiplications export plain code version know probably much easier framework devs implement rather someone external take bit completely portable computer code hosts model serialisation etc seems like good thing might imagine thinking make portable models integrating local software game engine little hassle possible
rx3vgj,1,mit opencourseware ml courses anyone working gone mit opencourseware courses intro machine learning machine learning latter graduate level course find experiences planning using knowledge research machine learning reading handouts listening videos working hands stuff
r5vyec,0,getting started explainable ml good resources get started explainable ml thanks
rjblex,1,multi layer neural network scratch go post covers concepts behind neural networks walks implementation multi layer network go performs quite well mnist dataset put post together focus better understanding backpropagation particular hope others may find useful
rdefe9,1,idea model use data like want predict next letter z like loop example b go next letter c go b bcdcbcbcdedcbcdcbcb yxyxyxyxwxy bcb yxy yxwxwxyxy yxwxwvwxwvwxwxy yxyxwvuvutuvutsrqpqpqponmnopqrqpqrqrqponmnopqrsrqrtutuvutututstuvutstututsrsrsrqrstutstrqrqrqpqpopqrqpnmlklmlkjkjkjihgfedcbcb bcb yxy b yxyxwvutsrqrsrqrqrstutuwxyxy bcdcdedefedefgfdedefghihgfghijijkjihgfedckjkjkjkjihihiklkjihijihijklkjijijkijijkjihgfghghgfghghijijklmlmlklmlkjijihghihghijijihghghihgfedededcbcb xy bcb bcb yxwvwvwxyxyxwutsrstutsrqpqponmlmnopqrsrqrqpqpqponopqrststuvwvututstutstsrqpqpqpononlmnopqpqpopqpqrstsrqrststsrsrqpqrqpqpsrststuvuvutsrqrqrqrsrqonmnonoponmlmnmlkjijihghihihihgfedefefgfghghghghgfdefefefedcbcbcb b bcdedefghghghihghghgijkjkjijijkjkjklkjihihghijihijihikjihghijijkjijlmlmnmlkjijihihijhghihijkjlmlklmnonmlmlkjkjijijkjkjijihijkjihgfgfghgfefghijijijijijihghgfghihghijijijihfededcbcb bcbcdcbcb b bcb yxwxwxy yxyxwvwvutstsrstsrqrqpqrqrstvwvwvuvwxyxy yxy b bc b ywy bdcdcdcedefefgfghghgfefgfededefhghihijijkjklklkjmlmnmlmnonmnopopqrqrqpopqonmlmnopopqrqrststststutststutusrqrqpqrqrstststuvuvuvwxy yxyxyxwvuvutstststuvwxyxyxyxwvutuvwxwxwxyxwvuvuvtsrsrqrqrststuvwvwvwyxwvwxy b bcbcbcb bcdededefghghghgfghihihgijijihghghgfhijklmnmnopqrqpqrsrstsrststutsrqponoponoponmlmnonmnopqrqpqrsrststuvwvwvwxy b yxyxy bcdedefghgfefhihghihghihghgfededededefefghihghghgfgfgfefghghihgjihghgfededcbyxy yxyxwxy b b b b yxyxyxyxwvwvwyxyxy bcbcbcb yxyxwvwvwvwvututuvwxwxwxwxyxyxwvwvwxwvuvuststsrqrsrsrsrqrqrsuvututututsrqrqpopopqrqrqrqpqpnmnmlkjihijijklkjkjijijkjijkjkjihghgfdcdcdedefgfghijklmlkjihgfdefgfedefgfefefghghgfghghgfedcdcdcbcb yx b bcbdededefghgfgfgfhghihghghgfghijijijklkjihijihihgfgfghihjihgfgfgfededefgfgfededcb b yxy bcdededcbcdcdedcdefdefefefefghijijihghghgfedcbcbcb b bcdcbcdedcb yxyxwvwvuvwvutstststststuvututsrsrststsrqrqrstuvwvuvwxwxwxwxyxyxyx yxyxyxy bcbcdedcdedefghghghgfghihghghghghihijkjijijijihgfgfgfgfededefghijklklmlmlmlklmnmlmnononopqpqponmnoponopopopqrqrstsrqrqrqponononmnonpqrsrstuvuvutututsrqpopqpqrsrststutstututstsrqpqpopqrqrsrstsrqrqrqrqrststuvututsrqrqrqrstutsrststutuvwvututsrqrqrqrstutuvuvuvwxy bcdcbyxyxy bcdedededcbcdedcdcb yxwvwvuvwvutsrsrqpqrsrqponopqpqrqpqrqrqrqpqrststsrqponopoponopqrpqrqrsrstststsrqrsrsrqpononmnmnonoponopqponmlklmlmlmlkjijijkjijijkjkjklmnononmnmlkjijklmlmnonononlmlklmnopqrsrstsrqpqrqrqpqrstutststutstvwvusrqpqrqrqrqrstututsrqrsrststsrqpqpqrqrqrqrstsrqrsrqrsrqrqrqrqpqponmopqrstututstsrststsrqpqrsrqpopopqrqpqrqrqpqrstututsuvwxy bcbcb yxy ywxwxwvwvwvwvwvwvutstsrststutuvuvwvuvwxyxwxy yxwxyxy bcdcdcbcdefghghghgfgfefghijkjkjijihijhghgfghghgfghghjkjkjijijijklmlmnopopqrqrsrqpqpopqrqrsrqrqpqrqponopqrsrqrqrqpopqrqrqrqrstsrstututststsrqponopqrqrstsrqpononmlklmnonopqrstutsrqpqrstststsrqrqrqrstststutututstuvwxyxy bcdededcbcbcdcb b bc bcbcb yxwvwvtsrqprstsrstutuvwvuvwvuvwvwvututsrqrqrsrqpqsrsrqrqrstuvwxy yxy b yxwvwxyxyxywvwvututstututststsrqrqrqsrpqpqpqrqrstsrstststststsrqpqrqpononmnopqpqrqpqpqrstststststutstsrqstutututstsrsrqrstuvwvwvutututututuvwvutsrqruvwvpqrqrqpqrsrqpqrstuvwvwxyxyxwxwvwxyxwvwxyxy bcdedefgfghghgfededefghikjihihihijihghijijijkjihgfefedefedededefghihgfghghihgfefgfefededcbcbcbcb yxwxy b bcbcdededefedcbcdedefghijijijijihijijijijkjihijkjihgfgfededededc yxy yxyxy yxyxyxwvutststutstststsrqpqpopqpononopopqpqrstutsrqrqpopqronmlmlklmnmlmnmlmnonmlmlmlmnmlkjkjklkjihijklklmlmnonmljklmnmlmnopqrsrststutsrqrqrqrqrstutsrsrqrstututstsrsrqrsrqrstututstsrqrqrqpqrqpqrqrqrqststuvwxwvwvwxwxyxwvwxyxyxwxwxy b xyxyxy yxwvwxy bcdefghgfededefedefgfgfghijihghgfgfghghghihijihghihijijihgfgfghgfghghihghgfedcbcdededefghihijijijkjijkl
rftcum,1,good part time data science ms programs bad gpa
radyhw,1,distributed training parameter servers vs mapreduce watching andrew ng video data parallelism talks using mapreduce update weights distributing training several gpus thing tensorflow parameter server approach way understand parameter servers several gpus sending gradients server aggregates sends back global weight updates seems essence ng video mention separate parameter server gpus somehow combine results see e g timestamp 12 18
rr61kh,1,üíäyour daily dose machine learning different types gans series posts post almost daily call ‚Äúyour daily dose machine learning‚Äù several types generative adversarial networks gans past years here‚Äôs quick summary ùêÅùêöùê¨ùê¢ùêú ùêÜùêÄùêçùê¨ first form gans generator discriminator competing ùêÇùê®ùêßùêùùê¢ùê≠ùê¢ùê®ùêßùêöùê• ùêÜùêÄùêçùê¨ extension gans conditional sample generation allowed controlling specific modalities data generation ex generate face less beard ùêñùêöùê¨ùê¨ùêûùê´ùê¨ùê≠ùêûùê¢ùêß ùêÜùêÄùêçùê¨ alternative algorithm training gans wasserstein distance used also techniques like weight clipping made training stable ùêÉùêÇùêÜùêÄùêçùê¨ cnns used instead mlps image generation ùêèùê´ùê®ùêÜùêÄùêç progressive growing gans increment generator discriminator networks gradually helped generate high resolution high quality images ùêàùêßùêüùê®ùêÜùêÄùêç enabling gans learn disentangled representations control different aspects output eyes color nose shape hair type ‚Ä¶ ùêíùê≠ùêöùêúùê§ùêÜùêÄùêç gans generate images text ùêèùê¢ùê±2ùêèùê¢ùê± image image translation conditional gans example turn real images cartoonish images ùêÇùê≤ùêúùê•ùêûùêÜùêÄùêç got rid need pairs images image image translation case pix2pix ùêíùê≠ùê≤ùê•ùêûùêÜùêÄùêç extension progan generating high resolution facial images ùêëùêÇùêÜùêÄùêç gans time series data cnns replaced rnns recurrent neural networks accommodate nature type data ùêìùê¢ùê¶ùêûùêÜùêÄùêç another time series gan new techniques introduced stepwise supervised loss autoencoder connect favorite social network
rf95xz,1,nltk lemmatizer nltk lemmatizer pos tagger require active internet connection executing code
r9n6rs,1,someone please give breakdown math study machine learning related data science hi 20 south asia sl think studied pre calculus basic graphs statistics matrices studied biology chemistry physics due unavoidable reasons high school mind always stuck starting information systems bsc degree months working data science intern data automation basic ds ml good mentor guiding lack math knowledge affecting much however know understand certain functions work matrices dl want good really love field want enter field deep learning ofc ds ml unfamiliar math since virtually interactions past 5 years quite comfortable computer science especially python know beginner level study linear algebra statistics probability differential calculus know sections aforementioned fields study thinking hiring private tutor gain least basic knowledge within next 4 5 months tutor asked exactly wanted study know hence request could please give structure topics study beginner level e g x200b linear algebra topic 1 topic 2 statistics topic 1 topic 2 possible familiar topics suggested within mentioned time frame could please suggest good books self learning advice suggested resources books sites greatly appreciated thanks lot contribution feel free ask questions
rlixkx,0,mindall e pytorch implementation 1 3b text image generation model trained 14 million image text pairs hello introduce open source project released checkpoint text image generation model dall e link example 1 text image generation mindall e example 2 text image generation mindall e naming project mindall e originated mingpt mindall e transformer 1 3b params trained cc 15m addition larger model 4b released soon best knowledge first model supports english check project enjoy generate various images conditioned texts
qsv83w,0,theoretical analyses success alphago zero wondering whether theoretical guarantee convergence network used alphago zero optimality searching algorithm
raaipx,0,integrating self attention convolution tsinghua huawei baai‚Äôs acmix achieves sota performance cv tasks minimum cost new paper integration self attention convolution research team tsinghua university huawei technologies ltd beijing academy artificial intelligence proposes acmix mixed model leverages benefits self attention convolution computer vision representation tasks achieving minimum computational overhead compared pure convolution self attention counterparts quick read integrating self attention convolution tsinghua huawei baai‚Äôs acmix achieves sota performance cv tasks minimum cost code pretrained models released project‚Äôs github paper integration self attention convolution arxiv
rbdq2o,1,neural network determines gender severely blurred images 88 accuracy first let say layman passionate ai ml hobby programming skills experimenting image classification program intuitive lobe got website 35 male 35 female faces blurred quite bit labeled f accordingly lobe claims 88 accuracy training tested 10 test images roughly confirm earth possible website mentioned creates faces completely randomly image section always skin color age head position hairstyle different model manage find generalized criteria judge pixel mud attach 2 examples f sorry layman speaks answer obvious still welcome explanations x200b key question computer extract info gender complete pixel mud x200b x200b male 1 female 1 male 2 female 2
rkewa3,0,machine learning superstitions things believe zero explanations mine random seed 0 gives bad results random seed 42 good results even valued k k means insightful segmentation
rva0ey,1,autonomous driving hi guys advanced books material autonomous driving perception decision making field thanks
rl0jmt,1,model learns training data test data train keras model witch similar alexnet audio data witch transformed normalized mel specrograms equally distributed validation dataset get good train validation accuracy 80 90 test data lears class idea problem
regev9,0,automating process finding fixing bugs ml code hi everyone noticed regularly spend 30 50 time debugging machine learning models time ml models work even bugs really careful introduce first place attempt speed debugging process tried using static linters pyflakes pylint looks like work syntax errors basic result decided take matter hands build ml model fixes bugs ml models please let know think would really appreciate feedback
rmfog1,1,alternative google colab anyone know good alternative option rendering vqgan clip projects cant get colab pro country im looking another option
rvv6f8,1,proposal simple batch scheduling local gpu given list say predict jobs best keep 1 gpu busy one cmd time per gpu want simple solution one workstation skipping pbs slurm like solutions idea lightweight cmdline interface update seems work fine gpu q py batch file batch file looks like predict py params1 predict py params2 meat import threading queue q queue simplequeue open cmds r f cmd f q put cmd import subprocess def worker gpu true try item q get block true timeout 10 except empty break item item replace predict py predict py gpu str gpu subprocess call item shell true range n gpu threading thread target worker args start
qkef2i,0,good guide roadmap deeplearning large datasets clouds good guide roadmap deeplearning large datasets clouds around 50 200gb data npy format feed tensorflow pipeline preprocessing takes hours completely offline change pipeline structure
r74ci2,0,adding classification layer clustering model correct current project given list clients segmented using clustering model use kmeans get decent clusters well defined issue comes new client inserted database needs added one segments thought adding classification layer using data cluster information training test first attempts logistic linear svm decision tree resulted overfit downsampling analysis finally managed overfitted svm doubt still rose correct approach problem might sort data leakage occurring unintentionally biasing results aby insight highly appreciated
r81wi5,1,find good overview ai techiques use kind data combine different techniques pipelines taken ai courses university able grasp concepts struggeling make choices use especially comes creating pipelines lack orientation really gets way know goes well together rather avoided realize often easy solution specific use case always need fiddle around kind overview start try first certain kinds data would great help asking blueprint merely rough orientation tackle new projects
rkkqhn,1,feature selection advice recently working one way many side projects one current level knowledge ability slightly ambitious informal experiment regarding comparative performance various types models specific task especially hoping compare precomputed batch learning nn reinforcement learning method though also hoping expand categories variations upon base one working reasonably well since got game pretty much fully implemented bar refactoring reasons cleanliness stage need select features might relevant models ideally hoping use single universal feature set models comparability also need create training data hand ideas wanted ask advice miss something immediately obvious could beneficial refrence please consider screenshot game bit minimalistic know green square player character whereas red bars semi randomly generated platforms varying widths green number top score always equal number platforms highest platform player reached far game also features optional 20s timer intending use conjunction score benchmarking performance various models though screenshot note player leaves screen one side emerge game controlled using three keys left right jump want train model predict wheter pressed given frame possible features far considered include following 1 position player platforms one kind obvious though thanks fact always got 7 platforms 1 player screen x coordinates means already 16 features neccessity also considering wheter using relative measure position might usefull since helps generalise dataset eliminate need player position features 2 current time remaining models benchmarked fixed timeframe might relevant 3 current score think relevant tbh number feature considered advice course welcome though reiterate mainly looking select appropriate features stage especially anything might relevant problems considered case becomes relevant game implemented python using pygame intending use tensorflow ml parts
r6llo5,0,scalable gaussian processes compare neural nets discussions seen topic discuss scalable deep gps depth side discuss nns uncertainty estimates difficult draw solid comparison references would excellent possible
rgyhjb,1,filtering time series hi beginner first ml project python tensorflow lot time series wondering supposed filter case 1 sometimes noise data ex speed oscillating quickly 5 ans 20 clear reason except sensor sensitivity wind gusts honestly can‚Äôt say supposed right answer case 2 times time series pretty stable impulse nan 0 value i‚Äôm bit rusted signal processing tried add lowpass filter it‚Äôs ok oscillations around clear value it‚Äôs enough case 1 lowpass filter great impulse know use another filter impulse problem take time implement that‚Äô ok also wonder really filter data could add column np diff myvariable example model would see impulse wrong comes wind sensor mean std deviation skewness etc values could great know error sensitivity think believing magic kind overfitting really filter time series feed ml model thanks helping
rhu6fw,0,researcher regularly review basic math ml etc using anki spaced repetition app flashcards ml linear algebra prob stats review one advantage anki brings choose knowledge remember since reviewing basic cards bring immediate advantage wonder better discard cards learn knowledge actually need invest times expecting save lot time someday review basic subjects regularly simply learn need
rvn3dh,0,sieve processed 24 hours security footage 10 mins semantically searchable per frame hey everyone i‚Äôm one creators sieve i‚Äôm excited sharing sieve api helps store process automatically search video data‚Äìinstantly efficiently think 10 cameras recording footage 30 fps 24 7 would 27 million frames generated single day videos might searchable timestamp finding moments interest like searching needle haystack built visual demo link little back we‚Äôd love get feedback it‚Äôs 24 hours security footage api processed 10 mins simple querying export functionality enabled see applications better understanding data figuring data send labeling sampling datasets training building multiple test sets models scenario try videos visual dashboard walkthrough
rwq3tb,0,normalizing flows distributions finit support need learn map gaussain distribution gamma distribution custom parameters distributions sample evaluate probability density first thing came mind using normalizing flow approaches include log target probability density evaluation loss function obviously normalizing flow sometimes returns negative values term equals infinity positivation functions top nf break bijection properties regions space theoretically numerically defenetely nf approach inapplicable box simple problem missing something
rxj5d6,1,looking black box neural network hey guys recently started working research project analyzing cancer prediction algorithm hoping get alls advice algorithm described paper effectively uses cnn amino acid sequence data cell receptors determine whether responding cancer algorithm performs remarkably well even public cell receptors removed indicates biochemical difference cancer non cancer cell receptors responsibility analyze neural net determine specific features heavily weighted determining difference cancer non cancer cell receptors hopefully leads us specific biochemical difference bit lost start however would go looking black box advice would much appreciated
rjckcw,0,optimizing model input based desired output hey would like hear opinions regarding research lab working bio inspired small aerial vehicle resembles fly suggestion use ml principles enhance performance one option related framework provide input motor via controller say sine wave input maximize measurable output example robot altitude question would best maximizes one could think problem mathematical form suppose robot represented function r unknown us r goal would find arg max r somewhat trivial suggestion go options find maximum value r space inputs big anyway trying classify task known frame ml related tasks find answer last thoughts maybe spark discussion thought problem sort cross validation input checking sound like bad idea another thing problem seem supervised even unsupervised input changes perhaps rl also input thought sequential still think rnns really suitable supervised per se read thus far thanks would appreciate comment
ripn6f,0,human loop drive ml model improvements data compute one way improve ml models direct user feedback may best way rapidly improve ml model human loop necessary mt truth existing training data set perfect complete comprehensive produce algorithm consistently produces perfect translations link inconvenient truth ai every successful deployment ai either one two expedients person somewhere loop cost failure system blunder low deeplearning linguistics data neuralnetworks
rlxggy,0,since gradient continues decrease training loss decreases need decay learning rate always bothered write question point point make clear 1 weight update sgd gradient times learning rate 2 initially training loss high gradients naturally going high usually use high learning rate step 3 training progresses training loss falls gradients also become smaller usually lower learning rate training progresses 4 reducing weight update learning rate gradient 5 interestingly adaptive optimization methods adam normalize gradient second order moments kind counteracts effect gradients becoming smaller sure though 6 question need decay learning rate
qliy7s,0,natural language coding co pilot stream 11 2 10pm pst kinda late party got access github copilot ai backed code auto complete tool pretty impressive things played hour pretty impressed 10pm pst today streaming twitch tv evanthebouncy 1 2 hours attempting perform simple coding exercises writing comments letting co pilot complete code natural language inputs fun come spam ideas case chance play yet also giving commentary reactions work program synthesis living pretty cool piece tech definitely change people think programming near future mod kinda spammy feel free delete thread idc
qkfuzn,0,phd phd made following post subs posting get input larger machine learning community hi recently completed research based masters computer vision currently working company computer vision researcher current role requires lot paper reading improve existing models really like research satisfied current role following questions 1 decide pursue phd able save money next 3 4 years better 4 years phd 4 years research job experience 2 long term goal get job big companies like google facebook computer vision roles big companies require phd multiple publications join companies without phd 3 company encourages publishing papers let‚Äôs say publish papers next three four years would help competing phd degree holders would still need official degree 4 hard get admission good uni years research experience publication record would thankful someone could comment questions
rge3b1,0,new dataset text classification domain adaptation social media dataset 22 500 labeled documents across four different domains find
relhfv,0,difference posterior distribution predictive distribution posterior predictive distribution trying understand difference three terms everytime read answer online back square one
r4mlvk,1,get basic lstm example working created basic scenario see effects development model hit snag including lstm layer see code without lstm assuming inputs 100 correct model produces 100 accurate forecasts however including lstm layer forecasts go way since inputs sum correct output model learn anything output correct answer time done wrong import numpy np import pandas pd tensorflow keras models import sequential tensorflow keras layers import dense input lstm tensorflow keras preprocessing import timeseries dataset array x np arange 100 df pd dataframe index x df input x df input addition 1 df target x 1 x df input input addition df target max bars back 1 batch size 2 train timeseries dataset array x iloc 0 40 values iloc 0 40 values max bars back batch size batch size validation timeseries dataset array x iloc 40 70 values iloc 40 70 values max bars back batch size batch size test timeseries dataset array x iloc 70 values iloc 70 values max bars back batch size batch size model sequential model add input batch input shape batch size max bars back len x columns model add lstm 1 kernel initializer ones return sequences true model add dense 1 kernel initializer ones model compile loss mse optimizer adam metrics mse model fit train epochs 3 validation data validation shuffle false test index index 70 outputs pd series index test index predictions model predict test x range len predictions outputs iloc x predictions x 0 0 test df df loc test index test df outputs outputs test df
rhppgq,0,research new library bayesian optimisation hyper parameter tuning research glad open sourced new library bayesian optimisation low high domains library includes hebo algorithm neurips bbo challenge used hyper parameter tuning lbo algorithm combines deep metric learning latent space bayesian optimization enable high dimensional opt arrive optimal molecules reducing 97 data demands compbo algorithm based jmlr paper efficiently optimize acquisition functions continue grow library contributions comments welcome please look share star repo find useful view poll
rlc7b7,1,machine learning andrew ng courseera would like learn machine learning upon research seems popular cs229 course says 12 weeks finish winter vacation 4 weeks long
rmewsq,1,linux desktop windows laptop data science student starting data science masters program january got hp z840 desktop dual 12 core zeon 256gb ram quadro m5000 along gen 2 thinkpad x13 thinking putting linux ubuntu mint z840 keeping windows x13 need office windows specific software thoughts plan seems like great combo looking thoughts setup running data science workloads locally also disposal work system unlimited aws access larger projects need
rpbgpz,1,simple post beginners machine learning scikit learn prepare data machine learning using scikit learn prepare data machine learning using scikit learn x200b
rgwroe,1,trying compute knn np array weights l2 distances np array labels k x would best way approach problem issues actual writing part understand theory computing knn vector weights array labels vector relates distance array labels alternatively resources could review better understand problem class taking want sure fully understand concept problem proceeding thank time would happy give information clarification
rhohk2,0,best way perfectly overfit dataset minimal variables hello guys approaches reach 100 training accuracy using minimal variables think binary searching variables nn although training nn guarantee perfectly efficient usage parameters similar thing done gbts guess
rgadbt,1,svm model interactive visualisation jupyter notebook code blog learn svm visualise svm interactively jupyter notebook
rjk88d,0,cheap conferences neurips last week first conference cost 25 dollars student good machine learning conferences affordable aaai 22 e g find little expensive 145 dollars students
rgpqix,0,exaggerate extent gpt 3 capable reasoning extent gpt 3 capable reasoning reasoning ability gpt3 shown article shocked experiment using gpt 3 api far less effective results article confuse trick making gpt 3 answering like result last comment gpt 3 produced one dumbest things ever read
r6b4uu,1,need regularisation l2 l1 norm logistic regression revising logistic regression notes came around loss minimization interpretation logistic regression argmin w log 1 exp zi 1 2 lambda w 2 zi yi wi xi summation 1 n know l2 regularisation used optimization function used find balance good seperating hyperplane decision surface weight coefficients large tending infinity overestimated seem intuitively understand regularisation working balance weight coefficients avoid overfitting underfitting also might misunderstanding loss function optimization part expression consider using regularisation ideally minimise loss function points correctly seperated weights corresponding features tend infinity value zi tends infinity results log 1 exp zi tending 0 minimizing sum correctly classified points plane infinitely big weights point comes incorrectly classified loss function value tend infinity makes working optimisation problem accordingly weights get readjusted smaller values sum loss minimized without need regularisation term really confused even need regularisation logistic regression yes regularisation term expression working towards balancing weights
qoyyy7,0,happened compressive transformers promised solve one transformer architecuter greatest weaknesses lack long term memory seem find bigger experiment using work
r2przt,1,get data related company cost equity capital list company identifiers gvkeys sic permno etc need associated cost equity capital mind method used estimate cost equity capital access standard databases like compustat crsp ibes auditanalytics eventus etc
raen75,1,training produce text file dipping toes world looking guidance community never used machine learning anything think use case hoping someone could steer right direction background programming background mostly c python languages plus shell scripting glue c embedded programming python general purpose programming language experience popular dashboard libraries python problem something like 100 examples configuration files mostly generated based contents 1 file 100 example test cases unique config source file pairing currently new configuration files filled hand could partially automate way past parsing files like see learn something new approach problem machine learning perspective specific types libraries training model consume produce simple text files appropriate use case machine learning training sample set small overthought problem would require work worth thoughts criticism appreciated
rudcli,1,remove missing values sklearn pipeline hi straightforward way implement list wise deletion missing values part sklearn pipeline want delete rows one values missing seems method sklearn like imputing way make class
rgsvpq,1,shot learning determine number query images class hello studying shot learning general know 7 15 query images allocated class however dataset use training minimum 2 maximum 200 pieces per class used one query image two questions 1 okay use one query image like 2 meaningful see good performance using fewer queries
r22tyr,1,nothing shows try create bounding box around object trained model jupyter notebook worked error 0 8 generated yolo weights file using create bounding box around image following code create bounding box using yolo ocv nothing shows image bounding box training worked sure know whats wrong code throw errors dog image shows box around
raobgu,0,research deep learning algorithms best forecasting covid 19 cases using exogenous variables research want forecast covid 19 cases using exogenous data exogenous data planning use time series data cumulative number vaccinated individuals change mobility weather possible incorporate variables univariate forecasting also looking incorporate data population density location time series data still include currently considering lstm really sure best algorithm kind algorithm suggestions would greatly helpful
r6kviy,0,image transformers overhyped metaformer need explained 5 minute summary casual gan papers unless living rock past year know hype beast vision transformers well according new research team sea ai lab national university singapore hype might somewhat misattributed see vision transformer papers tend focus fancy new token mixer architectures whether self attention mlp based however weihao yu et al show simple pooling layer enough match outperform many complex approaches terms model size compute accuracy downstream tasks perhaps surprisingly source transformers‚Äô magic might lie meta architecture whereas choice specific token mixer nearly impactful full summary blog post x200b metaformer arxiv code subscribe casual gan papers follow twitter weekly ai paper summaries
r9soc8,1,training accuracy vs test accuracy i‚Äôm little confused whether difference training dataset accuracy compared test dataset accuracy shows whether overfitting occurred say i‚Äôm training mlp using cross validation stop learning validation set accuracy increase tolerance value set number epochs intuitively look validation accuracy curve see increases remains constant however testing model training test data training accuracy percentage points higher question small difference indicate overfitting occurred better score training dataset compared test dataset generally indicate overfitting occurred
r247qp,0,dimensionality reduction dataset linear nonlinear features used regression data set 900 features 12000 examples believe features linear relationship target variable others nonlinear relationship i‚Äôm looking reduce input space save time training prevent fitting using either feature selection sort feature compression method believe pca question assumption linear relationships amongst features i‚Äôm currently looking using mutual information i‚Äôve never used method thoughts
rbbl88,1,image emotion classification researched subject found paper pleased results got dont give code model far saw equations explanations pretty much magic untrained eyes also found classifier really cant get work sure everything explain set isnt clear enogh anyone explain give lead appreciated
r5xx1d,0,extending trained gan hi would like know opinion extending already trained gan example lets say gan trained 3 types classes segment wanna add 4th type segment ideal approach keep already learned weights run training 4th class impossible gan forgot segment previous classes planning use pix2pix model thanks advance
rhkcl9,0,harm phd application worked applying hear fairly frequently around really succeeding finding evidence rationale usually get work away research lose student mindset position supposed apply phd programs year due personal reasons looking work instead wondering anyone also thinks statement title
qnktqk,0,league legends patch 11 21 game playing ai reinforcement learning supervised learning dataset dataset meant anyone would like try create deep learning agent either using supervised offline reinforcement learning play league legends dataset contains 72 games patch 11 21 last patch game ended early surrender games chosen game lengths guaranteed low kept dataset large download dataset go github link click google drive link dataset stored sqlite database file schema relatively self explanatory happy answer questions preliminary dataset demonstrates possible within next days dataset contain 1000s replays means 10 000s champions worth data time player plays champion edit database contains 191 early surrender games games ending 3 5 minutes dataset table shows top 10 champion occurrences within dataset champion nami 116 miss fortune 103 lucian 61 khazix 36 viego 35 lux 34 jhin 32 yone 30 camille 29 graves 29 edit 2 larger dataset containing 987 games targeting miss fortune early game first 5 minutes schema format first dataset also contains game objects recorded 4 times second games chosen getting games mf player lived longest gave dataset players overall 64 4 win rate roughly euw diamond ii edit 3 728 games also targeting miss fortune early game first 5 minutes schema format first second dataset brings total number games mf longevity datasets 1 715 1 715 games 5 minutes 60 seconds 4 frames second 2 058 000 frames total enough least create deep learning agent play miss fortune first five minutes game least basic level edit 4 another day another dataset 773 games mflongevity dataset uploaded also included jupyter notebook analyse data 191 earlyff dataset works completely standalone google colab feel free also run locally wish github link open notebook colab edit 5 depth explanation process used create dataset blog post
rb06pu,0,locally deployed model management software need deploy manage multiple computer vision models get models input deploy apis accessible another app computer really need cloud service like aws reasons get expensive quickly data using sensitive security camera video better leave computer computer running powerful enough run models faster response time since use internet everything place deployment time manually get hectic easily question software solution manage models get api links local usage try
rab8fe,1,discussion applications deeplearning questions related completed course deeplearning without background ai machine learning really understand made model affect like enough informations design models second thing told finished course anyone suggest ink page suggestions ideas projects types models application idea ann cnn rnn som autoencoder rbm expand ideas real life situation application
ru5xw8,0,nn actually overparametrized often read nn cnn overparametrized example resnet18 11m parameters cifar10 50k 32 32 3 153m data points overparametrized network cifar10 even mnist 60k 28 28 47m data points
r0k72x,0,walkthrough keras model internals includes distribution performance optimizations callbacks training loop source keras model class grown several thousand lines code makes incredibly challenging sift especially beginners wrote blog post walks keras model class works hood guide explains various pieces model class implements simplified version model class guide doubles tutorial tf distribute batched execution following guide able write fully optimized custom training loop post available companion git repo feedback appreciated happy incorporate changes provide deeper explanations specific components model class using keras many years may glossed details please let know helpful anyone
r3esql,1,paper overview florence new foundation model computer vision video paper abstract automated visual understanding diverse open world demands computer vision models generalize well minimal customization specific tasks similar human vision computer vision foundation models trained diverse large scale dataset adapted wide range downstream tasks critical mission solve real world computer vision applications existing vision foundation models clip align wu dao 2 0 focus mainly mapping images textual representations cross modal shared representation introduce new computer vision foundation model florence expand representations coarse scene fine object static images dynamic videos rgb multiple modalities caption depth incorporating universal visual language representations web scale image text data florence model easily adapted various computer vision tasks classification retrieval object detection vqa image caption video retrieval action recognition moreover florence demonstrates outstanding performance many types transfer learning fully sampled fine tuning linear probing shot transfer zero shot transfer novel images objects properties critical vision foundation model serve general purpose vision tasks florence achieves new state art results majority 44 representative benchmarks e g imagenet 1k zero shot classification top 1 accuracy 83 74 top 5 accuracy 97 18 62 4 map coco fine tuning 80 36 vqa 87 8 kinetics 600
r74784,0,working tensorflow affect chances getting research internships practicing ml using tensorflow 3 years looking research internships plenty rejections professors institutes primarily use pytorch starting wonder correlation rejections focusing tensorflow primary tool experienced note way trying say ideal applicant trying improve turns true problem switching pytorch want sure switch necessary planning learning jax flax research future due optimized xla support
rw8jxd,0,ray skorch distributed pytorch ray sklearn api tl dr train pytorch models large tabular datasets scikit learn skorch api hi r machinelearning principal author ray skorch library lets run distributed pytorch training large scale datasets providing familiar scikit learn compatible skorch api integrating well rest scikit learn ecosystem hood ray skorch uses ray train distributed pytorch training ray data handling shuffling large datasets ray skorch works tabular data currently use numpy arrays pandas dataframes ray data datasets pip install ray skorch switch skorch code ray skorch changing lines import numpy np sklearn datasets import make classification torch import nn pip install pytorch tabnet pytorch tabnet tab network import tabnet ray skorch import raytrainneuralnet x make classification 1000 20 n informative 10 random state 0 x x astype np float32 astype np int64 net raytrainneuralnet tabnet num workers 2 new mandatory argument criterion nn crossentropyloss max epochs 10 lr 0 1 tabnet specific arguments module input dim 20 module output dim 2 required classification loss funcs iterator train unsqueeze label tensor false iterator valid unsqueeze label tensor false net fit x predict proba returns ray data dataset proba net predict proba x pandas examples including ones bigger datasets found package experimental i‚Äôd love hear feedback package concept distributed training tabular data simple familiar apis comments suggestions bug reports hugely appreciated
rlgrnt,1,high school student ml let start saying idea trying research project one high school classes grade really taking beating issue basically super new ml research project trying predict solar flare activity using ml basically guy done know convert large dataset fits format csv preprocess go first link section called getting hands data data preprocessing basically know either one stuck like 2 weeks met teacher today visibly pissed made much progress also good programming make matters worse know lot ask anyone help three days either terribly fail well class
r6bttm,1,back basic linear regression python code 2 minutes one biggest fields application python days machine learning one first things anyone learns machine learning linear regression linear regression attractive simple understand least compared learning models also know implement efficiently importantly make sense results made 2 minutes video exactly topic design 2 minutes never exhaustive hopefully serve good intro topic give motivation dive deeper sounds like something interested know started wished resources like available would love hear think format
rl6pnt,1,task difficult intern recruitment process hey everyone background college sophomore trying pursue ml dl one startups college incubating wanted interns ml applied got interview round second task however receive next step towards selection kindly take ml dl use case choice create suitable gui integrate solution ensuring utility end user mind supposed unpaid internship certificate end time time given assignment 41 hours 2 days first internship startup asking much inexperienced edit conveyed task skill level thank everyone responses
rhdr2b,0,rudall e text image 12 billion parameter commercial version xxl 12b available press release project page russian english translation post 1 3 billion parameter free version rudall e x200b examples created using 1 3 billion parameter free version upscaled 256x256 1024x1024 woman rainbow hair sketch chipmunk semi abstract art
rjnwo2,0,sinusoidal kernel nearest neighbors partially ordered set want find n subsets set consists numbers least error original order values evaluated harmonic motion weird stuff motions frequency phase may different geometric lattice e elements subset may go away fixed window come back later order therefore change change frequency erroneously evaluate data gist ground truth manual way i‚Äôm hoping find vectorized solution uses target function number subsets find uses something efficient solve set membership like knn approach theory fft target window size make use imaginary part deconvolve set
rqi2zq,1,quickly train classification model text classification want use ml text classification app different sentences want classify content sentiment gpt 3 api need provide 100 sentences clarification classification done downside cost money use model api looked deploy model firebase pretty easy training model much harder descriped process gpt 3 model would appreciate tips helpful links dont want learn everything scratch justy quickly train deploy model
rmaqvu,1,bootcamp best way learn ml talk hello searched find scenario hoping canvass views experiences already 2 years decent fortune 50 multiple deployed projects paid professional ml belt sought go guy oddball problems got chucked turns like ml data trying make models predictive quite lucky oceans data play relatively easy softball problems try solve self taught google taught udemy taught moment major gaps knowledge know know every time talk ml teams throw 5 dollar words around handed vp data science role expected talk talk ml far potentially far talented folks much peter principle situation like stay ahead rather become incompetent potato pointy hair considering boot camp level ability formally discuss ml concepts implementation primary goal love tinkering stuff would benefit new hands techniques also mainly though need learn lingo others talk need broad shallow set talk rather specialization bootcamp good path sf bay area dozens better path take pausing sit degree program match time load appreciate thoughts matter
rgfo6n,1,build grid map x addresses hello guys anyone knows way build grid array 2 addresses may api googlemaps like link respective sql database addresses positions map convert latitude longitude like study points field geostatistcs kriging x200b thank
r9bwjd,1,getting better nn recent cs grad took several classes discussed neural nets also internship got job one tasks implementing testing new nn architectures task solved ml know building nn lof trial error hoping guidance continue learning books online courses could help become better building new nns choose architecture hyperparameters tricks like batch norm etc expose new ideas options try
rcuyax,0,paper application direct interpretation given hidden state rnn rnn optimized estimated resulting hidden state sequence used direct interpretation given based subject rnn applied
rbz69k,0,lazy regularization wgan gp training reading paper stylegan 2 talk using lazy r1 regularization applied every 16 steps training wgan gradient penalty significantly slows training anyone ever tried lazy regularization gradient penalty thoughts
rwc10x,0,implemented conformer convolution augmented transformer implemented google ai conformer convolution augmented transformer speech recognition paper achieves best worlds combining cnns transformers model local global dependencies improves local inductive bias transformers
rkxmt2,0,question collaboration data scientists engineers hello newby group wondering would mind elaborating potential demand ml operations collaboration data analytics departments data scientists first started data science last thing wanted integrate apis perform etl clean format data feature sets manage models fun calculating interesting statistical reports building running simulations wondering today data science landscape tools available folks still feeling problem problem exists folks handling particularly building production capabilities research models little background interested started developing 20 years ago interest data science particularly finance economics multi agent systems machine learning lisp time career application hosting platform engineering architecture took away objective gradually became unhappy building managing made choice get back passion started working platform underlying plumbing managing cloud infrastructure data integration processing personally could create multi agent simulators self optimizing recommendation engines admittedly behind knowledge really going data science market created open source project brother died would built even nobody else world wanted connected experience passion really needed something fun reason thinking demand might friend small ai consulting shop able work team us federal government project allowed work others model strategy implementation strategy initial model implementation notebooks rough libraries translated pluggable scalable application code open source software handling production ops recommendation engines got loved seems working well client project took back place loved software engineering wondering would good market serve starting new year start generating revenue would prefer area passionate
rtukp2,0,plug integrate gnn pytorch code base spark cluster anyone better explanation resources share plug integrate pytorch based gnn models pyspark similar cluster services
r3j2j2,0,simple hosted mlops auto scaling solutions i‚Äôm looking deploy vqgan clip models pretty large scale 30 gpus want explore options kubernetes aws gcp hosted mlops platform simply upload ml service docker flask platform completely take care scaling gpu provisioning essentially outsource scaling handling traffic product service minimal setup
r1kzu7,0,build knowledge graph neo4j transformers knowledge graphs essential information extraction article show build knowledge graph job descriptions using fine tuned transformer based named entity recognition ner spacy‚Äôs relation extraction models enjoy read questions leave
rsm6ec,1,error loading image installed necessary libraries still unable figure problem x200b please help figure problem
qqkwas,0,discussion iclr 2022 submission statistics statistics iclr2022 submission found 1 number reviewers submission submissions maximum reviewers 2 item total 7 reviewers submissions minimum reviewers 20 item total 2 reviewers 2 rating variance submissions highest rating variance submissions highest rating gap max min submissions lowest rating variance x200b x200b data found
rdg7fs,1,need help correct data type model predict sample code post detail hey everyone reading book techniques natural language processing github page simple implement bidirectional lstm sentiment analysis however since still new life figure correct data format use model predict reach model evaluate sample code tried using training data prediction always get error x200b know something shape data model predict know anyone look help bit data format use
qkb6ga,0,plagiarism case detected iclr 2022 news discussion x200b submission withdrawn authors program chairs posted desk reject citing serious case plagiarism happening figures tables look like lifted straight previous papers
rxgv6l,1,resources improve coding ml ai looking improve upon coding skills specifically ai ml would say relatively advanced terms ml knowledge however struggle implement papers concepts fly scratch mostly due weak coding also due little knowledge gaps well x200b wondering good resources tutorials implementing research papers scratch cool projects watched aladdin persson videos super good looking something similar x200b thank
rgiu1t,1,industry certifications curious industry certifications around ml ai fields like cissp cisa ccna ckad etc networking security everything else curious thing exists ml ai space thank
r70stj,0,feature store useful scenario recently came across feature stores mechanism operationalize ml pipelines commercial setting sounds good finding hard understand use exploratory data analysis example consider simple use case prediction house prices given geographical location source raw data remote server contains historical house prices columns features latitude longitude total rooms house size total bedrooms year construction small representation feature set data engineer one probably look raw data simple statistical analysis like 1 identify null nan values impute 2 identify co relation features respect target variable determine features dropped 3 identify unique count numeric variable determine remove feature column unique count certain threshold 4 delete duplicate rows 5 perform onehotencoding categorical data 6 identify remove outliers 7 perform dimensional reduction feature scaling assuming would performing first steps would performing steps would like know employing feature store would speed rather operationalize ml pipelines
r130k1,1,beginners helpful guide training deep learning models cloud ago friend mine contacted linkedin asked could give guiding train deep learning models cloud realized facing issue faced tried train models cloud first time issue lots resources online might take hours days find exactly need want take training code data put cloud order address issue wrote guide exactly titled beginners helpful guide training deep learning models cloud link article follow medium want kind articles
rcdbiz,1,version machine learning experiments instead tracking distributed versioning vs centralized tracking ml experiments dvc git ml experiments often get split git code experiment tracking tools meta information git manage compare experiment meta information still better code following guide explains apply dvc ml experiment versioning combines experiment tracking version control track ml experiments version instead managing separately keep everything one place get benefits like experiments code track meta information repository version like code versioned reproducibility save restore experiment state track changes execute new distributed experiments organize locally choose share reusing existing repo setup experiment versioning treats experiments code saves metrics hyperparameters artifact information text files versioned git becomes store experiment meta information article shows dvc tool push experiments like git branches giving flexibility share experiment choose
r85ozs,1,recommended sub learn ml first time opining sub see 65 memes 35 showing achieved ml question posts links useful articles books courses showing results mind preferred sub actually learn ml
rkasgc,0,easy framework pretraining lanuguage models project link hi ml redditors colleagues u seopbo made la nguage framework elf upervised l earning lassl lassl aims provide easy use framework pretraining language model using huggingface transformers datasets currently bert roberta gpt2 albert provided model continuously updated future also order see effectiveness code continue add models trained using model hub along downstream evaluation hope helps whoever want make language models make easy fast hope helpful someone cheers
r15mte,1,anyone used algorithmia deploy models unconvinced algorithmia advertises mlops platform data scientists provide easy way host models scalable rest api sounds like perfect solution data scientist hobbyist wants host models cheap worry devops gotten familiar questions base tier algorithmia requires host model request handling code github repository owned separate repository request handling code seems like strange pattern develop also encourage develop web ui another pattern feels forced also ominous section terms service says transfer ownership software algorithmia hereby grant algorithmia fully paid royalty free license use permit others use software feels overly aggressive forcing use source hosting unnatural development environment sketchy ownership clause reluctant continue using algorithmia anyone similar experiences algorithmia overly skittish misinterpreting ownership clause better repository patterns git subtrees people used better companies host models never even attempted leaving aws gcp hosting land
rsn8mc,0,mse difference magnitude tabular data deep learning need reading paper tabular data deep learning need looking differences mse various dl architectures xgboost ensembles tested table 2 particular yearprediction rossman data sets comparisons shown mse frankly values seem fairly close close example deep ensemble xgboost best yearprediction mse 76 19 node really close 76 39 several others within couple points differences really great given error squared would guess unless error 1 know exactly 76 bad good 76 vs 78 seems close enough sure dl xgboost ensemble worth trouble suppose correct answer would depends need accuracy vs cost training etc trying get intuition look dl models seem pretty close xgboost even furthest seem bad 83 vs 78 missing something question applies cross entropy comparisons less familiar metric
rnyeyl,1,free python courses udemy learn python 3 scratch python absolute beginners practical python python data science ‚Äì great learning starting python 3 programming absolute beginner
rs7juu,0,rust stable mature enough used production ml making rust based python wrappers good choice performance heavy uses internal ml dependencies 2021 hi ml engineer team using python extensively code using libraries c c backened couple specific situations happening right need really write non python dependencies libraries get far better performance cases people blindly jump writing c c wanted see would make sense go rust backend rust mature enough seen huggingface writing tokenizers rust badass tokenizers main inspiration asking question gone route hurdles look look forward packaging rust python hard dockerizing packaging environment hard made model deployment challenging thank
qkwk6j,0,correlate results extremely imbalance data performance relative random guess hi project work handle extremely imbalance dataset 699 cases positive outcome around 15 000 negative cases addition cases relatively hard separate cases domain knowledge experts struggling manual classification context asked explore different classifiers present poc report first tried naive approach dumped data used train validation test splitting stratification option models predicted 0 time maximize accuracy used sampling smote package python changed criteria area precision recall curve text set predicted correctly 30 21 70 false positive rate around 12 regardless possible model modification boundary predict 1 model predict higher score 0 65 example problems defining metric evaluate results data imbalanced 30 good addition false negative strongly worse false positive addition today presented results manager asked prove argue results better random guessing thought two things evaluate results randomly draw 1000 observations prior 5 equal 1 rest 0 randomly guess 5 1 compare results bootstrap scheme get distribution check model performance along distribution take examples test set assign number normal observations give hr experts classify manually compare results glad hear think problem suggestions thanks
rr8key,0,wrote program openai codex fixes errors x200b find program github ai generates mostly wrong solutions enough generated solutions actually working useful already helped installing right dependencies docker build even many desperate google searches help think
r9yjtq,1,minimum support hi basic question working data set told minimum support value 0 3 would mean help greatly appreciated thanks
qmhthd,0,difference top tier papers others hello guys although read various papers top conferences iccv eccv icml know difference first second tier papers top paper know obvious writing skills technologies understand yet x200b specific different way distinguish
qs7g4t,0,causality research ml scam warning controversial get wrong causal inference methods application areas observe bunch random variable want figure causal relationship rant method ml research recently getting exploiting term causality sake hype citations ml two main paradigms supervised learning rl work causality e g bernhard sch√∂lkopf judea pearl etc tells us impossible determine causal relationship variables observe without performing interaction therefore supervised learning cannot learn causal model need impose one period regarding rl tabular q learning guaranteed converge maximum expected reward policy period nothing else needs said however despite two fundamental statements currently growing hype general ml research causality completely fine causality research long focuses application area mentioned first sentence recent trend brings concept computer vision nlp etc things become vague quite fast exaggerated fact research causality already extremely vague deeply philosophical e g practical implication newcomb paradox computer vision causal model known even vision processing humans animals little understood moreover cv tasks inherently specified instance cartoon drawing elephant still elephant distribution ood class multiple classes talking causal relationship pixels patches concepts makes elephant ear elephant ear vagueness combined general trend ml throwing bunch overly complex math statements paper impress reviewers really concerning bet hundreds papers topic published next years contribute little understanding create millions self citations
rbo6s4,1,comparing bayesian linear regression model decision tree regression model would go comparing bayesian linear regression model decision tree model
rab6lt,0,pytorch distributed training libraries current options currently distributed training either use manual implementation torch distributed use pytorch lightning also nice bonuses like fp16 training also deepspeed however unsure deepspeed beneficial multi node training model fit gpu ram deepspeed would also bring benefits standard data parallel multi gpu single node training model would fit gpu ram practitioners insights libraries frameworks missing
r24q0v,1,creating numeric word representation input sentences resulting memoryerror trying use countvectorizer obtain word numerical word representation data essentialy list 160000 english sentences import pandas pd import numpy np sklearn feature extraction text import countvectorizer df train pd read csv data train csv vectorizer countvectorizer ngram range 1 2 token pattern r b w b min df 1 x vectorizer fit transform list df train text printing x x 160000x693699 sparse matrix type class numpy int64 3721191 stored elements compressed sparse row format converting whole array get numerical word representation data gives x toarray memoryerror traceback recent call last appdata local temp ipykernel 11636 854451212 py module 1 x toarray c users crrma virtualenvs humor detection 2 8vpiokuk lib site packages scipy sparse compressed py toarray self order 1037 none order none 1038 order self swap cf 0 1039 self process toarray args order 1040 flags c contiguous flags f contiguous 1041 raise valueerror output array must c f contiguous c users crrma virtualenvs humor detection 2 8vpiokuk lib site packages scipy sparse base py process toarray args self order 1200 return 1201 else 1202 return np zeros self shape dtype self dtype order order 1203 1204 memoryerror unable allocate 827 gib array shape 160000 693699 data type int64 example linked schikit learn doc page used five sentences thus x toarray seem returned array numerical word representation since dataset contains 160000 sentences error message seems resulting vocabulary size 693699 contains unique unigrams bigrams due ngram range parameter passed countvectorizer hence facing insufficient memory issue q1 fix thinking simply reject x separately transform mini batches shown correct x batch list df train 10 text 160000 batch size batches x batch encoding vectorizer transform x batch toarray x batch encoding array 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 dtype int64 x batch encoding 0 shape 693699 q2 thinking train neural network decision tree encoding humor detection guess wont great idea 693699 length vector represent single sentence right yes instead opt use unigrams fitting countvectorizer capture even minimal context words unlike bigrams ps creating baseline humor detection required use countvectorizer
re0oxk,1,much linear algebra required able follow core ml theory hi already know basics linear algebra high school wondering would enough able follow one ml lecture series like cs229 andrew ng cs156 caltech go linear algebra lectures professor strang first would really time consuming follow course completely along assignments
rsstqr,0,top arxiv machine learning papers 2021 according metacurate io 2021 almost books still couple hours go time writing top machine learning papers per month arxiv pre print archive picked metacurate io 2021 january 1 fruit fly learn word embeddings 2 switch transformers scaling trillion parameter models simple efficient sparsity 3 muppet massive multi task representations pre finetuning february 1 represent part whole hierarchies neural network 2 patterns predictions actions story machine learning 3 fast graph learning unique optimal solutions march 1 fast flexible human program induction abstract reasoning tasks 2 learning resize images computer vision tasks 3 prevalence code smells machine learning projects april 1 retrieval augmentation reduces hallucination conversation 2 getting point index sets parallelism preserving autodiff pointful array programming 3 nice algorithm nearest instance counterfactual explanations may 1 pre trained convolutions better pre trained transformers 2 content disentanglement semantically consistent synthetic real domain adaptation 3 klue korean language understanding evaluation june 1 scientific credibility machine translation research meta evaluation 769 papers 2 time aware language models temporal knowledge bases 3 multiplying matrices without multiplying july 1 deeptitle ‚Äî leveraging bert generate search engine optimized headlines 2 demystifying neural language models‚Äô insensitivity word order 3 reading race ai recognises patient‚Äôs racial identity medical images august 1 mitigating dataset harms requires stewardship lessons 1000 papers 2 program synthesis large language models 3 avoid machine learning pitfalls guide academic researchers september 1 physics based deep learning 2 finetuned language models zero shot learners 3 machine learning media bias october 1 learning high dimension always amounts extrapolation 2 non deep networks 3 lambeq efficient high level python library quantum nlp november 1 gflownet foundations 2 rebooting acgan auxiliary classifier gans stable training 3 masked autoencoders scalable vision learners december 1 player games 2 linear algebra transformers 3 ernie 3 0 titan exploring larger scale knowledge enhanced pre training language understanding generation metacurate io metacurate io continuously reads number sources ai machine learning nlp data science aggregates links stories therein scores according social score number shares likes interactions social media 5 days they‚Äôve entered system metacurate io retrieved 240 000 links 2021 1 124 links arxiv papers published last year
r6op6g,1,inter rater reliability metrics understanding cohen kappa often see subtle misuses interrater reliability metrics example imagine running search relevance task search raters label query result pairs 5 point scale relevant 2 slightly relevant 1 okay 0 slightly irrelevant 1 irrelevant 2 marking relevant vs slightly relevant big difference relevant vs irrelevant however irr calculations take kind ordering account gets ignored wrote introduction cohen kappa rather simplistic flawed metric good starting point understanding irr hope helps welcome feedback
robe2y,1,master project ideas hi masters project looking ideas final project aim nail ds concept also get exposure deep learning ai algorithms want build end end solution bi data engineering background initially nailed three broad areas reserach remote sensing cybersecuity algorithmic trading although open areas well long basic criteria met learning easy availability data suggestions topic would appreciated ps afraid take complex assignments well
rttp81,1,need learn machine learning beyond programming language started course machine learning python everything going well mathematics began appear leaved course another time would like take want start studying statistics probabilities huge subject know start learning machine learning
r8w3zc,1,neural network pipeline look hey third year cs ug decided learn ml neural networks make cool projects also help cv started classical ml algorithms like linear regression logistic reg classifying algorithms learnt math behind enjoyed like gradient descent works scale features etc implement using sklearn problemo absolutely hate feature engineering part find mundane boring maybe know start becuz suck plotting graphs sure tried kaggle competition titanic got accuracy 80 mostly becuz chose features properly given much time signed andrew ng deeplearning ai first course quiet like find really cool matrices learnt highschool put use stuff efficiently among things also maybe wrong find straightforward machine learning models chose one thing nn question working nn need good feature extraction like ml example image detection stock market prediction im talking harder projects feel like important since feeding model redundant info may slow training process even result even worse accuracy correct way feature extraction hell become decent also helpful guys give top view nn pipe line edit constructive criticism naivete appreciated long steers right path
rgksre,1,choose algorithm something thinking choose neural network xgboost random forest algorithm ml problem sorry noob question trying pick machine learning question
qlsp3s,0,apps real world would like connect ml models hi side project building tool connect ml models make jupyter notebook apps real world lines extra code example think building model predicts customer churn jupyter notebook make sure pulls new customers mailchimp account company make run discord slack bot sign early access tool building fast wanted ask apps think would awesome integrate machine learning models far mailchimp shopify slack google sheets thanks advance
rphw9x,1,good model seismic signal would good model regression problem inputs seismic signals csv form specifically something quite along lines earthquake prediction seismic signals thanks
rbqnxf,1,learning working ai simply want connect people field come join us discord community 20 000 members ask questions find teammates share projects find job offers much programming way fun learn work someone help ask questions brainstorm etc much benefit joining community field especially cannot find question looking stack overflow üòâ thing ai little less year ago created discord server anyone learning working field could come share projects learn together work together much community close 10 000 members unbelievable glad see growing see everyone active come join us field ai introduce
rqe2bt,1,python r following language worthwhile learn ml c scala java perl i‚Äôve seen javascript typescript used
qq75zu,0,choose optimizer many choosing optimizer training anns one critical design choices anns black boxes theoretical guidelines overall design limited mostly anecdotally strongly depend developers experience x200b comes optimizer hunderts available sgd adam specific ones feels like custom tailored optimizer every problem every architecture x200b hard come something general many optimizer choose optimizer project
rkla87,0,hyperparameter optimization noise robustness imagine propose novel method claims robust towards label noise e g classification setting sense generalization performance suffer much methods facing increasing degrees noise common scheme demonstrate behavior empirical evaluation vary noise degree training data report resulting test performance models trained data however many studies miss proper hyperparameter optimization conduct somewhat fair comparisons report way optimized parameters depth often become clear whether validation data think e g single train val splits employing cross validation makes even less obvious assumed noise training data cleaner representative test data distribution kind evaluation scheme noisy train val data clean test vs noisy train clean val test data would preferred
qwt9ni,0,graph neural networks lens differential geometry algebraic topology differential geometry algebraic topology encountered frequently mainstream machine learning new series posts show tools fields used reinterpret graph neural networks address common plights principled way first post series introduction x200b part ii discuss expressive power gnns topological message passing part iii deal geometric flows non euclidean diffusion pdes graphs part iv show squashing phenomena related graph curvature offer geometric approach graph rewiring inspired ricci flow
r74pmb,0,mito speed dataset manipulation coding x200b mito spreadsheet provided python library allows manipulate dataset simple fast way interactive way mito provides graphical interface within jupyter lab environment manipulate dataset mito 1 import dataset load new dataset file system browse among directories filesystem supported formats include excel xlsx csv 2 export dataset download manipulated dataset csv local filesystem 3 add column add new column dataset change column name well column values either enter values manually calculate values columns double click first row column insert formula 4 delete column erase column completely 5 pivot table build pivot table resulting table manipulated separately 6 merge two datasets choose among following merge types lookup left right inner outer 7 plot graph choose columns associated x axes choose one supported graphs
rdbkhu,1,forum show data get recommendations algorithm use create model around data
rkt0u9,1,last survivors washington army enhanced colorized using artificial intelligence
qsh3z0,0,best favorite format writing without conference mind usually ideas experiments draft manuscript etc afterwards edit format particular conference journal wondering among community preferred format writing something know sending yet something arxiv format visually easiest read kinda like jmlr format pretty plain pretty basic appreciate wastes lot space author list neurips seems like default definitive format find bars around title kind ugly ieee formats always present visually completely fucking impenetrable fan iclr format putting titles caps
r6dzjf,0,uniform sampling feature space hello question concerning master thesis thougth maybe someone help put simply supervised learning problem need predict regression target value based circa 20 features want go detail disclosure classical ml model like random forest regression project uses real world data model need generalize beyond test set prediction test set want split dataset training test set randomly uniformly sampling entire feature space simply spoken done calculating euclidean distances points feature space iteratively sampling points farthest away result uniform distribution feature space training test set kennard stone algorithm came without heard algorithm algorithm ensures test set lies within training set feature space even ensures outliers included training set thought process ml model need perform extrapolation test set yield better results performance shows data split indeed performs much better test set random splitting supervisor wants prove works better random splitting empirically anyone idea prove including outliers training set performing data splits uniformly sample entire feature space improve performance test set mean often read extrapolation hard machine learning models cannot come experiment show something unclear pls ask questions thank advance alex edit note called outliers points surface feature space think ball 3 dimensional feature space data points surface sphere included training set
qorekl,0,simple questions thread please post questions instead creating new thread encourage others create new posts questions post instead thread stay alive next one keep posting date title thanks everyone answering questions previous thread
r2ouvr,1,large dataset hey guys ai student new deep learning want work breast cancer detection using cnn architecture found ultrasound imaging dataset contains 437 benign 210 malignant 133 normal images image mask think sufficient amount datat train validate test model thanks advance
ru7drf,0,quick deploy optimize convert deploy machine learning models hello reddit releasing one oss projects quick deploy github blog post x200b early stage feel free contribute give star üôÇ
rfdieq,1,learn linear algebra traditional way python traditional way mean way taught undergraduate studies found fantastic resource mit intro linear algebra gilbert strang comparison course book codethematrix com teaches essentials needed ml seems take considerably less time finish also learning lot real applications means lot practical learning
r8n2dr,1,go exercises ml textbooks hello trying self study ml theory using text ‚Äòunderstanding machine learning theory algorithms ‚Äô follow content book pretty well i‚Äôm difficulties solving exercises hoping get advice 1 long work problem consulting solution manual 2 decide questions work 3 stuck short term goal get deep enough understanding ml able contribute research undergrad advice would helpful thanks
qqdkeo,0,gpt 3 style shel silverstein recently playing openai api beta gpt 3 used fine tuning api create model fine tuned corpus shel silverstein poetry resulting model generates whimsical poems based prompted title one example liked walking whale walking whale feel move swell feel mist come float feel rain cold mind like walking ground model quite good understanding semantics even flair vaguely metaphorical pick notion rhyming seems like would hard thing model learn given training data seen since rarely rhyme scheme affect conditional likelihood word question commenters rhyming dictionary could simply look whether two words rhyme instead trying infer latent attribute could update large nlp model like take account declarative knowledge examples discussion
rnfmrr,0,learning weights connections efficient neural networks research paper walkthrough neural networks computationally intensive memory intensive making difficult deploy embedded systems research proposes 3 step method training efficient neural networks lightweight deployed device yet retaining sota accuracy numbers paper summary paper link
rqm5y6,0,measure fairness without access demographic data hi working paper measuring algorithmic fairness cases direct access demographic data example want see whether lender discriminating particular race lender collecting releasing race data loan applicants 10 minutes work ethical ai space would great help hear community whether often faced issue practice think done mitigate survey link
rvol2l,0,reviewers score submissions nominated best paper award top ml conferences neurips icml aistats etc submitted paper aistats 2022 breakthrough outstanding contributions paper received 876 scores reviewers could improve 877 rebuttals chances submission enters short list best paper recognition average reviewers score ones getting nominated top conferences
qry8i4,0,palette image image diffusion models website paper samples x200b x200b
re6jy9,1,use matlab instead octave ml course wonder literally performance wise terms syntax
rde0ti,0,codeparrot ü¶ú train copilot scratch releasing codeparrot ü¶ú first project hugging face code base build large language models code generation scratch includes code clean github scale datasets train gpt 2 style models scratch distributed infrastructure evaluation benchmark openai humaneval dataset also releasing two trained models demos play models first project hugging face thought good opportunity talk difficulties behind scenes project usually much communicated release topic story distributed debugging hard first small experiments training gpt 2 models code dataset went well decided scale train first smallish model longer mysterious reasons training stopped hours error training loop continue even interesting repeating experiment happened always another step never observed experiments fewer workers hell going literally weeks debugging experimenting finally insight started log everything maximum verbosity possible interestingly training stop always coincided little debug message concerning sort retry always stop message appeared whenever stopped message small details matter used feature called streaming datasets library read data fly retry mechanism reading next chunk file fails turns many workers reading file one worker fails tries read tiny chance deadlock workers higher chance deadlock also explains never observed smaller experiments retries could easily avoided changing streaming settings later switched single data processing worker iterable datasets later resolved issue altogether also improved training efficiency 25 main takeaways totally underappreciated hard stressful expensive debugging sessions scale every experiment expensive might take long time fail limits iteration cycle considerably distributed systems also behave differently simple single process programs takes time adapt mental model make scaling training models simple sounds resources curious takes train model checkout blog post brief tutorial training codeparrot ü¶ú also code open source free use available interested details design considerations setting large dataset building efficient tokenizers architecture choices make sure look codeparrot ü¶ú chapter upcoming book transformers nlp
ra1brw,0,gold standard visualizing train vs validation loss using cross validation hyperparameter training divided data six equal parts five training parameter fitting cross validation last one hold test set five training sets used validation set model trains four randomly drawing parameters 20 times therefore training 20 different models moving next validation set means training 100 sets comparing 100 sets parameters wondering anything differently also trying plot training validation loss producing 101 graphs 100 training one final test ideas edit never mind see forest trees measuring loss validation runs hyperparameters best run used end plot loss curve best validation run final run
rtv9cd,1,ml learning advice 3rd year economics student completed statistics calculus mathematical economic based vastly matrix algebra microeconomics macroeconomics courses gpa 3 51 familiar web development almost 4 years also general knowledge programming currently taking econometrics algorithm design analysis introduction database systems courses interested quantitative analysis ml advices could give improve field also want pursue academic career
qwxil2,0,advcl protecting contrastive learning models adversarial attacks adversarial contrastive learning advcl new technique mit ibm watson ai lab make cl models robust without reducing downstream classification accuraction paper accepted neurips 2021 key highlights advcl replace two view contrastive loss optimization four view cl including adversarial perturbations high frequency component view pseudo supervision stimulus generation component uses clustering algorithms create pseudo labels training data advcl addresses cross task robustness transferability happens mismatch loss function cl training stage downstream supervised finetuning pivotal security ml applications data constrained rely cl analysis paper interview lead author full paper code
rgk8db,0,challenges quantum machine learning solve looking study quantum machine learning ph want understand solve problems understand bpp bqp problems it‚Äôs ability speed computation solve problems common life usually need understand something go job postings case quantum machine learning engineer see companies using qml chemistry agriculture help would appreciated
r5la61,1,valueerror using target size torch size 64 different input size torch size 64 2 deprecated please ensure size trying build binary classification model cnn however getting error code class parallel want nn module define layers present network def init self num emotions super init transformer block maxpool input feature map tensor transformer rectangular kernel worked better rectangular input spectrogram feature map tensor self transformer maxpool nn maxpool2d kernel size 1 4 stride 1 4 define single transformer encoder layer self attention feedforward network attention need paper 4 multi head self attention layers 40 512 40 feedforward network transformer layer nn transformerencoderlayer model 40 input feature frequency dim maxpooling 40 282 40 70 mfc time nhead 4 4 self attention layers multi head self attention layer encoder block dim feedforward 512 2 linear layers encoder block feedforward network dim 40 512 40 dropout 0 4 activation relu relu avoid saturation tame gradient reduce compute time using 4 instead 6 identical stacked encoder layrs used attention need paper complete transformer block contains 4 full transformer encoder layers w multihead self attention feedforward self transformer encoder nn transformerencoder transformer layer num layers 4 1st parallel 2d convolution block 3 sequential conv2d layers 1 40 282 16 20 141 32 5 35 64 1 8 self conv2dblock1 nn sequential 1st 2d convolution layer nn conv2d channels 1 input volume depth input channel dim 1 channels 16 expand output feature map volume depth 16 kernel size 3 typical 3 3 stride 1 kernel stride 1 padding 1 nn batchnorm2d 16 batch normalize output feature map activation nn relu feature map activation map nn maxpool2d kernel size 2 stride 2 typical maxpool kernel size nn dropout p 0 3 randomly zero 30 1st layer output feature map training 2nd 2d convolution layer identical last except output dim maxpool kernel nn conv2d channels 16 channels 32 expand output feature map volume depth 32 kernel size 3 stride 1 padding 1 nn batchnorm2d 32 nn relu nn maxpool2d kernel size 4 stride 4 increase maxpool kernel subsequent filters nn dropout p 0 3 3rd 2d convolution layer identical last except output dim nn conv2d channels 32 channels 64 expand output feature map volume depth 64 kernel size 3 stride 1 padding 1 nn batchnorm2d 64 nn relu nn maxpool2d kernel size 4 stride 4 nn dropout p 0 3 2nd parallel 2d convolution block 3 sequential conv2d layers 1 40 282 16 20 141 32 5 35 64 1 8 self conv2dblock2 nn sequential 1st 2d convolution layer nn conv2d channels 1 input volume depth input channel dim 1 channels 16 expand output feature map volume depth 16 kernel size 3 typical 3 3 stride 1 kernel stride 1 padding 1 nn batchnorm2d 16 batch normalize output feature map activation nn relu feature map activation map nn maxpool2d kernel size 2 stride 2 typical maxpool kernel size nn dropout p 0 3 randomly zero 30 1st layer output feature map training 2nd 2d convolution layer identical last except output dim maxpool kernel nn conv2d channels 16 channels 32 expand output feature map volume depth 32 kernel size 3 stride 1 padding 1 nn batchnorm2d 32 nn relu nn maxpool2d kernel size 4 stride 4 increase maxpool kernel subsequent filters nn dropout p 0 3 3rd 2d convolution layer identical last except output dim nn conv2d channels 32 channels 64 expand output feature map volume depth 64 kernel size 3 stride 1 padding 1 nn batchnorm2d 64 nn relu nn maxpool2d kernel size 4 stride 4 nn dropout p 0 3 final linear block linear softmax layer take final concatenated embedding tensor parallel 2d convolutional transformer blocks output 8 logits full convolution block outputs 64 1 8 embedding flattened dim 512 1d array full transformer block outputs 40 70 feature map time avg dim 40 1d array 512 2 40 1064 input features 8 output emotions self fc1 linear nn linear 512 2 40 2 softmax layer 8 output logits final fc linear layer self softmax nn sigmoid dim 1 freq embedding define one complete parallel fwd pass input feature tensor thru 2 conv 1 transformer blocks def forward self x 1st parallel conv2d block 4 convolutional layers create final feature embedding 1st convolutional layer input features pased 4 sequential 2d convolutional layers conv2d embedding1 self conv2dblock1 x x n batch channel freq time flatten final 64 1 8 feature map convolutional layers length 512 1d array skip 1st n batch dimension flattening conv2d embedding1 torch flatten conv2d embedding1 start dim 1 2nd parallel conv2d block 4 convolutional layers create final feature embedding 2nd convolutional layer input features pased 4 sequential 2d convolutional layers conv2d embedding2 self conv2dblock2 x x n batch channel freq time flatten final 64 1 8 feature map convolutional layers length 512 1d array skip 1st n batch dimension flattening conv2d embedding2 torch flatten conv2d embedding2 start dim 1 4 encoder layer transformer block w 40 512 40 feedfwd network maxpool input feature map 1 40 282 w 1 4 kernel 1 40 70 x maxpool self transformer maxpool x remove channel dim 1 40 70 40 70 x maxpool reduced torch squeeze x maxpool 1 convert maxpooled feature map format batch freq time time batch freq format transformer encoder layer requires tensor format time batch embedding freq x x maxpool reduced permute 2 0 1 finally pass reduced input feature map x transformer encoder layers transformer output self transformer encoder x create final feature emedding transformer layer taking mean time dimension 0th dim transformer outputs 2x40 mfcc embedding time feature map take mean columns e take time average transformer embedding torch mean transformer output dim 0 dim 40x70 40 concatenate freq embeddings convolutional transformer blocks concatenate embedding tensors output parallel 2 conv 1 transformer blocks complete embedding torch cat conv2d embedding1 conv2d embedding2 transformer embedding dim 1 final fc linear layer need logits loss output logits self fc1 linear complete embedding final softmax layer use logits fc linear get softmax prediction output softmax self softmax output logits output softmax self sigmoid output logits need output logits compute cross entropy loss need softmax probabilities predict class return output logits output softmax def criterion predictions targets return nn bceloss input predictions target targets optimizer torch optim sgd model parameters lr 0 001 weight decay 1e 3 momentum 0 8 define function create single step training phase def make train step model criterion optimizer define training step training phase def train step x forward pass output logits output softmax model x predictions torch argmax output softmax dim 1 accuracy torch sum predictions float len compute loss logits nn crossentropyloss implements log softmax loss criterion output logits compute gradients optimizer use loss backward update network parameters based gradient stored calling loss backward optimizer step zero gradients next pass pytorch accumulates gradients backwards passes convenient rnns optimizer zero grad return loss item accuracy 100 return train step
r5k7op,0,nvim dvc neovim plugin dvc data science machine learning teams moving dvc framework data version control avid neovim user wrote neovim plugin allows interact dvc stages files configurations within neovim nvim dvc nutshell plugin allows populate quickfix window pipeline stages metrics storage files like quick navigation likewise autocompletion enables easily reproduce pipeline stages interest look using dvc neovim may find useful early stage may encounter errors please point link repository
rwxt8r,0,legal use functions pytorch tensorflow anybody know legal use dropout batchnorm pytorch tensorflow due google patents two functions library avoided patent infringement implementation functions
rhxfwl,1,make simple stock price prediction google colabs hi simple csv file closing p 500 price x amount days date found trying make prediction sort using data going scikit learn estimator brings lasso thought would use something like ltsm anyone help appreciated
rvn3vp,0,knows paper address code especially spatio temporal curvature streaming
quta9z,0,get access gpt 3 api late 2021 trying get access gpt 3 api applied following guidelines form email cto üòá yesterday wait days go creative still super hard get access api thanks
r1wirr,0,aistats 2022 reviews early got caught guard morning aistats reviews woke got 5 reviews pretty amazing detailed high quality thank reviewing organizing
rg2g5v,0,cloud provider using gpu inference evaluated several options come lacking hostkey unreliable shoddy service gcp perform well expectations aws expensive
rdy6gz,1,fun way learn calculus programmer come realize learn things better put practice learn say person learns way learn calculus math general using learning approach use programming want go deep math interested machile learning need learn things graduate cs college lol
rbeq9o,1,projects beginners games hi months ago graduated ba psych recently applied phd programs psych interdisciplinary programs primary research researching social cognition games took calculus undergrad also couple years data analysis python things need learn order project highly likely happy learn like start ml projects various reasons interested beginner projects let mess game data datasets kaggle might mess far thinking sentiment analysis chat logs twitch sentiment analysis dota 2 chat data really interested analyzing toxicity within specific games communities think would good starter project anyone ideas resources love hear x200b thank
qmm9z7,0,hierarchical transformers efficient language models team google openai university warsaw proposes new efficient transformer architecture language modeling setting new state art imagenet32 autoregressive models
qzsrtq,0,area chair posts new review paper 6 hours author rebuttal deadline genuinely open advice feedback weird situation area chair submitted conference paper posts new review paper two weeks initial reviews released 6 hours deadline author make new changes pdf would contact program chair oddity circumstances situation complicated even fact two original reviewers paper marginally voting accept 6 yet responded rebuttals initial reviews believe addressed three original reviewers main concerns questions reviewer 2 even raised score points response rebuttal bringing borderline reject 5 one small suggestion raise score new review released anticipating positive responses original reviewers however new review posted last second bit concerned scope trajectory discussion forum diverted diluted result new reviewer opinions paper instead structured notably quite divergent three original reviewers opinions thoughts scenario instance would consider ethical release paper review less 24 hours author post formal rebuttal response
rbl2r4,1,seeking guidance hi jay aslaiya second year bachelor engineering computer science branch want pursue machine learning career dont know become good asking guidance road map pursue know things required known field dont know resources learn please guide would great get resources suggestions know python know maths next step resource guidance helpfull thanks lot
ru4tmp,1,predicting multiple targets multi class text classification hi currently working project requires predict category subcategories organic product based ingredients list brand description product anyone tell explanation 6 columns b c e f need predict e f based b c want predict 3 columns one go thanks advance
rivass,0,looking deep learning python program low light image recovery w cpu support macos looking scripts run cpu since cuda available ones finding cuda prefer use something existing rather try build scratch atm hardware training never created ml algorithms modified existing programs personally need raw files cr2 dng etc part larger project specifically recover rebuild detail remove digital noise artifacts streaking images underexposed dslrs nothing like exists supports cpu perhaps may know libraries could help building something like this‚Äîaside usual opencv numpy etc‚Äîor perhaps resources modifying cuda scripts function using cpu pytorch appreciate point right direction thanks
rhdisq,0,recommendation books information theory statistics want know f divergence renyi shannon kl wasserstein divergences math student book kind resources recommend non math students get general high level understanding metrics divergences
rkoj9z,0,open source dataset lacks annotations hi r machinelearning data scientist dagshub personally excited open source software open source data science part recent hacktoberfest decided host community challenge create curate open source audio datasets ‚Äì got 40 contributions awesome easily accessible want keep momentum continue contributing open source dataset ecosystem ‚Äì time focusing labeling challenge want make something useful everyone love hear open source dataset lacks annotations think help community limitation data must open source made available everyone love hear thoughts feedback
rakeii,1,want product recommendations online shop idea start model suggestions data need collect school coursework
rq6uih,0,ai methods algorithms except deep neural network promising heard ai methods except deep learning like free energy principle thousand brains theory hierarchical temporal memory tsetlin machines spiking neural networks predictive coding associative memory fractal ai updated hyperdimensional computing hyperbolic machine learning complex valued neural networks x200b think methods algorithms interesting become popular methods algorithms think promising x200b
r3krw2,0,paper explained implicit mle backpropagating discrete exponential family distributions video walkthrough backpropagation workhorse deep learning unfortunately works continuous functions amenable chain rule differentiation since discrete algorithms continuous derivative deep networks algorithms part cannot effectively trained using backpropagation paper presents method incorporate large class algorithms formulated discrete exponential family distributions deep networks derives gradient estimates easily used end end backpropagation enables things like combinatorial optimizers part network forward propagation natively x200b outline 0 00 intro overview 4 25 sponsor weights biases 6 15 problem setup contributions 8 50 recap straight estimator 13 25 encoding discrete problem inner product 19 45 algorithm distribution 23 15 substituting gradient 26 50 defining target distribution 38 30 approximating marginals via perturb map 45 10 entire algorithm recap 56 45 github page example x200b paper code tf code torch
ra89tu,1,starting masters data science i‚Äôm trying get ahead game learn much book online instructional youtube recommendations
rq9shn,0,project color detection wound detection project detects color wounds bruise cuts already implemented hsv detection satisfied result maybe range good research colorspace use program best colorspace read use lab colorspace good hsv result detection using hsv lab good resources topic implement efficient way
qpwdps,0,three reviews mean iclr 22 iclr 22 reviews hope guys got good reviews noticed papers got three reviews others got four five reviews papers get reviews signify
rpo9he,1,creating optimization algorithm cost function nn possible find article example new optimization algorithm cost function nn
qordhq,0,something took time learn benefitted saw thread cscareer questiosn thought great question could help lot people machine learning since much learn field could use direction
r6aja2,1,someone explain simpler terms paragraph means simpler terms sure nmf directly machine learning try someone explain simpler terms passage means paper reading nmf get formula got transformed 2nd formula
rtymaq,1,ml map predicting hello sure right place post give shot anyway play game called warframe really fun one ways progress open something called dragon vaults way go map search door randomly spawns around map normally takes 5 10 minutes find way long considering get thinking bunch runs maybe even crowdsourcing taking screenshots map screen labeling dragonvault feeding back ml see predict dragonvault things games actually random patterns noticed good pattern recognition
r3elk4,1,tips interesting school project ideas maybe read interesting paper done something recommend i‚Äôm familiar pytorch tensorflow doesn‚Äôt beginner level shouldn‚Äôt advanced can‚Äôt build scratch finish within month
r7lc1e,0,unsupervised outlier detection advise requested hi working strategy detect outliers mostly multivariate data using unsupervised methods currently using dbscan optics one group kmeans finding points 3 std mean group similar centroid 3 standard deviations second group lastly isolation forest copod third group output group could overlap groups since method finds different outliers part spectrum model executed point time data order find outliers moment respect values present time trained first step model applied incoming data since regular outlier values want consider point time incoming values regardless might happened past since values might affected time number factors sensible approach would suggest something different would add method either parallel ones mentioned end like voting anything else thanks advance
rt1vfy,0,play around stylegan2 browser built little page run manipulate stylegan2 browser pretty fun learning onnx port gans web play around random seeds also distort intermediate latents produce really wacky results check gif twitter let know come anything cool
rckmfb,1,would involved training machine convert complex images simplistic art style heard seen results programs generate images follow rules certain art styles creating fake pok√©mon sprites despite looking like depict anything clearly use style know software look images try figure contents makes curious would involved example computer look images fictional beasts like thousands mario enemies digimon using images franchise wikis try convert something follows rules
r1swhm,0,aamas 2022 paper reviews creating discussion thread aamas 2022 reviews
qphg92,0,amd launches mi200 ai accelerators 2 5x nvidia a100 fp32 performance source info today‚Äôs announcement amd revealing 3 mi200 series accelerators top end mi250x it‚Äôs smaller sibling mi250 finally mi200 pcie card mi210 two mi250 parts focus today‚Äôs announcement amd announced full specifications mi210
qn2jg5,0,effective way mix scalar value cnn feature maps feels like easy task seem recall find much info trying use scalar value try mix intermediate feature maps cnn know typically might concatenate kind scalars flattening feature map fc layer want value combined intermediate feature maps convolutions end whole cnn encoder categorical information know could use learnable embeddings add concat case continuous scalar seen suggestions treat scalar like bias term simply add look strong quite convinced also thought copying value h x w x 1 array concat next convolution sure either effective method
r8gbid,0,iclr 2022 possible competing bias review paper got 6663 reviewer gave 3 kept comparing paper concurrent work posted arxiv days iclr deadline asked us cite acknowledge explicitly compare concurrent work paper common likely reviewer author competing concurrent work reviewer also delivered lot deliberate attacks reviewing process really frustrated situation update pc responded review
rerlx0,0,question regarding downsampling convolutional networks every convolutional network kind encoder downsample steps transforms tensor shape batch size h w filters tensor shape batch size h 2 w 2 filters 2 always done one two ways using convolutional layer strides 2 2 filters filters 2 using maxpool2d layer followed convolutional layer filters filters 2 option never used concatenating maxpool2d layer convolutional layer strides 2 2 filters filters least apparent third option never used drawbacks
rrcav2,0,made modern data catalog tool anyone using word document excel sheet data catalog i‚Äôm curious anyone would like try old job seemed like new project new dataset every weeks hardest part job understanding data completing project last year built data catalog using code platform bubble shared ended quite people testing using personal projects last 12 months took original platform built leveraged open source platforms like amundsen rebuild modern data catalog focused making data documentation transparent collaborative straightforward anyone company sandbox environment dummy data looking user feedback anyone interested giving spin please let know planning release public version anyone use early next year happy new year appreciate anyone willing give try
qyxxcs,0,simple questions thread please post questions instead creating new thread encourage others create new posts questions post instead thread stay alive next one keep posting date title thanks everyone answering questions previous thread
r01mcs,0,source code hi guys looking source code paper reached one contributors wait response however methods obtain source code paper exist thanks
rvui3a,1,subreddit algotraders ml methods use hello guys ml engineer trying use ml trading even lot people say possible know people high profits since subreddit includes lot topics created subreddit algotraders using learning use ml stock trading subreddit
r1sqzw,1,start ml hi everyone 22 beginner knowledge python past learning experiences really interested world machine learning neural networks artificial intelligence know roadmap learning anyone could point steps maybe recommend good learning material courses really grateful
rkkpcg,0,much single 3d model replace dataset example sake argument love epoxy encased hotdog life likes travel around world want make sure safe every camera existence try track location times day night collect data training left took perfect 3d scan simulate angles lighting conditions resolutions occlusions optical parameters matter camera using find tolerance false negatives case baby ever needs help x200b put seriously 3d ml techniques evolve datasets like objectron co3d help us isolate objects simulated datasets begin play bigger part well good controlled experiments detection training really good renders vs real world data companies focusing custom data augmentation right
qvfn86,0,heterogeneous processor architectures machine learning recently introduced intel alder lake processors less relevantly application apple silicon take new approach processor design providing two types cores cpu die one side full sized high performance cores optimized single core performance high clock rates full support smt side smaller energy efficient cores make performance compromises yet aim capable parallel tasks machine learning community intel processors really relevant prototyping workstations equipped one two gpus don‚Äôt scale large production servers someone intending build machine like intel 12900k amd 5950x may interesting contenders processor choice deliver high performance without falling much expensive workstation class xeon threadripper processors intel slight edge single core performance synthetic benchmarks things unclear comes multi core performance clear winner emerges somewhat digressing also discussion new forward looking features intel platform pcie 5 0 ddr5 support ‚Äî set upsides also come early adopter instability price inflation ‚Äî versus stable affordable yet forward compatible status amd‚Äôs am4 platform far aware much discussion taken place heterogeneous architecture might translate training inference workloads lot press covering new processors focus gaming admit well versed ways cpu side parallelism leveraged tasks impact might performance obvious correct scheduling assigning performance cores threads actually need likely part ‚Äî meaning operating systems matter vaguely know certain models architectures rnns rl data related tasks augmentation pre loading strongly tied cpu performance would curious peoples‚Äô perspectives insights
rhl2pj,1,trying see anyone clue hidden screen sure right either new reddit would upload screen picture show speaking
rrxl86,1,self taught ml engineers non cs background currently 2nd year mechanical engineering phd research involves lot machine learning self teaching concepts research i‚Äôve thinking mastering want work already ‚Äî tired student budget i‚Äôm wondering anyone non cs background self studied way ml engineer role how‚Äôd long take thanks
r36vlk,1,could someone help understand parameter estimation finding optimal predictors contributes big picture simple ml model like polynomial regression please i‚Äôm wondering things fit together i‚Äôm taking first ml course university right it‚Äôs basically math course provide foundation calculus probability statistics linear algebra involved ml feel like don‚Äôt good understanding sections relate yet though example know parameter estimation use mle map bayesian etc estimate parameters different distributions know create optimal predictor minimizing expected value cost function example optimal squared error cost function conditional expectation e x x i‚Äôm sure fits together something like polynomial regression use things model relates fitting data coefficients polynomial found etc could someone please explain fits together starting initial data matrix things come together create final model able make predictions thank advance
rwrnk2,0,recommend funny papper like single headed attention rnn stop thinking head really enjoyed reading change textbook papers abstract leading approaches language modeling obsessed tv shows youth namely transformers sesame street transformers transformers bonfire worth gpu tpu neuromorphic wafer scale sil icon opt lazy path old proven techniques fancy crypto1 inspired acronym single headed attention rnn sha rnn author‚Äôs lone goal show entire field might evolved different direction instead obsessed slightly differ ent acronym slightly different result take previously strong language model based boring lstms get within stone‚Äôs throw stone‚Äôs throw state art byte level language model results enwik8 work undergone intensive hyperparameter optimization lived entirely commodity desktop machine made author‚Äôs small stu dio apartment far warm midst san franciscan summer2 final results achiev able plus minus 24 hours single gpu author impatient attention mechanism also readily extended large contexts minimal computation take sesame street
qt1pnz,0,giou loss generalized intersection union used stn module spatial transformer network model uses stn module number detection mean squared error loss would like replace giou mse take account much target area detected close individual coordinates close target wonder makes sense anyone tried insight
r7hd01,1,zero research deep learning vision depth courses google colab tutorials anki cards hey arthur final year phd student sorbonne france teaching graduate students computer vision deep learning made courses available free website x200b tree deep learning course yellow rectangles course orange rectangles colab circles anki cards start basics neuron forward backward pass gradually step cover majority computer vision done deep learning course extensive slides lot resources read google colab tutorials answers hidden never stuck finish anki cards spaced repetition forget learned course date even learn research papers published november also lot information good old models tell liked hesitate give feedback improve happy learning edit thanks kind strangers rewards nice comments motivate record lectures
qp9bra,0,project jorldy opensource reinforcement learning framework hello world reinforcement learning rl engineers kakaoenterprise south korea published opensource rl framework named jorldy join reinforcement learning framework developing jorldy opened helping rl researchers students study rl features jorldy follows 20 rl algorithms pytorch various rl environment provided algorithms environments run simple command algorithms environment easily added customized distributed rl algorithms provided using ray benchmark algorithms conducted many rl environment jorldy github link mentioned jorldy open source rl framework accordingly team wants work many people develop jorldy better framework would grateful use widely give us lot comments jorldy thank
qkvy6l,0,movie dataset movie summaries know dataset contains movie summaries know researchers legally allowed download imdb movie summaries research purposes
rc08a4,0,latest large dataset used benchmark feature selection models submitted paper aaai22 rejected one comments left reviewers used traditional small datasets used uci datasets read many papers domain none seem consistent datasets use many still using uci data paper field dimensionality reduction feature selection classification algorithms good large dataset use field research
qnhf5f,0,tf keras hugging face datasets edition hi tensorflow maintainer hugging face posted months ago big change making library make everything keras native people seemed like thought give another update changed since made couple big changes reduce amount duplicate boilerplate code common scripts massively love get people using new approaches get feedback happened last week episode hugging face story models keras models still write training loop use models layer larger model everything like remains unchanged incredibly convenient load model immediately compile fit gave examples post linked last week usually refers times less four months ago telling anything already performance reviews worry least delivered eventually new first big new change really nice integration ü§ó datasets unfamiliar datasets data equivalent hugging face model hub load uploaded dataset one line code way load pretrained model load dataset function nlp people using transformers audio vision everything else days kinds data check see example load dataset action standard workflow datasets transformers goes something like transformers import autotokenizer tfautomodelforsequenceclassification datasets import load dataset load pretrained model tokenizer model name bert base cased tokenizer autotokenizer pretrained model name model tfautomodelforsequenceclassification pretrained model name load dataset use cola dataset glue benchmark data load dataset glue cola define function tokenize data apply dataset tokenizer returns dict map add keys dict dataset columns def tokenize function dataset return tokenizer dataset sentence tokenized dataset data map tokenize function far good point problems start arise really hard get tokenized data model data often 1 quite large 2 jagged different samples tokenize arrays different lengths result want load data single dict np ndarray tf tensor end huge amounts padding bloats memory usage massively slows model way get good performance load random batches samples pad batch entire dataset basically required write custom training loop least python generator would work keras anyone using transformers tf love hear solving huge recurring pain solution solution added method tf dataset datasets basically wraps dataset tf data dataset time data padding want also updated datacollator classes work generate dataset like transformers import datacollatorwithpadding data collator datacollatorwithpadding tokenizer tokenizer return tensors tf tf dataset tokenized dataset train tf dataset columns input ids attention mask labels batch size 16 shuffle true collate fn data collator note data collator needs model tokenizer every god damned research group every god damned university every god damned country handles data slightly different way universal approach padding works hundreds different models guarantee though tokenizer comes given model pad method works model data collator use idea much pain saved method okay calm tf dataset bit easy lot people familiar tf data actually really cool tf data dataset pass straight model fit iterate loop get batches need compile model fit loss use great question brings second big change made models automatically compute losses suitable task way accessible keras words use tfautomodelforsequenceclassification model compute loss appropriate sequence classification tasks e crossentropy know loss need train gpt 2 problem tfautomodelforcausallm pretrained gpt2 wait stop advanced user want loss loss panic still use whatever loss want old code work exactly change compile model without loss interpret wanting default internal loss specify loss argument compile use internal loss addition applies using keras api like fit writing manual training loops using model layer larger model none relevant convenience easy disable skip loss argument exactly continue code samples need tensorflow keras optimizers import adam optimizer adam 3e 5 transformers work much better lower lrs model compile optimizer optimizer loss argument model fit tf dataset dataset loaded tokenized trained changes lines almost nlp task translation token classification summarization handled similar way want see bunch example notebooks tensorflow pytorch tf examples date new methods
raod73,0,tune learning rate large models know lr finder pytorch lightning fastai deepspeed large models 2b params methods two slow faster methods try thanks
ricr2w,1,gaussian likelihood loss equivalent mse loss vae tutorial appears author uses gaussian likelihood log scale reconstruction loss explains likelihood encountering image cannot view link code see gaussian likelihood function recon loss vae implementations typically use mse reconstruction loss instead typical justification equivalent gaussian distribution places seen dismiss mse reconstruction loss e g link states basically gaussian fixed variance understand implemented tutorial sampling gaussian distribution dynamic variance worth swapped gaussian likelihood loss mseloss perform well kl term basically went 0 almost immediately reconstruction loss original implementation though significantly better kl term kept increasing time rather new vaes bit bad statistics likely missing something reason discrepancy
r1c03v,0,paper explained parameter prediction unseen deep architectures video interview w first author boris knyazev deep neural networks usually trained given parameter initialization using sgd convergence local optimum paper goes different route given novel network architecture known dataset predict final network parameters without ever training authors build graph hypernetwork train novel dataset various dnn architectures predict high performing weights results show ghn predict weights non trivial performance also generalize beyond distribution training architectures predict weights networks much larger deeper wider ever seen training x200b outline 0 00 intro overview 6 20 deepnets 1m dataset 13 25 train hypernetwork 17 30 recap graph neural networks 23 40 message passing mirrors forward backward propagation 25 20 deal different output shapes 28 45 differentiable normalization 30 20 virtual residual edges 34 40 meta batching 37 00 experimental results 42 00 fine tuning experiments 45 25 public reception paper x200b errata boris name obviously boris bori 36 05 boris mentions train first variant yet closer examination decided like second x200b paper code
rotyst,0,repo many light weight self attention mechanism introduced hi guys searching many applications self attention reaching efficiently computation fast convergence reformer longformer bigbird could ask organized paper repo
rwxuxp,1,converting categorical variable city one hot vectors giving nan values currently working basic machine learning project revolved around predicting house prices given several different features house categorical variable using linear model city listing belongs convert cities one hot vectors keep getting nan values linear regression model throwing error anyone know happening attached code 12 numeric variables linear model categorical city variable last screenshot seethe nan values occur third data point columns part one hot vector anyone know happening x200b x200b code convert city one hot vector method one using get dummies pandas housing pd get dummies housing drop first true housing head x200b method two using onehotencoder sklearn performing label encoder lab enc preprocessing labelencoder housing city encoded lab enc fit transform housing city x200b confirming label encoding housing city encoded value counts x200b dropping city variable housing drop city axis 1 inplace true x200b one hot encoding one hot encode onehotencoder one hot encode df pd dataframe one hot encode fit transform housing city encoded toarray x200b x200b merging one hot encode df main housing dataframe housing housing join one hot encode df housing head x200b dropping city encoded variable housing drop city encoded axis 1 inplace true x200b x200b nan values x200b
re1s36,1,tutorial using image super resolution without photoshop video used github project
reaxwn,0,train decision making ais gradient hi want share latest article gradient train decision making ais revision summary paper recent advances leveraging human guidance sequential decision making tasks written lead author paper personally think quite interesting hope enjoy
ru2pcb,1,local explainability using shap feature engineering using shap explain model predictions particular instance wondering interpret feature engineering applied data think three scenarios 1 features dropped added case shap would simply give value feature indicating particular feature affected result 2 features dropped shap would output value features make sense say features dropped affect model output sounds silly sure statistically correct use shap case 3 new features generated existing features lets say generate bunch features original ones retaining original original b c new b c ab bc abc thanks advance
qqmhiz,0,mit ai researchers introduce ‚Äòparp‚Äô method improve efficiency performance neural network recent developments machine learning enabled automated speech recognition technologies siri learn world‚Äôs uncommon languages lack enormous volume transcribed speech required train algorithms however methods frequently complicated costly broadly used researchers mit national taiwan university university california santa barbara developed simple technique minimizes complexity sophisticated speech learning model allowing run efficiently achieve higher performance method entails deleting unneeded components standard complex speech recognition model making slight tweaks recognize given language teaching model unusual language low cost time efficient process minor adjustments required larger model trimmed size read paper checkout project 5 min read mit blog x200b
qw55w6,0,project overview methods text segmentation text segmentation task splitting text meaningful segments many good overviews online put together project outlines different approaches models exist evaluate models open source datasets used training text segmentation models full overview read outline
qnq3m4,0,open source project collect data multiple databases apps saas tools prepare ml tasks lots business data scattered across different databases apps rudderstack integrate data various sources activate data warehouse business tools ml operations unlocks potential applications data hard rudderstack e g ux personalization business analytics etc seeking feedback improve would want use ama
qstdsm,0,language model rugpt 3 13b apparently available download english translation rugpt 3 13b model contains 13 billion parameters capable continuing texts russian english well programming languages download page english translation sign try download file reference english translation
rfb0c0,1,designing cnn image want design cnn scratch using figure new concept getting little confused adding conv2d maxpooling layers go
relg9r,1,bridging gap theory trying kaggle finished andrew ng course coursera august also finished course university could try scikit learn tensorflow keras filters etc came kaggle complete loss start kind bridge help connect two tbh historically lost implementing theory comes practice
rsyiyj,1,shared weights different implementations hey currently implementing convolutional neural network rust goal compile wasm deploy browser far implementation matches tensorflow almost perfect accuracy slight errors probably unavoidable floating point computations f e 0 70710677 vs 0 07071068 goal port model already works python rust already weights trained python model simply load rust problem small numerical differences add harder harder layer first layer outputs might differ something like 1e 6 layers suddenly completely different values ideas prevent way training rust pretty unfeasible would basically write complete backprop training procedure ‚Äì conv implementations likely fast enough nice result x200b greetings
ruofql,0,gui based machine learning applications previously using azure machine learning studio classic course discontinued last month free ml applications new azure machine learning studio free school project aiming free simple suggestions maybe someone else using studio classic knows way around
r8d36l,1,split training set hi need little help one homeworks training data set testing data set two separate data sets possible split training data set 70 30 use testing datatset actually test model anything wrong believe 30 data would validation data set im sure anything wrong thanks
qt4k2c,0,quick history gans 8 years gan evolution intuition behind explained casual gan papers tutorial covers intuition behind variational auto encoder vae og gan stylegan vqgan telegram post blog post subscribe casual gan papers follow twitter weekly ai paper summaries gan tutorials
rchvfb,1,point hardswish ago swish got attention outperformed relu quite tasks far understood main benefits keeps nice properties relu differentiable everywhere fact bit like leaky relu negative values took away fully differentiable leaky relu using always worked great implemented pytorch always defined easy enough today found torch 1 10 hardswish similar values swish composition 3 functions much faster calculate far understand continuous points switches one functions another taking away one big benefits swish basically give fully differential function save compute questions understanding correctly errors misconceptions thinking title worth use hardswish swish bit compute actually make difference
qkert8,0,reusing parts open source code potential publication new open source code currently developing new method builds upon existing work literature order address limitations provide reasonable improvements already done earlier reached authors possible academic collaboration received reply work already published conference paper two years ago code available github regularly maintained also deployed pypi package installed using pip question clearly use certain parts work without plaigarising breaking copyright agreements extent acceptable rely people work producing new method especially open source since method working largely inspired existing method seems currently track adopt around 30 code follow general code structure oop layout
rcttt3,0,carbon footprint machine learning energy costs ai risen 300 000 fold 2012 2018 focus large language models like gpt 3 make worse reducing carbon footprint become critical need ai community huge models best way forward blog link x200b outlook ml training costs source ark investments llc
rev9yn,1,tutorials beginner friendly resources hi short i‚Äôm beginner whole ai ml world i‚Äôve beginner tutorials past week find excel project based learning i‚Äôm wondering go tutorials building text classification sentiment model anyone would recommend concerned libraries third party im programmer 15 years btw say beginner beginner mean specially ml read somewhat write python build experience
qkyyno,0,anyone check ykilcher video siraj raval interview love yannic video see point interview mean even interview siraj seemed like someone started learning machine learning mentions superintelligence digital organism god seems like imagines ml hollywood movie much like general person
rk7ffw,0,discussion gpt 3 trained achieve double descent phenomena question title also language models attempted create phenomena aid breaking sota benchmarks
rjbsxn,0,need recommendation demosaicing halftone cpu well versatile film grain detection looking existing ml programs preferably python based good demosaicing halftone printing patterns retaining bringing back lost details per last post finding ones require cuda would also like recommendations libraries may good tackling problem need attempt build something basic scripts straight image filters would like find build one robust working large batches large sizes work float32 rgb raw formats like cr2 dng metadata retained float tiff exr metadata needed film grain noticed ml programs seem handle denoising blanket number percentage anything exist setting target grain size channel independent grain attributes find old images tend get larger grain size think 1800s software needs pushed far remove causing image become blurry process thanks
r15jix,0,anyone used algorithmia model deployment tool unconvinced algorithmia advertises mlops platform data scientists provide easy way host models scalable rest api sounds like perfect solution data scientist hobbyist wants host models cheap worry devops gotten familiar questions base tier algorithmia requires host model request handling code github repository owned separate repository request handling code seems like strange pattern develop also encourage develop web ui another pattern feels forced also ominous section terms service says transfer ownership software algorithmia hereby grant algorithmia fully paid royalty free license use permit others use software feels overly aggressive forcing use source hosting unnatural development environment sketchy ownership clause reluctant continue using algorithmia anyone similar experiences algorithmia overly skittish misinterpreting ownership clause better repository patterns git subtrees people used better companies host models never even attempted leaving aws gcp hosting land
qlcj9r,0,best simple machine learning api service looking integrate machine learning application example want classify images users uploading site aware lot saas machine learning companies wondering anyone recommendations ones worked best basically want send data service train model able call api get answer model
r3bi9l,1,problems gaussian processes without quaternions problem involving gaussian processes use quaternions researching solution problem requires use gaussian optimization looking rotation 3d object maximize arbitrary score space problem unit quaternion score real scalar possible rotation think would better 1 use gaussian process h r 2 use gaussian process r 3 r cyclic kernels discontinuities space wraps around unit sphere saw lot papers using dual quaternions translations required case need find way rotate object maximize score don‚Äôt mind get euler angles instead quaternion uncertain best way go quaternions feel theoretically beautiful confident abilities implement combine gaussian processes 4d complex numbers hope gave enough information post give relevant answers otherwise feel free ask questions edit post
r04dcs,0,apply fid score gan generate time series instead images data hello working using gan generating time series data common use inception score frechet inception distance score evaluate performance gan however rely inception model designed 2d image data could applied 1d time series data straightforward way solve problem train time series classifier substitute original inception model like paper tsgan lacking enough data computation resource seems good choice another way using stft transform 1d signals 2d images apply fid score method 2d images might feasible solution know really make sense stft image absolutely similarity image imagenet inception v3 pre trained questionable whether result produced inceptionv3 makes sense Ôºõ‚Ä≤‚åí
rhcxmu,1,one better uva masters ds georgia tech masters data analytics see links currently software developer trying get data science ml i‚Äôve taking online classes company willing pay masters program choice one go uva georgia tech uva program gtech program
qm0upg,0,extended submission deadline ‚Äî evomusart 2022 conference good news submission deadline evomusart 2022 extended november 24th üôå still time submit work 11th international conference artificial intelligence music sound art design evomusart work artificial intelligence techniques applied visual art music sound synthesis architecture video poetry design creative tasks miss opportunity submit work evomusart evomusart 2022 held seville spain 20 22 april 2022 üíÉüá™üá∏ information visit conference webpage evostar org 2022 evomusart
rbc6qb,0,transformers might know physical world
r10z1o,1,finding optimal sell limits stock trade based historical data buy signals crypto know know following data inputs available historical data open low high close prices 4hr intervals eth usd buy signals close intervals ie buy appear pretty reliable 60 70 time q buy buy signals could start training system detect optimal sell limits trade ie rise 0 7 sell fall 2 sell loss etc coded quite bit new ml hope use tensorflow js open ideas guess conceptual starting points useful resources would helpful
rie3i9,0,collect speech datasets voice enabled technology good examples speech data collection technology alexa siri
rjye12,0,sudden increase accuracy specific epoch model learning convolution neural network recently reading papers related optimizers radam looking visual results papers found images showed sudden increase accuracy 80th epoch figure6 paper variance adaptive learning rate beyond 150th epoch Ôºàthe figure3 paper adaptive gradient methods dynamicbound learning rate Ôºâ matter kind algorithm anyone tell happened thank x200b radam x200b adabound
r1du84,1,forecasting using lstm pytorch hi want use lstm time forecasting problem actually want replace convolutional layers rainbow algorithm rl network lstm consider time series seen many examples internet none helped confusion input divided encoder part decoder part divided continuous variables categorical variables best way build architecture takes consideration types variables anyone share helpful resources also confusion loss function need anything like picture attached thanks lot x200b
qo03fo,0,google mum details google anounced mum several months ago blog post says uses t5 framework multimodal name multitask unified model help lot anybody know trained combines text images thing think train generate image captions finetune multiple tasks something like bigscience t0pp
r9lzpr,1,project advanced methodology robust optimal design base agent oriented approach means reinforced learning addressing hope set business contacts areas machine learning methods multi objective problems robust optimal design intellectual diagnostics systems uncertainty goals robust optimal design rod achieve maximum efficiency system developed reduction rejects manufacture batch products algorithm numerical solution multicriteria multidisciplinary rod problem base agent oriented approach means reinforced learning 1 values confidence intervals design parameters system manufacturing accuracy based available technological equipment confidence intervals regime variables measurement accuracy system prototype known 2 training sample alternatives analogs designed system generated set design parameters regime variables alternative include different subsets values design parameters regime variables objective functions values objective functions obtained cfd modeling processes analogs designed system supplement training set data prototype 3 searching robust metamodels processes form analytical dependences objective functions variables using discrete data analogs prototype 4 choice target values average values objective functions corresponding confidence intervals designed system assuming batch n systems manufactured 5 reducing dimension space design parameters based analysis informativeness metamodel variables sensitivity analyzes 6 searching solutions multicriteria optimization problem deterministic formulation 7 determination values average values objective functions corresponding confidence intervals prototype optimal variant according paragraph 6 assuming batch n systems manufactured monte carlo analyzes 8 searching solutions multicriteria optimization problem stochastic formulation results solving problem rational values average values design parameters need found system well corresponding confidence intervals required manufacturing accuracy ensuring conditions according paragraph 4 thus solutions direct problem calculating dimensional design chains obtained determine rational values average values design parameters control variables system well corresponding confidence intervals quantities need found according definitions accepted mathematics elements pareto set normal solutions obtained given target values average values objective functions corresponding confidence intervals 9 cfd modeling processes optimal variant system design parameters control variables obtained paragraph 8 calculation verification determination accuracy numerical solutions obtained using proposed numerical model paragraphs 3 8 comparison solutions using original cfd models 10 reinforcement learning supplement training set point 2 new alternatives based results calculations processes optimal variant system design parameters control variables obtained paragraph 8 11 continue process finding solutions multicriteria multidisciplinary rod problem according paragraphs 2 9 look figure based agent oriented approach means reinforced learning specified solution accuracy achieved given statements reliable confirmed experience using ‚Äúrod ids‚Äù software different fields activity look please articles presentations results section achievements publications www linkedin com mykhaylo ugryumov 63148313b right consider specialists development effective machine learning methods mlm‚Äôs solving ‚Äúrod ids‚Äù problems offer software order resolve specific problems calculations addition always open contacts discuss methodology computational methods realization form interactive compute decision making support software system ‚Äúrod ids‚Äù helpful company hope use mlm‚Äôs increase customer demand new versions software
r6hf44,0,papers mathematics deep learning simple classical ml algorithm find good list papers mathematics deep learning
rr34rv,1,freecodecamp math curriculums enough self teach math start ml anyone self teach math start reading books ml think freecodecamp math curriculums youtube channel already googled math know start learning ml linearalgebra bullshit guide linear algebra looks good calculus 1 2 limits differential integral calculus statistics probability big lapses school algebra introduction calculus also reviewed parts sure remember khanacademy one side understand math whole world big difference accent purposes learning math fcc curriculums prof university college understand research works google deepmind others like reserved masters phd interested machine learning tool solving applied problems clasification recommended systems spam filters etc grasp common data structures algorithms experience leetcode shunting yard algorithm example ps warmest wishes holiday season
rkrgq3,1,finding subject part sentence containing multiple words hi trying find ways extract subject sentence python tried spacy nsubj tag sort works associates single word subject example following text three conspiracy theorists walk bar want subject three conspiracy theorists spacy tokenization produces following three nummod conspiracy compound theorists nsubj walk root prep det bar pobj achieve mentioned something missing relatively new nlp help would appreciated thanks
rdtf5w,1,go studying gans looking prerequisites understanding gans familiar bunch ml algos level discussed intro stats learning comfortable probability linear algebra college level please suggest go 1 papers need start 2 familiar need deep learning network architectures cnn rnns 3 dl concepts need familiarise 3 deep learning book goodfellow good resource learn gans
rasfxo,1,parts khan academy statistics learn hey jumped directly book bit hard thought would learn python work c dev thought figure someway statistics khan academy thing lot content khan academy wanted ask units learn parts unrelated could skipped get everything statistics probably worth learning mean like parts actually useful practice
rkrcyh,0,inverting photodna machine learning microsoft photodna creates ‚Äúunique digital signature‚Äù image matched database containing signatures previously identified illegal images like csam technology used companies including google facebook twitter microsoft says photodna hash reversible therefore cannot used recreate image project shows photodna perfectly hide information source image used compute signature fact photodna hash used produce thumbnail quality reproductions original image x200b rough body shapes faces recovered photodna hash face thispersondoesnotexist com details photodna approach used invert photodna hashes blog post code available github
qw603s,0,sort path neural network take parameter space training spend training getting slowed going saddle points spend time overshooting minima finds going around chaotic orbit often reach local minima shot sample large gradient roll towards different local minima questions even answerable answers known awhile
ruk1ge,1,good fluid dynamics data repo hey everyone anyone know good repositories sensor data fluid dynamics matter vibration data trying use dmd find dominant coherent structures dataset limited funds experiments limited access sensors thank
raz2bt,0,eccv 2022 happening far little information eccv 2022 according schedule accepting submissions around early march information find old tweet announcing venue official website important dates anyone know going
r53277,0,cost distributed deep learning aws hi everyone trying learn distributed deep learning techniques parameter servers ring reduce found paper uber describe implementation ring reduce implementation looks fairy straight forward unsure costs problem access multiple gpus thought would use aws ec2 since never used used gcp fair amount distributed deep learning hoping someone could shine light costs using say 2 gpus initial idea train cnn using imagenet 1k would use s3 ec2 two basic gpus goal simply see much faster reach convergence rather achieving state art times would expensive personal project rather spend 200 gbp
r4pjqu,1,tensorflow cache dir requirements txt trying install tensorflow flask application write cache dir requirements txt file write
r8ce7f,0,discussion painful thing machine learning hi curious approach debugging models software engineer find code usually easier test spot bugs quicker ml always long process problem actually code datasets tools using pros cons compared manually
r3gz0z,1,making synthetic render small electronics realistic using real pictures dcgan hi first big dive machine learning first real big project stuck stage however leaded synthetic dataset real dataset gan taking inputs however matter long run gan final image seems grainy resolution getting better far normal shape synthetic images renders object different angles real images basically pictures irl object multiple positions taken fixed angles 30 60 90 etc currently put real images folder synthetic images folder without worrying position wondering problem things would love input guys 1 label real dataset images taken position together however confuses would make dataset smaller also exactly know use positions together since synthetic dataset positions correspond real dataset 2 reading cyclegans reddit posts suggest moving 3 something gans please feel free ask questions need properly stumped except apple simgan paper clue go common celeb face example used tutorials good headshots angle unlike electronics
rv7yos,0,measure accuracy knn imputation dataset best way impute missing values use knn go ahead like check kind accuracy form imputation specific dataset k used original solution follows 1 original dataset remove rows missing values 2 dataset impute nans randomly throughout dataset frequency missing originally store values replaced nan new dataset ground truth 3 impute using knn 4 check accuracy imputed values ground truth values stored step 2 using mae different k values easier way using mae another accuracy score
r7lq0u,1,resources practicing advanced maths data science machine learning familiar basics solve easy ones encounter difficult questions recently faced job screening test get stumped want resources learn basics want resources try solve see solutions difficult maths questions preferably context data science machine learning tried googling found entry level courses
r79oqv,0,aaai manipulated reviewer scores without reviewer permissions professor makes serious accusation review manipulation surprisingly realaaai chairs updated review without permission never experienced rethink whether submit review conference anyone info shed light
rcemnh,1,feeding mri data spherical cnn hi first time posting currently working master thesis involves detection certain diseases using mri scans brain tissue individuals using spherical cnn dividing brain left right hemispheres mri surface pial sphere curve files hemisphere free surfer get create 3d point cloud map using nibabel open3d pial sphere files files also contain co ordinates mesh triangles curv file contains list values whose number equal number points point cloud map way display surfaces read pial sphere file display like shown freeview part free surfer display free surfer files save output freeview form numpy array read using nibabel screenshot zip
ro8p1o,0,dataset containing similar looking objects wild seen dataset like 10 images car 10 images car b b looks similar almost looking exactly
rh0j05,1,variational autoencoder tf2 typeerror trying implement vae mnist using convolutional layers using tensorflow 2 6 python 3 9 code data pre processing steps input image dimensions img rows img cols 28 28 load mnist dataset x train train x test test mnist load data tf keras backend image data format channels first x train x train reshape x train shape 0 1 img rows img cols x test x test reshape x test shape 0 1 img rows img cols input shape 1 img rows img cols else x train x train reshape x train shape 0 img rows img cols 1 x test x test reshape x test shape 0 img rows img cols 1 input shape img rows img cols 1 print f ninput shape used input shape input shape used 28 28 1 specify hyper parameters batch size 64 num classes 10 num epochs 200 convert datasets floating point types x train x train astype float32 x test x test astype float32 normalize training testing datasets x train 255 0 x test 255 0 convert class vectors target binary class matrices one hot encoded values train tf keras utils categorical train num classes test tf keras utils categorical test num classes print ndimensions training testing sets print f x train shape x train shape train shape train shape print f x test shape x test shape test shape test shape dimensions training testing sets x train shape 60000 28 28 1 train shape 60000 10 x test shape 10000 28 28 1 test shape 10000 10 x200b specify latent space dimensions latent space dim 3 define encoder encoder input input shape 28 28 1 x conv2d filters 32 kernel size 3 strides 2 padding encoder input x leakyrelu x x conv2d filters 64 kernel size 3 strides 2 padding x x leakyrelu x x conv2d filters 64 kernel size 3 strides 1 padding x x leakyrelu x x conv2d filters 64 kernel size 3 strides 1 padding x x leakyrelu x shape flattening k int shape x 1 x flatten x instead connecting flattened layer directly 3 latent space connect layers mu log var mu dense units latent space dim x log var dense units latent space dim x keras model outputs values mu log var given input image encoder mu log model encoder input mu log var print f shape flattening shape flattening shape flattening 7 7 64 def sampling args mu log var args epsilon k random normal shape k shape mu mean 0 0 stddev 1 0 return mu k exp log var 2 epsilon lambda layer samples point z latent space normal distribution defined parameters mu log var encoder output lambda sampling mu log var keras model defines encoder ‚Äî model takes input image encodes 2d latent space sampling point multivariate normal distribution defined mu log var encoder model encoder input encoder output decoder input input shape latent space dim x dense np prod shape flattening decoder input x reshape shape flattening x x conv2dtranspose filters 64 kernel size 3 3 strides 1 1 padding x x leakyrelu x x conv2dtranspose filters 64 kernel size 3 3 strides 2 2 padding x x leakyrelu x x conv2dtranspose filters 32 kernel size 3 3 strides 2 2 padding x x leakyrelu x x conv2dtranspose filters 1 kernel size 3 3 strides 1 1 padding x x activation sigmoid x decoder output x decoder model decoder input decoder output complete autoencoder input autoencoder input encoder model input encoder input output autoencoder output encoder passed decoder model output decoder encoder output keras model defines full autoencoder‚Äîa model takes image passes encoder back decoder generate reconstruction original image model model model input model output loss function defined follows weight reconstruction loss r loss factor ensure well balanced kl divergence loss r loss factor 1000 def vae r loss true pred reconstruction loss r loss k mean k square true pred axis 1 2 3 return r loss factor r loss def vae kl loss true pred kl divergence loss kl loss 0 5 k sum 1 log var k square mu k exp log var axis 1 return kl loss def vae loss true pred vae loss reconstruction loss kl divergence loss r loss vae r loss true pred kl loss vae kl loss true pred return r loss kl loss compile model model compile optimizer tf keras optimizers adam learning rate 0 003 loss vae loss metrics vae r loss vae kl loss train autoencoder training hist model fit x x train x train batch size batch size shuffle true validation data x test x test epochs num epochs gives error typeerror traceback recent call last appdata local temp ipykernel 11960 995477119 py module 1 train autoencoder 2 training hist model fit 3 x x train x train 4 batch size batch size shuffle true 5 validation data x test x test anaconda3 envs tf cpu lib site packages tensorflow python keras engine training py fit self x batch size epochs verbose callbacks validation split validation data shuffle class weight sample weight initial epoch steps per epoch validation steps validation batch size validation freq max queue size workers use multiprocessing 1191 r 1 1192 callbacks train batch begin step 1193 tmp logs self train function iterator 1194 data handler sync 1195 context async wait anaconda3 envs tf cpu lib site packages tensorflow python eager def function py call self args kwds 883 884 optionalxlacontext self jit compile 885 result self call args kwds 886 887 new tracing count self experimental get tracing count anaconda3 envs tf cpu lib site packages tensorflow python eager def function py call self args kwds 931 first call call initialize 932 initializers 933 self initialize args kwds add initializers initializers 934 finally 935 point know initialization complete less anaconda3 envs tf cpu lib site packages tensorflow python eager def function py initialize self args kwds add initializers 757 self graph deleter functiondeleter self lifted initializer graph 758 self concrete stateful fn 759 self stateful fn get concrete function internal garbage collected pylint disable protected access 760 args kwds 761 anaconda3 envs tf cpu lib site packages tensorflow python eager function py get concrete function internal garbage collected self args kwargs 3064 args kwargs none none 3065 self lock 3066 graph function self maybe define function args kwargs 3067 return graph function 3068 anaconda3 envs tf cpu lib site packages tensorflow python eager function py maybe define function self args kwargs 3461 3462 self function cache missed add call context key 3463 graph function self create graph function args kwargs 3464 self function cache primary cache key graph function 3465 anaconda3 envs tf cpu lib site packages tensorflow python eager function py create graph function self args kwargs override flat arg shapes 3296 arg names base arg names missing arg names 3297 graph function concretefunction 3298 func graph module func graph py func 3299 self name 3300 self python function anaconda3 envs tf cpu lib site packages tensorflow python framework func graph py func graph py func name python func args kwargs signature func graph autograph autograph options add control dependencies arg names op return value collections capture value override flat arg shapes acd record initial resource uses 1005 original func tf decorator unwrap python func 1006 1007 func outputs python func func args func kwargs 1008 1009 invariant func outputs contains tensors compositetensors anaconda3 envs tf cpu lib site packages tensorflow python eager def function py wrapped fn args kwds 666 function weak reference avoid reference cycle 667 optionalxlacontext compile xla 668 weak wrapped fn wrapped args kwds 669 return 670 anaconda3 envs tf cpu lib site packages tensorflow python framework func graph py wrapper args kwargs 992 except exception e pylint disable broad except 993 hasattr e ag error metadata 994 raise e ag error metadata exception e 995 else 996 raise typeerror user code c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras engine training py 862 train function return step function self iterator c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras engine training py 852 step function outputs model distribute strategy run run step args data c users arjun anaconda3 envs tf cpu lib site packages tensorflow python distribute distribute lib py 1286 run return self extended call replica fn args args kwargs kwargs c users arjun anaconda3 envs tf cpu lib site packages tensorflow python distribute distribute lib py 2849 call replica return self call replica fn args kwargs c users arjun anaconda3 envs tf cpu lib site packages tensorflow python distribute distribute lib py 3632 call replica return fn args kwargs c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras engine training py 845 run step outputs model train step data c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras engine training py 803 train step loss self compiled loss c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras engine compile utils py 242 call self loss metric update state c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras utils metrics utils py 88 decorated update op update state fn args kwargs c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras metrics py 171 update state fn return ag update state args kwargs c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras metrics py 403 update state sample weight weights broadcast ops broadcast weights c users arjun anaconda3 envs tf cpu lib site packages tensorflow python ops weights broadcast ops py 157 broadcast weights values ops convert tensor values name values c users arjun anaconda3 envs tf cpu lib site packages tensorflow python profiler trace py 163 wrapped return func args kwargs c users arjun anaconda3 envs tf cpu lib site packages tensorflow python framework ops py 1566 convert tensor ret conversion func value dtype dtype name name ref ref c users arjun anaconda3 envs tf cpu lib site packages tensorflow python framework constant op py 346 constant tensor conversion function return constant v dtype dtype name name c users arjun anaconda3 envs tf cpu lib site packages tensorflow python framework constant op py 271 constant return constant impl value dtype shape name verify shape false c users arjun anaconda3 envs tf cpu lib site packages tensorflow python framework constant op py 288 constant impl tensor util make tensor proto c users arjun anaconda3 envs tf cpu lib site packages tensorflow python framework tensor util py 435 make tensor proto values np asarray values c users arjun anaconda3 envs tf cpu lib site packages tensorflow python keras engine keras tensor py 254 array raise typeerror typeerror cannot convert symbolic keras input output numpy array error may indicate trying pass symbolic value numpy call supported may trying pass keras symbolic inputs outputs tf api register dispatching preventing keras automatically converting api call lambda layer functional model ‚Äã x200b help
rwqgyl,1,topic would like see covered hey everyone quick survey see areas interest application architecture discussion computational methods theory focused code focused use package whatever feedback welcome specific better well get sense popular topics run poll
r73zke,1,metalearning conditional weights reading paper class weighting domain adaptation reading club issue still bit unclear us least seems like paper updates model sample conditional weights alternatingly looking algorithm 1 sure epsilon values updated line 13 backpropagated updated model theta
ra5m26,1,idea project cnn hello currently looking cool project ideas work cnn familiarize interesting project learn pls drop cool project ideas comments thanks advance
rf08z6,1,ml beginner looking best course direction find catalogue audio matches similarities large database searches getting railroaded nlp seems unnecessary purposes large database audio files 8 30 seconds looking pull ones match certain words phrases need ml understand words phrases think need match within acceptable level error sounds currently pure human listening going per audio basis per audio time 3 5 seconds determine match course thousands audios inefficient also mentally exhausting running much nlp hard time finding audio processing harder time finding niche know little ml getting extremely basic understanding figured ask help much appreciated hoping python known language unwilling go different route fits use case
qn0ynq,0,fuzzy c means clustering line graph data hi trying apply fuzzy c means data represented line graphs hourly electrical load profiles understand must cluster points hour get relate clustered points hour adjacent hour obtain result clustered line graph example input desired output input electrical load profiles x200b output clustered results
ri20hq,0,good research ideas fail hi ml peeps write major lessons learned ml research google pathai including time working samy bengio ian goodfellow main questions curious answer 1 many good seeming ideas fail 2 curious know alls answers questions
r8uwp1,1,nlp preprocess text running deep learning model recommended preprocess text training preprocessing mean things like lemmatization stemming etc intuitively think first answer preprocessing might remove information original data introduce noise would answer kaggle competition processing time recourses limited x200b thanks
r2o6jr,1,need help project part final year project decided project leaf disease detection using image leaf started studying machine learning months know basics problem searched papers tutorials kind project use cnn teacher allowed use cnn since started machine learning says need use traditional methods like random forest svm etc anyone help method use provide basic roadmap learning
rakmq4,1,learning learn im trying get machine learning im trying learn learn ive programming 30 years fairly firm grasp portions things dont even know approach learning things example would like something like following identify bikes identify bike parts identify bike statistics ie wheel size height etc identify specific bikes approach something like presume thats 3 different tasks maybe like many pieces data need collect 1000 marked images bikes enough least start mean thats im going start
r6h3h8,0,created auto updating kaggle dataset collects high frequency crypto market data updates daily 20 related trading notebooks happy announce finally finished cleaning organizing creating baselines developing automated collection pipeline collects minute minute market data cryptocurrencies updates kaggle every day keep competition maybe even whole project took lot time develop easy maintain please find value feedback support highly appreciated competition know crypto forecasting competition running kaggle g research crypto forecasting competition need use machine learning forecasting short term returns popular cryptocurrencies bitcoin ether dogecoin provided dataset millions rows high frequency market data dating back 2018 use build models submission deadline passed final score calculated following 3 months using live crypto data collected auto updating kaggle dataset make things interesting created auto updating kaggle dataset collects high frequency market data multiple cryptocurrencies updates daily kaggle available anyone play also also released 20 starter notebooks demonstrating different model method forecasting future returns project meant currently running crypto forecasting competition g research however since publicly available assumed many others would like also look mimics real life better typical datasets unique opportunity work much real life setup usual kaggle datasets update daily mess overfit see tomorrow üòÇ anyway ongoing project also beginner friendly since highly documented many time series finance related notebooks released future also serve first stop studying time series analysis baselines starter notebooks cv model hyperparam optimization time series models feature engineering neural network starter ae analysis 1 lightgbm starter analysis 2 catboost starter written scratch series agg xgboost starter supervised ae janestreet 1st ae janestreet 1st engineering transformer volatility features reinforcement learning ppo starter validation grouptimeseriessplit ‚è≥ making fork please enjoy auto updating full price datasets created today auto updating dataset contains full historical data assets competition easily build models utilize datasets split asset since much heavier competition data datasets also labeled described competition overview organized way exact format competition data goal provide dataset 1 contains full history asset currently competition data goes back 2018 dataset contains data even earlier 2 auto updating daily due high volatility cryptocurrency market train models recent data available datasets backend pipeline collecting formatting reuploading kaggle scheduled updated daily every single day end competition 3 preprocessed datasets ffilled overcome missing values issue present original competition dataset datasets binance coin bitcoin cash bitcoin cardano dogecoin eos io ethereum ethereum classic iota litecoin monero maker stellar tron bonus dataset also uploaded dataset containing powerful source predicting cryptocurrencies movement elon musk twitter üòÇ simply updated dataset elon musk tweets üòÇ must check elon musk help us win üëå play technical details data every asset competition following fields binance official api endpoint historical candlestick data collected saved processed 1 timestamp timestamp minute covered row 2 asset id id code cryptoasset 3 count number trades took place minute 4 open usd price beginning minute 5 high highest usd price minute 6 low lowest usd price minute 7 close usd price end minute 8 volume number cryptoasset u units traded minute 9 vwap volume weighted average price minute 10 target 15 minute residualized returns see prediction evaluation section notebook details target calculated 10 weight weight defined competition hosts 11 asset name human readable asset name indexing dataframe indexed timestamp sorted oldest newest first row starts first timestamp available exchange july 2017 longest running pairs enjoy thank advance support easy system maintain
r5q2re,0,google cambridge u alan turing institute propose polyvit universal transformer image video audio classification research team google research university cambridge alan turing institute proposes polyvit single transformer model capable processing multiple modalities datasets polyvit parameter efficient learns representations generalize across multiple domains quick read google cambridge u alan turing institute propose polyvit universal transformer image video audio classification paper polyvit co training vision transformers images videos audio arxiv
rtapnm,1,sigmoid activation function discriminator tf dcgan tutorial referring tensorflow tutorial constrain discriminator output within 0 1 advantages approach
rla8ja,1,use beginner project colab seems slow experience google colab similar tools trying train models beginnier looking something relatively easy use work well enough trying tried run macbook took 5 hours suggestions thank
rcszwx,1,best path learn ml given experience hello currently work professionally data engineer would like learn machine learning think like eventually try transition machine learning engineer sure start comfortable python sql aws remember basic calculus linear algebra college studied chemical engineering tips guidance start learning ml
r29dks,0,interactive compute platform recommendations ml research looking switch small laptop travel much capacity running experiments love hear opinions people use interactive compute platforms specifically looking sort vm ssh use like computer except also want something easy hook gpus tpus ideally demand payment scheme think google microsoft amazon version much hard tell one go key factors looking ease use value per dollar ability easily upscale downscale amount compute using ability open notebooks via web would also nice
riknbs,1,image generation project idea need tool pointers though bunch photographs taken years primarily landscape like feed network kind learn photographic style generate new images based tools libraries looking programming like 10 years never done anything ml related honestly super interested technical details project output easier tool better fwiw supply probably 1000 high res landscape photos cheers
rw5sqt,1,question kernel svm wondering would say important hyperparameter comes naturally picking kernel sorry question seems ambiguous repeating word word another person asked sure answer
rtpci8,1,drop ai referring ai ml wonder two seem always come bundle ai superset ml realize examples ai ml however seems work discussed articles even job listings specifically machine learning hence ai portion dropped thanks
qvt9vu,0,integrate squeeze excitation block yolov5 understanding squeeze excitation block improves performance networks cost additional computation trying integrate yolov5 object detection struggling figure integrate unable find open source implementation would really appreciate help
rhmz6m,1,ever taken andrew ng machine learning course coursera friend thinking enroll course start new year 2022 saw 11 week course thinking start community increase accountability completion rate may wonder take ml cohort based courses well good idea budget rare find good cbcs low cost well taught famous instructor like andrew ng stanford find course valuable since free want study alone two us either know anyone also planning start year learning new skill ml would nice start january link way
rcftpj,1,complex model look like overfitted anybody tell model overfit want intentionally overfit training validation test set seem fine even though model complex dataset accuracy model use sklearn iris dataset code
rwm2qi,1,please suggest resources learn ml online preferably cost
r06rs4,0,strategies deal label sparsity training protein function prediction model protein function prediction task requires take sequence amino acids think words sentence 20 words output functions protein take around 30 thousand labels protein function labels mutually exclusive protein function prediction essentially huge binary prediction multitask catch labels common others rare overall protein labeled small number labels 30 thousand labels biologists came total represent protein functions binary vector 1 entry represents function carries vectors extremely sparse majority labels positive examples training set leads extreme label imbalance strategies use deal problems
rmtcvh,0,shape changing conformal mapping looking shape changing algorithm meanly linear nonlinear shapes think conformal mapping related anyone experience shape changing works
qxjr94,0,scikit learn feature selection using ai hi want let know sklearn genetic opt version 0 7 0 available implements feature selection using evolutionary algorithms uses multi objective function optimize cross validation score minimizing number features used compatible sklearn classifier regressor let know question suggestion new feature compatible callbacks tensorboard mlflow example using progress bar callback check docs would like contribute check implementation repo
rgzza1,1,colonel trick x200b
ra3kwq,0,looking ocr datasets benchmark hi everyone currently working complete benchmark cloud ocr engines gcp aws azure ocr space etc carry work looking datasets differences performance could appear engines already looked kaggle public dataset available perhaps might know good datasets project thanks jeremy
ruekwq,1,reading group ml books research paper anyone know ml reading group could ml books research papers something like discussion group would like part
r9sm6d,1,ml project help hi currently taking theory computation course uni question ml assignment understand part little bit time could help would appreciate
rsjil0,0,pretty extensive 6 part blog series ai accelerators article found pretty extensive solid resource dizzying array specialized hardware see nowadays ml
rfzbr7,0,training large language models need waiting hope training large language models need many patient especially gpus finetune model later period finetune finetuning 2 6b gpt 2 model finetuned 10 datasets loss curve continued finetuning nearly drop within 12 days makes feel doubt fear uneasiness uncertain model go divergence change anymore bad waste much compute time fortunately long hyperparams datasets good codes model bug slow improvement loss later period finetune ok always improve end training update see loss curve image keeping drop end 12 days reminds last sentence count monte cristo need waiting hope x200b updated image loss black line finetuned 12 days black line loss start drop x200b finetuned 10 37 datasetsÔºåloss curve continued finetuning 12 days x200b
qtiqlw,0,walk forward target encoding data leakage hi working time series problem need also test historical predictions time using walk forward procedure data leakage dealing time series using simple target encoding features like df groupby col target transform mean x200b wondering anyone built custom function build target encoding features without leaking data kind like walk forward target encoding function
rm4i9v,0,lottery ticket hypothesis paper implementation screencast hi created video tried implement paper lottery ticket hypothesis finding sparse trainable neural networks scratch made lot simplifications order fit everything single video e g working multilayer perceptron using mnist dataset also ran couple experiments see whether could get similar results paper presented video also contains quick tutorial pruning pytorch lastly video includes chapters easier navigate hope find interesting hesitate share constructive feedback video link
qqzuh0,0,cedille largest french language model 6b released open source üìù demo üìò repo spent last 3 months lives teraflops compute gone 300gb text bring cedille ce que j aime quand je mange une baguette c est quand celle ci est craquante je ne saurais d√©finir le terme craquant mais je sais que lorsque c est le cas je peux √™tre s√ªre que la baguette est bonne entirety french spirit captured measly 6b parameters üá´üá∑ü•ñ seriously super excited share cedille far largest french language model play right playground long servers hold üòÖ proponents ‚Äúopen ai‚Äù released checkpoint world use mit license another aspect fun dataset filtering run whole c4 french dataset detoxify classifier clean ü§¨ acknowledgements cedille based gpt j 6b model developed wizards eleutherai cedille also generously supported google tfrc program
quclg6,0,ml helping us understand animal communication video hi r machinelearning visualizing sound enabling us analyze animal voices get better sense information contain form audio analysis powerful empowers human speech recognition technology opening door towards better understanding dolphins whales prairie dogs communicate second video look spectrograms fascinating look x200b related research dcase detection classification acoustic scenes events analysis interpretation animal vocalisations call papers
qss5os,0,micro grants micro grant sources besides ai grant unitary fund
rbc8s5,0,organize track reading list feel like given time million tabs open like 20 different browser instances tab linking arxiv abstract want read even get started github stars browser bookmarks keep track stuff want read keep track things read want able dig later tie notes neurips barely started already feel like drowning stuff want dive lose immediately afterwards like top secret tools hip grad students know something hell collect organize stuff want read recently read please tell better way
qukqtz,0,knowledge graph applications ml ai open source database links 2022 new terminology coined google 2012 ‚Äúknowledge graph‚Äù knowledge graph significance field machine learning due performing capabilities machine learning techniques getting better day day high accuracy rate read
rljh5w,1,github repository lists topics resources learn saw repository data structures algorithms wondering something similar beginner wanting learn machine learning quick google search returns lots repositories anyone experience repos also heard microsoft ml beginners looks like good resource definitely start lessons anyone opinions looks interesting
r6ldxk,1,ai recruitment survey hey everyone name reeti work consulting organization called illinois business consulting we‚Äôre working company ai sector hoping improve recruitment made survey ai recruitment guys wouldn‚Äôt mind filling survey would awesome thank
rw3nba,1,ml home hi dudes would like know ml home answer yes compute cloud pc
rplzc6,1,need help competitions related autonomous driving want build career autonomous driving flight engineer competitions forums opensource projects use something good resume well
razoa5,0,audio far behind ml application domains like image processing nlp like gain intuition actual insight someone worked audio seen major breakthroughs audio ml tracking localization saw image processing nlp ofcourse issue data annotating except laborious task also hard define e g label conversation multiple individuals dinner table would assume needed large corporations would attempt project seems either people convert data images use image processing methods use signal processing mean signal processing lightweight outperforms prospective ml approach
rrbc6q,1,advice highly motivated newbie barely technical background options 1 take ai ml 6 month post graduate program i‚Äôll learn fundamentals curriculum looks like intro python numpy pandas exploratory data analysis matplotlib seaborn 5 statistics hypo testing probability courses supervised ml unsupervised ml feature engineering ensemble technique model deployment model selection tuning ai deep learning neural networks computer vision nlp recommendation systems 2 take 6 month data science data management systems post graduate program first ml concepts taught 6 month data science course option adding 2 elective ai classes well question need depth comprehension database systems data science understand fundamentals ml ai 3 take 5 month computer science ai post graduate program learn fundamentals computer science like abstraction data structures algorithms programming python sql css html learn python ai graph search algorithms adversarial search knowledge representation machine learning reinforcement learning neural networks natural language processing main question holding factors constant like difficulty level feasible start ai ml starting data science help understand ai ml even better computer science instead long term goal impact want make health field requires use ai ml accomplish need ds cs first
rknejs,1,machine learning algorithm would learn barber rejects customers imagine barber works appointments would system would track times barber would reject appointments example mondays guy usually rejects appointments 10 12 tuesdays 1 3pm program learns guy pattern closes hours future appointments machine learning algorithm would suitable kind work
rgg36d,0,results nethack challenge neurips 2021 report excerpt results section results showdown showed ‚Äî time ‚Äî symbolic bots red quite clearly upper hand difficult environment top three spots overall best agent went agents symbolic agent track following three went top neural agents track winner track highly hybrised model competition alternating symbolic neural play depending proximity monsters margin victory significant top symbolic agent beating top neural agent factor almost 3 median score fact increased looking best agents team frequently might see almost order magnitude improvement median score best symbolic neural agents best symbolic teams moderate expert nethack domain understanding surprised find often extensive ml experience well fact winning symbolic teams said intended enter neural track found symbolic methods scaled much better half million evaluation games agent managed ascend mean ai ‚Äúdespite game nethack far solved agents seeing descend 20 levels deep dungeons nethack achieve scores 100 000 points encouraging past versions nethack rich history symbolic agent type bots methods machine learning may catching specific realm playing nethack optimistic future methods seeing results challenge amazing see game cherish dearly used make new advancements machine learning artificial intelligence look forward seeing teams improve next year‚Äôs challenge ‚Äù ‚Äì tonehack nethack far solved first results show nethack still tremendously hard challenge ai agents whether symbolic bots deep reinforcement learning agents top median score 5 000 several orders magnitude short typical human ascensions bots managed achieve much higher scores limited runs runs descend deep dungeons nethack instead stayed within early stages game median score good ‚Äî ascensions would better challenge highlighted complex relationship score ascension found many entrants elected ‚Äúcamp‚Äù early stages dungeon grinding high score killing monsters instead progressing dungeon undoubtedly helped weakest ‚Äòroles‚Äô game like tourist healer lead winning game learnt score ascensions always well aligned objective may due rethink future challenges said focus median agent performance still important incentivising creation robust general agents focus game score may less symbolic bots strategize like human neural ones nethack benefits ‚Äòstrategic‚Äô play ‚Äî good play often involves executing series actions well defined expressible sub objective eg ‚Äúfind sokoban‚Äù ‚Äúapply unicorn horn cure poison‚Äù symbolic bots found easy define ‚Äòstrategy‚Äô like subroutines decide deploy based rich human legible representations game state made easy participants developing symbolic bots incorporate domain knowledge neural agents struggle area since hierarchical rl open problem research field hard agents discover ‚Äòstrategy‚Äô type patterns behaviour environments featuring large action space sparse reward symbolic bots know alls neural agents find harder game nethack partially observable single dungeon level ever visible many objects player states often hidden unless inspected remembering discovery incorporating extra knowledge often key making good decision symbolic bots excelled keeping full game state memory incorporating external knowledge strategies found easy transfer domain knowledge decision making process contrast neural agents find harder maintain information memory especially reward directly associated
rse1fs,1,know columns drop use python reading islr learned importance checking p values predictor decide wheter worth keeping since straightforward way python without statistics libraries like statsmodels usually decide whether column data worth keeping learn statsmodels make educated decision rather trial error thank
rly1g0,1,new r planning start google data analytics professional certificate course coursera also started watching bunch r youtube videos practicing r studio right path better place learn r
qqfe6y,0,train gans really fast projected gans converge faster explained 5 minute summary casual gan papers despite significant progress field training gans scratch still easy task especially smaller datasets luckily axel sauer team university t√ºbingen came projected gan achieves sota level fid hours instead days works even tiniest datasets new training method works utilizing pretrained network obtain embeddings real fake images discriminator processes additionally feature pyramids provide multi scale feedback multiple discriminators random projections better utilize deeper layers pretrained network full summary blog post projectedgan upd originally included wrong links arxiv code subscribe casual gan papers follow twitter weekly ai paper summaries
rovtz1,0,research looking interesting ml papers read break new year curated list made video explanation short read paper code best ai papers 2021 clear video demo short read paper code depth blog article full list github short recap video
robzib,1,model learn replicate art style twitter artist real ¬øwhere find example characters request also explained following steps 1 download artwork twitter account 2 use one sample twitter art model output set inputs 5 10 images character original basic art style 3 let model find relationship 4 repeat steps 2 3 samples characters much want 5 find appropriate start submitting set inputs 5 10 images character never considered artist 6 model learning past processes generates new output resembles artist style hope get idea however please consider still new area something think missing please let know merry christmas
rboh2r,0,transformers overhyped wondering pretty long time since never seen anybody say anything bad transformers seemed pretty flawed moment read paper way ml expert aspiring phd student even specializing nlp way wrong really like hear tl dr believe transformers long term pretty small contribution world nlp may even damaging due shifting focus research community wrong direction address long term dependency problem transformers nlp used dominated rnns specifically encoder decoder architecture case translation encoder would encode input sentence fixed length vector decoder would decode vector output translated sentence transformers also use encoder decoder architecture one big difference rnns encoding decoding actually happens every step way words know tokens call words inserted sequentially rnn every single word encoder rnn look current encoding vector input word choose update encoding vector meaningful way problem approach call long term dependency arises rnn look long sequence words humans easily distill information read remember important bits example name character mentioned 5 pages ago rnn models trouble encoding happened 5 sentences ago research community starts solving problem original attention paper transformers come comes transformer starts dominating nlp world transformer huge model encoding simply takes 512 input words arbitrary number looks simultaneously works wonders look transformer remember happened 5 sentences ago previous 5 sentences combined less 512 words hooray remember happened 10 sentences ago though uh well improve way solve long term dependency problem well smart sentences feed means still distill information large body text back beginning obvious solve long term dependency problem ever hope achieve human like nlp models seems transformers nothing solve problem dominating field nlp research maybe optimal solution include combination transformer model information distillation still need solve long term dependency problem throwing rnns quickly
qo3704,0,professors research groups neural program synthesis want collect list professors research groups work neural program synthesis program induction find simple search groups microsoft google brain eth zurich mit one know groups work topic especially outside usa ps find groups professors germany helpful tips phd topic would nice
rd7tlx,1,someone explain author means say pic figure machine learning helping people learn
r7n8k8,1,recommendations creating nlp text harmonization algorithm hello looking recommendations tips solve issue hopefully allowed question deleted stackoverflow know ask help x200b let say set company kpmg usa kpmg europe microsoft corporation microsoft dhl global forwarding dhl express want harmonize company names get result like following x200b company company harmonized company kpmg usa kpmg europe kpmg microsoft microsoft corporation microsoft corporation dhl global forwarding dhl express dhl x200b problem segmented two core problems 1 matching fuzzy matching 2 harmonization providing cleanest representative name matching part know several pyhton packages fuzzy match tokenization algorithms python record linkage among others harmonization part clue available seeking recommendation part additionally looking implement form supervised nlp algorith learn time output best harmonized name match correctly seeking recommendations part well already large dataset around 1m rows get needed examples like kpmg usa kpmg
rcovbp,1,machine learning 1 know code pretty well 2 know math needed data science anyone please recommend free resources books courses etc learn math order need study math topics currently 10th grade according need study first move math needed data science
rxbl9w,1,record chess moves webcam online play hello would like share project records moves made real life chess board using webcam also transmits moves chess website playing online
qznqdl,0,microsoft asia‚Äôs swin transformer v2 scales award winning vit 3 billion parameters achieves sota performance vision benchmarks microsoft research asia upgraded swin transformer new version featuring three billion parameters train images resolutions 1 536 x 1 536 advance sota four representative vision benchmarks quick read microsoft asia‚Äôs swin transformer v2 scales award winning vit 3 billion parameters achieves sota performance vision benchmarks associated code available project‚Äôs github paper swin transformer v2 scaling capacity resolution arxiv
rwgk3l,1,implementation u net architecture keras hello everyone tutorial explains working u net architecture explains benefits implement keras visualize final structure using one keras built functions u net implementation keras
r4h993,1,sentiment analysis ideas hi working final project one classes involving sentiment analysis data set imdb movie reviews data set courtesy keras fairly straightforward binary classification classify review positive negative thing little short ideas regards accomplish already utilized models feel like little simple interest getting spirit things wondering advanced techniques novice like could still use already done inputs word embeddings dataset imported keras 1 cnn 2 lstm 3 transformer 4 cnn lstm 5 lstm svm things looking sure implementing 1 generative discriminative models generator would perform feature extraction data 2 data augmentation techniques use data feed aforementioned models thus far fairly simple like swapping synonyms antonyms randomly deleting adding words etc recommended course action source two ideas looking suggestions could feasibly implement using google colab got premium membership data set large 25 000 training samples computational expense concern appreciate suggestions guys
ql9d4s,0,neurips 2021 accepted paper list list accepted papers appears public spot particularly interesting ones
r6srub,1,look data hello newbee trying make elearning ehealth website want big data sets implement machine learning ideas website find data sets easier find data elearning ehealth appreciate machine learning suggestions guys thanks
rklgw8,1,python ml u learned python basics started ml following book hands machine learning popular recommended books coursera machine learning course andrew ng observed book well course authors teach python ml specifically please share resources python ml
qq21s6,0,amd much less work ai nvidia assumption title false amd care get left behind somehow catch x200b know question vague maybe still somebody point fitting interview something else
r3aoz0,0,sensory neuron transformer paper implementation screencast hey created video tried implement paper sensory neuron transformer permutation invariant neural networks reinforcement learning scratch much based official code however introduced multiple modifications simplifications able fit single youtube video notably focuses cartpoleswingup environment lastly ran couple experiments test permutation invariance robustness noise hope could find useful would happy get feedback answer questions video link
rcszg7,0,research survey silent bugs keras tensorflow framework hello research group polytechnique montr√©al supervision prof foutse khomh conducting survey ‚Äúsilent bugs keras tensorflow frameworks‚Äù prepared online survey takes around 10 minutes valuable time complete asking relevance severity observed silent bugs keras tensorflow moreover could kindly share survey colleagues students consider eligible participate send reminder week remind completing study please want take part study simply ignore two emails disturb results survey publicly accessible arxiv org anonymized form point survey ask name logging ip address allow anonymity would like know study feel free contact us questions link really appreciate time supporting research best regards amin nikanjam amin nikanjam polymtl ca mailto amin nikanjam polymtl ca swat lab polytechnique montr√©al montr√©al canada
qk90b9,0,project bert tokenizers nuget package c x200b inspired challenges faced using bert models ml net built small open source project nuget package easy tokenization c üöÄ package worry different vocabularies build input bert models quicker üëâgithub üëâblog post
ruj3ja,0,iclr 2022 open discussion quality first time submitting iclr wondering open discussion openreview net really different review rebuttal procedure used conferences papers reviewing half reviewers reacted authors responses clarifications modifications additional experiments etc submission 5568 run extra experiments answered concerns directly received 0 feedback reviewers reviewer think matter rebuttal changes mind quality submission basic manner reply authors responses simple thank authors responses think addressed concerns would work saying nothing means uncertain responses make sense care figure authors spent whole week running experiments answer questions give responses keep questions submit review hoping different experience submitting iclr realized discussion basically broken
rpneke,1,ml study group meet working projects research papers hello indian student done electronics bits 1 year stint sde mtech computer science iit hyderabad done 2 3 official courses machine learning besides infamous coursera course deep learning specialization currently working official project 75 computer networks 25 machine learning however current interest towards ml would love study group working ml based projects research papers could contribute even study group primarily discusses ml based concepts thank
rd0hm6,0,utility scripts coco json format object detection segmentation sharing simple collection functions scripts manipulating object detection segmentation datasets coco format dominant dataset format dataset conversion utilities provided well feedback contributions welcomed
rdfdcv,0,collection 33 psychology related datasets finally released happy announce finally finished cleaning organizing creating baselines uploading first version massive 33 datasets collection openpsychometrics whole project took around two weeks clean organize way planned please find value feedback support highly appreciated would want work small size similar titanic clean someone cleaned lot intuitive see question see individual answered nothing nothing less funny fun work questions like would rather gun dishes prettiest starter notebooks ever seen life seriously invested embarrassingly amount time basline notebooks check collection collection includes 32 psychology pseudo psychology datasets links
r13jq5,1,possibilities object recognition complete newbie ai machine learning space wondering far possibilities currently reach came idea recognize whether different types glasses filled empty idea extremely novel still therefore wondering already possible ai machine learning time wondering something like easily programmed learned program beginner requires certain experience done someone experience field
rsx4n0,1,data problems live words build models data problems ton data mean use train model larger dataset easier garbage hide thousands data points gathered different methods spanning across years trust massive problems hiding data models amplify obfuscate anything missed data analysis cleansing use little data possible control data sourcing aggregating cleansing analysis research existing data much possible use data fully trust reduce features start hear massive datasets used google facebook expertly curated mean years spent reviewing grooming data best practices require level obsession detail business built capability smaller better
rer126,1,improving convergence dqn network trying train dqn carracing v0 environment openai gym trouble getting retain knowledge learned time using prioritized experience replay model architecture relatively simple cnn link code let train overnight total reward plot per episode ideas looking guidance increase model complexity maybe underfitting decrease learning rate time could causing fluctuating reward
rbbd2u,0,convolution papers question regarding convolution 3bluebrown video neural networks states researchers initially misunderstood convolution thought layer finding different edge image multiple layers convolution would notice entire image compilation many important edges however video interview ian goodfellow explained incorrect seems convolution rather continual refinement image layer look different edge rather gives us precise understanding image representing know may explained wrong interested learning convolution actually works know papers explain concept well appreciate help x200b x200b interview referring üì∑ preview 1 08 37ian goodfellow generative adversarial networks gans youtube ¬∑ lex fridmanapr 18 2019
rgvkki,1,interpretation singular value decomposition svd reading singular value decomposition trying figure signifies math makes sense finding hard interpret decomposed matrices example 2x2 matrix rank 2 svd gives us expressed sum 2 rank 1 matrices u1 v1 sig 1 u2 v2 sig 2 1 matrices signify 2 view linear transform something special linear transforms two matrices get decomposition 3 assuming technique like pca leave matrices signify
r8pr04,1,picking kernel svm working higher dimensional data need differentiate two classes svm pca training data two classes separate cleanly cleanly manually draw vertical line though x axis pc plot great job separating classes seem like tutorial page look people use slightly different parameter dictionaries grid search gridsearchcv scikit learn case kind wondering much art step like intuitively see linear kernel probably right choice always still iterate rbf poly
r8jea7,1,specific ml technique use predicting country religion attributes flag colors stripes crosses etc 2nd year computer science field machine learning already broke brain trying learn get wrong concept machine learning cool gambled learning making something bit required create system predict something dataset idea dataset countries flag attributes still trying find somewhere online create predict country religion either nationally recognized practice largest population however seem think technique would help project thinking neural networks might bit difficult technique start mind long best option asking humble powerful redditors guide peace
qoptlo,0,flops calculation please consider following table x200b facenet publication trying learn flops calculations help model calculate ones wrong first row 110 110 64 7 7 3 113 836 800 flops bit lesser given value likewise second convolution row conv2a 55 55 64 64 12390400 near exactly x200b right track wrong also mentions multiplying 2 operations confuses
rux10m,1,unsupervised classification optimal images ocr want categorise image 2 categories see numbers properly image like 1st image number rubbed like 2nd image 1st image x200b 2nd image
rs31ic,1,l2 distance 2 minute visual guide x200b üîµ euclidean distance üîµ üöÄ talk distance generally mean euclidean l2 distance one commonly taught school euclidean geometry wide variety applications üëΩ l2 distance computed using pythagorean theorem cartesian coordinates points space l2 distance p norm p 2 course like distances applied measure closeness two objects higher dimensions well üî∑ squared circle since many shapes concepts geometry involve distance way define distance effect properties choose distance defined l2 distance get familiar round circle l1 distance get rotated square circle ü§ì l1 l2 norms used perform regularization parameters model neural network simple linear regression model regularize prevent overfitting lasso ridge regression use l1 l2 regularization respectively like content would like steer topics cover feel free suggest topics would like know comments
rmd67z,1,inerview question repositries someone give good resources interview questions ml answers theoretical question case studies tougher practical basic ones get google
rb2fsv,1,managing ml experiments code git dvc machine learning experiment tracking tools log ml experiments central database show dashboard makes easy share teammates compare however active experimentation phase may create hundreds experiments team members may overwhelmed loose ability effectively share experiments team members following article shows dvc tool push experiments like git branches giving flexibility share experiment choose track machine learning experiments version dvc experiments run stored local repo best experiments promoted central repo github example share teammates distributed experiments shared people code repo traditional experiment tracking tools log ml experiments central database show dashboard makes easy share teammates compare however active experimentation phase may create hundreds experiments team members may overwhelmed loose ability effectively share experiments team members dvc experiment versioning treats experiments code saves metrics hyperparameters artifact information text files versioned git need centralized database online services git becomes store experiment meta information dvc data versioning backs artifacts anywhere
rkwowu,0,improving expressive power gnns using subgraphs expressive power message passing graph neural networks inherently limited due equivalence weisfeiler lehman graph isomorphism test several concurrent recent works show limitation overcome applying gnn collection subgraphs obtained removing nodes edges input graph new post co authored leonardo cotta fabrizio frasca haggai maron christopher morris lingxiao zhao review common themes nuances different approaches x200b
rd26uw,0,increasing accuracy textual data analysis corpus 2 billion words soroco ingest 200 million 2 billion words course model training analysis single team workers using scout product blog post talk tips tricks might use increase accuracy models including appropriate processing text purpose leveraging standard techniques machine learning demonstrate showing represent text high dimensional vector space applications toy regression problem
r6fzxo,0,discussion sharing node information mcts runs scenario single player game pretrained value prior probability model given state reached different order actions imagine something like knight move followed bishop move vice versa series moves lead state different ways used today reuse information states already explored possible connect nodes different branches search tree known states identical sort hash function effort save roll time possible also reuse current approximation q based w n particular action two actions lead state
rhum5j,0,research bayesian optimisation live series lecture small survey conducted today realized lot people engaged bayesian optimization literature would like learn x200b would attend lecture series view poll
r5rkbv,1,making bonding discord group everyone hey everyone ive wanting make bonding group ppl discipline thing study groups often dont work sometimes work different stuff plus incentive us tend treat like burden id like group work mutually independently chill someone distract boredom coding studying researching throughout day server voice chat general text chat nothing thanks guys
r20dmh,1,predicting number citations essays hello machine learning project students working group build python based citations prediction algorithm dataset 10 000 essays json file unique doi number citation count publication venue title topics authors small abstract paper use make model predicts number citations essay tried lot models feature engineering seem get much higher r2 score 0 42 dived lot literature regarding topic none seem specify exact model works best features work best made log transformations original created features like references years old oa yes venue count topic count abstract avg nr chars author count experience give us hints tips regarding model usage feature engineering thank advance input much appreciated
qq2rbm,0,iclr 2022 reviews share rants
qn8com,0,optimization hugging face transformer models get inference 1 millisecond latency deployment production ready inference server hi released project showing optimize big nlp models deploy nvidia triton inference server source code project description please note real life large scale nlp model deployment based open source softwares using tools often discussed usual nlp tutorial performance benchmarked compared recent hugging face infinity inference server commercial product 20k single model deployed single machine open source inference server carefully optimized models get better latency times commercial product scenarios shown demo gpu based hesitate question
rjgkbl,1,text analysis low resource language long shot hit dead end research hope someone community help company internal project text analytics product arabic found great resources helped far arabert arbert etc things keyword extraction hate speech detection ner available mostly research paper pilot studies fairly outdated notebooks understand text analysis nlp general arabic language still early stage would anyone know anything help models transformers fine tune datasets use particularly three things mentioned thanks
rchib3,0,neural flows efficient alternative neural odes neurips 2021 want use neural odes solver takes long try modeling ode solutions directly instead using neural flows model takes initial condition time want obtain solution outputs solution corresponds solving underlying ode done single call without numerical solvers enable model satisfy two simple properties 1 return initial condition 0 2 return unique solutions easily implemented code speed measured orders magnitude get better results across time series density estimation tasks check paper code info
rib9m5,0,work embeddings understand different measurements would like know possible create embeddings understand different measures example embeddings would text similar b c terms cosine similarity text 13 inch screen text b 30 41 x 21 24 cm text c 3456√ó2234 pixels create embedding understand order magnitude measurements
qseien,0,dilemma ml guy industry machine learning engineer healthcare sector thinking lot rapid fuck advancements research ai keep research focus learning engineering solutions pipelines etc example reading paper using new gan vs reading case study
rb2gi0,0,label efficient semantic segmentation diffusion models tl dr image representations extracted ddpms contain high level semantic information might useful downstream vision tasks examples k means clusters formed features extracted different diffusion steps unet decoder blocks work show simple approach based ddpm representations provide strong performance context semantic segmentation amount labeled data scarce arxiv code
rq68jj,0,modern artificial intelligence 1980s‚Äî2021 beyond schmidhuber‚Äôs talk schmidhuber uploaded talk relatively new youtube channel youtube description keynote talk premiere 3 dec 2020 aij conference moscow translated russian also presented nvidia gtc 21 conference us 2021 2021 machine learning summit beijing translated mandarin big data ai toronto 2021 ific china 2021 ai boost lithuania 2021 iconip 2021 jakarta 2021 according organizers millions viewers outside youtube abstract significant historic events appear occurring frequently time goes interestingly seems like subsequent intervals events shrinking exponentially factor four process looks like converge around year 2040 last major events said occurred around 1990 cold war ended www born mobile phones became mainstream first self driving cars appeared modern ai deep neural networks came talk focus latter emphasis metalearning since 1987 call miraculous year deep learning saw birth of‚Äîamong things‚Äî 1 deep learning unsupervised pre training 2 vanishing gradient analysis led lstms running smartphones really deep highway nets resnets 3 neural fast weight programmers formally equivalent what‚Äôs called linear transformers 4 artificial curiosity agents invent problems familiar many nowadays form gans 5 learning sequential neural attention 6 distilling teacher nets student nets 7 reinforcement learning planning recurrent world models i‚Äôll discuss 2000s much begun impact billions human lives timeline predicts next big event around 2030 final decade convergence might hold happen subsequent 40 billion years take grain salt though
qzmp8l,0,verizon uses ai enhanced business decisions anil kumar executive director ‚Äì head ai industrialization verizon thursday december 2 2021 11 30 et hi r machinelearning wanted share free webinar details event website featured speaker anil kumar executive director head ai industrialization verizon companies looking leverage cognitive technology deploying machine learning models organizations need make sure correct people processes technology place succeed presentation anil share ai industrialization verizon moving pockets ai ml ai ml implemented across organization also share unique opportunities challenges adopting ai across organization large verizon presents agenda 11 30 12 30pm featured presentation 12 30 13 00pm q interaction link free registration
rpkn6w,1,starting route beginner hi guys completed bachelors mechanical engineeering taken classes like calculus statistics probability lineer algebra differential equations also good knowledge python libraries think need take concepts thought would start according person follow beginner road map kinda covers general topics ml python data science handbook essential tools working data jay l devore python libraries probability statistics engineering sciences books remembering statistics probability concepts books thougth would learn exploratory data analysis feature engineering learning go machine learning concepts think would good route suggestions
reys09,1,validation accuracy siamese networks improving stuck 0 5 posted another subreddit forgot also exists implementing network learn yoga poses far crested dataset 6000 images positive negative pairs arranged alternatively positive negative pairs divided equally training validation datasets training accuracy improving 0 90 epochs val accuracy stuck around 0 5 anyone tell week cannot get please answer question stuck week
rm9rrt,1,machine learning design evaluate generator sound hey guys thinking following task somehow sure solutions curious others would develop system extend approach right company monitoring electrical generators goal detecting generators need maintenance based sounds emit generators continuously monitored microphones audio data stored processing need design machine learning pipeline would ingest data use training order detect sounds would indicate need maintenance three main questions 1 supervised vs unsupervised learning think supervised better approach load labeled data model labeled data undrestand generators sounds two sets need maintenance need 2 ml architecture recurrent recursive neural networks rnns used solve sequence problems like speech processing etc 3 stages pre processing would pipeline involve thinking need create lists labeled data load dont really come ideas looking forward ideas
qm578v,0,python toolboxes probabilistic graphical model inference hi folks libraries toolboxes would recommend ideally based personal experience performing inference probabilistic graphical models actual practical application academic toy examples minimal requirements must able specify bayesian network factor graph consisting categorical nodes hidden others observed use set observations identify factors dependencies think essentially pgm toolbox fulfill requirements bonus points given good documentation maturity stability works efficiently many factors many datapoints implements many different inference methods modeling paradigms simplicity use know promising toolboxes pgmpy pymc3 pyro used either purpose bit loss picking one start
rorax0,0,use images vqgan clip generation newbie machine learning using google colab play vqgan notebook code understand bit wondering modify notebook use set images generate images let say would like generate fashion portrait wardrobe styling model take thousands wardrobe details model put gan generate abstract image yes x200b bunch thanks
r7mekf,1,class lost confused greetings x200b let‚Äôs get disclaimers way first yes graded project makes significant part class grade looking y‚Äôall homework x200b y‚Äôall need help lost even know correct question ask machine learning college class pre requisites class linear algebra probabilities stats 2 calc 3 i‚Äôve also got compsci classes belt passed good grades past 12 weeks class theory lecture homework show calculate algorithm application functional use textbook cracked open mine still unscratched access code inside font cover project discussed semester last weeks ‚Äòresearch‚Äô yesterday get rubric presentation 10 days one class lost said would presentation financial forecasting data set pulled public source historical data ease discussion i‚Äôll call 50x50 matrix ‚Äì 50 numeric observations rows 50 categories col category averages wildly vary first thing normalize 1 col col drop new matrix ok great also know need break training verification active use testing picked 70 15 15 lop first 35 normalized rows matlab prescribed course instructor said ‚Äúoh perfect support vector machine ‚Äù couple days ago gave stack finance time series journal article x200b frack don‚Äôt know go instructor believe could help idea question ask least get either classification app ‚Äòeconometric modeler‚Äù computational finance app section open data loaded don‚Äôt understand anything outputs completely lost
r45wdo,0,ai everything whole wide world benchmark really interesting criticism general benchmarks e g glue imagenet construct validity issues 1974 sesame street children‚Äôs storybook grover everything whole wide world museum stiles wilcox 1974 muppet monster grover visits museum claiming showcase ‚Äúeverything whole wide world‚Äù example objects representing certain categories fill room several categories arbitrary subjective including showrooms ‚Äúthings find wall‚Äù ‚Äúthe things tickle room‚Äù oddly specific ‚Äúthe carrot room‚Äù others unhelpfully vague like ‚Äúthe tall hall‚Äù thinks seen grover comes door labeled ‚Äúeverything else‚Äù opens door find outside world children‚Äôs story grover‚Äôs described situation meant absurd however paper discuss similar faulty logic inherent recent trends artificial intelligence ai ‚Äî specifically machine learning ml ‚Äî evaluation many popular benchmarks rely false assumptions inherent ridiculous ‚Äúeverything whole wide world museum‚Äù grover visits particular argue benchmarks presented measurements progress towards general ability within vague tasks ‚Äúvisual understanding‚Äù ‚Äúlanguage understanding‚Äù ineffective finite museum representing ‚Äúeverything whole wide world ‚Äù similar reasons ‚Äî inherently specific finite contextual
rn63vl,0,cliprcnn tiny text guided zero shot object detector propose new tiny architecture zero shot object detector inspired classical r cnn object detector named cliprcnn works 1 first generate region proposals need class agnostic region proposals generator chose classical algorithm named selective search instead modern pre trained object detection networks step similar proposal generation r cnn network 2 compute clip loss proposal user prompts texts images 3 last return top k best proposals minimum clip loss prediction cliprcnn model approach perfect really simple works writing lines code find implementation cliprcnn
rmxri9,0,residual skip connections work looking literature share comments insights discussion particular works mind aside mention extremely subtle hint recent work remember coming across cannot recall title info work analyzing residual connections one sections commented function block skip connection easier train learn domain shift original input data space sure specifically used words domain shift close met particular framing paper besides everything else may want comment share appreciate comment share papers dealing particular framing well come ml literature stats math etc also welcome help clarify jargon heavy parts existing relevant threads
rpqudp,0,paper explained federated learning mobile keyboard prediction ever wondered mobile keyboard gives next word suggestions give personalised suggestions time ensuring privacy individuals check blog post federated learning mobile keyboard prediction talks happens privacy preserving manner blog post ppml series 3 federated learning mobile keyboard prediction annotated paper annotated ml papers federated learning mobile keyboard prediction
r4um3h,1,anyone recommend courses give solid foundations nlp guys please recommend good nlp courses free computational neuroscience robotics masters solid computer vision course gave basic knowledge like optics matching geometry basic algorithms etc overall provided great overview field started cnns moved gans need use nlp techniques analysis looking similar courses give basic info first
rwcikx,0,predicting future labels future values features known rnns time series done fair bit work rnns time series data realize may fundamental gap knowledge reading raised questions different kinds time forecasting problems summary works please correct i‚Äôm wrong scenario 1 want predict value stocks future let‚Äôs say k stocks n days data don‚Äôt features labels stocks rather input stocks‚Äô current previous values output stocks‚Äô future values two main options ‚Äòauto regressive‚Äô approach predict values one step time feed back ‚Äòsingle shot‚Äô approach predict values fixed amount time steps future time seems pretty standard well understood problem scenario 2 3 time series features b c one label know dependent b c goal predict future values understanding can‚Äôt use auto regressive approach aren‚Äôt predicting b c single shot approach input would shape 4 w output would shape 1 p w number warmup timesteps p number timesteps want predict y‚Äôs value know future values b c p timesteps future extra information would obviously help prediction essentially could allow model peak future b c however complications arise can‚Äôt pass input shape 4 w p don‚Äôt know future values that‚Äôs trying predict i‚Äôve able find papers information type problem likely don‚Äôt know name type problem i‚Äôve couple thoughts future values features known future values separate features would input shape 7 w rather 4 w since know future values 3 features sort recursive approach first come arbitrary prediction p time steps future training would pass input shape 4 w p past future values b c past predicted values output shape 1 p loss would distance difference predicted future values actual future values replace arbitrarily generated future values ones generated repeat process 100 sure would work though i‚Äôd suss details scenario 3 similar previous scenario instead knowing future values b c know future values example let temperature b c rainfall humidity cloud cover respectively let‚Äôs say use meteorological rainfall predictions know values future would ‚Äòknow‚Äô future feature information obviously issues using another prediction future value feature let‚Äôs leave aside i‚Äôm curious work done area intuition says treating b c inputs know dependent b c adds extra layer context would good incorporate problem
rn9f19,0,cool random forest ml applications seen jaw dropping examples neural networks deep learning e g deep fakes looking similarly awesome examples random forests please share
qzr5z6,0,working nlp engineers industry learn get par basically interviews coming nlp focused mle jobs familiar basic stuff like word embeddings rnn lstms bit transformers however knowledge latter pretty shallow definitely ton need learn currently working stanford cs224n problem sets projects current schedule kind forces skim also working building nlp web app uses gpt 2 model generate onion articles familiar ml worked mle though mostly computer vision stuff currently work mlops engineer mostly avoided nlp recently currently work nlp engineers know really qualify nlp engineer without ms phd x200b edit cs224n cs22n
rjeqln,1,fastai killing hey guys working fastai course currently chapter 8 feels like retaining maybe 20 information taught rushing anything try understand every topic feels like things quite explained trying search puts large math rabbit hole normal feeling new user taking course weeks going start 2 courses supposed teach essential mathematics stats machine learning hoping completion two courses stuff start make sense guys think go even slower start learning math soon possible anyone else sort experience fastai getter better later course thanks advance guys
rbaq7a,1,apply normalisation data let assume data set tweets binary variables represents positive negative tweet need normalise binary data want apply naive bayes 3 category positive 1 negative 1 natural 0 need normalisation
r0g2o2,0,ai safety needs great engineers top line think could write substantial pull request major machine learning library major ai safety labs want interview today work anthropic industrial ai research lab focussed safety bottlenecked aligned engineering talent specifically engineering talent always like ops folk researchers safety work limited shortage great engineers spoken several ai safety research organisations feel engineers may last year openai released gpt 3 system surprisingly well surprisingly broad range tasks limited many important ways lot ai safety folk sat noticed systems like gpt 3 might existential threat many us worried plausible issues found future systems might already present gpt 3 plausible think solving issues gpt 3 help us solve equivalent issues future systems worried ai safety suddenly developed empirical subfield could make predictions might go wrong might fix things actually run experiments experiments never entirety field new promising direction leverages different skill set classic ai safety particular different skill set leverages engineering running experiments real weak ai system requires substantial stack custom software projects running hundreds thousands millions lines code dealing projects skillset many folks ai safety invested prior last 18 months shows recruitment kind engineers looking engineers anthropic right every one great software engineer prior joining ai safety every one also easy get beyond common traits experience distributed systems experience numerical systems caring thinking lot ai safety comfortable reading contemporary ml research papers expertise security infrastructure data numerics social science one dozen hard find specialities requirements list though based people working already great software engineer easy get hard requirements things list much nice haves several folks one none right job listings bucketed security engineer infrastructure engineer research engineer like noun phrases lot people like identify actually concerned generally great software engineers ideally extra bit deep experience lack engineering compare research anthropic hard distinction researchers engineers organisations retain distinction increasing reliance research substantial custom infrastructure dissolving boundary every industrial lab familiar might hard believe think archetypal research engineering organisation one researchers come fun prototypes toss wall engineers clean implement think archetype common enough dissuades lot engineers applying engineering roles instead applying research positions evaluated different set metrics ones best underperform changed modern ai safety prototypes require serious engineering prototyping experimenting engineering problem get go thousand line nested loop carry research far think might hard sell folks endured older kinds research organisations anecdotes first two authors gpt 3 engineers pure engineers anthropic spend weeks staring learning curves experimenting architectural variants one pure researchers anthropic spent week rewriting rpc protocol excited ever seen anthropic folk new hire engineer builds academic clusters hobby apply hard judge sight unseen whether specific person would suit ai safety engineering good litmus test one given top post weeks work could hypothetically write new feature fix serious bug major ml library already could get month two effort like litmus test close colleagues day strong enough engineer make successful pull request pytorch likely strong enough engineer make successful pull request internal repos actually litmus test one half actual litmus test give folk meet half tell thoughts ai future pass nuanced well thought response skill post aimed folks already pass litmus test originally intended pair another post skilling point able pass test turned much difficult topic expected recommend starting 80k software engineering guide take homes want great engineers could write pull request major ml library apply anthropic know one great engineers ask could write pull request major ml library yes tell apply anthropic like watch space working skilling advice twinned version post lesswrong
r70j6e,0,questions possible self plagiarism neurips paper neurips 2021 around corner came across scene text related work titled centripetaltext efficient text instance representation scene text detection paper interesting paper authors claimed introduced new text instance representation scene images based text kernels central text regions centripetal shifts upon reading paper found main idea proposed method closely related similar another paper bidirectional regression arbitrary shaped text detection paper b original authors strangely paper b published paper mentioned paper think paper high similarity paper b timeline papers paper b first submitted accepted icdar 2021 submission deadline 8th feb camera ready deadline 17th may paper submitted accepted neurips 2021 submission deadline 28th may camera ready deadline 26th oct questions 1 common contributions text representation proposed method introduced twice two independent papers submitted two separate conferences 2 authors cite latest previous work explain similarities differences case authors already known icdar paper accepted right submission neurips 3 considered case self plagiarism rules publication conferences especially dual submissions
r7f2wb,1,built pill identifier using edge impulse wanted way classify images phone without using internet built project edge impulse used deep learning cnn image classification model deployed phone simply stated mom mixes pills takes pill containers always good internet connection model perfect think shows useful ml project versus cool trendy one one actually use real world wrote steps took edge impulse project public available fyi edge impulse free devs
rv91cx,1,make ai similar jarvis iron man started coding eventual goal make ai could similar jarvis iron man know lot stuff need download wondering anyone knew needed safest ways also using python sorry really sub meant better place please send
qk144p,0,anyone powerful computers deploying locally computer pretty good cpu gpu rather spend time get static ip set computer official server isp move model setup expensive instance cloud easier way run inference server machine aware
rc32jz,0,looking website like arxiv foreign language would like document classification topic modeling non english documents nlp work downloaded many articles arxiv faq says accept submissions multiple languages found way search ideally looking hundred documents 4 5 different topics like chinese russian anyone know open free source journal articles languages
rlkyv6,1,basic linear regression first article please look give inputs
rwo3ia,1,messing cnn convert grayscale rgb hello fellow learners might seen old black white photo colorized ever tried done simple architecture auto encoder tweaking colorspaces read converting grayscale image rgb please leave feedbacks improve content quality thank üôÇ
r81v4u,0,supervised training spiking neural networks robust deployment mixed signal neuromorphic processors spiking neural networks running mixed signal devices promise highly energy efficient edge ml inference deployment issue ‚Äî mismatch devices breaks network performance mixed signal devices rely sub threshold analog signals small differences transistors lead large changes neuron network behaviour chips train calibrate every chip that‚Äôs hugely expensive scale used class spiking networks called efficient balanced nets labs sophie den√®ve christian machens correct errors network encoding built supervised training method around architecture copying network dynamics trained teacher ann learn arbitrary tasks robust snns compared approach methods training snns ‚Äî liquid state machines spiking force training bptt ‚Äî showed approach robust parameter mismatch preprint arxiv open access sci rep code github
rnkwp7,1,good beginner exercise improving programming monte carlo simulation approximation number pi œÄ good exercise understanding basic data science concepts monte carlo simulation
r47lvu,1,looking beginners try machine learning online course hello preparing series courses train aspiring data scientists either starting scratch wanting career change example software engineering physics looking students would like enroll early free give feedback courses first course foundations machine learning cover pretty much everything need know pass interview field worked data science ten years interviewed lot candidates course focused important know avoiding typical red flags without spending time irrelevant things outdated methods lengthy math proofs etc please send private message would like participate comment
r76vis,1,help first complex ml code hi limited experience ml would like create somehting little complex expecting full solutions anyone could tell keywords would need google get started kind ml could solve would appreciated baisically work contract manufacturing pharma company get new drug need make list around 20 30 different liquids involved making drug itll come form like liquid volume needed l corrosiveness factor needed step 1 needed step 2 water 10000 1 x x acid 5000 50 x etc x200b want automate long tedious process allocating tanks 1 make 2 store liquid list tanks like 1 preparation tanks 1 storing tank name volume l material p 001 20000 steel p 002 5000 reinforced steel etc x200b complicate another table like tank p 001 p 002 etc 001 x x 002 x etc x200b basicially long table showing connections preparation storage tanks x means piping available pump liquid example piping prep tank 001 storage tank 001 p 001 002 therefore cannot prepare liquid p 001 store 002 also another table tank step 1 step 2 001 x x 002 x etc shows connections storage tanks acutal manufactuing floor liquid needed step 1 2 stored 001 avoid needing new piping etc long list rules x200b must strictly follow preparation storage connections new connections made liquid corrosiveness factor number 30 needs preped stored reinforced steel vessel less corrosive liquids stored tank volume tanks sufficient prep store liquid inevitable new connections storage tanks manufacturing floor might need made allocation designed keep minimum output look something like liquid prep tank allocation storage tank allocation water p 001 001 acid p 002 002 etc along number new pipings need made storage manufacturing understand complex method ml take multiple tables like along strict set rules along around 30 previous allocation projects make algorithm determine allocations give list liquids volumes corrosiveness needed process thanks
rrydgm,0,ecco language model analysis visualization toolkit hi r machinelearning last couple years building toolkit explore visualize transformer language models brings together wide variety tools methods visualize analyze model inner workings activation spaces features support wide variety language models gpt2 bert roberta t5 t0 others ability add local models based hugging face pytorch models feature attribution integratedgradients saliency inputxgradient deeplift deepliftshap guidedbackprop guidedgradcam deconvolution lrp via captum capture neuron activations ffnn layer transformer block identify visualize neuron activation patterns via non negative matrix factorization examine neuron activations via comparisons activations spaces using svcca pwcca cka visualizations evolution processing token layers model logit lens candidate output tokens probabilities layer model x200b released v0 1 0 brings encoder decoder model support t5 t0 feature attribution via integrated gradients many methods provided captum github paper
qlilnf,0,zillow‚Äôs nn based zestimate leads massive losses home flipping business zillow announced laying quarter workforce due 420 million loss incurred zillow offers home flipping arm business business model reliant zestimate neural network based model forecasts housing prices seems like colossal misstep part begs question companies avoid similar fate making large gambles based machine learning models predicting market movements additionally much consumers rely market predictions like zestimate making financial decisions speaking someone recently bought home researched market zillow process
remf8e,1,ocr handwritten devanagari script hi everyone project handwritten word recognition devanagari script using deep learning find good resources even datasets somebody please help edit trying implement cnn rnn hybrid model require segmentation characters word takes whole word model input
r9b2kp,1,public streaming datasets hi personal research purposes looking tabular streaming data public datasets two classes something along lines would love hear know something similar thanks
ruifyk,1,understanding architecture dnns specifically cnns intuition behind many neurons layer dnn example resnet50 first layer 7x7 64 kernel convolution 64 kernels specifically instead 63 66 random number industry accepted number number kernels treated hyperparam validated tf‚Äôs tutorials use dnn classify mnist digits layers flatten input 28 28 dense 128 relu dense 10 second layer output 128 neurons seems like magic number can‚Äôt find reasoning behind
qybvz4,0,asking multi model architecture papers hey trying read gap resources demanded multi model applications resource limitation commodity hardware right looking former exemplification deepeye employs 4 5 cnn models perform unique task single device anyone aware similar papers employ multi model architectures would amazing left paper comments new community need specify question need make edits please let know looking forward participating community
rsn858,0,drop best open source deep learning related project would anyone like share open source projects deep learning federated learning blockchain deep learning distributed job parallel scheduling based pytorch tensoflow look
rbm3yp,1,i‚Äôm interested machine learning start i‚Äôm looking advice tips start learning
rogiq2,0,weak supervision practice collect strongly labelled data spent time trying find good resources practical weak supervision tutorials guides research papers somehow reasonable etc assume strongly labelled validation set start defining rules labeling functions cases validation set also used evaluate labeling functions e g snorkel via lf summary context love know people practice first label collect validation set weak supervision workflows starting rules lfs labelling subset weakly labelled data would love hear experiences pointers
rhrqcf,1,unsupervised learning string matching python advice go hello data engineer around 8 months experience currently working company one recurring requests fuzzy matching two lists names current solution using ssis fuzzy lookup feature although opinion simply scale well limited features envision create something ideally ml model improve based whether user tells model something match moment threshold ssis simply static missing matches like included top manually maintained model something like idea want make fairly hands familiar working python various libraries basic understanding ml although particularly sure right thing tried fuzzywuzzy rapidfuzz well tried dedupe recordlinkagetoolkit however seem wrap head around need order persist model improved way trying achieve totally get small project ready put time like know achieve want libraries already mentioned incorporate scikitlearn fuzzy matching using one libraries service already exists handle enterprise scale fuzzy matching thank
r086ev,0,regularizing method use batch size 1 besides dropout right dealing transformers good amount encoder decode layers result due limited memory one batch time given heard dropout popular right heard try another method regularizing method recommend works batch size 1 batch normalization probably work
rcz0bd,1,beginner want build natural language model classify comments start hello redditors beginner machine learning currently working entry level data analyst team wants implement model classify comment provided customers customers consist people coming non english background know english sometimes cannot spell properly sure spelling mistakes would affect model attached image sample dataset give idea data working bogus data sample like 50 000 80 000 comments dataset know start done bit traditional machine learning using sci kit learn never stepped nlp searched internet results overwhelming saw implementations using bert huggingface shed light suggest good reading materials youtube videos tackle using model like bert huggingface would easy building rnn scratch job better another question comment service ws fasr need spell correction passing model libraries using python familiar r well model good would deploy cloud later question forum thanks advance
rvt8ik,1,download 20gb dataset train tensorflow model google colaboratory hi guys developed tensorflow model google co lab used mini dataset train model want train model real dataset would around 20gb capacity wonder 1 train model much high volume dataset 2 possible train model google colab 3 download dataset programmatically google colab go ahead start training process fyi downloaded mini dataset google colab proceed next steps download large dataset like large dataset
rfhp4g,1,ml roadmap hi im actually learning ml wondering kind roadmap study guide focused important learn ty reading
r3ajsj,0,discussion face recognition remarkable papers new face recognition anyone help recent remarkable papers field started facenet spherface cosface arcface want continue new papers
rvv2j4,1,moving keras python mlpacl c hey guys first steps keras implemented nn new usecase want translate mlpack c definitions seem quitedifferent struggling maybe someone could answer following questions 1 use ffn class model add linear equivalent keras dense layer e fully connected 2 viable replace dense activation relu model add relulayer 3 equivalent lambda layer thanks advance guys
rsumxg,1,l1 distance 2 minute visual guide x200b üîµ manhattan distance üîµ üóΩyou might heard euclidean l2 distance heard l1 distance also known manhattan distance üöïthe manhattan distance computed treating geometry street manhattan one square city block followed another way travel go along right angled streets shortest euclidean distance two points unique path case manhattan distance multiple paths distance üåÉ mathematically l1 distance sum absolute value difference coordinate point vector extended n dimensions ü§ì l1 distance l1 norm also used regularize model parameters regularization penalizes model parameters fitting l1 norm forces model parameters sparse would shrink non important features towards zero coefficients model could used understand features important e features correspond model parameters larger values example linear regression model l1 regularization predicts price house features number rooms area color house fitting model data see coefficients corresponding number rooms area color house 0 5 0 6 0 01 see model treats number rooms area important features color house 0 5 0 01 0 6 0 01 determining price like content would like steer topics cover feel free suggest topics would like know comments
qlffsv,0,neural architecture search neuroevolution hi fellow readers come recently slight confusion understand nas neuroevolution would like hear explanations current understanding follows nas technique used automate process designing optimizing neural networks nns 1 technique divided three components search space defines types layers depth type connections search strategy defines approach used explore search space evaluation strategy evaluates performance build ann design neuroevolution technique harnesses evolutionary algorithms ea design optimize nns augment nn changing topology connections weights 2 based observed action environment confusion therefore description understand nas neuroevolution explained applied techniques design optimize nn topologies nas use kind algorithm design nn topologies rl ea gradient descent neuroevolution uses ea neuroevolution defined search strategy nas means neuroevolution used nas search strategy like example reinforcement learning rl 3 simplify would like understand think nas neuroevolution researching goal understand put puzzles together building automated machine learning process specific task anomaly detection made mistakes silly comparisons please point comments future readers grasp mistakes knowledge comments welcome thank advance
rebagw,1,process creating machine learning model using u net understood correctly university course learning u net used image segmentation e g cancer biology however someone please tell misunderstood things wrong workflow things tried find research papers online give overview find dang thing apart articles outline basics 1 step would clean sort data e g making sure duplicates image dataset 2 divide dataset test set validation set training set 3 training set used model learn 4 validation set used look accurate model change hyper parameters make accurate type hyper parameters used though looking image segmentation 5 use test set final test evaluate well performs evaluate well performs using things like dice coefficient used test set go back refine model supposed training phase thanks advance
qlt4cz,0,aaai 2022 paper reviews aaai 2022 reviews creating discussion thread year reviews
rs8a2z,1,create retraining protocol pipelines machine learning models specifically case computer vision models using cnns inevitably encounter data model well suited one could keep log store data points use later training justified way retrain models time especially case something like face detection data distribution necessarily change time rather simply certain points underrepresented original training data let assume model already performs acceptably data points trade offs 1 retraining whole dataset including erroneous samples good labels 2 training model perhaps early stopping validation original new datasets erroneous samples 3 training whole dataset preferentially sampling erroneous samples x200b name process find best practices theoretical discussions kind process would appreciate help real issue wish solve deployed model
rwwdh0,0,multi agent deep reinforcement learning hello hope you¬¥re well working multi agent system maddpg time agent asks task agents busy e busy agents still processing task didn¬¥t finish yet configuration learning phase don¬¥t know mask state busy agents injecting state action pair critic network thank
r7vnzk,1,say model nothing rmse sd validation set training univariate regression model using neural network training set sd 0 6 train model val rmse get around 0 6 also understand rmse sd unit similar formula sd rmse say model predicting mean value dataset basically useless rmse sd say model poor predictor
r21dxk,0,project aim 3 1 open source images tracking images explorer hey r machinelearning gev co author aim sharing aim v3 1 new version contains following three notable changes images tracking explorer improved ui backend performance better runs navigation images tracking major item crossed aim roadmap images explorer aim ui short aim ui images explorer video code web release notes bit ly 3clz6hy x200b would love feedback work make aim better
qmw43e,0,best approach noisy language detection need predict language e g english portuguese russian words sentences noisy real world data meaning words might misspelled poor grammar emojis language switching etc bunch different github repos unclear one works well especially noisy real world data e g wikipedia hoped websites like would solved see relevant category language detection strong requirements solution particularly efficient although transformer approach seems overkill advice simple python library use
rs6k65,0,important metrics labeling data lot great articles measuring performance data annotators‚Äô agreement labels like one see mentions lot places cohen‚Äôs kappa krippendorf‚Äôs alpha fleischer‚Äôs kappa comparing predefined ground truth etc you‚Äôre managing annotation process organization evaluate annotators challenges faced process side note anyone using programmatic labeling real dataset thoughts
ru86ji,1,time series classification hi everyone like take pictures freight trains area railroad broadcasts simple data unencrypted radio show basic info going get exact position train confirmation train certain piece track 30ft 30 miles long info track signals showing long story short able collect data written programs identify train passed given part track image data plotted distance time manually highlighted would like able ml classify given train detection location belonging given train trains moving right eastbound trains moving right westbound trains almost exclusively make journey way one end common eastbound stop siding let one westbounds pass fairly rarely train start one direction turn around come back way complication radio data missed depending various conditions guarantee pick every train every control point pick enough data fill gaps least brain half software half electrical engineer taken couple beginner courses ml really sure type classifier use organize data also 100 sure task ml seems natural would especially considering sometimes missing data inferred would appreciative would willing help screenshot 20220102 062841 2 png
qsvzio,0,tsflex flexible efficient feature extraction time series looking time series feature extraction package efficient flexible tsflex got covered published release allows integration tsfresh well go check üëâ
qmw9lr,0,survey study examining practices nlg evaluation work research natural language generation nlg yes interested participation 20 minute survey practices evaluating nlg systems models participants experience working type natural language generation nlg systems tasks purpose research uncover unnamed practices assumptions made evaluation nlg systems applications tasks hope understanding practices assumptions able better unpack ways could lead unintended consequences related fairness inclusion interested please fill form thank much consideration help
r1sj1e,1,gan make people real smile best like train gan get people sad smile people real people like people famous painting like mona lisa gan model pretraned model use dataset use please thanks lot
rjmsc4,1,think educational game true learn
r7otcv,0,methods measuring label consistency within object detection relatively small custom dataset multi class object detection get maximum 0 48 map data labeled one person practices field dataset related xrays would want measure label consistency correctness annotations methods
qtzbu1,0,machine learning wayr reading week 125 place share machine learning research papers journals articles reading week relates researching means elaborate give us insight otherwise could interesting paper read please try provide insight understanding please post things present wiki preferably link arxiv page pdf easily access pdf summary page way around pertinent links previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 121 130 week 1 11 21 31 41 51 61 71 81 91 101 111 121 week 2 12 22 32 42 52 62 72 82 92 102 112 122 week 3 13 23 33 43 53 63 73 83 93 103 113 123 week 4 14 24 34 44 54 64 74 84 94 104 114 124 week 5 15 25 35 45 55 65 75 85 95 105 115 week 6 16 26 36 46 56 66 76 86 96 106 116 week 7 17 27 37 47 57 67 77 87 97 107 117 week 8 18 28 38 48 58 68 78 88 98 108 118 week 9 19 29 39 49 59 69 79 89 99 109 119 week 10 20 30 40 50 60 70 80 90 100 110 120 upvoted papers two weeks ago u catalyzex code bot paper link besides rules fun
rgosh5,0,future artificial intelligence self organizing self assembling blog post sebastian risi excerpt first post series plan write work group others combines ideas deep learning ideas self organization collective systems first post we‚Äôll look developed approaches domains applied ranging growing soft robots minecraft machines self assembling modular robots creating resilient adaptive reinforcement learning agents merger ideas could ultimately allow ai systems escape current limitations brittle rigid able deal novel situations however combination methods also poses new challenges requires novel ways training work efficiently possible one fascinating aspects nature groups millions even trillions elements self assemble complex forms based local interactions display called collective type intelligence example ants join create bridges rafts navigate difficult terrain termites build nests several meters high without externally imposed plan thousands bees work together integrated whole make accurate decisions search food new nest surprisingly achieving incredible abilities result following relatively simple behavioral rules process self organization camazine et al 2001 define ‚Äúas process pattern global level system emerges solely numerous interactions among lower level components system moreover rules specifying interactions among system‚Äôs components executed using local information without reference global pattern short pattern emergent property system rather imposed system external ordering influence ‚Äú self organizing systems made many components highly interconnected absence centralized control allows quickly adjust new stimuli changing environmental conditions additionally collective intelligence systems made many simpler individuals built redundancy high degree resilience robustness individuals collective system fail without overall system breaking multicellular organisms learned exploit self organizational principles self assemble starting single egg cell process local cell interaction embryonic development similar robustness swarms organisms self organization cell populations remarkably robust perturbations animals goes far able regenerate complete body parts salamander‚Äôs tail type self repair common feature self organizing systems interestingly involve additional processes ‚Äúthe self organization process built initial pattern operate repair pattern ‚Äù ‚Äî camazine et al 2001 rest blog
rdl6ib,1,uncertainty estimation input space hi assuming input array 0 1000 output corresponding system velocity training data generated randomly applying one input values 0 1000 corresponding system velocity saved output way estimate uncertainty input data input area 0 100 applied system measure uncertainty
rcqvzh,1,games like true learn help learn machine learning seen others game specifically given free epic games basically least first 20 puzzles get random input shapes colors sort outputs using decision trees x else random expert systems x else others similars fulfill output requirements like red blue red red blue triangles green circles playing understand puzzles logic behind relation machine learning basics could ml concepts applied solving kind puzzles develop logic path used game apply ml understand cannot build ml system playing game like know involves lot structure logic data treatment data collection wanted ask relate playing game others personally tried one apply others usually try sorting balancing outputs get satisfying solutions experience programming electronics know way
rjyll8,1,list 8000 programming resources hope helps list list resources python machine learning web design etc list write feedback comments
roy4nw,0,comparison player games alphazero blog post describe differences deepminds new algorithm player games alphazero describe gt cfr code examples implement python blog post
