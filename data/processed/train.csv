label,text
0,ml libs robust to missing data so i was searching on how to handle missing data and came accross this post from machine learning mastery this article states that some algorithms that can be made robust to missing data such as naive bayes and knn not all algorithms fail when there is missing data there are algorithms that can be made robust to missing data such as k nearest neighbors that can ignore a column from a distance measure when a value is missing naive bayes can also support missing values when making a prediction but then it says that sklearn implementations are not robust to missing data sadly the scikit learn implementations of naive bayes decision trees and k nearest neighbors are not robust to missing values are there ml libs preferably in python but could also be in other languages that these algorithms are robust to missing data thanks
2,help with multi class segmentation images hi guys i need some help with how to transform my images to create a multi class segmentation using a u net my question is regarding the target images they are 1d with three colors 0 to background 128 to my first class and 255 for my second class when i m calling the images they get transformed mask mask 128 1 mask mask 255 2 i m using 3 out channels for the last part of the u net and i m using cross entropy as my loss function is this correct because i m feeling that i might need to one hot encode my target masks
0,how many projects are you able to work efficiently on hey everyone i m currently working actively on 2 projects with 3 more coming up in the next weeks i ve been working with sql trying to define patient cohorts for later statistical analysis in r the sql work has been slow to my liking but i only started working here in january and on a new database of which i had to move out from after the first 2 months to another database same data source but different structure so i had to redo a good chunk of the initial progress i would say i ve been only making real progress during the past 4 months add to that that i have at least 2 meetings a week my current contract is supposed to expire by the end of this month and they ll extend it for another 3 months for me to deliver the first 2 projects but they still expect me to work to some degree on the other 3 that are coming my question is if you feel like they re setting me up to fail on this position or working on 5 projects is actually doable obviously i m going to focus on the ones that they re going to evaluate me on but considering i m the first data scientist here i think they re not used to having the vast majority of the time defining the data extraction parameters and cleaning the data and only a small portion on the actual statistical analysis or ml modelling at the time of starting this job i had 9 months of experience but this is my first job after getting my msc i already had some sql experience but mainly from internet tutorials and some simple joins from my previous position what do you think
0,physics phd transitioning to data science any advices hello i will soon get my phd in physics being a little underwhelmed by academia and physics i am thinking about making the transition to data related fields which seem really awesome and is also the only hiring market for scientists where i live my main issue is that my cv is hard to sell to the data world i ve got a paper on ml been doing data analysis for almost all my phd and got decent analytics in python etc but i can t say my skills are at production level the market also seems to have evolved rapidly jobs qualifications are extremely tight requiring advanced database management data piping etc during my entire education i ve been sold the idea that everybody hires physicists because they can learn anything pretty fast companies were supposed to hire and train us apparently from what i understand now this might not be the case as companies now have plethora of proper computer scientists at their disposal i still have 1 year of funding left after my graduation which i intend to use to search for a job and acquire the skills needed to enter the field i was wondering if anyone had done this transition in the recent years what are the main things i should consider learning first from what i understand git version control sql nosql are a must is there anything else that comes to your mind how about soft skills how did you fit in with actual data engineers and analysts i m really looking for any information that comes to your mind and things you wished you knew beforehand thanks
2,repvgg making vgg style convnets great again update our recent repvgg model reaches around 83 5 top 1 acc on imagenet not included in the paper but released on github do you still remember what happiness convnets convolutional neural networks brought to you seven years ago when you could improve the performance by simply stacking several more conv layers x200b our recent work repvgg is a super simple vgg like architecture the body has nothing but a stack of 3x3 conv and relu it has a favorable speed accuracy trade off compared to other state of the art models on imagenet it achieves over 80 top 1 accuracy such good performance is realized by a structural re parameterization so that it is named repvgg x200b repvgg uses no nas no attention no novel activation functions and even no branches how could a model with nothing but a stack of 3x3 conv and relu achieve sota performance x200b x200b paper pretrained models and code（pytorch）： got 1 7k stars and pretty much positive feedback how simple can it be after reading the paper you may finish writing the code and start training in one hour you will see the results the next day if you use eight 1080ti gpus if you don’t have time for reading the paper or even this blog just read the first 100 lines of the following code and everything will be crystal clear what is vgg like when we are talking about vgg like we mean 1 the model shall have no branches we usually use “plain” or “feed forward” to describe such a topology 2 the model shall use only 3x3 conv 3 the model shall use only relu as the activation function the basic architecture is simple over 20 3x3 conv layers are stacked up and split into five stages and the first conv of every stage down samples with stride 2 the specifications depth and width are simple an instance repvgg a has 1 2 4 14 1 layers for its five stages repvgg b has 1 4 6 16 1 the widths are 64 128 256 512 scaled by multipliers like 1 5 2 2 5 the depth and width are casually set without careful tuning the training settings are simple we trained for 120 epochs on imagenet without tricks you can even train it with a pytorch official example style script so why do we want such a super simple model and how can it achieve sota performance why do we want vgg like model except for our pursuit for simplicity a vgg like super simple model has at least five advantages in practice the paper has more details 1 3x3 conv is very efficient on gpu the computational density theoretical flops time usage may achieve four times as that of 1x1 or 5x5 conv 2 single path architecture is very efficient because it has a high degree of parallelism with the same flops a few big operators are much faster than many small operators 3 single path architecture is memory economical for example the shortcut of resnet increases 1x memory footprint 4 single path architecture is flexible because we can easily change the width of every layer e g via channel pruning 5 the body of repvgg has only one type of operator 3x3conv relu when designing a specialized inference chip given the chip size or power consumption the fewer types of operator we require the more computing units we can integrate onto the chip so that we can integrate an enormous number of 3x3conv relu units to make the inference extremely efficient don’t forget that single path architecture also allows us to use fewer memory units structural re parameterization makes vgg great again the primary shortcoming of vgg is of course the poor performance these years a lot of research interests have been shifted from vgg to the numerous multi branch architectures resnet inception densenet nas generated models etc and it has been recognized that multi branch models are usually more powerful than vgg like ones for example a prior work stated that an explanation to the good performance of resnet is that its shortcuts produce an implicit ensemble of numerous sub models because the total number of paths doubles at each branch obviously a vgg like model has no such advantage a multi branch architecture is beneficial to training but we want the deployed model to be single path so we propose to decouple the training time multi branch and inference time single path architecture we are used to using convnets like this 1 train a model 2 deploy that model but here we propose a new methodology 1 train a multi branch model 2 equivalently transform the multi branch model into a single path model 3 deploy the single path model in this way we can take advantage of the multi branch training high performance and single path inference fast and memory economical apparently the key is how to construct such a multi branch model and the corresponding transformation our implementation adds a parallel 1x1 conv and an identity branch if the input and output dimensions match for each 3x3 conv to form a repvgg block this design borrows the idea of resnet but the difference is that resnet adds a branch every two or three layers but we add two branches for every 3x3 layer x200b after training we do the equivalent transformation to get the model for deployment this transformation is quite simple because a 1x1 conv is a special with many zero values 3x3 conv and an identity mapping is a special the kernel is an identity matrix 1x1 conv by the linearity more precisely additivity of convolution we can merge the three branches of a repvgg block into a single 3x3 conv the following figure describes the transformation in this example we have 2 input channels and output channels so that the parameters of the 3x3 conv are four 3x3 matrices the parameters of the 1x1 conv form a 2x2 matrix note that the three branches all have bn batch normalization and the parameters include the accumulated mean standard deviation and the learned scaling factor and bias bn does not hinder our transformation because a conv and its following inference time bn can be equivalently converted into a conv with bias we usually refer to this as “bn fusion” the paper and code contains some details just a few lines of code x200b after “bn fusion” of the three branches note that identity can be viewed as a “conv” and the parameters form a 2x2 identity matrix we use 0 to pad the 1x1 kernel into 3x3 at last we simply add up the three kernels and three biases in this way every transformed repvgg block has the same outputs as before so that the trained model can be equivalently transformed into a single path model with only 3x3 conv x200b here we can see what “structural re parameterization” means the training time structure is coupled with a set of parameters and the inference time structure is coupled with another set by equivalently transforming the parameters of the former into the latter we can equivalently transform the structure of the former into the latter experimental results on 1080ti repvgg models have a favorable speed accuracy trade off with the same training settings the speed examples second of repvgg models are 183 of resnet 50 201 of resnet 101 259 of efficientnet and 131 of regnet note that compared to efficientnet and regnet repvgg used no nas nor heavy iterative manual design it is also shown that it may be inappropriate to measure the speed of different architectures with the theoretical flops for example repvgg b2 has 10x flops as efficientnet b3 but runs 2x as fast on 1080ti so that the former has 20x computational density as the latter semantic segmentation experiments on cityscapes show that repvgg models deliver 1 1 7 higher miou than resnets with higher speed or run 62 faster with 0 37 higher miou a set of ablation studies and comparisons have shown that structural re parameterization is the key to the good performance of repvgg the paper has more details faqs please refer to the github repo for the details and explanations 1 is the inference time model’s output the same as the training time model yes 2 how to quantize a repvgg model post training quantization or quantization aware training are both okay how to finetune a pretrained repvgg model on other tasks finetune the training time model and do the transformation at the end reference 1 andreas veit michael j wilber and serge belongie residual networks behave like ensembles of relatively shallow networks in advances in neural information processing systems pages 550–558 2016 2 4 8
1,how do i calculate the average of irregular intervals hello first off english is not my main language and i m sorry if my question is not 100 clear i conducted a survey for my bachelor thesis and i m now trying to analyze the results i m trying to calculate the average time in minutes that the respondants need to go from point a to b based on 5 different intervals 1 to 5 minutes 14 respondants5 to 10 minutes 42 respondants10 to 15 minutes 35 respondants15 to 30 minutes 20 respondants30 minutes or more 1 respondant ugh i know that if it were regular intervals if that s how they re called in english i d simply need to take the average of each interval then multiply that by the number of respondants for that interval but in this case the 4th interval is 15 wide while all the others are 5 wide can someone please explain to me how i can solve this bonus question is there anything i can do with the one respondant that answered 30 thank you very much
0,do you respond to cold emails offering jobs how was your experience for example here is the subject line in an email to my personal address that i recently received 222k 276k cash year 10 20k public shares remote principal machine learning engineer goes on to explain that the shares are worth 200k then it starts telling me about the actual role explaining that they just raised all this money and blah blah blah i m not currently looking to change jobs but for those of you who ve responded to similar emails how d it go would you say that you had a positive experience edit i am surprised at the number of you saying that you respond at all i almost uniformly ignore them
2,does anybody know good research papers or resources about identifying drivers behaviors hi everyone i m starting my master s thesis in ai and i need to find information related to identification of driving behavior patterns spatial temporal data processing and determination of mobility patterns based on driving places types of roads places of interest etc could you please provide me with information thanks in advance
0,eastern university ms in data science my review so i m currently enrolled in eastern university s online ms in data science i haven t finished the degree yet but i m far enough into it that i can give an honest assessment for those interested in enrolling first the good stuff eastern university puts their students first the professors administrators and admissions staff at eu have it together when it comes to solving problems and interacting with students they usually respond to my emails within an hour and give helpful feedback eu is super military friendly i m using the post 9 11 gi bill to fund my studies if you have 100 gi bill then you get a degree for free and pocket at least 15k in stipends even if you only have 60 gi bill like i do you ll still net around 6k at eastern you only need to take one class at a time to qualify as a full time student so you can collect that 894 month stipend while only enrolled in a single class they also offer a 30 per credit discount for veterans and active duty personnel if you re using tuition assistance while serving the whole program will only cost you 500 out of pocket which is an insanely good deal for a graduate degree everything is self directed no weekly modules if you burn the midnight oil you can finish an entire seven week course in just a week or two that s quite convenient if you re a full time working professional with an unpredictable schedule the professor s videos give a clear description on how to use programming languages i enrolled with zero programming experience but they hold your hand through the whole thing and explain in detail how it all works at a theoretical level i usually have the instructional videos open in one window and my python editor open in another so i can pause and play around with it it s an effective way to learn and there are a few drawbacks academically it s not super challenging for example all the assignments can be retaken until you get the grade you want also as an online program it s possible for an unscrupulous student to cheat their way through although i have no idea why someone would spend all this money on a degree to learn data science and then just coast through the program if you want to gain proficiency with python r statistics etc you ll have to practice writing code and crunching numbers on your own in addition to the class assignments they do however provide you with all the learning materials you need to gain proficiency the student account website isn t the most user friendly simply signing up for a new class is a drawn out cumbersome process that took me a while to figure out the learning management system they use brightspace isn t the most efficient either the bottom line i know this sounds cliche but you ll get out of this program what you put into it if you only do the bare minimum to pass each course you can pay all this money just to get a fancy diploma but lack the data science knowledge to succeed in the real world if you go the extra mile and take time to master the material on your own you ll have a strong enough foundation in data science to get into the industry gain experience and build a successful career in a lucrative field eu s program is a convenient and affordable option for working professionals who want to branch out into data science and have the motivation to work for it and now i ll answer a few frequently asked questions how much time per week would i need to put in as a student it varies depending on how much time you need to practice and how many classes you take each course is seven weeks long i ve been taking one course per term while working full time as a civil engineer part time as a national guard officer and taking care of my 9 month old daughter and i still have had plenty of time to finish each class with a few weeks to spare if you plan ahead each week it s possible to balance this program with a full time job eastern university isn t as prestigious high ranking as for example ut austin or georgia tech how will employers judge a degree from eu i wouldn t worry too much about university rankings once you re in the industry and have experience no one cares where you got your degree you ll also find that many data scientists are completely self taught and have degrees in unrelated subjects some of them don t have degrees at all for example my brother attended a bottom tier open enrollment university and studied computer science for a few semesters before dropping out even though he never finished his degree he became proficient at writing code and now makes six figures as a software engineer all you have to do is get good enough at writing the code to pass a technical interview to make yourself marketable do i even need a degree for this career field isn t it possible to learn data science simply by watching youtube videos yes it is possible to learn data science on your own but the structure of a degree program makes it easier to keep a consistent study schedule it also gives you something palpable to list on a resume when you start applying for jobs as i ve stated earlier many private sector tech companies are loose on degree requirements however government and military jobs stipulate certain degrees for their applicants i currently work for dod as an engineer and every time i apply for a new position within our department i have to submit my transcript along with it an ms in data science can help you land a federal job as a data engineer operations research analyst statistician etc if you have further questions don t hesitate to ask
2,reconnaissance blind chess join our neurips competition create a bot for our neurips 2021 competition in reconnaissance blind chess reconnaissance blind chess is a chess variant designed for new research in artificial intelligence rbc includes imperfect information long term strategy explicit observations and almost no common knowledge these features appear in real world scenarios and challenge even state of the art algorithms each player of rbc controls traditional chess pieces but cannot directly see the locations of her opponent s pieces rather she learns partial information each turn by privately sensing a 3x3 area of the board rbc s foundation in traditional chess makes it familiar and entertaining to human players too there is no cost to enter this tournament winners will receive a small monetary prize and authors of the best ais will be invited talk about their bots at neurips the world s largest ai conference reconnaissance blind chess is now also a part of the new hidden information games competition higc being organized by deepmind and the czech technical university in prague learn more play a game of rbc yourself and join our research community at x200b organized by johns hopkins university applied physics laboratory with ashley j llorens microsoft research todd w neller gettysburg college raman arora johns hopkins university bo li university of illinois mykel j kochenderfer stanford university
1,measuring the sensitivity of decision trees on training data decision trees are known to be quite non robust and can fundamentally change due to small variations to data hence the appeal of random forests but is there a way to measure the potential sensitivity of a decision tree preferably before fitting one it s intuitive to imagine that decision trees and i guess all models get better with more data assuming the data is representative ofc but is there a generalisation theory of a decision trees robustness given an increasing sample size or is it too reliant on the features of a given dataset and thus varies on a case by case basis i guess this also applies to other non parametric information theoretic methods to various degrees
0,product sales marketing data reporting hi all i am the cmo of an ecommerce company and have a shopify store i have been working towards improving the way i go about presenting in our weekly advertising meetings in the past i ve listed out campaigns the performance of key metrics sales attributed from each platforms and then discussed budget based on the performance recently i have been trying to correlate our advertising campaigns with the percentage of sales related to specific advertising objectives for instance if we re spending 50 of our ad dollars promoting wedding products what percentage of our products sold are wedding related this has been helpful but i want to take it a few steps further i d like to chart product sales week over week but i am struggling to figure out the best way to go about it we have a handful of products which each have different designs customization options more specifically if one of our wedding products sold 30 times last week and only 12 times this week i would like it to say the product name the units sold and then the percentage change from the previous week the goal will be to use this data to monitor how certain products are selling and then make changes or not based on the data for instance if a product is not selling and we think it could be due to visibility we could test highlighting it on the homepage or other landing pages and see week over week if it preforms better does that make sense does anyone have a better way that they track and analyze sales and marketing dollars i also posted this in r analytics and someone suggested using ecommerce tracking in google analytics i am just confused how to go about setting this up
1,question uncertainty in binary outcomes i have a larger problem but have presented what i believe is a minimal example imagine that you are trying to determine the true probability of a potentially biased coin landing on heads and want to take a bayesian perspective our prior is hence that the probability of heads is beta 1 1 distributed say that we flip the coin once and we get a heads our posterior is now beta 2 1 then we flip once more but the coin lands crooked against an object on the table it looks like it would have landed tails but say that we are only 70 sure that it would have landed tails so 30 sure that it would have been heads obviously the best solution is to ignore and retest but if these coin flips are limited expensive that might not be ideal so is there anyway to include this result even with the uncertainty possibilities i ve considered are 1 ignore result p beta 2 1 2 include and pretend we are certain p beta 2 2 3 include with uncertainty p beta 2 1 7 4 include with uncertainty for both p beta 2 3 1 7 4 seems reasonable but i m worried this is a statistical golem and i m missing something obvious cheers
0,carvana lets you google while taking a coding test do you think more companies need to do this hi i recently found out that carvana lets you use the internet while taking their technical test they wrote something like this in the email invitation we all know everybody googles the syntax on their job i m sure there are many companies out there with similar mindset that i m not aware of i found it interesting and was wondering what are your thoughts on this should more companies start allowing the use of internet in their coding tests thanks
1,calculate appropriate but practical bin size hello people first post here please don t hurt me d here at work i have a set of 330k observations user consumption levels and i need to visualize them in a histogram first then construct a customer base model both hinge on the bin size if i m not mistaken sturge applies to normal distributions so it can t be applied here but what other choices do i have i tried a few more rules but apart from the freedman–diaconis one which gave me a realistic 53 bins to work with the rest give me a huge number of bins i simply can t deal with in their thousands in some cases any help would be greatly appreciated i can try and provide any further info that s needed x200b obligatory statement not a statistician
0,alteryx vs python for collaborating in a team so i have a silly newbie question i am trying to work out pros as cons for using these tools we have a team of 20 people and some like 3 people use alteryx and the rest use python i am trying to work out what the pros and cons would be for working a team does alteryx make it easier or harder to work together what are the other pros and cons for a large team trying to work together vs in their own silos
2,sub list of only high quality papers is there such a resource there are a lot of new papers these days but not all of them are actually good is there a resource where somebody else has already gone to the effort of sifting the wheat from the chaff so that we can see what a great paper should be like alternatively what are the papers that you think are the most well written do science in the best way best explanations actually reproducible based on the paper don t fluff results
1,question what are the best most up to date methods of causal analysis with time series data hello this might not be a very correct question given that much depends on the nature of the data but what are the best most up to date methods of causal analysis with time series data i ve only had an introduction to econometrics and therefore have only been exposed to granger causality but since the method seems quite old i am wondering if there are any newer and potentially better methods of inferring causality i would be grateful if anybody could just point me to the method itself or some papers using it i can figure out the rest regarding my rq i am particularly interested in analysing economic growth mostly gdp per capita and other similar variables as the dependents using varying lengths of time series apologies if anything is not clear enough and thank you in advance
2,project codenet ibm releases 14m sample coding dataset for ai for code ibm research released project codenet a dataset of 14 million code samples to train machine learning models for programming tasks key highlights largest coding dataset gathered yet 4 000 problems 14 million code samples 50 languages the dataset has been annotated problem description memory time limit language success errors etc possible uses translation from one programming language to another code recommendation completion code optimization analysis github
1,asynchronous time series i have two time series a and b b a c a is a random walk and c is an ornstein uhlenbeck process im trying to estimate c the challenge is that my measurements of a and b alternate so i can never observe a and b simultaneously ie i see a1 b2 a3 b4 etc i ve considered some kind of kalman approach to estimate c but that requires some assumptions on the relative variances of a and c in each time step i think are there some approaches you d recommend to dealing with this thanks
0,visual cleaning tool time series hi looking for a suggestion on a visual cleaning tool for time series or any data i am running my raw data through a time series decomposition to pick out some potential errors in the source but ideally it would be nice to have something interactive to work with to validate the errors and add more trying to avoid writing a bespoke thing in dash but i can ultimately do that if it comes to it not bound to a platform but python is preferred or even something electron based thanks
2,yoshua bengio team designs consciousness inspired planning agent for model based rl a research team from mcgill university université de montréal deepmind and mila presents an end to end model based deep reinforcement learning rl agent that dynamically attends to relevant parts of its environments to facilitate out of distribution ood and systematic generalization here is a quick read yoshua bengio team designs consciousness inspired planning agent for model based rl the paper a consciousness inspired planning agent for model based reinforcement learning is on arxiv
1,is error in linear model a statistic parameter or something else in linear model for regression both error and residual depend on sample so are both statistics a statistic is a function of sample correct error also depends on the mean of the true distribution is error a parameter a parameter is a function of the true distribution correct see bickel and docksum s mathematical statistics vol i is error not a parameter because it depends on sample does a parameter necessarily not depend on sample is error not a statistic because it depends on true distribution does a statistic necessarily not depend on true distribution if error is neither a parameter or statistic what does it belong to thanks
0,how important is aws i recently used amazon emr for the first time for my big data class and from there i’ve been browsing the whole aws ecosystem to see what it’s capable of honestly i can’t believe the amount of services they offer and how cheap it is to implement it seems like just learning the core services ec2 s3 lambda dynamodb is extremely powerful but of course there’s an opportunity cost to becoming proficient in all of these things just curious how many of you actually use aws either for your job or just for personal projects if you do use it do you use it from time to time or on a daily basis also what services do you use and what for
1,choosing what prediction interval to use is that purely based on the business problem hello while working on a project and finding prediction interval i was wondering if business problem allows instead of 95 prediction interval can we use 80 prediction interval if i m getting it right it will just mean that instead of 95 we are only 80 sure that the prediction will lie in this range thanks
1,question about data transformation for anova in r i have some data from a mixed between and within subjects experiment where participants were divided between 4 conditions and each group completed 2 blocks of a task i want to run a mixed two way anova to see if rt in the task changed as either a function of condition group or block number the rt data is all normally distributed except for one section which contains many outliers condition 1 block 2 this is making rt non normal so i can t do the anova i can t find any non parametric equivalents of the mixed anova and because the majority of the data contains no outliers i feel like there should be a parametric solution my question is whether i can use data transformation to help here i m using r and when i do sqrtrt sqrt mydata rt then sqrtrt passes the shapiro wilk normality test but how can i use sqrtrt in place of mydata rt in the anova
1,spatial statistics vs network models if an undergraduate student in applied statistics had to take a course in either spatial statistics or in network models which do you think is the most useful in industry research grad school etc obviously both are useful but is there one that stands out on your experience
1,does it make sense to compare the standard deviation of 2 samples of very different sizes say i have two samples of very different sizes sample 1 has 50 observations sample 2 has 500 i would like to compare the standard deviation of both samples does it make sense to do so without any use of undersampling oversampling methods
1,help with my a b test loser score i work at a tech company with a large user base that makes many decisions based on a b testing that affect user experience i put a question to r askstatistics about identifying losers in a b tests which sent me down a rabbit hole investigating conditional average treatment effects most papers i found suggested using additional subject data in one way or another even using random forests to generate a treatment effect function i see how this can be used to segment subjects especially in controlled experimental settings but in our case there are many potentially relevant dimensions that we can t or won t without adequate justification measure i realized that if you don t have the data to discriminate a sub population that is harmed by a treatment you can t in general determine that it happened at all based on the treatment results themselves for example one could imagine a treatment that perversely swaps subjects outcomes about the median so that the resulting distribution is identical to the control despite half of the subjects being harmed by the treatment and half helped however setting aside swapping i cooked up a method for summarizing a minimum number of losers in a treatment group for an outcome o in control count the number of subjects c in the control group scoring below o count the number of subjects t in the treatment group scoring below o calculate t c as losers compute the maximum losers across all outcomes call this minimum losers it s a minimum because it can t detect swaps compute minimum losers sample size assuming control group and test group have the same size this is the minimum loser ratio of the treatment i ran some simulations with this measure starting with a normal distribution and applying a positive ate normally distributed to a portion of the population and a negative ate to a portion of the population the minimum loser ratio is consistently zero when there is no negative ate portion and increases with the proportion of the population that is negatively affected and the magnitude of the harm it seems fairly sensitive too i tried out some different starting distributions as well and they do affect the way minimum loser ratio behaves in terms of the cate of the sub population i m curious about a couple of things is this like something else that exists that is probably much better is there a good way to quantify the confidence of the minimum loser ratio in my simulations i used populations of 100 000 samples and at least visually it seemed like a stable measure we have millions of unique users per month so we can potentially run fairly large experiments but we do like to start small currently these tests are not assessed by statisticians if the ate is positive and statistically significant as reported by our a b testing framework then it s generally a go for launch in particular outcome distributions are not usually examined i m hoping this measure might be used as sort of a yellow light signal that hey maybe you should look at the outcome distribution because it seems likely that x of users are actually harmed by the treatment which is relevant for us both for ethical and product reasons e g it may be that the treatment completely breaks the experience for a small subset of users on a particular device in my simulations it seems like the minimum loser ratio helps identify the presence of negatively affected sub populations even when examining the control treatment distribution plots doesn t make it obvious
2,what is the current sota for extracting a knowledge graph from images i am looking to get a knowledge graph of an image using some pre trained model what is the current sota work that has a reproducible implementation can you share some pointers from your experience trying to extract kg from images thank you
1,are sections for permutation tests in lehmann s tsh applications of ump unbiased tests for multiparameter exponential families in lehman s tsh is chapter 5 unbiasedness applications to normal distributions conﬁdence intervals an application of theorem 4 4 1 on p121 is theorem 4 4 1 based on a parametric model of an exponential family 4 10 on p119 are sections 5 8 permutation tests and 5 9 most powerful permutation tests for nonparametric models are sections 5 8 and section 5 9 applications of theorem 4 4 1 section 4 4 ump unbiased tests for multiparameter exponential families or chapter 4 unbiasedness theory and first applications if none of the above whose application are section 5 8 and 5 9 about permutation tests i am lost in the relation of sections 5 8 and 5 9 with respect to the earlier sections in chapter 5 and chapter 4 thanks
2,schmidhuber s blog post on kurt gödel s 1931 paper which laid the foundations of theoretical computer science identifying fundamental limitations of algorithmic theorem proving computing and artificial intelligence link to the article abstract in 2021 we are celebrating the 90th anniversary of kurt gödel s groundbreaking 1931 paper which laid the foundations of theoretical computer science and the theory of artificial intelligence ai gödel sent shock waves through the academic community when he identified the fundamental limits of theorem proving computing ai logics and mathematics itself this had enormous impact on science and philosophy of the 20th century ten years to go until the gödel centennial in 2031
0,how do you improve analytical thinking writing hi recently i m doing my eda and i found that my analyst is my weaknesses my friend said textbooks are the best and i can read more articles published on towards datascience etc secondly just wondering how you guys improve analysis i d like to model some great analysts or collect those best analytical writings would you like to share any best analytical articles or your favourite authors thanks
1,does if a minimal sufficient statistic exists then any complete statistic is also a minimal sufficient statistic mean to say any complete sufficient statistic instead p42 in lehmann and casella s tpe says since complete sufﬁcient statistics are particularly effective in reducing the data it is not surprising that a complete sufﬁcient statistic is always minimal proofs are given in lehmann and scheff´e 1950 bahadur 1957 and schervish 1995 see also problem 6 29 p289 in casella and berger s statistical inference says theorem 6 2 28 if a minimal sufficient statistic exists then any complete statistic is also a minimal sufficient statistic does the theorem in the second book mean to say any complete sufficient statistic is also a minimal sufficient statistic instead thanks
0,new sub for honest feedback on your data analysis r destroymyanalysis r destroymyanalysis was created to help data scientists and students find pitfalls in their analyses i ve found it extremely helpful to have someone else look over my work and provide honest feedback for my sources methods conclusions etc so i thought it might be a good idea to create a sub just for that purpose anyone can post a link to colab binder github blog etc to get ideas for improvement find pitfalls and anything else to help you improve and make your future analyses more robust
2,unity tool for generating a 2d images of 3d objects from several viewpoints hi guys for my research project i have created a unity tool that allows the creation of image datasets by taking snapshots of 3d objects from a variety of viewpoints you can change the latitude and logitude area the distance from the object the lightning and some more things so today i have decided to clean it up get rid of some unnecessary features and write some documentation check it out and let me know what you think valeriob88 unityml datasetgenerator github com
1,a bot that plays battleship using probability i recently built a bot in python that can play the battleship board game using probability the bot is able to guess where all the ships are in 54 moves on average this average was found by placing ships randomly and seeing how the bot does the bot builds the probability of where ships might be based on the hits and misses that have been on the board so far in this bot i didn t include the option for the bot to know which ship was sunk i feel like the code could be optimized more if you would like to take a look at the code and give me some comments here is the link to the github repo i also made a youtube video if you d like to check it out let me know what you think
2,books to learn eda and feature engineering i have been reading hands on machine learning with scikit learn and tf 2 by aurelien geron on and off for the past 1 5 years i honestly love the book but recently while i was taking part in a hackathon i felt the knowledge i had for eda and feature engineering was really limited i only knew whatever i had learnt from the 2nd chapter of the book and didn t really have more to go with i had felt similar when i was trying the titanic challenge on kaggle so please recommend some books which can help solidify my understanding of eda feature engineering and other things required to approach a real world ml problem i am looking for something that gives a more end to end knowledge
0,model for ranking people hi i am looking for pointers on how where to start the use case of ranking associates for example there is a class of 10 students who gave exams for last 3 years and we have the marks in those tests now can i use the historical marks for each subject and predict the future marks and rank the associates accordingly also can i give suggestions to students on where to improve to get a better rank marks
2,what can ai ml contribute to a crowdfunding site hi i m designing a crowdfunding site personal project that would help to provide financial services to the poor financially excluded people any advice on how can i leverage crowdfunding using ai or ml thanks in advance
0,weekly entering transitioning thread 18 apr 2021 25 apr 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
2,scaling automl driven anomaly detection with luminaire checkout this years data ai summit talk on scaling automl driven anomaly detection with luminaire github repo scientific paper blog
1,what does depend on in the likelihood principle corollary mean mathematically in casella s statistical inference 6 3 2 the formal likelihood principle on p294 says likelihood principle corollary if e x theta i x theta is an experiment then ev e x should depend on e and x only through l thetalx what does depend on in the likelihood principle corollary mean mathematically does depend on mean exactly the formal likelihood principle on p293 and p294 if yes why bother to have the corollary thanks
0,graph databases for data science neo4j just raised a huge 325m series f in this article covering the announcement there is a very striking quote according to gartner by 2025 graph technologies will be used in 80 of data and analytics innovations up from 10 in 2021 facilitating rapid decision making across the enterprise what are your experiences with graph databases do you also see this trend we re currently using it as a poc to combine poi with population data you can find it on our github
1,question about displaying significance levels in regression tables when presenting the results of regression models in a table do you prefer to display stars for just one significance level e g p 0 05 i know some people prefer to present all three i e p 0 1 p 0 05 p 0 01 but i ve also heard that it s best to just select one for the sake of presentation any thoughts on this
2,papers on disentanglement that study limit failures of disentanglement hi for some time ago i read a paper that discussed how some generative model maybe beta vae vae gan can t fully disentangle independent sources of variations in observed data x200b i vaguely remember that it showed how a kind of variations e g hair color in a face dataset was spread through multiple dimensions and how either adversarial loss or additional constraints on the latent space was not a constraint strong enough to achieve full disentanglement x200b does anyone know papers that study this kind of failed to disentangle problems i ve been trying to recall the paper but it hasn t come back yet nevertheless i d love to know more about works along this line thank you for sharing in advance
2,got ml role but dislike ml any advice hi all before hand i want to preface that i understand the field is very competitive and many people would be very glad to trade for my position so i m sorry if i look entitled i ve recently been very fortunate to get a really high paying swe role that is very relevant to ml have to read and implement ml algo and tune parameters the amount of pay and level is what drives me to take this position however i have been secretly developing a sense of dislike and despair toward learning ml related stuff even though my resume clearly show that i only do ml stuff undergrad courses internship and job exp i mainly think that its a waste of my time in long run and feel like the materials are too detached from systems level thinking i have realized that i couldn t care less about the next best paper in ml or any new algorithm to tune model what i trully care about in long run is mostly software systems distributed system and devops how long do you think i will last in ml field if i have no passion for it or any advice from other who went through the same phase
2,any good sci fi books up to date with ml ai slightly off topic for the sub but i think it s still relevant many of us have read a lot of sci fi in our youth inspiring us to think beyond what was possible back then things like the internet brain uploads self replicating machines sentient ai and similar ideas were all seeded in the minds of geeks and scientists decades before they were technically achievable have there been any great sci fi books around ai published in the last 10 20 years i m looking to expand the realm of imaginability for my children just like my imagination was expanded by ingesting hundreds of books written in 60s 90s all recommendations welcome
2,cossover with genetic algorithm i am curious as to how other have efficiently implemented crossover of neural network in regards to genetic algorithm in the past i have typically have very small networks of 100 weights and would simply use my cpu to loop through and swap weights 50 of the time thisnworked well for my needs today my networks have 10 million weights each and manually lopping through and swapping value will obviously not work due to time constrains hence this post i have a few ideas of how i could get these to work specifically by using xor over the entire matrix any ideas as to if this would work are there better alternatives you can think of thanks
1,does anyone here study game theory i have always been curious about game theory but have been to intimidated to try to learn it this week i have decided to try and shake my fear and begin to learn about some basic principles in game theory x200b i am looking at this basic problem in game theory that asks how should the owner of a company split the profits between her and her workers x200b x200b i am having some difficulty in understanding the notations x200b 1 formally a coalitional game is defined as there is a set n of n players and a function and a function v that maps subsets of players to the real numbers v 2 n r where v empty set 0 x200b a why does v 2 n is the point of this expression to show all the possible interactions that can exist between any groups of players x200b b when considering the value function i e v why does v empty set 0 i guess this is an obvious statement but is it supposed to mean that the potential value of the interaction between no players i e the empty set is 0 x200b 2 now the problem where the owner of a company split the profits between her and her workers x200b the owner is represented by the symbol o there are m number of workers w1 w2 w3 wm each worker contributes an amount p to the profit x200b i am having difficulty understanding how the value function for this problem is determined x200b the value function for the coalition of players the coalition is denoted by s is x200b v s mp if the owner is included in the coalition v s 0 if the owner is not included in the coalition x200b my question how are v s mp and v s 0 initially determined or are these values assumed x200b 3 in this question m is defined both as the number of workers as well as the cardinality of s o x200b i have generally heard cardinality referred to as the size of a set what exactly does cardinality mean in this question are you supposed to be able to compute s o why does s o m x200b 4 can someone please walk me through the calculations why according to the shapley values should the profit be distributed such that the owner receive mp 2 and each worker receive p 2 x200b 5 lastly why is this question important is the point of this question to show the fairest distribution of profits according to contributions x200b thanks x200b x200b source
2,any way to train models on phone using pytorch i m currently doing a research in federated learning which requires training a lightweight model on a mobile device i read about pytorch mobile but it apparently cannot be used to perform backprop on the phone itself correct me if i m wrong is there any workaround for this task
2,survival analysis prediction of time to event in r i m doing survival analysis in order to predict the time to failure of machines but i have some doubts regarding the predictions with the cox proportional hazards model i can assess how the covariates influence the risk of failure but how can i actually predict the time to failure of test data i have a dataset of 100 machines and i want to use 80 for training and 20 for testing the covariates are age and values from sensors like pressure and vibration after applying the coxph function to the training dataset how can i predict the time to failure of the test dataset based on the values of the covariates sorry if it s a dumb questiona but i can t find how to do it anywhere thank you in advance
2,linkedin open sources ‘greykite’ a time series forecasting library linkedin recently opened sourced greykite a python library originally built for linkedin’s forecasting needs greykite’s main algorithm is silverkite which delivers automated forecasting which linkedin uses for resource planning performance management optimization and ecosystem insight while using predictive models to estimate consumer behavior data drift has proven to be a great challenge during the pandemic in 2020 in such a situation predicting future expectations is challenging as well as necessarily helpful to any business automation which allows for repeatability can increase accuracy and can be used by algorithms to make decisions further down the line according to linkedin silverkite has improved revenue forecasts for ‘1 day ahead’ and ‘7 day ahead’ and weekly active user forecasts for 2 week ahead full summary github pypi paper
0,selling data hey as a side effect of one of my projects i now have a dataset full of random images that have been described and labeled by a person this made me curious about the market for selling data that has had human intervention in this case maybe for a test set in a machine learning algorithm so that the model s accuracy can be calculated i am not necessarily interested in what i can do with my current dataset but just curious about how this type of data is generated and how it is distributed if anyone knows any information about this field i would be very grateful thank you
0,is it normal to manually adjust forecasts i work at a data science consulting firm and there i was faced with the established procedure that if the team presents some forecast to a client and the client thinks that the forecast is too high or too low instead of changing hyperparameters or even changing models the team basically changes the forecast values manually using market insights they basically look for other specialists forecasts in the internet and subjectively choose a value close to those this makes me incredibly uncomfortable is this a normal procedure to be used at data science consultancies i never worked at another consultancy so i don t know if this is normal or not
1,assesing probabilty of an individual sample having been clustered incorrectly for further testing i have a truth set for some samples that have been clustered into discrete ordinal classes based on a single continous variable where the larger the indipendant variable the higher the class number will be so 0 4 might go into class 1 0 9 into class 2 1 5 into class 3 2 6 into class 4 for example x200b i know that there are samples that are being misclassified and that even a perfect model would misclassify some samples as for example a sample which should be classed as 3 has a lower indipendant variable associated with it than even the highest indipendant variable associated with with class 2 samples all samples that i am less than 99 or other cut off of having been clustered correctly i will send for further varification using other methods i want to identify which samples that applies to i want to minimize the number of samples that i have to test with other methods i am given by software i am using the classification and the indipendant variable there are batch effects so i am currently unsure if pooling data for multiple batches will work so i am not sure if i can use my truth set to mix in with the new batches for anaylsis or if i should perhaps include some samples that i have ground truth on into each batch x200b i am lacking the correct terminology to find literature books papers discussing assesing the confidence of the classification of an individual sample i know bayes classifiers do give some information but i don t think is really helpful with only one variable also the prior distribution in general is heavily weighted towards class 2 but individual matches may vary from that what i would ideally want is some way of telling if a datapoint is inside the margin in an svm and having that margin defined in a way that 99 of calls outside the margin are true positive
1,interaction between year and treatment in lmm my trial consists of one single factor called system in a completely randomized block design with four replicates the dependend variable yield was mesuared every year for eleven years the model i came up with in lme4 was lmer yield system 1 year block data data many publications in my field suggest a random interaction between the treatment factors an the repeated factor like adding 1 year system to my model but i don t understand why or when this is necessary
1,weighted least squares parameter estimation how to define weighting matrix hi i m a noob at this so i ll try my best to explain my question i m looking to use the weighted least squares formula for a parameter estimation problem basically the the formula consists of sum of the residuals squared multiplied by a diagonal weighting matrix consisting of the inverse of the variance from measurement data this is to weight certain values of measurement data and their associated residuals that are more accurate than others lower variance more accurate more weight please correct me if i m wrong my question is why do you have to use the inverse of the variance does the formula only work for variance or can the weighting matrix be defined in a different way for example why not the inverse of the standard deviation or the inverse of the range because basically we are saying that the greater the variance of a probability distribution the more inaccurate it is so really any term that is a measure of the spread should suffice to define the weighting matrix any thoughts would be much appreciated
1,question modelling the optimal interval for two dose vaccine administration i m curious about the statistical approach is to this problem there are a number of wrinkles to the question efficacy of one person with two doses vs one fully vaccinated vs two partly vaccinated the level of virus in the community at the time which may itself feed back into the immunization status that is you have less efficacy in an individual with one dose rather than two but getting a first dose in more people sooner may offer more population health benefit additionally you have what appears to be an enhanced immune response with a longer delay but have to weigh that against while one might get better immunity by waiting longer if you become ill before getting that second dose you ve lost that bet leaving aside the factual specifics which are subject to investigation what is the statistical apparatus for thinking about these kinds of contingent scenarios all premised here on a two dose regime in which the first does confers some efficacy but is not optimal till some time after the second dose
0,100 turnover on data team i joined a new organization in january and i am now the only member of the data team remaining the bad my boss the data engineer and two analysts the whole team besides me have all already resigned or are going to be leaving by the end of the month some senior leaders in other departments have also resigned in the last few weeks retention has apparently been an issue here for many years although i have had a pretty pleasant experience working with my colleagues and stakeholders so far i have only met lots of bright motivated people so maybe they are all just getting poached by other companies my pay is below market rate the good we have an embedded consulting group that handles a lot of our project management data engineering and data analysis and they are staying if not expanding in the short term we also have a new team lead and department director starting soon i interviewed the team lead and have seen the resume of the director and am confident i can learn a lot from both of them the work we do here is incredibly interesting and meaningful and i am motivated to build great data products obviously this situation is a cause for concern has anyone weathered a similar situation that turned out for the best or is this likely going to be just miserable plodding along for the next year or two if i stay
1,how does one compute the log probability of arbitrary models i m wondering whether there is a method of computing the log probability of observing a value from an arbitrary model apologies in advance i don t have a good background in statistics but rather a background in probabilistic programming and programming languages the wording i m using and questions i m asking may therefore be silly to elaborate it s relatively easy to compute the log probability of observing a given value from a primitive distribution in probabilistic programming languages there exist functions for doing so for example normal 0 0 1 0 pdf x in more complex hierarchical models we may sample from many distributions and the output data of our model will not result from a primitive distribution for example where we sample from two normal distributions and add these samples together def model x normal 0 0 1 0 y normal 3 0 1 0 return x y in the situation where we want to condition some data against the result of this model is it possible to compute the log probability of this somehow
2,unsupervised progressive learning and the stam architecture ijcai 2021 check out our recent work which was accepted by the 2021 international joint conference on artificial intelligence ijcai 2021 we pose the unsupervised progressive learning upl problem an online representation learning problem in which the learner observes a non stationary and unlabeled data stream learning a growing number of features that persist over time even though the data is not stored or replayed to solve the upl problem we propose the self taught associative memory stam architecture which leverages online clustering novelty detection outlier forgetting and prototypical feature storage rather than a replay buffer of raw examples
0,how to finalize job offer if a choice is given to work as a data analyst at a rapidly growing startup which would have responsibilities of an analytics engineering as well as future progression into becoming second data scientist in the company or to work as a senior data scientist at an analytics and measurement company what would you think would be a good choice if compensation is a factor in deciding the data analyst offer could fetch somewhere around 80k no data as the company was recently founded and the senior data scientist would be around 120k as per published data
1,getting a ms in stat after failing to find job as an actuary i am currently interested in majoring in mathematics and taking the actuarial exams while still in school i have heard that the job market for entry level actuaries are really competitive right now and will become more so i am worried that i will not be able to find a good actuarial job after graduating with a bs in math so i m wondering if it would be possible for me to then go back to school to get a ms in stat and became a statistician afterwards if my job hunt to become an actuary fails i would appreciate it if anyone could also point out any other flaws that they see with my plan for further clarification i want to major in math and minor in cs
0,data streaming question hey i m looking for a solution open source docker deployable service what i need to do is stream data from several mysql db s to a central postgres db polling once every 30 min so performance and query size should matter some of the tables in the mysql db do not have a date on them now initially i planned to join the 6 tables in one of the db s to make a view of 130 columns in order to get the date data in then select from table where date last 1 day this should give me an option to limit the query by filtering it one can quickly see how the massive view will lag the origin server especially if i poll it every 30 mins there are plenty of null values in there but that s beside the point its the best solution i can think of at the moment correct me if i m wrong about the filtering performance and if its actually sensible alternatively i can stream individual tables but as mentioned above there isn t any date column to filter so it d cause even more of a performance bottleneck to select all rows for each table at every update i initially used airflow to achieve this by writing the data to a csv as a staging area for loading on to pg i ve also looked at airbyte but they seem to be pretty early stage though correct me if i m wrong could be difficult to use a mysql slave db as the two db s are different types i also don t think there s probably sufficient infrastructure currently to handle several additional slave db s what would you do with the scenario above
0,causal data science my background is economics and currently i’m a data scientist intern i really like causal relationships but haven’t seen anything too advanced only stuff like granger and impact evaluations i want to know which are the hot topics in causal inference any tips edit so many comments i’m very grateful and i’m reading them all
2,network science applied to images structure community analysis i am working with a large image dataset embedded via a cnn model with the goal of clustering the images in an unsupervised manner the majority of works on clustering in the embedding space tend to use classical clustering methods e g k means agglomerative but those are often limited in the types of structures they can detect in the data i have also looked into deep clustering but it seems like there the methods assume non overlapping or non hierarchical structure which is quite a limitation i was therefore wondering if someone tried converting the image embeddings into a graph and applying some network analysis methods on top of it e g community detection ones surprisingly after a week or so of research i couldn t find papers that did so especially on a large scale has someone encountered a similar idea or can help me understand why are there no or just a few works on it would appreciate any links or hints
1,likelihood of spurious correlation q what is the likelihood of spurious correlation that exists between two random variables this guy takes sets of 18 samples and shows correlations of up to 0 5 even though the correlation should be zero i have an engineering background i sometimes need to troubleshoot which of several inputs might be causing some output
1,can someone please explain the representer theorem in simpler term can someone please try to explain the representer theorem in simpler terms why is it considered important in the realm of machine learning thanks
1,how to learn about weibull distribution sampling i m a biologist and i have some code i run to sample from a dataset of genes where most genes have skewed gene lengths most being very small but i can sample an equal number of genes across their gene lengths with this code but i am stuck on understanding statistically how it is doing the equal sampling i do this in r with input data view gene length gene1 5 gene2 6 gene3 400000 gene4 1000 gene5 25000 gene6 10 gene7 50 gene8 4 gene9 100 gene10 2000 classes df order df length classes density dweibull 1 nrow df shape 0 1 scale 1 classes gene length density 1 gene8 4 3 678794e 01 2 gene1 5 1 353353e 01 3 gene2 6 4 978707e 02 4 gene6 10 1 831564e 02 5 gene7 50 6 737947e 03 6 gene9 100 2 478752e 03 7 gene4 1000 9 118820e 04 8 gene10 2000 3 354626e 04 9 gene5 25000 1 234098e 04 10 gene3 400000 4 539993e 05 dfrep classes rep 1 nrow classes classes density 100000 classes table dfrep gene density calc classes sum classes dfrep density calc density calc match dfrep gene names density calc density prob 1 dfrep density calc gene sample data frame sample dfrep gene size 100 prob 1 dfrep density calc i haven t been able to find any statistics resources to learn specific about sampling from a weibull density especially those that are not maths intensive if that is possible for this in particular i don t understand the use of the word classes in this code and how this code over represents the short genes when creating dfrep and that somehow leads to equal sampling of both the short and long genes i would ve assumed increasing the number of short gene representation would increase the probability of those genes being sampled in sample but i can see it doesn t do that somehow are there any resources i could find for learning about sampling methods like this
0,how to become a data scientist there’s a million ways if you read posts comments from this sub people are quite assertive that their path is the main path they say get a masters phd in “ “ and that’s the only way in they say don’t get a masters in analytics data science but encourage the analytics georgia tech worry about your own goals and domains that you want to get into and focus your path for that there’s data scientist from a million different backgrounds and educational levels
2,dataset big enough to train rl agents to accurately identify gunmen with malicious intent how do i find dataset big enough to train rl agents to accurately identify gunmen with malicious intent and not law abiding citizens what are my sources other than free internet with information on cartels isis and boko haram
1,theoretical performance of machine learning algorithms on imbalanced datasets suppose you are working on a supervised binary classification task you have patient medical information e g age weight gender height blood pressure etc and whether they have a certain disease or not this is the response variable yes or no let s imagine that determining if patients have this disease is time consuming and costly so a machine learning approach is being considered let s assume that this disease is very rare in your data set only 1 of patients have this disease thus the dataset is imbalanced intuitively we know that any machine learning algorithm trained on this data will likely perform poorly that is the performance will likely be deceptive you might get an accuracy of 99 but misclassify all of the patients who have the disease mathematically speaking is there any mathematical explanation for this very logical concept e g if only study 1 hour for a chemistry exam i might only learn how to solve 2 3 types of problems thus on a true false style chemistry exam there will be many questions that i don t know how to answer because i never saw them before and i will be likely to perform badly on material that i have not prepared for do machine learning models work the same way for popular algorithms like neural networks xgboost and random forest can it be shown that for classification problems you need a minimum number of observations or a minimum proportion of the minority class to probabilistically achieve a certain model performance on a more abstract side i have heard that researchers are interested in trying to make machine learning models generalize without seeing thousands and thousands of examples e g a 5 year old child can learn what is an elephant after seeing a few pictures of an elephant e g it s perfectly reasonable to expect that a young child would see a picture of the cartoon character dumbo and identify dumbo as an elephant after coming back from a zoo but a machine learning algorithm would likely need thousands and thousands of pictures of elephants and likely require to see the same pictures upside down inverted with added noise different color scheme etc prior to be able to generalize and learn the concept of an elephant perhaps the same analogy applies to machine learning models struggling to correctly classify patients with a rare disease since there are so few of them does the above concept have anything to do with the bias variance tradeoff or is it just logic if there is not enough variability and information within the data the machine learning model just learns the noise within the dataset i am really curious to see if such a threshold for measuring minimum level of variability within the data has ever been studied ps in a 1 dimensional sense on a number line if you have a point at 3 and another point at 5 you could consider all inferences outside of 3 and 5 as extrapolation and all inferences between 3 and 5 as interpolation when dealing with higher dimensional data could you simply consider observations from the test set that have a smaller euclidean distance to other observations from the training set as interpolation and observstions that are farther away as extrapolation in reality can you just consider all prediction as extrapolation small scale extrapolation for closer points large scale extrapolation for further points thanks
1,if there is a 1 56 chance for an event to be occurred how likely it is for it to happen after the 56th attempt context in the post before i start i d like to excuse any misuse of terms or mathematical symbols since i am not studying anything related to statistics or mathematics what i am doing here is for my own curiosity i ve been playing a game recently and i want a specific item however it has an extremely low chance of it dropping 0 0005 and if you make the calculations you would need to kill around 200 000 monsters to get it which takes around 56 hours i imagine that it is safe to assume that the chance of the item dropping in each hour is 1 56 i am curious to see what the chances are of me getting the item only after the 56th hour thank you
0,data scraping risks i am working on a fun personal project where i am scraping some info on places and reviews i want to analyze a large dataset based on the scraped data and put the analysis on github also i would like to talk about this project during my job interviews can i get into trouble doing so i know google specifically does not like its data being scraped stored
2,nice paper on compositionality based on attention based architectures solving math problems nice paper on compositionality in human cognition based on nn attention based architecture solving math problems pdf « compositional processing emerges in neural networks solving math problem by a m uc davis and johns hopkins research group they look for corresponding sub expression from math questions in different parts of attention based network architectures poorly named transformers they found some ability to compose the meanings of math symbols according to their structural relationships
0,tools for analyzing images hello data scientists i am but a mere engineer trying to pretend that i know what i’m doing i have a small set of labelled greyscale images 2 classes and a lot of unlabelled images i want to understand what qualitatively differentiates the sets of labelled images my engineer brain understands how to train something to do a prediction but absolutely fails to understand how to analyze the image data with the tools at my disposal i would love to be able to tell someone “ this is the difference between these images” what’s the data science y way to look at images
1,question what does it mean when the significant difference is at 0 01 had an anova result and most of the variables had a sig diff of 0 01 what does it mean is it better worse than 0 05
1,where have phd statisticians ended up in tenure track academia they say a statistician can play in everyone s back yard i m curious statistically speaking where do stats grads end up aside from the pure and bio stats departments that is if you or you know of someone who got a stats phd and is now at a tenure track position abroad or in the us which other departments have you or they gone to aside from statistics biostatistics
2,why positional encodding is learned in vision transformer hello everybody i read the vision transformers paper and then looked at the implementation in pytorch x200b it is usual in transformers to use positional encoding using a purely deterministic sine and cosine embedding x200b however in the vit implementation the positional encoding is learned by the model itself x200b someone can explain me why a standard positional encoding is not used and what is the meaning of learning this positional encoding thanks you
1,what is being assumed by the sampling process if one uses randomization inference permutation tests vs calculating standard errors normally say i am regressing a dependent variable y on a treatment indicator to get an average treatment effect i could calculate standard errors and p values by taking the variance of the difference in means taking the square root and then using t z tests to calculate p values i could also do randomization inference where i randomly assign units in my sample treatment status run the regression and see the proportion of times these are at least as large as my actual effect why use one over the other what assumptions are being made when one uses either does the first assume i could resample a new pair of units and assign treatment again and the latter assumes my sample is the population of interest
0,in which cases random forest algorithm requires cross validation i m modeling with random forest algorithm medium size data set but i didn t get any difference using cv roc auc score i don t know if i m just spending time doing cv then could someone explain which cases it s good to do it for rfc
2,any reason not to make t depend on h in a highway network the typical formulation for a highway network is y h x t x 1 t x x however what seems strange to me about this is that the transform gate has no knowledge of what it is gating seems like it might make more sense to make the transform gate a function of h x y h x t h x 1 t h x x i tried this yesterday on a large 1d conv network i had processing raw audio and it seemed to work any reason this is not a good idea i m just going off intuition but not sure if that is theoretically correct
2,paper explained endless loops detecting and animating periodic patterns in still images the animation is generated from a single still image have you ever taken a still photo and later realized how cool it would have been to take a video instead the authors of the endless loops paper got you covered they propose a novel method that creates seamless animated loops from single images the algorithm is able to detect periodic structures in the input images that it uses to predict a motion field for the region and finally smoothly warps the image to produce a continuous animation loop read the full explanation in the casual gan papers blog to find out about detecting repetitions in images predicting the motion field and generating seamless animation loops from flow vectors full explanation post arxiv project page more recent popular computer vision paper explanations comodgan
2,should i implement every famous dl paper i found a really great list of introductory and popular dl papers github com floodsung deep learning papers reading roadmap and i would absolutely implement every paper on this list if i had the time at least a mini version e g cifar10 instead of imagenet is is essential for me to implement every single paper on that list to become a good dl researcher and to start reading implementing more recent ones all the papers on the list are from before 2017 and i can t wait to start exploring the latest research would i be able to get away with just implementing a handful of papers from that list thank you
0,how to explain to management that data cleaning is a really important part of my job hi all i recently started my first job working as an entry level data scientist i’ve been working at this company for roughly 3 5 months now and was put on a project where i am to extract phrases and classification codes from pdf documents in different languages there is more to it than that i’m just keeping it brief without disclosing too much i had relatively finished most of the algorithm that is able to extract and compile these phrases codes however the dataset that i am using has all been entered manually by multiple different people who work at the company 100 people this requires a lot of data cleaning to process duplicate phrases that are mapped to different codes categories of codes etc additionally it appears that many people have formatted their inputs drastically differently i am currently only doing this for the english language and then will have to do it for french spanish and german in the coming weeks each dataset is initially 250 000 records where i can automate roughly 90 of the cleaning the rest are all either really obscure cases or the classification of the duplicate phrases are too close to call causing me to have to closely examine and google them online to determine which one shouldn’t be there i know all of this is all super vague i am trying my best to explain what i can share some things i can’t back to my question i have weekly meetings with management where some of them seem surprised when i tell them that i am still working on data cleaning been working on it for 2 weeks now and will likely need more time than this as i haven’t even finished the english dataset i would estimate that up to this point 70 75 of the code i’ve written is for the sole purpose of data cleaning preprocessing and determining what belongs where using fuzzy logic and embeddings my question is how do i explain to them that the data cleaning process is most of the work a data scientist needs to do am i looking into this too much had i been given a perfectly clean dataset i would be able to complete this in no time also this is my first job out of college bachelors degree in data science and i definitely acknowledge the skill gap between me and the other members on my team who are sr data scientists they are much more efficient than i am when it comes to things such as deep learning the cloud etc any advice is greatly appreciated tl dr my first job out of college been working at the company for 3 5 months as a data scientist management seems to be surprised that data cleaning is taking me so long 2 weeks and counting to complete which makes me feel like i am not working efficiently enough does management have it backwards where they think building the ml models is more intense than the data cleaning portion edit thank you all for the input and advice i have a meeting with management later this week and i will definitely be using the suggestions and advice provided here edit 2 wow i really can thank everyone enough for all the advice and feedback i received you all have gave me some great guidance as to how i can navigate this issue thank you edit 3 grammar formatting
0,how do you handle marketing speak by execs and misrepresentation of yours your team s work so i m the lone data scientist in my company startup in the healthcare tech space like i imagine many here are last week i saw a linkedin post from the company s ceo flaunting how data we provided was used in an article published by a major news outlet i m omitting details here to preserve anonymity he said it was all thanks to our new product insert flashy name here and our cutting edge work with big data all caps ai and business intelligence the thing is neither this particular data nor this product are thing which i ve even heard of now like i said i m the only data person in the company and we re not a large company at all so there s no way there is someone working on stuff like that without me knowing we re nowhere near close big data and there s very little ai stuff being done like in most startups the majority of my time is spent juggling data engineering data analysis demands and if i had to bet i d say this particular case was probably something done in some random excel spreadsheet by the product marketing people at the last minute i know part of the ceo s and other execs jobs is making the company look good but sometimes the way they misrepresent the technical work the company is actually doing by using random data related buzzwords is really hard for me to overcome is this me being immature should i just learn to embrace this kind of thing as an inevitable part of corporate work
2,differences between multi view and multi modal learning greetings to the community i m currently reading and studying about multi modal learning by reading papers on a specific multimodal problem i came across a multi view model as i have so far understood and if i m not mistaken the scope of multi view learning is to train a model on all views but test it by using a subset of these views that s not the case for multi modal learning where the model needs all modalities during inference time so is that it what s the actual difference between views and modalities are multi view and multi modal models comparable or do they solve a different problem
0,is there a comprehensive overview of ml ranking recommendation system techniques i am looking to study recommendation system techniques such as collaborative filtering and how to build evaluate machine learning models for ranking instead of classification problems google search results have been messy so far does anyone have a good overview for these two topics
1,statistical tests in spss hi all i’m currently in my first year doing an undergrad in ecology and as part a self directed project i have chosen to do on bird feeders i am using spss i am trying to find out if the feeder distance from cover e g a hedge and other bird presence on feeders close by affects the frequency of feeding i’ve got the data and put it into spss but i’m a bit stuck on the statistical test side of things i have one column with feeder position 1 to 5 with about 100 rows per position each row represents when a bird lands on the specified feeder and then the column next to it records how many birds were present on the other feeders at time of landing i am wanting to run a test that compares feeder position to total number of landings for that position and another that compares median other bird presence to total number of landings for each position stats isn’t my strong point but i would really like to include this in my work so any pointers would be appreciated
0,coming up on 1 year of being a data scientist what’s next a year ago i was graduating with a masters and signing the papers for my first job in data science all has gone well so far at first i thought i wasn’t nearly tech savvy or math minded to belong but i’ve improved in those areas and i’m learning i bring other things to the table my question for the senior scientists out there is what should i be working towards at this point in my first year i’ve learned basic linux basic version control object oriented programming debugging python anti patterns to avoid how to write clean reusable code plenty of nlp techniques enough aws to pass the practitioner and ml specialty although i haven’t used them what mlops is and kinda sorta where to start with it so my question is what should i be working on ml ops sounds like it will be in big demand before too long cloud stuff seems like something i should capitalize on while it’s rare math isn’t going away but i don’t know how much more value spending a lot of time on this will bring to my situation software engineering concepts practices are great but i feel like i’m running out of ones that directly apply to me should i drill down on one of these skills or try to get a general understanding of each something else
1,how can i predict the probability of winning based on data of wins and losses okay so i m playing a video game and i m trying to figure out which character i should play based on data about wins and losses on that character so i want to estimate the probability that i will win given i picked a particular character it seems intuitive to consider win rate but clearly this isn t enough if i play 2 games on brand and win both of them that doesn t mean there s a 100 chance that i will win the next game likewise it seems like i should be more confident i will win if i have 15 wins and 5 losses on soraka than if i have 6 wins and 2 losses on thresh even though those are both 75 win rate i know that one of the possible methods is to play as many games as possible on each character and then just observe their wrs but this would be very time consuming and the point of doing this in the first place is so that i spend as few games as possible and characters i m bad at and as many as possible on characters i am good at so yeah what s the best way to get the probability i will win given data of wins losses also is there a better way to figure out which character i m best at without needing to do calculations like that
2,what should be the data driven chatbot architecture using nlp2sql given a table i am able to convert natural language questions into appropriate sql query with transformers the architecture of a chatbot should be 1 natural language question to sql query translation using transformers this part is completed 2 fed sql query to sql engine and collect response this part is completed 3 convert sql engine response to natural language response how can i accomplish the last part what kind of architecture or model should i use
1,those who did graduate level stats research in time series forecasting phd ms how much forecasting is used in your job hello was wondering for those of you who had studied graduate level time series in an ms or phd program how much you are able to use these methods in your day to day work in industry is your role in a company as a “data scientist” or whatever job title it is to just be a forecaster if you do research in time series forecasting does your job in industry make you just assigned to forecasting tasks or are you expected to do other things to how much of your day to day work is forecasting tasks in industry and do you think some jobs in industry have more forecasting needs than others
1,how important are the statistical assumptions in this question in this question they assume that people arrive according to a poisson process 1 if you have real data e g a list of times at which each customer arrives how would you check to verify that your real data follows a poisson process 2 suppose your data does not follow a poisson process does the formula in this question still apply is there a non parametric form of this equation that does not depend on the poisson process
0,need some advice on tracking specific words on a website over time hi guys hope you can help me out with this one i m looking to do some research i ve never done before to track visualize how many times a word has been mentioned on a website and on what dates they were mentioned so for example lets say i want to get data on how many times the word covid 19 has been mentioned on a specific news website on each date over the past 2 years i don t have a clue on how to attach a date to each mention how could i do this thanks in advance
1,do the size and critical value of a test depend on the sample size the power of a test depends on the sample size similarly does the size of a test depend on the sample size the critical value of a test for a significance level is computed from its size does the critical value of a test for a significance level depend on the sample size my guess is yes to both questions but in statistics books i found that the size and critical value of a test is calculated once for all without referring to sample size and then the power of the test is calculated with sample size involved in particular for estimating the required sample size for achieving some power is sample size not important in calculating the size and critical value of a test thanks
1,has anyone heard of extreme value theory let s start with an example problem case say we measure two variables that are non normally distributed and correlated for example we look at various rivers and for every river we look at the maximum level of that river over a certain time period in addition we also count how many months each river caused flooding for the probability distribution of the maximum level of the river we can look to extreme value theory which tells us that maximums are gumbel distributed how many times flooding occured will be modeled according to a beta distribution which just tells us the probability of flooding to occur as a function of how many times flooding vs non flooding occured source does anyone know why the maximums are gumble distributed shouldn t their distribution depend on the data itself does anyone know why the numher of floods are modelled by a beta distribution again shouldn t their distribution depend on the data itself e g maybe normally distributed thanks
0,effective ways of choosing number of neurons layers in a neural network i have been reading more about the theoretical backgrounds of neural networks e g universal approximation theorem and have seen several authors demonstrate that even a simple few layers many neurons neural network can theoretically approximate the variable of interest i e the response variable to a decent level of precision however the implication being that to use simple neural networks in order to achieve good results this would require a very large number of neurons therefore deeper neural networks have been developed over the years which attempt to provide good results with more layers but a fewer number of neurons this brings me to my situation i have never been able to successfully fit a neural network to any real world data that i have used i have always gotten really bad results with neural networks after trying all sorts of combinations of number of neurons number of layers learning rate activation function drop out regularization etc this seems to be a hyperparameter grid search problem ironically models like cart decision trees have good results on the same data supervised binary classification and random forest has produced even better this data is not small by any means contains around 30 columns and over 300 000 rows of data does anyone know if routines have been written e g in tensorflow keras that can assist in this problem of deciding the number of layers and the number of neurons is there a ground rule for deciding how many layers and how many neurons to begin with is there something around that can intelligently point you in the right direction for how many neurons layers to choose
0,how is data science being applied in our lives eli5 please just a brief description of my background i m an education counselor in an asian country where data science program is starting to trend my job includes to explain to students what kind of degree would achieve their desired job or help them to choose the right career path very often i am able to explain what data scientist do thanks to this subreddit and eli5 but i struggle to provide examples to high school student how is data science applied and shape our daily lives i ve tried searching this subreddit but i am getting very technical answers which i struggle to understand so to put it simple how do you apply what you do in your job into our lives
0,moving to a company where you are the only ds growth opportunity or suicidal move in my current ds job i am free to experiment with models and my boss lets me do some r d however the production side is deficient they use old tools and very outdated pipelines and technologies most of the time we do ad hoc analyses for clients that require no production at all i feel the need to grow my skills on production and that s something i can t do where i am now that s why i m looking for another job even though i like my current company boss and colleagues i was contacted for a new opportunity by a cool non profit organization they want to start a ds project that sounds very interesting to me and useful for society imho but there s one caveat i d be the first ds ever there i would basically need to build everything from scratch all by myself one one side i could build a good career i d be plenty of room to do things my way on the other side i d be all alone without anyone to learn with and from in career terms would that be a suicidal move is there a risk to cut myself out the ds job market have you every been the first ds in a company did you ever move from multi ds to single ds companies and what have your learned from that i hope this might help also other readers too any advice is very welcome
2,does anyone want to share their independent or hobbyist ml research setup i recently moved from a scientist role to an engineering role but i still have an itch for running academic experiments i have a windows 10 desktop with a ryzen 3900x and a 1080 ti but mostly use it for gaming i m thinking the best research setup might be a dev box w cloud computing and a something like a local linux terminal instead of dualbooting i want to minimize dev time and optimize for experiments without demolishing my wallet any tips
0,dae get frustrated after finally finishing something cool and everyone takes for granted how hard it was i built a sweet tool and showcased it and everyone loved it then i got the “that looks easier than you made it seem” comments like yeah because i had to break everything else to figure all of this out maybe it’s just me
1,help with poisson distribution i am really stuck with a statistic approach i have data of outgoing calls from a month with the hour of the day and day it was made what i first did is groupby hour of the day and count the number of calls for the entire month and i noticed it has the shape of a possion distribution left skewed what i want to do is to get the pdf so i can now the probability of finding anyone doing a call at a given hour of the day yet i am really confused because of 1 time unit i am graphing the entire month and i would just like to calculate the probability of one day of the month so should i take the mean of calls at each day and hour and then fit the distribution 2 besides count an event like how many cars with pass in 1 hour i would like to count the hour passed in one day for example x 1 means an hour has passed from 12pm and this is the probability of finding someone doing a call is that possible i would really appreciate your help or any reference i feel like i am overthinking it
0,can you please come up with sota methods and approaches for time series forecasting recently quite surprisingly for me but the main focus of my work shifted to time series forecasting i have read a lot in the great book by hyndman but i still have a feeling there are a lot of approaches and methods i haven t tried currently my baseline approach is facebook prophet to which i add a few custom regressors and try to find the optimal values for the hyperparameters mentioned in the official documentation there are some special traits in the data i work with due to how billing is implemented there are some significant spikes on the 28th of each month and then there might be a sudden drop in the next few days 29 31 adding regressors helps but not that much because those values 28 31 might be unexpectedly high or low i have also tried greykite by linkedin this library is quite new and is supposedly better than prophet at least according to their own blog post i find it much more challenging and confusing to work with though and their documentation has yet to be improved in order to be coherent and readable as far as i know the fancy deep learning models are cool on paper but because of their non existent interpretability high probability of overfitting and relatively mediocre results in practice are not used very often what should i read try pay attention to in order to maximize the prediction quality i should note that i am a python guy so i use everything written in for python thanks in advance
1,how do i interpret these results spearmans rho 0 107 p value 0 378 what do they show about the correlation between the two variables i need to interpret these for an academic report
2,do es the target s need to distribute normally before training i have built many classification models now i am working on a continuous target that like many other cases is heavily skewed to the right value from 0 and up that becomes less frequent as the value increases i have read that it is necessary to first have a transformation to make the target distribute normally and invert it upon predictions i have also read that residuals have to distribute normally is this necessary does it also depend on which model you are using i have been playing with a random forest regressor thanks
1,outliers robust vs standard mahalanobis i m cleaning some messy survey data that i ve collected with a dozen or so psychometric instruments i ve been reviewing it with the careless and routliers r packages i ve got literature for both methods but there seems to be less literature showing the effectiveness of using the robust minimum covariance determinant mcd on psychometrics what do we think about choosing one or the other mahalanobis thanks
2,project evolving beyond 100 billion recommendations a day how reddit does recommendations
2,how can i run real time pose recognition with out doing it on device i want to as close to real time as possible upload images from live camera from a weak device to an api cloud and then immediately render the result what can do this
1,education stuck in one location for 3 years ms in data analytics worth it tldr i’m looking for viable ways to “break” into the field of data science from a social science background and wondered if the following program will give me enough of a foundation to make the move i have a ba in political science brief work r was required for methods and i really enjoyed it i’ve taken java i and ii discrete mathematics calc i iii and linear algebra at my uni i already attend we have a ms that goes as follows core intro to database systems intro to machine learning big data analytics deep learning intro to statistical computation advances statistical methods 2 electives from the following data mining nosql databases data visualization advanced topic classes vary by sem operations research methods and then practicum would this be useful it wouldn’t cost me too much out of pocket 10k i have a decent govt job right now doing something completely unrelated it work with mainframe but it’s being automated away i don’t have a super strong comp sci background clearly and while i’m in this location for personal reasons i thought i could make the most of it if i’m shooting myself in the foot thats totally valid also and i’m okay with just working and waiting to apply later to applied stats or something else i didn’t get any chance to do research in undergrad and there’s a decent opportunity for that in this program i’ll be able to keep my current job when enrolled in this program also so no huge blow to “opportunity costs” since that’s what i’d be doing anyway beyond this would i need a second ms or any further education or will this be sufficient to get me started thanks for any help advice
0,feeling overlooked and undervalued as a self taught data professional when compared to advanced degree holders i am a self taught data professional well versed in most data analytics data engineering program tools math statistics and software at my current work place i am constantly coming up with new analysis tools automation processes troubleshooting and qa i have earned the respect of my peers and managers and i have excellent technical teamwork and communication skills i am at par if not better overall than most of my colleagues whom have masters and phds i personally feel as if i have a chip on my shoulder because of me being self taught the grittiness needed to learn the material and find things efficiently on my own is something i value in myself it’s just really unfortunate that at face value no matter what i do there will be tons of people with advanced degrees in data who get put ahead of me and paid more than me in the job search i also feel like in the workplace i also have to go the extra mile with new seniors i meet a lot of the times for them to take me seriously i understand why it happens but it really is just saddening and upsetting please tell me there are people in here that have felt and experienced the same thing as i have with this if so how have you learned to engage with this yes i know one of the answers is to get a advanced degree
1,discussion digesting statistics textbooks i was wondering if others would be open to sharing their process and thoughts as they digest a textbook with the intent to do well in a upperdivsion graduate level statistics course topics do you read sections before lecture how many times do you read how do you time manage how do you effectively digest do you read and summarize do you read and take notes do you read completely a section before taking notes or perhaps take notes ideally one would like to do as many practice problems as possible but in consideration of managing time life and school avoiding diminishing returns and maximizing value what is your approach to practice what are some habits that pays it dividends long term more efficient tips please hahahah
0,update just had my interview was it an air raid style of interview that i was afraid it was going to be prior post for context sadly i think it was an air raid for the most part and i don t think i did very well i didn t really get many rapid fire trivia questions although i got some and i had to answer a couple with i don t know for example i was asked what is the mathematical equation to calculate p values i answered with i don t remember i would have to consult my college notes from 11 years ago in my head i was thinking why does it matter when scikit learn or r does that all under the hood there was another trivia question i got which i answered i don t know but will not say what it was for the sake of not outing myself incase the interviewers are reading this i got some data engineering questions not my strong suit and i don t believe i gave good answers everywhere i have been i have worked along side with data engineering teams but have never actually done the data engineering work when i got to the point of the interview where i could ask questions i asked if this role was more of a data science role or data engineering role and they said definitely data science they have separate data engineering teams so i have no idea why i was even asked these questions i think my strong suit was using my past work experiences to outline how i got things done but even then i am not sure if my responses were good enough because i felt like i was on defense from the get go i do better in interviews when i am on offense so to speak for example i was asked for an example of how i segmented customers in the past and used that to drive revenue growth i said i would look at various segmentation techniques and choose the best one based on model performance against test data and how i determined that to be k means clustering after using pca to reduce the number of features and using a scree plot to determine the number of segments and how this segmentation approach allowed us to grow our revenue from 4th place in the market to 2nd place i don t think i was precise or deep enough in this answer and was too vague and i didn t project enough confidence but in an hour long verbal interview how deep am i supposed to go i struggled with finding the right balance of depth and respecting the time constraints i ll be surprised if i don t get rejected but i feel like this format was tough to succeed in i believe i am a strong data scientist but i am at my best when i know exactly what scenario i am facing questions like here is what we want to accomplish how would you do it are what i do best i didn t get any of those questions it was more open ended broad questions such as what is your go to machine learning model and i literally could have given anything it depends on the task at hand i said random forest to go with the question and how i like the built in oob functionality in r although if i could answer the question again i d go with gradient boosting another question i hated was how do you stay on top of data science industry trends i didn t really know how to answer that one i said i stay engaged with publications and gave a personal example of what i do in my free time that i won t say since i don t want to out myself incase those who interviewed lurk here but i don t like this because i believe you stick with what works and you don t always need to be jumping to the new and shiny toy out there unless there is a good reason and if there was you d probably know about it without having to be proactive to learn about it another question was do you follow up on your own models performance against real world data i didn t have a problem with this question because of course i do who doesn t i said its one thing for a model to have high accuracy on train and test data its another thing for it to actually perform on real world data that you collect after the training and testing process therefore you want to make sure its actually hitting the marks that you calibrated during the model building phase otherwise go back to the drawing board then they followed up with what if you gained more responsibilities on your plate and could no longer follow up on all of your models which i think was a silly follow up because i feel that is putting the cart before the horse i said in a hypothetical scenario where my bandwidth was overextended to where i could not do such a thing as i hope it would not get to that point i would have to resort to being a leader and calibrating the bandwidth of my team and allocating resources to ensure it does get done i got asked how do you perform feature selection and i simply explained various ways to do it based on the model at hand i e p values for linear regression feature score for random forest etc i didn t like this question because its something you would ask an entry level candidate not someone interviewing for director i answered it anyway but with only an hour i feel like better questions could have been asked the interview was solely to gauge my technical abilities not my leadership or management abilities so i understand that the interview was aimed to do that with that being said i don t think i answered the questions given to me with enough confidence or precision i am good at gauging body language facial expressions and tone of voice of those interviewing me and i don t think they were impressed if its up to these 2 guys i don t have a chance the only way i have a chance is if all i needed to do was show some basic technical competency and the hiring manager who was on the call but was just there to listen values my management leadership attributes and track record more than my hour long call with these 2 guys comparing this interview to the 3 bad ones i had before i think this one was definitely the worst of the bunch in terms of how i gauged my performance i am starting to lose confidence in my interviewing skills as i believe i have a hard time translating my actual 10 year track record into verbal competency i know how to do things i just have a hard time coherently explaining how those things are done fortunately i have another interview starting in 10 minutes and 5 others lined up this week but this was the job i really wanted out of all of them
2,survival analysis how to calculate the concordance index on a test dataset hi guys i have fitted different types of survival models with a training dataset and the output of these models provide a concordance index however this concordance index is calculated in the training dataset i then used these fits in a test dataset to make predictions of time to event and i want to calculate the concordance index on the predictions made on the test dataset because obviously a model should be evaluated on a test dataset instead of a training dataset i ve searched online but i didn t find how to compute the concordance index on a test dataset how can it be done thanks
0,how many layers neurons are required for a neural network to be considered as deep i know this might sound like a silly question but when people use the term deep neural networks is there a minimum number of layers neurons required for a neural network to be called deep
1,has the multi armed bandit problem been solved if so what is the optimum algorithm to follow so there s a gambler who is playing slot machines and each slot machine has a chance between 0 100 of outputting a win the gambler wants to win as often as possible and lose as few times as possible what is the optimum strategy the gambler should follow also how does it change if there are an infinite number of slot machines also what about the case where the results are not binary w l but there are many possible results e g every machine can either lose win £5 or win £100
2,graphdf graph deep factors for forecasting implementation anywhere hi all i came across the graphdf paper a while ago it s dated oct 2020 seemed interesting and i ve been checking periodically for an implementation a couple projects libraries i work with have already requested it too does any of you know about it thanks in advance
0,what’s the best source for raw medical data i’m currently researching different prosthetic heart valve replacements and how they affect patient outcomes in a certain region of california i want to get my hands on county specific data that shows which type of valve was used and then follow up on the patients outcome is this possible i have no idea where to acquire these data thanks for your help
1,in structural equation modeling can a manifest variable indicate more than one latent variable i ve run my model and the fit is not good so i looked at the modification indices to make adjustments and the numbers suggest i should correlate a manifest variable with multiple latent variables is this okay in sem i was under the impression that each manifest variable should go with a single latent variable am i mistaken
0,entry level position needs a phd with 3 5 years experience is it an entry entry level or i ve got a phd and 5 years experience entry level job ad
0,steps when working with a new database what steps would you take to make sense of a large new dataset that has been sent to you had the above question pop up in an interview so from a data science perspective what would you do
2,master thesis multidimensional spectral clustering project research i am about to choose my topic for master thesis and i just thought about spectral clustering algorithm i am not much into this topic and i am wondering if somebody has more experience with this and could help me i was thinking about taking this algorithm to multidimensional level e g take multiple pairs of classes in huge dataset and try to take decision of clusters based on results of each pair maybe somebody know if there are some algorithms using this or similar approach with additional weights maybe you could tell me shortly if mine thinking of using this algorithm is correct and is there any sense to dive deeper into this topic there are some ideas in my mind but i am not sure if this can give any positive clustering outcome i am also wondering if pca method not using similar concepts
0,setting up a feedback loop for performance evaluation and retraining of a model i am working on a project which shows a warning to the end user whenever a particular event takes place for the feedback loop i have decided to give the end user an option to annotate the warning as correct or incorrect i can calculate the precision of the performance using this feedback info since precision is the proportion of positive identifications that were actually correct but i don t see how i will be able to calculate recall since recall is the proportion of actual positives identified correctly and i am not really getting false negatives when warnings aren t generated when they should be in this project from the feedback should i set up my feedback loop in some other way sorry for the vague language can t disclose specifics of the project because of nda
0,data science specialties i’m a data scientist at a large firm and i specialize in deep learning approaches to nlp over the last few years i’ve gotten particularly deep into this field and it got me wondering do other people specialize or do they stay generalists i’ve found with all the different areas of data science growing so fast you’ll never be able to keep up the pace with cv nlp etc if you try to do them all also for those that do specialize what’s the distribution of the different specialities across the industry like what proportion are computer vision experts vs nlp vs other apologies for the formatting and grammar on mobile today thanks internet
0,anyone started a phd after a few years as a data scientist hi all wondering how many people have worked as a data scientist for a few years then gone back for a phd whether just for fun or to advance the career mostly wondering how you were able to sell it like we use a ton of ml models to solve business problems but they re rarely cutting edge and probably difficult to sell as academic research did anyone get any impressions of how data scientists were viewed in academia whether the industry data science experience helped or hurt you in being admitted to top schools and what it was like to go back to a phd after working as a data scientist
2,reward is enough david silver richard sutton this is a text based post only because the paper is behind a paywall and so the yannic kilcher video may be more useful to those who don t have access paper video
0,looking for a data visualization tool i am looking for an open source data visualization tool ideally for python that can generate graphs similar to what is shown in the following link i don’t know what this graph type is called any help is much appreciated
1,confused on how to proceed binning data i am working on the following problem i have a dataset and i want to fit a random forest model to the data to illustrate my example i created some fake data and fit the random forest model i am using the r programming language load library library randomforest create data var 1 rnorm 1000 10 10 var 2 rnorm 1000 5 5 var 3 rnorm 1000 6 18 favorite food c pizza ice cream sushi carrots onions broccoli spinach artichoke lima beans asparagus eggplant lettuce cucumbers favorite food sample favorite food 1000 replace true prob c 0 5 0 45 0 04 0 001 0 001 0 001 0 001 0 001 0 001 0 001 0 001 0 001 0 001 response c a b response sample response 1000 replace true prob c 0 3 0 7 data data frame var 1 var 2 var 3 favorite food response data favorite food as factor data favorite food data response as factor data response fit random forest model rf randomforest response var 1 var 2 var 3 favorite food data data ntree 50 mtry 2 in this example you can clearly see that for the favorite food variable most people either like pizza or ice cream the other categories e g onions eggplant etc are almost none histogram data favorite food in this example it would make sense to bin reduce the favorite food variable into 3 categories pizza ice cream and other in this example this is clear but for bigger and more complex data this is not always as clear is there a common method in statistics that can be used to automatically bin variables having many categories into fewer categories i was looking into methods such as factor analysis and latent class analysis but i am not sure if these methods apply to this problem that i am working on in my real data i am also trying to fit a random forest model and i have several categorical variables e g such as favorite food with hundreds of categories can someone please suggest a statistical method that can be used to deal with this problem thanks
2,an incorrect claim in a paper which the authors will not acknowledge respond to time series anomaly detection a recent paper a claims that the matrix profile does not work well on a particular dataset nasa smap g7 we can test to see if this is true with a single line of publicly available code lets do it interactivematrixprofilever4 website g7 1 length g7 1000000 50 hmm the matrix profile results are perfect the yellow circles highlight the fact that the three peaks of the matrix profile coincide with the labeled ground truth you can double or half the parameter and get the same results i did point this out to the authors but they don’t answer emails there are quite a few papers that do this cripple the matrix profile to make their approach look better including papers in good venues like vldb as a community as authors and reviewers we need to do better a local anomaly detection for multivariate time series by temporal dependency based on poisson mode ieee trans neural netw learn syst 2021 jun 2 x200b x200b
1,use case for a hidden markov model hello guys i work with my schools baseball team and i built a markov chain to simulate pitch sequences given after the first pitch was thrown so day fastball was thrown what was the next most likely pitch or maybe 20 most likely they said that they wanted me to include outcomes in my markov chain as in just simulating pitches wasn’t enough they wanted to know what kind of pitch sequence would produce a specific outcomes such as an out strike etc in other words what sequence of pitches has the highest probability in yielding a specific outcome my first through was to use a hidden markov model saying my hidden states are pitch types “fastball curveball change up slider” and my observed variables are the specific outcomes strike double play out etc that way i can get the sequence with the max probability of yielding a specific outcome that’s observed for example the coach told me “our main pitcher had 7 strike outs one game using the same sequence of curveball slider curveball and he usually has a weak curveball so we don’t know how that happened but if there’s a pattern between what outcome there is and the specific sequence we want to know which sequence had the highest chance at yielding strikes or outs” so would this be a good opportunity for using a hidden markov model any other suggestions
0,predicting number of employees that will leave for a high turnover workforce hi all at work i am looking to predict how many employees will leave the company on a weekly basis we contract a lot of our staff so every week we can have new joiners and those that leave either due to their contracts ending poor performance etc depending how work is going we may be looking to hire 10 new contractors every week with around 3 4 leaving however in times when we get particularly busy i want to be able to predict that more employees will leave and so we can tell hr to hire more contractors the data i have available but can try and get more is total number of employees that have left every week total number of employees we hire every week current end of contracts rating of contractors those with particularly poor ratings we will look to terminate x200b can someone help me understand which direction i need to go in to do my forecasting just to confirm i m asking for you to point me in the direction as i have never done this before i ve read a few articles on python that try and predict which of your current crop of employees will leave but as there is a chance that our contractors can change i need this model to be future proof x200b my initial thoughts were to create a basic model in excel where i calculate a baseline using the past 4 weeks actuals then apply an uplift in weeks with a high end of contract and when we see high number of low rated contractors this is quite manual though so i was wondering if there was a more clever and robust way to do this x200b any advice information would hugely appreciated x200b thanks all
1,spss alternatives hi all we re long time users of spss and are currently looking at alternatives we conduct research into business use of software applications and services and have a team of three analysts using spss we re looking into alternatives that are cheaper than the 99 per month per user that we re paying now for spss we run on macs as well most of our analysis consists of the following simple pie bar chart generation using syntax means comparisons frequencies cross tabs we do frequently need to generate charts using the select cases capability e g to create charts showing adoption of an app for a specific industry we use spss s chart template capabilities to produce custom chart layouts using our standard colors and fonts beyond that we really don t use the more advanced features of spss our typical data file consists of between 500 and 1500 records and a 100 or so variables we ve started looking at jamovi and jasp and are wondering if they could potentially be cheaper alternatives that are as easy to use as spss we don t want to climb the learning curve for r and we are looking for something that has a more visual gui thought recommendations thank you in advance
2,dynamic image editing using clip and l grammars and the implications for hci hi everyone x200b i am a storytelling researcher at georgia tech who has a large interest in computational creativity i also work at eleutherai directing the multimodal grounding subgroup as well as the less official computational creativity horde x200b we eleutherai have spent the last few months working on an image editor based off of clip that allows you to specify edit and most importantly execute natural language substitution rules for editing images for these purposes we ve custom tailored a vqgan 16k on wikiart that we feels compliments this style of image editing fairly well x200b this method drastically differs from competitors like latentrevision in that the masking component is automatic and it differs from stlyeclip in that we can perform all editing zero shot we do not need to train a special model to perform edits we also use a very different optimization technique thats based off of weighted spherical geodesic distances x200b this is the first stage of a larger computational creativity effort where not only intend to see how these methods apply to image generation but to text as well can we specify reviews of stories in the form of l grammars that when executed allow us to constrain and ground a story can we use clip as a discriminator of stories in a way that makes sense for collaborative writing more on this project in the coming months x200b here is the colab notebook x200b if you just want the wikiart model x200b paper and parallel blog post on coming within the next month or so x200b happy to answer questions x200b if you just want to see how the method performs here is an example of editing the face of my advisor and here is an example of sequential editing
0,do you ever stick with legacy estimates or always use the most recent i couldn t think of an elegant way to explain the question in the title so hold judgement on it until i explain further with an example many of the data sources i use are estimates for intercensal population from the census bureau as one example when a new version comes out the source will update the prior year s estimates using the new data in v2020 population for springfield in year 2015 could be 1 000 but then in v2021 population for springfield 2015 might be 1 005 in some cases the changes are so extreme using older data is just not possible however most of the time it s minor enough not to change takeaways but rather just to make the results a little different this is the case with a good number of the sources i use i feel bound to use the freshest data possible the non technical folks hate this it means prior materials no longer jibe with new ones and they have to explain it to clients so i always get pushback on it there have been a few situations where i simply couldn t convince my boss and had to use prior estimates so my question to you my data brethren does anyone ever utilize the old estimates in situations like this do you think it s defensible or are there no circumstances in which you would ever do so
1,causal language do you agree with this this is a list of words that authors suggest should be avoided if the study is observational i would like to know your take on this i m not sure if words like contribute or determinant point to causality
1,how to formalize the notion that a you can be more confident with a certain rating with more reviews something that seems empirically true to me but i am not sure how to formalize is the notion for example on steam a game is rating by thumbs up down it seems clear that game with 95 positive from 40 000 users will be more likely to be a good game to you than one with 95 positive and from 4 000 users but how much moreover how can we compare one with 88 from 500 000 vs one with 93 from 5 000 intuitively it seems like the one with 500 000 should be more likely to be a good game than the one with 5 000 despite it s lower rating
1,is there a term for analyzing a single set of data using multiple approaches i thought it was something like all possible worlds but haven t been able to find this
1,statistical test for comparing proportions of single population over time i have a sample population and am trying to compare whether the proportion of the sample who belonged to a certain weight category changed after march 2020 so i have data on the proportion of participants whose weight was 200 lbs and the proportion that was 200 lbs before march 2020 and i have the same 2 proportions post march 2020 which statistical test can i use for this i was thinking about a 2 proportion z test but i wasn t sure if this was only for 2 separate populations vs in this case i am looking at the same population just at 2 different time points i don t know much about stats so apologies in advance if this is a silly question thanks
2,how do i do sentiment analysis and classification for an unlabelled dataset i am trying to implement sentiment analysis and classification on tweets of abc organisation using machine learning x200b note the data set is unlabelled i only have a bunch of tweets x200b for example x200b input abc company provides the best and quick service x200b output sentiment positive classification service x200b x200b i am stuck on how to implement this problem i tried using pre trained models for sentiment analysis like textblob and vadarsentiment but got an average result i still know anything about how i will implement the classification part x200b i am stuck can anyone please help and provide insight on how can i tackle both problems should i try any other pre trained model if yes which one or should i label the dataset manually and train a model or anything else
1,do you agree with this statement i came across the following statement when reading a paper various researchers have provided an indication how to interpret bayes factors e g jeffreys 1961 raftery 1995 for completeness we provided the guidelines that were given by raftery 1995 these guidelines are helpful for researchers who are new to bayes factors we do not recommend to use these guidelines as strict rules because researchers should decide by himself or herself when he or she feels that the bayes factor indicates strong evidence do you agree with that last sentence because from my understanding until you choose a loss function you can t say anything about the difference between two bayes factor i know that it is not always feasible but if this choice belongs to the researcher then it is subjective and arbitrary like the 0 05 or 0 01 thresholds when using p values
1,how can i determine the required sample size for a qualitative interview i read only that there is not a lot of consensus with regards to the required sample size for interviews how can i figure out how large my sample should be and what kind of power analysis could i do for qualitative data
0,i wanted to create a project using the instagram follower and following lists how do i extract list of followers and following of public accounts from instagram don t want to use any third party apps
0,any advice on how best to parse 1tb of excel files with horrific formatting i got lucky enough to stumble in to an analyst role at my job and have recently been handed a huge archive of documents that have been collecting dust for the last couple of years i have been tasked with seeing if there is anything worth finding in this beast because apparently someone up the food chain recently read a mckinsey article on strategic analysis ¯ ༼ ಥ ‿ ಥ ༽ ¯ up until now i have been lucky enough to only mess with curated data and on my worst days a folder of excel docs full of simple transactional data this dataset is altogether terrifying each files contains a single sheet but is structured almost like a comic book by which i mean whoever put the intial template together was clearly never intending it to be parsed by anything other than a human varying field names merged cells no actual tables imported pictures clip art check boxes and other odd bits and bobs that i don t understand existing in excel i prostrate myself before you actual data scientists with a simple query where the hell do i start do i try to programatically convert them to csv json is this legit ml territory that i have no business touching i am at such a loss that even suggested search terms for me to start researching what to do next would be a huge help
0,thoughts on getting stuck with working on a specific domain my current position is kind of a consultant style role where there isn t a specific product project to work on and it s kind of whatever projects come through the door my next position however is working with a specific domain risk fraud i m curious if working on a specific area e g fraud ads recommender systems etc ends up locking you into that area for the remainder of your career and future positions also out of curiosity which domain has the greatest potential for future positions
1,can you still use survival analysis models without censored data suppose you want to use survival analysis models for longitudinal time to event modeling but suppose you are very lucky and have no censored observations in theory this should not prevent you from using survival analysis models
1,does basu s theorem require sufficiency or minimal sufficiency p287 in casella and berger s statistical inference says theorem 6 2 24 basu s theorem if t x is a complete and minimal suffi­cient statistic then t x is independent of every ancillary statistic p42 in lehmann and casella s tpe says theorem 6 21 basu’s theorem if t is a complete sufﬁcient statistic for the family p p θ θ ∈ omega then any ancillary statistic v is independent of t is minimal required thanks
1,why are neural networks better than polynomial approximation has anyone ever come across a formal mathematical explanation as to why neural networks are more powerful than polynomial approximation have some results e g papers been proven that conclusively show neural networks have certain advantages over polynomial approximation
2,looking for references on overparametrized models and overfitting has anyone ever come across some papers that give mathematics explanations as to why non regularized i e overparametrized models tend to overfit data as far as i understand this is only an empirical observation overparametrized models have just been observed to often overfit data we don t actually know if there are mathematical reasons as to why overparametrized models tend to overfit in any case whether math based or empirical can anyone recommend any references papers sources that explain why overparametrized models overfit data also is there a mathematical intuition behind why lower order poynomials aren t very powerful but higher order polynomials tend to overfit can anyone recommend a source on this as well thanks
1,career opportunities after masters in statistics i have recently completed my bachelor’s of science degree in economics mathematics and statistics from india i enjoy statistics and it s application quite a lot i am planning to pursue my masters degree in statistics in the us is it a good country to pursue statistics what are the career options after my masters i do not have any work experience will that be a problem how can i improve and build my resume during my masters to become more employable any suggestions
1,what is the best way to quantify the strength of association between different things rated on the same dimensions factors i have x images rated on 25 dimensions i also have x words rated on those 25 dimensions when i factor analyze the ratings i get the same four factors for images and words though extracted in a different order i want to use this to quantify which images words are most similar dissimilar i ve done this by 1 z scoring each factor rating 2 taking the absolute difference between each image word on each factor 3 averaging these differences is this a good approach or is there something better i can do
1,i m in six sigma training and the course director told us statisticians don t really know stats i have my masters degree in statistics and i ve noticed that during this training a lot of the black belts have no idea what they re talking about while the main course instructor is an amazing engineer he does tend to say that what i learned in my degree program is incorrect because my uppity stats professors do not understand true stats because they ve adapted or changed the original meaning of things therefore what they say and what i learned is irrelevant and not true and i m not paraphrasing this is what they said word by word my argument to this is why should i believe you they seem to be looking down at people with a phd in statistics since they claim they don t understand true stats i find it hard to believe that an engineer knows stats better than well known statisticians from a top 10 stats program i find that i am unable to suspend my disbelief and feel like i m becoming quite resistant to the learning process even though i would like to keep an open mind i just find it hard to believe that my professors are all poorly informed and don t know anything while the engineers understand true stats someone explain this to me please is this normal for 6 sigma it has honestly really turned me off towards the whole thing i think i m struggling because i have a lot of respect and admiration for a lot of my professors i still speak to my advisor even though i graduated years ago i have no intentions of continuing my six sigma training after i finish this one
0,is wgu masters in data analytics program any good reviewing an applicant long story short have a candidate for a jr ds job he has a bs in cs from a state school respectable one and ms in data analytics from wgu personally idc but i know a few others on the decision team are concerned anyone have thoughts on wgus program
2,end priming transformers techniques i m interested in techniques to prime the end sequence of a transformer and am wondering if anyone knows of any papers or has suggestions on the best way to do this here s what i mean by end priming let s say we re training a generative language model and want to generate a sequence which ends with straw so that s the last straw and that s the final straw might be valid completions that the network could give us i ve tried two techniques so far and haven t had good results with either technique 1 alter the attention mask such that all time points can access the last n tokens with n being the length of the primer this approach results in training instability the loss starts to oscillate widely after a certain point in training so i modified it further such that i do crop the loss and do not include the last n tokens in its computation this too is unstable and i ve had to reduce the learning rate further but have still gotten poor results technique 2 place the primer at the start of the sequence while leaving the attention mask alone i e each time point can only see itself and the past time points for example i d want the model to output something like straw this is the last technique 2 also results in learning instabilities when i reduce the learning rate low enough to avoid them i m left with convergence to a poorly performing model i m able to train vanilla transformers just fine so i believe it s unlikely there are problems elsewhere in my pipeline
2,when will a non resnet efficientnet cnn derivative improve accuracy for imagenet currently the top 3 pure cnns on the imagenet leaderboard are just modifications of resnet or efficientnet which improve accuracy by either increasing the number of parameters or by mass pretraining with massive datasets i m not including transformer based networks in this why arent we seeing any new interesting cnn architectures why aren t we seeing modifications of other architectures which had once achieved state of the art and what architecture ideas do you guys propose which could show promising results my latests idea is to take xception nets and augment them with dense skip connections using concatenation instead of addition following the popular naming schemes this would be called densenext i found densenet to be incredibly promising however with most ml frameworks it s inefficient to train due to the concatenation steps not happening inplace exploiting seperable convolutions in a densenext architecture will massively reduce parameter count per layer which should improve memory compactness reduce fragmentation and improve cache efficiency what ideas do you have
2,the dimpled manifold model of adversarial examples in machine learning very very interesting new work by adi shamir et al it proposes a new mental model for why adversarial examples exist the central claim is that adversarial examples come from the fact that we fit high dimensional decision boundaries to low dimensional images this leaves a lot of space for the adversarial examples to exist perpendicularly from the true location of the low dimensional object the natural image more precisely the decision boundary is like a thin horizontal sheet of metal that is being bent up and down to fit clusters of training examples this creates dimples in the sheet around where the training examples lie the sheet can be thought of as the space in which natural images exist the dimples go into the extra dimensions because we represent those natural images in a very high dimensional form rgb images adversarial examples well these are just unnatural images above and below the sheet i e off the manifold of natural image robustness accuracy trade off this just comes from the fact that you have to bend the sheet way more out of shape than normal to fit adversarial examples so you end up missing the details small clusters being able to fit a model with good clean test set performance by training on adversarial examples with target labels that comes from the fact that you end up recreating the manifold by moving around the labels humans being insensitive to adversarial examples they have just learned to do projections into the low dimensional space so anything that lives above below the dimple gets projected onto the dimple this mental model seems to provide a lot of compelling explanations and there is good experimental work what do you think about it
2,finding good sources of data hi i am writing a university level course on ml and need to find data for exercises what are your favourite places to get data of the datasets that are available to the public which are the lines that you think are the most interesting and why
1,significance of a relative influence factor hi i am currently reading a paper in which the authors study the effect of relative merger and acquisition experience on bargaining power in a deal their argumentation specifically we argue that the differential between the acquirer and the target’s experience determines which one will bargain better terms and thereby obtain more value at the expense of the other party they state that most prior studies have only theoretically considered the acquirer’s experience and even the few studies that considered both parties’ experience e g porrini 2004 did so in isolation of each other without theorizing about relative experience as the authors argue m a experience of the parties involved in a deal is a factor frequently examined in the literature and has been included as an explanatory variable in many studies in many cases the experience of the target and the bidder is included as iv s in a model for example to explain the deal premium i e the difference between the purchase price and the share price of the target the relative experience variable is constructed as the count of all m as that the firm completed during the 10 years preceding the focal m a in a second step the target’s m a experience is substracted from the acquirer’s m a experience to measure the difference in experience between the parties this variable takes positive values if the acquirer’s experience exceeds that of the target and negative values if the target’s experience exceeds that of the acquirer the calculation of the variable is straightforward yet i am having trouble interpreting the relative m a experience to what extent is the meaningfulness of the relative factor higher compared to a model where i include the deal experience of the target and the bidder as two separate variables
1,has anyone pivoted from statistics ds to finance i have a statistics education and when i started earning money i researched the best way to invest today i max out my tax advantaged accounts and put a little bit of each paycheck into index funds as i do this i’ve learned that i really enjoy investing i think the stock market is one of the most interesting data sets there is this led me to consider taking up financial advising or investment banking as a side hustle or even a future career has anyone done this i fear that without an education in accounting or a relevant cert like cfa the transition would be hard to muster also i hear the hours are some of the most brutal in the corporate world then again hard to compare a venture capitalist with a fiduciary who has a handful of clients
2,twitter bot that tweets trending ml papers hey everyone i created a twitter bot that tweets trending papers in the ai ml category cs ai cs cl cs cv cs lg and stat ml on arxiv org arxivaiml tweets are based on the engagement score of feedly the algorithm is simple and naive implementation if you have any idea to improve the bot please let me know
2,deepfakes talk by kaggle grand master yauhen babakin b e s next june 14 at 19 00 utc 2 will be our next online event x200b yauhen babakin kaggle grandmaster will talk about deepfakes x200b the talk will be in english and english transcription subtitles will be available x200b and at the end of the event will be our kahoot quiz x200b free event limited places sign up here bit ly ekdmspain1
1,interaction term in regression hi i made a correlation matrix of gender and a variety of clinical outcomes however i was told that i needed to also consider weight status either continuous or categories like obese not obese as a cause for the clinical outcome and in particular an interaction term between gender and weight status and how that interaction term relates to the clinical outcome my original approach was correlation matrix but i ve only heard of interaction terms in context of regression it would be so complicated to include many factors in a regression what are my options to see if there s an interaction between gender and weight status on the clinical outcome could i do an interaction term of gender x weight weight gender in the model nothing else and see how they all relate to clinical outcome in a regression or do i just do the interaction term alone at first in univariate regression what kind of regression and does inserting the variables need to be ordered a certain way how would i interpret the results regarding interaction and how do i create an interaction variable w a binary such as male female thanks
2,discussion xgboost is the way came across this paper where the authors show xgboost or an ensemble of xgboost nn outperforms dnns when it comes to traditional tabular data would love to hear your views or professional anecdotes which may or may not agree with this
1,overall prevalence hi everyone rookie here if one study has a population of 200 wherein 35 are sick and the other has a population of 300 wherein 40 are sick what is the overall weighted prevalence of the disease i am trying to complete this clac in spss but i feel like my logic 200 500 35 300 500 40 is very wrong please can someone help me understand
2,darknet yolo v4 with cuda 11 hello everyone has someone managed to build yolo v4 darknet with latest cuda versions 11 2 or later i am stuck in dependency loop of older drivers not supporting 3000 series and newer cuda requiring latest drivers
1,complexity of time series models arima vs lstm 1 when it comes to time series analysis i am trying to understand what makes newer models such as lstm s capable of capturing more complex patterns in the data compared to older modeis such as arima in statistical learning theory there is something called the vc dimension of an algorithm the vc dimension is apparently what describes the relartive level of complexity a machine learning algorithm can capture does this concept of vc dimension carry over to models in time series analysis is it possible to show that lstm s have a higher vc dimension compared to arima style models supposedly neural network based time series models were developed because modeols like arima was unable to provide reliable estimates for bigger and complex datasets mathematically speaking what allows a lstm to capture more variation and complexity in a dataset compared to arima 2 lately i have seen many people starting to suggest that convoloution neural networks cnn can also be used for the purpose of time series forecasting just as a general question in what instances would it be better to use a cnn for time series forecasting compared to an lstm thanks
0,i used vader sentiment analysis to track and invest based on wallstreetbets stock sentiment i m up 33 annually 16k here s source code process result and an article source code mine article not mine but it s an amazing look into how this works how i did this scraped wsb sentiment got the top most positively mentioned stocks on wsb for the better part of this year that s been gme and amc recently some spce and nvda and about 13 other stocks i have the strategy rebalancing monthly right now i m up 60 ytd compared to the sp500 s 13 the recent spikes in gme and amc have helped tremendously some stats and a picture of a card i made giving more info about the strategy the strategy is backtested only to the beginning of 2020 but i m working on it it s got an annualized return of 33 compared to 16 for the sp500 max drawdown of 8 7 thought this was pretty interesting wsb would be a very cool hedge for financial markets at large rode covid like a wave happy to answer any more questions about the process results i think doing stuff like this is pretty cool as someone with a foot in algo trading and traditional financial markets
0,how can i practice finding data quality issues before an interview assessment a recruiter has sent me a paper to review including “the six primary data quality assessment” before an online assessment i will go through with him in which he will send me a dataset to find data quality issues using python pandas within an hour where to find datasets to practice on what do i expect to be doing in this hour how to practice and be ready in the next couple of days link to the paper
1,whats the difference between individual fixed effect model and two way fixed effects model my dataset is a balanced panel with 290 individuals all observed in 3 different time periods 2010 2014 and 2018 i want to measure the effect of x on y with a fixed effects model when i include both time and individual fixed effects my regressor x and its lag interacted with a time dummy for 2018 x 2014 dummy2018 is not significant however when i remove the time effects from the model they both become significant and their estimates increases by a lot is a fe model with only individual fixed effects called an individual fixed effects model is a fe model with both time and individual fixed effects called an two way model is the year dummies from my fe model with both time and individual fixed effects consuming the effect off x on y x does vary over time and is not constant any help is greatly appreciated thanks
0,what s a good first step data management software for a team that currently has no data infrastructure hi all i m a data analyst on a non technical team that collects and utilizes a lot of data because it s a non technical team there is essentially no data infrastructure set up specifically there is no data storage or management software being used we simply access our data directly in the file system the scope of our work has grown a lot recently and this way of doing things is quickly becoming unmanageable especially with respect to logging versioning and being able to find and reproduce old analyses so i m wondering if it s time we use some actual software to help us with this i ve been reading about data lakes our data is whole files can t be put into a relational database data warehouse i m wondering if a data lake might help us however i ve read that data lakes don t have any inherent structure to them and this is what allows them to store any type of data so what s the benefit of using data lake software services over just organizing our file system storage if i use a data lake will i still end up with the same question of how do i organize all this is there another type of software that i should be looking at that s more attuned to my needs keep in mind this is a non technical team so i will need to train any newcomer on whatever i use and can t assume they will have ds cs knowledge but they will likely be comfortable with simple scripting in python r bash etc thank you for any help
2,ml workstation vs google colab pro hi i have been using google colab for some time now and it was convenient however we would like to check if building a workstation e g can be found here worth it those who build their own workstation could you please chime in on what are the benefits that you gain of using your own workstation vs google colab amazon sagemaker
0,what is your thought on sas as a tool for data science cheer everyone i just moved from python to sas for 4 months due to new job requirements i wonder how you think sas compared with other languages any future mine sas is not so complex the only problem is we have to memorize weird syntax lots of problem can be solved with proc sql unfortunately proc sql has some different characteristics compared with standard sql e g why row number is missing in proc sql i likely use sas mainly for practicing sql the syntax is unique and not transferrable if you re in sas industry for too long then it s likely hard to move to other jobs with different tool unlike if you know matlab or python you can easily move to r or even c c they re interconnected with each other very well sas is a standalone hero company uses sas likely for security purposes need an organization who is responsible for the tool if anything bad happened then sas visual analytics is another story and if you program for advanced filter in sas viya then again it s more or less different systax compared with sas guide what s your thought
1,importance of kolmogorov s probability axioms i read earlier today that kolmogorov s probability axioms are some of the most important results in probability can someone please explain why these are so important what relevance and application do they have thanks
0,how to filter data fast hello i have a data science machine learning question i have raw data how can i filter it problem is there might be irrelevant data duplicate values i am working on images now or any other edge case so how can i detect outliers and irrelevant images
0,choosing a combined title e g data scientist engineer if given some say in setting a job title for someone who does both and will be expected to do both in the role would you go for a merged title like data scientist engineer or just pick one context data scientist roles are already on the resume so goal is to highlight the additional skill responsibility if appropriate
1,question temporal aggregation of random walk hello everyone i have this doubt about random walk that i can t seem to find a convincing answer to hope someone in here could help me if i have a daily time series that has all the characteristics of a random walk and if i have another weekly time series that results from the aggregation of that daily random walk time series the weekly time series does not have the characteristics of a random walk is it safe to say that the weekly time series is not a random walk despite being an aggregation of a random walk thank you very much
0,id like to draw and create better visualizations i m working as an intern data analyst till march of 2022 at a big tech company and i want to learn how to draw better and more unique visualizations of data i m currently using python in jupyter pretty much standard from what i ve seen here but i know that there is rstudio available in my company what is better and how would you go about it this is just a little context maybe it helps i ve worked as an intern full stack dev 6 months before using java and doing some docker database stuff from then i worked as a part time backed developer untill i got this job as a data analyst intern i m still defining if i like it more than dev to make a career out of it but until about a month ago was pretty fun and challenging to play with data and try to extract value out of it now i m a little bogged down with stakeholder management meetings and having impostor syndrome feeling like q model fitting monkey so to address that i wanted to try and do something different like more ad hoc visualizations or even drawings using plotting i feel like the math is there for it and i know a few libraries i haven t been able to test during my free time so i figured that the people who share things that inspire me you might want to share a few tips recommendations dos and don ts sorry if the english is weird stll learning corrections are appreciated
2,what are your thoughts on dl works on nature science usually a multidisciplinary work usually with fancier images than ml dl conferences but for arch most only use simple combination of models like cnn resnet lstm what are your thoughts on those works
1,concordance index for censored observations references the c statistic gives the probability a randomly selected patient who experienced an event e g a disease or condition had a higher risk score than a patient who had not experienced the event just a question for a given survival model is the c index only calculated using the set of patients who experienced the event for the same survival model do we not use censored patients for calculating the c index thanks
2,is this type of problem possible with ml if so how i have a dataset of 10 labels i want to give an input and the model outputs the top 3 similar labels from that input the input will never be one of the 10 labels however i want to see which 3 labels the input is most like what sort of ml problem is this i hope that makes sense
0,using python classes to streamline data modelling cleaning i work as a data analyst and i often have downtime between assignments so i m trying to practice oop and make my work more efficient does anyone have success creating a library of python classes to then call methods on pandas df instead of writing out the same lines of code i m trying to figure it out but haven t yet below is a simple class for string cleaning although it isn t finished yet class phonenumbers this class sanitizes phone numbers and appends any digits beyond 10 to a separate column def init self pandas obj self obj pandas obj def sanitizephonenumbers obj obj replace r 0 9 this is the code i m trying to replace sanitizing phone numbers df phone replace to replace 0 9 value inplace true regex true adding extensions to seperate column pt 1 df phone main df phone str 0 10 adding extensions to seperate column pt 2 df phone ext df phone str 11 removing original phone column df df drop phone axis 1 if anyone could point me to a resource help me with my code or show the code that you used for this task it would be much appreciated
2,manually calculating probability from xgboost model in python discussion hi i’m trying to manually calculate probability from xgboost in python i have used predict proba function to get the probabilities but would like to calculate them on my own can someone please help me with the formula code
0,missing values in xgboost hello everyone does anybody knows how xgboost treats missing values in the y dataset when a model is trained i know it handles well missing values in the independent variables but i have not found what happens if there are some in the y dataset i am doing several regression models and it seems to work fine but dont quite understand yet if the nan rows are excluded automatically or if the training of the model is affected in some way there are just a few nans by the way less than 5 thanks a lot
1,find minimum number of points to qualify in a sporting event hi please direct me to another subreddit if this is the wrong one i m following an e sport where you qualify to the word finals by competing in 4 different tournaments throughout the year and getting placement points from these tournaments i m trying to find out what the lowest number of points would be for a team to be in the top 7 within a certain probability after all tournaments have ended is there a method or formula i could use for this the concrete example can be looked at here e g heroic won the first of four tournaments and have 200 points how many points would they need from the next 3 tournaments to safely qualify as a top 7 team thanks for any help or tips
0,why do so many of us suck at basic programming it s honestly unbelievable and frustrating how many data scientists suck at writing good code it s like many of us never learned basic modularity concepts proper documentation writing skills nor sometimes basic data structure and algorithms especially when you re going into production how the hell do you expect to meet deadlines especially when some poor engineer has to refactor your entire spaghetti of a codebase written in some jupyter notebook if i m ever at a position to hire data scientists i m definitely asking basic modularity questions rant end edit i should say basic oop and modular way of thinking i ve read too many codes with way too many interdependencies each function should do 1 particular thing colpletely not partly do 20 different things edit 2 okay so great many of you don t have production needs but guess what great many of us have production needs when you re resource constrained and engineers can t figure out what to do with your code because it s a gigantic spaghetti mess you re time to market gets delayed by months who knows spending an hour a day cleaning up your code while doing your r d could save months in the long term that s literally it great many of you are clearly super prejudiced and have very entrenched beliefs have fun meeting deadlines when pushing things to production
1,appropriate way to present statistics for age collected in ranges hello i performed an audit which involved recording audited individuals age into discrete age ranges 19 20 29 30 39 etc what would be the appropriate method of displaying the statistics of this age range is it appropriate to calculate the mean age range of the study and the iqr sorry for such a basic question its been many years since my undergrad stats course thanks for your help
2,best drone simulator for ml purposes hello can you recommend some drone simulators i am trying to create ml algorithms for autonomus flying there is airsim but it would be nice to use more than one simulator and compare results thanks in advance
1,question how to handle wrong statements in longitudinal data set hi i have the following problem i m writing a thesis and i m using census data households get randomly selected and get questioned 5 consecutive quarters i can idendify people over the quarters and i want to observe an effect over time finding a job if certain criteria are met beforehand but now i have the problem that a lot of the cases i find are based on wrong answers in the questionnaire e g in january a person says they ve been unemployed fror 4 months and 3 months later they say they ve been unemployed for 9 months now i can identify the obviously wrong statements in my key variables but i can t simply exclude them can i excluding these people certainly influences the other people s weights so how would i handle this situation thank you in advance
2,dmd isn t as flexible as you think koopman operator discussion since my anti koopmanism post where i shared a document addressing many misconceptions in the field i have been talking to a lot of people about dynamic mode decompositions and koopman operators one thing that i found is that many people take an agnostic point of view to the underlying vector spaces that the koopman operator acts on there are a bunch of issues with this approach where a given function space not only governs which koopman operators are bounded but also function spaces themselves define what limits mean for instance a space of real analytic functions force koopman generators to be real analytic i go over this in a video i put out yesterday here but you can t just select any space and frustratingly l 2 spaces don t work the principle reason they aren t actually function spaces they are spaces of equivalence classes of functions that can differ on measure zero sets this means from the data driven standpoint that samples don t actually carry any meaning since they constitute sets of measure zero what is your take on this i d be curious to hear what the overall community thinks
2,paper study request are most of ml ds use cases easy we all know the data science pyramid of needs where deep learning sits on top of classical simple ml i ve heard a lot that most of ml ds use cases are easy also this fits my intuition for example i feel that the old resnet is the way to go in many computer vision tasks not vit or other sota close to sota architectures i ve talked with other ds ml consultants which think the same they often plug in some lasso ridge regression and they are done is there any data arxiv paper market study to back this up i know there is a problem with how we define easy and even with what we define as a project use case but i expect to be a study regarding this anyway do you think most of the ml ds use cases are easy or not
1,question university statistical genomics courses can anyone recommend any free open source courses offered by universities on statistical genomics i don t really like platforms such as edx because i do not feel that the resources are deep enough and everything is like an introduction i was looking for something hosted presented more like these courses cs229 machine learning stanford edu cs230 deep learning stanford edu
1,zaslavsky s theorem in statistics has anyone heard of zaslavsky s theorem on hyperplane arrangement supposedly it says that there are only a finite number of ways that hyperplanes can be arranged does anyone know why this is important apparently it has implications to decision boundaries of machine learning classifiers thanks
2,pseudo labeling on mnist dataset i couldn t find any good resources code examples on pseudo labeling on mnist dataset so ended up experimenting and figuring it out on my own this is the results of my experiments
2,cost effective solution to deploy transformers model i m working on building a saas application which utilizes a transformers model for inference i was wondering what is the most cost effective way to deploy this model on the gpu obviously i could pay for an aws ec2 instance that has a gpu but the per hourly costs and the fact that the service would be running 24 7 is daunting i was hoping for an aws lambda type solution but for the gpu this probably doesn t exist but just wanted to get ya ll opinion before going the ec2 route
2,an experimental machine learning package for easy and fast prototyping hi all igel is a fairly new machine learning package that allows you to create ml prototypes on the fly you can use igel from the terminal without writing any code or from python if you want to i tried to keep the api simple enough and flexible as possible recently igel supports serving trained models by exposing a rest server using fastapi and uvicorn for production use hence you can train evaluate test generate predictions serve and use your model in production i m pretty excited what users will think about the last release i wanted to share it with you all github repo
1,using r for hierarchical cluster analysis to segment products instead of customers for those familiar with hierarchical cluster analysis hca and segmentation on rstudio what are your thoughts on applying hca to segment products such as movies based on variables like their financial performance and characteristics sales figures genre ad budget etc or is hca only appropriate for segmenting customers based on their clear demographics and behaviours i m trying to work out what movies in a large dataset are more successful than others i m still new to statistics and i feel like hca could work but have only learned about it in the context of a customer based dataset idk if this helps but i can only really work with simple stats tools like regressions interactions and principal component analysis to analyse my dataset any help or insight appreciated
2,anti koopmanism and koopmanism is wrong hello everyone over the past couple of years i have been working on problems surrounding dynamic mode decomposition this is an intersection between time series analysis machine learning and mathematical operator theory i want to present a paper that my colleagues and i just posted to arxiv and also some fun youtube videos that go over the contents the theoretical understanding of researchers of this topic are really all over the place some of the work rests soundly in ergodic theory but then gets misconstrued when transported to other contexts truly like all data science this topic requires samples and a more appropriate theoretical underpinning is in sampling theory akin to hardy shannon and nyquist s work this paper is my group s take on the theoretical underpinnings of koopman analysis and we demonstrate where lots of people go wrong with a bunch of counter examples we conclude the paper with a cogent numerical routine while avoiding the shortcomings of other approaches our algorithm is very simple compared to what s out there and we never touch a feature vector this is really the best way to go about the problem we just posted this manuscript to arxiv and it is based on a lecture i presented here koopmanism is wrong the arxiv publication can be found here i have a series of youtube videos on this topic and they are a lot of fun the parable of koopman modes the dmd algorithm dmd quasiperiodicity and juggling
0,where to find good quality ds project examples hi all i was just wondering is there a good place to read some good quality actual data science projects end to end ideally i think it would be good to just have a place with a good set of jupyter notebooks of varying length and complexity that i could read to get a better understanding of the whole end to end process of a typical data science project i understand that that code can be found on kaggle and github but it feels like it is kind of hard work to be able to find them all organised in one place its hard to discern the good from the amateur and a lot of the time its just small little tricks and tutorials with not much descriptive text of what the problem is and what they are trying to carry out i m kind of looking for some gold standard data science projects i can read to get a better understanding of what is required and what to strive for maybe i am just doing it wrong but does any one have any suggestions
2,resnet 18 magnitude based pruning resnet 18 global unstrctured magnitude based and iterative pruning with cifar 10 dataset the pruning goes on till 99 08 sparsity this is based on the research papers 1 learning both weights and connections for efficient neural networks by song han et al 2 deep compression by song han et al 3 the lottery ticket hypothesis by frankle et al 4 what is the state of neural network pruning by blalock et al original and unpruned model has val accuracy 88 990 original model size 42 7 mb zipped model size 40 mb pruned model with sparsity 99 063 has val accuracy 91 260 pruned trained and zipped model size 3 5 mb this results into a compression ratio 11 43 you can refer to the code here note post pruning pytorch doesn t cast tensors to sparse format therefore the tensors are of the same dimensions as before but with 0s in it to denote pruned connections thoughts
1,case numbers are aggregated why isn t the population when it s reported to be 50 cases a day for a week 350 out of 1 4 million population why isn t the 1 4 million aggregated to it s 7 day total of 9 8 million population like the 50 cases a day are is it not algebraic disclaimer outside of deal making i am not a math guy thanks in advance
2,discussion does anyone have a use case specific for the airlines industry that can be used to learn ml examples 1 dynamic pricing for airfare 2 virtual travel assistant 3 recommendation engine for travel shopping 4 disruption management what else can be a good use case and where can we find training data
2,unfair comparison neural networks vs taylor polynomials and fourier series this is a question i have always had early on in machine learning history the perceptron algorithm was invented for classification tasks the perceptron being the predecessor to the multi layered perceptron i e the modern neural network later on the universal approximation theorem was developed that showed a simple neural network is able to well approximate any continuous function a few hundred years before that both taylor polynomials and fourier series were developed both of these are also able to approximate continuous functions as well i know this sounds like a stupid question but at what point did people abandon the idea of using taylor polynomials and fourier series for function approximation were there certain problems associated with using these methods for function approximation what lead to the decline of these methods thanks
1,post hoc testing for two way anova with many group levels in r how to do it i m currently working on a statistics project where i m performing a two way anova on a dataset with two categorical independent variables one continuous dependent variable the dependent variable is a relative growth rate measurement and the independent ones are a temperature condition 2 levels and a bacterial strain condition 49 levels i think you can see the complication here the assumptions are met and the analysis on a full linear model growth strain temperature strain temperature returns a significant interaction variable this is good news because it confirms our general expectations however things get messy when we want to do some post hoc testing to for instance assess whether growth is significantly different between temperatures for each bacterial strain condition i ve not been able to find a way to do this that doesn t bury me in a thousands of entries long pairwise tests table that compares every combination of strain and temperature with every other combination of these two does anyone know what methods might be useful to assess such data in r thanks in advance
0,has anyone had any experience selecting priors for bayesian models in real life i often see in textbooks and blogs when dealing with bayesian models popular choices of priors seem to be well known distributions e g the gaussian prior but how would you go about selecting priors for a specific model for example suppose you are interested in making some bayesian model e g for causal inference on some medical data one of your variables smoking status you have knowledge that patients who smoke vs patients who don t smoke have different life expectancies using historical data how would you choose a prior for this model thanks
2,best infrastructure options for large corpus training i want to do transfer learning on a news corpus of 1m news using bert multilingual model currently i have colab and kaggle tpu you know the limits of these more than me 😊 i am also planning to buy rtx 3070 is it a better choice than colab tpu what are the cloud alternatives for this sort of task
2,ibm releases uq360 ai tool an open source tool to measure model uncertainty deep learning based artificial intelligence ai systems have a history of generating overconfident predictions even when they are inaccurate which can have significant repercussions if a self driving car firmly misidentifies the side of a tractor as a brightly illuminated sky and refuses to brake or alert the human driver would you prefer to travel in that i doubt it self driving cars aren’t the only issue there is a slew of additional applications where ai’s ability to convey doubt is essential for example if a chatbot is uncertain when a pharmacy shuts and gives a false answer a patient may not receive the medicines they require here’s where ibm’s uncertainty quantification 360 uq360 comes in to rescue the day uq360 allows the ai to communicate its uncertainty making it more intellectually humble and increasing the safety of its deployment its goal is to provide data scientists and developers with cutting edge algorithms for quantifying analyzing enhancing and exposing the uncertainty of machine learning models article ibm blog
1,spss partial correlation how can i control for a variable while looking at the correlation between two variables one of which is categorical basically the iv has two categories and i’m looking at how they differ on the dv how to i run a partial correlation analysis with all of this i tried running a normal partial correlation but it doesn’t account for the two groups for the iv
0,what futuristic industries do you see data science playing an integral role in the next 50 years data science is continually changing and is unlikely to settle down and stay in one spot for a long time what industries sectors etc that only exist in concept or even ones that are just starting to emerge now do you see data science dominating
1,churn probability with ordinal logistic regression hello everyone x200b i am trying to create a questionnaire to evaluate satisfaction with which i would like to estimate the probability of the customer s churn i know the classic models like ols logit mnl probit etc since y is a probability i ll use the logit model the problem is there are so many questions 17 ordinals with 5 choices each 7 categorical 1 continuous and the sample will be very small 200 consumers maximum when i studied these models at university there were no problems with the sample size and i don t know what to do now should i reduce the number of questions on the questionnaire for example two questions like 1 was the package large enough 5 4 3 2 was the color of the packaging to your liking 5 4 3 i could replace them with 1 was the packaging to your liking yes no x200b or is there a way to aggregate the answers for each question category packaging shipping after sales etc perhaps taking the mode of each category x200b ps english is not my first language i hope my question is understandable x200b thanks so much
2,are all machine learning models based on the concept of similarity do all machine learning models indirectly work based on the concept of similarity e g two data points that are very close to each other share more similar properties with each other compared to some point that is further away is this one of the inductive biases
1,those of you “m s ed” out of a statistics ph d program or didn’t want to continue after m s how did it go after hello i’m in college right now and have been thinking about applying to phd statistics programs for a long time however ive been thinking about it often as to what would be a downside to just a m s in statistics vs a phd i get it a phd will open more doors etc etc but i’m now wondering for those of you who only got an m s in statistics instead of going onto get a ph d what made you decide that how easy was it to get a job after how was the job itself did you get to work on cool modeling do you wish you had gone further to get a phd my concern is that i’m afraid spending that much time doing a phd will not give me any more benefits with regards to the type of job vs if i had an m s and incorporate my previous intern experiences im still on the fence about a phd in stats so those of you who just got an m s i’d like to hear how things are going for you after
2,is knowledge about probabilistic graphical models a core competence in ml i have read a thread a while ago where the question was whether one should take deep learning as a course in university after taking a ml course lot of people said that it is better to focus on the foundations and to learn more about ml in general deep leanring isn t the universal remedy people are making it out be stick to the basics and learn them very well and if your job wants you to do something with dl than you can begin to learn it is the same true for pgms
1,steve kirsch covid 19 vaccine claims one of the worst misuses of statistics in recent memory hi i recently listened to a podcast from bret weinstein which features dr robert malone and steve kirsche kirsche has put together a paper claiming that he has evidence that research shows that in one study the vaccine has cause a miscarriage rate of 82 it is 3 on his key points the link leads to his paper where he has cited a study done in the new england journal of medicine and altered some of the findings the results of the study in the new england journal were among 3958 participants enrolled in the v safe pregnancy registry 827 had a completed pregnancy of which 115 13 9 resulted in a pregnancy loss and 712 86 1 resulted in a live birth mostly among participants with vaccination in the third trimester adverse neonatal outcomes included preterm birth in 9 4 and small size for gestational age in 3 2 no neonatal deaths were reported now he has taken that quote and claims the authors report a rate of spontaneous abortions 20 weeks sa of 12 5 104 abortions 827 completed pregnancies however this rate should be based on the number of women who were at risk of an sa due to vaccine receipt and should exclude the 700 women who were vaccinated in their third trimester 104 127 82 my background is in math not statistics however this seems very odd to me can someone please articulate what is going on here
0,offered managerial role for likely a big hit on qol i am currently a sr analyst doing research on business questions and support our ds team as a sme it s a fairly relaxed but unsatisfying job the pay is ok but enough we will have hybrid model when going back and the commute is only 10 minutes i went to my grad school networking event pitched myself to a company and got offered a managerial role my responsibility is building out data solutions such as dashboards analytics and ml models in addition to overseeing a small data process etl team asides from the increased workload this role requires 5 days in office and the commute is 40 min one way there s no plan to move to hybrid in near future i also have no experience in managing a team let alone doing that while building out data functions from scratch now the plus side is it s a 20 increase in total comp i have a lot of say in how the data analytics will run lastly the chance of managerial experience seems to be valuable and rare i m not too worried about delivering the technical aspect of work i have experience building out dashboard db and pipeline i have also delivered 1 ml model into production and currently delivering another one granted they are off the shelf models i ll essentially be repeating what i had been doing given that new company is in the same business i think the company is sincere in matching my ask and trusting me with a managerial role that said would you consider this a good move do you think there s a high chance of failure considering that i had no prior experience in managing and have to build a new function at the same time do you think it s worth the quality of life drop for advancement in career
0,should i feel bad for stepping on other people projects hi i m kinda new at the company 7 months since i ve started working i noticed that there are a lot of tools and programs that simply are poorly done and don t work pretty well the thing is i have tons of ideas on how to improve these tools or re made them and i also have the ability to do so but i m afraid of what the people that made them would feel like if i step on the programs that they ve worked so hard on how should i approach this should i just get used to how things work here
1,intuitive explanation for the shape of the normal distribution here is my attempt to explain where the distinctive bell shape comes from in a simple and intuitive way visual article why the normal distribution has a bell curve shape i would be interested to read your comments feedback ideas
1,in order to test for a structural break do my variables need to be correlated at all my dependent variables are several stocks and independent variables are several indices have tried different combinations but the r is quite low can i still test for a structural break
1,distributions for parametric ph and aft models in survival analysis when estimating fully parameterised proportional hazard ph or accelerated failure time aft models we select a distribution according to our assumptions made on the dependent variable and then model the covariates effect on either the hazard or the expected duration i get that some distributions for example the weibull can be used in both cases but others for example the log normal cannot be used in ph models why is that i suspect that it has to do with whether the relevant hazard function is monotonic or not is that correct my text book cameron trivedi 2005 doesn t really explain why
0,confidence intervals for classification models the idea of creating confidence intervals in regression models is quite straightforward for example but do confidence intervals carry over to classification models 1 for example in this picture here confidence interval for an roc curve corresponding to a classification model can the lower limit of the confidence interval from this roc curve be used to gauge how well this model might generalize to new data 2 for a classification model can we make a confidence interval for the prediction of an individual observation for instance i wrote some r code you can directly copy paste the code for a particular example where a random forest i e classification model is used to predict whether if an observation will be approved or rejected see here for the code thus for each observation the classification model predicts the probability that this observation will be approved or rejected whichever probability is higher the model selects that outcome for the given observation also the higher one of these probabilities are this means the classification model is more confident with its prediction e g for two observations the ratio of approved rejected 0 9 0 1 vs 0 6 0 4 the model is more confident about the first prediction thus is there anyway to apply the notion of confidence intervals to the probabilities for individual predictions thanks
2,what is the architecture of the quick96 deepfake model what is the architecture of the quick96 deepfake model
2,looking for a paper result about nn trained on language doing math i have a vague memory of a result where a nn was trained on language i can t recall whether it was for generation or classification etc that when prompted with mathematical expressions did surprizingly well does anyone have a link for this sort of paper article edit u interartigen correctly identified the result as section 3 9 1 from the gpt3 results
0,i’m doing an excel project for a job interview and i’m supposed to do a presentation that includes my “process and methodology ” my methodology is just making lots of pivot tables what do they want to see more info they gave me a list of a dozen questions to answer all of which were easily answered by cleaning up the data and then making pivot tables i submitted a report in excel and now i’ve been asked to present my findings they said my presentation should include my methodology and process i’m kind of stumped for how to include it in a powerpoint presentation thanks for any insight
2,paper explained fnet mixing tokens with fourier transforms full video analysis do we even need attention fnets completely drop the attention mechanism in favor of a simple fourier transform they perform almost as well as transformers while drastically reducing parameter count as well as compute and memory requirements this highlights that a good token mixing heuristic could be as valuable as a learned attention matrix x200b outline 0 00 intro overview 0 45 giving up on attention 5 00 fnet architecture 9 00 going deeper into the fourier transform 11 20 the importance of mixing 22 20 experimental results 33 00 conclusions comments x200b paper x200b addendum of course i completely forgot to discuss the connection between fourier transforms and convolutions and that this might be interpreted as convolutions with very large kernels
0,making the transition from small data to big data in the past all the work i have done has involved small data what i mean by that the data i have used to perform statistical modelling and machine learning models this data was carved out from a larger data source using sql and is able to fit into a very large excel document multiple spreadsheets from here i use r studio to import all these spreadsheets into r and concatenate the spreadsheets together i e stack them on top of each other so in the end the final dataset i am using for my analysis is about 30 columns and 500 000 rows from here the analysis can take some time but i usually put some coffee on and step away from the computer a family laptop from costco no gpu while the numbers crunch however i am starting to realize that this approach will become less efficient and eventually stop working for larger data sources i am aware that there is a whole world out there that is dedicated to dealing with and solving these problems a world that i am not very familiar with this world uses words like hadoop spark aws the cloud parallelizing apache containerize chunkize etc as i see it i feel that the problem can be viewed from different prespectives 1 performing machine learning and statistical analysis on large data might not be possible because of limitations in the individual computer you are using this means these procedures on this large data might not work on my computer but it might work on my friend s more powerful computer who lives down the street 2 performing machine learning and statistical analysis on large data might not be possible because of limitations in the software e g python r you are using i am not very knowledgeable about what happens behind the curtains of the software but my understanding is that software you are using regardless of how powerful your computer is might have certain limitations related to memory and ram 3 both 1 and 2 with this being said i want to start exploring different ways to address these limitations based on my limited understanding of these topics i think there are two main ways to address these limitations a with money apparently you or your company can buy cloud services such as aws which will allow you to perform machine learning statistical analysis on large data using remote servers i was told that this does not require a lot of knowledge or extra work after purchasing these services only a slight amount of extra code is required and then you can effectively perform machine learning algorithms on big data b with less money this is where my understanding stops apparently tools like hadoop can divide the computing costs between several computers and reduce the required time or there is something called chunkize which allows you to sequentially feed your data into the algorithm without maxing out your computer how can i learn more about this suppose i have the same dataset with 30 columns but this time there are 100 million rows i want to use the random forest algorithm for a binary classification task how have people on r datascience approached this kind of problem in the past thanks
0,what github template do you guys follow hi all i have to set up a github repo for an upcoming project and was researching some data science templates to follow i came across cookie cutter and this template by drivendata x200b it looks pretty comprehensive but i feel i might not need a lot of it like my data would be pulled straight from a db and not from a dump and neither it would be stored somewhere so the need for data folder is not there i would be developing a modelling pipeline and would not be saving the serialised model files so no need of model folder as well i think you guys know where i am going here so i just felt like i will get to know what the community is following thanks
1,mixed model plotting hi everyone i am running the following mixed model lmer score group time group time 1 sub data df score is a continuous variable group is control treatment and time is a pre post measurement time 1 time 2 subject is modeled as a random intercept i then plot this with the lsmeans package in r the problem i m having is that at time 1 the groups don t start from the same place so the effect of the intervention is somewhat difficult to interpret at the second time point in other words the baseline looks uncontrolled for even though i m controlling for baseline with subject as a random effect does anyone have any suggestions for this
0,why is php nowhere to be found when talking about data science i started intensively learning php a couple months back but still can t figure out why no one wants to use it for complex systems everything seems to line up it s decently fast scalable with a good framework very easy to mantain and develop in yet its popularity is steadily decreasing especially in complex areas like data science
0,are statistical models theoretically designed to make predictions about individuals are statistical models in theory able to make predictions about individuals suppose you have an individual with observed covariate information x a y b z c in theory can a regression model trained well on some data predict the expected value of this individual s response variable i heard today that statistical models are not designed to make predictions about individuals they are only designed to predict the average behavior of a large group of individuals and in theory should not be used to make predictions about individuals is this correct does this mean that any time statistical models are used to make individual predictions this is going against the intended use of statistical models if i understand correctly this means that when a statistical model makes a prediction about an individual with observed covariate information x a y b z c it s making a prediction for the behavior of all individuals in the universe with observed covariate information x a y b z c is this correct does this mean by definition the idea of making predictions for individuals is a fallacy thanks
1,brand new stats student doing into to stats need help hey everyone i have some data and i am told i have to rank it by ordinal rank i am confused as the prof is asking us to look for tied values the count row shows you which ranks are used up by the tied scores break any ties by assigning the average of the ranks involved to all the tied scores i am unsure what to based on that my two number sets are as follows 63 64 67 69 70 82 85 88 96 97 11 12 13 13 14 15 15 16 17 18 any help or advice on how to do this would be great my book isn t really helping
1,comparative analysis a quick aside stats was my worse class in grad school so the measurement i m imagining in my head may not really be a possibility but here goes i m working on a grant and i need a tool that measures food affordability the problem is not only is that a nebulous term but the agreed upon measures already in existence are widely disputed in public health in short i need a was to quantifiably prove that the price of an objectively nutritious thing a 1lb bag of apples is affordable and for the sake of continuity let s say affordable is a reasonable purchase for a family of 4 that is just above the federal poverty line current measurements include price per unit of mass price per portion and price per unit of energy calorie the problem of course is if you compare a bag of chips and bag of apples per calorie the chips seem like the better option price per mass fails because we don t consume foods in mass comparative samples i e few people eat the same mass of ice cream and carrots at dinner what i would like would be a measure that compares price per recommended serving size vs nutritious value but again such a thing may not yet exist
0,when did you know the data science was for you what made you realize that you wanted to pursue data science did you go to school for it or did you wind up the field by accident was there a certain class you took the sparked your interest
2,library for plotting image clusters based on distance affinity matrix are you familiar with the diagrams where image thumbnails are grouped in 2d automatically based on a distance affinity matrix do you know what they are called does a python library for plotting them already exist thank you
2,are there parallels between patch based image models and biological vision systems recently we have seen patch based image models perform very well on image recognition tasks first we saw this using attention based models like vit and more recently in work like mlp mixer in a way this seems very loosely similar to how our own brains and indeed the brains of many other species process visual information namely there is a high acuity structure in the eye fovea centralis which saccades from place to place the brain then processes this stream of saccades along with lower acuity peripheral vision to construct a 3d model of the world that is to say the brain looks to be stitching together visual patches using some presumably not fully understood mechanism perhaps this comparison has been made in either the papers eluded to above or elsewhere in the literature if so i ve sadly missed it i would be interested to hear if anyone thinks there is something to this comparison or not
2,google proposes efficient and modular implicit differentiation for optimization problems a research team from google research combines the benefits of implicit differentiation and autodiff and proposes a unified efficient and modular approach for implicit differentiation of optimization problems here is a quick read google proposes efficient and modular implicit differentiation for optimization problems the paper efficient and modular implicit differentiation is on arxiv
0,inferring values for one column based on other columns what is the best statistical approach hey i have a table which is 10000 rows and contains financials fields i want to infer values for field1 it looks like this id field1 year financialfield1 financialfield2 financialfield3 financialfield4 financialfield5 financialfield6 financialfield7 financialfield8 financialfield9 1 2 000 2018 0 000 37779 37779 000 23719 000 14060 14060 2 12 000 2018 1922468 000 3909002 4352188 000 769411 000 3582777 3266110 3 2018 0 000 12590 12590 000 551321 000 538731 538731 4 10 000 2018 0 000 0 1 000 0 000 1 1 5 2 000 2018 13887 000 26866 61139 000 59261 000 1878 1878 6 5 000 2018 8943 000 469020 614197 000 716975 000 102778 128778 7 84 000 2018 117720573 000 502937 000 565222 000 4772234 000 36220798 37446611 000 31198748 000 6247863 6247863 8 7 000 2018 0 000 248181 257646 000 284207 000 26561 33793 9 4 000 2018 468357 000 808098 809070 000 457911 000 351159 351159 10 25 000 2018 2899000 000 4695000 000 4683000 000 0 000 8590000 8590000 000 0 000 8590000 8590000 11 2018 0 000 46660 50955 000 18855 000 32100 32100 12 2018 0 000 0 0 000 0 000 0 0 13 1 000 2018 0 000 130736 132205 000 99254 000 32951 32951 14 12 000 2018 2106164 000 3325246 3437339 000 1257829 000 2179510 2179510 15 2 000 2018 0 000 20189 23254 000 52510 000 29256 29256 16 2018 0 000 615955 616522 000 104137 000 512385 512385 17 2 000 2018 2717 000 16251 45660 000 44965 000 695 695 18 4 000 2018 0 000 923824 1759978 000 1090139 000 669839 669839 19 2018 0 000 27341 27341 000 28324 000 983 983 20 3 000 2018 0 000 135932 270622 000 249758 000 20864 20864 21 2018 0 000 1588 1588 000 0 000 1588 1588 as you can see there is missing data in the first 3 financial fields but the availability of data in the rest of the fields is great field1 will often contain values but is often blank i am looking for some advice on how to infer values for field1 for rows 3 11 12 16 19 and 21 what is the best approach here i m not sure how to do it but i believe there must be a way to use the availability of data from the financial fields to apply a sound logic for field1 i m mainly looking for advice on what to read up on
0,imposter syndrome kicks in for next week interview data analyst i hope this is allowed here because the interview i am about to have is data analyst instead of data science x200b so i have been doing python self learn i even took ibm certificate on data science sadly enough i did not have much experience in doing it my only it experience is an web developer internship that i just recently ended x200b i am determined to give data science analyst a try now that i will be interviewed for one my imposter syndrome just kicked in and feels like i might not qualified for the job what are the good advice for the interview i have python and sql skill myself the company i applied for listed required around 1 2 years of analyst experience also needed work on azure this could be my very first data related job and i wish to overcome my imposter syndrome
0,data scientists who switched to data engineer how is it going i’m considering a switch to data engineering as i really enjoy the engineering side of things i’ve had to do as a data scientist as a data scientist what were you doing before how do you like it any regrets
1,are all statistical models based on the concept of similarity do all statistical models indirectly work based on the concept of similarity e g two data points that are very close to each other share more similar properties with each other compared to some point that is further away is this one of the inductive biases
1,guide classification and regression tree forest algorithm hi everyone i m just wrapping up a course i m taking this semester on classification and the guide algorithm i thought i would share some details about the guide algorithm developed by my professor wei yin loh over the past 30 years guide generalized unbiased interaction detection and estimation has many features that make it stand out among other classification and regression tree forest algorithms from the guide manual guide is the only classification and regression tree algorithm with all these features 1 unbiased variable selection with and without missing data 2 unbiased importance scoring and thresholding of predictor variables 3 automatic handling of missing values without requiring prior imputation 4 one or more missing value codes 5 missing value flag variables 6 periodic or cyclic variables such as angular direction hour of day day of week month of year and seasons 7 subgroup identification for differential treatment effects 8 linear splits and kernel and nearest neighbor node models for classification trees 9 weighted least squares least median of squares logistic quantile poisson and relative risk proportional hazards regression models 10 univariate multivariate censored and longitudinal response variables 11 pairwise interaction detection at each node 12 categorical variables for splitting only fitting only via 0 1 dummy variables or both in regression tree models 13 tree ensembles bagging and forests additionally some things that i have noticed while using guide are 1 very neat aesthetically pleasing tree diagrams of even very large trees in latex 2 comparatively short run times 3 variable importance scoring guide can be downloaded for free here
0,on the quality vs quantity of job applications from your experience is it worth tailoring resumes to job openings or is it a better idea to have a one size fits all resume and cast a wide net
0,data scientists what are few things that you wish you knew before starting your data science journey
0,questioning my qualifications for an analytics director position i have been interviewing for and have advanced far in the process for i have 8 years of experience in data science and was reached out to by a recruiter in regards to a director of marketing analytics position for a company that is not a no name company many of you have probably heard of them i have gone through the interview process and have advanced far in the process so i would imagine i am coming to the end of the line the problem is i keep reading the job description and am wondering how qualified i truly am for this role the last 5 years i have managed teams with my most recent role managing a team of 8 as a senior data science manager my team was laid off in december i consider myself to be very well versed in data science and everything that comes with it however the job description in terms of duties has a lot of data engineering lingo in it something i really don t have experience in everywhere i have been we have had a separate data engineering team that handles all of that which i work parallel with in all my roles i have been strictly data science modeling machine learning deep learning analytics and insights client and executive presentations and nothing to do with data warehousing and data pipelining but in the job requirements section i meet all of the qualification criteria i can send the exact job description qualifications and responsibilities in private message if anyone asks but i feel like my data engineering skills are lacking and you think the recruiters and those i have interviewed with so far would have been able to tell that but i keep advancing in my interviews i talk up my technical skills but most importantly i talk up my people skills as i have advanced in this profession the one thing i have learned is the higher you go the less important your techinical skills become and the more important your people skills become those i have spoke with so far are really glad to hear someone say that but i don t want to set myself up to fail should someone with my level of experience in this industry be expected to have data engineering experience as well i ve always viewed the two as two separate discplines and i believe a position marrying the two would be overkill i believe in specialization in that regard i believe in terms of pure data science and analytics i am a home run for this role but all of the data engineering mumbo jumbo in the job description is scaring the bejeezus out of me surely in my next interview this week i could ask if there is a separate data engineering team or if its something i would have to handle or am i worrying over nothing surely a company this size would have enough resources to split the two disciplines out
2,why do second order methods work for relus which have 0 second derivative a e my question is a simple one starting with the backprop chain rule since the gradient of the relu is zero almost everywhere how are there methods that are able to use second order methods with relus d 2 loss dw 2 d 2 loss da 2 d 2 a dy 2 d 2 y dw 2 0 since d 2 a dy 2 0 where y is the output of a matmul w is the weights and a y is the activation relu
0,what s your experience as the first data scientist in the company
2,🐝 cvpr buzz discover trending papers at cvpr 2021 website best viewed on desktop github mit license x200b demo with cvpr 2021 starting this week i scraped the interwebs to put together a quick site to help discover some of the papers that have been talked about and cited the most pretty excited with how it came out it s helped me skim through some new work that i hadn t previously seen let me know if you have any thoughts suggestions you can also missing data on the github repo
1,skewing results by filtering out any item with missing values taking entry level statistics here and working with spss for my statistics class i m doing a regression on certain stats on different countries however one of the 3 independent variables i m using just happens to have a good amount of missing values i decided to filter out every country with missing values but of the 198 countries i had only 66 remained will this skew the statistics terribly
2,image similarity challenge facebook ai x200b welcome to the image similarity challenge in this competition you will be building models that help detect whether a given query image is derived from any of the images in a large reference set content tracing is a crucial component on all social media platforms today used for such tasks as flagging misinformation and manipulative advertising preventing uploads of graphic violence and enforcing copyright protections but when dealing with the billions of new images generated every day on sites like facebook manual content moderation just doesn t scale they depend on algorithms to help automatically flag or remove bad content this competition allows you to test your skills in building a key part of that content tracing system and in so doing contribute to making social media more trustworthy and safe for the people who use it
1,a few basic questions about gaussian processes hi all so i ve been exploring gps these last few days after seeing a neat little interactive animation of them earlier this week see earlier question here if anyone is curious they re quite straightforward to work with so i was able to whip up a quick demo in r from scratch quite easily to e g visualize prior and posterior predictive distributions corresponding to arbitrary psd kernel functions i have a bit of prior experience here e g performing joint inference of ancestral states integrated over phylogenetic uncertainty in evo bio contexts or approximating truncated brownian bridges in biogeographic diffusions etc but stats isn t actually my field my degree is in anthropology so apologies for any flagrant abuse of language or misunderstanding below i have a few questions though as i play around with this stuff 1 it seems people often either fix the mean vector of their gp to 0 or else estimate for it a single scalar constant i e an intercept but it seems in many adjacent contexts e g phylogenetic regression or when exploring geospatial confounding etc what we d really want the gp for is to accommodate pseudoreplication in our data to perform inference of some linear model parameters to answer some substantive scientific question of association between variables this seems to me an entirely sensible procedure but would the resulting model be identifiable for example say i stuck a linear model featuring some sinusoidal function on the mean if i used a periodic kernel for my gp to model covariation in the residuals from that linear model wouldn t they sorta be capturing the same thing likewise what if i stuck a linear model w e g a log link on my dispersion term 2 with respect to answering substantive questions given e g posterior distributions of model parameters we might not necessarily care about the posterior predictive at all our goal being inference are there maximum likelihood or other estimators for these or can we exploit conjugacy to obtain their solutions in closed form or must we resort to numerical methods is the kernel trick still helpful here it seems like it d still be pretty intractable at even moderate n e g suppose we have a thousand observations evaluating the likelihood for each unique kernel matrix would require inverting a 1000x1000 covariance matrix which seems like it would make moving across the space of linear model parameters utterly hopeless though i guess if you re not varying kernel function parameters too often you could transform the data by its cholesky factor and explore the linear model parameters on that transformed space back transforming to evaluate parameter prior densities 3 similarly are there any conjugacy properties that we can exploit to obtain estimates for kernel function parameters in closed form one thing i noticed from the interactive visualization is that they re all fixed alongside the variance of the noise function but this is quite dissatisfying e g when i specify observations that are tightly clumped since if i d tossed e g an exponential and not a point mass prior on that delta noise down below the model will have learned the clumpiness i guess here i m thinking of that extra gaussian noise as the sum of your meaningful kernel and a a separate scaled identity kernel per one s ability to sum psd kernel functions indiscriminately according to the properties of multivariate normals alternatively in the case that we are just interested in the posterior predictive can we marginalize over those kernel function parameters in closed form 4 looking at wikipedia s list of common kernel functions i have a question about the one corresponding to the ou process in phylogenetics we d fit much more parameter rich ou models i e at its most basic form with parameters corresponding to phenotypic optima i e the locations of attractors and with returning force parameters corresponding to the strength of that attracting or returning force in the stochastic diff eq these would typically be denoted by mu and theta respectively and we d specify all sortsa hierarchical models with multiple optima and returning forces etc that could shift according to e g changes in a ecological regime as determined by some markov process throughout the tree the kernel function wiki gives for the ou process exp x x l clearly lacks these parameters though maybe the lengthscale corresponds to the returning force can you help me map the ou process as i understand it to the one described by that kernel function e g does it assume the process starts at stationarity what if i wanted to extend it to have a stronger returning force or shifts in the location of the optimum throughout my input domain apologies for the length and thank you for any help
2,semantic scholar introduces semantic reader an ai powered augmented scientific reading application while everybody knows the the pain of reading pdf it will unlikely to go away anytime soon due to its portability i m quite excited with the new reader it allows you to click on a citation to see the reference click on math symbol to see their definition and usages and many more features in the future
1,question indicating the difference between missing and censored data so i m working with a dataset let s say i have a variable time to fever clearance or tfc there are missingness markers for censored stuff like 7 discharged before clearance or 8 died before clearance and then there is blank for actual missing i m going to be doing regression on this data so i can t really keep values of 8 and such so how do i indicate the difference between actual missing vs censored without keeping values like 8 in my dataset should i just make them all as missing should i learn to work around the negative indicators or should i make a new variable just for indicators and mark the rest as missing in the original variable how would i take that into account in my regression thank you in advance for any help also i m sorry if this is a dumb question maybe i m overthinking it
2,reviewers at top conferences i have been submitting to top medical imaging conferences miccai and isbi i feel like half of the reviewers either didn’t thoroughly read the paper or they were just not familiar with the topics or the conference requirements i heard reviewers at other conferences are pretty noisy as well so i’m wondering how we can improve the quality of the reviews i’m gonna provide a few examples from my personal experience 1 a paper of mine was accepted as isbi and there were 2 reviewers one of them thought my methods could do a lot of things that it was not capable of i feel like they didn’t understand my method and just thought my results looked cool since it’s pretty easy to see the limitations of my algorithm the other reviewer obviously didn’t read my paper carefully either i explicitly explained that the only thing i had in common with a competing previous work was that we both used a graph representation but the actual methods were completely different i literally spent most of the introduction explaining this then that reviewer claimed my method was not much different from the previous one 2 this paper were rejected at miccai while the reviewers’ concern were valid and they wanted to see more experiments it was only a 8 page single column paper there was no way i could fit all the ablation studies in there i have already included 3 experiments that were both quantitative and qualitative also a reviewer had a really weird comment regarding this paper i was using a hierarchical conditional vae and the reviewer said the difference between my method and the original vae was not clear 3 i’m not sure if this is a misunderstanding but a reviewer straight up said my results were suspicious because the numbers kind of counter intuitive at first glance i might need to analyze the results more but i thought the claim was kind of rude
0,alternatives to cross validation hey guys i am currently working on my thesis in this thesis i work with time series data and use a model based on fused regression estimated on a rolling window unfortunately i do not habe enough obervations to utilize cross validation or even any kind of validation if someone here could point me on something i could use instead to choose my hyperparameters i would be incredibly thankful thanks in advance and have a nice day edit to clarify a little the task is basically to evaluate penalized regression models against a standard factor model essentially linear regression in hedge fund replication at every point t i estimate a model based on the data in the rolling window abd use the coefficients as portfolio weights in t 1
1,one way anova topic ideas that use secondary data i m doing a statistical activity at the moment and i am having a hard time looking for data that can be used for one way anova test any suggestions thanks
1,risk theory how to find the distribution of the insurer s claim payment per loss event via cdfs so the payment for this insurer is that he pays for anything above d and he also has a reinsurer that covers him for anything above m so his payment per loss event is denoted as such 0 x d x x d d x m m d x m
1,question please help me find the name of this distribution i have been pondering about the name of this specific distribution d if it has a name d is a distribution such that the numbers that are sampled from it are binary and are comprised of n bits such that each bit of a sampled number is actually an independent identically distributed bernoulli random variable with success probability p in other words d outputs the result of n i i d bernoulli trials with success probability p in n bits note i know that d would be binomial if i were counting the number of success only however this setting involves knowing which trials are successful and which failed so i believe i am looking for something else thanks in advance it has been really bugging me
0,can dashboard software tableau powerbi help with this business case a client produces 6 12 spreadsheets each quarter they have an excel guru put together a big document that visualizes the data the document contains things like bar charts and descriptive statistics i would like to prepare a dashboard application that is hosted online or could be shared as a standalone application the application should be able to accept the data sets as input ideally with a drag and drop graphical interface combine the data sets behind the scenes and produce the necessary data viz goal is to automate the data viz process and the mechanics should be straightforward since the data sets have the same structure from year to year challenges • the data sets are company sensitive i cannot host a web application on any old web server i need some contractual guarantee the data isn’t being spied on my understanding is that most companies have freedom to intercept online info maybe then i don’t host the application online at all an html file could work • client has restrictions on what kind of software they can install i may be able to install powerbi or tableau on my system but client may not be able i am aware that tableau can easily visualize data from multiple sources i e it would be simple for me to organize the spreadsheets visualize key metrics and host it online what i would like is a freestanding application where the client drags and drops the spreadsheets and an application spits out some visuals thoughts is this too advanced for powerbi tableau do i need shiny dash
1,measuring the effect of a new emergency dispatch system hey guys i m in my master s program in emergency management in germany our local 911 service tries a new way to dispatch it s emergency units fire department and ambulances the system tries to reduce the response time for emergencies in non urban environment by taking the callers position and forwarding it to the units by doing so the should be able to arrive on scene earlier than previously where they didn t have the exact callers position and literally had to hope to finde the scene in a reasonable time my task is to prove if this system significantly reduces the time it takes the units to arrive or not i am able to use a data set of several thousand emergencies during the test period and like 60 000 emergencies are available for each previous year for every emergency there is a huge set of metadata available like time stamps position etc given that the data might show the average time of arrival went down from an average of 10 minutes in the previous year to 9 50 for the test period i m not sure if this can be linked to the new system or is part of the usual variance this is because the dependent variable in question the driving time is depending on many more factors first it s dependent on the distance and average speed of the units both are probably affected by the new method but not only there are many many more factors to both these variables like a different distance for each and every emergency different weather conditions different urgencies my question is how can i show which role the new dispatch system plays do i need to do an analysis of variance or regression or a totally different approach as you can see my statistical knowledge is not to big but i really like this question so i am motivated to put some work in it and learn a lot thank you in advance
2,video the basics of spatio temporal graph neural networks i m a phd student studying machine learning and applications in transportation systems and autonomous systems think rl and robotics while there are several gcn made easy videos out there on youtube i feel like these videos often miss the forest for the trees especially since gcn is just 1 algorithm that was developed in 2016 and videos often don t cover the broader historical context of how gnns were developed and don t cover how different variations of these models allow them to model new types of systems this is the third video in a series i m making about graphs graph neural networks and the application areas where they have the potential to make big impacts please let me know what you think of the video and if you learned anything new from it
0,are there any python packages that can work with big data i ve got a relatively big dataset 8 gb and pandas crashes when trying to load it into a dataframe i ve tried modin and pyspark all with no luck are there any python packages that can work with big data currently the data is stored in sql i m running this on a company vm which has 16gb ram i believe
1,how would you go about to prove analyze a model with a quadratic effect on the dependent variable i will try not to go into too much detail i have data on the number of times someone is late at a particular event the dependent variable in relation with their success age and a number of other personal characteristics i want to predict the number of times someone is late based on these characteristics so i have a model with a count variable as dependent variable which has a poisson distribution i expect and can observe in the univariate analysis that the success of a person at first is negatively related to the number of times he is late the less success someone has the more this person is late however for very persistent latecomers high values of the dependent variable this relation becomes positive i e the more success the person has the more this person is late in other words i get a parabola where the axis of symmetry is horizontal a relation with a quadratic effect on the y variable the problem off course is that this is not a function because for these kinds of relationships you have multiple x values for a given y value hence my question how would i best go about to model this relationship and statistically prove show the non linear effect of success on being late without disregarding the other control variables i now use a multinomial logistic regression were every value of my dependent variable is a category so i can show that the sign of the relationship between number of times late and success is reversed at a given value of number of times late i e the top of the horizontal parabola thanks
2,how to decouple classifier output from associated risk hi all i have the following problem and i hope you can help me i have a classifier that needs to be retrained every once in a while as the model take action on a stream of payments every time we perform a retrain we need to recalibrate all the thresholds of the business logic i would like to decouple the classifier output score from the risk associated in order to provide always the same risk associated with a score and avoiding the need of recalibrating every time the only thing that comes to my mind is to stacking my model with k means and fixing once for all the number k in order to create buckets of risk i would like to know what is the proper way to solve this problem in your experience opinion
2,similarity score of a document and keywords i have a list of documents which are online articles and i would like to test their similarity against a list of keywords the trick is that the documents were scraped from the internet using those keywords so more often than not they have some sort of similarity the problem is sometimes articles are not exactly what you re looking for e g an advertisement that had one of the keywords or a completely different topic article but it unfortunately had the same keyword i have tried huggingface s zero shot classification to try to see which keywords the model classifies that document as but this gave me 2 problems 1 the model was taking very long to classify because the labels are a lot 2 i realized this still doesn t help me with the underlying problem of trying to find which document is more likely to be relevant to those keywords any guidance or help would be appreciated thank you
1,undergraduate course load advice hi everyone i’m currently an undergraduate seeking for some advice on picking courses for my upcoming term i’m not sure where else to ask about this so i apologize if this is not very relevant the course load in my upcoming term which consists of 5 courses will be very heavy i will be taking abstract measure theory functional analysis continuous stochastic process and measure theoretic probability and for my fifth course i’m debating between advanced regression and analytic number theory as someone who is interested in aiming for top stats phd programs in the future and doing research in probability theory do you think i should take more math or more statistics the most common advice that i’ve heard is that stats phd programs prefer pure math major and that stats courses for undergrad are not very beneficial so this is the reason why i’m asking thank you in advance
0,how do i deal with ml models taking soooo long to train when i have to optimize results noob data scientist here this is a bit of a general question but i ll ask it nonetheless i ve lately been involved in two projects at a company that wasn t very interested in the quality of my development approach and code just the results and i think that i have not really learned best practices regarding model training for example at the last project i regularly changed the architecture of a neural network and ran the usual tests and went to do something else using jupyter notebooks also probably didn t help since i always had to come back to run another test manually however there was no real method to how i did it and it sometimes just felt like a stroke of luck when i found better results apart from feeling like a huge waste of time when the model underperformed so i was thinking if you could point out some things that may help me to structure my code and model development
1,what does the following say about the 2 variables say you have 2 variables a and b when plotted in a lin lin plot they have a positive correlation correlation coefficient 0 5 but when plotted in a log log plot they have a stronger positive correlation correlation coefficient 0 7
1,seeking intuitive books or papers or resources to learn the backend of the widely statistical tools in research with this spss or any other software programming that research people use to analyze a sample data i don t understand how the final number reveals the relation what does it how does it work i know we have these wiki pages but the notations are so tedious is there any intuitive guide or books or papers that can you know get the heart of the tools out for a better understanding
0,asset retirement year optimization problem hello all i have been assigned to consult a traditional mom and pops logistic company in deciding at what year they should retire their trucks currently they are running them down until they cost more to repair than to sell i have data on their current demand for asset types and total maintenance cost of assets and obviously the number of assets on hand i was wondering if there s hope looking to build a model using optimization with this limited set of parameters i know in terms of cost we could potentially look at their revenue per asset and do a minimization on cost but they don t have that kind of information available and i m only dealing with demand and maintenance cost any advice would be highly appreciated
1,what do we know mathematically about statistical analysis got kinda burned by my college recently long story short all statistics courses available are applied statistics and they don t have a statistics course for mathematics students we just have to pick from the engineering ones we don t learn any math no analysis no proofs no theorems no abstraction nothing the phrase memorizing formulas appears in the one paragraph course description and memorize this appears all over the lecture notes in their defense they don t rely on memorization for the applications they really do want us to understand experiments and how to come to a meaningful conclusion based on the numbers that pop out of formulas i asked my academic advisor about this and they said they would mention to the board that they might consider a class for mathematics students does that mean it didn t even occur to them someone might be interested in how statistics works beyond how you use it surely leonhard euler scientists didn t just figure out all this stuff a long time ago and then forget about it forever right i almost feel a little frustrated because i don t know where to go from here i have to have a statistics course but part of the reason i became a math major in the first place is because in general i find myself much more interested in how mathematics works and less interested in how we use mathematics
0,mentorship or a discord to ask questions i’m a 2 year decision scientist and i’m on my own i went from knowing little excel let alone python or sql to writing code all the time but there are still things i don’t know i was wondering if there was someone who wouldn’t mind being friends on discord or something where i can ask questions when they pop in my head for the most part i think google answers the base line of my questions but it would be nice to be able to tailor my question based on my role an example is something like it’s me my laptop and the company’s sql server should i get hadoop should i run pyspark for these pesky longer operations when i don’t have multiple machines to run operations on how can i grab weather data for all us zip codes and store it as it’s over 25 50 gb of data being the sole guy for these kinds of developments is challenging but anything to make my life easier would be a blessing
0,what is the best package for combined speech recognition and diarization on long conversation audio files i have maybe 1000 hours of audio recordings i want to convert to text with timestamps to match diarization timestamps or at the minimum at least convert to text without diarization the files are a few hours each and add up to maybe 200 5hr sessions quality isn t always great but a human can clearly understand what is being said approaches i have tried mozilla freespeech convoluted installed no diarization kaldi also somewhat convoluted install could revisit speechbrain with huggingface pretrained got working but attention model may need 30 second or less inputs worried about splitting 6 hour session into 30 seconds and the information loss
1,estimating a transition matrix with irregular time points i have a dataset which has variables id state nr between 1 and 9 say date and nr days the number of days since the first observation for each id the observations were sampled at irregular time intervals i would like to estimate the probability of going from one state to any other state can i estimate the transition matrix by counting the number of transitions between each pair of states and then normalizing these counts i e can i just calculate m ij nr transitions from state i to state j nr of transitions from state i to estimate the proportion of people going from say state 1 to state 7 or would this be a bad idea
1,question canasta odds in a game of canasta there is a deck of 108 cards 2 card decks 4 jokers when you starting the game you deal 13 cards to each player there are 4 red threes in this deck 2 decks of standard cards what are the chances that you get one red three in your starting hand
1,choosing 9 squares in a row from a 5x5 grid of squares without choosing the square with a bomb in it what is the probability of this this is basically the game mines from stake casino dont gamble and i was wondering what the probability of getting 9 squares in a row is vs the probability of choosing the one bomb 1 25 i m not sure how to approach this i think it has something to do with combinatorics and the binomial coefficient formula but i m not sure how to apply it anyway let me rephrase it if it isn t clear there is a grid with 25 squares only one of them has a bomb the rest are gems you need to choose 9 gems in a row without choosing the square that has the bomb what is the probability of this
0,data science job postings asking for both python and r i m seeing this quite a lot is this normal or are they just throwing in some buzz words they ve seen for data science skills i ve always been under the impression combing both is largely unnecessary and their use depends on the business or the individual data scientist s preference
1,what is the best approach to analyzing how normal patient visit counts are i m currently looking at patient visits and no show rates for my clinic and i m wondering what would be the best way to show if the current month quarter patient count is within a normal range i usually just report the descriptive statistics and show the counts by month along with an overall average and then i also show what the last 5 months of patient counts looked like to see how the data trends i m wondering if there is a test out there or a better way of showing if the current report data is within a normal range and if not to flag it for attention in my report only if it s outside of a normal range because right now it s nice getting raw counts but we don t know if we should be concerned or not would it be better to show this data in comparison to the previous year s data or is there a statistical test out there that shows this better it will be mainly used to facilitate a discussion on whether the patient count drops rises requires a discussion from our leaders
0,should i accept the fang offer hi guys i have 4 yoes and am in a data role right now current co has lots of challenging projects and i enjoy the technical work that i’m doing recently got a fang offer tc lift is more than 60 scope of this role is 50 50 tech and stakeholder management i am pulled towards this offer solely because of the brand and the tc lift not too hyped about the role due to the reduction in technical scope but i also do recognise the need to have more experience in stakeholder management in order for me to grow what do you guys think to go or not to go super torn
1,stochastic processes vs statistical inference i m considering pursuing a major in statistics and so i need to complete either a course on stochastic processes or statistical inference using casella and berger is either one of these courses obviously more useful than the other if my goal is to become a machine learning engineer or data scientist the stochastic processes course uses the book introduction to probability models by sheldon ross would one of these books be much more difficult time consuming than the other
2,resnet50 implementation for face detection does anyone know any good repositories that have implemented resnet for face detection or face recognition i want to implement this on my own dataset
1,how to analyze a survey question where respondents rank 5 out of 10 i am currently trying to figure out how to evaluate a question from a survey i conducted for my thesis but i am unsure of how to do it and i am grateful for every hint disclaimer i am not very good with statistics so far i am still learning the respondents were presented with a selection of up to 12 value propositions and were asked to select their top 5 and rank them some respondents picked and ranked even more the problem i encountered is the following i tried to just calculate the weighted average but in some instances a value proposition has been picked only once but as no 1 and now it looks like this value proposition is the most important when in fact there are other that have been picked multiple times for instance 3 times as no 1 twice as no 3 and once as no 5 etc therefore the average appears to be lower when actually this value proposition should be more important how do i best find out which value proposition is the most important now i hope it was somewhat comprehensible what i tried to explain i m forever grateful for every possible solution on how to tackle this issue lola x200b edit thank you guys so much for all the suggestions i am slightly overwhelmed but working my way through all of them and figuring out what would be the best approach for my case 3
0,wrapping up a data intensive phd but most industry data science seems really boring are there interesting jobs title basically says it all i m wrapping up a phd in computational biology field and starting to think about what s next for me i don t really want to stay in academia at this point the odds of getting the fabled tenure track jobs are low and i m pushing 30 so i haven less interest in bouncing around post doc to post doc until getting a tt or burning out a lot of my friends who graduated before me went the data science route they re making good money much better then we made as graduate students or would make as tenure track profs but the work just seems so boring instead of wrangling with interesting data types and trying to solve interesting problems a lot of it seems to be basically financial or behavioral user data and the goal is to deliver actionable business insights which always seems to boil down to optimizing profit to cost ratio far less of the interesting questions about mathematics and inference that pulled me into computational modeling and a lot more focus on business learning how to pitch ideas to managers etc i don t give a d mn about that and kind of chafe at the idea of using skills i spent 6 years developing at the cutting edge of scientific research to help make already wealthy investors in a company richer for context my thesis research involves developing a very niche kind of computational model to explore distributed information processing in biological systems that i know has absolutely no relevance to anything in the world of business or finance
2,how to improve image inversion with gaussianized latent spaces explained improving inversion and generation diversity in stylegan using a gaussianized latent space 🎯 at a glance in this paper about improving latent space inversion for a pretrained stylegan2 generator the authors propose to model the output of the mapping network as a gaussian which can be expressed as a mean and a covariance matrix this prior is used to regularize images that are projected into latent space via optimization which makes the inverted images lie in well conditioned regions of the generator s latent space and allows for smoother interpolations and better editing samples from the model 5 minute summary of main ideas arxiv p s thanks for reading if you found this useful check out other popular ml papers explained on my channel links to other recent papers explained vq vae2 stylegan2 ada mlp mixer
1,discussion predicting euro 2020 matches with bayesian statistics see my repo here basically andrew gelman has a little world cup case study for stan when a coworker approached me to fill in some predictions for the euro group stage i thought i should use that model i collected qualifying match data along with euafa rankings for each team and built a bayesian model to predict the outcomes of each game the model does well for now using prediction argmax p x i ve succesfully predicted 5 7 games i predicted denmark to beat finland but a tragic event likely affected the players on that team and so i chalk that model error up to unpredictability with numbers i m averaging a log loss of about 0 8 random guessing of team a wins v team b wins v draw would result in average log loss of 1 1 when i fit the model to euro 2016 data it has an auc to predict the winner of approx 0 72 this may not be impressive to a seasoned football fan but for a guy who passively follows international matches its been a huge improvement lots of time for me to screw it up i write up some thoughts about the model and some model checking in the linked repo would love to hear what other people have to say
1,how do i create this regression output in rstudio
1,what tips do you have for successfully making the most of graduate school in statistics
2,have we abandoned kernels hello everyone i have a question i was hoping that the community would be able to help me out with my own research is almost completely about kernel functions i did my phd in pure mathematics where i studied densely defined operators over a variety of classical kernel spaces and even made some of my own hello polylogarithmic hardy space after i graduated i have been working in approximation theory and numerical analysis with engineers and recently came back to operator theory through the study of koopman operators and dynamic mode decompositions reading some textbooks by big guys in the field i notice that steve brunton for instance makes almost no mention of kernels in his textbook data driven science and engineering and through my conversations with engineers over the years there might be some nod to the gaussian rbf but then it s all about deep learning i have always been able to find new and interesting perspectives on kernel functions for learning theory and a lot of these innovations are really just twists on ideas from 40 or 50 years ago thanks to the great wabha i feel that there is still a lot more life in that subject however as far as i can tell most of my colleagues are of the opinion that kernels are something that were concocted to do some esoteric classification methods with svms and to perform inner products in feature spaces and are otherwise unaware that kernel spaces were central to things like shannon s theorem and many other classical topics have we abandoned kernel functions for deep learning is there a good reason why people don t use kernels that i m just missing i d be interested in hearing everyone s perspective
0,data project you’ve never worked on before if someone at work approaches you to consult on a type of analytical project that you’ve never worked on before what’s the best way to respond a sorry i haven’t done this before and can’t really help you b while i’m not familiar with this kind of analysis i’d be glad to take a look and provide my input c send it my way and i’ll make magic happen d
2,project dvc studio – git based ml experiments management hey everyone our team is working on open source tools for data scientists and these two products help ml teams track ml experiments and run training in the cloud using git gitops approach today we are launching dvc studio user interface for dvc and cml it is an ml experiment tracking and cloud training platform but build with git and gitops and devops principles in mind 1 visualizing dashboard of ml experiments 2 graphs for your ml training 3 manages connections to your clouds data is not stored in git but cloud storages 4 modify hyperparameters in ui run ml experiments in clouds or kubernetes all of this through git gitops paradigm and with connection to gitlab github and bitbucket an intro video how it s connected to git looking forward to your feedback
2,jürgen schmidhuber swiss ai lab team boost linear transformers with recurrent fast weight programmers a research team from the swiss ai lab idsia leverages fast weight programmers fwps to advance linear transformers and explores the connection between linearised transformers and outer product based fwps to release the power of improved fwps here is a quick read jürgen schmidhuber swiss ai lab team boost linear transformers with recurrent fast weight programmers the paper going beyond linear transformers with recurrent fast weight programmers is on arxiv
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
1,can we use dissimilarity measure in cluster analysis instead of distance measure hi i’d like to ask that weather we could use dissimilarity measure in any cluster analysis algorithm e g k mean hierarchal or k median etc instead of distance measure as well if the distance measure do not provide the true difference between the objects if not then why not thanks
0,is there diversity in machine learning engineer backgrounds like there is with ds i’m a statistics student in school and my goal is to become a “data scientist” in quotations because that can mean many different things anyways i’ve heard that data scientists tend to have very diverse backgrounds depending on the industry ie statistics computer science math civil engineering or some other engineering physics ie list goes on and on even saw someone who was a geospatial data scientist with a background in something gis related point is it seems like the “data scientist” has very diverse backgrounds for the role however for something like “machine learning engineer” is this as diverse i mean yea there is some machine learning involved so some statistics but 90 i’ve heard is swe related so most of the backgrounds are generally computer science am i right in saying that for people who are non computer science is there a higher barrier to entry to become a mle than it is to be a data scientist can non cs backgrounds still be considered for mle positions
2,discussion research fault detection and diagnosis for telemetry data in near real time i m concerned about an industrial iot scenario where the various telemetry data temperature humidity and etc of machinery will be logged and sent near real time to analytics layer which will then process on detecting any fault that can happen in near future here i m worried about that do we need to implement ml model to predict the future telemetry data values first and then based on the predicted values do we have to pass that values to a fdd fault detection and diagnosis model to prevent any fault that can happen in future within the predicted time period
1,experimental design for simulations hi all i m looking for resources about designing experiments for simulated data as background i m a mecheng bioeng grad student doing fluid simulations and i m planning on assembling a dataset to investigate the relation between a geometric measure of a set of related fluid domains eg diameter of an orifice in a set of varying irregular pipes and a measure of the resulting simulated flow eg shear stress on each pipe s wall i also want to run each domain using multiple flow rates where a mean and standard deviation flow rate is known and it is known that flow rate and wall shear stress will be correlated i don t have much experience in experimental design and want to know if there s a smarter way than brute force running each domain at say 5 different flow rates centred around the mean or if there are any significant downsides to creating the dataset this way i assume that randomly sampling the inlet flow rates from our known distribution would help reduce confounding factors but i just don t know enough and would like to find some canonical sources for this type of experiment for my purposes my supervisor is fine running each domain at 5 flow rates but he s also more of a fluids numerical methods expert than an experimentalist i know i m in the realm of uq so any advice from that perspective would be greatly valued as well
2,complex valued neural networks so what do you think about complex valued neural networks can it be a new interesting field to look at mostly for the signal processing or physics community
1,help help i’m doing a project for statistics regarding comparing two sample proportions what are some easy examples for some statistic projects to do there needs to be an excel from which the data has to come from it can be a confidence interval or hypothesis test i’m sorry if my english is bad it isn’t my native language
1,hedging against criticism and misunderstandings when presenting statistical work to non technical or ill informed audiences so i just had a work related thought for technical analysts it is quite often that the work is presented towards an audience that has a non technical background or even worse an audience with some technical knowledge but still has misunderstood essential ideas and concepts in such settings it could be quite challenging to present and defend the work you have done if you do not have the respect of the audience take this hypothetical scenario as an example a statistician or economist presents a forecast from a model where the forecast is later shown to be off the realized target to some degree from a statistical perspective it is natural that a prediction will be off given that it is unreasonable to expect a model to be entirely deterministic due to the stochastic nature of the problem however a non technical audience might interpret such a scenario as the forecast being bad due to a flawed model and might then assume you do not know what you are doing how would you position yourself to avoid this in a practical context i think it might be reasonable to present prediction intervals and a benchmark that the audience can understand to hedge against this problem then you could intuitively present uncertainty while showing that the model outperforms something that the audience might be thinking of using i am curious if anyone else has given this scenario a thought or possible have relevant experience
1,what are the odds probably simple question for this sub i m standing in line for my covid 19 vaccine it s my birthday today both the guy behind me and the guy in front have birthdays today what are the odds curious about this also if possible would you include the equation used to figure it out many thanks in advance
1,help multivariable linear regression help multivariable linear regression i have very limited stats knowledge but i wasn’t able to out source to a statistician for my research project i have a pretty small sample size of patients n 159 it is definitely not powered and basically i’m looking at a lot of relationships in a variety of ways i’m wondering if i can present my data in terms of just the r 2 values as far as what the best overall relationships are or if i really need to get into the b coefficient and p values ci etc can i just call it “descriptive statistics” since it’s not powered anyway and avoid all that or maybe if i use the r2 values to determine the best relationship maybe i can go into that one in further detail
2,distilling the knowledge from normalizing flows paper icml workshop innf 2021 code conditional normalizing flows demonstrate competitive performance in several vision and speech synthesis tasks in contrast to other generative models normalizing flows are latent variable models with tractable marginal likelihoods and stable training however these benefits usually come at the cost of inefficient model architectures compared to feed forward alternatives e g vaes gans this work allows us to significantly speed up the inference of normalizing flows by transferring knowledge from flow based models to efficient feed forward architectures overall scheme of the proposed knowledge distillation approach
1,will p values vary more from study to study when the effect is small rather than large just want the title says if you run ten studies with n 50 will the p values vary more if examining an effect with cohen s d of 1 vs 5
2,fortran and neural networks i just started to work in a company that do predictions for chemistry and they use fortran my first comment was i didn t know that fortran has nn implementation and they answer that the algorithms was implemented from scratch they are consider to migrate to another language but the head programmer believe in fortran because it is used by scientists physic and chemistry what do you think are they insane or i need to be open mind about this
0,gourdian free dataset download project sunroof solar electricity generation potential by census tract postal code hi there we ve just added a new dataset to gourdian this one courtesy of google s project sunroof this dataset essentially describes the rooftop solar potential for different regions based on google s analysis of google maps data to find rooftops where solar would work and aggregate those into region wide statistics it comes in a couple of aggregation flavors by census tract where the region name is the census tract id and by postal code where the name is the postal code each also contains latitude longitude bounding boxes and averages so that you can download based on that and you should be able to do custom larger aggregations using those if you d like this dataset seems like it d be interesting to cross reference with things like weather and perhaps electricity prices to find the best places for people to invest in rooftop solar if you have any other ideas of what it d be good to combine with let us know and we can try to prioritize ingesting those
2,xlnet annotated paper paper summary although bert became really popular after its release it did have some limitations and there were certain limitations associated with autoregressive methods like elmo and gpt as well xlnet was introduced to get the best of both worlds while at the same time not include their weaknesses in continuation of my paper notes series i have written an informative summary of the paper personally reading the xlnet paper was a very fun experience i was amazed at every step how they were including stuff to make the whole model work so well the paper contained many interesting concepts that i had to give time to understand so don t worry if you don t get it on the first go check out the links below and happy reading paper summary xlnet generalized autoregressive pretraining for language understanding annotated paper
0,advice on types of jobs i m currently looking around for a different data science job and i m not exactly sure what i m looking for but i could roughly classify the types of jobs i m hearing about into a few broad categories i ll list them and give my current thought but i was hoping you could all weigh in with your experience and help me make the right decision x200b 1 the big tech company these are your faang m s i assume that they are the ones doing really cutting edge stuff and i imagine the work is pretty fun i also assume they pay really well i m not sure if i can get past one of those leetcode interviews though this one is probably out of reach for me but i imagine i d take it if i had the chance 2 the small tech company we just got xxxx million in funding and just landed a contract with big company that you ve heard of yeah idk this is the one that i m most wary about i think fast paced and great for self starters are just code language for shitty work life balance and poorly organized but of course i could be wrong 3 the big retailer non technical company that has a whizzbang data science team this sounds kinda fun but then other times i wonder if it s a little mundane anyone actually doing advanced statistical models and ml or are we just a b testing all day long 4 the huge non technical company that doesn t yet have a data science team yeah we think we could find a lot of business value if we brought on a data scientist this is the one i m most conflicted about some of the most fun i ve ever had with data science is when i m working with people who have absolutely no tech background and couldn t even imagine what was possible i think in the right situation you could really be hot shit around there the thing i m most worried about is that you d soon run out of things to do with some of these places the work they describe sounds more like a 3 month project than a career x200b what are some of your thoughts
1,project scaling and power analysis the instrument i am using to survey is somewhat expensive if i am trying to minimize the number of surveys i give out for evaluation even though the number of individuals we are treating is increasing does my a priori power analysis change if i am scaling my rct project from n 500 to n 5000 my initial thought is no because the a priori analysis would indicate that i need a certain number of surveys to meet power to detect a certain level of effect size but just wanted to see what others think
1,is the chi squared test appropriate to determine statistical significance for grouped continuous data for instance if i wanted to find out if the age distribution of people at an event was reflective of population age distribution could i group people by age eg 0 4 5 9 10 14 etc and then perform a chi squared test based on the number of people in each age category if not what would be a better statistical significance test for this kind of application
1,research msc student looking for help on deciding statistical analyses for thesis hello to anyone who feels inclined to help out a struggling grad student i am currently working on my thesis proposal and racking my brain trying to figure out what statistical analyses make the most sense for my project what i am mainly looking for is an appropriate test or tests that ll help me compare different metrics of corals that undergo two different coral restoration methods any help would be greatly appreciated
0,weekly entering transitioning thread 13 jun 2021 20 jun 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
0,pandas query not giving the same results as the same query in sqlite edit i ll post this in stack overflow i forgot about the subreddit s rules x200b i have made a query with the file sample superstore csv from that will give me the count for each case that contains some type of criteria for strings as well as a count without any of the previous criteria for the other counts the column with the strings that will be analyzed is customername x200b basically count for number of clients where the full name starts with an upper case a number of clients where the full name has a lower case t number of clients where the full name finishes with a lower case n then the number of clients where the full names don t have any of the previous criteria x200b this is the query in pandas x200b import pandas as pd import numpy as np import re df pd read csv path of csv file sep pd set option display max rows none pd set option display max columns none pd set option display width none pd set option display max colwidth none df strings conditions np where df customername str startswith a starts with a capital a np where df customername str contains t has a non capitalized t np where df customername str endswith n finishes with a non capitalized n something else df new df loc strings conditions customername drop duplicates dropna df new groupby strings conditions strings conditions count x200b which gives the following results x200b x200b strings conditions count finishes with a non capitalized n 100 has a non capitalized t 288 something else 341 starts with a capital a 64 x200b but the same query in sqlite x200b select finishes with a non capitalized n as strings conditions count from select customername from mag correction where mag correction customername glob n group by customername union all select has a non capitalized t as strings conditions count from select customername from mag correction where mag correction customername glob t group by customername union all select something else as strings conditions count from select customername from mag correction where mag correction customername not glob a and mag correction customername not glob t and mag correction customername not glob n group by customername union all select starts with a capital a as strings conditions count from select customername from mag correction where mag correction customername glob a group by customername x200b gives me x200b x200b strings conditions count finishes with a non capitalized n 187 has a non capitalized t 313 something else 341 starts with a capital a 64 x200b would anybody know why this happening two of the count results are the same but i m don t know where i have gone wrong for the other two x200b if any clarification is needed i ll happily provide more x200b edit creating a view in sqlite exactly like df new in pandas with the following query select case when customername glob n then finishes with a non capitalized n when customername glob t then has a non capitalized t when customername not glob a and customername not glob t and customername not glob n then something else when customername glob a then starts with a capital a end strings conditions customername from mag correction group by customername then querying it select df new strings conditions count from df new group by df new strings conditions gives once again a bunch of different results except for two rows compared to the other sqlite query x200b strings conditions count finishes with a non capitalized n 187 has a non capitalized t 234 something else 341 starts with a capital a 31 x200b i m so confused x200b x200b
0,digitizing printed archives of data tables does anyone have advice or ideas on how to digitize several hundred pages of printed tables of data engineering measurements table headers are a mix of english french and german data is mostly measurement data imperial measures and fractions in a courier style font of the typewriter era i know i have to ocr all this but i can’t find a tool well suited to tables of data let alone engineering notations what’s an efficient way to scan all this kinos or officemax perhaps but then how do i get this into something like excel am i really going to have to hand key all this it could take me months if i can stay sane long enough any advice welcome
1,research does anyone have any idea on how to tabulate results for a social science paper i apologize if this is not the right place to ask this question if so please point me out to the correct place most of my statistical analyses include anova and the reviewers are asking me to tabulate the results screenshot i haven t seen any paper in my field that has tabulated the results so i don t know which way to go how should a result that mainly consists of anova and subsequent pairwise comparison if the effect was found to be significant be analyzed in the first place isn t the norm presenting the statistical results in text are there any papers that have tabulated such results i would really be grateful for any insight
2,help on how to extract article text from time magazine pages i ve downloaded 90 years of time magazine from time vault i ll put scraper code on my github soon i want to analyze the contents in all these years using text analytics or natural language processing first i need a reliable and scalable way of text extraction from page images any suggestions will be appreciated currently i m considering ocr on extracted page layout
1,mann whitney u test or correlation for context my iv is experimental condition between subjects design and my dvs 3 in total are measured on likert scale therefore ordinal am i correct in thinking that because i want to look at the differences between the two groups in their responses to the likert scale items i would use a mann whitney u test as the data is ordinal or would i be better off using a correlation for non parametric data essentially does the method of analysis depend on how i phrase my research question for example if i state i want to look at differences between the two groups i would use a mann whitney but if i said i wanted to examine the relationship or association between condition and response i would use a correlation i think i’ve just got myself a bit confused any help is appreciated as i am very much a novice thanks edit i have a small sample size 15 participants in each condition if that makes any difference
1,introduction into linear models glm multivariate linear regression mixed linear models hello everybody i am an undergrad student who wants to take a deeper dive into some areas of statistics specifically i constantly read about glm multivariate linear regression or mixed linear models and think that i could really benefit from knowing more about it since i am a complete beginner i don t really now how to start i e which topic to read about first i don t even know if there is a big overlap between the types of linear models that i mentioned so my question is if you could recommend me a order in which i should read about the topics and maybe even point me to some freely available resources big plus if it has some hands on examples with r cheers
2,full page handwriting recognition via image to sequence extraction the authors propose a model that does not require prior segmentation however achieves state of the art accuracy original paper here more hard to find independent stuff related to ai data science here
2,explainable artificial intelligence xai concepts taxonomies opportunities and challenges toward responsible ai a comprehensive overview and introduction on xai highlights • we review concepts related to the explainability of ai methods xai • we comprehensive analyze the xai literature organized in two taxonomies • we identify future research directions of the xai field • we discuss potential implications of xai and privacy in data fusion contexts • we identify responsible ai as a concept promoting xai and other ai principles in practical settings link to the paper
1,what areas in statistics are the following topics in machine learning closest to what areas in statistics are the following topics in machine learning closest to supervised learning linear nonlinear random effect model for regression hypothesis testing and categorical data analysis for classification unsupervised learning multivariate statistics reinforcement learning does statistics have areas for or close to that deep learning and neural networks thanks
1,ordering data by low medium high on spss as part of my study i asked presented participants with a list of behaviours and asked them to tick which ones they believed to be associated with a learning disability i have since ran a frequency test on spss for each behaviour which tells me how many participants selected or did not select the behaviour i now want to sort the data into groups such as high frequency medium frequency and low frequency how can i do this without thinking of an arbitrary number as a cut off for each group thank you edit the data is nominal as i coded 1 selected 2 not selected
2,the institut lumière tries to delete from youtube the ai enhanced video arrival of a train at la ciotat made by ai enthusiast denis shiryaev you all have seen this video it has gone viral around a year ago video is already blocked around march april 2021 lumier institute send a pre trial claim to delete ai enhanced version of the video from youtube they didn t like that it is different from the original 60ps and stuff like duh that s the whole point in ai enhanced videos copyrights violation a couple of days ago denis wrote that il decided to sue him for real this time and not strike in youtube they requested a formal answer in 7 days if the video will be deleted or not as denis says he has no time neither money to fight with the lawsuit right now so he decided to block the video on youtube i was writing about that situation here and then just now denis shared his own explanation here so i decided to post both denis runs from the train i think that smells super bad looks like we will have more and more strange legal cases with ai solutions what do you guys think about all of that
1,how sensitive is inference to model specification should test error accompany p values how sensitive is inference to model misspecification should out of sample error be reported for inference so one of the things about flexible ml models is they are not easily interpretable but the simple glm models may not be flexible enough to capture everything in the data and so even if you get a treatment effect estimate it could be off is there a way to quantify when you can trust an inference made by a simpler model if the predictive accuracy of the simpler but interpretable model is way below that of a flexible nonparametric ml model then can one say that the inference should not be trusted since it is likely that the model is misspecified and if multiple models are close to the same predictive accuracy then even if the ml model is slightly better the inferences from the simpler model will be approximately correct intuitively even when doing inference shouldn’t we be caring about out of sample error since the goal is still to generalize our results to data we have not yet collected along with the p value ci should cross validation or test error metrics be reported it just seems off to me when there is data where the model gives a low r 2 of 0 05 or something but then people still report p values and something being significant especially for observational data
0,audiogram data i m considering taking on an internship project that involves using audiogram data to predict translate into parameters that can be used for hearing aids does anyone know which libraries models can be utilized to treat this type of data
1,what is the process to correctly compare the popularity of items over time i have a list of items which were published at specific times going back a decade these items have been downloaded many times each for each item i have the total number of downloads over their entire life downloads do not occur evenly spread out they are bunched towards the beginning of availability how do i compare the popularity of a recent item with that of an older one if i divide the downloads by the period that they were available i m assuming that downloads are evenly distributed and i end up with the most recent item being extremely popular when this is clearly not the case how do i do this background i publish a weekly podcast and i am attempting to determine if it s being listened to more or less i have the actual server logs
1,how important is graduate level probability theory hi all during my undergrad i took probability theory 1 and 2 in probability theory 1 i learned the principals of counting the axioms of probability random variables fundamental limit theorems concepts relating to conditional probability and independence discrete and continuous random variables how to compute their moments the law of large numbers and central limit theorem in probability theory 2 i learned about discrete and continuous markov chains stochastic processes and brownian motion both of those classes had some theory but it was mostly understanding concepts and solving problems none of those classes dealt with measure theory my grad program has advanced probability 1 and 2 i was told that i can forego taking both those classes because of my stellar grades in the undergrad probability theory 1 and 2 courses i m a bit hesitant to forego taking them what do you guys think should i take advanced probability theory 1 and 2 thank you
1,topsis and inverse rating in other multi attribute decision making methods it is okay to use the inverse of values if that attribute has the property where the smaller the better such as cost of expenses in reading about topsis it uses a different approach of instead manually determining which criteria has a positive or negative impact on the decision making my question is that if you could just take the inverse of particular attributes where smaller is better then you would not have to manually determine which has a positive or negative impact can anyone see possibly mathematically what could be problematic about taking inverses and still normalizing these inverse values
1,any good books about an introduction on survival analysis
1,i need recommendations for books on statistics and probability i can purchase a maximum of two books i m opting for a phd in machine learning and need to brush my abilities in stats and probability i ve studied these before at undergrad but seem to have forgotten some of it over the last 6 years the book which i d prefer is which would provide me should conceptual understanding first before diving into the notation and mathematics due to financial constraints i can purchase a maximum of two books only i have understanding of machine learning but i ll need books that ll help me understand research papers and be able to write my own i d prefer the book which uses the most common notations particularly for bayesian probability ps i m opting for a phd in machine learning i did my masters way back haven t been actively involved in ml and i need to brush up things i prefer reading over watching tutorials lectures
2,paperfella where we learn from research papers together hello everyone want to join the place where people learn from research papers while talking to each other paperfella is an app for that it currently has two main functionalities 1 it creates a smart real time chat per research paper 2 it improves the research papers so you can read it in vertical mode in your mobile without zooming and you can also break down a word or expression to its most basic meaning by just touching the word or expression cambridge dictionary s api it also applies better styles on the typography and math symbols i want to talk to some people who d like to help me with this idea or are interested in it
1,anyone who is attending has attended colorado state’s master’s in applied statistics do you recommend it what are some good and bad things about their program they’re one of the schools i’m strongly considering but i want to make sure people have enjoyed the program i’m told the first two courses in fall are some of the toughest and most theoretical courses in the program do you agree with that do the 8 week courses feel rushed or are they designed well enough to not feel overwhelming
1,importance of neural tangent kernels has anyone heard of the neural tangent kernel i originally had thought this was an activation function for a neural network looking here the neural tangent kernel ntk is a kernel which describes the evolution of deep artificial neural networks during their training by gradient descent it allows anns to be studied using theoretical tools from kernel methods can someone please help me understand what this means why are neural tangent kernels important thanks
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
2,earthquake prediction ai challenge
2,research project how to read more research papers in this article i am sharing the best tips and practical tools i use daily to simplify my life as a research scientist to be more efficient when looking for interesting research papers and reading them please let me know if you use any other tools that i did not mention in my article that could be of great addition quick summary of the tools discussed 42 papers arxiv sanity preserver papers with code crossmind catalyzex yannic kilcher what’s ai letitia two minute papers
0,how much of your work is descriptive rather than predictive or explanatory and how limited are your analyses by your data sources
2,how do you handle cases when little relevant data is produced in document related ml systems in production for example when extracting data from an unstructured document such as an order confirmation document if only a few fields like date of order and name of the supplier are retrieved but details such as the amount and the items are not how is this handled what are the fallback mechanisms used
0,setup for a dashboard based business i m looking for a variety of opinions on the following x200b i m looking to build a website that brings together a variety of data sources and presents them as dashboards the business model is paid subscription for access to the dashboards x200b for the dashboards i m trying to decide whether to use plotly dash or powerbi x200b i feel that dash gives me more flexibility but the time taken to build fully interactive dashboards is higher compared with powerbi also the user authorization side is more complex sticking to the free open source version x200b on the flip side i don t necessarily want a business that is reliant on a product of another company like microsoft x200b anyone have any thoughts on this
2,facebook to launch nethack competition at neurips 2021 ai facebook has been interested in nethack for some time now for a simpler explanation from facebook and now they re announced this year as part of a neurips 2021 competition we are proud to launch the nethack challenge—the most accessible grand challenge for ai research—with our partner and co organizer aicrowd
0,applying ml ds for cybersecurity vs finance these are two areas that broadly i’m interested in applying ml and ds techniques and skills to i was wondering if some people could shed some light on some of the pros and cons of working in a machine learning or data science capacity in either of these areas thanks
2,managing compute for long running ml training jobs hi r machinelearning i have been curious to see what the community’s biggest issues are around running large training jobs we run a modest sized gpu kubernetes cluster but there seem to be a fair gap of functionality when it comes to fair resource access hpc style backfill time estimates etc and general usability to many of our scientists for instance some of the issues i’ve seen gpu machines being fragmented lots of single gpu jobs making full width jobs very hard to land interactive sessions being much harder than traditional hpc schedulers like slurm in the absence of a cloud parallel file system like lustre dealing with dataset location and loading what are some things you guys have run into re resource access for training
0,data science in a regulated industry i’m a ds in a highly regulated industry pharma and i was wondering what y’all thought about validating ai or ml models i have my own opinions on what is best but i tend to see attempts at validation fall into two buckets first some teams try to go at it all and validate the whole pipeline ml model and all these teams usually spend loads of money and time to validate only to do it all over again when they retrain the model in six months the other path i see is where teams try to treat the model as a black box and validate the surrounding infrastructure these are much quicker to validate verify but run high under high risk in the eyes of regulatory bodies so what approach do y’all think is best or if there’s another one i’d love to hear some suggestions
1,how to identify the upper threshold of a skewed plot sorry for probably my bad use of terminology i work in biology and i m learning statistics as i go i have a distribution plot of data plotting genes with different p values which looks like this y axis is counts of genes x axis is p values per gene i want to select the upper threshold of genes with very high p values so insignificant genes but i don t know how to best methodically do this for example i know it would be best to get a final gene group of a few hundred genes as that number works best in downstream gene analysis tools but i feel it s biased of me to set a p value cut off that gives me the gene number i want but isn t actually completely the upper gene group in the total distribution if that makes sense based on a skewed plot how can you select thresholds in the plot methodically to view upper and lower quartiles of genes
2,the pdlt document from facebook is currently worthless as a scientific text and exemplifies the serious problem we are facing with reproducibility within the community from the preface first and foremost in this book we’ve strived for pedagogy in every choice we’ve made placing intuition above formality this doesn’t mean that calculations are incomplete or sloppy quite the opposite we’ve tried to provide full details of every calculation – of which there are certainly very many – and place a particular emphasis on the tools needed to carry out related calculations of interest in fact understanding how the calculations are done is as important as knowing their results and thus often our pedagogical focus is on the details therein second while we present the details of all our calculations we’ve kept the experimental confirmations to the privacy of our own computerized notebooks our reason for this is simple while there’s much to learn from explaining a derivation there’s not much more to learn from printing a verification plot that shows two curves lying on top of each other given the simplicity of modern deep learning codes and the availability of compute it’s easy to verify any formula on your own we certainly have thoroughly checked them all this way so if knowledge of the existence of such plots are comforting to you know at least that they do exist on our personal and cloud based hard drives i can t speak for everyone reading this i can only speak for myself that said i have a phd in physics and i currently work as a deep learning research scientist and engineer for a major tech company i have dedicated my entire life to the study of natural law science and the nature of scientific inquiry as a whole never once in more than two decades of my time as a graduate student a scientist and an engineer have i ever read a text book that began we know you want to see the evidence we know you want to know what experiments we ran to appropriately test these theories we know you want to know to what degree of significance we have rejected various other hypotheses and how that was done we know you want to know but we aren t going to tell you we leave the verification of this work as an exercise to the reader trust us what are we even supposed to do with this no wonder we have an issue with reproducibility in the field when deep learning scientists are publishing documents without any actual science in it no wonder we have literal collusion rings in our peer review process there is a full litany of mathematical assumption that form the foundation of the deductions made in this document and the author s claim to their validity stand solely on their trustworthyness need i remind you all that they work for a corporation facebook specifically that isn t exactly known for its honesty i am reminded of the motto of the royal society one of the most well established scientific institutions in modern history nullius in verba words are empty take nobody s word for it
2,spacy transformers wrapper for newer transformer model hello all i am working with spacy transformers which is essentially a wrapper for huggingface transformers it already supports few transformers model out of the box my question is how can i use this for other transformer models within spacy if you need any other input from me please let me know your help is really appreciated thank you for you time
1,can any give a non technical explanation of the different between the survival and hazard function on a kaplan meier graph
0,open dashboards for app data analysis need to analyse app data for work and showcase it on a dashboard don t have a lot of experience in it and would like to see how similar data has been analyzed before are there any open projects dashboards that you are aware of that i could use as a point of reference any other resources on the same are also appreciated thanks in advance
2,yoshua bengio team’s recurrent independent mechanisms endow rl agents with out of distribution adaptation and generalization abilities a research team from the university of montreal and max planck institute for intelligent systems constructs a reinforcement learning agent whose knowledge and reward function can be reused across tasks along with an attention mechanism that dynamically selects unchangeable knowledge pieces to enable out of distribution adaptation and generalization here is a quick read yoshua bengio team’s recurrent independent mechanisms endow rl agents with out of distribution adaptation and generalization abilities the paper fast and slow learning of recurrent independent mechanisms is on arxiv
2,sp mlp mixer implementation in flax pytorch video hey there i made a video where i implement the mlp mixer in both flax and pytorch among other things i try to discuss in what way it is similar to cnns also if you have never used flax before the video contains a quick tutorial on the most important concepts x200b original paper my video
1,theoretical regression books i m looking for recommendations on a proof based regression book i know this question has been answered to death but i m specifically looking for a rigorous and compact regression book some of the current regression books out there are way too long 1000 or 800 pgs so far i have found linear regression analysis by seber and lee is esl good for regression i have a math undergrad and an incoming grad student for computational finance i m have self studied statistical inference by casella up to chapter 10 finishing up the exercises right now please lmk if you recommend reading the last 2 chapters of casella they re about anova and regression thanks
2,data challenge by oak ridge national laboratory news if you are interested to play with large scientific data generated by various divisions of ornl please visit
0,should proprietary clinical ml algorithms be subject to external review some recent events have spurred discussion around if or how clinical ml tools should be regulated some suggest fda regulation others peer review the vendors claim to be transparent but just with their customers what are yall s thoughts on the matter
0,transitioning from nlp to satellite and image based cv i m currently in a role where i ve solely been working on nlp for the entire time recently i was offered an opportunity at a new company where i would be working with satellite and time series data with the goal of doing combining time series and cv the thing is i have zero experience working with both time series and image data the company are aware of this though and they are fine with it my main reason for posting this is to try and gather some training resources so i don t go in with zero knowledge looking for any interesting projects courses involving those types of data that would be beginner friendly and go through the processes that are generally required for this work appreciate any help you can provide
2,paper explained expire span not all memories are created equal learning to forget by expiring full video analysis facebook ai fair researchers present expire span a variant of transformer xl that dynamically assigns expiration dates to previously encountered signals because of this expire span can handle sequences of many thousand tokens while keeping the memory and compute requirements at a manageable level it severely matches or outperforms baseline systems while consuming much less resources we discuss its architecture advantages and shortcomings x200b outline 0 00 intro overview 2 30 remembering the past in sequence models 5 45 learning to expire past memories 8 30 difference to local attention 10 00 architecture overview 13 45 comparison to transformer xl 18 50 predicting expiration masks 32 30 experimental results 40 00 conclusion comments x200b paper code
1,calculating if something falls within the standard deviation word of mouth story question discussion hey so i took first year statistics 12 years ago and i m rusty this is the problem a friend heard from a friend who manages a company of 5000 people in texas ages from 18 80 100 s of employees had covid but only 1 died the friend of the friend spoke to a doctor he knew who says he treated 8000 patients with covid and none of them died the conclusion my friend is making from this word of mouth story is confirmation of his idea that covid isn t as bad as claimed and there s a conspiracy i disagree i know it s generally bad to base important decisions on word of mouth or small sample sizes so i m trying to see if there s a way to test this story mathematically now i know that the smaller the sample population the more the reality can deviate from the true percentage but i vaguely remember that there might be some way to calculate standard deviation based on the sample size we know the death rate is around 1 9 in general and varies by age case fatality by age graphic and it varies depending on how healthy the population is case fatality by underlying conditions graphic but assuming his friend meant around 300 cases you would expect based on a 2 death rate around 6 deaths or if you assumed most of them were under 60 expect closer to a 1 death rate with 3 dead ultimately i understand it s difficult to give a fair answer to this question without knowing what 100 s mean or without having an idea of the age distribution of the people who actually caught the illness but roughly how would you go about evaluating this word of mouth event statistically
0,is it okay to have an updated title on my resume to get a foot in the door i work at an outdated company that calls my role something vague with words like “quantitative analytics” recently a recruiter told me to just call it “data scientist” as that’s what i’m doing functionally as demonstrated by the details and what i convey in interviews is the risk of being called a liar too great or should i go with it to improve my traction when applying
0,how to deploy a real time model which gets the variables features in phases for example i have a linear regression model ready i dont get all the features at the start for example i get 6 out of 10 features i need to see the target variable range i will get x200b also if i need to deploy it in such a way that it tells me what should be the values of other features provided some features and the desired target variable range
1,need recommendations of statistical software for beginners hello i am an undergrad student in cs i ll be pursuing a master s in data science after my graduation i am looking to learn some statistical software but there are many on the market can you recommend to me what software should i learn edit i know python and libraries like numpy pandas etc was just wondering if knowing any softwares will up my profile
1,i want to do a cluster anlysis on sets of numbers while taking into account fluctuations within those sets is there something more advanced than using variability i have sets of numbers the sets will be clustered not the numbers if i have for example four sets 4 4 4 4 7 3 4 8 6 2 2 6 6 6 5 5 the real sets are much larger in size and number and range 1 10 with one decimal now likely the first and third set would be put together and the second and last set this is based off their means they are the same for 1 and 3 and 2 and 4 however and this is my question i d also like to take into account the fluctuations that occur within the sets in addition to the means in that case sets 2 and 3 will be put close together in the analysis since they both have the same drop from the first to the second number and the same rise from the third to fourth number making a u shape and their means are somewhat close to each other taking something like variablity to do this works somewhat but different trends in the sets of numbers could lead to the same variability for example another set 2 6 6 2 would still be in the same cluster as 6 2 2 6 this is because their means and variability are the same even though the sets are the exact opposite of each other so by using variablity there is too much loss of information a lot more interesting interpretation of the results could be done if similar fluctuations at similar points in the dataset are grouped together so is there any way to do this
0,has anyone else noticed a huge uptick in unsolicited data science recruiter emails lately over linkedin i can t tell if this is a sign that the field is growing or if everyone s hiring data scientists for departments that will inevitably collapse in 5 years historically when i ve been on the hunt for a new data science role i ve found it s generally taken me anywhere from 6 months to a year to find and start a position prior to this year i d get maybe a few linkedin recruiter emails every few months or so but something about the past few months seems to have thrown everyone into overdrive literally 3 5 emails a day have been rolling in for everything from 12 month contract role 3 years experience to want to be a director of data science starting a department at this particularly well known company i can t tell if this is a good sign that the field is growing or if this is a sign everyone s hiring data scientists for projects that are going to ultimately end up disappointing them and will collapse the department in a few years
0,prospects of becoming a chartered data scientist cds offered by adasci i came across this certification where you become a chartered data scientist aimed for the working professional any person who successfully passes the exam mcqs and has prior 2 years of work experiences is bestowed this charter what i wished to know is whether it is recognized enough in the industry like the tensorflow developer certificate and other exams like sas etc and whether it offers exclusivity in terms of an increase in pay range in the data science industry and what benefits could one avail holding such recognition your views
2,garments crease wrinkles dirt removal using computer vision hi reddit i ve been looking for some ai computer vision based automated way to remove crease wrinkles folds dirt from clothes in fashion images captured for ecommerce i did try to search for gan based methods or image inpainting based methods but couldn t find anything reliable i am looking for some way to retouch the image in a way that the wrinkles folds crease dirt on the clothes could be removed i could find few websites which are doing the similar work which i require to attain programmatically but i couldn t find exactly where in computer vision to look for sample solution could anyone provide me any pointers help what and where i can look any help is highly appreciated thanks
1,i ve implemented nonparametric bootstrapping but i still don t believe that it should be able to give useful statistics what should i look at to better understand why bootstrapping works
2,how to create a pre training model for three different datasets i know a simple way if i a dataset i train a cnn model and i get a pretraining model but if i have three different datasets x ray ct scan mri i cant combine them to a single dataset as the model will learn the dataset instead of classes as data characteristics are different so if i train the model on one dataset then 2nd then third will all the training weights of three different datasets is preserved or weights will be overwritten or what is the right approach to get overall embeddings
2,what should i do after building a baseline solution i m new to neural network i am really curious to know what will you guys do after building a baseline model when participating a competition kaggle i often build an end to end baseline solution to ensure it work without bug for example in image classification competition i usually do something check list below data preparation resize and normalize only no special augmentation setup neural network learning rate 3e 4 adam optimizer cross entropy loss or binary cross entropy loss pre train neural network such as resnet18 resnet34 turning off all regularization hyper parameters training sampling one batch to overfit easily no learning rate scheduler training and validating one fold only early stopping plotting loss and metric scores testing running above model with test set exporting test predictions to submission csv then submit to public leader board lb if the result makes sense the baseline seems work without bug then i train all samples with all folds and submit again to check baseline score after that i struggle to figure out what should i do further to improve lb score i ve tried to adjust learning rate to overfit train dataset then adding some augmentations such as random resize crop flip transpose… then check lb score again the score was improved a little but it was still far from top lb of course some time it was not much different from baseline score i read some kaggle forum discussions some nice guys shared their solutions to get higher score in lb they usually used different parameters than me such as fancy augmentations bigger pre train model or adding some layers blocks to pre train model custom loss function modern learning rate scheduler ensemble many models etc etc… i want to reproduce their solutions to learn their knowhow but i don t know what to do first should i try switch to bigger model first or switching loss function first or adding fancy augmentation first or changing loss function first etc… since there are too many black boxes in neural network it is not easy to understand or interpret how a network works but i think there are some strategies or disciplines to help what will you do to overcome this problem please share your own wisdom any comments and suggestions will be appreciated best regards p s my baseline checklist is inspired by this famous blog
1,i need help with regression model a we know that the value of company2 is 15000€ in december that is month 12 calculate the value of company1 in december using regression model y 3 986x 1023 606 month 11 is company1 3400€ company2 11500€ how can i calculate that
2,decision transformer reinforcement learning via sequence modeling paper website github transformers is all you need
0,need advice about building out a model in terms of algorithm picking so the situation is basically i want to break down parsed data from a game called counter strike and try to figure out what locations on a map are important to winning a round the features i was thinking of collecting would be the tick 128 1 second so would probably normalize this as the maximum amount a round can last is 1 minute 55 seconds 40 seconds usually lower amount than this 10 vectors of 91x1 which would be one hot encoded to break down roughly 90 in game locations that i have labelled a player can only be in one of those locations at a time a player can also be labelled dead or off the map so was thinking of indicator for that 10 1x1 vectors to indicate whether a player is ct or t training label would be 0 or 1 to indicate either t or ct side win two teams of 5v5 face off each round to a best of 30 conclusion for anyone unfamiliar with the rules so all in all the ending goal of mine would be to try and figure out how important those locations is my idea for the feature set adequate or do you see certain aspects of it that would be an issue also if the feature set is fine what sort of algorithms or approaches would be best to go by i just have not flexed these ml muscles in a decent amount so would like the advice
2,aws announces redshift ml to allow users to train machine learning models with sql amazon has announced the general availability of redshift ml enabling customers to utilize sql to query and combine structured and semi structured data across various data warehouses operational databases and data lakes redshift ml can deploy train and create machine learning models directly from an amazon redshift instance previously aws customers who wanted to process data from amazon redshift to train an ai model were required to export the data to an amazon simple storage service amazon s3 bucket then they can configure and start training the process needed many different skills and more than one person to complete thereby raising the barrier to entry for enterprises aiming to forecast revenue predict customer churn detect anomalies etc full article aws source
1,can you help me intuitively understand an interaction coefficient in linear regression with effects coded predictors let s imagine an experiment where people look at pictures and then rate their mood after each one from 1 sad to 7 happy the pictures can either be happy or sad iv1 and participants are either doing the experiment on a high resolution or a low resolution monitor iv2 this is not my real experiment just an example these two predictors are effects coded and the data are analyzed at the trial level with mixed effects linear regressions let s say the coefficient for picture type is 1 i think this means that on average mood after happy pictures is rated two points higher than sad pictures remember it is effects coded let s say the coefficient for screen type is 5 people are on average 1 point happier when doing the experiment on high resolution monitors can you help me intuitively understand the interaction coefficient let s say for example i expect the effect of picture type to be a three point difference on the high resolution monitor and a one point difference on the low resolution monitor what interaction coefficient would that translate to i think that i know how to work this out with dummy coding but i am at a loss now with effects coding
1,perceptron convergence theorem has anyone ever heard of the percptron convergence theorem does this theorem basically state that a single perceptron can perfectly classify linearly sepperable data
0,accumulated local effects i recently came across a newer technique called accumulated local effects that attempts to explain the effect of predictor variables on the response variable has anyone tried using this method on real data did you find it useful any stories anecdotes experiences comments reviews you would be willing to share regaeding this method
1,p value indicates no significance bootstrap confidence interval does neither cross nor include 0 however spss hi guys i´m asking this for my girlfriend who s currently working on her master thesis and stuck with her data analysis and interpretation in spss she executed a normal moderation model 1 macro process by hayes and due an asymmetrical confidence interval chose to run a bootstrap whereas the main output p value indicated no significance the bootstrap interval does neither cross nor include 0 however which is usually a sign a significance the lower ci limit is 0 013 however which to me seems dangerously close to 0 as you can probably tell by now im not that much into spss or statistics either her english sucks however that s why i m writing this right now any help on what she could try next would be immensely appreciated her supervisor sadly couldn t care less cheers guys stay healthy
1,after pca i have 98 variance explanation on the first component does that mean there is only 1 important feature in my dataset say i have a dataset with 5 features i want to display my data in 2d so i reduce the dimension with pca it turns out the first component explains 98 of the variance i computed it with the ratio of eigenvalues does that mean there is only one important feature in my initial dataset i am currently not so sure because the relationship between the initial features and components produced by the pca is not straightforward
1,how do i go about advancing my career knowledge of statistics recent graduate here and i realized far too late that statistics is something i want to pursue a career in in my last year at my university where i obtained a bachelors in computer information systems there was no option for a major in statistics and my minor was already completed in ux design i studied under my professor after i was finished with her class where she gave me somewhat of an apprenticeship i was hoping this would lead to some kind of internship after i was done with school then covid happened i was told it would probably be 2021 before i was able to get involved with a recommendation to a relevant firm and in the meantime i should “keep my skills fresh ” whatever i worked a temp job the last year fast forward to now i’m informed she “can’t guarantee me a spot on their team” because of how much time has passed not sure if this is the right subreddit for this but i’m not really sure what the best option for me is at this point should i go back to school somewhere else when i have enough savings do i attempt to get a job somewhere even with my lack of direct work experience degree any help would be really beneficial thanks
2,project unable to use emojis for masked language modelling using bert i am new to hugging face and masked language modelling mlm and i was wondering how to include emojis when doing such a task i have a dataset with tweets with each tweet containing an emoji at the end here is a sample of my data id tweet 1 looking good today 😎 2 weather is so hot lol ☀️ 3 i hate you 🤬 at the moment i have fully trained my masked language model using my dataset but when i predict something it does not output or predict the emojis it just predicts words this is my desired input from using my dataset for mlm you look great mask this is my desired output from using my dataset for mlm score 0 26041436195373535 sequence you look great 😎 token 72 token str score 0 1813151091337204 sequence you look great 💯 token 2901 token str score 0 14516998827457428 sequence you look great 👌 token 328 token str however this is what i am actually getting from my output score 0 26041436195373535 sequence you look great token 72 token str score 0 1813151091337204 sequence you look great token 2901 token str score 0 14516998827457428 sequence you look great token 328 token str i know it is possible to do this but how do i do it i am close but not very likewise i have my model fully trained on my dataset but it just does not seem to output emojis even though i have included them in the training does something need to be included to accept emoji if so what thanks i would really appreciate the help
0,language agnostic deployment setup i m in a situation where i need to productionise a large number of models written in various languages we have a system set up for deploying python models in docker containers accessible via api currently our approach is to attempt to wrap any non python models in python code and deploy using our existing framework for example we can wrap an r model to resemble a python one via a library such as rpy2 however this isn t particularly elegant and new wrappers have to be written for each new language or even for particularly different models in the same language another option i d considered was having the models kept in their own language specific scripts which can be executed from inside python app via calls to the command line this seems a better approach since we re no longer dealing with wrappers though i m concerned that having to run import statements load the model for each inference could cause high latency a third option i ve been toying with though haven t managed to figure out the details for is to have the web app run in one container and the model in a second with some very minimal model serving code i think this might allow us to sidestep the issue of loading dependencies though it seems we d have to write the model serving code in the model s language the original problem i m interested to hear your thoughts on this has anyone else found an elegant solution to this
2,sergey levine audience questions soft robotics podcast x200b hello we are going to have sergey levine on the podcast if you have any questions you can send them here x200b
1,rules of thumb for effect size of interaction coefficients in mixed effects logistic regression lets assume all continuous predictors are standardized and all categorical predictors are binary are there rules of thumb for what interaction coefficients would be small medium and large in mixed effects logistic regression i been shown this resource when has a formula for converting cohen s d to log odds ratio to answer my question could i simply convert a small effect by cohen s d standards to log odds ratio and use that as a small interaction coefficient in mixed effects logistic regression this doesn t have to be very precise it is just for the purpose of ball parking number of participants using a simulated power analysis
0,volatile ds role vs usual sde i ve received an offer letter from an amazing research company where my work profile would be exactly what a data science student could dream of i d not think twice before accepting the offer but there are many buts it s a termed employment and i don t see anywhere on linkedin where a grad student read huge loans has worked for more than a year also the salary is way less than what sdes would be paid as their starting salary i do not like sde roles i m not good at it or any role where there s no room for research but at the same time i ve prepared myself to accept such roles as they re highly paid this would mean getting rid of the huge education loan and taking responsibilities of my family parents i ve asked my parents and sister for advice but i feel their opinions are biased dad being in research field says it would be foolish to leave this job since he knows i love this role rest of the family having seen the financial downsides of it are pestering me to go for high paid stabler jobs as it s a one solution for all problems loans visa finances etc i m super confused that s why reaching out to the community would appreciate an unbiased input about this thanks
2,what is currently the best theoretical book about deep learning i m looking for the book about deep learning most of them deep learning for coders deep learning with python etc focus on practical approach while i d love to dig a little bit deeper into theory one way is probably reading pivotal papers but i still find it a bit intimidating therefore i d love to find a book with good but more theoretical explanations i heard good opinions about deep learning by ian goodfellow et al but i wonder if it s not a bit outdated since the field is changing rapidly and the book already is 5 years old how much will i miss while reading this one is there a better option currently
2,vkfft now supports discrete cosine transforms on gpu hello i am the creator of the vkfft gpu fast fourier transform library for vulkan cuda hip and opencl in the latest update i have added support for the computation of discrete cosine transforms of types ii iii and iv this is a very exciting addition to what vkfft can do as dcts are of big importance to image processing data compression and numerous scientific tasks and so far there has not been a good gpu alternative to fftw3 in this regard vkfft calculates dct ii and iii by mapping them to the real to complex fft of the same size and applying needed pre and post processing on flight without additional uploads downloads this way vkfft is able to achieve bandwidth limited calculation of dct similar to the ordinary fft dct iv was harder to implement algorithm wise it is decomposed in dct ii and dst ii sequences of half the original size these sequences are then used to perform a single complex to complex fft of half size where they are used as the real and imaginary parts of a complex number everything is done in a single upload from global memory with a very difficult pre post processing so dct iv is also bandwidth limited in vkfft dcts support fp32 and fp64 precision modes and work for multidimensional systems as well so far dcts can be computed in a single upload configuration which limits the max length to 8192 in fp32 for 64kb shared memory systems but this will be improved in the future dct i will also be implemented later on as three other types of dct are used more often and were the main target for this update hope this will be useful to the community and feel free to ask any questions about the dct implementation and vkfft in general
1,continuous covariate on chi square test of independence i am currently doing a chi square analysis on 2 categorical variables asd group high trait low trait and anxiety change no change worsened improved i found age to be significantly different between asd group levels and i want to control for that in this analysis how do i do that on spss x200b thanks
1,what test should i use for normally distributed in regression equation what should i use to identify of the outliers of the individual regression equations are normally distributed what are the common reasons for outliers in regression analysis
1,variables in pca model with same loading hello everyone i am just wondering if it is possible for a pca model to have variables or some of the variables with similar loading for example in a model we have variables x1 up to x5 and under pc1 variables x1 to x3 or all of them have 0 05 as their value if it is possible what causes the variable to have similar loadings so far i have not encounter this situation but i m thinking this kind of situation may come up in exam thanks
1,help with a stat problem the question is as follows 27 to compare the effectiveness of two treatments researchers conducted a well designed experiment using a randomized block design in which the subjects were blocked by age group under 40 years and 40 years or older which of the following must be true about the randomized block design of the experiment a the number of subjects in each block is different b treatments are randomly assigned to subjects within each block c the design cannot have a control group because subjects are blocked by age group d the experiment uses a matched pairs design where subjects from one block are paired with subjects from the other block e the subjects in one block receive one treatment and the subjects in the other block receive the other treatment i understand why b is right but why would c not be right also what would the control group be for this experiment
1,pstat gstat certifications is there any point in getting these certs from asa or rss i see some people have it but this doesn t seem like industry standard as the cfa cpa is in your opinions are the benefits afforded by these certs worth the annual fees
2,google survey explores methods for making dl models ‘smaller faster and better’ researchers from google conduct a survey on how to make deep learning models smaller faster and better the team focuses on core areas of model efficiency from modelling techniques to hardware support and open sources an experiment based guide and code to help practitioners optimize their model training and deployment here is a quick read google survey explores methods for making dl models ‘smaller faster and better’ the paper efficient deep learning a survey on making deep learning models smaller faster and better is on arxiv
0,is it really possible so for the past 3 years i have worked in multiple dead end jobs due to having no goal or career in mind leading to not going to college or university which i really do hate myself for worked in a bar two warehouse jobs and now work in an apprenticeship in a financial services company as a quality assurance analyst as part of my apprenticeship i take part in courses where they are relevant to our jobs and i was introduced to data science which caught my attention we get told what it is it’s purpose the jobs that come under it and i don’t think i ever felt in love with such a thing before in my life fast forward three months i have signed up for online courses been reading articles programming languages and it’s never gone away and that’s rare for someone like me with autism and adhd but one thing i keep seeing is articles and people saying “how to become a data analyst with no degree or experience” this has me interested but also very wary due to the current work climate and strict job requirements so i want to ask everyone is it really possible to land a job as a data analyst with a portfolio background with no experience and no degree i’m happy for the job i am grateful to have kept since pre pandemic but i know i wouldn’t like to stay in a job like that or in the same company which is pretty terrible to staff if any advice or knowledge can be given as well it would be much appreciated in my career path thank you
2,seminal papers on machine learning for audio processing anyone have recommendations for seminal interesting papers or researchers in the field of machine learning for audio processing also papers that are not explicit on audio processing but that might also be helpful thanks
0,are data science skills transferrable to regular swe roles i might be getting an automation data science python job soon and it s full time and i m still in school full time to me this job would mostly be worth it if the experience here would be sought after by future swe roles that i apply at when i graduate is this the case i love automation with python but am not personally a huge pandas numpy guy myself
2,youtube discussion tree api hey there this last days i ve been working on a python api that allows you to obtain the discussion that occurs in the comments of a youtube video as a tree structure youtube data api doesn t give enough information in order to construct the full conversation tree because when you enter a reply to a comment that is a reply to another comment youtube doesn t match the parent reply id as the one that you are replying to instead it automatically puts the id of the top level comment so i made a library that using youtube data api and an algorithm that automatically resolve this kind of conflicts let you download the full discussion tree that happens on a youtube video this has been made as my final degree project for a research group of my uni as a module of one of his projects that is about analyzing the argumentative and the discussions that take place on social media i thought that could be cool posting it here in case there is someone looking for something like this you can go check it out at or if you have any comment on the implementation or you want to share some features that can be added to de library hit me up any kind of feedback will be pleasantly accepted
1,are statistical models able to make predictions about individuals are statistical models in theory able to make predictions about individuals suppose you have an individual with observed covariate information x a y b z c in theory can a regression model trained well on some data predict the expected value of this individual s response variable i heard today that statistical models are not designed to make predictions about individuals they are only designed to predict the average behavior of a large group of individuals and in theory should not be used to make predictions about individuals is this correct does this mean that any time statistical models are used to make individual predictions this is going against the intended use of statistical models thanks
2,who said ai art has no soul a summed up history of progress in the ai art world x200b medium ai art when i started experimenting with a i art back in 2018 the compute power was barely enough to generate blobs it wasn t too different for the art stars of those days who produced fairly appalling aesthetics edmond de belamy anyone the issue with this is that it stuck in the art commentators mind as gimmicky and far from the real thing the art created by trained artists but we had to start somewhere a lot of water has passed under the bridge since then and today it is difficult to tell the origin of an ai work of art was it created by a human was it created by a machine next step in this movement is the merging of a i art blockchain and the delivery of a i art as nfts one day my real hope is that a few human artists will draw what the a i has helped them imagine it is an amazing tool to enhance imagination the medium article illustrates those aspirations
2,synthetic data for agriculture an example with houdini and arabidopsis x200b i haven t seen too many examples of synthetic data in ag particularly with houdini which i ve found extremely difficult to learn but very rewarding
1,what level of abstraction in linear algebra is appropriate for coursework in a statistics phd program really is two questions 1 in a stat phd program to what level of abstraction in linear algebra do the courses typically get i find linear algebra done wrong and axler s linear algebra done right on two levels with the latter having more difficult exercises 2 what is the expected level linear algebra training for a student beginning first year curriculum in a stats phd program is it at the level of say strang or at the level of axler and specifically what parts of linear algebra are particularly important in the first year i suppose we wouldn t be doing too much gauss jordan elimation or reproving rank nullity
1,testing multicollinearity homoscedasticity in a structural equation model i m unclear about testing these assumptions on a model without clear independent dependent variables which observed variables should i be testing for multicollinearity and for homoscedasticity do i just plot the residuals for relationships between all observed variables any advice you have would be amazing tia model here
2,consistency regularization for variational auto encoders
2,should we tell them that they re optimizing the wrong metric source they are focusing on precision when they should be focusing on recall which isn t that great or f1 beta the team noted that for their use case the worst outcome is a prediction of no damage for a storm that does indeed cause damage the predictions are based on a support vector classifier which achieves 81 precision and 61 recall
1,sampling variability from collapsed survey data hi all i m using some survey data of individuals that have weights let s say i wanted to collapse this individual level survey data to the geography level for the purpose of learning about locations not individuals or individual weighted locations after doing so with the appropriate weighting option is sufficient to regress my now collapsed variables simply as reg y x ignoring survey clustering and stratifying i can t help but suspect that such a regression does not capture the sampling variability from the initial survey the initial survey features uncertainty regarding population means but after the collapse there is no uncertainty in principle of course i could be wrong but this has been picking at my brain for a while thanks so much nb this is not a coding question but you can surely see i m using stata here
0,designing a seminar for undergrads greetings i am working on my phd i have the opportunity of doing a seminar and wanted to introduce the fundamentals of python for ml the situation is that i have no experience developing course content for bachelor students i have been guest lecturer for m sc but rather on applied modelling i received most of my education via on job trainings such as bootcams one on one trainings paid by the project or by jumping in projects i have the idea to ejem let me be inspired from the famous udemy tutorials but i am unsure if the content can be fit for engineering undergrads anyone has any other suggestions i will do this as volunteer and will nor want to not receive any compensation i want this on my resume
1,question about linear mixed models for longitudinal analysis from my understanding using mixed effect models is one way to look at longitudinal data let s say that i wanted to look at student s final exam score vs the amount of time they spent studying over months so i repeatedly measured their time studying and then i looked at their final exam score is the mixed effect model appropriate in this case i m not sure if the dependent variable can just be one point
1,question how can i create a dataset featuring clusters with inhomogeneous internal densities with python do you guys have any idea sklearn doenst have a built in library to do that
2,similar image retrieval i am trying to build a similar image retrieval system where given an image the system is able to show top k most similar images to it for this particular example i am using the deepfashion dataset where given an image containing say a shirt you show top 5 clothes most similar to a shirt a subset of this has 289 222 diverse clothes images in it each image is of shape 300 300 3 the approach i have includes 1 train an autoencoder 2 feed each image in the dataset through the encoder to get a reduced n dimensional latent space representation for example it can be 100 d latent space representation 3 create a table of shape m x n 2 where m is the number of images and each image is compressed to n dimensions one of the column is the image name and the other column is a path to where the image is stored on your local system 4 given a new image you feed it through the encoder to get the n dimensional latent space representation 5 use something like cosine similarity etc to compare the n d latent space for new image with the table m x n 2 obtained in step 3 to find retrieve top k closest clothes how do i create the table mentioned in step 3 i am planning on using tensorflow 2 5 with python 3 8 and the code for getting an image generator is as follows image generator imagedatagenerator rescale 1 255 rotation range 135 train data gen image generator flow from directory directory train dir batch size batch size shuffle false target size img height img width class mode sparse how can get image name and path to image to create the m x n 2 table in step 3 also is there any other better way that i am missing out on thanks
1,how many times should i use the nonparametric related wilcoxon when having many groups and just 2 pre and post my questionnaire is between subjects where everyone answers the same pretest questions then they get randomly assigned to read view only 1 of the total 4 stimuli and then everyone answers the same posttest questions based on the paper i’m using the likert questions from they averaged the pre together and the post together im aware likert is ordinal but all these peer reviewed papers did it to a total average for the pre and an avg for the post i know this warrants a paired test and i was using the wilcoxon signed rank test bc data is not normal and sizes are a little small however here where my question resides i have many many groups i wanted to analyze the differences between each of the 4 stimuli and across the 4 stim there are commonalities for which i wanted to group based on too so say stimuli number 1 can be combined with stimuli 2 3 with 4 1 with 3 and 2 with 4 so that’s 8 also i was interested in breaking apart the pre and post test questions to look at each one individually 3 questions to see difference between pre and post so even though there’s an absurd amount of groups i still just have 2 paired measures so i don’t think i need friedman’s i just have a shit ton of groups with some overlap i’m assuming this introduces problems or can i run each test for each group and so on also bonus i’m having trouble understanding hodges lehmann i always get an estimate of 0 000 and usually the lower and upper bounds are also 0 000 for some upper was 5
0,where are my bayesians at my current job i ve been using bayesian structural time series models to hopefully deal with covid related outliers more gracefully in forecasting curious if anyone here has found a way to incorporate bayesian approaches into their work if so what applications are you using them for
2,adaboost algorithm can adaboost algorithm return the same weak classifier after round 1 and round 2
1,appropriate statistic for measuring something over time what would be the appropriate statistical method for measuring something over time i was thinking paired t test or regression but i’m under the impression they require a comparable set of data that isn’t just a year progressing
0,standard python resources for data modeling are there any mainstream python libraries which given data from some source suggest normalization schemes for data e g recommended table structures for a 3nf or star schema in an rdbms profile relationships between fields for cardinality x of the time this field has a 1 1 relationship with this other field the remaining y are missing data and 1 many relationships or complete other data modeling related tasks i could cobble together some of this functionality using builtins pandas numpy etc but am looking for industry standard tools that data scientists use for the kind of “lite” data modeling that comes up on the job people’s personal github repos for these tasks are ok but not exactly what i am looking for here are some sample cases to clarify 1 you received a huge raw denormalized extract from somewhere and will continue to receive incremental files on a regular basis you want to create a profile of the initial data use said profile to make some tradeoffs to “tidy” the data into an rdbms suitable format maybe force 1 1 relationships where they exist 99 of the time for instance or fix overlapping datespans and then monitor subsequent incremental files to ensure the underlying data profile had not changed drastically 2 you received access to a new database with no documentation and little support from dbas or the business on structures assume there are not keys or constraints defined in the rdbms itself to leverage perhaps the database was created maintained by a skilled business user without dba level skills you would like to create an erd or some other documentation on this database quickly the focus on rdbms as the endgame is because the goal here is to support analysis by a bi team that is skilled in sql but no other programming languages
1,continue my formation hello first of all thank you for taking the time to read this i have been studying stats for the last year from probability ci hypthesis testing inference and regression i have also done a bit of time series analysis and an intro to bayesian statistics mainly all through coursera the thing is that i want to dive even deeper into stats specially glm anova mixed effects what would be the path to follow is there any online course which you will recommend is there logical path that i should follow thank you so much
2,deploying ml model for inference on user devices v2 hi all this is technically a repost of a previous post i m still looking for an optimal way to be able to infer my convnets and gans on user macos and windows devices using their gpus i went through torch to tflite conversion but it doesn t seem to support user gpus and frankly i m stuck looking for optimal solutions any suggestions are super welcome thank you
0,convincing sme depite good result model i m in the situation where sme refuse to believe the model i created despite the good score 70 f1 score now im clueless on how to convince them as a background i did my degree in the related field so i have good understanding about the input data plus via eda i can clearly see the seperation between classes in output so i m not using deep learning to develop thr model just simple logistic regression hence the result is pretty easy to intepret and present have you encountered this situation how you go about convincing sme
2,key computer vision trends in 2021 hi data scientists i wrote an brief article based on a talk about key computer vision trends in 2021 given by sayak paul who s an ml engineer what trends do you think are missed out let me know in the comments thank you article
0,data science in practice i am a self taught data scientist who is working for a mining company one thing i have always struggled with is to upskill in this field if you are like me who is not a beginner but have some years of experience i am sure even you must have struggled with this most of the youtube videos and blogs are focused on beginners and toy projects which is not really helpful i started reading companies engineering blogs and think this is the way to upskill after a certain level i have also started curating these articles in a newsletter and will be publishing three links each week links for this weeks are 1 a five step guide for conducting exploratory data analysis 2 beyond interactive notebook innovation at netflix 3 how machine learning powers facebook’s news feed ranking algorithm if you are preparing for any system design interview the third link can be helpful link for my newsletter will love to discuss it and any suggestion is welcome p s if it breaks any community guidelines let me know and i will delete this post
2,most effective algorithms for multi output classification tasks discussion based on benchmark datasets what algorithms perform the best in an array of multi output classification problems i have found limited research papers exploring a diverse spread of algorithm families on multioutput problems i would like to guess the the best in the industry is a convolutional neural network or simply a deep neural network all the same i would like some input thank you
0,standard approaches for binning data suppose you have a dataset where some of the predictor variables are categorical and have hundreds of possible discrete values are there any common ways to bin all these values into general groups e g suppose one of the variables is 50 a 25 b 20 c 0 5 d 0 5 e 0 5 f etc could you reformat this variable as a b c other is this a common technique is this acceptable some statistical computing software can not always handle so many categories other times perhaps it is advantageous to bin many low frequency entries together to facilitate statistical modelling thanks
1,changing a tyre turned into a stats problem on my head okay so i have to change a tyre on my car and i realised there s kind of a hidden stats problem here for the sake of the stats problem i must change my tyre within 48 hours and every hour on the hour the weather changes at random and stays that way for an hour based on observing the weather i can give it a rating between 0 very bad and 10 very good and my goal is to have the highest possible chance of changing the tyre in the best possible weather for the sake of simplicity changing the tyre will take exactly one hour weather forecasts do not exist and nothing will cause me to become unavailable to change the tyre which stats problem have i stumbled upon what is the algorithm i should follow how does the algorithm vary depending on how long i have to change the tyre i know this is overthinking it i just changed the tyre now but was still interested in the problem i thought up
1,question how do i prepare for a phd thank you to everyone who upvoted or commented on my last post about whether or not i should apply for a phd in statistics i have decided to go for it and i m very excited i will take care of the gre transcript letters of rec etc and apply this winter for the following fall i like to read in my free time to learn now i am thinking about tailoring my reading diet to be more stats focused could you guys recommend any reading course materials for me in preparation for a stats phd here s some background 1 i studied math with stat minor undergrad and did very well but i ve been in industry 3 years and i bet certain instincts knowledge have atrophied 2 back in college i took all the math stats classes phd programs usually recommend linear algebra real analysis number theory proofs mathematical stats stochastic processes regression advanced regression time series probability theory etc 3 i taught probability theory to undergrads for 3 semesters after i took the class so that s pretty much burned into my memory everything else will be weak 4 i am pretty skilled in data manipulation in r and python and done some real work modelling in r any non fiction books textbooks online courses or lectures you guys would recommend for me to brush off the dust and get ready for phd level coursework all advice is appreciated thank you
0,what file format do you like to save your python numpy pandas data i m just wondering what people like to use for saving data typical conventions i ve heard of are pickle python shelve csv sqlite3 sql why not throw it into a database hdf5 json numpy s npz format just wondering for fast reading writing what options do you like to use
1,what advice would you give to someone who is going to start their undergraduate in statistics this year
0,data science coding standards i know there s a lot of different tasks people do but i ve generally worked on code bases that utilize many models and our data has hundreds of variables i ve seen some guidance that if you ve written any more than 10 lines of code it should be its own function and that just seems insane to me obviously it s a balancing act and you shouldn t have functions with thousands of lines of code but i ve seen plenty of 100 200 line functions that generally define either a clean group of variables say defining the 15 inputs to a specific model or a business process that s a bit complex but one clear process as you d explain in english i ve seen code that follows the paradigm of keeping functions short and i ve ended up trying to search for how a variable is defined and literally following a path of 10 functions to find the answer i know it s a balancing act and you can say it always depends on details but i didn t know if people could share their thoughts and whether in their actual day to day work they tried to follow the small function paradigm or whether the way i work is closer to how people handle their coding standards edit seen some posts about functions doing 1 thing and that s basically what i m trying to figure out is prep data for model x considered one thing say you have 15 variables that all take 1 5 lines of code and in my current project we have hundreds of models with 15 variables each and not a ton of overlap my opinion is that a function that defines those 15 variables is doing one thing and much easier to read understand and maybe have to use the scroll wheel once or twice than to trace through different functions which are often factored into different code files do people tend to agree with this if not how would you refactor such a function would you just have functions called prep variable1 prep variable2 etc some of which are one line long and called exactly once in your code base
0,jira for data science i am a data scientist and in every company i ve worked for in the past years my team had to use jira for project management i am not a big fan of it and i think it doesn t really fit data science teams needs i am curious what tools others are using to manage their data science work if you use jira do you enjoy it if not what tools do you use how do you organize your workflow also i had a hard time convincing engineering managers to start using other tools because the whole company is using jira have you managed to transition away from jira and how did you convince your managers
1,statistics internship advice hi for a current undergraduate freshman looking to major in statistics what advice would you give me to set myself up to be a solid data science statistics internship applicant i have basic skills in r java and will be taking an online course in sql what else should i be doing or do you wish you had done thanks
1,interaction and squared terms in regression how do i go about finding interaction and which terms to use as squared terms within a model i have a model with 12 relevant variables is it just common sense or is there some formula that could be used to determine whether an interaction can occur or squared terms would make sense
1,multiplier for average growth i have a list of student test scores ranging from 0 20 20 being the highest score a student can receive i am looking to find the average growth of students and with all students included my average is 5 5 points from pretest to posttest where i m struggling is that students who scored 15 or higher on the pretest are bringing down the average even if they grew some up to 20 which is the maximum how can i account for this to find a true average growth score do i multiply growth points by some factor for any student who started at a 15 16 17 and so on if so how do i know what to multiply it by
2,kubesurvival easy k8s cost optimization useful for clusters with a lot of ml training jobs model servers just wanted to share a cool new open source tool i built to significantly reduce kubernetes compute costs by finding the cheapest machines that successfully run your workloads it s designed for clusters with a lot of ml training jobs model servers these can get really expensive especially if you use gpus check it out
0,how hard is powerbi to learn coming from plotly dash so i’m looking at this job and they mention wanting tableau or powerbi prefer powerbi so i feel like that’s the big one how long would it take to pick up enough power bi to feel comfortable listing it in on a resume tableau took all of a few hours to pick up the basics and i’ve heard powerbi is even easier edit should probably mention i’m a ds with 6 years xp
1,q mlr with categorical predictors i m running an mlr on mixed churn data after using pairwise selection to reduce the ivs i m left with only categorical variables to predict monthly charge i think this makes sense intuitively and the predicted vs actual plot on my test data is very close but i just want to make sure i m not missing something the plot does not have the typical cloud of points i m used to seeing there are 2 perfect lines of points slightly above and below the y x line
2,real2sim interactive demo with aogmaneo link to blog post and interactive demo this is a real2sim demonstration not to be confused with sim2real i made using the online learning biologically inspired fast learning system i work on called aogmaneo the demo is interactive and can be controlled with wasd it works by learning a world model for a real life robot arm environment by associating my gamepad commands with the visuals it was seeing through a camera it learned a controllable model that i then created a webassembly application for it s far from perfect but you can notice that it learns some basic physics especially with the marble since it always rolled left in the real environment and does in the simulation as well it has trouble with illegal actions that are very out of distribution resulting in some awkward behavior sometimes to create this demo i controlled the arm for about 5 minutes if you want to know more about how aogmaneo works here is a guide let me know what you think
0,why do you think data science became this sexy job and what job was hot before this happy sunday i was hoping we d have a light discussion before jumping in the week tomorrow i m 26 right now when i was in my undergrad i remember android development or some kind of website development were pretty popular among the college students today i was wondering that apart from money what makes a job hot would love to hear from you guys especially who have been in the market before data science even existed thank you
0,how important is knowledge of statistics really let me start this off with a disclaimer that i m still a beginner in data science and i haven t been exposed to many projects so far the real life projects i ve worked on concerned time series data energy demand and price forcasting supply optimization quantitative analysis for algorithmic trading which i imagined to be heavy on stats before i began however after i ve worked on those projects i feel like knowing or not knowing stats doesn t really affect my ability to complete the projects i m still able to analyze the data well gather actionable insights and build models around those insights to optimize processes for example linear regression is constrained by its assumptions so what we want stationary time series data for arima based models so sample statistics are not time dependent so what bayesian methods like mcmc allow us to sample a distribution similar to the actual distribution due to the law of large numbers so what don t get me wrong i really like learning about statistics as it fills me with a wondrous sense of appreciation every time i understand the underlying reasons behind why certain models work but i m just really curious as to why statistical knowledge is so valued in this field when based on my experience so far it doesn t really affect the quality of your work as long as you know when and how to use statistical tools models even if you don t know why
2,fast and accurate camera scene detection on smartphones abstract ai powered automatic camera scene detection mode is nowadays available in nearly any modern smartphone though the problem of accurate scene prediction has not yet been addressed by the research community this paper for the first time carefully defines this problem and proposes a novel camera scene detection dataset camsdd containing more than 11k manually crawled images belonging to 30 different scene categories we propose an efficient and npu friendly cnn model for this task that demonstrates a top 3 accuracy of 99 5 on this dataset and achieves more than 200 fps on the recent mobile socs an additional in the wild evaluation of the obtained solution is performed to analyze its performance and limitation in the real world scenarios the dataset and pre trained models used in this paper are available on the project website arxiv paper project website mobile ai 2021 challenge on quantized camera scene detection paper website
0,visualising anatomically linked data interactively i am a phd candidate in pharmaceutical science with a dataset from a very wide lit review meta analysis the dataset describes the concentration of a protein in all different tissues and cells of the body i would really like to make some kind of interactive visualisation like a shiny dashboard using r of this data where hovering over different regions of a human silhouette would show summary statistics for that region mean max min concentration ideally clicking the region would zoom into it and display all the values references for the papers they were obtained from i ve done a good amount of data wrangling processing visualisation etc as part of my phd in both r and matlab and a little python is my goal a achievable at all and b achievable with these tools if so what packages modules would you recommend if not i guess post f in chat 3
2,how do models like arma and arima fare against sporadic memory is it fair to assume that standard time series models like the arma and the arima model are not well designed to handle sporadic and irregular memory patterns as i understand these models are usually used to handle data with well behaved notions of trends and seasonality e g you specify these in a given arima model when you start to deal with more complicated and irregular patterns do arma arima models tend to perform poorly was this the motivation for eventually moving towards neural network based models e g rnn lstm for time series analysis x200b thanks
2,paper recommendation on causal reinforcement learning hi guys so i m currently a master s student in machine learning computer vision specialisation and it s about time i start with my thesis the thing is my knowledge so far has been heavily emphasized on computer vision related deep learning but i m still a virgin regarding causal inference however my ultimate goal for now is to do phd in causal ai especially in causal reinforcement learning i am very interested in ai in games as well so ideally i would love to try to teach ai how to play basic games in a simulated world and learn through reinforcement learning as well as having the concept of causality so i want my master thesis to be the first stepping stone towards my phd proposal the thing is i m not sure if this sounds too simple or if its sounds too much i have read and studied about causal inference and i have done projects on reinforcement learning but i think i m still stuck on how to combine the two together at this point i have been looking through papers in the past few days but i don t think i have looked deep enough yet if anyone here has any recommended papers interesting projects that could help me or even ideas on how to improve my current master s proposal i ll be extremely appreciated it thank you in advance
2,machine learning interview book by huyen chip i have just skimmed part of the book but it looks very good and contains lots of insight from a recruiter point of view that i would never know otherwise and is applicable to more than just ml interview imo what do you think quote from the github page this book is the result of the collective wisdom of many people who have sat on both sides of the table and who have spent a lot of time thinking about the hiring process it was written with candidates in mind but hiring managers who saw the early drafts told me that they found it helpful to learn how other companies are hiring and to rethink their own process the book consists of two parts the first part provides an overview of the machine learning interview process what types of machine learning roles are available what skills each role requires what kinds of questions are often asked and how to prepare for them this part also explains the interviewers’ mindset and what kind of signals they look for the second part consists of over 200 knowledge questions each noted with its level of difficulty interviews for more senior roles should expect harder questions that cover important concepts and common misconceptions in machine learning
2,has anyone used visdom on google collab project i m using a gpu hosted runtime on google collab and trying to use visdom for visualisations i started the visdom server using my computer s terminal and hosted my localhost using ngrok and passed the generated url inside visdom visdom in google collab i m getting an error saying connection timed out and as a result another error saying visdom python client failed to establish socket to get messages from the server this feature is optional and can be disabled by initializing visdom with use incoming socket false
1,favorite classification metric for seeing improvement over imbalanced baseline q what is your favorite evaluation statistic metric loss for quickly seeing if a model is doing better than just predicting the majority class in cases of imbalanced classification data x200b i lean towards towards roc auc in general but it doesn t necessarily give an intuitive score 0 5 model is doing better than predicting all 0 one solution would be to compare the model to the dummy classifier each time per metric but ideally i d like to use something where i can tell at a glance if the model is doing better than majority class guessing across a wide range of tasks
2,interesting reinforcement learning applications can you mention any interesting application of reinforcement learning you have heard of or read about i have experience in the topic and wanted to see what applications i could pursue
0,optimizing based on all multiple counteracting factors hi all i am working on a problem for a client wherein they want to streamline sourcing of their products based on 3 factors cost of goods lead time risk factor to fulfill the demand x200b i tried approaching it in the conventional linear programming model specifying constraints and minimizing cost but this method has a drawback that it is optimizing on 1 function i e cost and doesn t take it into account risk lead so i will get lowest value but my risk lead time is high i am looking for an approach which gives balanced answer x200b i am thinking i should create a new objective function which is a mix of all the factors like objective qntity country1 qntity country2 risk country1 risk country2 but i cant understand if i should maximize or minimize this x200b any thoughts on how can i proceed
1,question what effects to these have on regression models so let s say some variables have a high skew value it means the model is not symmetrical skewed to the right it has outliers but what other implication does this have on a regression model like how and why would this affect a regression model
0,how do you remember all the data science machine learning terms been studying data science machine learning for about a year now and struggling to remember all the terms and information like what each model does validation methods statistics etc how do you remember all these terms what they do and when to use them
1,how do i support using the median instead of the mode as measure for central tendency in 5 point likert items i chose to use the median as measure for central tendency instead of the mode i can t however find any literature supporting use of the median over the mode or visa versa is there a clear explaination on why the median is the better choice or is the mode perhaps a better measure
0,made an interview for a company their project does not seem doable this week i ve had an interview with a company that manufactures wooden floors they want to hire a data analyst software engineer to digitilize their manufacturing processses within their factory they told md which improvements they had in mine and some of them seemed completely okay like installing sensors around the factory and gather data from those sensors yo atart building some big data bank the second is that they want to build applications for internal consumption for other workers to be able to better visualize what s going on in the factory however their main focus was some other thing they wanted to do was to build from zero a computer vision app in which you could take a picture of raw material and it wpuld predict its prone to failure or not the problem is that they don t hsve any data for that and i dont think they are well aware of the effort it would take not to make the app itself if possible but to build a dataset large enough to be able to train the model the contract they are offering is just 6 months so i assume they need to see results within that time frame i don t their degree of expertise in the topic but to mind it seemed that they are not being realistic about this and the contractual relation wpuld end up rather bad what do you guys think
1,survey weighting is a smaller sample with low average weights or a larger sample with larger average weights better to generalise to the population hi everyone as i work in social survey research i often weight samples to be representative on several fronts this is very effective as we tend to have a problem recruiting specifically lower educated and they are underrepresented in samples compared to the population for a study i am doing i have a sample of 10 000 respondents but lower educated are quite underrepresented with these respondents having an average weight of 6 one of my colleagues suggests to randomly sample from my high and middle educated respondents to decrease the sample size such that the proportions will be closer to the population and weights will be smaller this will lead to a smaller effect size etc however i do not see the use for this unless the effective sample size actually goes up which will decrease the margin of error on my results my idea is to keep the originally weighted data n 10 000 design effect 5 and use analyses adjusted for weighted data eg survey weighted confidence intervals that reflect the original smaller n that was blown up to be representative as i do not see the use do any of you have advice as to which of both methods would be the most helpful to check the effects of a certain variable in the population
1,introduction to computational statistics using pymc3 by srijith rajamohan the purpose of this series of courses is to teach the basics of computational statistics for the purpose of performing inference to aspiring or new data scientists learn the basics of pymc3 for various bayesian modeling including linear regression hierarchical regression classification robust models and assessing the quality of models source the enrollment is free
0,data science salaries in india for those from india can you share the base salary you are earning with your years of experience i ve been offered a raise at my company and want to understand if i m being offered according to market standards as for my background i m an iit graduate and have 3 years of experience i m currently working out of bangalore india my current base salary is 19 lpa which is being revised to 25 lpa 34k usd 24 fixed 1 variable from april 21 any data would be helpful thanks
1,ability of statistical models to extrapolate to new data recently i have been reading about some interesting research that talks about the theoretical limitations of certain machine learning algorithms to effectively extrapolate to new data e g decision trees do not generalize to new variations for example researchers mathematically proved that decision trees can not generalize to data they have not seen before i e an infinite number of points is required to guarantee a certain level of generalization another interesting result is runge s phenomenon that shows polynomial are likely to highly oscillate and become unpredictable outside of data they are exposed to 2 questions i had 1 does anyone know if piecewise splines suffer from the same theoretical constraints such as runge s phenomenon or does strategic combination of many splines somehow mitigate this problem from the wikipedia page on spline interpolation splines apparently have an advantage over methods like polynomial regression since they can achieve similar errors but maintain a low degree than polynomial regression models e g a popular choice of splines are cubic splines i e 3rd order degree splines it seems to me that this might be related to the bias variance tradeoff that is it might be beneficial to approximate the target function using several 3rd order polynomials i e splines compared to a 8th order polynomial regression model are splines known to struggle with extrapolation 2 i have been reading more on the ability of neural networks to extrapolate to unseen data supposedly neural networks are optimistically described as some of the few machine learning algorithms that at least have the theoretical structure that could permit extrapolation apparently this is due to properties such as adaptive basis functions and distributed representation these allow neural networks to better handle adversarial attacks e g small changes to inputs are less likely to produce drastically different results require fewer examples to generalize better bang for your buck and are able to use all the variables in unusual combinations to make more complex decision surfaces that can better model complex data all in all is this correct thanks
0,not sure where in the spectrum i fall background i was hired as a data analyst a couple of months ago it s a startup with a small data science team the team is really endearing and i absolutely adore them that sounds all good i love the work i do as it s writing code and i love writing code but i m not doing any data analyst work i write etl processes come up with formulas to do certain calculations fix historical data because they were calculated wrong i e fix formulas that are close enough but there are better ways to calculate them and reprocess the entire data and writing sql queries to see things i wanna see the closest to what we can call analysis i do is to verify if the etl process i wrote shows updates the graphs and numbers in the platform and if they re correct so what exactly am i edit grammar
1,var vs vecm models what are the advantages and uses of vecm models how do they compare to var models when would you use each of them can anyone summarise the purposes of these models
2,‘minecraft’ structures that build themselves hello guys we are going to have sebastian risi to discuss his latest research on ‘minecraft’ structures that build themselves and regenerating soft robots through neural cellular automata if you have any questions you can send them here for more details 1 growing 3d artefacts and functional machines with neural cellular automata 2 regenerating soft robots throughneural cellular automata x200b x200b x200b
0,how was your first job out of college ive been at this job for about three months now and i feel like i’m not being challenged i get assigned work that takes me about four hours to complete then the rest of the day i’m not doing anything except maybe 3 4 meetings that last 20 mins and then im done i also spend most of my time just cleaning and pulling data in excel which often makes me think why i’m being pay well when this stuff is so easy is this the norm for a jr data analyst role or should i find another job
0,data logging there is not much available on internet about data logging any resources or explanations from anyone which can sum it up
1,research which research areas are currently hot in statistics at the moment or upcomming
0,tabular sequential and image data is there any other type of data are there any other types of data that we interpret with programming i assume theirs multiple different types of tabular data sequential is anything with a time series such as audio image is a 2d picture is that all their is thankyou
1,question about determining statistical significance in my free time lately i ve been messing around with the idea of algotrading basically using an algorithm to find an investing strategy with that being said establishing statistical significance for a given strategy is paramount to ensure that your strategy is actually legit and not merely due to chance especially when you are running hundreds of different trials on various parameters so my question is as follows if a stock over the last year has increased in price say 195 days out of 365 and i have found a potential strategy that says after some indicators have been reached the stock in the past year has increased 36 days out of a total of 60 days where these indicators were met how would i go about determining whether this would be statistically significant or not i ve been researching the p value and the bonferroni correction which i know i need to make use of but i am struggling to find the exact method by which i should calculate these things especially when my desired results are binary price increases price does not increase thanks
1,what exercises to do in casella berger hi i am to start my msc in machine learning late this year however my statistics background is weak as i took a few years to work in industry after much searching i have been told that there are two books i should work through 1 casella berger 2 degroot and schervish both these books have a lot of end of chapter exercises as my time is limited i would like some sort of guide to which problem i should solve one of the resources i found was this this lists all the exercises i need to do as i work through 2 is there something similar for 1 that anyone here is familiar with would more experienced folks recommend working through 1 as opposed to 2 any pointers would be much appreciated
2,continuous instrument continuous treatment is there an ml causal model to help with this hey thanks for reading it and even more for any help i work with historical data percentage of slave population in my country s counties and its relation to homicide rates nowadays as when my interest variable was collected many counties didn t exist yet there are many many zeros in this predictor i am using inequality gini index as endogenous treatment and slavery as the instrument both are numeric i would like to employ a causal approach to get the indirect effect of slavery inequality is visible and present different from historical data is there an ml method that can handle instrument and endogenous as continuous numeric variables my advisor asked me to use data science and not only classic econometrics so we can have a kind of robustness check with the test group and in the face of so many zeros if you have an r solution it would be better but if there s no other way god help me with python thanks again
0,i got blocked from an online data conference for loving data too much i don’t want to say much more than that but i was at an online conference a few days ago and in the chat i said i was totally stoked to be thinking about data analytics all day and they blocked me from the chat because they thought i was sarcastic i just love me some data is all so i reposted saying that i meant it and then the organiser thanked me i am just too hard core data data data nom nom nom
0,question on control variant testing non data scientist struggling to find the “right” way to set up populations for control vs variant testing and am wondering if my methodology holds any ground we’re targeting our best customers with a campaign and want to see how much incremental revenue it generates to find the control and variant populations i’m simply ranking the customers by their recency aov order count and other order economics to get to a weighted rank i’m doing this in sql which is the primary language i know then once i have every customer assigned a unique rank i choose only the odd numbered ranks and keep the even ranked customers as the control that is excluded from the campaign my thought is that this would provide an even distribution of our best customers in the control and variant groups when i plot the distributions for the different kpis the control and variant do look aligned but am not sure how confident i can be in the results does this methodology for selecting control variant groups seem ok
1,question shouldn t people use the right terminology when they say normalization or standardization i often note that people in the machine learning domain often with no statistics background define normalization and standardization such as below the two most discussed scaling methods are normalization and standardization normalization typically means rescales the values into a range of 0 1 standardization typically means rescales data to have a mean of 0 and a standard deviation of 1 unit variance i believe they are etymologically wrong in using the two terms alternatively here is my rationale a normalization in classical statistical sense normalization means x mu sd when statisticians says “normalize” they mean to say a distribution having mean 0 and variance 1 this is also known as standard normal distribution any normal random variable x can be transformed into a standard score or z score via the equation x mu sd if one thinks etymologically it makes sense to call the above process ‘normalization’ since normal here refers to the ‘standard normal’ b min max scaling informally referred as normalization by people in machine learning but i would call it ‘standardizing’ x x x min x max — x min the formula helps in scaling the values in the range of 0 1 and in a true sense ‘standardizes’ the values come to think of it how is scaling the values between 0 1 normal or normalization it simply does not stick or make sense i would hence rather call this process ‘standardizing’ as things are standardized in the value range 0 1 may be i am thinking from a pure statistical perspective i would like to know what the statisticians or people with statistical training here think
2,research on activity tracker for dogs hello guys i m developing an activity tracking app for pets i m looking for a dataset of accelerometer values of dog s activities but i couldn t found one if anyone having a dataset or know something please let me know thanks
1,any tips on rare event binary classification i have a dataset where only 5 of the samples have a positive value for the outcome variable i trained logistic regression and some other models but most of them result in predicting negative about 99 5 of the time should i lower the probability threshold for acceptance so that more positives are predicted how should i decide which threshold to use is there some method model that can deal with rare events
1,best next step to analyze this set of data hello i have a set of data which includes the daily number of cases of influenza in our hospital for a whole year we also have the number of diagnosis of fever recorded overall and some more symptoms how can i evaluate if there is a trend here i read some articles that used analyze it for some studies that are a bit alike right now i have the databse in excel the contents are dates fever diarrhea inflenza i want to see if the number of confirmed influenza as it was rising or falling if it followed a similar pattern to our fever cases we recorded should i use analyze it should i do a spearman analysis with it but thay will not give me the trend will it sorry if i don t make much sense i am still learning my way thanks
0,what is your preferred workflow for working with documents and code at the same time i assume at least 70 will say jupyter probably mostly with python maybe a few with julia another 20 will say r with rstudio etc a handful might say spyder or something i just want to say that i ve found nearly every stack at least as a beginner looking to get started fast is horribly convoluted i say that as a software developer who writes javascript applications a well known convoluted platform i just want to find something as simple as basic markdown that also lets me execute code and it doesn t require a special ide or a command for the code to run ie it continuously runs or it creates live updates on changes i really like the r markdown format but i don t love r itself and i hate having to use r studio it also doesn t seem trivial to set up live updates while editing i like python more as a data science language and it s cleanliness as a language also has a similar philosophy as markdown but there doesn t seem to be a document format that i like i don t want to write in cells a la jupyter or use an ide that controls where my cursor can go i want to edit raw code i even looked into writing code in javascript yes javascript math is ugly but i thought oh well at least i can easily write documents and upload it to the internet if that is my goal i tried mdx engines and you can t even write js in mdx at most you can write a component in a separate file and import it into the mdx file which is super bloaty so what are your thoughts on this is there any stack that has 1 markdown as a base document not cells i could be persuaded to something other than markdown but it would have to be something that is easy to write books and author blogposts with markdown seems like the best game in town for that 2 uses a decent high level language i m pretty flexible that can be executed within the document and ideally create visuals too 3 live updates to the executed code not just the markdown code whenever the file is saved or by some other similar mechanism 4 no major requirements on ide i am a big time vs code user and i don t want to change my ide just to write a little math
0,explain the need for non swe ml pipeline vs existing swe frameworks hi looking at metaflow tutorials and trying to see how different better it is from the setup below that can work in a swe based pipeline and if any reason to switch to metaflow or all its features listed below are covered adequately by this setup x200b 1 datawarehouse source of data pumped in from dbt pipeline 2 compute resources covered by autoscaling in aws 3 job scheduler quartz spring application reads configuration from a job database with a ui for entering schedules jobs schedules are decoupled from jobs can evolve independently without touching code most jobs are to send a message to a kafka queue to kick off spring batch or python ml code 4 architecture orchestration handled by java spring cloud stream data flows plus kafka topics producers consumers ml jobs are python classes in house python libraries so mostly in the ml pipeline only parameters are read from db and applied to the appropriate ml class emphasis on avoiding one off ml python procedural scripts data transfer between steps done using dbt sql is understood by more folks only ml steps done in python x200b 5 versioning inspecting results organizing results dbt python ml jobs results histories based on batch run id are stored and can be queried in sql micrometer prometheus metrics used to monitor success failed stages of all jobs sleuth distributed log tracing also used namespaces for development artifacts are simply git branches of each developer namespaces isolation when inspecting results is based on run ids user ids embedded in micrometer metrics and intermediate results of dbt python ml job histories 6 loading and storing data intermediate steps data is stored in database s3 7 loading and storing data again no need to reinvent the wheel gradle for java code pip aand requirements txt for python in all these steps using a mature framework like spring boot spring cloud stream makes life a lot easier this setup will allow use common workflows that are already well defined in swe rather than redefine new workflows in the existing infrastructure
1,can i run a multinominal logistic regression if the dependent variable comes from “check all that applies” question i’m looking for a way to analyze a dataset with multiple xs and a single dependent y which however comes from a “check all that applies” survey question i have decided to code these all that applies responses into three categories as it is about people’s attitudes positive negative and neutral however these are not mutually exclusive and person could for example choose both positive and neutral answer how do i go about analyzing this can i still use a multinominal logistic regression and if yes how do i treat this y or what are my options thank you in advance
1,question inference on normal multiple linear regression model parameters hi guys statistics student here really striving to understand how inference on the parameters of a nmlr model work i can t get what tests are meant to be used and why would appreciate if you could link me anything text videos which could make this argument less opaque thanks in advice
2,confusion matrix plot in few single shot learning in few shot learning fsl in every episodes they sample n classes say 5 out of all available classes and then model predict its output classes during model prediction stage all labels are reconverted to 0 4 for 5 way 1shot classification no matter what their actual value is i wanted to plot the confusion matrix but this is not possible as both actual label and predicted label are always in 0 4 range instead of actual labels is it possible to get model prediction in terms of actual label or any other pointer so that i can plot confusion matrix in fsl experiments
0,lowballed for faang ds contracting as new grad advice needed hi everyone i had an unusual situation happen in the past few days and i d like some advice a staffing agency in the bay area offered me the opportunity to interview for a ds role on a faang team that would directly impact a product that is popular worldwide think 100m users i like the role but am hesitant about it being a contract position considering i have a full time job lined up post ms in the bay area that is paying 135k 150k if you include benefits 170k if equity options aren t worth crap with a team i like though at a much smaller scale more relatively unknown company with far fewer ds to learn from the staffing agency told me the team wants to bring me in for an additional 7 interviews testing me on everything statistics ml product sense python sql behavioral but that the position would only be paying 120k i told her that is ridiculous since this is just a contract position and it would need to pay at least 180k for me to waste my time preparing and interviewing for the role considering i have a full time offer already i was told today they would match the 180k was i being extremely low balled initially the staffing agency is well known and i ve heard decent things about it for context this team has been looking for nearly a year for someone and i m the only person to make it to the final stage as far as i know do you think it is worthwhile to continue the interview process would you any advice is appreciated thanks edit 1 i d be a w2 employee of the staffing agency they d have the contract with the faang
1,question categorical variables sample sizes evening all i have been scouring the internet trying to find the answer to this question imagine we have two categories male and female i want to know if there is a difference between i dunno heights to determine if there is a difference between them one choice is the dmb ovs distance between medians over the overall visual spread if the sample size is 30 then the critical value is 1 3 if the sample size is 100 the critical value is 1 5 what i m stuck on is the sample size is this the overall sample size i e 100 males and females or is the 100 males and 100 females most of everything i ve read just says if the sample size is but does not specify if talking about the overall size or the group sizes any help would be much appreciated thanks
2,simplet5 train t5 models in just 3 lines of code simplet5 is built on top of pytorch lightning⚡️ and transformers🤗 that lets you quickly train fine tune your t5 models on custom dataset pandas dataframe 🌟github 🌟medium 🌟colab notebook
0,is it reasonable to ask for a full kaggle task as part of an interview process for a contract ds role i have been recently asked to do a kaggle task and 40 minute presentation and given a week to do it i think it is an overkill for a 3 6 months role how to politely decline it
0,question about data product hello guys i d like to discuss about data product concept or situation in a growing company so here is the situation i am is a one of the data lead in a fast growing company in southeast asia i bet in this year we ve doubled our mau and getting the attention of market but right now in our data team there are a lot of different spectrum that need to be achieved ranging from data warehousing data collection ingestion government into creating data product like recommendation system fraud detection etc the problem here is the data team doesn’t have the skill like project management or pm skill related like scrum or agile methodologies to perform those wide ranging tasks does anyone in this data science forum has a problem to integrate your expertise into your product or even doesn t have any idea to breakdown your analysis or model creation into several part of tasks cause i feel that we are a bunch of experts that having no strategical or tactical solution for managing project does anyone here has a same experience when serving data for product team or your company
1,so this is probably a frequent question but why do hypothesis tests require the assumption that n n 1 as i study for my ap statistics exam i would like to know the relevancy of this assumption
0,what is your current process like to source clean prepare and collaborate with datasets as part of a bigger project i m looking to put some effort into open sourcing a data sourcing and data collaboration cli tool the utility of the tool has been great limited enterprise and research settings as a sort of a shadow it tool that replaces emailing csvs around please note i m not a data scientists or engineer so i m looking to understand this use case further thanks edit if possible when commenting could you include organztion context for instance enterprise startup tech research or other
1,pair trading w mean reversion cointegration adf etc hi everyone at work i ve been tasked with a pair trading side project i m taking two pre selected securities finding their last prices over x number of years using pulled data from bloomberg and plotting outputting the buy and sell points throughout that time based on abnormalities in their ratios rolling z scores i m midway through an undergrad computer engineering degree so i have some background in programming currently using python excel and bloomberg for this project though have only taken one intro class in statistics and another in linear algebra over the last couple yrs my price data is all from bloomberg how i ve gone about it so far i scraped 200 securities from different sectors on bloomberg got their price histories weekly prices from 3 years ago until today on several different excel workbooks one for each sector and read those excel workbooks into my python program i ve maintained separate dataframes for each sector s securities pricing histories i wrote a cointegration function applied that to all pairs within each dataframe and for a p value 0 02 i kept those pairs i ve landed up with 100 pairs of securities that exhibit cointegration from there the user can input any pair on a separate excel sheet that will give the daily last prices of both securities over the last 5 yrs i take in the ratio of the two securities do a rolling z score test and if it s above below 2 sd i indicate a buy or sell opportunity for each security within the pair where i d appreciate guidance a number of my pairs based on pre cointegration test price ratios do not have normal distributions i d say around half i m confused on when i test for normal behaviour before or after cointegration what to test for normal behaviour the price ratios or each security itself how to test for normal behaviour in this context shapiro test something else and how that relates to a rolling z score is this even needed if i have a normal distribution test and or the adf test is my methodology for finding the buy sell points correct if anyone here would like to take this offline tutors i wouldn t mind chatting as a session or two going over this could be very helpful for me i ve read a lot in the last couple weeks but because i have elementary knowledge many concepts are starting to become jumbled up thank you
0,will palantir replace data science teams i ve been reading a lot about palantir lately and how they are creating software that analyzes data and creates models for you so naturally i ve been concerned that this new technology will eventually replace data analysts scientists i know right now they are only working with the government but from what i ve read they hope to eventually move into the private sector as well anyone have any thoughts on this and how this might affect the future of data science jobs i m worried the job will be made redundant by this tech and the more i read the more worried i get it just makes me sad tbh
0,tips on how to properly mentor and train junior data people my company hired a junior bi analyst a couple of months back to handle the backlog of dashboarding analysis demands i was not able to juggle in a timely manner due to more pressing data science engineering concerns i was the sole data person in the team at the time she s done a great job but now that most of the overdue demands she was given are done she has expressed interest in gradually transitioning to a more broad data role within the company most of her previous job experience is in a strict bi role dashboard building reporting etc but she has done a few python courses on her own under my supervision she has begun working on a few basic tasks like data cleaning pipeline building etc i can tell she s interested but her progress has been remarkably slow and underwhelming people who have more experience with mentoring juniors what are some things i can do to be a better mentor and help her develop in the field i try to give detailed feedback in each of her deliveries but as i said progress is slow so far edit i didn t expect this many good responses thank you so much starting next week i ll try to come up with a more active plan of mentoring based on the input you guys gave here s hoping this post can help more people who are or might eventually be in a similar situation
0,data science poc project tracking in an agile environment hello folks i work as an analyst and we are currently exploring some data project poc’s and explorative studies we don’t have a clear requirement from the stakeholders we run 2 week sprints and most of the engineering team have dedicated back log items and tracking how do you track data science exploration tasks in the backlog and how do you currently size tickets with something like an explorative task that has no specific requirement i’m struggling to set sizes as i uncover something everyday it may or may not be useful for the business but it’s an insight that has emerged the business thinks i’m not sizing tickets right any pointers is highly appreciated tia fellow data travelers best
1,is abstract algebra or topology more helpful in preparing for a phd program in stats i want to enrich my mathematical background before beginning a phd program i have found a couple courses i can take so far i have had some exposure to measure theory and have done exhausted the analysis sequence back when i was in undergrad i am looking for these things in this priority 1 usefulness in theoretical stats research phd level stats coursework 2 difficulty the more rigorous and difficult the better should i take abstract algebra or topology
2,news research high resolution photorealistic image translation in real time apply any style to your 4k image in real time using this new machine learning based approach the paper is high resolution photorealistic image translation in real time a laplacian pyramid translation network by liang jie and zeng hui and zhang lei 2021 they use high and low frequency versions of the image to optimize the computation time and resources needed it can run a 4k image in less than a tenth of a second with a single regular gpu code publicly available short read video demo
2,5 minute paper digest gans n’ roses stable controllable diverse image to image translation works for videos too by min jin chong et al your dream anime waifu did you ever want to see what you look like as an anime waifu thanks to the authors of gans n roses you can animefy pretty sure this isn t a real word selfies and even videos with a multitude of unique styles the authors from the university of illinois propose a new generator architecture that combines a content code computed from a face image and a randomly chosen style to produce consistent diverse and controllable anime faces with attributes matching the content image read the full paper digest reading time 5 minutes to learn about the encoder decoder architecture of the authors content style image generation method the tricks for ensuring style diversity and the losses required for high fidelity anime image synthesis meanwhile check out the paper digest poster by casual gan papers gans n roses full explanation post arxiv code more recent popular computer vision paper breakdowns cips simswap decision transformer
1,xavier and he weight initialization in neural networks i was reading this article here on xavier weight initializations in neural networks e g is this now the default for python libraries e g tensorflow keras when initializing weights in neural networks
2,styler style factor modeling with rapidity and robustness via speech decomposition for expressive and controllable neural text to speech new publication from interspeech 2021 we introduced styler which is non autoregressive based style modeling tts model paper demo code
1,question about analyzing results from stata my advisor has asked me to further analyze some results of a regression in my dissertation i have two ivs that i need to compare one is a count variable the other is a percentage variable i need to find out what a one unit change in the count variable is equivalent to in the percentage variable
1,question regression model with time series data i am a first year student so bear with me i have data that spans from 1980 2019 of major league baseball average game time and viewership share statistics among other league average statistics runs per game etc would it be incorrect to make a regression model with the game time as the x and viewership as the y inferring that a game time correlates to a in viewership furthermore viewership share is the of homes using their tv that watched the game so i am hoping this helps prevent the lurking variable of increased decreased tv habits over those 40 years i have all the graphs and the regression summary which appear to show a strong correlation indicating that for every 10 minute increase in game time viewership drops by 6 percentage points i apologize if this is extremely simple i m new to the field thank you in advance for any insight edit to clarify the data has 40 rows with each representing that years averages i e the time per game in a given row is the average of all 2430 games that year same goes for all the other stats
2,history of non parametric models in machine learning i have been reading about the use of non parametric models in machine learning e g kernel methods like svm kernel regression decision trees gradient boosting random forest and i tried to contextualize the reasons why these methods emerged in my head this is the conclusion i reached 1 parametric models like standard regression models are tricky parametric models require certain assumptions about the data to be true also require the analyst to manually specify interaction terms within variables but the biggest drawback regression models tend to require more beta coefficients to capture more complex patterns within the data a regression model with many beta coefficients behaves similar to a higher order polynomial function and higher order polynomials are notorious for behaving in very unpredictable ways outside the range of observed data runge phenomenon this basically explains why higher order regression models have a bad reputation of overfitting training data and generalizing poorly to new data this is all related to the bias variance tradeoff 2 non parametric models do not require interaction terms between variables to be manually specified e g in a decision tree you don t need to specify this the decision tree will try to recover these interactions by itself and have less stringent assumptions about the data e g choice of kernel the appeal of non parametric methods was an attempt to defy the bias variance tradeoff the idea of trying to make a less complex model with the ability to make predictions comparable to a complex model with the hope that the lack of explicit complexity leading to better generalization on test data 3 the popularity of neural networks a parametric model is due to the fact that researchers found out ways to make these explicitly complex models generalize to unseen data e g effective regularization methods is my interpretation of the history correct thanks
1,likelihood of winning an rng wheel not sure if this is right place or format but i am curious about the odds of something my friends and i are entered into a wheel 10 total participants we have 3 spots the wheel will be spun 5 times and each time a name is picked it’s removed from the wheel how would i calculate what are the odds of each of the following we win 0 times 1 time 2 times 3 times i believe i calculated the odds of winning once at 91 67 and the inverse winning 0 times would be 8 33
1,just a guy whos confused a poll reported 56 support for a statewide election with a margin of error of 2 89 percentage points how many voters should be sampled for a 90 confidence interval round up to the nearest whole number 90 1 645 i have been struggling with this for days and cant seem to get it is there a way to type this in excel or a calc
2,tsetlin machine framework attains f1 score of 0 901 on politifact and 0 896 on gossipcop with interpretable and rules new interpretable state of the art x200b tsetlin machine tm vs state of the art techniques the framework uses the conjunctive clauses of the tsetlin machine to capture lexical and semantic properties of both true and fake news text the resulting model decomposes straightforwardly into meaningful words and their negations for increased interpretability
2,hugging face has released an official course link the incredible team over at hugging face has put out a course covering almost the entirety of their ecosystem transformers datasets tokenizers accelerate model hub they also plan on hosting live office hours and facilitating study groups via their forums x200b ps if there s enough interest from apac regions i would love to help organise a study group i do not work at hf but i m excited to dive into this course
1,do i need to run a regression before checking for structural breaks question what i want to verify is if certain stocks experienced a structural break in 2021 as variables do i need market return the return on an index for example and the stock return do i need to run a regression on them first for example dow jones and amc returns amc being the dependent variable i guess i don t entirely understand what structural breaks are
2,thoughts on augmenting transformers knowledge graphs with each other this is something i ve thought about for a while transformers are really powerful and the larger models produce semi coherent responses to most questions as long as the question is somewhat straightforward who was the main hero of the lightsaber movie luke skywalker but once you add more degrees of reference and abstraction you see answers like this who was the master of the main antagonist of the lightsaber movie the master intended palpatine or we ve all seen questions like this what happens if you leave milk out of a fridge for too long milk is typically inside of a fridge while transformers do high dimensional anaysis of these queries it would be great if you could quantize them and find the nearest vertices entities on a knowledge graph then try to get the knowledge graph and transformer to learn from each other the knowledge graph behaves kind of like a discriminator in a gan has anyone done any work like this or thought of this poke some holes into my idea basically i want to quantize some of the high dimensional aspects of text into a knowledge graph to a transformer and back maybe repeat a few times the benefit a knowledge graph brings is that it s interpretable can hold truth and can be used to easily traverse the domain surrounding a transformer query i have a few ideas for how i would implement this but i don t want to go down a rabbit hole if this is a dumb idea and or already tried and failed
1,how shall i understand the ump test theorem via mlr in lehmann s tsh theorem 3 4 1 on p65 66 says theorem 3 4 1 let theta be a real parameter and let the random variable x have probability density p theta x with monotone likelihood ratio in t x i for testing h theta ≤ theta0 against k theta theta0 there exists a ump test which is given by phi x 1 when t x c gamma when t x c 0 when t x c 3 16 where c and gamma are determined by e theta0 phi x alpha 3 17 iii for all theta the test determined by 3 16 and 3 17 is ump for testing h theta theta against k theta theta at level alpha beta theta in iii is the test determined by 3 16 and 3 17 a size alpha test according to 3 17 how can iii be so sure that it is level alpha what is alpha relative to alpha is iii already mentioned in i what is the difference between iii and i thanks
2,gradient descent algorithm vs normal equation for regression fit with large n i m currently working my way through andrew ng s beginner course on machine learning at one point he compares the pros and cons of using a gradient descent algorithm to iterate and find the optimum parameters for a regression fit and using the normal equation to find the parameters analytically he explains that the main drawback of using the normal equation is that for n 10000 features the cost of computing the inverse matrix becomes prohibitive my background is in numerical analysis and computational math whenever we re working with large matrices we avoid computing the inverse of a matrix unless it is absolutely necessary for most systems a variant of lu decomposition is sufficient professor ng skipped over the derivation of the system he shows so maybe the answer is in there but my question is does anyone in machine learning use an alternative to computing that inverse matrix such as an lu decomp that makes the normal equation viable for very large n problems the equation in question θ x t x −1 x t y where θ is the parameter we are trying to minimize x is the matrix of features and y is the vector of values we d like to predict thanks
2,is beta vae regularization equivalent to using a gaussian prior with variance smaller than 1 hello everyone the beta vae model offers a regularization over the 𝛽 kl q z x p z term of the elbo in vae formulation increasing this term forces some latent units to be distributed as unit gaussians given that p z is standard gaussian i wonder is it possible to acquire the same regularization by only tuning the prior such as having a gaussian with smaller std such as 0 01 edit for the original vae part i corrected the distribution for the prior as standard gaussian
2,keyword key phrase extraction hey guys am a beginner at nlp and i want to create a model to extract keywords from text sentences predefined models aren t detecting enough keywords i ve collected a dataset where a column is full of sentences and another column that has manually extracted keywords any ideas on how to create a model
1,poisson distribution vs poisson process until now i was familiar with the poisson probability distribution i also learned how to determine if a given variable e g 1000 recorded measurements follows a poisson distribution this can be done by simulating different poisson distributions and seeing how closely they match your data now i am trying to learn about something called the poisson process my question is there a way to check whether your data follows a poisson process i am learning about queueing models in these problems you try to represent a queue people arriving in line waiting in line and getting served using statistical models e g the m m 1 model that require arrival times a common and important variable used in queuing problems to follow a poisson process suppose i have a list of arrival times e g the first customer arrives 10 minutes after the shop opens the second customer arrives 7 minutes after the first customer the third customer arrives 3 minutes after the second customer etc this can be expressed as either 10 7 3 or 10 17 20 is there a way to find out if this variable follows a poisson process i saw that there are ways to simulate a poisson process e g or but how can you check if individual measurements follow a poisson process thanks
2,a new codebase for self supervised learning with vision transformers that provides evaluation on down stream tasks of object detection and semantic segmentation the codebase includes an ssl implementation tuned to achieve 72 8 top 1 accuracy on imagenet 1k linear evaluation using deit s 16 and 300 epoch training it is slightly higher than that of moco v3 and dino at similar training budget but with lighter tricks the project also includes evaluation of transferring performance to down stream tasks which is missing in previous related papers and codebase this goal is achieved by involving swin transformer as one of its backbones which is friendly for down stream tasks of object detection and semantic segmentation the method part basically has no new things and is just a combination of moco v2 and byol has anyone gone through the codebase and is there any comment about following this codebase to do some exploration
0,possible to export jupyter notebook to powerpoint slide deck i imagine that this has been asked before i m aware that you can create html slide decks using reveal js but i was wondering if it was possible to get what is in my notebook into powerpoint whilst i think i can get away with using notebooks and reveal slides within my team senior management will probably just be confused with a html file even though the concept is the same
0,how do i ask for more resources in a secure environment i work at a relatively small publicly listed bank corporate hq as a data analyst intern and both physical and cyber security standards are high my job and most people’s job in my department finance entails importing data into excel or modifying data from excel as you may know excel doesn’t really play nice with all kinds of data for example we have excel functions calling a cell from a different sheet if the new data to be imported is minutely different it may invalidate the function many times this results in hours or troubleshooting manually parsing through cells and rematching them furthermore excel isn’t really robust enough to efficiently do linear algebra so any automation involves manually going through cells and writing functions additionally everyday that i’ve come in so far the first half of the workday involves my colleagues complaining about how the software used to manipulate data results in errors on my first day they left me a text book about said software and i read it in full it’s essentially visual simplified r i come from a stem background and i think it would be evident to anyone from my background that the current data storage system is highly inefficient and attempts at improving efficiency data manipulation software is negligible at best the problem is to access anything we go through a portal the portal has applications we can use you can’t access the shell there are no language interpreters or text editors even if i could access the shell it would either be cmd or powershell usbs aren’t allowed and data cannot be stored locally it has shutdown any propositions i’ve made about adding tools the cybersecurity administration is extremely hesitant along with my boss from an accessibility stand point i do understand i’m probably the only person in that entire building who knows how to code outside of a query if statement if we were to make a project i’d be the only one who could understand it and if i left they’d have to hire someone with those skills and completely change this hypothetical project so that’s not really what i’m asking for what i want is more tools to do the job they hired me personally for without those tools i’m just every other employee i don’t really have a purpose i feel like because i’m young they don’t really take me seriously but it just seems so clearly evident that there is no way they can continue to grow like this it’s just too much data to parse through with the existing process unless they just hire more and more but at some point the overhead will become too high which is what is currently happening paying sums for software that doesn’t work hiring more to parse negate data issues ironically one of the company commandments for a lack of a better word relates to cutting costs anywhere possible so my question is has anyone under similar circumstances successfully passed a proposition for more resources in a similar context how did you do it tl dr how do i ask for more resources in the context of a highly secure network
2,schizophrenia in lamda at the google io 2021 they showed an example conversation between a human and a paper airplane human what s it like being thrown through the air lamda the wind blowing against you and the trees flying past the question was about it s feelings not mine the answer should be like the wind blowing against me this kind of schizophrenia is very common in our days in texts and live conversations when people use you instead of me when talking about their own experience so ai have picked this up as a normal behavior using you instead of me is a hypnotic technique to convince a person to accept one s feelings as his own feelings
1,how to combine probabilities to establish consensus hi all hope someone could provide me pointers to address my problem problem statement is below i have a list of probabilities that are calculated realtime that looks like below p hot p cold p neitherhotnorcold p hot p cold p neitherhotnorcold p hot p cold p neitherhotnorcold at each step t 0 t 1 and so on p hot predicts the probability of next hour being hot p cold predicts the probability of next hour being cold and so on these probabilities are arrived independently of each other and is bit noisy and not perfect what are the best ways to combine them and arrive at a consensus probability at a given time i could do simple averaging of previous windows and vote the one with highest probability but it feels like there must be some smarter way any pointers
1,career please help advice needed hi guys first of all let me apologize if this doesn t belong here or on this sub i didn t see any rules saying otherwise i m just looking to get some clarity on what i want to do when i grow up and then start laying the foundations to do so i have a ba in psychology and fell in love with the research and statistics portion of that degree i was a ra for two years and took all the advanced statistics courses and capstones in that degree the process of collecting data analyzing it and then coming to a conclusion or lack thereof is something i think i may want to do as a career though i don t know what job title that would be or what to look into i think data analyst is the only thing that comes to mind my problem is i don t know if that s the only thing out there and how to start gaining qualifications i graduated almost 5 years ago now and definitely need to retouch on statistical analysis software do i go back to school and get a masters in stats do i take a bootcamp from a university and get a certificate or get certificates for individual software like sas excel python or perhaps there s an entry level job i don t know about that will teach me these things i m quite lost and i figure this would be the community that would have the insight and knowledge to assist me as i continue to age and start to settle down and look for a career i d love to hear your advice thanks so much
2,machine learning international internship hi guys i am looking for an international internship in machine learning or computer vision i am really enthusiastic about these areas but i had no luck in finding an appropriate internship whether remote or not i am located in the middle east do you have any suggestions where can i gain experience in this area thanks in advance
0,best way to summarize text i am looking to create a website including a function similar to tldrthis what would be the best way to approach the summarization part of development it would mainly be used for longer messages news or articles
2,can anyone recommend papers for machine learning in credit based fraud hello all i wanted to ask the community whether there are any recent good papers that concern general credit fraud or perhaps even financial fraud in general that you d recommend i m aware there s a lot of content based on the famous credit card fraud dataset however i m more interested in credit fraud that doesn t concern that specific dataset i m also open to hearing about machine learning applications that could be considered adapted for such modelling please feel free to plug in your own research too thanks in advance
1,central limit theorem and addition of independant random variable hi all so i m studying clt and i see that its at the basis of the normal law in most cases so if we add independant random variables together and stack them we will end up with a normal law but i have rouble understanding what s being meant by independant random variables in this case for example with the usual sample means and sampling distribution how are the means independant since they all come from the same distribution that has a pretty well defined logic behind it i have been told that to obtain a normal law each observation we make must itself be the result of an addition of independant phenomenon and that it is misunderstood a lot because if it s true almost no phenomena should follow a normal law since a single observation is most often the result of interdependant events determined by precise logic and laws of nature what do you think about that where can i learn more about it
1,strange results calculating a robust cohen s d q using wilcox s shiftpbci and shiftes wilcox s q is purported to be a robust equivalent to cohen s d i have vectors with 1000 and 1000 items for the vectors with 1000 items shiftpbci seems to work well when there are more than 1000 items the q effect returned is outside the confidence interval returned from shiftpbci and the upper and lower bounds of the confidence interval are equal the culprit would seem to be the shiftes function which tests for the number of items in the two vectors passed to it if less than 1000 a simple outer x y fun is executed if greater than 1000 a maximum of 50 samples are selected from each vector and the outer product is calculated and appended to the vector l whose median is eventually taken and used to calculate the elements above or below the median etc if i add the vector v to the variables returned form shiftpbci i see that all 500 entries in it are the same and thus the high and low values of ci are the same as well i m not as well versed in r and statistics as other languages and would greatly appreciate any help the source for the two functions are pasted below and a link to wilcox wrs package is here and a link to the paper that talks about the method shiftpbci function x y locfun median alpha 05 nboot 500 seed true confidence interval for the quantile shift measure of effect size same as shiftqsci if seed set seed 2 x elimna x y elimna y n1 length x n2 length y v na ef shiftes x y locfun locfun q effect for i in 1 nboot x sample x n1 replace true y sample y n2 replace true v i shiftes x y locfun locfun q effect v sort v ilow round alpha 2 nboot ihi nboot ilow ilow ilow 1 ci v ilow ci 2 v ihi list n1 n1 n2 n2 q effect ef ci ci shiftes function x y locfun median iter 100 seed true probabilistic measure of effect size shift of the median ef size na ef sizend na q size na x elimna x y elimna y n1 length x n2 length y if n1 10 n2 10 nt n1 n2 nt max n1 n2 if nt 10 3 l outer x y fun if nt 10 3 if seed set seed 2 l null nmin min c n1 n2 50 for i in 1 iter id1 sample n1 nmin id2 sample n2 nmin l c l outer x id1 y id2 fun est locfun l if est 0 ef sizend mean l est est ef size mean l est est if est 0 ef sizend mean l est est q size ef size 5 5 list q effect ef size non d effect ef sizend scaled effect q size
1,trying to understand what best predicts employee working hours hi guys i am really no statistics expert so i hope this is clear if you need clarification please let me know background we have a casual workforce where there is no measure of ‘efficiency’ we don’t allocate a specific amount of hours for a shift the employee just works until it’s done assumption in the business there is an assumption that employees work longer in summer than winter because our carton size is larger in summer although it doesn’t appear that hours change that much problem trying to understand what variable contributes the most to employee shift hours and what statistical test to use data available age start date cartons by shift gender weather season location pay rate day of the week thank you
2,new milestone for deep potential application predicting the phase diagram of water a research team from princeton university the institute of applied physics and computational mathematics and the beijing institute of big data research uses the deep potential dp method to predict the phase diagram of water from ab initio quantum theory from low temperature and pressure to about 2400 k and 50 gpa the paper was published in leading physics journal physical review letters and represents an important milestone in the application of dp here is a quick read new milestone for deep potential application predicting the phase diagram of water the paper the phase diagram of a deep potential water model is on physical review letters and arxiv
0,cmu researchers propose ratt randomly assign train and track a method for guaranteeing ai model generalization the approximately correct machine intelligence acmi lab at carnegie mellon university cmu has published a paper on randomly assign train and track ratt ratt is an algorithm that uses noisy training data to put an upper bound on a deep learning model’s actual error risk model developers can use ratt to see how well a model generalizes to new input data the researchers demonstrate mathematical proofs of ratt’s guarantees and conduct experiments on various datasets for computer vision cv and natural language processing nlp models in their publication when a trained model gets a high error rate on randomly labeled or noisy data but a low error rate on clean data the model is assumed to have a low error rate on new data full summary paper
1,what does it mean if a predictive test has a high specificity but a low ppv hi what does it mean if a predictive test has a high specificity but a low ppv i know that high specificity unlikely that a person without the disease will receive a positive result therefore a positive result means that they likely have it however a low ppv large percentage of positive results were false positives how can have both a high specificity but low ppv at the same time thanks for any help
2,cascaded diffusion models for high fidelity image generation paper project page twitter thread
0,inevitable manual work required in data science projects i have feeling that not many people are willing to admit but ultimately is a significant part of many data mining projects e g checking data quality parsing through data etc still done manually for example here is an example i just made up relating to supervised nlp natural language processing classification suppose i have 1000 medical reports of patients containing unstructured text made by a doctor during a hospital visit for a given patient each report contains all the text notes that the doctor made for that patient for visits between 2010 and 2020 these reports make mention of the patients bio data e g age gender medical history etc and the details of the symptoms that the patient is experiencing over a long period of time e g let s say that these reports are 2000 words on average the problem is different doctors have different styles of writing each of these 1000 reports is different from another if a human were to read the report the human could figure out what happened to the patient did the patient have a serious condition let s call this class 1 or a non serious condition let s call this class 0 this is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients the problem is there is no clear and fast way not that i know of to take the 1000 medical reports that are available and label each report as class 1 or class 0 for example for class 0 one of the doctors could clearly write at the end of a report all medical tests were conducted and the results and were all negative and another doctor could end the report by saying the patient should seriously consider changing their lifestyle and eat healthier food benign in this example how would someone assign labels to all these 1000 cases without manually reading them and deciding if the information in the report corresponds to a serious condition or a non serious condition i was thinking of using something like sentiment analysis to capture the mood of these reports and use sentiment analysis a method to informally gauge if the tone of the report is dark serious condition or light non serious condition but i am not sure if this is the best way to approach this problem is there a way to do this without reading all the reports and manually deciding labels in the end this is what i am interested in doing suppose a new patient comes in and on the first visit the doctor makes some quick notes e g patient is male 30 years old 180 cm 100 kg non smoker frequently complains of chest pains no high blood pressure works a construction worker and takes daily medicine for acid reflex just based on these quick notes and the 1000 reports available note i am trying to illustrate a point here that the medical notes for the new patient and the 1000 reports do not have the same format can a researcher predict supervised classification e g decision tree if this patient will have a serious or a non serious condition ps suppose the doctors have a very detailed medical encyclopedia on their computers can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results
1,calculating a significant statistical difference between the numbers of three groups calculating a significant statistical difference between the numbers of three groups hi i should start by saying that i am statistically illiterate i am writing a review on a scientific paper and would like to know if i can write that the difference in lost to follow ups in each group is significantly different statistically by significantly different i mean a less than 5 chance that it is a random occurrence there are three groups at the beginning of the study they had group 1 41 participants group 2 39 participants group 3 39 participants over the course of the study 20 participants were lost to follow up dropped out and these where divided as group 1 8 group 2 9 group 3 3 i have tried using some of the various calculations and my brain did a little melty i then triumphantly gave up somewhere a small spark went off in my brain that said the absolute wealth of ability that resides on the various subreddits of the world would delight in completing this piece of arithmetic for me if anyone can help me i would be incredibly grateful and pay you with kudos i basically only want to know about group 3 it is not beneficial for me to know if there is a difference between 1 and 2
1,euro 2020 predictions update last week i posted some predictions for the 2020 euro now that the first round is over we can examine some of my performance my predictions and results for the first round are shown in this table sorry it isn t prettier i achieve an average log loss of 0 92 where assigning all outcomes as equiprobable yields an average loss of 1 1 my multiclass roc for predicting the outcome is 0 77 in short in the first 12 games i perform slightly better than random guessing which is honestly fine for me however most people who have watched international football wouldn t assign all match events as equally likely is italy drawing turkey really as probable as italy losing to turkey no its hard for me to measure against a reasonable guesser my work pool records all our guesses and so at the end of the group stage i can use that as a sort of ensemble method to compare against we ll see here are match predictions for the remaining group stage games conditioned on the results of the first games the model is not perfect and still makes some weird predictions for example portugal is given higher probability to beat france than they are to beat germany even though france beat germany in the first round if you subscribe to some sort of sports law of transitivity this may sound weird my predictions for the second round and the results of the first can be found here
2,a neural tangent kernel perspective of gans
0,with the growing number of undergraduate majors and ms degrees in data science will employers expect data science degrees in the future i will admit my question does not make sense now but cs was a relatively new field decades ago and most older software engineers i have met do not have cs degrees but degrees in math engineering physics or are even self taught now it seems like we see less self taught programmers and more programmers with cs degrees couldn t the same happen with data science
2,icml 2021 volunteer hi guys i have been selected to be a volunteer at icml 2021 any advices on how to make the best use of this opportunity
2,introducing sklearn genetic opt hyperparameters tuning using evolutionary algorithms project i recently released a new open source python library that makes it easy to fine tune scikit learn models hyperparameters using evolutionary algorithms the package is called sklearn genetic opt and provides several optimization algorithms build in plots to understand the results custom callbacks to control the iterations and more check the documentation to get started if you want to know more the details or contribute you can check the github repository install the package runing pip install sklearn genetic opt i hope this can be useful to the general community and that the package keep growing to bring new features any feedback is very welcome still a lot of work to do
0,for those living in japan for those who would know what is the state of the data science field in japan like i heard there is a huge demand for data scientists there hello guys i saw from this post two years old that robotics nlp and ar vr were quite big in japan i was wondering if that is still the case what other additional data science ai fields are picking up over there additionally i read that there aren t many good formal educational opportunities to learn data science is that still true lastly would people in japan be interested in data science presentations by speakers from canada and or the states thank you for your time
1,bayesian statistics question hi guys i’m conducting an independent study and was wondering if anyone could help me get a general grasp on it i’m doing research on cancer rates and pollution levels in the us particularly along the mississippi river delta and basin my professor wants me to conduct some tests using bayesian statistics i know that a lot of statistical tests have certain parameters and assumptions that must be met — i e the t test needs relatively equivalent sample sizes between the two groups as well as a uniform bell curve distribution in order to work to its fullest extent what are the most basic assumptions for bayesian statistics is it possible in bayesian state like with an anova test to test conflicting or compounding variables i e variables that work together to overall amplify the risk of the development of cancer
1,should i omit an unreliable dimension from an efa i have 25 dimensions that were rated by 85 participants i ve computed the iccs on each dimension and they are all good 75 except one dimension which is moderate 69 and one that is bad 39 should i omit the bad dimension from my efa what about the dimension with a moderate icc
2,interview with privacy guru anne cavoukian today hello machinelearning today at 2pm edt we ll be sitting down for a conversation with the inventor of privacy by design part of the gdpr and former 3 term information privacy commissioner of ontario ann cavoukian ph d together with the ceo of private ai patricia thaine msc we ll discuss why it matters to embed privacy by design into your workflows how the most successful tech companies embrace privacy what to expect from upcoming privacy legislation around the world we ll be touching on these points from a machine learning perspective in particular if you re interested please register here
2,google’s multitask unified model mum transforms how google ai understands complex queries google launches a multitask unified model mum that offers expert like answers to user’s questions with fewer queries to complex tasks with google search one can get answers to what they are looking for however one needs to type out many questions and perform many searches to get the desired results most of us generally require multiple steps eight search queries on average to tackle a task with google present search engines are not advanced enough to respond with expert like answers article google blog
1,scaling variance in multivariate normal model i have a bivariate regression model where one variable is a predictor for both outcomes there are other predictors as well i want the variance of both outcomes to scale with the magnitude of that predictor variable when the variable gets larger sigma for both outcomes should be larger as well while keeping the correlation constant is there a way to do this the model is coded in stan but i think potential solutions to this should be software independent
1,exploratory data analysis 1 how to learn exploratory data analysis in python any links or references would help 2 what statistical values should we verify while performing this min max and similar things or do we need to find more information on the problem statement 3 does this include data description and visualization as well what libraries do we use generally for python
2,library for making splits a few times i ve needed to split my data while attempting to keep various meta features of the data in particular proportion across the datasets for instance if i am training an image classification model on images which i scraped from the web i might want to attempt to keep roughly the same proportion of images scraped from instagram in the training test validation so that if my total dataset is 30 instagram each of my splits has 30 instagram images conversely i might want to ensure that all the images from a particular user on instagram ends up in one particular set because the user might have some specific logo on their images or something that i don t want to accidentally learn and claim great performance discriminating against another example is in a multi class classifier i would want to ensure the number of examples of each class are represented roughly equally in each split the basic problem would be given a table where each row represents an example datapoint and each column represents a meta variable i want a program that partitions the rows into three groups presumably i can choose the percentage that goes into each partition such that each column i specify to be split evenly is roughly distributed evenly in the partitions and each column that i specify to be to be split exclusively has all rows with the same value for that column stay in exactly one split i ve written some code to do this a few times but i normally just roll a random split and then check if everything is within some tolerance i m sure there is a more optimal way to do this but before i spend the time to figure this out on my own i just wanted to know if something like this already exists thanks
2,recognizing the brand of an advertisement based on advert video i am enrolled in a deep learning course for my masters degree and my semester project is to design a model that takes a video file more appropriately a pre processed input data captured from a given video file and outputs the brand in the advert in the video for example ideally by feeding a video of a bmw commercial the model outputs that the video was an advertisement for a bmw or feeding a video of dove soap commercial it predicts that the advert was a dove soap advertisement the important thing is that this is not a classification problem the model is expected to recognize what brand the advert was for even if it has never been trained on it just like a human recognizes what the brand in an advert is even if he or she has never watched any advertisement of that brand before i am sort of blank as to what approach i could be using any help is appreciated
1,i need help with spss this is the task b create a new variable grade by using the following rule grade is 5 if ratio ≥ 0 9 4 if 0 8≤ratio 0 9 3 if 0 7≤ratio 0 8 2if0 6≤ratio 0 7 1 if 0 5≤ratio 0 6 and 0 if ratio 0 5 can you tell me step by step how can i apply that to spss
1,explanation on normal probability plots p p q q plots hello r statistics i m doing some classwork that pertains to normal distribution plots and i m having a bit of a hard time understanding the meaning of the plot axes i understand that probability plots indicate how well a set of data matches a normal distribution or some other distribution however scipy probplot labels the axes as ordered values and theoretical quantiles see the plot generated from my data set here i ve looked into quantiles a bit more and i don t quite understand what the x axis and y axis ranges really mean i don t quite understand what a 3 quantile is nor how i would take a particular data point and map it to these coordinates can anyone help shed some light on this online descriptions aren t very intuitive to me but that s probably because i am a bit new to this depth of statistic and this kind of jargon
2,deep learning vs wide learning today my friend was telling me about something called the universal approximation theorem which apparently contains the mathematical foundation behind neural networks work after watching a lot of youtube videos on this topic i think i have some understanding on this topic supposedly a 2 layer neural network can approximate any function and the error of this approximation is proportional to the number of neurons within the neural network the more neurons there are the better the approximation will be the only problem with this is that a 2 layer neural network will likely require a very large number of neurons to produce a decent quality approximation apparently so many neurons that the computations will become very inefficient conceptually you can imagine a 2 layer neural network as being very wide looking supposedly the solution to the above problem is to create neural networks that are deep instead of wide so instead of a 2 layer neural network with a very large number of neurons it s said that a neural network with fewer neurons and more layers is more computationally effective and these deep neural networks also have the universal approximation property the question i have if wide neural networks require a very large number of neurons to adequately approximate a function why don t deep neural networks require a very large number of layers to approximate a function at a similar level of accuracy what explains this mismatch between the number of layer and number of neurons equivalence suppose a wide neural network with a relu activation function 2 layers and 1000 neurons is able to approximate some function with an error of 0 05 clearly we believe that a deep neural network also with a relu activation function with 1000 layers with each layer having 2 neurons is not necessary instead we believe that the number of layers does not need to be so deep what explains this mismatch why are deep neural networks typically able to maintain a relatively low number of neurons and layers compared to wide neural networks where only the number of layers can be kept small
0,people who ve built data science architecture from scratch how did you go about it and what were your experiences like this is the accompanying question to the how did you build a ds team from scratch that doesn t quite seem to be discussed as much let s say you have a very simple product that offers a portal for users to log in runs on ec2 servers in the cloud does read and write into a db etc how do you build on the above gradually with things like a data warehouse and how do you decide when this is needed and introducing machine learning into the mix in a product where the only thing it references is just the database itself
1,research or internship in prep for phd applications hello all i’m currently an undergraduate stats major who will be a junior in the fall my goals is to apply to phd programs in senior fall if i wanted to look at opportunities for the summer prior to it should i look into doing research within say the stats dept or should i be trying to look for an actual internship at a company my research interests are within statistical learning so i was thinking a research role would be better suited for me than some data analytics position at a company i feel that it will be hard for me to get on any papers or do research because i won’t have a ton of theory knowledge but i’m hoping i can get on something more applied so what do you think i feel like trying to get a research role would be better when applying then doing sql all day at a company for a summer chances are even if i expressed my case as i want to do some more data science and quantitative sort of internship it wouldn’t be as good as if i did research with a prof i will be applying to stats phd programs
1,bayesian statistics how to introduce posterior probability interpretation in the body an article i m in the process of writing various bayesian data analyses and i feel it is required to add a quick guide to bayesian statistics within each article to help readers in their interpretation especially if they have little or no statistical training i was wondering if people here know good resources for that it would be good to have some ready to use prose to include in each analysis x200b ideally the content i d add to a data analysis would be something like this a below each major posterior based result e g a 99 credible interval a few sentences to help interpret the 99 for instance how to interpret according to our model and prior assumptions we have inferred that with 99 credibility the effect size lies between 1 8 and 7 8 points the effect size is the treatment s average reduction in perceived pain level compared to a placebo the phrase with 99 credibility may be understood as follows given the results a rational person who believes in the model and prior assumptions would be willing to bet 99 to 1 that the effect size lies in interval 1 8 7 8 x200b b as an appendix a brief motivation of the use of bayesian statistics rather than classical frequentist statistics featuring 1 evidence and explanation for frequentist results such as p values and confidence intervals being commonly misinterpreted in particular being interpreted as bayesian posterior probabilities 2 brief explanation references for grasping the philosophical implications of bayesian probability along the lines of you have to be ok with using probability as a measure of subjective uncertainty and beliefs as we saw above most people do that intuitively and the related tradeoffs and controversy 3 references for learning bayesian statistics at various technical depths x200b if i m being facetious most people should interpret posterior probabilities like they usually interpret p values except that this time they ll be correct
2,discussion c usage for machine learning hi all i m wondering if i should learn c if i want to get a job as an ml engineer or other ds role i am currently comfortable with python and r 1 would learning c be beneficial to getting a job in the ml industry 2 are more and more ml jobs today requiring c proficiency
1,evil geniuses presents data analytics and tech in esports hey everyone my name is sean and i’m an intern at evil geniuses tier 1 esports organization working with the data and tech team we will be hosting a workshop this week 5 19 at 11 am pt 2 pm et focused on the use of data in esports the two panelists who will be running the workshop are soham “valens” chowdhury head of data science and zach kamran head of tech and analytics x200b i ve copied the eventbrite link below the event is completely free if you have any questions please don t hesitate to reach out to me via reddit dm or on discord info on the eventbrite page x200b eventbrite
1,what is some good outside resources to learn statistics so my college level statistics class is not cutting it for me and my professor is the least cooperative as there is so i need to know some outside resources so i can learn some of this stuff
2,contrastive loss on sequences i m working on labeling a sequence of speech embeddings with a speaker these speakers aren t known so usually a unsupervised clustering approach is taken my hunch is that i can incorporate some temporal information across the speech embeddings by passing the input embeddings through an lstm and then doing the clustering on the hidden states to enforce that embeddings from the speakers are close i ve thought of simply encouraging the cosine distance of hidden states for the same speakers to be 1 and for different speakers to be 1 something along the lines of x speech embeddings shape n d1 y labels shape n 1 h lstm x shape n d2 h normalize h dim 1 normalize for cosine sim h h t pair wise cosine distance in 1 1 sim 0 5 sim 1 cosine distance in 0 1 target y y t boolean if i j same speaker loss ce similarity target i have very little experience with this kind of supervised contrastive learning so this was just the a simplistic initial approach i thought of when looking at some papers e g simclr it seems that the losses are designed for a source image an augmented positive and some negative examples which seems amenable to a similar simplistic approach what s the reason why the below loss is so much better
0,whats with all the sudden hate for ds and shift towards de a few months ago data scientists were saints sent from heaven now suddenly data science is not as important anymore i understand that analytics and data science departments won’t function properly without an sound data foundation but why is everyone glorifying data engineers on the expense of data scientists why is everyone following the hype whatever direction it goes is there a real reason for all of this or are people really just following bloggers and “data influencers” any opinions ideas
0,that feeling when c suite would rather keep you grinding support and busy work while asking you to teach the middle management your entire graduate degree instead of just making your title data scientist and tasking you with that stuff yeah so innocent question about some analyses by an executive to me because i have a mscs with focus in data science is turning into me giving a lecture series to middle management about how to do said analyses instead of just promoting me changing my title and taking me off the shit dino tech generalist bs role i’m in already they’d rather just keep me marginalized doing menial support and basic bs programming in some godawful proprietary stack knowing full well i’m underemployed there ask me to show them how to do stuff i spent years learning instead of just making my role the one to do that and take me off the bs work i wish i had enough cash to just ghost this place but i can’t even get an interview while i’m employed because of said garbage work i do and why i pursued my masters independent of this job
0,is it a waste to not go into data science after getting my ms i m on track to graduate with a ms in statistics in a few months time i have no prior working experience only software and data engineering internships and my bachelors was in engineering industrial engineering with computer science minor i tried applying to data science jobs but not much luck so far most of my interview callbacks are from engineering positions i honestly do not mind being an engineer however does it mean that i would be wasting my ms ps i currently have an offer as an engineer in cyber security just would like to know the general feedback for a graduate with no work experience thanks for reading
2,seldon core 1 8 0 released seldon core 1 8 0 has been released seldon core is an open source library for cloud native machine learning deployment this release contains an integration to our new tempo data science library updates to support the latest alibi detect release for outlier and drift detection as well an example for gpt 2 inference on triton
0,does having few girl students in ds is common even in developed countries is it harder for girls to be in this field
1,planning to apply for master in stats i failed econ class twice would it matter i took this intro macroecon class twice but failed both the class itself wasn’t difficult but i just could not get the concept of macro so i just did not even have desire to study would it matter when i apply for stats master’s program if i failed it twice im curious since there are lots of folks in stats grad who are coming from econ background and stats econ are pretty famous combo so im worried that admission center will care
2,multiple products time series with singlevariate lstm project description my project is to pre order movies online people receive them at release date and also to sell them after release date as well i have historical for many movies their historic sales plot looks like a bell shape like curve with the symmetric center at the middle when the movie is released i use lstm from the following project with python 3 7 keras tensorflow the goal would be to train the model based on all the previouse plots without just averaging all the sales all of my historical data contains 2 parameters 1 sales date in the following format negative integers represent dates before release 0 date when the release of the movie happening and than positive numbers maxing out at 30 to count the dates after release x200b how to do this what i want to do is multiple products timeline with the only feature numebr of sales with lstm python 3 7 keras tensorflow prediction should be on a brand new product until day 30 not just continuing each product x200b what i have found so far my issue is that all the recomendation and project that i have found are for 1 product so 1 time line and maybe for multiple features temperature revenue gdp at that time of the country example x200b what i have done at the end i am happy to get feedback on my solution i have tried to connect the time scales after each other in training and if the training data have a wide variety of events than the test sets have pretty good response whit this methodology i am going to turn the problem to a multivariant problem to get better results with tons of features like added to favorites list viewed movie rank
0,is it possible to use s arimax to predict multiple time steps ahead topic given the lack of future covariates can one use s arimax to predict k instead of just one time steps ahead another related question can we use sarimax to predict non negative integer series such as count of item sale if yes how if not which alternatives do you recommend
0,freelancing can you do freelance works in this field
1,cross validation for mediation models estimated with bootstrapping does it make sense to perform cross validation on mediation models that were estimated using bootstrapping with hayes process macro i know that the two techniques have a different purpose but when i look online i find many articles posts comparing their performance and how one may perform better than the other since bootstrapping already uses resampling will performing cross validation add any value additionally since direct effects are not by default estimated using bootstrapping in hayes process macro should i use bootstrapping for all coefficient estimates thanks in advance
1,i need help with a question for my graduation project about ordinal data for my graduation project i am working on the impact of social media influencers on consumer purchase intention i collected data through a survey the data is mainly nominal ordinal and 80 of it is likert scale the problem is my supervisor told me that we will consider the likert scale as ordinal unlike my previous marketing professor where we considered it interval so basically i want to know what analysis can i do with ordinal data he suggested working on correlations and said he will accept pca but won t accept any hypothesis testing regression or anova because they can t be conducted on ordinal data i really need help because i only have 10 days left for now i am planning on doing pca and chi square anyone can help
2,have machine learning conferences become obsolete with collusion rings poor reviewership and sparse if not empty poster sessions what is the point of a conference especially an online one the main proponents that still support conferences seem to be the select few that run them and have their reputation staked into them i have learned more seen better feedback and had more networking opportunities from twitter arxiv discord reddit and other online networks so what s the purpose of a conference these days extra lines on a cv jobs promotions recruiting now it becomes pretty obvious why there are collusion rings bad reviewers low effort etc references and more reading
0,data science projects for local governments a friend of mine is going to make a presentation to an indian state government on how it can better use data and how data may be used to have more effective policy and decision making for government bureaucrats in the indian state to achieve traction with the bureaucracy the focus needs to be more on projects and topics that will lead to quick wins show some quick results make like for the bureaucrats policy makers perform better and enable a virtuous loop where they can see how data can help in an efficient manner… rather than being long projects that rarely result in concrete measurable benefits what are some projects topics that you would suggest maybe you have worked on some for local govts in other countries or in india itself any links and suggestions will be helpful note that most of these bureaucrats policy makers are not coders and data they have access to is often not clean many data bases do not talk to each other and there are data inconsistencies as well any ideas and suggestions are much appreciated thanks in advance x200b edit also my friend s ngo will support with some data science volunteers…they will do the data cleaning and analysis… looking for some initial ideas that you think may lead to interest from bureaucrats policy makers
2,it s really funny how authors introduce dozens of new variables and notation to explain really basic concepts so that the paper would seem more formal that s the post
2,building a data flywheel for data centric ml development we ve been working for a long time to provide an easy way to implement the data flywheel for cv i m happy to present you v0 1 the data flywheel is the idea of having an ml pipeline which allows you to flag mispredictions in your production environment you could pick the ones with the low confidence for example pushing these images back to your annotation environment to relabel them retraining your model with the new data and then put it back to production again by this you’ll have an ever improving model this will enable an easy way to do model centric ml development check out the blog post to learn more and how to implement the flywheel yourself the flywheel comes up at the end of the text as said in the beginning this is only v0 1 and we have much more things planned for the future
1,question binomial distribution with n 15 how to solve it hi i m doing an exercise where we have 15 as n and p 0 2 they ask what are the probabilities of success more than 5 times of course there s a way of solving it by adding all the probabilities from 5 to 15 p x 5 p x 6 p x 7 p x 15 but it s a bit too long i thought that i might have to approximate the binomial distribution to a normal one and then convert the normal one into an standard distribution to get the answers from the table however the binomial distribution is too short to be converted into a normal one but quite too long to calculate by adding the probabilities of each n should i solve the exercise by adding those probabilities from 5 to 15 or there s another way
2,train clip a pytorch lightning framework dedicated to the training and reproduction of clip in order to replicate and further build from openai s clip we’ve been working to create a scalable and easy to use library to train clip from scratch as of right now we are working to expand the library to incorporate a recent paper on data efficient training for clip and further performance boosts if you’re interested in contributing feel free to send a pr
0,designing a team sort of curious what people would do if they had the opportunity to work for a team that was something “like the brochures” and not just a team that people came to with analytics questions suppose you are given the opportunity to design a small 5 8 data science machine learning team in an engineering department your team’s skills are legitimately data science machine learning not analysts and there is skill in building production ready software among some members the question is sort of an open ended “what if ” but am curious how people might answer some of these questions as examples how do you help people in the org who might come to you with analytics kpi questions without brushing the off but helping them get to the right data they need in other words what is your interface to non engineering business types if they don’t have direct access to your priorities how are tasks handled are you still in an agile process where people pick up tickets as they free up resources do you allow multiple people to work on the same ticket what is considered a “unit of work” in other words is your focus solely on direct applications or do you devote significant time to experimentation and if so what does experimentation look like what do you differently that other engineering teams wouldn’t even think to do but you think are important to your team how do you get buy in for all of the “data science has to do it this way because ”
1,how to create a heteroscedastic heteroscedastic variable in a dataset hello r statistics i m currently preparing a workshop on linear regression and wanted to include a heteroscedastic variable in my dataset so that the students could learn how to look for it i tried a lot of things in python but couldn t get the typical cone shape of the residuals i wanted has anyone any idea on how to transform the data in a way that it looks the way i want thanks in advance
0,getting sent 4 years of data what is the best way to make this searchable for the whole team independently hi sorry for the newbie question we are getting 4 years of donation data sent to us as we can no longer use the service portal to pull specific reports and search through it i need to set up a substitute that will allow members of our team who have no experience outside of basic excel to search through the data and if possible save spreadsheets based on reporting parameters x200b what is the best solution for something like this could i use power bi am i right in thinking sql would probably be best but the person submitting the queries would need to know sql which would rule it out for this use case the size of the data would crash most of the team s excel programs x200b thanks for your help
2,ml model server 📢 you can 𝗖𝗿𝗲𝗮𝘁𝗲 𝗔𝗣𝗜 𝗳𝗼𝗿 𝗔𝗻𝘆 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴 𝗠𝗼𝗱𝗲𝗹 ml dl image classification nlp tensorflow or pytorch 𝗼𝗻𝗹𝘆 𝗶𝗻 𝗳𝗲𝘄 𝗹𝗶𝗻𝗲𝘀 𝗼𝗳 𝗰𝗼𝗱𝗲𝘀 𝘄𝗶𝘁𝗵 𝗮𝗹𝗹 𝗻𝗲𝘄 𝗖𝗛𝗜𝗧𝗥𝗔 𝟬 𝟭 𝟬 🔥 powered by fastapi and pydantic 🤓 𝗜𝗻𝘀𝘁𝗮𝗹𝗹 pip install chitra 0 1 0a0 𝗦𝗼𝘂𝗿𝗰𝗲 𝗘𝘅𝗮𝗺𝗽𝗹𝗲 𝘁𝗼 𝗵𝗲𝗹𝗽 𝘆𝗼𝘂 𝗚𝗲𝘁 𝗦𝘁𝗮𝗿𝘁𝗲𝗱
2,temporal network graphs has anyone ever worked with temporal network graphs before basically these seem to be graphs in which a new nodes can appear and old nodes can disappear as time progresses b edges between nodes can appear disappear as time progresses these seem to be newer methodologies has anyone tried to perform algorithms like community detection node classification or edge prediction on temporal graphs i was trying to find more examples of this kind of stuff and all i was able to find was the twitter github page and this one general blog on temporal graphs with r and can anyone please recommend anything else thanks
1,different statistician picked up project changed t test p no longer significant quick q working with a dataset of 2000 patients that was initially analyzed by 1 statistician then again by another our primary outcome is regarding how much of a drug was used during hospitalization before and after an intervention updated guidelines a group of patients pre intervention and a different group post intervention so just a difference of means aka report two tailed unpaired two sample t test of means assuming unequal variances right anyway the 2nd statistician got a different p value enough such that our difference before and after intervention was no longer significant so just trying to figure out why he explained it as first statistician was doing mean1 mean2 0 and not testing for what you want now it s mean1 mean2 or mean1 mean2 0 i know mean1 mean2 is what we want is there a difference between the means but what is this mean1 mean2 0 test in layman s terms what s mean1 mean2 0 asking appreciate any help thank you
1,are statistical models able to make predictions about individuals are statistical models in theory able to make predictions about individuals suppose you have an individual with observed covariate information x a y b z c in theory can a regression model trained well on some data predict the expected value of this individual s response variable i heard today that statistical models are not designed to make predictions about individuals they are only designed to predict the average behavior of a large group of individuals and in theory should not be used to make predictions about individuals is this correct does this mean that any time statistical models are used to make individual predictions this is going against the intended use of statistical models thanks
1,modern time series analysis does anyone know what are the most modern statistical models being used for time series analysis i have heard of transformer and attention mechanisms models that are used for modelling sequential data but these seem to be more relevant for modelling data from the nlp domain when it comes to classical time series modelling e g a vector of temperature measurements does anyone know what are some of the more modern models being used for this i did some searching online it seems like arima style models were some of the first ones followed by state space models hidden markov and the more recent ones being rnn and lstm are lstm and rnn the most modern models that are being used for classical time series problems thanks
0,are data science interviews still a work in progress i was recently on the job hunt and did not encounter a single interview that was alike for a bit i thought i was unprepared but then i realized some of these teams weren t sure what they wanted to hire what do y all think are data science interviews still in the works mostly or do you know of a good process that will get adopted throughout the industry i made a video about my experience taking and creating an interview didn t go well
1,honestly just a rant post regarding the post bacc analytics job hunt hi everyone i m just a bit frustrated to be honest a little background i have a bachelor s and i graduated just over a year ago i didn t major in statistics but i was exposed to it from research with a professor in an applied field during my senior year it was fun we coded in r got a few studies going and i still coordinate with him to this day to do work on some hobby projects despite not taking classes anymore after graduation i also wound up tutoring stats and r programming at a large company and taking on some extra responsibilities got into a role hiring new tutoring applicants etc part time but between this and a simulation project or two that i still work on with my former mentor i enjoy things but that s where the buck seems to stop in terms of professional development i get it i don t have a phd or master s that s perfectly fine it d be absurd to expect anything more than running analyses or managing data under the supervision of a statistician i don t mind that and i d welcome it but i m just disappointed in how hard it s been trying to nail a job in statistical programming that s where i wanted to lead things from here but despite studying up on sas getting a certification in my spare time and coding a lengthy project for github that involved base sas sas stat iml sql and macro programming i struggle to get noticed given the few entry level positions i seem to find i d like to conduct a quick post mortem and settle this once and for all honestly is sas a no go for a college graduate in terms of roles even at cros is hiring mostly for those with a master s or with experience as you might be able to tell in terms of statistical programming i m interested in the clinical area as that s where a lot of this sort of work seems to be performed some pretty old resources seem to say that cros are a go for recent graduates but at this point more than likely for the best it s time for me to move on to a technology that s more widely used does this seem to be supported by your observations
2,new directions in neural differential equations hello everyone i m happy to say we got three new papers accepted at icml 2021 all on neural differential equations and shameless self advertising wanted to share them here one is on neural odes one is on neural sdes and one of them introduces the new concept of neural rough differential equations yes another neural xyz differential equation hopefully you should find one or more of them interesting if you work on gans rnns time series or of course neural differential equations hey that s not an ode faster ode adjoints via seminorms with ricky t q chen solving an ode numerically typically means accepting that you re going to make some numerical error we show that by being a bit more careful about where you make the numerical error in particular in the gradient calculations you can reduce the number of steps needed in the ode solver by as much as 62 train twice as fast ymmv based on the problem what s great about this is that you can basically drop it in to anything you re already working with it s now supported natively in torchdiffeq so all you have to do to use this is torchdiffeq odeint adjoint adjoint options norm seminorm as an approach it has essentially zero downsides paper on arxiv code on github neural sdes as infinite dimensional gans with xuechen li and james foster we demonstrate that the modern machine learning of gans is very similar to how we ve been fitting sdes for decades sdes being random are basically generative models so joining up all the dots we show how you can train neural sdes as incredibly flexible models for time series if you work on gans then this should be pretty interesting as a pretty unique application of gans if you work on rnns then these are basically continuous time generative rnns diff eqs are a great prior on model space if you work on sdes or math finance then compared to traditional sdes these have essentially unprecedented modelling capacity paper on arxiv walkthrough example code on github neural rough differential equations for long time series with james morrill james foster and cristopher salvi we showed in our neural controlled differential equation paper last year that you can train neural cdes as essentially continuous time rnns this gives you a nice way to handle irregular data and a great prior on model space this kind of thing is actually a pretty long standing connection we give some examples in the neural cde paper going back to the 90s now we re back with a generalisation essentially the mathematics of rough differential equations deals with how to handle fine scale wiggliness in your input data and what else has fine scale wiggliness long time series by essentially very carefully binning the data to extract information on how the wiggles drive a differential equation we manage to extend the neural cde model to tackle long time series up to 17k in length without the usual issues like expoding vanishing gradient or indeed the model just taking a long time to evaluate paper on arxiv code on github thanks for reading questions comments let me know
2,why don t conferences publish a review graph dataset for transparency with recent allegations of rampant collusion in the ml conference industry i wonder why literally no conferences make an anonymized reviewer paper author dataset public one of the prominent themes in ml is graph analytics we can detect communities we can predict links we can detect anomalies and measure hundreds of graph properties why not publish an anonymized graph with review outcomes we re supposed to be doing ml research why don t we apply graph analytics to data generated by the most respected members of our community it can be anonymized fake nodes can be added review scores can be bucketed to 0 1 etc to prevent deanonymization any obvious bad patterns of collusion like cliques and strongly coupled communities should be clearly visible in the data why has this never been attempted the current zero transparency approach seems to be insufficient
1,a concrete introduction to probability by peter norvig i thought r statistics might be interested in this code i just found in peter norvig s github repo teaching probability
0,machine learning is not always the best answer hi i ve seen enough of this trend that every big company especially in north africa is forcing the inclusion of machine learning in every aspect of its activity people are literally misunderstanding how things work the state of art of how to tackle every subject in hand hence creating problems that don t exist it s solutionism at its worst they dumbing down machines that are inherently superior gilfoyle s quote from sv
1,has anyone ever heard of a scissors plot i came across this interesting graph called scissors plot i have never heard about it before has anyone else heard about it is this a well known plot it would be interesting to know if there was some way to roughly approximate the n o point perhaps the n o point could be used to decide if it makes more sense to use complex models or simple models
2,video explaining self distillation with no labels dino paper from facebook ai in our series of videos explaining ai papers concisely we explaing the paper emerging properties in self supervised vision transformers from facebook ai this week hope its useful
0,books blogs suggestions for feature engineering i am working on a data science project in the banking domain it would be helpful if anybody suggests books blogs for feature engineering for the same thank you in advance
0,i m scared for my future i m a college student with a data science major and accounting minor and i m frightened for my future recently i ve been reading about how competitive the market is and i m afraid that when i m finished with my degree then i won t be able to get a job furthermore i struggle with coding i know coding is such a large part of this job so it just hurts me to know that i suck my professor says i m doing the right things going to office hours almost every single day but i feel like i get the concepts but don t know how to create the code i hate the feeling of writing code but it fails but when i get it right it’s like the best feeling ever generally learning comes easy to me i hate to say it but time flew by in high school school came to me easy i graduated with a really high gpa and perfect attendance what a nerd i m not saying i never worked hard at all in school because when it came to dual classes i worked my ass off i feel like i have imposter syndrome and that i m not learning anything i love data and stats but i love the business side of the career even more i like the concept of being able to explain the models and have an impact on the company would the best course of action be to take online python courses in the summer and stick it through also in my course we have three cognates which are inferential thinking business intelligence and analytics and machine learning which would be the best inferential thinking contains mostly statistic classes business intelligence and analytics contains bia and infs classes and machine learning contains a bunch of cs classes thanks ds guys this year has been rough on me mentally this is my 2nd semester and it s been hard after this semester i will have 51 credit hours and i feel like life is moving so quick for me i barely get to hang out with friends anymore and i am pledging for a fraternity mainly for networking so this semester has been my hardest any tips or advice would be awesome edit i haven’t started on my accointing minor at all and i have decided on switching to business administration for my minor to be more educated on the business side of things
1,creating synthetic predictor variables in time series my goal here is to use a predictor variable to assist in the forecasting of performance in a test market where we launched a new media campaign this forecast will be used as the counterfactual in my causal impact model i have time series data broken out by country and am wondering if there are ways of creating synthetic predictor variables to be used as regressors in a bsts model another idea i am playing around with is aggregating a group of regions to use as a highly correlated predictor variable and optimizing towards correlations between those aggregates and the test region but cant seem to formulate a process for this any assistance is greatly appreciated thanks in advance
0,the journey of problem solving using analytics in my 6 years of working in the analytics domain for most of the fortune 10 clients across geographies one thing i ve realized is while people may solve business problems using analytics the journey is lost somewhere at the risk of sounding cliche enjoy the journey not the destination so here s my attempt at creating the problem solving journey from what i ve experienced learned failed at the framework for problem solving using analytics is a 3 step process on we go 1 break the business problem into an analytical problem let s start this with another cliche if i had an hour to solve a problem i d spend 55 minutes thinking about the problem and 5 minutes thinking about solutions this is where a lot of analysts consultants fail as soon as a business problem falls into their ears they straightaway get down to solution ing without even a bare attempt at understanding the problem at hand to tackle this i and my team follow what we call the cs fs framework extra marks to those who can come up with a better naming the cs fs framework stands for the current state future state framework in the cs fs framework the first step is to identify the current state of the client where they re at currently with the problem followed by the next step which is to identify the desired future state where they want to be after the solution is provided the insights the behaviors driven by the insight and finally the outcome driven by the behavior the final and the most important step of the cs fs framework is to identify the gap that prevents the client from moving from the current state to the desired future state this becomes your analytical problem and thus the input for the next step 2 find the analytical solution to the analytical problem now that you have the business problem converted to an analytical problem let s look at the data shall we a big no we will start forming hypotheses around the problem without being biased by the data i can t stress this point enough the process of forming hypotheses should be independent of what data you have available the correct method to this is after forming all possible hypotheses you should be looking at the available data and eliminating those hypotheses for which you don t have data after the hypotheses are formed you start looking at the data and then the usual analytical solution follows understand the data do some eda test for hypotheses do some ml if the problem requires it and yada yada yada this is the part which most analysts are good at for example if the problem revolves around customer churn this is the step where you ll go ahead with your classification modeling let me remind you the output for this step is just an analytical solution a classification model for your customer churn problem most of the time the people for whom you re solving the problem would not be technically gifted so they won t understand the confusion matrix output of a classification model or the output of an auc roc curve they want you to talk in a language they understand this is where we take the final road in our journey of problem solving the final step 3 convert the analytical solution to a business solution an analytical solution is for computers a business solution is for humans and more or less you ll be dealing with humans who want to understand what your many weeks worth of effort has produced you may have just created the most efficient and accurate ml model the world has ever seen but if the final stakeholder is unable to interpret its meaning then the whole exercise was useless this is where you will use all your story boarding experience to actually tell them a story that would start from the current state of their problem to the steps you have taken for them to reach the desired future state this is where visualization skills dashboard creation insight generation creation of decks come into the picture again when you create dashboards or reports keep in mind that you re telling a story and not just laying down a beautiful colored chart on a power bi or a tableau dashboard each chart each number on a report should be action oriented and part of a larger story only when someone understands your story are they most likely going to purchase another book from you only when you make the journey beautiful and meaningful for your fellow passengers and stakeholders will they travel with you again with that said i ve reached my destination i hope you all do too i m totally open to criticism suggestions improvements that i can make to this journey looking forward to inputs from the community
0,how would you explain linear regression to a non technical business partner my approach would be to explain how it s a modeling technique that identifies the line of best fit between the inputs and the desired output that s being predicted i d probably also mention that the framework can explain relationships between features and the individual contribution each feature has towards final output anything else you d add or mention these people are truly non technical so anything to simplify this would help thanks in advance
0,odd question but will a tattoo hurt my chances of being hired in this field currently studying for a bs in data science and i d like to get some tattoos but i don t want to hurt my chances of success i assume something easily visible like a hand tattoo would be a bad idea but what about something like a forearm or ankle sorry i know this is a ridiculous question i just don t have any experience in an office environment haha
1,how important is measure theoretic probability for statistics outside of research right know i am making my masters in math and i wanted to take mathematical statistics for this course it is recommended to know measure theoretic probability how realistic is it that you would ever need some of this theory when you gonna use statistics in industry areas for example as a data scientist machine learning engineer or an actuary for me it seems that this is overkill if you are not planing to go into research what are your thoughts about that
1,official term for model stability is there an official mathematical term for model stability e g for two inputs that are slightly different the same statistical model would produce notably different outputs
2,disrupting model training with adversarial shortcuts it’s not always great that people can train machine learning models on your data in this new work we create adversarial shortcuts to prevent neural network training adversarial shortcuts are hand crafted modifications to images in the training set that exploit simplicity biases in models to prevent them from capturing the semantics of the dataset adversarial shortcuts are also easily ignored by human perception x200b x200b x200b while this idea is more broadly applicable this work begins its study in the context of a well known machine learning problem supervised classification adversarial shortcuts all share a common idea fixing a pattern for each image in a particular class encourages models to fit that pattern over anything else it turns out that even fixing a few pixels prevents the model from fitting the semantics here is an example of an imagenet sized image with a pixel based adversarial shortcut x200b this is neatly illustrated by these plots of imagenet validation and training accuracy progression notice how with the adversarial shortcuts applied the training acc 1 reaches close to 100 while the validation is stuck close to 0 x200b x200b x200b x200b x200b of course the pixel based pattern may be easily disrupted so we also explore more complicated patterns watermarks with the class index made up of mnist digits and brightness modulations x200b x200b x200b x200b x200b read more including ablation studies and comparisons to related work in the new arxiv preprint by ivan evtimov ian covert aditya kusupati and tadayoshi kohno x200b i m posting this for my friend u ivanevti who is the first author of the paper whose posts keep getting mysteriously removed he ll be reading the comments here and will be able to answer any questions you might have
1,is it ok to use transformations if x axis is small to begin with question i am wondering if it s advisable to use transformations when the dependent variable is already small when using sqrt or ln transformation it reduces the x axis to two or three originally an 11 point scale would this potentially impact the test making it more difficult to distinguish differences between the independent variables i m running a two way anova it s pretty skewed by default
0,what are some examples of data science application in israel iron dome technology
1,calculate the probability of an individual having sex through tinder hi guys i want to prove to my friend statistically he has a low probability of having sex via tinder i have the amount of times he’s had sex through tinder the amount of times he’s had sex overall and the amount of times he’s got rejected through tinder what’s the best method for calculating this with the limited data i know it’s a strange one thanks
2,paper explained large scale image completion via co modulated generative adversarial networks large scale image completion via co modulated generative adversarial networks iclr 2021 spotlight is it true that all existing methods fail to inpaint large scale missing regions the authors of comodgan claim that it is impossible to complete an object that is missing a large part unless the model is able to generate a completely new object of that kind and propose a novel gan architecture that bridges the gap between image conditional and unconditional generators which enables it to generate very convincing complete images from inputs with large portions masked out continue reading about co modulation and paired unpaired inception discriminative score in the full paper explanation in the casual gans channel samples from the model full explanation post arxiv code more recent popular computer vision paper explanations gancraft dino mlp mixer
0,continuous learning advice i ve been using r python for data science for about 10 years and i have a phd in industrial engineering we all know this is a field requires constant learning for example i am starting to work on a time series clusterings project which is totally new to me concept like dynamic time warping is so great however i found it difficult to dig into the algorithm i ended up just calling some package to solve the problem rather than trying to understand it inside out science is progressing fast and it s very difficult for me to stay on top of everything what s your experience on keep yourself relevant on the latest and greatest things
0,how to pronounce sql i transitioned into data science from the life sciences in the past 3 yrs and i’m wondering how i can pronounce sql so i don’t sound like a total novice i have heard the developers in my company say it like ‘es que el’ but the data science stat folk mostly say ‘sequel’ which is the correct one
2,for linux users who develop ml dl projects with python what ide do you use in particular what are some advantages to use the popular pycharm i use sublime and i feel good to use it my friend always persuades me into using pycharm what are the adv to use it sync with remote server sublime has plugin to do it or i can use sshfs to mount remote files syntax autocomplete or auto setting the conda env hmnn not appealing enough for me i never get familiar with pycharm anyone can give some badass features for it being the ide for ml dl projects p
1,stat test to use when comparing discount amount to sales i ve taken a few statistics classes and understand the basics up to anova regressions chi square and basic non parametric tests i am currently trying to determine if the discount level on a given product in an e commerce site has a significant effect on level of sales i believe my hypotheses would be as such h0 discount level has no effect on sales numbers h1 discount level does have a significant effect on sales numbers and my data is structured as shown in the following example date sales discount amount 01 01 20 100 765 0 01 02 20 105 643 1 01 03 20 96 765 0 01 04 20 97 000 0 01 05 20 100 765 15 let it be noted that the above data is an example and is only shown to demonstrate the way my data is structured i appreciate any and all help
0,how many people is data science are introverts untill college i was kind of extrovert who eventually with age started becoming introvert i started my career in sales but one thing after another made me switch to data science after 5 years of job i m mostly introvert and data science actually helps me this as many other career options do indeed need extrovert characteristics to break down on how it helps 2 major examples are •communication its mostly through email this helps me avoid unnecessary talking moreover any verbal meeting mostly translate into me listening to the person and identify the problem statement and then let my work speak •technology working on laptop pushing code on git for collaboration etc all further helps me with least human interaction things were it hurts me most •switching job terrible time at interview since now you have to speak and make eye contact •office friends many a times it had happened that i m the last person to get to know of office gossip in my past jobs it was kind off necessity since it helped me know if company will shutdown or not my last 4 company where i worked all shutdown and in 2 i was among last to know that it was going down
0,laws and regulations for working with large anonymised datasets of faces see title i m interested in working with a large dataset of almost 100 000 faces for a research project but i wish to remain within uk eu and us regulations for doing so what are or where can i find the regulations for such work
1,question biostatistics book for data science hello everyone i m a biology undergrad and beside university i ve done some self studying in computer science and data science now my professor advised me to read biostatistical analysis by jerrold h zar i was wondering if this book is enough for statistics part of data science or at least is it gonna be a strong introductory to statistics for me thanks for any answer or advice
1,question canonical correlation analysis how do we get two separate matrices from the cross covariance matrix usually i see the cross covariance matrix defined to be the matrix whose i j th entry is the covariance between the i th element of x and the j th element of y for two vectors x and y but this cross covariance matrix needs to be factored into two matrices because canonical correlation analysis wants to find pairs of vectors a b that maximize the correlation between a t x and b t y the wikipedia page does a good job of explaining this more clearly what i don t understand is how we go from the cross covariance matrix which is just one matrix whose elements are determined from a specific formula to two matrices x and y that can be used in the canonical correlation analysis there are many ways to factor a matrix so i m not sure which to use or if that is even the correct approach
2,research vision transformers are robust learners for some time now transformers have taken the vision world by storm in this work we question the robustness aspects of vision transformers specifically we investigate the question with the virtue of self attention can vision transformers provide improved robustness to common corruptions perturbations etc if so why we build on top of existing works investigate the robustness aspects of vit through a series of six systematically designed experiments we present analyses that provide both quantitative qualitative indications to explain why vits are indeed more robust learners paper code
0,what do you do when you are stuck on a problem i m up against a deadline and freezing up i have a massive dataset and feel like i can t organize it properly to get any insight much less answer the questions posed by the stakeholders any advice out there
1,masters in financial engineering programs or stats phds for quant finance hello i’ve not completely decided on it but have given thought to going into quantitative finance i plan on going to graduate school for statistics ms phd programs but don’t know if it’s worth it rather than doing a two year masters in financial engineering program from what i’ve heard mfe programs are quite competitive and to really get into a good job after you have to really come from one of the top mfe programs cmu nyu and i guess it kind of puts you in the role of quant finance from the beginning so you can’t really apply the degree elsewhere the alternative was doing a stats phd with a thesis related to something in quantitative finance and the benefits to this is the doctorate will show i have the credibility but also because if i decide i don’t really like quant finance i can go elsewhere in other industries with a stats phd for anyone out there in quant finance could you shed light on which of these routes is “better” or maybe speak to the benefits and cons of each and what you would recommend to a student for background i’m a stats major whose math background consists of real analysis courses linear algebra and calculus as well as the other stats classes probability theory etc
2,large memory layer hi nowdays huge pretrained model computes trillions of weight to absort the large dataset ideally such task should be carried by a huge memory layer while keeping the computation part small ideally the memory layer should met the following definitions 1 scalable the memory can be giga bytes while the training cost remains almost constant 2 key value it is a layer that take a key vector and reture a value vector 3 differentialable can be trained by sgd along with other network components 4 sparsity or locality combined with 1 to achived quasi constant update time current softmax attention type network does not met 1 4 thus is not scalable at all some other sparse one seems undifferntialble ad hoc my question is is there some lastest paper on this diretion that i missed and is my definition of ideal memory layer make sense if the definition is ok let s invent it
2,fast ai s jeremy howard on why python is not the future of machine learning gradient dissent clip tl dr python is nicely hackable not fast enough unless you do some tricks with external c code python is also not parallel enough python is good for a lot of things but not for ml dl julia may be an option because it is compiled language with a nicely designed typing and dispatch system and you also can write your own gpu kernels and other low level features like autograd i know that production stuff is already mostly python free like tensorrt c etc we need our models to work fast and efficiently especially on embedded devices but will research shift from python in near future i mean all the fancy modern packages are on python all courses as well what do you guys think about this
1,is there some general relation between bayes tests and u mp tests the neyman pearson lemma in bickel and docksum s mathematical statistics i first constructs the bayes optimal test for the simple testing problem and then proves the bayes optimal test procedure is and is the only the most powerful mp test at the level of its size is that bayes test being mp only a coincidence is there some more general relation between bayes tests and u mp tests or other tests if i am correct likelihood ratio tests for general testing problems are not bayes tests thanks
2,machine learning wayr what are you reading week 115 this is a place to share machine learning research papers journals and articles that you re reading this week if it relates to what you re researching by all means elaborate and give us your insight otherwise it could just be an interesting paper you ve read please try to provide some insight from your understanding and please don t post things which are present in wiki preferably you should link the arxiv page not the pdf you can easily access the pdf from the summary page but not the other way around or any other pertinent links previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 week 1 11 21 31 41 51 61 71 81 91 101 111 week 2 12 22 32 42 52 62 72 82 92 102 112 week 3 13 23 33 43 53 63 73 83 93 103 113 week 4 14 24 34 44 54 64 74 84 94 104 114 week 5 15 25 35 45 55 65 75 85 95 105 week 6 16 26 36 46 56 66 76 86 96 106 week 7 17 27 37 47 57 67 77 87 97 107 week 8 18 28 38 48 58 68 78 88 98 108 week 9 19 29 39 49 59 69 79 89 99 109 week 10 20 30 40 50 60 70 80 90 100 110 most upvoted papers two weeks ago u nerdninja deep reinforcement learning platform at facebook u dl updates byt5 towards a token free future with pre trained byte to byte models besides that there are no rules have fun
1,question why doesn t betting on the odds stabilizing work hi i ve got a question that s really been bugging me why is it that you cannot bet on the odds of a random event stabilizing that question is probably better explained with an example say there is a game where a coin is flipped heads or tails you have to bet on which side it lands on to win you notice that over 100 flips heads was flipped 65 times and tails was only flipped 35 you know that if this game goes on this variance should decrease over time eventually becoming nearly 50 50 why is it that you cannot bet on these odds my monkey brain says that if you keep betting on tails after the game has overwhelmingly been heads over time you should win since the odds are destined to even out over a long enough time at least i think they would so where is the flaw in my logic i know i m wrong that cannot be the case but it one of those things that feels like it should be true even though it isn t i d really love a low level explanation of where the problem in my reasoning is thanks
0,are there any companies out there that don t insist on owning everything you do in your free time anymore or is it standard practice to assume you re a slave 100 of the time as a data scientist these days curious if anyone has managed to land a job where they actually can frolic in their free time currently all of my code any models i make for research or a hobby and all stocks i want to buy are monitored by my company i was very careful to negotiate in my initial contract such that all work done for school would be owned by school me because otherwise my company forces us to send any academic papers we want to publish through a review process where they edit the document and review it in corporate first i ve had to deal with gnarly contracts like this for the last ten years and they re always a bit off putting curious to hear if anyone has had any luck not ending up in this situation i should mention i also had to take down my github when i started at this company and cannot have a blog or social media presence
0,i’m so sick of corporate morons rant hey gang stand back it’s rant time analytics is a new field at my work and i’m here to pioneer it i work in corporate at a large medical devices company i’ve had the luxury of an amazing boss some amazing colleagues and decent budget but for the love of fucking god i am so sick of being thrown responsibility or projects because good ol mary in sales watched a video on “gesture recognition” the ideas are a great and i have a framework for filtering them but the fucking pressure the initiation of projects with 0 data no aim at data collection no quality assurance or risk management and the icing on the cake “we should roll out an mvp in 2 months” what in gods name is that shit i’m the asshole i’m always the asshole “here are my requirements if we wish to complete this project in the given time frame ” “so why can’t you develop it now ” bro for starters i’m not a full fledged software engineer deep learning god i ask for resources or a relaxed time and i get 0 i don’t need advice i know what i need to do i just love this community and felt the need to rant
2,shap value for lstm model i have lstm model named lstm model and i am using shap value to explain model have tabular data import shap explainer shap deepexplainer lstm model x train shap values explainer shap values x test from my knowledge in order to calculate the shap value it uses a 2 number of features model i am interesting in what is kind of algorithm it uses for each individual model this question comes from the following fact when i am plotting of effect x feature on output this effect is linear increasing or decreasing for example when x increasing the effect increases decreasing what is the reason behind this one additional question is it possible to use shap to plot partial dependence plot for the lstm model
0,crossvalidation using facebook prophet hi i have a dataset containing the monthly number of shipments for the years 2014 to 2019 i used the facebook prophet algorithm to fit a basic model however i am not sure how to implement cross validation on this data i could not find any blogs github files that perform cross validation on monthly data is there a workaround for this thanks in advance
2,what is the appropriate reward function for maximizing the distance travelled with a limited amount of resources if my agent is like a drone trying to go the farthest with a limited amount of battery are there readings paper or reward function that suits this i only saw a reward of maximum possible distance minus the distance travelled are there any ways to engineer this reward function
0,explaining results to the business side how do y’all go about explaining results to the business that aren’t what they expected to be for example i have a model that is using variable b as the most important for predictions the business is very upset because they believe that variable a should be the most important for predictions we’ve showed them numerous examples within the dataset that was used for training as to why variable a is not the most important but they’re still dead set on hard coding that into the code regardless if it doesn’t improve the results any pointers here
2,over fitting in iterative pruning in global unstructured and iterative pruning algorithms such as 1 learning both weights and connections for efficient neural networks by han et al 2 deep compression by han et al 3 the lottery ticket hypothesis by frankle et al except the lottery ticket hypothesis where the weights are rewind ed to their original values and resulting sub network is trained from scratch thereby needed more time epoch since the usual algorithm is take a trained neural network and repeat steps 1 and 2 1 prune globally smallest magnitude p of weights 2 re train fine tune pruned neural network to recover from pruning usually the number of pruning rounds needed to go from original and unpruned network sparsity 0 to 99 sparsity requires 25 34 rounds depending on the exact architecture and number of trainable parameters in my experiments i have observed that during this repeated prune and repeat algorithm the resulting pruned neural networks start to overfit to the training dataset which is to be expected apart from using techniques such as regularization dropout data augmentation learning rate scheduler etc are there any other techniques to prevent this overfit i assume that such a resulting pruned sub network when used for real world tasks might not perform as expected due to the overfitting induced due to the iterative process correct me if i am wrong you can refer to my previous experiments here and here thanks
1,question why does e and pi show up in the normal distribution pdf i have often noticed that students and aspiring statisticians learn the formula of normal distribution pdf through rote learning however some curious people do ask the right questions instead of rote learning one such question is why does e and pi show up in the pdf of normal distribution quite clearly there is nothing to do with circle in the pdf for the pi to feature perhaps there is i don t know neither is there a some interest rate for e to feature so what could be an intuitive explanation for why e and pi feature in the normal distribution pdf
2,this ai startup is putting a fleet of airplanes in the sky without human pilots original article here there is no information on what ai models or algorithms were used i can t find any ai ml job openings at their career site only software engineers and flight controls lead i wonder if it is similar to boston dynamics in their limited use of ai and focusing on complex physics models instead the showcase video looks impressive if you like this piece of news you can find more here
2,what are the best resources to crack m l system design interviews hi for those who are brushing on their skillset for applied research roles how do you learn more about ml system design i have seen many resources about algorithms questions and how to cover theoretical ml questions but for ml engineers and for applied research positions there is more emphasis on ml system design interviews there are some resources that i used before such the following i think that the field is relatively new and some of these resources are still not known please feel free to recommend some of the things that you found rewarding in your journey as an experienced ml engineer or applied researcher
2,open catalyst challenge using ai to find catalysts for renewable energy storage x200b the open catalyst project is a collaborative research effort between facebook ai research fair and carnegie mellon university’s cmu department of chemical engineering the aim is to use ai to model and discover new catalysts for use in renewable energy storage to help in addressing climate change scalable and cost effective solutions to renewable energy storage are essential to addressing the world’s rising energy needs while reducing climate change as we increase our reliance on renewable energy sources such as wind and solar which produce intermittent power storage is needed to transfer power from times of peak generation to peak demand this may require the storage of power for hours days or months one solution that offers the potential of scaling to nation sized grids is the conversion of renewable energy to other fuels such as hydrogen to be widely adopted this process requires cost effective solutions to running chemical reactions an open challenge is finding low cost catalysts to drive these reactions at high rates through the use of quantum mechanical simulations density functional theory new catalyst structures can be tested and evaluated unfortunately the high computational cost of these simulations limits the number of structures that may be tested the use of ai or machine learning may provide a method to efficiently approximate these calculations leading to new approaches in finding effective catalysts to enable the broader research community to participate in this important project we are releasing the open catalyst dataset for training ml models the dataset contains 1 2 million molecular relaxations with results from over 250 million dft calculations in addition to the data baseline models and code are provided on our github page view the leaderboard to see the latest results and to submit your own to the evaluation server join the discuss forum to join the discussion with the community and ask any questions x200b
2,epsilon decay sporadic epsilon jumping reinforcement learning hey all see the graphic below i m working on building my first reinforcement learning agent i wrote a simple decay function with an initial epsilon of 100 the epsilon would decay by 1 every single episode as you can see i also set a minimum epsilon of 5 my question is this i implemented a sporadic epsilon jump increase with a 15 chance of being hit if the sporadic jump was indeed hit it would randomly increase the epsilon value by 1 10 and then on the next episode revert back to the previous epsilon and continue decreasing by 1 so on and so forth i m wondering if this would aid training in any way if there is another name for doing this besides my current made up terminology of sporadic epsilon increase thanks in advance x200b exploration exploitation graph x200b
1,career question things i could start to prepare for my future stats service providing business as an undergraduate math student hi everyone i am an undergraduate math student who has decided to pursue a phd in statistics after my current program when considering my future career besides becoming an academic in universities alternatively i also think about starting my own small service providing business related to statistics so that i could apply my statistical skills to the real world and hopefully make a living from that at the moment i know nothing about how to make this idea a reality i don t know what statistical skills are wanted in the market how to find clients as an undergraduate what should i do to prepare for it besides learning my courses well etc i have read some posts related to business in statistics in this sub but i hope to get more advice and insight directly form those who have gone through the process of building their own business some starting things to do and several rough directions will be very helpful for me and a little bit more about my background besides math skills i could program at a basic level c or python i also know how to use matlab and currently i am learning r language
0,interested in mentoring networks hi all i’m a data scientist working in australia i’m the sole ds in my company and all the other ds i know outside of there are at a similar level i am i used to be in academia in a different field so i don’t have a more senior person to talk to all this is to ask if you folks know of any mentoring networks that i could join
1,trading card probability myself and my friends needed 100 more of the 225 trading cards in a collection we bought 50 packets of 5 to make 250 cards we got all of the 100 missing cards to complete the collection what is the probability of this happening assuming each card is equally likely to be in each packet possibly multiple times i am pretty confident the total number of possibilities is 225 250 but really stuck on how to figure out the number of cases where all 100 missing cards are packed my instant thought is that as i bought the packs in a box of 50 to be sold individually in commercial stores the manufacturing process would create a set of 50 packs with as few duplicates as possible therefore including virtually the entire set of 225 thank you
1,new to r and r studio dataset with na values hey i have a dataset with 1028 observations how can i get rid of na values is it neccessary to get rid of them to do my analyse with some graphics thanks in advance
0,lost 700 gb worth of projects codes data sets tutorials reading materials as my external hdd got damaged it was 4 years worth of work and personal projects 😫 whats your data loss story tried recovery software but didnt get any valuable data 😴 please suggest some best practice to store data other than 3rd party cloud storage like google dropbox etc
2,can machines learn covariance this question came up when i was reading generative model paper such as vae i ve figured out as far as i know that probabilistic models does not learn off diagonal covariance matrix instead they tend to learn only mean and variance i assume this is because of the reparameterization trick or some other practical reasons it is quite odd that not many papers directly consider about this issue and this is the reason why i uploaded this post so here is my question 1 is there any reason why probabilistic models tend to avoid covariance terms i want to know both theoretical and practical reasons 2 is there any papers or models that learns covariance terms plus is it useful practical
0,hired as a data scientist not doing data science work i was brought on to pretty large multinational organization as a data scientist thinking i would do data science work right now let me preface with this issue might just be because the client really has no idea what they’re doing and has horrible data management and data engineering and there’s just a lot of pre work that needs to be done but every time my group encounters a data science problem the first though is “let’s just throw it in one of the aws services reckognize comprehend…etc and close out the project” i actually don’t think any of the other “data scientist” on the team are doing any data science more of data engineering perhaps i’m all for fast and efficient solutions but i’m not doing any data science work i get it like why train an nlp model when aws has already done that for you and you can make slight tweaks to it my last job i was able to build models pass them off to the engineering team and move on to the next has anyone encountered this what are your thoughts and how would you respond
2,potential logistic regression closed form solution hey r ml need a bit of feedback here so please give it to me if you can recently i was thinking about how linear regression does have a closed form solution but logistic regression doesn t i ve come up with a pretty lazy idea that sort of does give logistic regression an alternative to an iterative gd derived training process and i would like to share it to get any suggestions on what to do with it you simply formulate the logistic regression problem as linear regression all you do is take the labels and turn them to 100 or any small negative number guaranteed to be 0 when plugged into a sigmoid activation if a given label is 0 and 100 if a label is 1 you feed this into a linear regression solver where you can just use the normal equation and then when needing to predict simply pass in your data to the linear regression model and if its output prediction is closer to 100 than 100 it s 1 and if its closer to 100 than 100 it s 0 this has lead to small 2 decreases in validation accuracy but using a closed form solution does mean it will run faster not sure what to do this need a bit of help here thank you so much
2,discussion what s the implementational difference between ransac on line circle or some other shape discussion what s the implementational difference between ransac on line circle or some other shape if a package says just ransac then what is it line version
0,personal project runtime hey guys so i’m doing a personal project using spotify data to recommend songs and also using kmeans to cluster songs for analysis playlist i use pycharm and i have a pretty beefy cpu gpu but sometimes it’ll take forever to run i’m not sure if my ide is just not optimized or is there anything else i can do to get faster runtimes when creating models etc the dataset has 600k rows if that helps
2,nlp machine learning with low ram hello i am trying to do sentiment analysis on a large dataset of news headlines however every tutorial i find involves two ram consuming tasks getting a list of every word in the dataset creating a sparse matrix x200b is there any way to perform this kind of nlp without a sparse matrix a dataset of tens of thousands of rows becomes near impossible without a machine that costs tens of thousands even with batching you still need to first have a list of every word in the dataset to create your bag of words this alone is enough to cause out of memory errors on a good laptop with even 32gb of ram x200b is there any way to perform sentiment analysis without these constraints
2,questions relating to model evaluation with multiclass roc matthew correlation coefficient rstudio user to anyone who reads this thank you i appreciate the help i have tried searching online but to no avail so i m hoping reddit and people with more knowledge than myself may help as per the title i have 2 questions first currently i am trying to predict a multiclass output of that can be one of 4 different options when trying to use multiclass roc from the proc library to obtain the auc after building models for this model case assume cross validated rf i am getting an error relating to predictor must be numeric or ordered currently my y variable is structured as factors i did not have this issue prior with an output of 3 possible classifications different model is this just a limit of what the capabilities are for this function in other words is this not possible second i have come across matthews correlation coefficient as another means of model evaluation outside of such things as accuracy rmse f1score logos etc is this only applicable to categorical data models that can take factors categorical data i am using the yardstick package here and not to numerical models such as knn or svm i did ohe categorical variables as well as scale them depending on the model if you read this far thank you
0,new job no training too busy to help you we don t have documentation we want ai and machine learning applied wherever i say so even if we don t know what that means how many of you have started a new job as a data scientist and this is the culture you are thrown into how long did have you lasted in this type of company i had come to this company out of a bad situation so i m already pretty jaded and pissed off after almost 5 months i ve had virtually zero success i m ready to jump ship again and pursue consulting full time should i wait to try and get some large contracts first or try and operate in a dual capacity until i get fired honestly it horrifies me that i would be ok with getting fired but i am so tired of doing extra to be successful and overcoming bad management disorganized cultures and lack of support without being met in the middle edit thanks everyone for setting me straight today needed the course correction too easy to wallow in the struggles
1,question about iv regression if i have a model y b0 b1x1 b2x2 set of controls e x1 and x2 are explanatory variables does my instrument have to have a causal effect on both x1 and x2 or just x2 since my endogeneity problem is a measurement error in b2 x2
1,modelling baseline for longitudinal research hi have a question about study design i d hope brighter minds than mine can help with let s say i m trying to run a linear mixed model lmer in r for example with a single repeated measurement random slope and random intercept i have a cognitive measurement as dv the researchers have measured baseline three times over three days prior to intervention let s say for example we treat times 0 1 and 2 coded as time 0 for a repeated measure how would this effect the covariance structure would this just merely provide more variance for the baseline intercept would this bias the slope given the increased n for baseline what would be the benefit of this method over lets say averaging the scores for 0 1 2 as a baseline score i could imagine that having some knowledge of the individuals variability for cognitive score may be useful would there be a third option
0,any actuary transition to data scientists hello i am an undergraduate student looking for more potential career opportunities are there data scientists here from actuarial background who can share more experience on it thanks in advance
0,what made you feel like data science was right for you
0,testing gpt 3 powered survey tool feedback and thoughts hi i m building a tool that asks personalized follow up questions in order to get detailed survey responses it s powered in part by gpt 3 created by openai if you re open to testing it it takes a couple of minutes and i welcome feedback it s imperfect but that s why we re testing please dm me for a link i d also love to hear your experiences building standalone ai products like this
0,what do you use to version control jupyter notebooks i ve been struggling to maintain version control of jupyter notebooks through pure git because of all the issues of git dffing detecting cell output changes and stuff x200b do you use any specific tools to keep up a good gitlfow like version control scheme of your data science jupyter notebooks
0,how much of your time do you spend with boring data tasks because your colleagues cannot code hey when talking to other professional python r users i sometimes hear them complaining that they have to spend a lot of time answering basic data questions for their colleagues just because they cannot code i am wondering what s your perception about this do you have the feeling that you are hired for your data science skills where you are actually working on interesting and challenging tasks or do you spend a lot of your time just bridging the gap for colleagues who cannot code
1,career can you guys recommend me some good statistic textbooks that touch upon the syllabus below preferably by indian authors exploratory analysis design of experiments sampling sampling error sampling bias measures of central tendency and dispersion statistical survey and presentation of data statistical inference confidence intervals correlation formulating null alternate hypothesis type i and type ii errors regression z test t test p value probability basics of probability probability density function pdf and cumulative distribution function cdf standard distributions
0,what is the best structured ds project you have seen i am doing my post grad in data science and do a lot of projects that i think i could structure better from start to finish i look at top submissions on kaggle for reference what is the project you use for reference when doing your projects what is your general structure
2,eli5 explanation of meijer g functions hi i am trying to understand following paper by michaela van der schaar about symbolic meta models although i am struggling with understanding meijer g functions can anyone try to write eli5 explanation for them thank you in advance
0,does the type of company you work for matter for career progression i have been looking for a data science role in the financial sector for quite a while but its been tough with no experience i have an interview for mindshare which is a media agency this is a more analytics position which gives me exposure to projects and clients etc so i thought it would be valuable would it being a media agency primarily limit me in any way or is the analytics experience good regardless x200b
1,should you drop dimensions with poor reliability from an efa in particular icc 39
1,do all machine learning algorithms indirectly use the nearest neighbor principle do all machine learning algorithms indirectly use the nearest neighbor principle i e birds of the same flock fly together for example my naive understanding of neural networks neural networks make a network that is composed of a distinct combination of mathematically significant weights through the backpropogation algorithm that shuttles the data through the network and in the process predicts and assigns them to an ouput label e g 1 or 0 the assignment process has a certain statistical accuracy so for new data can we say that the neural network works indirectly using the nearest neighbor principle e g two similar data points will be shuttled and assigned to similar output labels
2,play with models ml ones i know it’s thrilling to find a cool ml model online but it’s almost always a hassle to deploy it at neuro we’re trying to make ml development easier so today we’re launching a new product which we call the ‘model hub’ it’s a library of popular open source models like gpt neo and detectron2 where you can learn about them experiment with different inputs and see how to use them in just a few lines of code all from our api in that way it’s like huggingface for everything i hope you find it useful the website is still young but we’re planning to add more models every week so if you have any suggestions or if you’d like to see a particular model then just send me a pm here and i’ll try to get it added for you deploying models can be pretty painful at the moment if you do it yourself which is why we built the neuro api so that ml engineers can focus on the fun parts and leave the hassle to us if you believe in our mission – to accelerate ml development – and want to help we’re hiring
0,all the wrong things with predicting purchase churn and similar targets in digital marketing recently i have been reading a lot about common prediction tasks in digital marketing like churn prediction and predicting the probability of purchase on a user level from the articles and books i have read so far and my own understanding as well there are several things that make such tasks more complicated in different ways it s pretty hard to commit the so called type iii error when the model you build in the end doesn t answer the real business question this can be illustrated by the now classical case of churn prediction even though it s not too difficult to build a binary classifier it s not clear at all what the end users can do with those predictions in general it s more useful to do uplift modeling but this creates another level of complexity because you need to conduct an experiment first to even collect the necessary data for uplift modeling trying to interpret the results to stakeholders is another challenging part of the process one of the issues is the falsely obvious belief that correlation implies causation and the graphs you get from plot importance and shap values do not really answer the questions like which feature defines users behavior in addition the traditional metrics used in classification are not really useful it s often stated that there is this well known trade off between precision and recall in most real situations especially when the distribution of classes is heavily skewed what i personally encounter now is that even the business guys do not have a very clear definition of success they just want to get some insights and have a model i usually try to put it this way we have a value of auc score that equals x which is overall a measure of the classifier s ranking ability on top of that here is the cumulative gain lift curve which you can use to understand how useful the model can be as opposed to a random classifier honestly speaking i realize that as a data scientist i have to answer all the questions myself and suggest something meaningful but sometimes it feels like the end users of the models have no idea about what they really want to have in the end which makes everything super complicated to be more specific about the target misconception i just recently found this post by frank harrell that emphasizes the idea that in most real world cases what you really need to output is probabilities this is more informative in general than just binary 0 1 answers that only appear when you make the threshold related decision the same applies to using improper scoring rules that are functions of the selected thresholds it s not often taught in machine learning courses but using accuracy as an evaluation metric might make sense for the iris dataset but much less frequently in real tasks currently i am working on a task where i am expected to predict the probability of a subscription purchase the problem can be easily boiled down to the standard binary classification problem with all the issues mentioned above there is some level of uncertainty when it comes to how the model will be used by the marketing team my simplistic and naive idea is that the pragmatic way around this problem is to get a decently calibrated classifier that outputs probabilities technically speaking not yet probabilities but after some calibration this is hopefully something similar the scores are sorted and top x users based on the best value according to the cumulative gains plot are selected for some form of communication i realize that taking any percentage of users implicitly means that we are selecting a threshold but the focus is different and we do not even need to report the shamefully low values of precision algorithm wise this is just plain old gradient boosting in xgboost lightgbm catboost i have a feeling that a lot of us have encountered similar tasks in this field and i would highly appreciate any advice and discussion if you have any great resources that discuss such tasks and approaches in detail please mention them as well here s one freely available book i like very much
0,help needed has anybody here have bought the udemy course the complete machine learning 2021 10 real world projects and if you have done can you guys lend me the course
1,estimating sample size required for an instrument variable study hello i m designing pretty much a textbook instrumental variable study paraphrasing the actually set up we re planning to offer in store discount coupons and trying to assess whether providing coupons is a good idea given there will be a clear self selection involved more loyal customers will opt in to more discounts we re sending a mailer with discount to a random subset of customers and a mailer with no discount to the control assuming that we have a strong first stage i ll estimate the true effect size of incremental discount via a 2sls however in this scenario how can i decide how many people to send the mailers to
2,are self driving cars really safer than human drivers more than 250 billion has been invested in self driving vehicles over the last three years more than 35 000 people die every year in motor vehicle crashes in the us alone since self driving vehicles can theoretically react faster than human drivers and don’t drive drunk text while driving or get tired they should be able to dramatically improve vehicle safety but will self driving vehicles actually be safer check out our new article on this topic
1,looking for gp kernel i am looking for a gp kernel i have a series of populations with binary features i want to predict and i have latitude and longitude information additionally i have population size estimates for each population i want to fit a 2d gaussian process with the latitude and longitude information but also take into account the population sizes the idea is that the large population sizes should reduce the absolute distance between two populations while small population sizes should increase it i think that a 3d approach or multiplying kernels would the wrong approach because i do not expect population size to have any direct impact on the features themselves only in the amount of covariance between two points thanks
2,pytorch 1 9 released with new mobile interpreter inference mode and new packaging format a ton of new updates i m mostly excited about the updates to the mobile stuff
1,university major not sure if this is the right place to ask but any answer is greatly appreciated i m in my second year out of 3 of university but still have the space to change majors i m unsure what i want to do after uni but definitely in trader quant ds ml field currently i m taking a ds and econometrics major at university of sydney in australia i m wondering if these majors are the right ones to pick for the jobs listed i didn t take ds stats because the way i m specialising in ds i will already be qualified for a stat major even though not recognised by the uni the other major i was thinking about is financial maths and statistic which goes through optimization stochastic processes financial derivatives and the likes currently in econometrics i m taking classes like forecasting econometrics in ml financial markets and in ds i take probability applied linear models and statistical machine learning my main question i guess is am i on the right track in terms of choosing classes and is it worth for me to switch my econometric major to the financial mathematics and statistics thanks in advance
2,neural networks inside an unconstrained optimization problem suppose there is a black box unconstrained optimization problem the objective it to minimize a given function f which is a scalar function several inputs one output by black box i mean that it is difficult to compute the gradient or even impossible of the function and every evaluation of this function is quite costly inside this black box function there is a neural network n that serves as a parametrization in a specific section of the computation of the black box function the idea is to find the weights of the neural network that can minimize this black box function unfortunately there is no data set that could be used to train the neural network in this case the idea is just to adjust the weights such that the optimization problem is solved i have some questions 1 is this feasible if it is what is the best way to approach such a problem 2 maybe neural networks are not the best way to approach such problem instead of a neural network can any other machine learning method approximate the parametrization inside the black box function such that the unconstrained optimization is solved 3 is there a research field which i could look into for similar problems
1,can you get a r squared and p value for a non linear regression let s say i plot a graph between two variables and the y axis is linear scale whereas the x axis is log scale the regression line would be y a ln x c with a being the slope and c being the intercept i can get an r squared value in excel for this regression line but not a p value in excel furthermore i read online that there is r squared and p value are not valid for non linear regression am i correct in saying this i don t really understand the reason why if the r squared and p value are not valid what ways can i get a measure of how good the regression is thanks in advance
2,labelbox threatens to sue small open source startup diffgram hi everyone did some of you already read this article what s your opinion on this
0,how would you feel about a handbook to cloud engineering geared towards data scientists think something like the 100 page ml book but focused on a vendor agnostic cloud engineering book for data science professionals edit there seems to be at least some interest i ll set up a website later this week with a signup mailing list i will try and deliver chapters for free as we go and guage responses
2,ml engineers do you find that you make yourself redundant was having a conversation with a friend the other day there s a bit of a principal agent problem going on with ml engineering as well as other fields i m sure if you do a perfect job as an ml engineer you make yourself 100 redundant or as close to as possible but if you are redundant you become replaceable if i build an ml pipeline that detects performance drift automatically retrains has error handling keeps costs low etc and provide perfect documentation there s no need for my involvement unless something goes completely off the rails if on the other hand i build a broken mess that no one else dare touch because no one can wrap their head around all the criss crossed band aid fixes then i m literally irreplaceable i guess it depends a lot on your company culture the company i work for has a few ml projects that come in here and there and i m finding myself worrying that i ll wrap them up way quicker than they come in the better job i do the more idle i look which is weird anyone else have similar experiences seems ml engineers are in need of a steady stream of new problems to solve vs data scientists who can be working on the same problem for years e g incrementally optimizing a profit yielding model
1,heteroskedastic binary probit regression model i hope this is the right place for this post i am currently working on a project where i analyze under which conditions the heteroskedastic probit regression model with an extended link function outperforms the common probit model this project gives me a lot of headache unfortunately since i am from the finance sector and not a statistictian i now have multiple questions and would be really grateful if you could answer some of them i use r for the project i ran both models over my dataset which consists of about 20 mostly categorial variables and optimized the model the hetglm model from the glmx package with the extended link function however does not really perform better than the common glm model even though the source of the dataset claims that heteroskedastic is present now to my questions how do i have to understand heteroskedastic in the framework of a binary choice model thats really hard to understand for me and how would i check for it i read that i can use the likelihood ratio test to check for het or do i only check if an extended model is a better fit i also read that i can use the lm score test to check for endogeneity which is often caused by het is this the case and how would i implement such a test for a probit model in r source this source suggests to use a reset test and other sources suggests that there is no point at all to care for het in this context there is not a lot of easy to understand literature but a lot of papers which all suggest different things now i thought it might be best to run a simulation study and implement heteroskedasticity of various degress myself to check on when the het probit model performs better but also here i am unsure how to control for het and properly run it since again it s really hard for me to understand het in this context i would be glad if you could help me i can add additional sources and r code if you want
1,how to analyze survey based data with likert scale predictors and a either binary likert outcome i m designing a study for my master s on factors influencing information system acceptance the literature so far has provided conflicting and inconsistent data as to how to analyze the results i see some sources treat likert questions as continuous and thus fit for linear regression if outcome variable is a likert while others classify them as nominal i am carrying out a survey with relatively representative sample of my target population here is how my predictors and outcomes are structured predictors 4 main constructs with 4 5 point likert scale questions each the responses under each construct will be added to obtain a score from 5 20 outcome i am trying to decide which is best for an outcome variable my two alternatives so far are as follow a binary outcome with either a positive or negative decision eg do you believe you will use this system within x amount of time or another or multiple likert scale question to measure the strength of their intention decision for example how likely are you to use this particular system analysis goal to see how well each of the four constructs with a score from 5 20 is associated with the outcome variable preliminary research points towards multinomial logistic regression which i know is used in classification type problems if it is not too much to ask can i get some insights as to what is the best way to analyze my data which type of outcome is best binary vs likert or what types of analysis i should read up on edit some questions can i do multiple linear regression if both my predictors and outcomes are likert is multinomial logistic regression a real option does having a binary outcome turn this into an easier classification problem and do you think it is recommended in the context of the study thanks a mil
0,multiple stock predictions most of the stock price prediction tutorials use a single stock as example with lstm models although stocks have been proved to be random walks and containing to use lstm for real time predictions is not recommended say that you stuck to lstm how does it happen in real world for 10 different stocks it could be 20 50 etc do you train 10 different lstm models or use a different approach the idea is to tell which amongst these 10 stocks would rise and which ones will dip thanks
2,should i submit a critique paper to cvpr i have a critique paper that pretty convincingly critiques papers one of which was published in cvpr should i submit my critique paper to cvpr more context the papers i am critiquing mostly introduced very marginal changes to existing methodology and applied it to an affective ai task i critique the dataset show there is large dataset bias and show several methodological flaws e g significant random errors unreported
1,no free lunch vs neural networks i read that there is a theorem called the no free lunch theorem that states there is no single algorithm that is better than all other algorithms this is somewhat obvious complex algorithms should perform better on complex problems and simpler algorithms should perform better on simpler problems this being said why have neural network based algorithms been accepted as the main type of algorithms for solving complex real world problems apart from the simple answer that they from observation they simply perform better on these problems if we were to look at neural networks e g mlp lstm cnn reffering to the mathematical properties of neural networks how what could we attribute their success to why do we use neural networks instead of regression models from a certain prespective are neural networks defying the no free lunch theorem
0,how do you motivate yourself to pursue your own projects in your free time when working full time like the title says i m struggling to spend my free time doing extra projects there are tools and project ideas that i want to explore but when i work m f full time it s just so hard to spend my evenings weekends doing this i m pretty early on in my career so i don t have family commitments but i really need my own time to recharge the weekend just flies by and it s been more than two months since i decided to do my own projects but nothing s really materialized anybody struggled with this and any advice on how to overcome this
1,how to understand taking a simple sample to compute the confidence value and confidence interval using clt suppose there is a population distribution having mean m and standard deviation sd we want to estimate the confidence interval by using a single sample using clt we take a sample and compute it s mean say sm and standard deviation say ssd the distribution of sample means is normally distributed with a standard deviation close to sd sqrt n where n is the sample size and mean say msm close to m we can understand this process in two ways 1 that what we are trying to compute is the range msm 2sd sqrt n this is what our confidence interval is now we approximate replace msm with sm sd and ssd are quite close so sd sqrt n can be written as ssd sqrt n now we can compute the confidence interval msm 2sd sqrt n using the sample mean and sample standard deviation but the problem with this understanding is that as a result of our process using clt and a single sample we can end up taking a sample having an exact mean of msm 2sd sqrt n or msm 2sd sqrt n with a 95 probability in such cases the confidence interval we will be computing are msm to msm 4sd sqrt n or msm 4sd sqrt n to msm which will be 50 off the confidence interval we were meant to calculate so how one solves this contradiction the second way might have the answer 2 that we assume msm is actually quite close or same as m population mean then we say that we have a 95 chance of taking out a single sample which has it s mean within range msm 2sd sqrt n in which case it will be 100 certain that our msm or population mean is within 2sd sqrt n of the sm sample mean so that is how it comes out to be our confidence interval but if the second approach is indeed correct that would mean forgoing the idea that the distribution of sample means provides a confidence interval of the population mean m around it s mean msm and will that be correct thanks
1,hi all i made a python package to plot distributions calculate probabilities check it out hi everyone i m working on a python package to do some applied statistics i learned a lot of the theory behind this in mathematical statistics so i wanted to try programming it all from scratch to do things like plot distributions calculate probabilities find mle s and perform hypothesis tests check it out here there s a jupyter notebook with examples of how to use the module here obviously you could probably do all of this with scipy but i m mostly making this for my own learning sake i also like how i was able to make plotting and filling in the distribution tails fairly quick that depiction of a plot was always helpful to me when i was learning about these distributions
0,can someone help me understand horizontal vs vertical scaling i ve been learning about apache spark and got into the topic of horizontal vs vertical scaling my company uses mysql and i ve read that mysql can only be scaled vertically in terms of master and can be scaled horizontally in terms of slaves my question on two different topics are is the idea of distributed computing with apache spark similar or the same as the master slave relationship it sounds like to me both are just using multiple servers for different purposes what makes it so that mysql can only be scaled vertically in terms of the master and other databases able to be scaled horizontally
2,what is the best package for combined speech recognition and diarization on long conversation audio files i have maybe 1000 hours of audio recordings i want to convert to text with timestamps to match diarization timestamps or at the minimum at least convert to text without diarization the files are a few hours each and add up to maybe 200 5hr sessions quality isn t always great but a human can clearly understand what is being said approaches i have tried mozilla freespeech convoluted installed no diarization kaldi also somewhat convoluted install could revisit speechbrain with huggingface pretrained got working but attention model may need 30 second or less inputs worried about splitting 6 hour session into 30 seconds and the information loss
1,decision trees and random forests for regression are there any advantages in using decision trees and random forests for regression compared to standard regression models it seems to me that there might be certain advantages for decision trees and random forests for classification problems e g more logical interpretations easier to combine categorical and continuous variables together but are there any advantages for using decision trees and random forests for regression problems compared to standard regression approaches such as glm thanks
2,feasibility of training model what’s the feasibility of training a model to view a video of a person’s head upper body as if you were on a zoom call and be able to estimate the person’s heart rate and or respiratory rate i work in data science so have a decent idea of ml ai techniques but don’t have a good intuition about what’s hard or feasible in a setting like this any insights are much appreciated
0,style guides for jupyter notebooks i started working my first job as a data scientist at a small startup and was wondering if other companies developed their own style guides or conventions for producing plots within jupyter notebooks something for stylistic consistency as exploration is being done and could be shown off to other team members is that something that s done more on an individual basis
1,understanding likelihood in simple language supposedly likelihood means the probability of observing your data given your model how do you understand this suppose i want to make a regression model that predicts someone s salary based on weight and height i have a dataset of measurements for salary weight and height i make a regression model e g salary 15 height 23 weight 7 i struggle to understand how the definition of likelihood applies here the probability of observing your data given your model i understand that maximum likelihood estimation can provide you with equations to estimate the values of the regression coefficients for height and weight but in this question what does likelihood mean i made a regression model using available data why should i be interested in the probability of observing this data given my model what exactly does that mean i have seen software like r and sas where you can calculate the likelihood value for a regression model but i am not sure if this answers the question can someone please explain this wouldn t the data still exist even if i had not decided to make a model how is the model related to or influence the existence of the data should it not be the other way around the model coefficients depend on the data thanks
2,deepnote – a collaborative python notebook in the browser we just made it free for small teams and it s always been free for academic use hi everyone i m a software engineer working on deepnote we are building a collaborative python notebook that runs in the browser we have just made the platform free for teams of up to 3 people our ceo wrote an article explaining our reasoning behind this let me know what you think or if you have any questions looking forward to your feedback
2,filling in large number of bib entries in survey paper hello i have been working on a survey paper that now has 170 bibliography entries which so far have been just stubs paper title online link etc now that the paper is close to the finishing line i am curious how can i avoid having to fill in all the bibtex fields that journals require i am just completely sandwiched with other work is there some automated way to just get the right fields filled in or should i try to get some paid help through others who are willing to help me with this
2,should i care about statistical relational artificial intelligence hello i am a mathematician with a minor in cs and i find ml quite interesting so that i am choosing more of the ml courses i could imagine working in that area not research and here is the thing there is a course on statistical relational artifical intelligence which seems to cover the topics of a book with the same title there is not so much about this topic in the internet so i doubt that it would be useful for me later i don t know if this course covers topics mostly relevant to research and i am no expert so i wanted to ask you
2,in neat neuroevolution of augmenting topologies algorithm is speciation really effective i tried implementing neat algorithm from scratch and it successfully solves xor problem i followed the original neat paper however when i run xor problem solving test and calculate average convergence generations it converges slowly with speciation and it is much better and faster without speciation complexity of the solution was also smaller without speciation original author of neat emphasizes the importance of speciation so my implementation or parameters must be incomplete somewhere but i failed to find the evidence that speciation actually helps the algorithm to work better is there any performance evaluation resources regarding effectiveness of speciation thank you very much here are my parameters input number 2 output number 1 population 150 hidden activation sigmoid output activation sigmoid mutation weight perturbation 0 8 weight assign 0 1 add connection 0 5 remove connection 0 5 toggle connection 0 0 add node 0 2 remove node 0 2 weight min 15 0 weight max 15 0 perturb min 1 0 perturb max 1 0 speciation c1 1 0 c2 0 5 compatibility threshold 8 0 elitism 1 survival rate 0 07 implementation source repository
0,at what point does data visualization require photoshop i am looking at trying to make much more attractive data visualizations using r software i came across articles like this that tend to show you the capabilities of data visualization these graphs look amazing i tried to search online for tutorials that show you how to replicate these kinds of graphs but all you end up seeing is how to make the basic graphs nothing wrong with that but nothing which allows you to make it pretty e g all the fancy labels blocks of text cool colors etc at this point i am starting to think that some of these graphs might have been created with advanced software such as photoshop and not so much using python and r does anyone have any ideas about this thanks
2,reinforcement learning for inverse kinematics i am trying to explore this area for my research need help with understanding problems shortfall with current implementations also has anyone used the learned model on the actual robot what were your learnings
2,deep learning for ai by bengio lecun and hinton discussion of the origins of deep learning a few of the more recent advances and future challenges by deep learning godfathers original article here more hard to find independent stuff related to ai data science here
2,what do we know about brain neurons and why can t we recreate one with access to so many advanced physical measuring techniques why can t we measure exactly how a real neuron behaves create a computer function to do the same and let it learn i imagine one could measure what kind of input causes what kind of output it s mostly electrical and chemical signals what is the closest mathematical description of what we know about a brain neuron which part is missing that we cannot measure to create a learning algorithm do we know if the brain sends any kind of signal backwards
1,demonstrating relationship between data what kind of analysis would i need to perform to see if there is some kind of relationship between the following data i have a data set where i have daily ems visits maximum patients in the ed on a given date maximum number of patients boarding in an ed on a given date and average boarding time for a given date i would like to see whether there is a relationship between number of ems visits and the other data listed above ie when there are many patients boarding in the ed there are fewer ems visits or when there are many overall patients in the ed there are fewer ems visis or when admitted patients are boarding in the ed there are fewer ems visits and whether there is a statistical significance to this relationship i have limited formal stats background i like data and extracted this data manually on excel from some clnical data but am stuck in terms of making further use of this data due to my limited knowledge of stats i don t have any special stats software beside excel can i do this analysis on excel thank you
2,alias free gan abstract we observe that despite their hierarchical convolutional nature the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner this manifests itself as e g detail appearing to be glued to image coordinates instead of the surfaces of depicted objects we trace the root cause to careless signal processing that causes aliasing in the generator network interpreting all signals in the network as continuous we derive generally applicable small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process the resulting networks match the fid of stylegan2 but differ dramatically in their internal representations and they are fully equivariant to translation and rotation even at subpixel scales our results pave the way for generative models better suited for video and animation paper
2,is highly imbalanced classification largely considered a solved problem i ve recently moved on from a position where i built fraud prevention models our typical class ratio was something like 2500 1 negative positive our typical method was to use xgboost with positive weight scaling in our hyperparam search space amongst other params we got decent results but i always felt there was more we could do however almost all recent news and publications seem to surround unsupervised learning deep learning transformers etc i m wondering if there have been any semi recent advancements in this field of study engineering new features is an obvious next step but i m wondering if there s anything we ve been missing on the training algorithm side of things
1,research question alternatives to latent profile analysis for a master s thesis hi r statistics i m so glad to have found this lovely community and was wondering if you kind folks might be able to help me with a quandary i m a clinical psychology student with an interest in eating behavior i am currently designing a study for undergraduate women that is aiming to use data collected from psychometric measures of eating to create profiles of different eating styles currently i have 5 measures i m hoping to use three regarding restrained eating one asking about intuitive eating and a final regarding binge eating originally my plan was to use latent profile analysis to form groups using variables from these instruments for example i might see groups of dieters non dieters intuitive eaters binge eaters etc emerge i was then planning on using these groups to run some anovas testing for differences in outcome variables of depression nutritional quality yada yada the problem apparently you need 500 people to run a meaningful lpa no way i can manage a sample size that big in this coming year do any of you know of a statistical analysis i could run to group sort individuals based on their scores from multiple measures ideally something that could be sufficiently powered with 100 200 people thank you for any help you can offer i d be happy to add more details as needed
1,having an a priori estimate of an interaction coefficient in logistic regression i ve posted on this topic before but this is a bit of a different question i am coming up with estimates of coefficients for a logistic mixed effects regression in order to simulate data for a power analysis i am struggling with the interaction coefficient let me start by describing the general pattern i expect i ll make up a study to be able to do that the dependent variable is recognition accuracy i have two binary predictors that are effects coded i expect the general pattern to be like this iv1 a iv1 b iv2 a high accuracy medium accuracy iv2 b high accuracy low accuracy how would i go about estimating a coefficient for the interaction edit iv1 and iv2 are my two predictors each with two levels a level a and a level b
1,how to build conversion tables from event logs would love feedback on this piece especially the term conversion analysis as a generalization of survival analysis
2,what are the current ways to compress time series data into a feature i m working in taking time series data over a span of 4 months and compressing it into a single feature as an input into another model i know there are things like lstm and gru but i don t know if the memory cell is large enough to hold a good latent representation i was considering vae but i think converting tabular data to an image is probably tricky
1,why does survival analysis tend to focus on sub populations instead of focusing on individuals does anyone know why the field of survival analysis tends to focus more on studying survival hazard and mortality rates of sub populations within medical studies e g how a certain medical treatment affects males vs females instead of individual patients many times i see how for instance the cox proportional hazards model can be used to estimate the survival and cumulative hazard functions for sub populations in a medical study and then how to use various parametric and non parametric tests to evaluate whether there are statistically significant differences between these sub populations but rarely do i see survival analysis being used to study individual patients is there a reason for this or is it just a practical thing researchers are generally more interested in extending their research to larger groups of people e g researchers are more interested in knowing if a certain drug will harm a large populations of males instead of just alex or john are survival analysis models ever used to study individual patients
1,specs in computer for master in statistics hello everyone this is my first post in the sub sorry if i make any mistake i would like to get some recommendations on what to look for in a new computer laptop to use it for r mainly next autumn i will start my master in statistics and coming from an economics degree i have not really needed a specially fast powerfull computer till now my knowledge in hardware is also very limited right now i have a surface 4 which i bought for university and mainly use it for my studies and normal things like checking email and so however as i have been working on my degree s thesis i have noticed that it is not fast at all when working with a larger data size so i think its time as a r computer is definitely coming to an end for reading underlining documents and doing an average use of internet still works perfect i have read that the more ram i have the better but apart of that i know nothing else any advice would be helpful i m also in europe if that matters thank youuu
2,machine learning baseline model assume you work for an apparel retailer as a data scientist and your task is to send out 100 000 advertising mail pieces to past customers about a new line of fall apparel you have access to a database of information about past customers including what they have purchased and their demographic information ramakrishnan 2018 what model should you build first a baseline in this article we will discuss what a baseline is and where it fits in our data analysis projects we will see that there are two different types of baselines one which refers to a simple model and another which refers to the best model from previous works a baseline guides our selection of more complex models and provides insights into the task at hand nonetheless such a useful tool is not easy to handle a literature review lin 2019 mignan 2019 rendle et al 2019 yang et al 2019 shows that many researchers tend to compare their novel models against weak baselines which poses a problem in the current research sphere as it leads to optimistic but false results a discussion on such contemporary issues and open ended questions is also provided at the end followed by suggestions aimed at the community
0,is it common to feel like you have no idea what you re doing in an internship i m a senior at university and i got a pretty nice internship somehow i keep getting assigned work with nlp stuff that i don t know how to do i read the theory behind it some time ago and i watch youtube videos but i haven t had the opportunity to practice yet is this feeling of not knowing what you re doing normal i ve mainly worked with basic machine learning in the past not much deep learning x200b edit thanks for all the responses guys i had some anxiety coming in this week and its settling down lots of great advice here as well
1,confused about the sensitivity of a medical test or any binary classifier vs probability of positive given that the test is positive data scientist here looking back on some basic stats hope this doesn t qualify as homework question it definitely isn t homework but it s basic enough that i m embarrassed to ask i ll take the example of a medical test as it is easier to reason about true positives etc in this context but this applies to any binary classification problem suppose our test is applied to a sample of 100 people with the following results sick healthy positive test 19 tp 20 fp 39 negative test 4 fn 57 tn 61 23 77 100 x200b the sensitivity of that test is defined as the proportion of true positives tp to the total number of positives so in this case sensitivity 19 39 0 48 now i was wondering suppose we know the test is positive for a subject what is the probability that that subject is actually sick this is p sick positive test which basic stats says is equal to p sick ∧ positive test p positive test but from the table above p sick ∧ positive test 19 100 p positive test 39 100 and thus p sick positive test gives the exact same result as the sensitivity which is obviously wrong according to what i ve read about sensitivity where what am i doing wrong here
0,text analytics lda topic creation any success stories i ve been doing some text analytics work and am trying out lda latent direchlet analysis as a way of categorizing text into different topics i ve used it for two sets of survey data one from local post offices and one from medical education and some police report data i m unwhelmed by the results for the data sets i d done topic modeling on by hand and have a fair amount of domain knowledge i ve played with varying parameters but had a fairly clear idea going in what a reasonable number of categories would be the category clusters use common words but i can t think what the clusters are supposed to represent even when i go back and look at the quotes that best match those categories for example with the post office one you d think damaged stuff late mail missing items or like my mail guy would come out but i can t go from the word clusters suggested by lda to any of these obvious categories in the medical stuff one concept that gets mentioned a lot is broken arm syndrome and i find that broken and arm appear in different clusters there s nothing else in this data set related to broken or arm separately i m wondering if text size is the problem while the corpus size is at least 10k s of words and 1m in some sets the individual quotes are almost all 100 words with some only a few could that be the problem those of you who have some experiences with real data sets and lda could you confirm or deny whether you ve been successful i m best in r but i ve done some work in python and sas viya also and done lda in all of them before getting into real data i ran through some online exercises in r and python for sorting chapters of classic books you know where you get 20 000 leagues under the sea and nemo submarine and so forth pop out and pride and prejudice where elizabeth and proposal pop out and so on
0,propensity score matching and then testing to compare group outcomes i need to bounce some ideas around and am looking for any input i have two groups one that benefits from a long ago implemented policy and one that does not i have data for the current year of these groups i don’t have data pre intervention i want to understand if the treated group offers any financial savings compared to the non treated group outcome variable is total cost per person in each group but other outcome variables are of interest too would it be unreasonable to propensity match and then use t test to compare different outcomes or another test depending on my metric i want to psm so it’s apples to apples some members are more costly than others and this could account for that to add extra layers of complexity from one year to another people could actually switch between groups i am not sure how to account for this please let me know if this isn’t a good sub for the question and point me somewhere else this sub is more active than stats
0,anyone got a side hustle i am a full time data scientist but thinking about pivoting some of my skills on the side to get some extra income anyone have experience having a sperate company or offering certain services
0,media pluralism and media development ranking for countries partially because of my profession photojournalist and partially because of sheer curiosity i started thinking about some sort of media index what would be some good indicators of media development to compare countries newspaper circulation number of newspaper titles number of television stations internet news analytics per capita there definitely are countries where there are more newspapers to choose from there are also countries where people give less shit about news and media how to find the most news obsessed and media curious country
0,what no code tools are you using everybody talks about no code but actually what no code tools are you using at your job do you have any specific tasks that you would love to replace with a no code tool
0,what tools are available for personal use i have some data mostly in excel that i would like to build some dashboards for and break down into understandable pieces but it seems like most tools like tableau don’t have a free personal tier option are any tools approachable for someone without deep pockets
1,question are generalized linear models glms linear i understand the difference between linear models linear regression and glms logistic poisson bernoulli and gamma regression i have always thought of glms as non linear models however i stumbled upon an online resource which states that the discoverers of glms consider it to be linear here is quote from the term generalized linear model glim or glm refers to a larger class of models popularized by mccullagh and nelder 1982 2nd edition 1989 in these models the response variable yi is assumed to follow an exponential family distribution with mean μi which is assumed to be some often nonlinear function of xitβ some would call these “nonlinear” because μi is often a nonlinear function of the covariates but mccullagh and nelder consider them to be linear because the covariates affect the distribution of yi only through the linear combination xitβ given this background my question is can generalized linear models be considered non linear models or not
2,paddlehub an awesome and easy to use pre trained models toolkit hi all i am glad to share that my team are working on an open source repository paddlehub which provides 300 deep learning pre trained models in practical paddlehub aims to provide developers with rich high quality and directly usable pre trained models x200b code： x200b features set abundant pre trained models 300 pre trained models cover the 5 major categories including image text audio video and industrial application all of them are free for download and offline usage quick model prediction model prediction can be realized through a few lines of scripts to quickly experience the model effect model as service one line command to build deep learning model api service deployment capabilities easy to use transfer learning few lines of codes to complete the transfer learning task such as image classification and text classification based on high quality pre trained models cross platform support linux windows macos and other operating systems x200b x200b some visualization demos
2,is there a particular reason why td3 is outperforming sac by a ton on a velocity and locomotion based attitude control i have adopted a code from github to suit my needs in training an mlagent simulated in unity and trained using openai gym i am doing attitude control where my agent s observation is composed of velocity and error from the target location we have prior work with mlagent s sac and ppo so i know that my sac openai version that i have coded works i know that td3 works well to on continuous action spaces but i am very surprised how tremendous the difference is here i have already done some debugging and i am sure that the code is correct is there a paper or some explanation somehow why td3 works better than sac on some scenarios especially on this since this is locomotion based of the microsatellite trying to control the attitude to its target location and velocity is that one of the primary reason each episode is composed of fixed 300 steps so it is about 5m timesteps x200b
0,what is your ds stack and roast mine hi datascience i m curious what everyone s ds stack looks like what are the tools you use to ingest data process transform clean data query data visualize data share data some other tool process you love what s the good and bad of each of these tools my stack ingest python typically it s not the best answer but i can automate it and there s libraries for whatever source my data is in csv json a sql compatible database etc process python for prototyping then i usually end up doing a bunch of this with airflow executing each step query r studio popsql python pandas basically i m trying to get into a dataframe as fast as possible visualize ggplot2 share i don t have a great answer here exports dropbox or s3 love jupyter ipython notebooks but they re super hard to move into production i come from a software engineering background so i m biased towards programming languages and automation feel free to roast my stack in the comments i ll collate the responses into a data set and post it here
2,cnn on mel spectrograms vs wavenet for audio recognition so i am creating an app in which i intend to rank a user inputed song according to its similarity to jimi hendrix songs similar projects have been done before and from what i ve seen the go to approach is to train a cnn on mel spectrograms this is also what was suggested to me well today i was talking about this in an ai discord and someone told me that the cnn on mel spectrogram approach is dead and that using wavenet is superior however i have found no literature nor resources on using wavenet for audio recognition i m still in the stage of needing my hand held on these types of projects i don t have enough experience under my belt to extrapolate for this kind of thing having only done a few machine learning projects is wavenet truly superior for audio problems its focus seems to be on generating audio what are you all s thoughts
2,have you used implicit layers in your research or personal projects i work with physics based time series models and not only did the neural ode paper blow my mind but it really changed the course of my research the training is currently slow on these methodologies and they’re fairly specialized which perhaps explains the lack of mainstream appeal so far but in my opinion neural odes and similar methods will come to dominate machine learning research in the physical sciences anyone been working on implicit methods for deep learning problems thoughts on their future prospects here is the 2018 paper in case you are interested
0,is it possible to self teach myself data science i am a sophomore student who is in college but i am interested in learning data science i am about to graduate college in two years but i am interested in learning math as a degree the applied math degrees are computational mathematics and financial mathematics i am interested in learning at least enough to build projects and show a good portfolio i just wanted to know how i can teach myself data science and build a good portfolio how exactly hard will it be to get a job without a degree in computer science
2,what are the best metrics to evaluate vae performance hello all i start to work on generative models especially vaes and i find the reported metrics a bit confusing there is not any problem regarding the very basic ones such as squared error etc i find the likelihood metrics a bit confusing there is the conditional loglikelihood of a datapoint which is given as q z x log p x hat z there is the marginal loglikelihood log p x which is calculated by importance sampling my first question is which one is the correct one to use what do they represent and is it okay to use importance sampling to compute conditional log likelihood too i e is it okay to sum 10 samples of loglikelihood values for all datapoint then average them or the log term is a problem thank you
1,using a t test to compare baseline and end value hello i hope someone here would be kind enough to help me with this i have a group of people with weight data which we measure during 6 weeks the average weight increases towards the end now in a similar case one of my colleagues has done a t test to compare whether the difference between the baseline mean value and the final mean value after 6 weeks is significant however a statistician i talked to said that a t test in this case doesn t make sense and that one shouldn t do it could you help me understand why or did i misunderstand something thank you kindly for answering my question in advance
0,effective sql for data science these last couple of years i ve spent a lot of time writing sql i put together some lessons learned to use it effectively for data science projects small things like using ctes auto formatting and jinja have made a huge difference for me what other recommendations you have to master sql for data science
1,best resources to learn basic statistics hello all first i apologize if this question has already been answered i couldn t find a useful answer through google so i m here a link to another thread would be awesome if such a thing already exists i m wondering what some good resources are for learning basic to intermediate levels stats i took a basic stats course about 3 years ago and i have more knowledge of excel than the layperson but very little knowledge relative to most all of the people in this sub lately i ve become interested in sports data mainly nba mlb and nfl fast forward 3 years i m graduating in about a month and i have the summer to just work and study for the gre i d like to learn more about statistics with the end goal of creating various statistical models for the major sports that i enjoy watching and occasionally betting on right now random forest models and neural network models are very intriguing but i don t know where to gather the requisite knowledge to create those i ve read that i ll need to learn how to code which is a welcome challenge tldr any resources you have youtube channels blogs etc which can help me understand concepts that can be applied to creating statistical analytical models would be greatly appreciated thank you all so much dj
1,need help in making research plan i am interested in applying for master s study and i need to make a research plan for the scholarship i am applying i plan to study mathematical statistics to understand about data science and machine learning stuff theoretically much deeper what are the courses classes i need to take and what would be good or redundant to take i only know i need to take theory of statistical mathematics class but i don t know what else some info i have an applied physics undergraduate degree oceanography so i am somewhat familiar with statistics also from learning data science these couple of months i plan to study in japan so i would be a research student first before actually taking classes
1,question can i use sample sizing to estimate return rates for a product given we produce 10000 items if we tests a sample size of 370 and get a 10 failure rate we are 95 confident that this is going to be our failure rate with a 5 margin error in real life though that production will always run therefore population would increase what can we do to calculate a sample size link with formula
1,assumption of normally distribution for gaussian process gaussian process seems to be an important topic in machine learning and statistical modelling but in many topics involving gausian process e g gaussian process regression bayesian optimization for the cost function in neural networks etc when i come across tutorials for how to implement these methods using statistical computing software e g r python no one seems to test if the data being analyzed follows a multivariate normal i e gaussian distribution based on the fact that no one is doing this is it really necessary to verify that the data you are using follows a multivariate gaussian distribution when using methods that involve gaussian process thanks
1,very varying views on log transformation in the context of regression when exactly is it correct to use log transformation in regression very confused i have read through many websites medium articles kaggle notebooks and many of them have different justifications for the log transformation some of the top ones i came across are 1 if your residuals vs fitted values are not random or have heteroscedasticity then log transform the variables 2 if your variables are skewed then log transform them for regression 3 if your variables have a non linear relationship log transformation helps express them in linear terms making it easy to understand the model 4 if your variables follow a log normal distribution then transform them to get a normal distribution 5 when log transforming makes sense on the scale since the variable is now expressed in terms i am just so confused because all of them give varying reasons let s say i have 2 variables and they seem to have some sort of a linear relationship based on the scatterplot but the residual plot is crap after i log transform one of the variables the scatterplot has a clear linear relation now and the residuals are much better is this a valid reason to log transform
2,facebook’s ‘expire span’ tool enables machine learning models to forget irrelevant data paper and code included facebook has recently developed an ai tool that allows machine learning models to preserve certain information while forgetting the rest it claims that the tool expire span can predict information most relevant to a task at hand thereby allowing ai systems to process data at larger scales conventionally ai models memorize information without distinction unlike humans therefore creating the ability to decide whether to forget the information or not at the software level is challenging usually state of the art models struggle with large quantities of information like books or videos and incurring high computing costs this can lead to many other problems such as catastrophic learning or catastrophic interference a situation where ai systems fail to recall what they’ve learned from a training dataset source codes paper
0,what pros or cons have you all seen by centralizing data science and data analysis operations across your organization my company isn t too large but there is starting to be a disconnect between different groups on what data is being used to make decisions how it is being used who knows what data exists etc as a result i m working on putting a business case together for getting a centralized data team consisting of data scientists and analyst i was hoping you all may be able to provide more insight into the benefits and drawbacks of doing this based on your experience
0,using jupyter notebook vs something else noob here i have very basic skills in python using pycharm i just picked up python for data science for dummies was in the library yeah open for in person browsing and it looked interesting in this book the author uses jupyter notebook before i go and install another program and head down the path of learning it i m wondering if this is the right tool to be using my goals well i guess i d just like to expand my knowledge of python i don t use it for work or anything yet i d like to move into an fp a role and i know understanding python is sometimes advantageous i do realize that doing data science with python is probably more than would be needed in an fp a role and that s ok i think i may just like to learn how to use python more because i m just a very analytical person by nature and maybe someday i ll use it to put together analyses of coronavirus data but since i am new with learning coding languages if jupyter is good as a starting point that s ok too have to admit that the cli screenshots in the book intimidated me but i m ok learning it since i know cli is kind of a part of being a techy and it s probably about time i got more comfortable with it
0,can you guys help me understand what data science is please tell me if this violates rule 9 and ill remove it thanks so i really don t get what data science is and how its different from statistics i ve read a bunch of websites but still can t come up with like a simple explanation of what it is from my understanding you get the meaning from data while in statistics you make the data also i don t get why coding is needed for data what does an average day look like for a data scientist do you like to make the data and then say what it means or something sorry if i sound really dumb
1,how much does a bad grade in a math class affect my chances hello all i’m an undergrad stats student i have hopes of going to a top stats ms program i have research experience in data science areas and have done internships main thing i’m maintaining is gpa i’ve been doing well in my stats classes but recently i didn’t get a great grade in my multi variable calculus course i got a b anyway i’m wondering how much a bad grade in non stat related courses matter i know linear algebra and proof based math is important to do well in but how does calculus affect my chances if i don’t have good enough grades right after my undergrad can i work in industry for a few years then go back to get a masters what’s a good gpa my stats classes have been great but it was this calculus class thay gave me a tough time this one final exam ruined me
2,help with understanding the mathematics theory behind the umap paper i posted this on the math subreddit but didn t get a lot of responses so let me try here i am from a comp science machine learning background i have taken some additional math classes including real analysis probability theory the measure theoretic one some convex optimization courses and some statistics courses that would presumably help with my ml education but i was reading this paper umap uniform manifold approximation and projection for dimension reduction and i found it very hard to follow the authors recommend background texts on category theory but they seem to be huge references to an entire field and i am not even sure i have the required background for it so if anyone is familiar with the paper idea can they please recommend some introductory texts or at least enumerate the background areas that would help me understand the concepts discussed considering my background particularly the local fuzzy simplical sets spectral methods probabilistic t conorm etc
1,media mix models where retail sales numbers are not available and have only distributor sales numbers has anyone any experience with these situations where as a product owner you want to use media mix models to understand impact of various media on sales consumer demand however retail sales numbers may not be available only distributor sales or warehouse stock data is available for a time period week
2,a decentralized network for training and distributing deep learning based ai is there anything like this being developed like distributing training of deep learning systems among computers around the world and then hosting it the same way no single point of control or failure i think it could be an interesting way to have truly open development of ai systems controlled by the people not behind paywalls could possibly be tied to a crypto to incentivize donating resources to the forward and backward computations
0,data augmentation idea help i ve been wracking my brains on trying to do this efficiently i want to learn a probability distribution of a bunch of values and sample new data points from that curve given a numpy array matrix of integers floats i want to be able to learn the probability density function of those values per array in matrix and sample or generate new values or columns for the matrix as per the probability distribution of that row if possible group similar rows in the matrix or pick a subset and then do a signal to noise kind of thing to generate more data points as features or columns is there any prebuilt package in numpy sklearn or scipy that allows me to achieve this or apply it to a whole matrix at a time to scale quickly even if some part of this is achievable through a package that would be a huge help
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
1,comparing data formats for multilevel modeling and general linear modeling suppose i have the same dataset one in short format one in long format if i run a multilevel model on the long form data using subjects as a random effects term then run a general linear regression model on the short form data would i get the same results i m specifically thinking about correlation coefficients p values and r squared but i d love to know what you think would be different or the same thanks
2,how to reduce latency of dl models i have a challenging task that i have to reduce latency of my deep learning model for faster inference i would like to do it without having accuracy loss if possible can i get any advice on how to do it i already have the weight models
0,what s your best use of automl
1,assumptions of k means clustering i have often seen blog posts like this which show that k means clustering algorithm is unable to handle complicated data and can only recognize clusters within spherical clusters i have often seen this shown empirically like in these blogs but are there any mathematical justifications that explain why k means is unable to recognize clusters in more complicated data e g concentric circles crescents etc why is k means only good for specifically spheres beyond empirical demonstrations are there any reasons why k means receives a lot of criticism
1,discussion opinions on nassim nicholas taleb i m coming to realize that people in the statistics community either seem to love or hate nassim nicholas taleb in this sub i ve noticed a propensity for the latter personally i ve enjoyed some of his writing but it s perhaps me being naturally attracted to his cynicism i have a decent grip on basic statistics but i would definitely not consider myself a statistician with my somewhat limited depth in statistical understanding it s hard for me to come up with counter points to some of the arguments he puts forth so i worry sometimes that i m being grifted on the other hand i think cynicism in moderation is healthy and can promote discourse barring taleb s abrasive communication style which can be unhealthy at times my question 1 if you like nassim nicholas taleb what specific ideas of his do you find interesting or truthful 2 if you don t like nassim nicholas taleb what arguments does he make that you find to be uninformed untruthful or perhaps even disingenuous
1,kruskal wallis in statistica hey i m trying to get the statistics for my thesis i ve maneged to obtain k w multiple comparison of mean rank but i would like to show the results on a bar plot with or letters anyone knows how to do that
2,google replaces bert self attention with fourier transform 92 accuracy 7 times faster on gpus a research team from google shows that replacing transformers’ self attention sublayers with fourier transform achieves 92 percent of bert accuracy on the glue benchmark with training times seven times faster on gpus and twice as fast on tpus here is a quick read google replaces bert self attention with fourier transform 92 accuracy 7 times faster on gpus the paper fnet mixing tokens with fourier transforms is on arxiv
0,how important is it to have a good grasp on scalable databases as a data scientist ml engineer i know that knowing databases is crucial in this area but what is about more advanced topics like scalable databases keywords would be hadoop and mapreduce spark parallel and distributed databases data warehousing this is an advanced course on databases at our university so i wanted to know how important such knowledge is if some one is lets say working as an ml engineer data scientist
1,question can someone explain the finer details of central limit theorem to me so according to the central limit theorem taking n number of random samples from a population means for many iterations and then plotting the mean of those n samples in another distribution we have a distribution where the mean of the sample mean s is roughly equal to the population mean but the standard deviation has reduced by a factor of 1 root n where n is the number of samples so my question is that the standard deviation of the mean of samples is representing the variability of the mean of the population rather than the whole process as in the population distribution as in getting a 90 percent confidence range in the sample mean distribution is the measure of the mean estimation being 90 percent accurate compared to the population mean which is the absolute mean that we are trying to estimate using the sample hence the population standard deviation is a measure of the variability of the process across thepopulation distribution whereas the standard deviation for the sample mean distribution is toshow the variability of the estimated mean this is my understanding of it and need confirmationon it extending upon the above consider that i want to build a confidence range of involving say 95 of all processes roughly 2σ will i will have to do it using the population standard deviation instead of the sample standard deviation since only the population standard deviation is indicative of all the process outcomes and the correct range
0,sentiment analysis recommendations on review data i m looking for recommendations on my project currently we have a couple of hundred rows of health care review data my project manager wants me to find a sentiment analysis tool that gives a compound score that correlates accurately to the stars given for the review my first attempt i used vadersentiment and it was around 55 accurate at the score to start rating my second attempt i used texblob and that was less accurate 35 i want to know if there is any off the shelf models or other libraries i can use with python especially if it understand healthcare lingo we hope that we can find something that is about 60 70 accurate from compound score to star rating eventually we will build our own model once we have more data and time for now we just want to demo the data we have also if you think i m going about this all wrong please let me know i am relatively new to data science and this is a part time project for my job
2,dangers of parametric models after doing a lot of thinking i think i am starting to better understand some of the basic concepts behind parametric models vs non parametric models historically it was thought that parametric statistical models with too many parameters e g regression coefficients neural networks with too many weights were said to be prone to overfit training data and generalize poorly to unseen data thus lots of emphasis was placed on methods like regularization how to simplify parametric models with too many parameters this includes approaches like l1 regularization pushes some parameters heavily towards 0 l2 regularization generally pushes all parameters towards 0 and drop out randomly cancelling some of the weights within the neural network apparently these problems contributed to the popularity of non parametric models non parametric models e g kernel based models such as svm support vector machines and gaussian processes e g gaussian process regression these models do not have parameters per say for instance gaussian process regression directly estimates the response variable by repeated simulation using conditional expectation formulas if you look at the estimation formula used in gaussian process regression there are no beta coefficients unlike standard regression somehow this absence of model parameters are desirable for statistical modelling seeing as this somehow mitigates potential overfitting and poor generalization all this is supposedly implied in the famous bias variance tradeoff simple models are said to be stable but are too simple to sufficiently capture complexity within the data complex models are able to capture complexity within the data but are said to be unstable poorly generalize machine learning is apparently about trying to make these complex models more stable and generalize better here is my question what initially lead researchers to believe parametric models with too many parameters are prone to overfit is there some mathematical formula that showed some relationship between the number of regression coefficients and error or variance or some formula showing the relationship between the number of weights in a neural network and the error or variance or was this all empircally observed i am curious to see the initial justifications and math formulas that first started to warn researchers about the dangers of having models with too many parameters note i am aware that models with too many parameters aren t necessarily doomed to generalize poorly apparently models like gpt 3 famous natural language model developped by ai researchers are said to have millions of parameters neural network weights and perform incredibly well in the real world however i am more interested in the general idea and mathematical justification relating to potential poor model performance linked to overparametrized models why were overparametrized models said to be more prone to overfitting is this really why non parametric models became popular because the absence of parameters made them more flexible and less prone to overfit is this all empirical or is there math behind it thanks
1,trying to justify the performance of statistical models on a particular data set i have a small dataset 20 columns 10 000 rows i tried to train a random forest model on the data and got good results i then tried to train a neural network on the same data and didnt get good results now is there any way to understand why the neural network did not perform well on this dataset can you breakdown the performance and the reasons why the neural network did not perform well something like i forgot to study this section of the textbook so i did not perform well on the corresponding section of the test are there some reasons theoretical or empirical that suggest why neural networks might not perform well on smaller datasets in general is it possible to say the model didn t perform well because these rows in the dataset gave it trouble because the model is not equipped to handle this kind of data
2,10 popular keyword extraction techniques in nlp this blog lists out all popular unsupervised keyword extraction algorithms in nlp here i summarize almost 10 papers w r t all these techniques enjoy the read 🎉
1,education youtube statsexamples playlist i ve made a youtube channel and website with statistics videos that i m hoping will be useful for people check it out if you re interested and feel free to post some constructive advice or suggestions my target audience is ap stats and college students and the intention is to include lots of step by step examples most recent video
0,there has to be an online tool for text generation right friend of mine wants to import a list of names and generate fake names that are similar based on the input training data he s been having trouble with installing tensorflow on his m1 mac i was sure there would be an online utility to do something like that without the need to train locally but i can t seem to find a decent resource for him to use any suggestions
2,understanding loss function optimisation of large margin nearest neighbour i am currently trying to implement a paper which involves k nearest neighbours with large margin nearest neighbour for distance metric learning to improve knn s performance i have checked out the paper distance metric learning for large margin nearest neighbor classification which had introduced the lmnn idea while i am clear about the role of lmnn in knn and other aspects i am not clear about how the loss function for lmnn algorithm is optimized authors have mentioned an algorithm for optimization but i am finding it complicated to understand the project is actually an android app java kotlin so can t take advantage of the python libs also i would really like to learn the optimization algo so don t want to blindly use a lib too so question is is anyone here aware of how the optimization works for lmnn or do you have any resources where this otpimization algo is explained in simpler way would really appreciate any help
1,different coefficient signs with cross sectional regression vs pooled ols regression panel variant hello i have a data set with stock returns for a 21 day time period of 229 firms when i run a pooled ols regression on the data set i get significant positive variables where when i run a cross sectional regression on the sum of the returns cars the signs of almost all variables in the regression change what is the intuition behind this
2,doubt with updating a policy considering a sliding window state space consider a trajectory with x states using discounted rewards of i to i x states i am updating my network for the i th state now sliding the window towards the next state and calculation the loss for i 1 th state by discounted rewards of i 1 th to i x 1 th i will again update the policy this time wrt i 1 th state this will go on as this algorithm is for a live reinforcement learning model doubt when i am updating the model for i 1 th state the loss i calculated was based on the rewards of the previous network i see this as incorrect am i doing things correctly what can be done to make thing right
1,how to use kruskal wallis to check if continuous variable y has correlation with categorical variable x i used kruskal wallis on my data that has a continuous y variable and categorical x variable more than 2 i got the following result statistics 784 547 p 0 0000 with alpha 0 05 what does this mean does this mean that there is some correlation between my x and y variables
1,i have a question about predictors in a multiple regression hi in my study i have two groups desktop vs vr and my outcome of interest is x however i also measure y as a control z and w and i am interested in their effect on x based on the literature the condition desktop or vr is likely to also affect the other variables i am measuring can i still run regression analysis in the form of x a b condition c y d z e w and then test for any interactions or would i need to run a regression condition group so run it once with data from desktop only and once with data from vr only and test the effect of condition on x with a different method i am aware of testing the assumption of multicollinearity i am just worried that because the condition likely affects the values of the other predictors it is not a good idea to run one regression on the entire sample my research questions are whether the variables condition z and w have an effect on x and whether there are any relationships between any of the three and if that relationship has an effect on x i see this as interaction any help with this would be greatly appreciated also i apologise if this is a stupid question
2,what is the impact of r machinelearning on you as a researcher mle someone in the ml space as the headline says if you ask me personally i learned more about nuances in ml from reading comments discussions topics on this subreddit than i did in my formal education additionally thank you r machinelearning and its mods this is a brilliant community from which i have learned so much
2,the modern mathematics of deep learning pdf on researchgate arxiv this review paper will appear as a book chapter in the book theory of deep learning by cambridge university press abstract we describe the new field of mathematical analysis of deep learning this field emerged around a list of research questions that were not answered within the classical framework of learning theory these questions concern the outstanding generalization power of overparametrized neural networks the role of depth in deep architectures the apparent absence of the curse of dimensionality the surprisingly successful optimization performance despite the non convexity of the problem understanding what features are learned why deep architectures perform exceptionally well in physical problems and which fine aspects of an architecture affect the behavior of a learning task in which way we present an overview of modern approaches that yield partial answers to these questions for selected approaches we describe the main ideas in more detail
1,what test should i use if the sample standard deviation is known and the sample is small 20 i know that z tests are used if the sample standard deviation is known and that t tests are used if the sample is small but what if both the sample is small and the standard deviation is known which test would i use then
1,test of proportions question i ve got a strange proportions question the proportion of neck pain for males and females following a collision is 0 24 and 0 45 respectively i have a data set where 10 males and 10 females went through two collisions each and none of them ever developed neck pain obviously this data calls for a binomial test but would the correct p value be 1 0 24 10 1 0 45 10 0 000162 or should it be broken down by sex to 1 0 24 10 0 064 and 1 0 45 10 0 0025 should i factor in the two collisions by assuming that they re independent thus doubling the sample size or is that too disingenuous x200b thanks in advance
2,is ml really data preparation most of the time i thought that the main task of a ml engineer is to construct the ml algorithm model and to optimize it but i am not longer sure if that‘s actually the case i have read a comment somewhere which goes something like this „actually there are no ml engineers all of these hired people are gonna do data preparation like 95 of their time because setting up the ml model is actually trivial so in most cases you are just a data engineer…“ is he right ps i remembered a comment which somehow fits this discussion „people won‘t pay you to type model fit … “ it‘s not the first time i read comments like this…
2,has anyone experienced ml recommending actions based on data so this is going to be a controversial question so i appreciate you giving me the benefit of the doubt in your answers i have been working with machine learning solutions for a while now and i am becoming increasingly agnostic about the fact that machine learning can provide large companies with actionable insights based on data i understand that it can identify patterns and all that but has anyone really and i m saying you ve experienced using a system that can actually tell you what to do and you are confident enough about its recommendation that you d apply it for a billion dollar company for example look at this article this looks more like a puff piece than reality what are the not telling us here
0,best platform for a school data dashboard i m moving into a new role at my school next year data strategist yay and i will be responsible for managing the school s data at the moment data is not accessible and we have struggled as a result to my question what do you think the best tool is for a dashboard that is easy to navigate filter for many people that are not tech savvy and that will be easy for me to update on a weekly basis my first thought was google data studio since it s pretty straight forward free and easily works with sheets however i m also somewhat proficient with python and will be utilizing seaborn probably for some of the visualizations further information this is for a high school that has about 600 students and i need dashboards for the entire school each grade level and each content area thanks
2,iccv 2021 challenges contests there are 84 workshops this year in iccv here s a list of all 40 challenges contests from among them please feel free to add to the list in case i missed something challenges all workshops official link markdown link
0,is it legal to create images with an open source nn implementation and sell them i want to use the recently published vqgan clip implementation of transformers to generate images based on a text description and then sell those images in any way i can maybe through a website i would use very specific seeds and text so these images would be almost impossible for anybody to replicate with the same network is this legal as i understand vqgan and clip are both open source thank you
0,authorization in streamlit i am working on a college project using streamlit for making a web app is it possible to get to this web app through some user authentication moreover if possible to do so via a mobile application to take users credentials and verify it and then direct to this web app thank you in advance
1,which statistical test to use for a comparative study that uses a likert scale i am a student who is currently doing an investigatory project wherein i made bio based utensils to compare to the conventional plastic cutlery in terms of comfortability and other variables which is why i utilized survey questionnaires with that said what statistical test is best for the data i have gathered thank you any help or comment is appreciated
2,data validation and model testing strategy on training vs serving data problem at hand once you identify the model degration problem in the serving environement 1 how can i evaluate new incoming data of serving environment to understand the following 1 drift and skew in the training vs serving data 2 test models and decide the next steps i believe the strategy would highly differ based on the domain volume of data or the complexity of model but are there any generic frameworks available that tries to addresses these issues potential solutions framework i could find 1 drifter ml 2 tensorflow data validation
2,gcp vertex announcement anybody know what this is or use gcp’s ai services before
1,why is height on an interval scale but length is on the ratio scale isn’t length and height the same thing except height is vertical and length can be in any direction this is genuinely bothering me my book classifies an interval scale as having a consistent difference between units which separates it from the ratio scale but doesn’t time weight and length have a consistent difference between units it’s not like how we measure those vary sorry if this is a dumb question but it’s been bothering me lol
2,paddlepaddle vs pytorch does anyone have experience learning paddlepaddle after learning pytorch here if so what did you think of it is it an easy language to code in for someone who is used to building dnn s from layers does it have any functionality that you appreciate that pytorch doesn t or vice versa any functionality that you want that pytorch provides but paddlepaddle doesn t i ve been thinking of learning another dl framework and trying to decide whether to look into jax or paddlepaddle thanks
2,how to predict nlp transformer model sizes when i read about bert base and bert large i read that bert base will fit into 10 12gb gpus but bert large won t i tried multiplying the number of parameters 110 million and 330 million respectively with 32 bits presuming all parameters are fp32 but according to that the models take 0 4 gb and 1 2 gb respectively what am i doing wrong here i assume there must be a way to actually to actually calculate this
1,how do generic probabilities work if a certain genetic condition is diagnosed in 1 100 people and i have a risk factor that studies have shown increases my risk by 2x does that mean i have a 1 50 chance of being diagnosed with the condition
1,high school statistics question can anyone help me find the anwser suppose a 95 confidence interval constructed from a sample for the mean weight of apples on a farm is 90 120 1 what is the average weight of the sample 2 what is your conclusion to the test that the mean weight of the apple is 103 at the significance level of 0 05 at the significance level of 0 01 still at the significance level of 0 05 but decrease the sample size the weight of the oranges on a farm is subject to a normal distribution with a mean of 40g and a standard deviation of 5g 1 what is the interval that contains the middle 60 of the orange 2 what is the probability the average weight of a sample of size 100 is less than 30g
0,question answering ai hey people i m working on my personal project which will be quite a challenge one of its features is that the user can interact with an open domain question answering chatbot which will be trained on the data i provide it i want the model to resemble a specific person group and it will be fed everything that that person group wrote said and etc have in mind that the model can answer questions in 1 4 sentences and it doesn t need to be based on pure facts this means that the user won t ask the model questions like what is the capital of france but more something along the lines of existential questions what is the meaning of thing here are the questions i have as i didn t dabble into the nlp world of ai at all 1 are there any pre trained or prebuilt models out there that i could use for this i ve found that the open source pavlov ai library has some interesting ones 2 which models would suit this task the best 3 are there any features i should watch out for or provide more information on the biggest part of the job will be to collect relevant data on the group i want the model to resemble what would be some of the best practices when making the data as informative as it can be also if i want there to be 4 groups that the model can resemble do i need to train 4 models or can i filter what a model learned into 4 categories thanks for all replies and questions in advance if some of you are interested more in the project feel free to send a dm and we could even collaborate on this part of the project to make the model great
0,relationship between no nosql hadoop and data lakes hi i am going crazy trying to figure out the relationship between these big data technologies i understand what they do but i cannot find anything that tells me the relevance of a particular one to a field or task are they technologies used in combination is one better for a particular type of data are there limitations that means the choice is budget dependent or is it just matter of preference many thanks to anyone who can point me in the right direction
0,i m burnt out with learning can t find work how do you guys keep pushing forward
1,the earliest mention of the target illustration hello my friends a couple of days ago my terrific girlfriend and i had a bet on the earliest mention of the famous illustration with targets about similar things pic i bet that i would find the earliest mention in psychology or sociology with validity and reliability terms used she bet on statistics and econometrics with bias and variance terms used currently i am winning with the image link to which i posted above this is babbie e the practice of social research 1995 pp 128 but one article cited this image with an edition from 1985 i am not really convinced that this image which uses targets for an illustration originated only in 1985 moreover the book of babbie cites an anonymous reviewer as a source so asking you for any help on discovering the nature of this illustration thanks in advance
0,finally graduated from computer engineering i ve been lurking around this subreddit since i started my final year project a facial recognition project i fell in love with data science overall and the stuff i was discovering every day i lost interest in software engineering and embedded engineering a year ago and i ve been looking for the field that i would be happy to go into and ds was the one i ve started applying for graduate jobs to become a data scientist and i m starting on a side project soon to boost my profile a bit more i wondered how hard it would be when looking for jobs in ds primarily when you haven t studied the course directly
2,what is breakthrough ai for self driving cars i m writing a story about waabi a self driving car company that has just come out of stealth and raised 80m in funding i m trying to make sense of the technology they re using since it s proprietary they didn t reveal much the company claims to have a breakthrough approach to ai which the press release describes as such emphasis mine the company’s breakthrough ai first approach developed by a team of world leading technologists leverages deep learning probabilistic inference and complex optimization to create software that is end to end trainable interpretable and capable of very complex reasoning and here s what the company s ceo told the verge urtasun and her team are also developing a new algorithm that will serve as the foundation for the “brain” of the self driving car which helps with motion planning and predicting what other vehicles on the road will do so the av can react accordingly and this is what she told techcrunch urtasun says she solved these lingering problems around deep nets by combining them with probabilistic inference and complex optimization which she describes as a family of algorithms when combined the developer can trace back the decision process of the ai system and incorporate prior knowledge so they don’t have to teach the ai system everything from scratch now i know that a lot of the stuff put into press releases and told to media by company execs is vague statements meant to do publicity and there really isn t much more about the ai technology they re using elsewhere but the company has raised an insane amount of money while the sdc industry is seeing a downturn and their backers include geoffrey hinton fei fei li and pieter abbeel does anyone here know what the combination of deep learning probabilistic inference and complex optimization can do that previous ml systems used in sdcs can t
1,question variable measurement i’m doing a very basic module in spss for a politics course and i’m finding the difference between nominal ordinal and interval variables quite arbitrary and difficult to understand some help and clarity would be greatly appreciated if a question to a respondent was worded do you agree or disagree with the statement ‘learning to drive is a lot of effort ’ 1 disagree 2 neither disagree not agree 3 agree is this an ordinal or nominal variable obviously there seems to be some rank order to it but doesn’t seem all that different from a “yes” “no” “don’t know” kind of question which in my very very basic understanding would be a nominal variable also if for instance respondents age was grouped into 3 categories 1 young 2 middle 3 old would this be a nominal or ordinal again there’s obviously a rank order to it but i read that age should be considered a nominal variable or if some was asked “how many days do you discuss sports” 0 no day 1 one day 2 two days 3 three days 4 four days 5 five days in my very limited understanding this would be an interval variable but again i could convince myself that it might be ordinal
2,androidenv a reinforcement learning platform for android this is similar to what openai universe aspired to be
0,working as a data analyst at a big4 europe in hope that this post won t get removed i will take you through the steps of getting an internship at a big4 in belgium yes mentioning the country is important because some things aren t the same in all the countries the position was in data analytics the full blog if anyone is interested the interview i had three interviews to pass technical with one of the nicest recruiters data analyst scientist hr also a very nice person manager very serious guy he scared the shit out of me but once i started working with them he was super nice technical 1 sql what would you write to do x y z small question to test if i know how to use sql or not at all 2 business question to test my business acumen 3 statistics questions regarding outliers and robust preprocessing median and mean in skewed data yes overall the interview was not that hard because i am a civil engineer master of ai i truly think that having a good educational gives a nice push the interviewer will think if he made it that far he won t be a dumb fuck unless i cheated all the way yes i know good grades does not mean intelligence but good grades good education means hard work and that s what most company want hard workers in my opinion hr 1 testing my french skills i am supposed to be bilingual but once you start talking english everyday you start losing the french vocab le science de data i said it like 30 times ffs mainly because in belgium the languages are french english dutch and german 2 general storytelling my cv why i came to europe why i chose my masters what i did before coming 3 motivation on why i found the position interesting btw deals analytics is one of the most fun position if you check the youtube videos 4 me saying some jokes to gain points p manager 1 same as hr but with a lot more pressure i got scared to death no joke i bet many people know what i am talking about first week as an intern right away i had to learn the software they use which is alteryx since i have already done my pandas and sql on datacamp i did not require lots of time to get used to it very fun no code software for data cleaning and prepping first project i directly started working on an red financial data the goal was to create insight from the financial data to direct the next investments in the right path most lucrative second project geospatial analysis oh boiii oh boiiiii this one was amazing i had no idea how good is creating a map and understanding the effect of the surroundings on the business this one was huge learned how to webscrape how to deal with jsons and much more third project computer vision project what the actual fuck are we still in the same internship haha matter of fact i had learned a course on cv and i was like guys i studied this a few months ago i can solve it with cv everyone liked it so much even though i copy pasted the code from my old projects evil laugh fourth project geospatial analysis again but this was one very tough and stressing fifth project at that point i had to work on my thesis because i had an intermediate presentation however the project was about traffic analysis traffic as in cars a dream coming true because i am a civil engineer specialized in traffic engineering transportation we did amazing stuff but they were very limited the end one thing to add for however gets an internship at a big4 i hope you get a supervisor similar to the one i had i loved him always supportive and encouraging he cared about me on the personal level to the readers i hope you benefit from this either motivates you to apply or to learn something new hope you enjoyed it♥
0,how to be taken seriously during a job interview when you don t have a stem degree nb this is not a rant post i swear i want to be proactive i m writing here to ask some advice on how to tackle my next interview processes i have a problem about this x200b some context quickly i am already a professional data scientist with almost 3 years of experience in a large company i have a phd from a social science department my main field of study has been application of statistical models i spent four years studying mostly statistics and econometrics and doing estimations my final thesis was completely statistical in nature before that i received good basics in cs i don t want to sound arrogant but i think i m good at my job i have a good understanding of math calculus statistics and algorithms my colleagues with a background in stem told me i m good at deep learning i am the reference guy in my company for the use of tensorflow x200b here s the problem i like my current job but i don t have faith in the future of my company i have seen countless potentially cool projects being supervised by corporate idiots that do nothing but speaking corporate jargon that know nothing outside marketing i m sick of this and i want to leave however every time i apply for a new job i feel that i m not taken seriously because of my social science academic background i can see how recruiters changed attitude when they found i come from a social science department they believe i got there by mistake this is so frustrating what can i do about this how should i approach recruiters and companies when i apply for a new job x200b thank you people love this sub edit to make myself more clear and give you an idea of why i wrote this post i have just received an email literally 1 minute ago by a company i applied for they had cool dl projects young data savvy team both interviews went great we all liked each other now they just told me listen we liked you very much but our company s policy is that no people with a social science background can be hired for this role they literally told me that i hope you will now better understand the reason for this post instead of calling my lack of humility again it s not a rant partially now but rather tell me what to do to attenuate bypass this problem
0,best etl processes tools hi data people hopefully this sub is the appropriate place to post this i posted to r dataanalysis and r dataengineering but since this sub has about 10x the members of either of those it would get a lot more attention and feedback i thought our company has been acquiring new companies at an increased rate lately and where possible and realistic we re looking to streamline and combine our data streams and etl process so that our team can be the central data go to team for reporting as a part of this process we re looking at potential new etl processes we could adopt and i d love some suggestions from other people in the field currently we use visual studio to manage dtsx packages where we do the majority of the etl work these packages kick off various stored procedures in our sql servers to load combine aggregate data wherever needed what systems or tools do you all use in your work environments and do you have any suggestions on good processes i could look into thanks in advance
0,can i set a language preference i’m very much a beginner and learning r and python but goodness do i prefer r in future roles will i be able to say that i prefer to use r over python or is there not that kind of flexibility
2,memory efficient transformer what s a good memory efficient transformer for causal sequence generation preferably with pytorch implementation the faster the better in my particular task any model smaller than 140m parameters cannot generate sensible output i m using an 11gb gtx 1080 ti and the longest input i can with has a length of 512 with a batch size of 6 i want to increase this length to 2048 can local attention or sparse attention help with that huggingface has these candidates bigbird gpt neo and reformer does anyone have experience with training these comparing to standard pytorch models i have found out that huggingface models use 1 1 5 gb more memory which is a deal breaker for me since my data is midi i have to train from scratch there is another reformer implementation on github but the model is so slow i think what i m looking for is a simple transformer implementation with local attention and i can t find any thanks in advance
1,sampling methods in stats in 10mins hi everyone i made a video which explains all the different types of sampling methods used in statistics in 10 mins please check it out and share it with anyone who might find it useful here is the link
0,is cross validation hyperparameter tuning sometimes not better than just setting reasonable values i have noticed this in practice like in kaggle comps sometimes i have done no hyperparameter tuning by just setting whatever seems reasonable ish in a ballpark or using defaults and it ends up performing better than doing computationally intensive and tuning every little hyperparameter in the real world i also wonder whether cross validation for hyperparams can result in being more sensitive to things like data and concept drift because well if the future data doesn’t look like your validation set then the cv would have resulted in you overfitting the hyperparameters themselves
2,openchat video opensource chatting framework for generative models x200b openchat is python opensource chatting framework for generative neural models currently openchat supports blenderbot and dialogpt and we plan to extend models if you use openchat you can talk with ai with only one line of code openchat model blenderbot size large you can build your own environment not just terminal or web demo application you can install easily pip install openchat and detailed information can be found here
0,suggestions for bi visualization tool that supports live feed connection from bq i was looking into tableau online which seems fantastic but is a little expensive for my needs ive been using google data studio but its fastest data freshness is 15min which isnt good enough anyone have another suggestions x200b thanks
2,find trending machine learning research papers on twitter we developed a website to find popular trending research papers on twitter link features that i like to highlight here analyses the twitter feed and shows popular trending research papers daily weekly and monthly basis shows tweets retweets and likes count for each paper so that the user can filter out random papers shows popular tweets that related to each research paper we love to hear your feedback and suggestions thank you all and i appreciate the support
1,question is this kind of monumental forecasting error common in real world sbi research huge credible corporation forecasted a second wave in india with 97 5 ci 1 8 million cases 5 million cases in 100 days starting from 15 feb 2021 i thought since it s one of biggest corporation of the country and the world the numbers would be somewhat accurate but in last 30 days alone more than 6 6 million new cases have been registered my question is is this kind of forecasting error really common if yes then what s the point of doing forecasting as a student of statistics i thought that forecasting done in big organizations must be somewhat accurate please note that i ask this question just to understand the practical importance of statistics in real world and not to defame any organisation link to sbi research report link
1,when comparing three different groups would using different sample sizes for the groups be accurate if you check the number of students used to measure iq kazakh has a way larger sample size while uzbek and russian are way smaller
1,question how do i calculate percentiles given this limited information hey there do you know of a formula to convert the absolute test scores into percentiles if only the following information is given test scores median 50th percentile 10th percentile 90th percentile 2 06 3 22 2 41 3 97 4 25 3 50 2 72 4 22 4 00 3 63 2 50 4 50 the minimum value is 1 and maximum is 5 i now would like to get the absolute test scores column 1 as percentiles how do i calculate that many thanks already in advance
0,weekly entering transitioning thread 04 apr 2021 11 apr 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
1,distinguishing between principal components and factor analysis the below article helps to distinguish and clarify differences between pca and efa and when it is appropriate to use one vs the other moreover it describes when you may come to the wrong conclusion or may be inappropriately using one vs the other x200b
0,weekly entering transitioning thread 21 mar 2021 28 mar 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
0,data science blogs title pretty mcuh sums it all the last post on this sub regarding this topic was 2 years ago so i m posting hopeful that the list is more updated currently reading towardsdatascience any other suggestions edit thank you for all the responses very helpful
1,how to choose k for hierarchical clustering i m beginning working on my m s thesis for a lab currently i am conducting a preliminary hclust in order to replicate what this lab has already done in regards to their data from what i learned in my coursework to determine where to cut the dendrogram requires estimating k and there are many ways to do so in particular i chose to use summary indexes ch hartigan kl and silhouette as well as gap statistics however these best estimates for k vary by a bit ch and hartigan estimated k 20 while kl and silhouette estimated k 2 the gap statistics method estimated k 5 any recommendations on how to choose k the lab performed an hclust using distance matrices generated from wgs results of antimicrobial resistant bacterial strains i have already sent emails asking them about how they came upon the number of clusters that they did but their explanation didn t make much sense to me as they explained they clustered any isolates with at most 15 snps of dissimilarity knowing that i want to select k 20 but i realize that decision comes from bias and isn t methodical or objective in anyway in fact i wonder whether or not they misunderstood what they were doing and selected k 15 thinking that means hclust would cluster together isolates with a dissimilarity of 15
2,why transformers are taking over the compute vision world self supervised vision transformers with dino explained in 7 minutes check out the new post from casual gan papers that explains the main ideas from self supervised vision transformers with dino 1 minute summary in this paper from facebook ai research the authors propose a novel pipeline to train a vit model in a self supervised setup perhaps the most interesting consequence of this setup is that the learned features are good enough to achieve 80 1 top 1 score on imagenet at the core of their pipeline is a pair of networks that learn to predict the outputs of one another the trick is that while the student network is trained via gradient descent over the cross entropy loss functions the teacher network is updated with an exponentially moving average of the student network weights several tricks such as centering and sharpening are employed to combat mode collapse as a fortunate side effect the learned self attention maps of the final layer automatically learns class specific features leading to unsupervised object segmentations full explanation post arxiv project page self supervised video segmentation more recent popular paper explanations mlp mixer vision transformer vit
2,question concerning taxonomy about training static ml models in production hi all i m trying to clear up some misconceptions of machine learning ai in my organisation my main point is to make clear that a ml model is an artifact of the training algorithm that can be put into production and then generally doesnt change anymore until it is exchanged by a new model also i want to know when this is not the case i e the model changes while deployed in production 1 so my first question would be if this assumption models usually don t learn change in production is correct 2 is this called offline learning and would online learning be the rare case when a ml model is trained in production how else would you label these difference 3 concerning reinforcement learning how do the terms off on policy reinforcement learning refer to this distinction between learning gradually in production or updating the model in bigger steps my assumption here was that while a rl model being in production it usually gets updated in bigger batches with new training data making it offline learning and necessarily off policy when it is trained online in production it could be either on policy or off policy learning the more i read the more i think i m incorrect here but it would be great to know the relation here thanks in advance everyone taking the time to clear up my confusion i appreciate it very much
1,significance test for a comparison of a group that is part of another haven t explained well in the title but i m basically comparing sales in a single office vs the regional sales vs national sales to see if it s performing statistically better worse and not sure what test to use the office is part of the region which is part of the national data set i only have access to the grouped data that looks like so group customers sales sales percentage office 15250 650 4 1 region 84000 5600 6 3 national 950000 76000 7 4 thanks
0,most onerous data formats hello i am looking for some suggestions or ideas surrounding the most onerous and irksome data formats you’ve had to work with here’s the skinny a very large and in my humble opinion just straight up malevolent company x is conducting retaliatory data requests at small public offices and institutions in a us state one of my friends happens to work there now i am all for free and open public data 100 but this is a nefarious attempt to drain and waste public resources and to intimidate they are making entirely useless requests i am a senior data analyst with a few skills here and there and would like to help my friend stick it to evil corp while still operating well within the law and legal requirements if you were tasked with handling hundreds of thousands of emails meeting notes minutes powerpoints etc what would be the most onerous and cumbersome file formats or organization like i was thinking of converting all text to binary and writing a little python program to generate folder hierarchies and compress every single email etc somehow scramble things up a bit while still ensuring the data has integrity etc any suggestions
0,ways to make money using tech data skills outside of a regular job outside of a job working for a company or freelancing online as an employee or contractor what other options are possible for a software developer to make money using their programming computer science skills data science background any of you using your technology data skills to make money outside of your regular job
1,which regression model is best for analyzing ordinal dependent and binary explanatory variables hey everyone so i m trying to run an analysis on some data i m working on i m working with 2 variables and want to compare them from observations made in the years of 2000 and 2010 dependent health status 1 good 2 average 3 bad explanatory partner status 0 nopartner 1 partner i ve done some research and come across different regression models i would like to know which one would be the most suitable for what i m after any help would be greatly appreciated thank you • cross sectional ols • static linear panel data model estimated by re and or fe • linear panel data model with dynamics • cross sectional binary logit probit • panel data binary logit • cross sectional ordered or multinomial logit
1,why do we use logit for logistic regression i ve used logistic regression as one tool from many other classifiers without any really deep understanding of what and why now i have to teach my team about it thus i finally want to understand the math it and so i m struggling with the following question so we use logit p on the left side of the equation because the linear polynom on the right side has the domain of inf inf while p is defined on 0 1 i ve tried alternatively to use the expression 1 if y pred 0 5 else 0 with the ordinary linear regression for classification and it produced the same accuracy on my toy dataset as a proper logistic regression so are there any mathematical reasons to use logit or it is just a customary thing possibly leading to more efficient computations especially if gradients are involved etc specifically does logit provide a guarantee that we re getting a probabilty of the class and not just some number between 0 and 1 if my questions doesn t make sense here again what i did i ve prepared a dataset consisting of the numpy matrix x with the shape 5000 3 and the labels y in shape 5000 1 the labels contained only 0s and 1s i ve fitted then scikits linearregression class directly to those x and y of course when i use the model to predict on a separate test dataset it can return values outside of the 0 1 domain i m bringing them back into 0s or 1s using the code line above and then i m calculating the accuracy score next i use exact the same dataset to fit a logisticregression class and i m getting essentially the same accuracy score on the same test set so on the first sight it looks like i can use just any way to convert the domains if we re not interested in probabilities of the class
0,what is the correct terminology for this type of survey i have a survey where users rate a number of recommendations the user does not know that half the recommendations are made by system a and the other half are made by system b i initially thought this was called a b testing however that does not seem to apply what is the correct name for this
2,equivalence between traditional state space models and recurrent neural networks can anyone recommend a source that explains the relationship between recurrent neural networks and state space models i often see recurrent neural networks being explained just in terms of neural network vocabulary can anyone recommend a website book video in which the hidden state aspect of recurrent neural networks are discussed thanks
1,relationship between data stat final project hey everyone for my final project in my statistics class i have to determine the relationship between online learning and gpa i understand that i will be asking other college students what their numerical gpa is since the start of this academic year because last year online learning started late in the year my question should i also inquire about their gpa prior to online learning so i have something to compare it to
1,have any of you managed to find a job with a four day work week hi i m a current undergrad majoring and math with a minor in statistics i was wondering what work life balance is like for most of you in the industry how flexible are your bosses with your schedule have any of you managed to find job with a four day work week how much vacation time have you been able to negotiate etc
1,question can you measure interaction in a chi squared test of independence i have statistical data where i want to look at differences of frequencies with a chi squared test of independence my main hypothesis includes 2 independent variables gender marital status and i want to add another variable country of origin and look at its effect if this were a continuous variable i would use anova and check for a statistically significant interaction but since i am looking at frequencies i use chi squared is there a way to measure interaction in chi squared tests or should i use another test thanks
1,primer for biological statistics in rstudio anova regressions correlations bar and line graphs ecologists environmental scientists foresters horticulturalists and biologists have two specific requirements for performing statistical analyses hereforth is a primer a simple and hopefully clear distillation of the important components amidst the vastness of the field this was presented to graduate students at the university of toronto first a general introduction to probability assumptions of inference and hypothesis testing the second requirement is to perform the analyses including managing the data using some statistical software at the vanguard of such software is the r statistical software and its integrative environment rstudio few programs constitute the virtues of open sources highly integrated and powerful programs as the r statistical software taught to the modern student in most institutions the following video comprises the time stamped functions and analyses 1 intro to data management probability and data presentation 0 00 min 2 intro to producing graphs with r studio 13 15 17 00 min 3 subsetting the data 23 20 4 attach function in r 23 47 min 5 summaryse 24 45 min 6 using the plot function 28 00 34 00 min 7 package sciplot for bar graphs bargraph ci function 34 00 36 00 min 8 analysis of variance including tukey hsd anova 36 00 40 00 min 9 creating line plots and scatter plot 42 00 min 10 regression analyses via lm function linear 45 00 48 00 mins 11 correlation analysis 48 00 51 00 mins the video finishes with a summation of what we ve learned if you like this material then please like and subscribe to this channel
2,what s missing in the ml bias debate hey guys i ve noticed that debates about bias in ml data rarely every consider whether ml should be used for a given decision in the first place i think it is important to ask a number of questions to determine if ml should not could be used to solve a given problem e g 1 can humans handle the decision making equally well at reasonable cost levels 2 does the decision have a significant impact on a human life 3 does the benefit of this ai use case outweigh the risks what do you think of these questions are they off track should more questions be asked i ve written more about the reasoning behind the questions and give a few examples of use cases where ml imo should not have been used in this article
1,appropriate data types for pca hello this is related to a previous question link but is covering different points so i am asking it separately also i ll be posting in both r bioinformatics and r statistics to get as wide an audience as possible hope that s ok and hope the below makes sense as i understand pca is classically applied to continuous data i am looking to apply it in two separate analyses one is looking at binary data representing the presence absence of various genomic structures within separate genomic stretches pca would ideally be applied to compare the different genomic stretches according to the presence absence of the genomic structures e g stretch 1 vs stretch 2 vs stretch 3 the other is looking at discrete numerical data representing the aggregate counts of the genomic structures mentioned above within separate groups of genomic stretches pca would ideally be used to compare groups of genomic stretches e g group of stretches 1 2 and 3 vs group of stretches 4 5 and 6 vs group of stretches 7 8 and 9 i ve found a paper that discusses pca applied to binary data principal component analysis of binary genomics data pubmed nih gov can anyone tell me if it is appropriate to apply classical pca to discrete numerical data integer frequencies of structures rather than continuous numerical data as i understand pca is commonly applied to discrete numerical data for differential gene expression in rna seq you are looking at integer transcript counts for a specific gene statquest principal component analysis pca step by step youtube which is analogous to looking at the integer aggregate counts for separate structures in a groups of stretches i think so i m hoping this would be fine thanks a baby bioinformatician
2,can the ai ml community learn more from naturalists there are certain behaviours in ai ml models that mimic natural phenomena zebras confuse their predators with the striped pattern on their body this is similar to adversarial attacks on ml ai models could we possibly learn more from naturalists as the ai ml community what other phenomena are there
0,how much to share in a public presentation hi datascience my team at work has been asked to share some of our work at an upcoming event from a best practices perspective how much of our methodology should we share my concern is that much of our data is publicly available and the rest can be purchased easily our models are largely off the shelf or require minimal tuning our innovations are really elegant and not hard to reproduce should i be at all worried that a larger more experienced ds team will piggyback off of our efforts and compete for our lucrative contracts what parts of data model configuration should be kept obscure
1,career what s the point of actuarian exams so basically my question is why does this exam system exist instead of university courses if you want to work as an actuarian and i guess it does not replace a uni degree so you need both why or do you even need them to work as one it s just a bit of a weird concept imo
2,timm vis visualizer for pytorch image models there are so many cool visualization techniques for cnns out there but the code implementations of these techniques in a lot of repositories libraries seems to be limited to very few models such as vgg or alexnet i m sure all of us would love to visualize and understand our own models so i created timm vis a library using which you can visualize your image classification models with just a few function calls so far i ve implemented filter and activation visualizations maximally activated patches saliency maps synthetic image generation adversarial attacks feature inversion and deep dream if you re interested in the project and want to try these methods out on your own models i encourage you to go through details ipynb in the repository i d love to hear your thoughts feedback and suggestions
1,how accurate were the statistical models you developed on real world data when it comes to real world data how accurate were the statistical models you developed were these models able to consistently and accurately make predictions e g for supervised binary classification has anyone been able to develop a model that had high accuracy high sensitivity and high specificity
1,combining multiple known correlations into one master equation hi i m doing a research project and have an idea but i m not sure how to set it up it involves determining how a person might react emotionally to situations it involves several independent variables a type of personality a mood and the quality of an external reaction each of these are defined by various numeric levels for example personality could be defined by extroversion agreeableness conscientiousness neuroticism openness each on a scale from 0 to 1 mood could be defined by two variables valance and arousal now say that i knew or declared one such correlation that made sense for example someone with low valance and high arousal would respond angrily my idea is two write these facts as linear equations with lots of variables then solving the equations simultaneously would result in one equation that represents all of the facts so the equation for the above would look something like mood valance mood arousal reaction angry however i am not really sure exactly how to write the equations does anyone have a better idea on how the equations could be written thank you edit should note this will be implemented using computer programming edit2 i think multiple linear regression might be relevant what this is still looking for feedback on how exactly to set it up though
2,how would you approach creating a paraphrase dataset i have been trying to create a paraphrasing dataset for a non english language arabic which has no datasets for this task however i am not sure on how to approach it my hacky way was to take a translation dataset arabic english set the arabic sentences as the target and translate english sentences using pretrained models or goolgle translation to arabic and set them as the source my impression was that this might workout since in translation phrasing might changed but the meaning is preserved however when fine tuning gpt on that dataset the model does not perform well as it changes the meaning of text often and add extra info that didn t exist in the first place my theory is that this is coming for the dataset maybe many mistranslated sentences what i am thinking of trying now is overfitting a pretrained translation model on my dataset so i be sure the translations are not wrong and generate a new dataset x200b what are you thoughts on my approach and how would you approach it yourself thank you
1,how to represent many different types of results for one sample and or in one figure opposite of pca sorry for the vague title i don t know how to word this i m looking for a way to simply visualize a lot of data i m picturing the solution to this problem as like the opposite of pca though i don t know if that s the best way to describe it essentially whereas in pca you condense many variables into like two i need to condense many categories or types of results into just a few so i can cleanly present them in only 1 2 figures data related to the kind i want to present has been presented as a complicated stacked bar graph but with all the data we want to show it would be way too cramped and ugly to use as a published figure i ll try to describe the data i m working with as simply as possible but it may still seem jumbled sorry in advance for a group of many individuals some reacted to a stimulus and others did not we want to compare the following for the two groups so i guess each group could be split into its own figure for each individual their data consists of info on 4 types of molecule found in their sample g1 g2 g3 g4 all 4 are relevant in our dataset each type of molecule can be modified with modifications a1 a2 a3 etc up to a8 all 8 are relevant in our dataset the abundance of each modification type is given for each of the 4 molecules for example for sample x 90 of modification a1 is on molecule g1 0 of a1 is on g2 and g3 and 10 of a1 is on g4 then 50 of a2 is on g1 5 of a2 is on g2 30 of a2 is on g3 and 15 of a2 in on g4 repeat for the rest of the modifications a3 a8 on each molecule g1 g4 repeat all of that for each individual sample so this is where my idea of the opposite of pca comes in though i don t know if such a thing exists is there some kind of graph that could condense many types of results for a single sample into one dot on a scatter plot it sounds pretty dumb to type it out i really doubt there is such a thing lmao right now for each individual sample we have grouped bar charts grouped by modification and each bar is the moledule g1 g4 looks nice but we need that condensed into one figure for many samples and like i said a cramped stacked bar chart is our last resort but maybe there is a kind of graph out there i have yet to learn about that could help visualize this data can anyone help me thanks
2,a theoretical reviewing video on adamp slowing down the slowdown for momentum optimizers iclr 2021 this short ⏱ video is a paper review on adamp slowing down the slowdown for momentum optimizers presented at iclr 2021 by heo and chun et al naver ai lab paper code project page the paper observed the growth of weight norm using the momentum based optimizers can be problematic and proposed an excellent remedy the optimizing step projection onto the gradient plane in this review focusing on the theoretical analysis this video offers more explanations on scale invariance a proof of orthogonality of weight and its gradient geometric interpretation on lemma 2 1 a proof using deductive reasoning for lemma 2 2 while the paper uses mathematical induction corrections to the proof of corollary 2 3 due to the typos probably they ll fix that in the next revision a more explanation on the step projection enjoy the paper reading 🙂
0,surprise 45 minute technical assessment in late stage interview with several of company s team members i m in the process of interviewing for a data scientist role i had taken a two hour python sql technical evaluation and passed it one week later i m in a late stage 90 minute interview with several team members with 45 minutes left they suddenly had me do a screen share with everyone and bombarded me with sql python questions this effectively left no time for me to ask questions i had for the team it was stressful and in no way reflects a typical coding environment that i ve been in i didn t botch the surprise technical assessment but didn t ace it either certainly wasn t an environment to do my best work i ll be honest it was a huge turn off i know data science is technical oriented but i felt that i had little opportunity to absorb ask about team and company culture bit of a rant but also curious if anyone else has experienced this and what your experience was like
2,am i missing any mlp vision architectures here i ve been trying to catch up on some papers and i m pretty interested in the whole mlps for vision thing that boomed last month so far i was able to gather x200b model paper mlp mixer mlp mixer an all mlp architecture for vision resmlp resmlp feedforward networks for image classification with data efficient training no name for this one i think do you even need attention a stack of feed forward layers does surprisingly well on imagenet repmlp repmlp re parameterizing convolutions into fully connected layers for image recognition gmlp pay attention to mlps eamlp beyond self attention external attention using two linear layers for visual tasks x200b just want to ask if there are any more that i missed that have also gained traction admittedly though i don t know how many of the ones i listed here have made a splash aside from mlp mixer thanks
1,anova results reported df2 is decimal value my google foo is not helping me here the paper i’m reading has anova results reported throughout that are like f 2 174 12 9 p 0 001 statcheck io says each is a decision inconsistency and recalculates p 0 64694 i thought df2 num subjects df1 some of the df2s have leading zeros i e 086 so i don’t think they are typos thanks for any pointers
1,robust two way dependent within anova i’ve been sifting through wrs2 and don’t see one something like pbad2way but for dependent groups with unequal sample sizes any other packages that might be helpful bootstrap would work an anova with trimmed means and winsorized variance would work too thanks
0,weekly entering transitioning thread 16 may 2021 23 may 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
2,suitable machine learning techniques algorithm for anti money laundering can someone suggest recent algos designed for aml fields or material to start learning more i have sample dummy data with different banking attributes how can i get started
0,is going from a data science job to people analytics corporate human resources in the same company a demotion
1,does it make sense to calculate d prime for a split plot design lets say participants study 100 words the words can either be happy or sad within subjects variable the participants either study them while in a hot or a cold room between subjects variable they are they tested on 200 words in a different room of neutral temperature these 200 words consist of the original 100 and 100 more these 100 extra words are half happy half sad am i right that it wouldn t make sense to calculate d for both word emotion and encoding temperature because it doesn t make sense to calculate false alarms separately for encoding temperature
1,statistically coherent method to detect time to steady state hi everybody have a question on a piece of research i am conducting i have currently a number of minute by minute time series of hardware temperature and my goal is to find a statistical method that would allow me to determine how long on average the equipment takes to achieve its steady state temperature the temperatures would start at 18 degrees then increase at a faster rate to reach 70 degrees steady state then it could either fluctuate around 70 or it could drift slowly upwards until about 100 degrees the issue is that different equipments have different steady state temperatures and i need to calculate the duration to steady state on average across all equipments i have unfortunately only excel at the moment to run the analysis would you have any advice on how i can estimate this duration thank you
0,what benefits can i have as a mechanical process engineer if i have knowledge skills in data science hello all im working as a process engineer in a process equipment manufacturing company i’m still in the 2 3 year experience category most of the decision making for which equipment to select in my company is done through experience they say it’s important to also develop a feeling for how the product runs and machine behaves i feel like this decision making can be replicated by analyzing the past data so i’m interested to learn data science and statistical analysis i have experience in programming in vba bash scripting and can implement algorithms in any programming language thanks to stack overflow does it help to have this skill in addition to my general process engineering background what benefits can someone like me have with this addition
0,data scientists who have podcasts hi among many data scientists who among them has the most number of audience followers in their podcast
0,de salary expectation hi guys i m currently looking for jobs in the san francisco bay area in data science data engineering i finished interviewing with a small company start up for a senior data analyst role two week ago but was not given an offer this week the recruiter reached back to me for a data engineering role and i was given a verbal offer of 125k 130k equity unsure how much while the recruiter and hr prepare the offer letter i would like to know what range of salary i should come with that would be fair for me to negotiate location san francisco bay area company small start up job title data engineer education b s in applied math with computer science m s in data science experience 3 years of experience as data scientist per se not much but i have a good background in maths stats data analysis for my current role i do a lot of automation in sql excel power query current salary 110k the recruiter told me that the first 6 months of the data engineering role will be focused in building etl processes and there are high potential of moving into a data science analytical role i have done my research in glassdoor but because i am in the silicon valley the salary numbers are saturated with big tech companies salary compensation that seems unrealistic for me to use to negotiate thanks in advance for any insight
1,very quick question is there a problem having one predictor that is predictable from a combination of the other two imagine a study where people rate pictures on mood the predictors are whether there is blue in a picture yes no and whether there is red in a picture yes no the analysis is a mixed effect linear regression is there a problem in adding a predictor that is whether there is both blue and red in the same picture yes no in this example the model is like this rating bluepresent redpresent blueandredpresent my gut says this is wrong because blueandredpresent is redundant with a combination of the first two predictors however r actually lets me run this with simulated data and gives a separate coefficient for each predictor
1,career can a statistics heavy thesis make up for a non quantitative degree long story short i am currently finishing my master in health economics the degree involves some statistics but not nearly as much as a degree in econometrics or statistics most of the people in the program have a background in medicine and therefore the math is rather basic since january i have been working on my master thesis in cooperation with a well known pharma company market access department which is centered around bayesian health economic modelling everything is coded in r and stan my supervisor and me will try to publish an article based on my thesis preferably in medical decision making but it is not clear yet whether that will work out throughout the course of this project have found that i enjoy statistical modelling a lot and am considering a career as a statistician or data scientist somewhere in the pharma healthcare industry however it seems that for such roles employers demand a strictly quantitative degree like statistics econometrics mathematics computer science etc i did a bachelor in economics which gave me a solid foundation in math and statistics that i am currently refreshing i now really wish i did a master in econometrics statistics instead of health economics but it s too late and i don t really want to do another master do you think i have a chance of landing a job as a statistician data scientist maybe someone has experience in the industry and could help i live and work in the eu if that matters
1,career can i become a statistician studying at home some context i m a pure mathematician with a master degree the career prospects for phd s in my field of study is not promising anymore and i made a decision to change my career towards data science i m working as a private teacher and i m using all my free time i have plenty to study stats programming the problem is i fall in love with statistics i ve learned more in the last year than cs related subjects the last stats subjects i ve been learning with a strong enthusiasm and rigor doing the exercises included time series bayesian statistics and design of experiments of course i ve already learned the more fundamental subjects such as probability theory modelling and inference if i make relevant projects can i apply for jobs as a statistician or is it better to only focus to data science jobs more cs related maybe it depends on the field are there easier fields without a bachelor degree in statistics i can apply for i ve see some posts here people having problems to get a job without a phd so maybe i m to naive thinking i can get a pure stats job without a degree in stats
1,question does a correlation between two percentages constitute statistically significant enough evidence to be used cited in a scientific context let s say 5 of people have x property and 5 2 of people have y property when combined with other evidence indicating a link between posession of the two properties would the similarity in percentages be valid enough to cite in a scientific research context disclaimer i know almost nothing about statistics i am just really curious and couldn t find the answer with a google search
0,weekly entering transitioning thread 30 may 2021 06 jun 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
2,heuristics for initializing gp length scale hyperparameters i have been training a sparse gaussian process using the matern 5 2 kernel and i am having trouble getting the objective function to converge and i think it has to do with my initialization of length scale hyperparameters i am not training on actual function observations but instead on summations of several function observations and derivatives of the function—due to the particular application currently i am initializing each length scales as the std dev over all training data inputs for the corresponding feature but it doesn’t seem to be working well does anyone know of other heuristics
0,how do you guys approach a new data science project i’m a newbie in data science and i haven’t worked on any real life projects till now the projects i’ve completed have been part of some online course where the instructors either give away the solution approach to the problem or give you a big enough hint so that you don’t think about the approach but just code whatever they say i’ve been trying to solve a couple of projects of my own and it’s been hard to build that ds intuition normally how do you guys approach a new problem what kind of preprocessing do you do to your data which model to fit which metrics to choose how do you guys decide it i’d really like if you could help me develop some intuition on this
0,what course book talk etc added the most value to your career as a data scientist
2,reinforcement learning on real robots neurips 2021 challenge on robotics in the cloud benchmark are you tired of rl in simulation with unrealistic behaviors or intrigued by the sim2real problem then participate in the real robot challenge ii hosted as an official neurips 2021 competition x200b we have multiple of these platforms hosted similar to a cluster in tübingen extensive baselines and starting packages are provided win prizes by participating and solving tasks in the real world we have multiple of these platforms and you can submit your algorithm similar to a cluster tasks range from lifting a cube orienting it in air to manipulating multiple of them into new 2d shapes the goal of this challenge is to design machines that generalize across different settings in the real world for this we are hosting multiple robotic platforms at the max planck institute for intelligent systems participants submit their code as they would for a cluster and it will then be executed automatically on our platforms this will allow teams to gather hundreds of hours of real robot data with minimal effort you can participate in different rounds ranging from relatively simple to extremely hard from pushing one cube to manipulating multiple ones at the same time extensive baselines and tutorials for example from last year are provided and for more information we refer to our twitter account and our homepage you can likewise win prizes organized by members from mila mpi for intelligent systems nyu facebook deepmind
0,what are some limitations long standing goals of probabilistic programming languages i m very interested in probabilistic programming language design but i have very little user experience in actually writing probabilistic models and performing inference hence i lack awareness of what technical limitations one might find when trying to program with realistic models in everyday life as an example i m aware that a desire is to be able to perform inference in a modular way so that distributions probabilistic queries and inference algorithms can be reused composed and modified but i have no in depth knowledge as to what this elaborates to it would be extremely useful interesting if people could provide their personal user experiences either as programmers or statisticians with any languages they ve used such as church anglican pyro tensorflow probability and pyprob etc or if someone could point me to some nice resources around this thanks a lot please let me know if i ve posted this in the wrong place
0,how is testing conducted in data science i ve seen people stress the importance of testing but how is testing used in data science i m aware of unit and integration testing in software engineering but am having trouble seeing how it applies to data science when an output given some inputs is usually not deterministic it would be great if any one has recommendations for resources to learn about testing in data science
1,question survival analysis censored quantile regression and interpration of parameters x200b dear statisticians i m approaching the censored quantile regression for survival analysis a very intrestring approach expecially in cancer research where some treatments e g immunotherapy have a delayed effect and the proportional hazards assumption of the cox model can t hold i know it can be a niche approach but for this i would like to have the opinions of those who worked on it normally the censored quantile regression models the quantile of the logarithm of time to the event pretty much like an aft model but know we are talking about the quantile of the log and not just the log of the time to the event and again pretty much like an aft model the covariates have a linear relationship since we are modelling the conditional quantile we do not have to assume any distribution for the time to event and this is a strong point for this approach immagine i have only one covariate arm with values 1 treatment and 0 placebo i want to ask you about the interpretation of the parameters of quantile regression for the log time of the event in a survival analysis with just this covariate immagine i want to estimate the parameters for the 20 quantile the parameters i would see would be an intercept parameter and arm parameter for my interpretation the parameter arm is the difference between the 20 quantile of log time for arm 1 and for the same quantile for arm 0 and as you can immagine for me the intercept parameter is the quantile of log time for arm 0 so by exponentiating the arm parameter i can obtain the ratio between the 20 quantile of time for arm 1 and for arm 0 i ask you first if this above is correct moving on i ask you from the parameters i can obtain modelling quantile of log time is there a way to obtain something not in terms of ratio between the quantile of time but in terms of difference between the quantile of time between arms thank you for your support
2,discussion citizen science platforms for annotating images with bounding boxes i came across this platform where one can upload images and volunteers can draw polygons or bounding boxes around an object of interest in my case insects the results will be later used for training a cnn i was wondering if there are other similar options mostly free and citizen science based
2,i’m ilya team principal of acronis sit autonomous racing team current n1 in roborace – ask me anything hi reddit sit autonomous is a machine intelligence consulting firm based at the schaffhausen institute of technology sit in switzerland we also have an autonomous racing team – acronis sit autonomous – of which i am team principal if you’ve heard of roborace the world’s first extreme competition of teams developing self driving ai for autonomous vehicles you’ve probably heard about us tomorrow june 24 from 4 pm to 7 pm cest 10 am edt to 1 pm edt i’ll be answering all your questions about ml in self driving cars and autonomous racing already eager to ask send me your questions now so i can be sure to answer them all the discussion is happening in r selfdrivingcars see you there
1,can someone please show me how the expected value of f x is calculated here this is an article on gaussian process regression in the interpolation section they show how to use gaussian process regression for prediction i assume you can use this formula for extrapolation as well f x m x sum from i to n of k x x i alpha i at the start of this article m x is defined as m x e f x where e is the expected value and f x is the true function you are trying to estimate does anyone know how e f x is calculated i understand that you assume a kernel function and a bandwidth value but how is m x calculated i e e f x also is the only assumption of gaussian process regression that the data in real life is generated using the specific kernel you selected in general does gaussian process regression make fewer assumptions about the data compared to standard linear regression
0,how annoying is it to see undergrads embellish their titles hello all i’m a currently a sophomore whose been actively involved with various data science projects obviously once i gain some good experience at an opportunity i like to put it on my linkedin or resume i never have been the person to “flex” internship or research positions and i don’t feel that those who do it are bad either if your excited about an opportunity you worked hard for good on you i love to see such posts but for me i don’t like the attention i even turnoff the notifications on linkedin regarding my change in positions or when i get a new position so other people don’t see it anyways the point of this post is to guage how much it bothers data science hiring managers about the embellishment of titles by undergrads who are applying for internships for example this summer i’m doing undergraduate research doing data cleaning and classical statistical analysis but i don’t call the position a “statistician” or my other two positions i had previously as web scraping data sources creating db for a professor and creating scouting reports for the university baseball team i didnt call them “undergraduate data engineer” and “data analyst” respectively am i right in not placing these titles on my resume i have friends who put their titles as “data scientist” but i feel as though if i don’t have a degree yet or am an undergrad it would feel like i’m embellishing a title and it may annoying hiring managers especially when i apply for internships thus far i just have “research assistant” or “project manager” and talk about what i did underneath what do you think am i right in not putting such titles quite frankly i feel like the title is not as important as the actual work that is done right
0,can i add university sponsored company projects as experience in my resume hi i am doing my masters in data science in berlin but my background is from automotive and have work experience as pmo for 2 9 years in digital transformation now to switch my career to data science i took masters in berlin now i have done 2 company projects with the help of university and it was a part of university curriculum can i add this as experience in my resume or i need to add this only as a side project will it be misleading the recruiter or the company i am giving interviews kindly let me know
2,how are you approaching prediction uncertainty in ml systems most models return a point estimate of some sort regardless of the task in some situations e g finance and risk management the uncertainty in the prediction is just as important as the prediction itself how are people dealing with these scenarios i usually turn to generative models e g probabilistic programs and bayesian inference i’ve written up my thoughts on how to engineer these into a ‘production system’ deployed to kubernetes using pymc and bodywork an open source ml deployment tool that i contribute to given the simulation based nature of generative models perhaps it’s unsurprising that the resulting system is a little slow i’d be really interested to get some feedback on the approach or hear about alternatives
0,how can i use data to help people i ve always been interested in helping people animals since as long as i could remember i m a data analyst in my current role tableau sql excel learning pandas and don t deal with any complex data science projects models but i am self teaching myself and having a blast does anyone know of any ways we i could use data science analytics to help people non profits etc thanks edit any projects you have done for the community would be awesome to hear too
1,can i use logistic regression in a case control analysis i m watching a linked in learning course that says that a general rule of thumb is that a cross sectional analysis logistic regression is not appropriate if less than 10 of your rows have a 1 in the dependent variable column in that case you should do a case control analysis instead my understanding is that in cross sectional studies the entire subpopulation is used and those w the outcome are compared to those w o the outcome but in case control analysis since the outcome is rare 10 you take all those with the outcome and only a portion of those w o it so you get a more balanced dataset a 1 1 up to 1 4 case to control ratio so why can t you just do logistic regression on that dataset just like you would in a cross sectional analysis or is it okay to do that you just interpret the results differently in cross sectional the odds ratio represents how strongly the exposure is related to the outcome whereas in case control it represents how strongly having the outcome is related to also having the exposure just want to make sure i m understanding things properly this is all very confusing
2,identifying bias poor performance in machine learning models let’s say you developed a machine learning model to predict whether or not someone will need food stamps this month and we want to see when where our model performs poorly for instance we might make poor predictions for people from low income populations and we want to identify that how can we systematically and automatically identify where our model is performing poorly can anyone point me to some papers thank you
2,from motor control to team play in simulated humanoid football deepmind a 20 authors paper from deepmind where they show that they can train humanoid agents to play 2 vs 2 soccer the video is pretty impressive youtu be khmwq9pv7mg
1,what are the differences and relations between these three exogeneity exogenicity exogenous assumptions in linear regression on p413 in r in a nutshell by adler technically linear regression is not always appropriate ordinary least squares ols regression implemented through lm is guaranteed to work only when certain properties of the training data are true here are the key assumptions 3 exogenicity of the predictor variables the expected value of the error term ε is 0 for all possible values of the predictor variables 6 exogenously generated data the predictor variables x1 x2 xn are generated independently of the process that generates the error term ε in weak exogeneity this essentially means that the predictor variables x can be treated as fixed values rather than random variables this means for example that the predictor variables are assumed to be error free—that is not contaminated with measurement errors although this assumption is not realistic in many settings dropping it leads to significantly more difficult errors in variables models what do the three assumptions mean mathematically rephrased in terms of math formulas what are the differences and relations between them is the weak exogeneity assumption in wikipedia the same as one of the two assumptions in the book thanks
1,course recommendations hi i m looking for recommendations for online courses that cover some advanced statistics for social sciences i m pretty good at regressions multiple regressions mediations moderations etc and factor analysis including multigroup stuff but would love something like quadratic regressions or ols or even introductory machine learning stuff i m also proficient in r so the course could be taught on r alternatively an introduction to python for statistics focusing on the python bit and not the stats bit could also do so that i could learn one more language haha i would also prefer audit only or free courses mainly because i m not from the us so the cost is too high generally some background i m planning to apply for psych grad school and have a lower gpa due to stats but in the years since my last degree i have become compareitively proficient with applied stats as i m in a research position currently tldr recommendations for advanced stats online courses with r introduction to python for people in psychology social science
1,how do i set up my data for linear regression if i have multiple observations for one sample i have a dataset that i need to do linear regression with to find the residuals it is gene expression data for multiple genes for a cohort of people i am using their health data as covariates the problem i am running into is there is 1 point of data per person for lets say sex m 1 f 0 there are multiple observations for each person for gene expression do i repeat the sexfor the patient as many times as there are observations if patient a is male and there are 3 different genes expressed that i have data for does my data look like this x200b patient gene expression sex a z 5 1 a x 6 1 a c 5 5 1 x200b thanks in advance
1,a neat way to visualize standard error a group of samples can be used to generate a normal distribution that approximates the population distribution central limit theorem repeating this process many times generates a cloud of curves whose spread is a representation of their standard error for example n 10 transparency was applied to the curves so that their density at any point is represented by its brightness increasing the number of samples looks like focusing a camera n 30 the mean and standard deviation used to generate each curve all bounce around the same value as in the previous graph but with less error this effect is exaggerated by increasing the number of samples again n 100 these curves were generated through simulated dice rolls so they can be compared to the true population distribution marked by a red curve here true value comparison it s not a great way to visualize standard error but it s a cool way to explain the concept
0,new to corporate should you submit your task way before deadline hello sorry this might be a very stupid question but i ve seen many people saying never submit your task before the deadline what s the thought behind this statement asking this because i m a junior at a company and my manager often overestimates the time required to do certain tasks he gives me and right now i happened to be in a situation where he gave me a task with a deadline of a week and i finished it within a few hours in your experience what s the best thing to do thanks
0,so what s the point of having a masters or a phd in this field we can all agree that most of the job is boring data stuff collecting cleaning designing transforming and the list keeps going this takes like 75 to 90 of our time the rest of the time is powerpoint excel meetings building models ml and ml dev ops so my question is why so many positions require master or higher education do you guys have this kind of education do you guys do actual complex stuff what do you do that you need to have a phd to do it i got 3 years of experience and i ve never needed to know anything that couldnt be learned in a basic blog post of some random dude i guess some positions do need very complex knowledge of npl or dl but the rest no way
0,data science projects in manufacturing hi all are there any data scientists here working in manufacturing food and beverage ideally i m part of a new startup data science teams and we re looking for good use cases thanks all from dublin ireland
0,how does your team deploy models i work on a data science team that does work that is hybrid of typical ds de da work some days it s simple sql and tableau other times its building pipelines through real time ingestion with kafka or batch etls using airflow sometimes it s hard statistical analysis and occasionally we do actual machine learning we are all data scientists by title but play the hybrid of just being good with data regardless of specifics coming to da versus de versus ds recently i ve felt like i ve been missing the ds side of things i get the occasional one off request for building data models but don t ever really deploy anything with them usually just local models and then sharing the results i am wanting to start putting some models in production as a way to show my usual stakeholders the value that ml can have for the business and i m curious in all the ways you guys do this on your team i specifically am curious about two scenarios scenario 1 model only needs to be running when the etl runs most of our work is batch processing data at some interval i typically utilize airflow with kubernetes pod operators to spin up python containers that have the code i need for the etl i figured a super simple way to do this was to just store a trained model in the container itself and have the scripts run the models the same way i would locally whenever the airflow job is triggered this seems simple but i m sure my idea is flawed i know a major limitation here is that to retrain the model i need to pull my container back to my local machine retrain the model locally rebuild the container and push it back to artifactory so the etl can now use the updated version there s also the limitation that other people can t use the model since its only present in production when airflow triggers the pod to spin up scenario 2 model needs to be available and running at all times i often utilize kafka triggers deployed in kubernetes pods to listen to topics of interest while they usually just write data to a db or send notifications to stakeholders about the event that occurred i see a lot of potential in being able to point data from an event into a classification or regression model in real time and then store the results of the model for later use to do this i think my approach would be to build a flask api that is deployed in the same kubernetes cluster that i could point my kafka trigger towards i have played with flask before but am not as great with the whole devops side in terms of configuring ips and making sure my flask api is available for other apps to use i would love your feedback on how your team deploys models and code especially if you run into the above two scenarios
0,to the senior people of this sub how much reality is there on the app blind hi i recently joined blind app to get some insights on an interview i had at one of the faangs i came across a lot of comments like amazon is giving you 110k are working as a janitor there i understand many of those people are just doing a playful banter but there were serious posts like i m depressed because i earn only 320k year and my friends earn way more than me my question is are those people totally oblivious of the average salary or the fact that not everyone earns 6 figure salary i have just started working in the tech industry so i don t have that much experience so i wanted to ask some of the senior people here that is a salary such 300 400k really possible in the tech industry and if it is what percentage of people do actually reach that level in life thanks
2,generating new points with smote there is a well known algorithm in statistics called smote synthetic minority over sampling technique which is often used to balance and imbalanced data set if i have understood correctly the premise of the smote algorithm is as follows suppose you have a dataset containing information for medical patients that are healthy and not healthy but let s assume that the majority of the patients within your dataset are healthy the composition of healthy not healthy being 95 5 if you want to make a statistical model for this data the data does not contain enough information for not healthy patients and it will be very challenging to build a reliable statistical model that can make accurate predictions for not healthy patients thus the smote algorithm can fix this problem by 1 rebalancing the data set e g after smote your data set can have a composition of 70 30 2 creating new data points from the existing data as i understand this is done by multiplying a given vector corresponding to a randomly selected individual observation by some random number between 0 and 1 this leads me to my question suppose you have already have a balanced dataset e g the healthy not healthy has a composition of 60 40 but let s assume that you have a relatively small dat set to begin with e g 500 rows can you use the smote algorithm to create new data points so that your dataset is bigger i understand that no algorithm can magically compensate for data quality issues but at the same time i don t see any major flaws with using smote on already balanced data for reference i illustrated this process below using r i would be interested in hearing a second opinion thanks load and install libraries remotes install version dmwr version 0 4 1 library dmwr create some fake data and put them into a data frame called f var 1 rnorm 100 1 4 var 2 rnorm 100 10 5 var 3 c 0 2 4 var 3 sample var 3 100 replace true prob c 0 3 0 6 0 1 response c 1 0 response sample response 100 replace true prob c 0 3 0 7 put them into a data frame called f f data frame var 1 var 2 var 3 response declare var 3 and response variable as factors f var 3 as factor f var 3 f response as factor f response smote algorithm simulate new points from the first class smoted data over smote response f perc over 100 simulate new points from the second class smoted data under smote response f perc under 100 combine everything together into a final new data file final rbind f smoted data over smoted data under
0,does anyone use stata i have a master s of public policy and i generally would do data cleaning etl in excel and then import it into stata for multivariate regressions it was nothing fancy but it was insightful and i enjoyed it now i want to expand my knowledge into sql but i am wondering if realistically so few people places use stata that i should start learning python or spss instead
1,machine learning reading recommendation hey today i realized that even though i have been using statistics for work and research for more than 5 years i have literally no knowledge of machine learning and classification algorithms neural networks random forests you name it is there a book that introduces machine learning and the most frequently used methods in a practical application oriented cookbook style it would be even more amazing if it included examples with r code i am mostly interested in random forests and neural networks but i appreciate whatever i consider myself fairly advanced in terms of my knowledge of applied statistics so it does not have to be a book written for complete dummies but i lack the mathematical sophistication to appreciate the finer intricacies so i would prefer something with less formalisms thanks
0,am i an idiot or is there some lingo i wasn t taught in school i had an interview for a ds position last week and the interviewer asked me a couple of questions that threw me for a loop first he asked me to compare a rectilinear decision tree to a random forest and second he asked me about scale free distributions my guess is that he wanted me to speak of the advantages of using an ensemble of trees along with randomly selected features and bagging over a single tree as for the distribution question no clue and i can t find anything on google x200b any thoughts
2,image search chatbot and chemical structure demo hi all we recently updated our demo page showcasing examples of vector search for image search chatbot and chemical structures using milvus open source software i m looking for feedback suggestions or questions you may have to make this relevant for any applications or use cases you may be developing thank you all i look forward to hearing your feedback demo page github repo
0,any pms here how do you run weekly meetings this isn t specific to data science but i was asked to project manage a data coding effort i ve been totally winging it but i realize it s time to get organized what should be the weekly agenda there are four of us meeting for 30 minutes twice a week they suggested doing two week sprints and user stories i figure a basic agenda looks like 1 discuss the sprints for the next two weeks based on user stories then next meeting 2 how far did we get since last time 3 challenges in the approach 4 other project related business there are various angles to this project long term goals etc 5 retrospective at the end of two weeks does that make sense also one member setup a kanban board in github very useful i searched agile meeting etc but keeping getting results for scrum we re not doing a daily standup but maybe it s a similar format
0,does knowing r instead of python makes you unhireable hello sorry for another r python post recently i got a task at work in which i had to read through multiple sheets of excel clean transform reshape it and make it into a single dataframe i hadn t done this type of task in either r or python since it was not really a time constraint task i decided to do it in both and learn how to do it in both languages i am better with r than python i barely know python actually so i started doing it in r and comfortably with google did it without taking much time after that i tried it in python but i m still struggling to finish it i will be able to do it but it s taking me significantly more time than r is that just the learning curve of python since i barely know the language or some things are just easier in r and i should just do it in the language i m comfortable with i m afraid that that i ll never be able to learn python like this and won t getting any interviews since i don t know how to do stuff in python thanks sorry for the long post
2,assumptions of k means clustering i have often seen blog posts like this which show that k means clustering algorithm is unable to handle complicated data and can only recognize clusters within spherical clusters i have often seen this shown empirically like in these blogs but are there any mathematical justifications that explain why k means is unable to recognize clusters in more complicated data e g concentric circles crescents etc why is k means only good for specifically spheres beyond empirical demonstrations are there any reasons why k means receives a lot of criticism
0,how can i create a dataset featuring clusters with inhomogeneous densities with python do you guys have any idea sklearn doenst have a built in library to do that edit i mean inhomogeneous internal densities
1,question statistical test for two sets of coordinates i would like to run a test to estimate if two sets of coordinates are coming from the same distribution in my case one set of coords are measurements of an underlying process and the other set are possible values coming from my own model i know that this can be achieved with a kolmogorov smirnov in 1d but i don t know how to translate this into 2d example image
0,features showing the decrease of a value in time guys need help on this one i am building a binary classificator to output whether a costumer is going to churn on the next few months or not i believe that the number of transactions that said costumers do on our app greatly reduces over time until he finally churns with this in mind i need your help to 1 decide if and on what time frame of time the decrease is relevant 2 how do i represent this as features let s say that we decide on item 1 that the of transactions on the last 3 months greatly reduces is it enough to include like of transactions on months 1 2 and 3 before the churn i was wondering that maybe i can set a hypothesis test to verify and confirm 1 though would still need help to decide on the time frame i tried to search something to read about this on the interwebs but couldn t find thank you guys for any help
2,paper explained efficient and modular implicit differentiation full video analysis many problems in machine learning involve loops of inner and outer optimization finding update steps for the outer loop is usually difficult because of the need to differentiate through the inner loop s procedure over multiple steps such loop unrolling is very limited and constrained to very few steps other papers have found solutions around unrolling in very specific individual problems this paper proposes a unified framework for implicit differentiation of inner optimization procedures without unrolling and provides implementations that integrate seamlessly into jax x200b outline 0 00 intro overview 2 05 automatic differentiation of inner optimizations 4 30 example meta learning 7 45 unrolling optimization 13 00 unified framework overview pseudocode 21 10 implicit function theorem 25 45 more technicalities 28 45 experiments x200b errata dataset distillation is done with respect to the training set not the validation or test set x200b paper
1,how to compare two datasets in r very basic question but i m very new to r programming i m writing my thesis and need to compare datasets with one another i will for example use the dataset of transparency international ti called corruption perception index cpi and compare it with the dataset human development index hdi from united nations development programm undp i also need to compare other indexes the other hypothesis all of them will be compared with the corruption perception index from transparency international i am very lost with this statistics programm so any tips about how to even start with this would be greatly apreciated thanks
2,running ml experiments on a dataset with 5000 samples and 500 000 categorical features i m using python and have access to a solid computing cluster with solid gpu power i am wondering how to do the following 1 load the dataset which i assume has to be done in memory but could also be parallelized distributed using something like dask or vaex 2 feature selection starting from the full 2m feature set using recursive feature elimination and top x features from different feature ranking methods e g f test gini importance shap however can these be done if the data are parallelized
1,adjusted r squared and colinearity hello everybody i recently took a statistics course on coursera and the final project was to build a linear regression model based on a couple of variables to predict movies popularity two of them being the movie audience score my dependent variable and imdb rating the correlation between audience score and imdb rating is 0 86 if i fit the model without the imdb rating the adjusted r squared is 0 23 if i include the imdb rating is 0 76 both are statistically significant with p value 0 with that being said was it a good call to exclude imdb score because of its high correlation with the dependent variable thanks
2,blog post graphs at iclr 2021 iclr 2021 has been over for 1 month i have put together a blog post where i summarize discuss a number of graph geometric dl papers from iclr 2021 along with some personal reflections on the field there are papers about spectral methods graph attention meshes simulation the bottleneck issue and some more you can find it here of course any feedback is appreciated
1,is this the correct understanding of partial dependence plots i was reading about partial dependence plots just to clarify is this the right idea suppose you have a model blood pressure 1 4 height 3 2 weight a partial dependence plot could help you understand the effect of height on blood pressure if you were to make a plot of height vs blood pressure you would be indirectly modelling the impact of weight as well a partial dependence plot is able to completely isolate the impact of height on blood pressure is this correct thanks
0,can the prediction of gradient boosted decision trees be parallelized i know that gdbt model cannot be parallelized during training because the trees are built sequentially what about its scoring making prediction phase for one single sample as far as i understand we can evaluate the outputs of all trees at once hence parallelizable and the final prediction is just a linear combination is that correct
1,indipendent pathways model and initial path model understanding problem i am a clinical psychology student that is writing his work for the degree discussion and i know the logical meanings and implications of those models however the commission want me to explain how did they calculate those results what s the interpretation of those numbers and so on my problem is about the two models that i mentioned in the title i know that for the calculation of those numbers the analysis of correlation and the multiple regession are involved but i don t know how exactly just discovered that tomorrow i have a demonstration to make of what my discussion will be and i don t have sccess to my statistics books any insights
2,finding best function in an interval with ml hi all i d like to use ml dl for a different objective than usual i want to find the f x in a b that best respects some constraints both on f x and on x to be clearer the result of the optimization should be a polynomial function so anything like x³ 3x or 2x² 8 ecc the reason for using ml and not some bruce force approach is that the ml optimizer can learn which kind of functions better respect the constraints by trial and error and eventually finding the way but that s just an idea maybe ml can t even be used i tried to search something in literature but i couldn t find anything could you help me out a little at least for knowing what to look for thank you in advance edit to be more specific i have a function f k n that with fixed k k0 and varying n discrete is something like a bell now the problem is that when k0 increases the bell width of f k0 n increases too however instead of f k n i m allowed to use f g k n where g k is polynomial in k now the question is how can i find the best g k such that when increasing k0 i have that f g k0 n has a constant width bell let s assume that the width is the number of points on the n axis inside the bell
2,discussion gan training tracking of gradients w r t input noise vector when training a gan should you track gradients w r t the input noise vector so if you have code like this python z torch randn generate noise vector z requires grad true is this necessary generated generator z is the requires grad necessary what would it actually mean if we were to track gradients in the input noise
2,experience with google s vertexai so i benchmarked an ml platform i created with some friends nyckel com against googles vertex ai formerly known as google automl i was honestly expecting them to blow us out of the water but to my surprise google left the room and came back 5 hours later with an inferior model to what nyckel returned in 60 seconds any ideas what they are doing during that time my best guess is some sort of network architecture search but in that case i d expect them to come back with a killer model this particular test was training text sentiment classifier on a public datasets i also found other parts of their service excruciatingly slow uploading and parsing a csv file with 5000 lines seriously took them 5 minutes has anyone else had bad or good experiences with google s vertex ai here is a blogpost i wrote that shares the results
1,drawing questions from a question bank how many in common hi everyone i m a teacher and while we ve been virtual for the past year i ve adapted some of my quizzes to help curb the possibility of cheating for my quizzes i have a question bank that the questions are randomly drawn from for example the question bank has 10 questions and each student randomly gets 5 of them i know that in this example the number of combinations is quite large and the odds that any two students get the exact same 5 questions is low 10 choose 5 252 at first it sounds like i ve done enough to randomize the quiz however i ve realized that s really the right question to answer while they may not get the exact same questions if two students are expected to get say 4 questions in common then i haven t really randomized the quiz how can i calculate the expected number of questions in common i studied math in college but don t use it much these days also sorry if this is obvious or has been asked before i wasn t exactly sure what to search for in order to find an answer thanks
0,tutoring for data science hi guys i m curious what websites you guys have used in the past for data science tutoring what has been your experience with particular websites services andrew
1,spectral density under non stationary processes does it make sense to talk about a time series in the frequency domain if it is not stationary in a weak sense i m asking because i ve only seen it formulated in terms of the autocovariance function for weakly stationary time series
0,what do you do to combat the afternoon slump there was some good discussion in a previous post about not working the full day im curious to hear for days when there is work to do that isnt entirely motivating what do you do to keep energy up since i ve been with jobs that involve sitting all day i usually hit a slump around 2pm and am trying to find more techniques for picking up my energy and focus sometimes i ll take my dog for a walk or give myself a 10 minute reddit break but what works for you
2,video datasets here is a list of existing video datasets for cv research x200b
0,rant from a hiring manager ms in buzzword programs are a waste of money i m a first time hiring manager and i m hiring a data analyst i ve reviewed hundreds of resumes and talked to dozens of people to calibrate myself on what people from different backgrounds are like call me a purist but i don t think it s possible to do good quantitative work if you don t understand how the models work from a basic mathematical perspective for example you should be able to answer the following questions about any given model you use in your work and mention in an interview how should residuals be distributed in any given model you re using what does a p value actually represent in mathematical terms what are the assumptions around independence of your predictor variables for any given model you re using what diagnostic tests should you run for any given model and what hypothesis are each of these testing and more if you don t understand these things then you are going to build an overly complicated model that is completely overfitted as opposed to a more generalizable model that can be applied to multiple datasets the number of people i ve talked to who have a ms in business analytics who say they re doing ml but then can t explain these basic underlying assumptions of the models they build is astounding and pitiful it doesn t matter how prestigious the school they went to i ve talked to people with degrees like this from both ivy league schools and from lesser known schools and it is all the same i m not trying to trick people with obscure trivia or anything when i ask them to tell me about a project they worked on they tell me i built insert model so i could solve whatever business problem and when i follow up with what are the underlying assumptions for insert model what kinds of diagnostics did you run for that model they get completely tongue tied even for the basics like logistic and linear regression the only things these people ever check for with diagnostics is sensitivity and specificity and area under the curve which barely scratches the surface in determining whether or not your model is good i ve looked into the requirements on these programs both when i was choosing a masters program a few years back and also more recently as i ve been interviewing people and none of these programs require people to take any courses that would actually prepare them to build models thoughtfully just courses like sql for analytics tableau for visualization and super tool specific classes that a would be super easy for a reasonably smart person to learn on the job and b are not going to give them skills that will be longterm valuable because the tools we use change all the time underlying mathematical principles do not so if you re thinking about getting an advanced degree to get more out of your career in this field do yourself a favor and choose a program in a real academic field ex statistics math computer science engineering and not some buzzword that s super hot right now no one will care about your bullshit buzzword degree 10 years from now at the bare minimum choose a program that requires you to take classes that are well founded in probability and mathematical statistics and not just plugging random shit into r or python hoping that you ll be able to predict the future
0,so uh what do you guys actually do i m studying data science and machine learning while working as a data analyst for a digital consulting agency i have the flexibility to experiment with what i m learning at work but machine learning is just a hammer for which i can never find a nail i understand how machine learning is used for voice and image analysis but i keep seeing data scientists say they create predictive models to find business solutions but what does that mean i d love to apply what i m learning at work but i can t never see a way that creating a predictive machine learning model would help me better solve a business problem what kind of projects are you guys actually doing at work
0,has anyone ever applied an unsupervised learning method or reinforcement learning method to define ‘roles’ at the company think permissions ad groups and on boarding
0,any good resources for learning statistics applied to datascience from scratch hello i finally came to realize that i use almost every libraries as pure blackboxes and that it is a problem i thought before it was not as soon as you can give the management a correct result if i want to improve myself professionally speaking i have zero background in mathematics so learning through a university syllabus is very very hard and i don t think i ll be able to finish that would you have any resources that is a bit interactive and noob friendly i enjoyed learning python and r through small own made project that s how i ended with my current data clerk position but i don t see how i could do that for stats thanks
1,how can i measure the relationship between one independent variable and multiple dependent variables but the dependent variables are a mixture of both continuous and categorical variables the independent variable is continuous which model would be most appropriate i would appreciate being directed to any solution using python packages such as sklearn or statsmodels api
2,datascience vs automation rpa future prospects hi everyone i started my professional journey only a few months back i was put into the automation team which is totally out of my area of expertise as i have had no background in computer engineering i majored in economics with a minor in computer science and have always felt that i would be more suitable for a datascience related role but being this early in my career i have heard sayings like you shouldn t fixate yourself on one field this early so i am super confused on how to proceed should ask my manager for a switch to another team as this will result in severing whatever relationship i have built with my manager which i might regret later or should i keep at it and see where this takes me i guess it boils down to 1 is it recommended to work in a field that i see no future in yet but does broaden my skill set 2 will it put me at a disadvantage if i dont get direct exposure to datascience projects if i do want to pursue datascience in future 3 how are the future prospects of automation rpa in comparison with data science sorry for the long winded question this has been troubling me for quite some time so please mind if its comes out a little vague any type of guidance is hugely appreciated thank you
1,incoming freshman in college about to take statistics major in actuarial science just for some context i just got my college results and i got into the university and program i wanted now that i’m sure i’m going to be taking statistics as my course i want to be productive with my free time and be ahead of the game if anyone has any resources for me that i should check out and learn let me know also if anyone has any advice for me would love to hear them as well
0,deciding a classification algorithm i am new in data science so please excuse my limited knowledge i am learning more about classification algorithms and how to appropriately selected algorithms for your task according to this website classification algorithms the author says when the classes are not linearly separable a kernel trick can be used to map a non linearly separable space into a higher dimension linearly separable space so do you just check if your class is linearly separable or the entire dataset in any case i could not find a proper source that would explain clearly how do you go about checking linearity of the data with actual implementation can you help how to achieve this further a lot of blogs and tutorials mention that when you are selecting a classification algorithm you have to consider dataset size distribution computation time data type of attributes etc but i came across a notebook which was authored by a university professor wherein he used almost all algorithms like svm adaboost decisiontree random forest bagging boosting etc for the same project does it make sense to use all of them straightaway is there like a resource that can help in making a decision i did come across a few cheat sheets that have a flow chart but they don t seem to be in depth
2,on the evaluation of deep learning models in case of highly im balanced data and small test dataset hello i am currently working on a project regarding the classification of images in particular chest x ray images unfortunately the train dataset is highly im balanced 1 6 of class positive and 5 6 of class negative while the test dataset is very small but balanced 1 2 of both class positive and negative i am trying using some relevant deep learning models for classification such as vggs resnets and densenets in order to mitigate the train dataset im balance issue weighted binary cross entropy is used as the loss function where the weights are inversely proportional to class sizes moreover scaling and translating data augmentation techniques are used as well evaluation metrics include precision recall and f1 scores for simpler networks in testing phase i obtained very good results on the test set however the results on the non augmented train dataset i e resubstitution metrics are much worse the main question is since the model performs better on the test set is this a sign of under fitting or is this due to the fact that i included data augmentation when training the data second for more complex networks with a lot more of parameters e g vggs the evaluation metrics on the test dataset are much similar to the evaluation metrics on the non augmented train dataset however generally performing worse than simpler models on test data so the other question is how should i fairly compare these models since the ones that perform very well on test data perform a lot worse on train data and viceversa thanks in advance
0,labeling whether or not a vest has been shot with an airsoft bb using ml hello mates i ve been thinking for a while on how to approach this let me give you some context before anything else i have an array of piezoelectric sensors inside a vest that will signal analog values to a embd system every interval lets call it t this sensors will throw data when they are hitted by a bb when a player crouches whenever he pick ups something basically whenever they receive a force the value of the analog signal will be as big as the force applied my toughts so far are on recording as much data as i can of different scenarios label it whether the vest was hitted by a bb or not and fit it in an algorithm all i could think about is to use a basic nn and feed it with lets say t t 1 t 5 iterations and see how well it does any thoughts ideas criticism everything is welcome thanks in regards edit just to make sure everyone is on the same page remember that the controller will be receiving dats from multiple sensors
0,how to choose between a well structured code base for all analysis a collection of one off scripts and the spectrum in between this question is primarily to understand how you all think of structuring your code and or file system structure for projects there are times when what i need to do is very well defined and it s easy to think of a package structure for a version controlled package of code within a docker container that i can develop on and deploy other times i m doing what seems like a simple one off type of analysis that s meant to support or check the results of a more major line of work do some computation and hand off something to someone else in a different format etc often i ll do this type of work in a different directory that s not version controlled because i don t want to complicate it by adding it to my clean code base or create options for my existing code base that are unlikely to be used in the future and will complicate its interface however sometimes these one offs build momentum and become more persistent and important and i often struggle with finding the right time to put them into the official package or their own versus letting them exist on the side as one offs and modifications thereof a related issue is that when my code is not structured into a well thought out package my directory structure tends to look like this └── new project ├── data prep │ ├── prepare data py │ └── raw data │ └── raw data file └── run model ├── model code py └── try with mod ├── model code1 py ├── output explore ipynb ├── with more iterations │ └── model code1 py └── write out intermediate for jim └── model code1 for jim py the problem with this structure is a couple things 1 sometimes the number of try with mod trees can get pretty nested in ways which are difficult to manage from a git branch perspective on the other hand when they do get nested they do convey useful information on the time ordered progression of my projects something that branching in git doesn t necessarily preserve that time ordered progression is one of the main reasons i keep this directory structure and have to actively fight against it 2 the code i put into write out intermediate for jim is usually something quick and hacky to give someone an intermediate result and is not version controlled not having it version controlled or documented is just not a great practice do any of you struggle with these types of situations how do you view the trade offs of clean well organized code bases versus one offs do you have a better way or organizing your one offs than to create subdirectories like i ve outlined here
2,acceptability of ai in healthcare we are still recruiting participants with a knowledge or interest of machine learning for our study exploring the acceptability of artificial intelligence in healthcare the survey should only take around 5 10 minutes to complete and is being conducted at the university of liverpool please let me know if you have any questions before participating contact details on link
2,repmlp re parameterizing convolutions into fully connected layers for image recognition this paper is about mlp but does not intend to build pure mlp models like mlp mixer gmlp and resmlp the motivation is that fc has global capacity but lacks a local prior so we use convolutions to incorporate locality into fc it proposes repmlp an mlp style building block and uses convolutions to incorporate locality into fc layers to make them effective in extracting features in images it significantly improves the performance of cnn like resnet 50 by up to 2 78 03 top 1 acc 80 07 like repvgg 2021 acnet iccv 2019 and diverse branch block cvpr 2021 it is also a member of the structural re parameterization universe as the convolutions can be equivalently merged into the fc via an elegant and platform agnostic algorithm it turns out that fc is significantly faster than conv with a comparable number of parameters and has a stronger representational capacity it also suggests that viewing convolution as a degraded fc may bring more exciting discoveries paper code x200b in other words the contribution can be summarized as follows assume a model has two components a some conv layers seen from an fc perspective each conv is equivalent to a sparse matrix with repeating parameters toeplitz matrix b a regular fc we merged a into b and got an fc that is neither a regular fc nor a conv it looked like semi fc global representational capacity and semi conv locality it showed a better speed accuracy trade off than traditional conv and b significantly higher accuracy than a regular fc
1,how is the standard error ci of multiple data sets while doing operations on their differences calculated hello i ll try using the proper vocabulary but don t hesitate to correct me if i seem to use the wrong words i m having an issue with interpreting statistics of data sets maybe i m overthinking the thing and it might be pretty straight forward p i have 3 sets of unskewed normally distributed data a b c from which i calculated their mean average with a 95 ci and a sample size n high enough for a 0 05 standard error n≃40000 per data set their mean cis are a 1574 0±0 8 b 1616 4±0 8 c 1595 9±0 8 we can safely assume their variance are equal my main goal is comparing the difference between the data sets to see the effect of variables applied to a 1 variable was applied to b another 1 to c and end up with a ratio of effectiveness by comparing data sets between them b s variable is x times as effective as c q1 i think we can safely say the average difference between a and b is 1616 4 1574 0 42 4 but what can we say about the standard error of this value is it 42 4±0 8 as we keep the cis range of initial data meaning its error is about 0 8 42 4 1 89 q2 what can we say about the relative effect of b s variable compared to c s c average difference being 1595 9 1574 0 21 9 it seems safe to say b variable is about 42 4 21 9 1 94 times as effective as c s the effectiveness ratio but what s the error range of this value experimentally it s been varying quite a bit by repeating the whole process i d say as high as ±0 4 but have no idea how to properly calculate that thanks to anyone willing to help
1,help design of experiment using taguchi method i m doing a process optimization project and i chose to use taguchi method to reduce the number of iterations required for it i need around 30mins per iteration i basically have 4 factors 3 level each and a cycle time as my response upon reducing cycle time i got different defect phenomenon total 3 types which i took as my responses i have setup my experiment levels in such a way that i get the required cycle time at level 3 each albeit with increase in defect phenomenon i suspect that there is a fault in the machine operation for which i m trying to find some proof through this solving this fault should give me acceptable times is what i m going with right now im a complete newbie in this field and have 0 experience setting up such a study the expert at the company suggested i go with full factorial and try to study the interaction plots another alternative he gave me was to explore rsm i m pretty sure that full factorial will definitely not cut it given the time constraints and i m unwilling to try rsm for the massive theory that i d need to understand before any alternative solutions to this problem are also welcome😄 the current situation i m currently using the full l9 array 3 4 with 2 repetitions and started facing an issue on how to do it s anova the df for residual error comes out to be 0 which doesn t generate any f value in minitab although i do have the main effect and sn ratios most inference i could make from them could be highly situational and probably biased towards this fault i tried removing factors from the analysis and i got p values upward of 0 5
1,multiple comparison find statistical significance from list of p values suppose i performed some experiments as part of one investigation and extracted one p value from each of them i now have a list of p values i would like to find the number of experiments in the list that are statistically significant where null hypothesis can be rejected if alpha 0 05 can i assume that 5 of the experiments will yield a false positive and then subtract this 5 from the number of experiments where p value is less than alpha or do i need to do bonferroni correction or holm sidak correction what if instead of the number of statistically significant experiments i need to know which experiments are statistically significant
1,standard approach for binning categorical variables together suppose you have a dataset where some of the predictor variables are categorical and have hundreds of possible discrete values are there any common ways to bin all these values into general groups e g suppose one of the variables is 50 a 25 b 20 c 0 5 d 0 5 e 0 5 f etc could you reformat this variable as a b c other is this a common technique is this acceptable some statistical computing software can not always handle so many categories other times perhaps it is advantageous to bin many low frequency entries together to facilitate statistical modelling thanks
0,farming automation what are people s thoughts on farming automation listened to the stuff you should know podcast where they talked about farming 4 0 and sounds really interesting
0,platform to create a data science blog as part of a few courses i ve been creating tutorials on the class forums which are hosted using using a platfrom called ed ed has the ability to write in latex natively using a built in wysiwyg editor or just writing out the latex code between dollar signs it doesn t seem to support markdown but i would be fine with markdown support i d like to begin transferring my content to my own site and was wondering if anyone has any recommendations so far i ve seen some excellent blogs hosted on ghost and using jekyll and am considering those two i m not interested in medium due to it s pay model and requirement that people either subscribe or use incognito mode to view it wordpress is another option but i ve used it before and it s just a little too bloated any thoughts anyone run their own site and can recommend any platform many thanks in advance
0,time series anomaly detection system hi all so i recently joined a firm with massive operations in the logistics delivery business we are currently building a system that can automatically flag anomalous events based on the time series nature of business kpis across multiple cities zones we are facing multiple roadblocks i feel completely stuck in a loop without any progress below are a few problems we re facing 1 prior to this system business never tried detecting anomalies manually so they don t know what an anomaly is this is my biggest concern as it makes the problem statement quite open ended also even after detecting an anomaly through time series models we don t have any mechanism to evaluate them 2 while building a time series model should we include the underlying variables as the predictors or just treat the target variable as a univariate variable and perform root cause analysis on the detected anomaly events please share any resources or case studies of a similar system that is implemented on a large scale i know this problem is very business specific but i assume the underlying techniques will be similar for such systems
1,is switching null and alternative hypothesis based on estimators valid we have a problem where an investor states that the average profit of certain economic sectors is greater than 1 so i think that we should test every sector with the following hypothesis set h 0 mu 1 h 1 mu 1 my teacher says that isn t right and we should get the point estimator average of every sector and then set the alternative hypothesis for that sector according to what the estimator suggests e g the estimators of 3 sectors are 2 1 0 5 1 1 so for sectors 1 and 3 h 0 mu 1 h 1 mu 1 for sector 2 h 0 mu 1 h 1 mu 1 why am i worried i feel like what the teacher is suggesting is like cheating as the null and alternative hypothesis are not symmetric but i m gonna take an exam on saturday he s gonna grade me and i lack the expertise to formally argue for my position i also don t know what to do should i dance to what he says or stick to my position even when i don t know how to argue for it
1,the rational view episode 50 statistics 101 with prof jeremy balka podcast episode it seems that in general humans are not logical we are poor reasoners and easily fooled misconceptions and misunderstandings of statistics are often at the root of society’s problems with communicating and understanding science in this episode i welcome a university curling buddy prof jeremy balka to discuss how statistics effects us every day we talk about statistics in science sports gambling and the mind blowing monte hall problem prof jeremy balka is a teaching focused associate professor in the department of the mathematics and statistics at the university of guelph he has a youtube channel on statistics with over 100k subscribers
1,equivalence between traditional state space models and recurrent neural networks can anyone recommend a source that explains the relationship between recurrent neural networks and state space models i often see recurrent neural networks being explained just in terms of neural network vocabulary can anyone recommend a website book video in which the hidden state aspect of recurrent neural networks are discussed thanks
0,do you think there is a stigma against us government employees trying to get data science jobs in the private sector i work for the government as a data scientist as my first and so far only post masters job i like my job but i don t like where i have to live to do my job at my specific branch of the government and there is no chance of this kind of work being done remotely from my preferred living location i may change my mind over time but i think maybe in two or three years when i have more on my resume i may go back to the private sector preferably a big and more stable company not a startup so i will be able to go back to where i m from which is nyc and there is a lot of tech and ds there so that shouldn t be a problem however i do hear a fair amount of chatter amongst ds people both inside and outside the government that there is somewhat of a stigma against government employees amongst big tech corps when looking to hire programmers ds primarily for two reasons 1 that gov employees get too used to working exactly 40 hours week less willing to put up with being pressured forced to do unpaid overtime frequently and 2 that gov handles ds and swe very differently than faang etc and that the skill set is too different i don t know about 2 but i think 1 maybe be true because i know a couple of people swe and ds mostly who left went private and came back because they discovered that they cared more about reasonable workloads and good pto than making 30 50 more money getting bonuses and honestly i may too but for me it s primarily the location i realize the answer is probably it really depends on the company and it really depends what part of the government but any general takes on this
1,is comparing the prevalence of something to the size of a sub population useful my apologies if this isn’t the right space to ask this question and that it is likely very basic i’ll start with an example first as i don’t have the exact language to clearly state my question example 30 of traffic stops were of black residents despite only representing 19 of the population question is this a useful comparison the comparison seems to say that because blacks represent 19 of a population and they represent 30 of the traffic stops the traffic stops should be lower than 30 so does that mean 19 would be acceptable less than 19 why does the percentage a sub population holds within the whole indicate whether something is acceptable or not what’s the relationship that i’m missing thanks for your help edit thanks all for your help with this and noted i’ll be more careful with my language next time
1,is there any relationship between this graph and this picture here is a picture of the famous bias variance tradeoff i am trying to understand what is responsible for the shapes of the orange curve and the blue curve i understand that this bias variance tradeoff often occurs in real life but i am trying to understand if it will always mathematically prove occur for instance if you look at the decomposition of the mse into bias and error where exactly is the tradeoff in this mathematical formula is there something which physically or probabilistically stops both the variance and the bias from moving towards 0 or is this just a general idea that has been empirically observed in this picture why have the bias and the error been depicted as curves instead of straight lines very squiggly lines etc are they always going to have this shape is there some mathematical logic behind this or is this just an artistic depiction of what is to be commonly expected also in this picture why do the orange and the blue curve intersect mathematically will they always intersect or again is this just an artistic depiction of what is to be commonly expected
1,how exactly is domain knowledge used when model building outside of well defined problems in engineering physics and chemistry we don’t have a functional form for things so how is domain knowledge used in more complex situation either in those fields or other fields if we have like 1000 genes and are trying to do some prediction or inference and given that some of these genes there may be little to nothing known about them how do you even add priors or regularization terms
1,logistic regression and fisher information matrix hello recently i ve been working on binary logistic regression and i came up to a problem i want to compute standard error of two estimators b1 and b2 manualy as far as i understand i have to compute fisher information matrix and do an inverse by definition square root of diagonal of the inverse should be what am i looking for i e standard error of estimators however when i try to make an inverse it seeems like my matrix is singular i don t know what am i doing wrong do you guys perhaps have somewhere a general form of fisher information matrix for logistic regression that would help a lot ty
1,degrees of freedom for nonlinear models this question is about the degrees of freedom complexity of a nonlinear model i know the degrees of freedom can be related to the expected difference between residual sums of squares and prediction sums of squares so it s possible to estimate the degrees of freedom for any model by splitting data up into training and testing groups but i want to verify the results of this simulation with something analytical does anyone know how to calculate the degrees of freedom or equivalently the expected difference between residual errors and prediction errors in linear models this quantity is easily obtained from the hat matrix but i m working with a model where there is no way to express predictions using a single projection specifically my model says predicted y sum f x i x i sum f x i so that the predicted y are a linear combination of x 1 x n but the combination weights are proportional to f x must sum to 1 and may be negative the function f x is possibly nonlinear and parameters are what we want to estimate
0,tools for error analysis are there any tools to make it easier to debug the model performance and evaluate the model errors for example i want to be able to able to dissect how my model is performing for certain classes etc
0,why when should i use object oriented programming i understand the basic concept of oop but i just don t see how implementing it would make things better defining a class to store information in an object that i would usually just store in a series of lists or a dataframe just doesn t seem like it would be an improvement so can anyone describe a situation where it would be a good idea to implement it
1,what is the variance of this if every minute i do x with probability 1 y then i m guessing that the expected period between doing x s would be y minutes is that correct how do i work out what the variance of this period is is it exponentially distributed and is the variance y 2 minutes edit this is geometric and there s a good video here about it thanks for the help
1,calculating probability with binary data i have a work problem that seems like it should be simple but i can t figure it out i m working with attribute data pass not pass and there was a change in the first set we had 3 of 14 samples not pass then we made a change and now in a separate test with different parts we had 0 of 11 samples not pass all passed x200b how do i calculate the probability that the issue has been solved wondering if the first test is irrelevant or how it might factor in any help is appreciated
0,securing machine learning model hey guys i have a scenario where a ml model needs to be deployed on client s server there exists the possibility that the model might be reverse engineered and therefore there is a need to protect it suggestions for achieving this thanks
2,pretrained models in jax flax gpt2 stylegan2 resnet etc i created a repository of pretrained models in flax that can be easily installed via pip github current models gpt2 stylegan2 resnet 18 34 50 101 152 vgg 16 19 i will also add more models in the future here are some notebooks to play with on colab gpt2 stylegan2 resnet vgg
0,what are some industry standard codex for data pipelining i recently moved from it governance audit in which we have codex like cobit 5 to follow i wonder if there is something like that for data pipelining
1,updating time series posterior distribution i have a distribution of probability densities by hour and minute of the day i want to be able to update the distribution based on new information the question that i am ultimately trying to answer is the estimated wait time at 5 00 pm is 30 min what is the new posterior distribution if 5 people had a wait time of 38 min if the density for exactly 5 00 pm is updated i would expect it to update the density for times in close proximity to 5 pm and decay as the duration increases the end goal would be to estimate the posterior distribution for each mapped location is a bayesian approach the right way to approach this problem if so could you point me to some resources that would allow me to research how to update the posterior using feedback
1,help understanding neurips 2013 dirichlet process mixture model paper i m struggling to understand certain parts of the 2013 neurips paper online learning of nonparametric mixture models via sequential variational approximation if anyone on this subreddit is familiar with bayesian nonparametrics and has five minutes i would really appreciate answers to the questions i posted at math stackexchange based on the paper s reviews reviewer 4 had these same questions and i think the author intended to add answers to the supplement the supplement starts with this document provides proofs of theorems presented in the paper but never actually got around to adding proofs or answers i ve emailed the sole author but i have yet to hear back from him
0,data scientist ancillary analyst i m curious to hear how many data science ai ml or other adjacent analysts find themselves working in tasks outside of what would generally be considered analytics i m at my second job now where the majority of my work comes from ticketing systems how common is this i can t tell if this is an issue that i keep walking into maybe it s how i m marketing myself or maybe it s normal in my previous role i was hired and worked on big data projects for around 5 years in an it department before slowly being shifted over into fielding support calls for confused users my current role senior in ds team is almost 100 ticket based work ex check that junior resource copied cells correctly between sheets it feels like i m being intentionally deskilled wherever i go and i m not sure if it s normal for reference i m an american with 7 years of experience and a bsc in cs and ms in ai
1,regression analysis i am writing a thesis on predicting tram faults by looking at past data i do not know statistics and i m having trouble finding accurate results i have data on which vehicle gave which malfunction in which month and year i cannot find what is dependent and independent variables should be in a multiple linear regression model i hope my question is clear enough
0,how much log loss is good enough basically the title i m working on a project using a random forest classifier and it performs pretty well on the classification task precision and recall however we will also use the probabilities of being 0 or 1 in some situations in order to assess the performance of the model in a context of probabilities i performed a log loss evaluation getting a value of around 0 5 how do i know whether this is a bad or good cost i know that the lower the better but how much lower does this value need to be i ve read that it depends on other parameters like proportion balance of 0 and 1 on the dataset in my case it s about 10 of 1 is there a proportion between the log loss and any other parameter that i can detect whether this is a good or bad performance for example an ideal ratio of log loss proportion of 1
1,interrupted time series analysis with 2 constant variable hello statisticians i am currently working on a study where i am analyzing time series data for 2 groups of people before and after covid using the date cali started lockdowns as the specific date i am wondering if anyone had a statistical test where i could compare these two groups change before and after covid i currently have them graphed on the same graph which hints that there may be a difference but do not know what test to use to definitively say i have tried to use an interrupted time series with the standard equation yt β0 β1t β2xt β3txt but am unsure how to manipulate it to add the two groups as constant variables any help would be amazing thank you
0,apart from kaggle where can i find data science challenges projects
2,magic cards classifier in streaming hi guys first post here and also first serious project with ml my master degree s thesis consists in a tool that detects and classifies magic the gathering cards during a tabletop streaming no online games mtg official tournaments have some streaming procedures to follow like a top view camera card positioning on the table a certain lighting etc that can help the machine a lot by avoiding harsh conditions main issue there are around 20 000 different cards and some of them have more than 1 image representing em alternative artworks or frames is classification with that number of classes even doable in a reasonable training time how to react i could accept training on a smaller subset like maybe the newest cards but i would like to create something useful and not just a demonstrative project so for now i m adopting a predictive pipeline to reduce the problem complexity and dispatch the classification to one of many networks trained on a specific different subsets of cards this should be the standard for face recognition so maybe it s the right answer with that every network should work only on 1000 classes with very few datapoints per class sadly and i don t know if it s the right path to follow do you think there s a better approach to this kind of problem p s macro features are recognized with a 99 accuracy so they re not a problem at all x200b
1,how to calculate sample size in example scenario i want to calculate how large my sample should be in the following example scenario there are 100 bathrooms being renovated according to the client they are all the same size so only one drawing is needed for all the bathrooms there will be n number of bathrooms 3d scanned in advance with a laser scanner for accurate dimensions from this n number the average of each dimension is taken to get the dimensions for the main construction drawing on these dimensions there is a tolerance of 10 mm 5 5 in which the bathrooms can always be renovated neatly how big must n be to be able to say that at the average of n with 5 5 mm tolerances 95 or 99 of the 100 bathrooms are within said tolerances
2,deepmind describes a possible solution to artificial general intelligence abstract in this article we hypothesise that intelligence and its associated abilities can be understood as subserving the maximisation of reward accordingly reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence including knowledge learning perception social intelligence language generalisation and imitation this is in contrast to the view that specialised problem formulations are needed for each ability based on other signals or objectives furthermore we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence full text
1,how did distribution tables come about hey guys i am confused as the two stat undergrad books i have read do not go into the history of the tables i mean how did w s gosset calculate the t distribution table also is there a geometric or algebraic interpretation of a distribution table please
2,any suggestions for regression task in deep learning background i am studying how to use video data to predict temporal regression value firstly i tried using some backbone with pretrained parameters here is a part of my code class mynet lstm nn module def init self model num classes super init self featuremodel nn sequential list model children 1 self linear nn sequential nn layernorm 2048 nn linear 2048 256 self rnn nn lstm 256 128 2 bidirectional true dropout 0 3 self classifier nn sequential nn layernorm 256 nn linear 256 num classes ​ for m in self modules if isinstance m nn linear n m in features y 1 0 np sqrt n m weight data uniform y y m bias data fill 0 ​ def forward self x b c t h w x shape x einops rearrange x b c t h w b t c h w b b t t x self featuremodel x squeeze b t 2048 x self linear x b t 256 x einops rearrange x b t d b t d b b t t x self rnn x x einops rearrange x b t d b t d b b t t x self classifier x x einops rearrange x b t d b t d b b t t return x ​ this is an example how to use my net model torch hub load facebookresearch wsl images resnext101 32x8d wsl mymodel mynet lstm model 2 frame for example torch rand 2 3 16 400 400 b c t h w the result are not good as i expected as shown in the picture below in the training time it worked so efficiently but then the performing became worse during the validating time after few days of debugging i found out that this is because no matter what the input data is my network will always show the output as “the last given ground truth” by the way i am using nn mseloss as my criterion x200b in the case of using ccc as my criterion the metrics for my tasks should commonly be ccc as well my results are shown below even keep training for more than 5 6 epoch the results are still the same question i would like to know why this strange result happened and is there any suggestion for me to train the regression network thanks joanna
2,what ensemble techniques should i use hey guys my project is based on ensemble learning i have used 4 pre trained models to obtain my predictions and they are stored in 4 separate csv files i am doing some research on different ensemble techniques but the ones that i have seen so far use raw data train and test models get predictions and apply ensemble methods altogether i already have my predictions so i am trying to look for ensemble methods that simply take in predictions and don t carry out the training and testing models bit p s i am quite new to coding and ml thanks in advance h
2,cornell ntt’s physical neural networks a “radical alternative for implementing deep neural networks” that enables arbitrary physical systems training a team from cornell university and ntt research proposes physical neural networks pnns a universal framework that leverages a backpropagation algorithm to train arbitrary real physical systems to execute deep neural networks here is a quick read cornell ntt’s physical neural networks a “radical alternative for implementing deep neural networks” that enables arbitrary physical systems training the paper deep physical neural networks enabled by a backpropagation algorithm for arbitrary physical systems is on arxiv
2,are ml phd programs toxic i ve heard grad schools can be really toxic but it also varies by field i am an incoming freshman in computer science and i am interested in studying ml in grad school after i graduate but i ve come across these two posts and it seems the ml phd programs can be really toxic x200b x200b x200b are ml phd programs really that toxic i ve seen many people advising me to stay away from grad school but at the same time there are a lot of people who claim that phd degree is a bare minimum for research scientists and one needs to have at least a masters degree to be a ml engineer for those people who pursued grad school in ml do you think it was worth it also was the environment toxic
2,is there accuracy drop when converting models from pytorch to tf through onnx i am curious if any accuracy drop happens when converting models to different framework using onnx what is it like from your experineces
1,best books for complete regression analysis with r that have every step for publication quality regression analysis regression assumption checking effect size estimation interpretation of results and everything else that is needed to write high quality article what book you suggest for regression analysis with r that every step for publication quality regression analysis by publication quality i mean everything that should be done for regression analysis for high quality peer reviewed article regression assumption checking effect size estimation interpretation of results etc
2,pose estimation can anyone provide me some good beginner resources for pose estimation i need some resources where i can learn from scratch thanks in advance
2,automated explainable feature generation for customer modelling analytics my company integrate ai is currently testing an alpha product free to participate and test it out which expands datasets to enable a new kind of customer insights analytics generation and ml modelling this is done through a combination of reusable ml pipelines and model ensemble methods to generate synthetic data from our network of datasets for almost all downstream tasks an example would be providing a propensity if a user is likely to shop at a discount store or determining if a user is likely to be the top 1 spender to be used as axis in cluster analysis segmentation or look alike modeling the current focus is on transactional data sets time series transactional data and joining them to your data sets through non pii indicators so if you have datasets that generally fit this shape there is a good chance the features we’re adding could help your use case the data augmentation is done in a privacy safe way to protect the underlying data in these features done through data anonymization and differentially private methods if you’re interested please dm me or drop any questions in the thread
0,what is the random state parameter for decisiontreeregressor i m a newbie who just started to learn data science via kaggle courses and the above said parameter was used when defining the model e g data model data model decisiontreeregressor random state 1 they mention that this was to get the same results every time but even after i changed values or just completely removed the parameter the same results were generated or at least that s what it seems like looking at the predictions head so what s the use of that parameter here why do we even use it when it brings no change also please give me any advice you d liek to as i ve only started learning like a week ago
1,advise for masters programme i m currently doing an undergraduate program in data science its mostly stats actually and realised i do enjoy learning what i learn in my programme and so i was considering doing a stats masters programme my dilemma however is that although i like what i study i m not an individual who excels at stats compared to some of my peers who pick up the concepts more comfortably i would say i m just average and struggle pretty often with certain concepts would it then be advisable for me to continue with a stats masters even though i will probably face a large amount of difficulty or should i consider something else instead thanks in advance
1,difference between ensemble models and stacking models i was reading the following tutorial over here i am trying to understand the difference between ensemble models and stacking models is my interpretation correct ensemble using the majority vote from different models to predict a new observation e g in the case of classification suppose we have a new observation random forest says this observation is class 1 svm says this observation is class 1 and k nearest neighbor says this observation is class 0 since the votes are 2 1 in favor of class 1 we say that this new observation is class 1 stacking i am a bit more confused about how stacking works as far as i understand it seems like you are taking the probability score produced by the first model and then treating this probability score produced by the first model as inputs for the second model e g suppose you are doing supervised binary classification you have 5 predictor variables you use a random forest as the first model and the random forest outputs a probability score that each observation belongs to class 1 or class 0 now you take these 2 variables the probability score and the original response variable and treat these as inputs for the 2nd model e g knn you can repeat this process for a third model is my understanding correct
1,do machine learning models handle multicollinearity better than traditional models e g linear regression when it comes to older and traditional models like linear regression ensuring that the variables did not have multicollinearity was very important multicollinearity greatly harms the prediction ability of a model however older and traditional models were meant to be used on smaller datasets with fewer rows and fewer colums compared to modern big data intuitively it is easier to identify and correct multicollinearity in smaller datasets e g variable transformations removing variables through stepwise selection etc in machine learning models with big data is multicollinearity as big a problem e g are models like randon forest known to sustain a strong performance in the presence of multicollinearity if so what makes random forest immune to multicollinearity are neural networks and deep neural networks abke to deal with multicollinearity if so what makes neural networks immune to multicollinearity thanks
0,time series for 125 distinct products suggestions i have north of 100 products with a request to build more accurate forecast for them for a 1 2 month turnaround what are folks thought process on the approach 1 go for each distinct product and determine best model and parameters or 2 find a higher level hierarchy to reduce cardinality from 100 plus to let’s say 10 12 models which is more manageable first time working on a ts project and would appreciate your thoughts ty
1,reputable crash courses to catch up to speed long story short i haven’t taken stats in about 10 years it’s sort of a blur i know of the terminology but can use a refresher i’m a cs master’s student but taking data science electives i’d just like to sort of catch up on standard deviation variances confidence intervals that stuff i know it’s not easy but i’m mostly looking to see if anything online could help me catch up do some examples and not be as lost on my data viz course as i currently am tried linkedin learning but found it very meh but any input would be appreciated if possible
2,dataprep v0 3 has been released dataprep is the easiest way to prepare data for ml in python we’re an open source data preparation library and we just dropped a very big very exciting release all 3 of dataprep’s current components have received some notable updates to make dataprep closer than ever to a full fledged off the shelf data preparation solution here’s a rundown of some changes eda added plot diff compare dataframes plot now supports geographical heatmaps customize plots by changing display parameters clean added clean duplication creates an interface for users to select how to cluster detected duplicate values added clean address clean us addresses added clean headers clean column headers added clean date clean dates added clean df clean a dataframe connector added support for more apis we now support over 30 apis on connector the connector team is currently working on expanding connector with the addition of connectorx these updates allow users to use dataprep for all their data preparation needs we are very excited to be taking this step forward and becoming closer to an end to end data preparation tool x200b release blog github pypi
2,cloud instances vs owning physical hardware for deep rl training i want to train a bipedal robot to walk using a deep rl controller what sort of hardware resources would you need to run this training in hours not days options like the nvidia dgx station a100 cost upwards of 150k but are as close to a data center in your office as you can get how much does this sort of system speed things up amazon has its gpu cloud instance on similar hardware but if you are iterating often does renting end up costing more than just buying hardware is there a general benchmark performance that you need to be able to do rl using sensors like lidar cameras efficiently if so what hardware fits this category
1,solve collinearity in a fixed effects model using multi level regression as well dear reader first and foremost i feel obliged to tell straight that i m not a statistician or econometrician when trying to look for solutions i often find it hard to put into words exactly what i am looking for i am not looking for someone to solve my homework i just want to understand this concept ill try to keep it as clean as possible and hope that someone can confirm what i want to do is possible or correct me if there is an easier more applicable obvious solution i have panel data from 30 different stores for 90 days time the dependent variable is the amount of a certain good sold per store location per day i am currently using a fixed effects model to differentiate between store specific differences x200b in my fixed effects model i use 3 explanatory variables the indicator variable day of the week different weekdays have different amounts of traffic i dayofweek dummy var for whether or not there is a sale going on in that particular store that day the sale is always on a friday but not active for every store location at once discountday anticipation variable object of my study i want to know if customers buy less on days prior to the sale if they know that there will be a sale later this takes on the value 4 to 4 where 1 is on the thursday prior to the sale 2 is on the wednesday prior to the sale etc this is also an identifier variable i anticipation for all observations not within four days of a discount day this value returns blank this value is not the same at a date for each station since different stations have discounts on different weeks this leads me to the following fixed effects regression the daily sales amount for each station regressed against the the dayoftheweek the discountday dummy and i anticipation as can be expected there is a lot of collinearity here the discountday always coincides with a friday and each value of i anticipation can only coincide with one particular day of the week still i need all this information for my model i have no idea how to solve this here is what i am currently thinking i am looking to make a fixed effects model that first applies the day of the week to the aforementioned system so i find regression coefficients for each day of the week under no discounts and no anticipation effects i want to lock those values and then apply the anticipation values and the dummy is this possible using a multi level model i am not familiar with this is there another solution that is more suited for this problem i am not that good at econometrics so i prefer to keep it as simple as possible all i need to find is a good estimate of the anticipation effects per day i use stata so if you can share a stata function that is used in your solution then i would also appreciate that a lot whatever the case any help is much appreciated i am immensely helpful for anyone willing to put time into this and will always take the time to reply to you to thank you for your help x200b bhta
0,horn’s parallel analysis in python am i doing it correctly hi all been learning factor analysis for the first time using datasets from kaggle i’ve been using factor analysis to break down the dimensionality of the datasets and want to justify the number of factors to keep with parallel analysis other than kaiser criterion and scree plot there’s literally nothing i can find on parallel analysis pa in python so i read a paper called ‘parallel analysis a method for determining significant principal components’ it suggests generating a random matrix with the same number of variables and samples after standardising my dataset i randomly generated normally distributed numbers with mean 0 and dev 1 for my random matrix hoping to extract the eigenvalues of the random matrix and perform parallel analysis my end scree plot result of the synthetic data was very lackluster almost a horizontal line with eigenvalues all close to 1 basically i would be doing a glorified kaiser criterion comparison so have i done something wrong are there any resources on pa in python
2,hugging face summarization adding wrong information hi guys i am using the summarization from hugging face in some of my summarizations i have realized that some information is added by them model without being present in the input is there any of you experiencing that issue if yes how did you resolved it thank you for your help and experience sharing
0,should i get a new job background info i was a stats major in undergrad and now i m getting my masters in cs from omscs georgia tech i ve been working at a large fortune 500 company for a year now as a data analyst data scientist my job consists mostly of building tables and metrics in sql and i recently started doing some python work involving solving the vehicle routing problem so i ve been introduced to a lot of machine learning models and libraries i m also familiar with cloud computing and use cloud technology in my day to day the problem is that this team has absolutely no guidance no one checks on my work and no one cares about my work it s really assigned to me as a learning opportunity the team doesn t really need a data scientist at all also i m the only data scientist on the team everyone else is a software developer i ve learned a ton but the lack of guidance really really frustrates me and i think it puts me at a big disadvantage when it comes to learning my question is given my experience will i even be able to find another data science job or should i just stay at my current job
0,agent based modeling vs statistical learning approaches hello i’m currently a sophomore at my university whose in a undergraduate data science club we had a speaker come talk about the use of “agent based” models network models feedback models spatial models etc which the way he described it as was simulation based approaches as a statistics major and only being familiar with the statistical learning approach to modeling this was very different from the usual tree based models clustering models that i’m used to hearing about can anyone go into a bit more depth of what those types of models are where are they used are agent based models part of reinforcement learning
1,interrater agreement icc 2 1 on a set of more or less correlated items hi i m trying to test whether my subject participants the raters rate the personality of a fictional character similarly think of a personality profile on a variety of items the cases i am using an icc 2 1 absolute agreement as i would like to generalize to other raters from the same population i e the general population in my analysis i noticed that the icc always decreased wenn less items are rated lets say my 50 raters first rated the character on how important 10 social values are to them the icc was quite good but when the 50 raters rated the assumed health of the character on 2 items mental health and physica health the icc was close to zero in the latter case both variables were highly correlated which might habe to do with it so my questions are 1 is it correct that a lower number of cases or a too low number is detrimental to the icc 2 is it correct that a high intercorrelation between items cases has negative effects on the icc 3 if that is the case would it make sense to create mean scores of highly correlated variables and then use an icc 2 k though in my example this would not work as it would leave me with a single item case which the icc does not compute for thanks for your help
2,why are we using black box models in ai when we don’t need to a lesson from an explainable ai competition very interesting insightful article why are we using black box models in ai when we don’t need to a lesson from an explainable ai competition by cynthia rudin and joanna radin in 2018 a landmark challenge in artificial intelligence ai took place namely the explainable machine learning challenge the goal of the competition was to create a complicated black box model for the dataset and explain how it worked one team did not follow the rules instead of sending in a black box they created a model that was fully interpretable this leads to the question of whether the real world of machine learning is similar to the explainable machine learning challenge where black box models are used even when they are not needed we discuss this team’s thought processes during the competition and their implications which reach far beyond the competition itself link to paper
1,is there a concept of power analysis for bayesian a b tests if not how do i know when to stop my test
1,counterfactual vs vanilla contrasts i still don’t quite get what the difference between counterfactual outcomes and just regression contrasts is with g computation it just seems like its the exact same thing rebranded so what is so special about it and what is the hype we never did causal inference in my program so i am trying to connect it back to what i do know about glms and contrasts so a regular contrast estimates e y t 1 x e y t 0 x a counterfactual contrast is e y 1 e y 0 or something like that where the indicates here its a potential outcome under getting that t but in practice with g computation what i see is happening is various assumptions are being made and people use the vanilla contrast to do it for glms it may just be on the response scale rather than link scale but couldn’t this be done via delta method and vanilla contrast too all this terminology in causal inference and especially the dags i feel really obscures the main point is the only difference that you impose additional assumptions such as exchangeability consistency positivity sutva and then interpret the vanilla contrast on observational data as a causal effect
2,nlp feedback system for eg if we are provided with an nlp sentiment analysis model that has already been trained like if we provide this model with some word like “excellent” it knows that this particular word is to be classified as positive now if we provide this model with a new set of words that might contain new as well as words that the model already knows but with some different labels for eg the excellent word which the model knows to be positive is labelled as negative in this new set then can the model adapt and change is there any work done similar to this or can someone explain to me a way to achieve this
0,how likely is it to work a fully remote job in data science
2,is there a mathematical model of the mind join us for a google workshop this mon may 17th is there a mathematical model of the mind please join us for a virtual google workshop on “ conceptual understanding of deep learning when may 17th 9am 4pm pst where live over youtube goal how does the brain mind perhaps even an artificial one work at an algorithmic level while deep learning has produced tremendous technological strides in recent decades there is an unsettling feeling of a lack of “conceptual” understanding of why it works and to what extent it will work in the current form the goal of the workshop is to bring together theorists and practitioners to develop an understanding of the right algorithmic view of deep learning characterizing the class of functions that can be learned coming up with the right learning architecture that may provably learn multiple functions concepts and remember them over time as humans do theoretical understanding of language logic rl meta learning and lifelong learning the speakers and panelists include turing award winners geoffrey hinton leslie valiant and godel prize winner christos papadimitriou and experts from diverse backgrounds including ml ai algorithms theory and neuroscience full details panel discussion there will also be a panel discussion on the fundamental question of “ is there a mathematical model for the mind ” we will explore basic questions such as “is there a provable algorithm that captures the essential capabilities of the mind ” “how do we remember complex phenomena ” “how is a knowledge graph created automatically ” “how do we learn new concepts function and action hierarchies over time ” and “why do human decisions seem so interpretable ” twitter conceptualdlworkshop please retweet hope to see you there rina panigrahy
1,randomized tournament style survey my team is looking to create a survey that identifies people s preferences amongst a set of words by making direct comparisons between two options for example let s pretend that we were asking people to determine their favorite ice cream flavor out of chocolate vanilla rocky road strawberry and mint they would be presented with a randomly generated choice between two of the options chocolate or strawberry and asked to select their preference chocolate being the clear winner here in my heavily biased opinion at this point either both options are replaced or the loser is replaced chocolate or vanilla this process continues for either a set or arbitrary number of times two questions is there a name for this style of survey i have seen them and participated in them before but cannot seem to be able to find it are there a particular set of techniques utilized to analyze the output of these surveys are there tools that people have already built to administer this type of survey what are they we ve discussed building a quick r shiny app to do this for us but i d really prefer to utilize a tool that someone else has already built please let me know if there is a better place to post this
2,google researchers merge pretrained teacher lms into a single multilingual student lm via knowledge distillation a google research team proposes mergedistill a framework for merging pretrained teacher lms from multiple monolingual multilingual lms into a single multilingual task agnostic student lm to leverage the capabilities of the powerful language specific lms while still being multilingual and enabling positive language transfer here is a quick read google researchers merge pretrained teacher lms into a single multilingual student lm via knowledge distillation the paper mergedistill merging pre trained language models using distillation is on arxiv
1,excel forecast function help please so i m taking a statistics class and i need to do a forecast for specific values basically i have covid cases covid deaths day by day in a month 30 days and i have to use the forecast function to predict how much deaths would occur given a specific case number on the 31st day how would i go about doing that i tried using the linear forecast function but the numbers that came back were pretty weird can someone tell me how to do this i have x values y values 30 days in the month 1 day to predict given a specific case number
0,linux for data science hi some data science job ads mostly of data engineering type want at least basic knowledge of linux or user knowledge of linux a while ago i read a post somewhere on reddit describing what that could be but now i can t find it looking at the most recommended linux tutorial on reddit will i be fine after completing the grasshopper level or is it more like that i should finish the whole course and then i have basic knowledge since i have 0 experience thanks for everyone s opinion and personal experience
0,so you trained a model now what the courses at university teach me how to understand and build a model however we do not learn what to do with the model once it s done like how to put it into production for a company i would like to understand this aspect a bit more as i understand it simple models can be saved and stored on a cloud server and accessed through api by the end application to make predictions based on new data is this realistic how do you deploy models in your work environment
1,how do i create this kind of scatter plots graphics in rstudio hello as the title says i m looking to create data visual representation that looks like this i m comparing the ti corruption perception index which dataset i found here with the three subindizes of the human development index that i found here the subindizes that i want to measure for are the life expectancy education and gni indexes both datasets are in excel i already could open them in rstudio but when trying to create a scatter plot i get multiple errors it is less to say that i am still new at using r ideally i would have three different scatter plots where on the x axis i have the ti corruption perception index on all of them and on the y axis the different indicators accordingly namely life expectancy education and gni any help or tips are very appreciated thanks
2,why does functional causal model fail to identify causal sturcture in linear gaussian case hi there recently i studied causal discovery with fcm functional causal model in review of causal discovery methods based on graphical models sec 4 it says ”it turned out that the former case is one of the atypical situations where the causal asymmetry does not leave a footprint in the observed data or their joint distribution“ i wonder if there are any proof about it thanks a lot
2,are machine learning models theoretically designed to make predictions about individuals are statistical models in theory able to make predictions about individuals suppose you have an individual with observed covariate information x a y b z c in theory can a regression model trained well on some data predict the expected value of this individual s response variable i heard today that statistical models are not designed to make predictions about individuals they are only designed to predict the average behavior of a large group of individuals and in theory should not be used to make predictions about individuals is this correct does this mean that any time statistical models are used to make individual predictions this is going against the intended use of statistical models if i understand correctly this means that when a statistical model makes a prediction about an individual with observed covariate information x a y b z c it s making a prediction for the behavior of all individuals in the universe with observed covariate information x a y b z c is this correct does this mean by definition the idea of making predictions for individuals is a fallacy thanks
0,biquery query editor alternative to console web interface using the query editor in the web bigquery console is a big pain it s too small doesn t really have a lot of functionality anyone have any ideas on a user friendly ide one could connect to bigquery directly to manage views tables scheduled queries bonus if you can create views and such directly from this ide instead of doing it from the console i m doing all analytics in datastudio so just need a simple way to browse and write out sql
0,is linear programming part of ds at a first glance lp seems promising but honestly the techniques seem really dated there must be modern tools vs using a simplex method to optimize appreciate any insight and experiences shared thanks
0,very interesting data scientist job vs highly paid data analyst job i m a software engineer that is trying to go into data science i received two job offers one is for a data scientist position doing work that i find very interesting such as working with cloud technologies deploying ml models and working closely with data engineers on the data pipelines the other offer is for a data scientist job that pays a lot more than the other company s salary but where the job responsibilities align more with being a data analyst i m more interested in the software engineering side of data science so i m wondering if it would hurt to take the higher paying offer since that company is well known i think i could still take that job and always internally transfer into a data or ml engineering role in the future
2,google unit deepmind tried—and failed—to win ai autonomy from parent london—senior managers at google artificial intelligence unit deepmind have been negotiating for years with the parent company for more autonomy seeking an independent legal structure for the sensitive research they do deepmind told staff late last month that google called off those talks according to people familiar with the matter the end of the long running negotiations which hasn’t previously been reported is the latest example of how google and other tech giants are trying to strengthen their control over the study and advancement of artificial intelligence full text
0,completely and totally burnt out at work unsure if i should quit or just go rouge and do my own core data science based projects edit disclaimer not stress burnout more like feels like a total dead end role now can t go on much longer like peter gibbons from office space that s how i feel ok context in a nutshell been at this tech company for about 4yrs in a mixture of roles from product to market research but been working as a data analyst for the last 2yrs nearly 2 5 my goal had been and agreed with hr to progress towards a data scientist role when i originally signed up to it but this has not materialised fast forward 2 5yrs i ve had absolutely nobody in the business to learn from never had everything has been self taught my day to day work has been much more bi related dashboards product stats etc as opposed to core data science tasks i really have to go out of my way if i want to write and code even sql data is all maintained by gatekeepers mostly inefficient ones so i have to request it mostly then it s just an out the box plug in to powerbi tableau or similar i ve absolutely maxed out my patience with this company and their talk the talk grand plans my technical skills have totally stagnated and if i was to compare it to when i left uni my skills regressed in areas such as advanced mathematical statistical techniques my day to day tasks typically involve adhoc requests solved with tableau with absolutely horrendous data quality there are some back end things involving sql mongo aws but these are probably 10 overall if even that when it comes to addressing the data i m more like a project manager because i m not allowed to fix or implement things myself so i end up making requests for things and gathering requirements like a po ba projman hybrid role but even that is a waste of space e g simple request for consumer product usage dead in the water for over a year despite weekly meetings because the development team won t implement changes needed to track them new pos can be seen mentally questioning life choices when the answer to so where can i see the usage stats is they don t exist the only thing it has that appeals is a degree of autonomy with what i do once i finish the adhoc requests i easily have 50 of my working hours to address anything i like in my more motivated enthusiastic days this was spent addressing the data quality issues data warehouse etc but the way these ideas and suggestions were valued met just killed my soul options and recommendations closed due to time cost effort etc i m thinking of just self teaching the core data science content i wanted to initially learn i m already doing this just not applying it to anything at work and finding things to apply this to at work the problem is i m all alone nobody in this company has a data science background so i can t learn from anyone here i ll have no idea if what i am doing is good practice or even straight up wrong even if i did the data is such bad quality and held in rigid closed off systems i am always refused direct access to it that i m not convinced there s a lot of real insight to get from it given that i have no real world data science experience i ll be competing for entry roles in a competitive job market so i m really not sure if i should stick at this place and do my own thing on the job flying blind probably with low output or just quit spend summer selfteaching and practicing on projects i actually care about personally while applying for jobs have some master s degree offers in september if shit hits fan
2,is one rtx 3090 enough for computer vision tasks hi i m a phd student in an eastern european university my department currently is considering building a deep learning workstation to do research unfortunately our budget is limited and due to the high gpu prices we can only afford one computer we also do not consider professional gpus due to the budget limitations we are making a decision now whether to install one or two rtx 3090 gpus i understand that the more vram the workstation has the better but i also think that adding two heavy hot and power hungry gpus will introduce engineering difficulties such as cooling and ventilation issues power supply issues more expensive ups setups etc it is also more expensive of course i am going to work with latest state of the art object detection models yolo variants retinanet variants etc will one rtx 3090 with 24 gb vram will be enough to comfortably train these models if yes how future proof one rtx 3090 is is it worth to pay more and face technical difficulties described above to obtain more vram and how about other areas of computer research that my colleagues might work at like image classification segmentation etc i don t have enough experience to answer these questions so advice is greatly appreciated
2,dataset for directed graphs classification i want to evaluate some algorithms for the directed graph classification task therefore i m looking for directed graphs data sets preferably without node or edge features as benchmarks do you have any ideas for datasets that could fit
1,possible u shaped distribution i am looking for some u shaped distribution in higher dimension for single dimension beta distribution with alpha beta 1 is a reasonable choice is there some known distribution in higher which can act like beta with alpha beta 1 i am basically looking for some thing like a u shaped quadratic function but which is rather almost flat in most of the space but increases rapidly as it approaches the boundary ps i am not looking for dirichlet which increases rapidly only towards the corners thanks in advance
0,conferences for 2021 disney conference with things starting to reopen my company is restarting training and conference travelling i was taking a look at the disney conference on data and analytics i know that most people find the disney conferences very useful and i was wondering if anyone in ds had attended this conference and if so what they thought of it fyi not affiliated in with disney in any manner just a canadian looking for a winter trip to somewhere warm or are there any good ones looking to be in person in the fall i know a lot of this may be still be subject to cancellations depending on what happens again in the fall
1,how to continue making progress in statistics and r after getting the first job first of all thank you few months back i made a post where many of you suggested learning r eversince that i started learning it and a month back got my first job as business analyst can t even begin to say how right y all were and how awesome this language is anyways my today s post is about how to keep on progressing on this domain i want to continue making progress both in statistics and r my problem is i am spending too much time looking at courses and resources and wondering what to do i don t even know how to classify myself as a r programmer whether i am novice or intermediate this all feels overwhelming my plan so far 1 to do tidy tuesdays kaggle notebooks on weekends 2 finish the statistical learning with r book yes i haven t finished it yet 3 learn shiny it is needed for my work too any help would mean a lot
2,simple fast gan training i m working on a theoretical project right now which i believe has interesting applications for generative modeling i ve successfully tested the modification suggested by my theory using the implementation of wgan gp here on mnist data but i m having trouble to get any gan working on a more complex dataset i m a theorist architecture choices and hyperparameter tweaking scare me does anyone have any pointers to a gan implementation wgan ish preferred for theoretical reasons but not necessary which is relatively easy to train on a more interesting dataset like some low res faces or cats for some reason completely unrelated to the neurips supplement deadline i d prefer something which can train in less than a day though faster is even better
1,mlr categorical interaction term question i am trying to understand when to add interaction terms in a mlr model i reduced my model using pairwise selection and was left with 8 categorical variables to predict monthlycharge using a telecommunication churn data set the residual vs fitted normal q q and residuals leverage plots do not appear to support that the model fits the data well i added an interaction term between all the variables v1 v2 v8 and the residual vs fitted normal q q and residuals vs leverage plots look great the predicted vs actual monthlycharge is also nearly perfect i m having trouble understanding how to interpret this term though i originally added this term just to experiment with possible iv inputs not expecting such positive results is there a systematic way to calculate which terms should be added by additional interaction terms between categorical variables when i examine the model summary in r it looks like this interaction term uses every possible combination with 2 or more of the variables not just all of them interaction together i assume there must be something wrong with adding an interaction term for all variables just looking to understand why
1,anova or kendall tau b correlation when reporting on datasets i ve done a survey where i ve divided participants in age groups and measured their loneliness scores when analysing the data the anova comes out insignificant however if i do a kendall tau b correlation between age and score it is significant therefore i m not sure which one to report and rely on what to do
2,a model theoretic view of storytelling and what it means for creative nlg evaluation hi everyone i’m a storytelling researcher based out of georgia tech some of you probably can already guess my advisor lmao and i’ve been working away at a formal theory of narratives from the perspective of category theory and model theory for the last bit over the last six months or so i’ve written a few papers on objective measures of story coherence with grounding in information and model theory that might be of interest to people here who work tangential to nlu nlg and evaluation the project is still in its infancy we’re only three papers in so far and we make assumptions like the narrator being a perfect realization of the author’s intent as well as transmission of information between the narrator and reader being lossless but those will soon be remedied i’ve shared prior approaches like this one here before mostly based around tda methods but model theory provides a much more elegant way to discuss nlu nlg evaluation methods the introduction post is here arxiv links it links to two of the three papers the third is still in review but it should be up soon the third is the evaluation methods applied to language models rather than artificially constructed datasets willing to answer questions in the comments edit i had a lot of fun answering questions if you want to stay updated i’ll be posting updates and memes my meme game is strong on my twitter
1,three way interaction model with more terms has lower aic than two way interaction model i made an linear mixed effect model in r the dependent variable is salary and there are three main features that i am looking at gender promotion level and team there are 2 genders 9 different promotion levels and 8 different teams a two way interaction effect model with the formula salary promotion gender gender team has an aic of 115 000 while the three way interaction model of salary promotion gender team has an aic of 52 000 unfortunately the amount of terms increases from 32 to 129 in the three way interaction model only a handful of terms have a t value below 1 96 comparing the two models with each other the model with the lower aic would indicate that it is a less complex model but with 129 covariates it doesn t seem that way
2,discussion single word recommendation with nlp i have the following nlp problem imagine short snippet of text no longer than 30 40 words an advertisement text to be precise there are certain important words e g sale 40 off etc which translate to either sales value directly or promo link click through rate ctr either way the resulting metric can be measured and i have the dataset with measurements the idea is to recommend better words i e create an nlp model to suggest better words to increase the metric the single word model would be enough so the model would take the planned input and suggest changing single word or a few words separately to increase the metric does this make sense is this possible what models neural network architectures could be useful pointing me in the right direction keywords would be greatly appreciated
1,understanding the relevance and the importance of the representor theorem i have often seen the representor theorem mentioned in machine learning but i have never been able to fully understand the application and relevance of this theorem in machine learning can someone please try to explain this in a simpler way how is it different from mercer s theorem thanks
1,urgent can someone please review my statistical methodology i just realized that i might have made a big mistake in applying a certain type of statistical model and desperately trying to figure out a way to fix this mistake or determine if i made this mistake can someone please review the following i am working with this algorithm called survival random forest this algorithm allows you to use the random forest algorithm in the survival analysis context the way i understand it 1 a decision tree in the context of survival analysis finds optimal partitions i e terminal nodes within your data then a separate kaplan meier curve i e survival function is generated for all data points in each terminal node so at this stage if you have a new data point the decision tree will decide which terminal node the new data point belongs to the new point will be sent to that terminal node and is said to have the same survival function as all other data points in that terminal node 2 next many decision trees are made survival random forest performs bootstrap aggregation bagging using all these trees 3 once you have a new data point each tree within the survival random forest estimates a survival function again kaplan meier for this new data point then all the survival functions for that new point are averaged and a single survival function for that new data point is produced goal as a result if you have 5 new points the survival random forest can produce 5 different estimated survival functions you can then proceed to compare these 5 different estimated survival functions to obtain some desired information e g what is the median survival time for each of these 5 new observations which of these 5 new observations will take the least time to reach a survival probability of 0 5 you can also use the c index to quantify the performance and accuracy of the survival random forest that you create can someone please tell me if my understanding and goal are logical mathematically valid and statistically make sense at this point i am only interested if the methodology makes sense and my understanding of the algorithm process is correct hoping for the best wish me luck much appreciated thanks
0,an end to end analytics process what tools to consider in my efforts to move towards a career in data science i am being fortunately afforded an opportunity to propose a set of deliverables for a 3 month data science poc for a customer requirements are define metrics what data points we want to grab develop data pipelines to collect data real time sourced from the internet static data pre loaded store data in a cloud data store aws google azure snowflake analyze data and present to customer on front end tableau dashboards i am thinking of the following tools data sources depending size either api calls to social media news data or pyspark streaming automated running 3 4 times a day data cloud store aggregates data sourced from real time sources with static sources s3 buckets or something similar scikit learn pipelines automated modelling and analysis tableau receives output of scikit learn models grabs data directly from cloud data stores for interactive visualizations there are probably many things i am overlooking but this is my rough solution at this time it will be delegated between 3 guys but i just completed my ms of ds so i hope this will be a good hands on oppty for me to do as much as i need to i don t think i need a serious data engineering stack kafka hadoop airflow etc because i don t believe that aspect is important at this phase it s more to demonstrate feasibility of modern data stack to solving analytical problems please critique gently thank you
1,how would you visualise a patient s psychiatric information sleep amount sleep start and end times medication periods evolving medication effects underlying symptoms and their progression key events etc over many years parallel information about other medical conditions too i rely heavily on notes and struggle to keep doctors informed i d also like to maintain my own records more clearly i have a huge amount of disorganised information i d like to be comprehensive with it but also allow a doctor to pickup key trends quickly any suggestions would be appreciated
1,statistics discord server hey r statistics i didn t plan on ever posting this again because it would be spam but unfortunately i tried editing the server invite to link to another channel on my last post and automod deleted it because at the time of making the post there wasn t a requirement requiring tags in the title so here s the invite if you re interested we re open to all levels and have a friendly community we d love to have you around
1,life expectancy and mortality statistics has anyone ever seen a graph that shows the hazard of dying for the average adult throughout their life on this graph the horizontal axis would be years and the vertical axis would be hazard of dying and this graph would show that when a baby is born they face a higher hazard of dying then the hazard decreases but slowly increase as the adult enters old age i tried looking on google images for a study graph in which this trend is shown but i was not able to come across this kind of graph also has anyone ever studied hazard functions and cumulative hazard functions in the domain of survival analysis before is the following statement correct a hazard function is free to increase and decrease but the cumulative hazard function as the name suggests can only increase thanks
0,can t land a data internship try volunteering for a political campaign s data team i ve seen a few posts about how to find volunteer opportunities or get experience before you are able to land a full time job one avenue i ve used to get experience was volunteering for a political campaign s data team campaigns are always looking for extra help and will usually be happy to assign you some easy data cleaning or analysis tasks that you can use to hone your skills to get started i reached out to the data tech director for a mid size pac after finding them on linkedin and asked if they had any data volunteering opportunities if you can t find this person reach out to anyone in the campaign and ask if they know who to talk to within a few days they had me sign an nda and i was working on getting insights out of their textbanking data figuring out which messaging was working best weeding out phone numbers that volunteers should have added to the opt out list but didn t etc this can be a great way to build a few industry connections learn some skills about working within real data infrastructure and have a killer resume bullet point
1,temporal network graphs has anyone ever worked with temporal network graphs before basically these seem to be graphs in which a new nodes can appear and old nodes can disappear as time progresses b edges between nodes can appear disappear as time progresses these seem to be newer methodologies has anyone tried to perform algorithms like community detection node classification or edge prediction on temporal graphs i was trying to find more examples of this kind of stuff and all i was able to find was the twitter github page and this one general blog on temporal graphs with r and can anyone please recommend anything else thanks
0,senior data scientist vs senior data science consultant senior data science consultant is my new title i would like to use senior data scientist on my cv my actual duties are hands on work for long term projects rather than just presentations and consulting do you think it is acceptable to have senior data scientist as a title on cv linkedin
0,has anyone worked with structural equation modeling or statistical process control i recently came across these topics they look very interesting but also very complicated has anyone ever dealt with them before what kind of projects did you use them for
2,predictions in survival models with time varying covariates i m having trouble with making predictions for survival models with time varying covariates when using the r flexsurvreg function i can fit parametric models weibull lognormal exponential etc to my data which is in the long format in order to incorporate time varying covariates i have intervals with start and stop times every time a time dependent covariate changes value when training my model with this fit there s no problem because i think it is able to know the id of every row of data what i mean is for example if a certain observation has 3 rows with intervals from 0 1 1 2 2 3 it knows that they refer to a single observation however when i try to use the predict function in order to predict the time to event of a new observation which has n intervals of covariates values it gives me n predictions instead of one prediction therefore it assumes that each of the n rows is a new case instead of assuming that it is only a single case with multiple rows how can i make a prediction for an observation with multiple rows of time dependent covariates tldr having trained a parametric survival model with time varying covariates how can i make a prediction of time to event of new data with time varying covariates in r using the predict function
1,major brain fart i can t remember how to do this working on a personal research project if i have a population with a standard deviation of x how would i find the average standard deviation of a random sample of 15 pulled from that population i tried googling it couldn t find what i was looking for
0,what are some methods to build google trend like indexes i have my own corpus of text indexed by date time i wish to generate google trend like index based on this corpus maybe for simplicity i just want to avoid dealing with more complex issues like synonyms etc are there some good references on building these indexes thanks
2,inference server alternatives hi there in the startup i am working we are exploring which might be viable alternatives to serve our ml models our product is mostly a ai service queried via mobile app we are looking for a solution which is scalable and maintainable for when requests will grow in number up to now we have opted for nvidia triton but i am asking for suggestions and or your feedback on possible better alternatives
2,oversampling for multivariate time series currently working on oversampling for time series and have looked into methods like spo structure preserving oversampling and ohit but both of them are univariate working only for time series with one feature are there any oversampling techniques for multivariate time series i am unable to find any also please suggest some papers that i can look into
1,inferring cv or even just a meaningful analysis of an unknown variable by comparing component cvs hi i am working on a project where i am attempting to figure out some qc criteria for an end product where in the process i can measure 3 out of 4 variables during manufacture the variable i am attempting to analyze is the most crucial for final consistency but it is only 2 3 of the total mass so even with a low variance in the other components it s actual consistency isn t that apparent from the final measurements i only took a single statistics course in college and honestly don t remember much so if anyone has insight into how to combine component cvs final result to infer analyze an unknown component it would be much appreciated thanks
1,clt what s the point of being able to do means comparaison tests when the original distributions aren t normal if i know the mean of a normal distribution i have a pretty good idea of what the distribution is and looks like if i know that 2 normal distributions have the same mean and the same variance which afaik is an assumption of mean tests i m pretty sure they are close to identical and so using t tests and other mean tests if i compare 2 sample means from normal distributions and conclude from my test that those means are the same i know that they belong to the same population distribution right but how does it make any sense of drawing conclusion from knowing that 2 sample means have the same population mean if the distribution they belong to arent normal those two distribution can have the same mean or not but could be very differents from each other therefore it s not like i can infer they belong to the same distribution based on their mean since they arent normal what am i missing here thanks for the help edit adding my thinking process i said in a comment i mention the clt because the clt apparently implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions and tells us that if a population distribution is non normal its sampling distribution of the sample mean will be normal for a large number of samples which makes us able to compare means of 2 distributions and do things like t tests for distribution with any shape now maybe it might make sense to compare the means of distributions that aren t normal but then my question is why what conclusions do you draw from knowing that the means of those 2 distribution are equal or not when the distribution aren t normal and i mean that practically in real world phenomenon if i have 2 conditions and i m trying to figure out if their distributions comes from the same population distribution knowing that they come from the same population have the same distribution it means to me they are the result of the same process if my observations are the product of 2 differents distribution i know it is not the same process at plays that generates those data when i uses t tests and i know that the mean and variance are equal or equal parameters for any other distribution i know my 2 means come from the same distribution null hypothesis and therefore have the same distribution come from the same process but it only works if the distribution are normal or test on parameters where i figure out that all parameters of distribution are equals that s afaik what s being done in science all the time using a t test and knowing that all my distributions are normal with same variance and that the mean are not different from each other i know they come from the same dsitribution same laws generate the data that makes sense to me but now when the clt claim that we can use those mean test for any distribution i wonder what s the point of knowing that the means are the same since the distribution tehy come from are likely not the same even with the same mean u cant draw the same conclusion as above so why would you do that what conclusion can you make from it
0,‘primer’ for leetcode i have 0 swe background and my coding skills include sql r and python pandas numpy etc my work is on the analytics side so i rarely think in terms of oop and i dont do a lot of functions even looking at the most basic leetcode problems i get stuck with even where to start terms binary trees hashes sorted lists etc is very foreign to me i want to start doing leetcode but is there any other resources i should start with first just to kind of get the basics of the basics down the idea is that if i see a leetcode problem i want to have some sort of reference of where to start
0,data science and data analytics is becoming ultra glorified romanticized and i don t think people are really told what they are getting into i honestly don t think people wanting to break into data science really know what all it entails it just sounds good and sounds like it will make them lots of money no one tells people what comes with the job there are a lot of headaches that come with it and you have to be a very patient person when any person starts out in it they learn some psychology how to manage users and their expectations you learn what to say and what not to say you learn how to appear confident and reassuring even if you re getting up to speed in the moment the good ones do anyway data science bi da you have to have those skills multiplied by ten you have to be better than the rest at managing expectations you have to learn how to avoid support drains and be thinking ahead all of the time the data science people are the only people i respect as much as the people in systems because other fields you learn one thing and only one side of it call yourself an engineer despite knowing one side sys engineers have to know a little about everything and base knowledge in all kinds of things they are constantly growing data science folks are similar because they have to know a wide assortment of things and they have to know all of the tips and tricks at their disposal to get their desired result which means they will know python multiple types of sql pandas jupyter and so on they ll pivot in excel in a pinch if they need to but the main reason i respect them is just because of how patient they have to be to want to work in their field for 30 years our da left in 2018 and one of my roles was a senior dba so they just put her job on top of mine i learned a lot and i got very good at sql and streamlining and reducing task turn around for reports and data tasks but i obviously didn t have the time to dive ultra deep into the rabbit hole and i didn t want to because i knew it wasn t for me we were acquired and i transitioned all of that stuff onto the bi team of the new company i have so much respect for those people i am still answering questions and taking one off requests this morning i was just hit in the face with how much i dislike actually doing he ds da side a sales senior manager needed something with some data i asked a follow up question i needed a key piece of info to ensure i did the right thing and didn t have to do re work later they said they would get it to me later they emailed it to me at 7 11am this morning then messaged me before my shift hey i don t see the data task with the blah blah being done we needed it 6 3 and i am thinking then why wait until 6 7 to give me the info we got the request 6 4 and i asked you on 6 4 then you waited the weekend to get it to me and those individuals who just keep coming back telling you the data wasn t what they expected or wanted when it is what they asked for i m so happy to be just a senior sys engineer again working on large scale infra it s not for everyone and i think they need to talk about and teach managing expectations so you don t shoot yourself in the foot luckily the bi team of the new company are phenomenal and now i am out of the game but i am learning more python at home in my spare time and things like jupyter so i don t regress skill wise python is useful in what i do anyway i ve rewritten several ps automation scripts in it
2,randon forest can i identify the best tree so i have a large database of patients with long follow up and high mortality i ran random forest and obviously it performs better than conventional clinical score for mortality prediction in addition to area under roc curve and importance of variables can i identify the best tree that predicts the best the outcome if not what other tangible result could i get using rf i need more materials to add to a paper i plan on writing many thanks
0,dealing with paperwork aspect in new project i recently joined a new software company the boss wants to build a recommendations system engine for an e commerce platform my background is machine learning so most of the previous job i received the requirements and implement algorithms and systems to process data this time i was required to build up a product solution include various paperwork like advantage features proposals business model use scenarios i have a feeling that this is not my job but a business analysis one is this normal for an ordinary software engineer to handle this one if so how can i do this effectively
0,data scientists ds management what makes you different interesting predicament i m in i m highly skilled in a unique domain manufacturing robotics sensors machines and have done data work for 2 3 years finishing up a ms of ds i naively thought that i d be a hot commodity on the job market but from my experiences people are looking for a certain type of ds cross ref to this post on popular specializations like ad marketing how do i solicit data from people to sell them stuff finance how do i use market data to make more informed financial decision and transactions computer vision converting pixels into data and inferring things person cat stop sign etc i thought being different would be welcomed but it seems like there is also a certain type of person that gets into these roles data analysts from day 1 people from the domains listed above and for the rest of us who are on the outside looking in as a personal anecdote it makes me feel like what makes me unique is actually a bad thing so in the spirit of curiousity what makes you different as a practicing ds ds management whatever what do you do know that makes you different than your peers
1,best statistical analysis for data that deals with duration hey all going about my virology course and i m currently trying to figure out if there s a correlation between the duration of sickness covid 19 between hospitalized and non hospitalized patients i have exact numbers for how many and how long these patients were sick i just want to know if their being in a hospital or not affects how long they are sick thanks so much
0,analytical uni project can t use certain terminology in research paper hi guys so i m enrolled in a university course called legal analytics for which i had to do a group research project on a dataset of case law metadata so our group basically made dictionaries to map existing metadata values to a new label i e apply label x if y exists in metadata in order to improve our dictionaries we repeated a certain exploratory exercise multiple times to discover additional values and add those to the dictionaries and after each update of the dictionaries we repeated the labelling proces as well ultimately we visualized this process and the difference in labelled cases between each repetition of updating dictionaries and labelling cases in some line charts in our draft research paper we referred to each repetition as an iteration and to the process as a whole as classification now this is where my question and confusion comes in we ve got some phds tutoring this course and grading our research projects a part of their feedback on our draft research paper was about our use of terminology they basically said that terms like iteration and classification are reserved to machine learning and we shouldn t use those terms in our paper this left me not entirely convinced because while machine learning is all the fuzz these days these terms are surely not limited to the field of machine learning so are my tutors right are more simple solutions suddenly excluded from using this kind of terminology i ll be looking forward to your responses
0,what interview questions do you think are really good indicators of ds skill ability and what makes a good ds i know this has been asked before but i figured i d bring it up again to compile some more responses i ve been asked tons of terrible interview questions that are more reflective of if i m capable of memorizing things about ds than if i m actually capable at all so i m curious what questions do you think are really good at illuminating whether or not someone is would be a good data scientist x200b and along the same lines what traits skills etc are you trying to select for when hiring new data scientists
0,would it be a better career choice for me to learn python with frameworks for a data science job or the field is already overcrowded with so many python programmers python is really an easy programming language and a lot of people claim to know so i am thinking about if it s a good investment of time and efforts to learn python or i should try software engineering with jvm c or some other cs field
2,project ideas based on machine learning hi radditors please suggest some project ideas based on machine learning ml
2,gpt j 6b on gpus through huggingface pr hi all i ve created a jupyter notebook with everything you need to convert run gpt j from jax over to work with the new huggingface pr for gpt j i ve also got the model working on our production environment that you can play around with use in production here average inference speed is surprisingly fast running on our t4s around 5s for 50 tokens will be trying with a v100 and quadro 8000 full precision model tomorrow to fit the model on gpus that are sub 24gb the model in the demo and notebook are half precision in torch this was kinda painful to get working so hopefully you find it useful x200b cheers
2,hutter prize entry starlit open for comments the judging committee has ruled artemiy margaritov s starlit compressor is a new prospective winner of the hutter prize as per the award rules for the hutter prize for lossless compression of human knowledge this initiates the open comment period prior to the final award ​the prize is awarded as follows award z× l s l where s new record size of comp9 exe archive9 exe opt or alternative above l previous record for s plus 5 000× number of days that have passed since 1 1 2021 z amount in prize fund 500 000€ update l s while z itself does not get reduced minimum award is 1 of z contributions are dealt with in the order of their submission the contribution is subject to public comments for a period of at least 30 days before the prize is awarded
0,is there any correlation between supply chain management and data science edit thanks a lot for the people who have replied to this post i now got a brief idea
0,sensitive data hello i m working on a project with a client that has sensitive data he would like me to do the analysis on the data without it being downloaded to my computer the data needs to stay private is there any software that you would recommend to us that would make this done nicely i m planning to mainly use python and r for this project
0,need pointers on where to start on how to model this data that is essentially arrival times or arrival delays the situation is like this we receive deliveries of data electronically and let s say they are expected to arrive at 8 am daily but historically they can arrive at say 5 10 am so it s periodic but not really the data is gathered only hourly at the top of the hour the data we re gathering now is basically time since last arrival which is zero when something has arrived in the last hour and ticks up hour by hour until the next delivery what i ve been doing so far is fitting this with a sawtooth wave and whenever a delivery is late the time since last goes above the threshold and alerts us i researched time series data and such but it s not really what i m looking for here s the issue we have dozens of different ones of these per customer and hundreds of customers and my fitting works well most of the time but there are still a large number of false alerts also some of these are 7 days a week others are m f others are more than once per day all very different kinds of sawtooths and a lot of manual adjusting what i ve started doing is just collecting a histogram of when the arrival delay is zero vs hour of the day this gives me just a set of times i did this for one example of a reasonably well behaved data set and got a distribution that was rather skewed but fittable is this a poisson process i have enough history to look back 6 weeks this gives only 6 data points for e g a tuesday but for 7 day per week systems there is no problem aggregating all the days and looking just at the hour of the day 42 total data points but knowing when to combine 7 days vs 5 2 split etc is a manual decision i would prefer it it could fit each arrival event separately i m actually looking at the data for 6 weeks plotted over 1 week timeframe of 168 hours per week so i see the scatter plots overlayed and can guess at the amount of variation when i do these so for example 8 am tuesday would be at t 56 hour and constructing the threshold sawtooth function on that axis what i am looking for perhaps is a way to give say based on the real history give me the time of day that if the packet hasn t arrived by there s a high chance that it s late and set an alarm since the history can and does contain some that were late or absent i know those should factor in with less weight i don t know what kind of distribution these should fit or how to output a cutoff value for a given chance it s out of compliance and the reason i m trying is so i can script this to analyze hundreds of such data sets and set thresholds automatically any suggestions where to look online article videos i would be interested in suggestions for textbooks as well thanks
1,help on figuring out what kind of variable i m dealing with the dependent variable i m using in my model is frequency of park visitation the values encompass once a day multiple times a week multiple times a month etc up until never i want to use linear regression but am not sure if this is allowed for this type of variable my guess is that this is an ordinal variable but i m not sure any help is very much appreciated
2,fastds quality of life wrapper for git and dvc i want to share a new open source command line tool fds which aims to help users do fast data science by streamlining version control for ml projects fds is a command line wrapper around git and dvc meant to minimize the chances of human error automate repetitive tasks and provide a smoother landing for new users quickstart pip install fastds fds h full blog post on the motivations and goals of the project summary why is it called fds just take a look at your keyboard it s so silky smoove to type fds this is important for a command line tool that exists to improve ease of use and delight users in fact due to popular demand you can also type sdf instead of fds for an even more epic experience 🤩 so smoove why did we do this as we were developing open source data science projects using dvc we often found ourselves making the same mistakes over and over again and constantly repeating pairs of commands like git status and dvc status so we set about creating fds with these goals in mind 1 automate common tasks when working with git dvc and later on potentially other tools which work well together 2 provide a more interactive and opinionated ui and ux git and dvc are low level utilities which need to work well in scripts and support all possible use cases this means interacting with them feels like interacting with a command line api rather than a wizard or app fds orients itself to be used by humans for convenience rather than total flexibility 3 provide a smoother landing for new users by making things easy by default and explaining what s going on we took inspiration from gitless a git compatible version control system that is easy to learn and use a project which works on top of git and attempts to make it more intuitive check it out would love to get your feedback feature requests are welcome pull requests even more so
2,graph embedding mean of node embedding this paper uses the mean of the embedded vector of the nodes as the embedded vector of the graph fig 1 is this the first graph neural network paper that has done this thanks in advance for any pointers
2,accidentally kept the author of previous code in my neurips supplementary material will i get rejected for non anonymous submission sorry if this thread might sound silly i just submitted my supplementary material and in the last minutes i realized that i had kept the name of the author of some previous ml paper and code as i used it again for my own submission the name appears at almost every file from this specific part of the code i clearly referenced to this paper and its authors and said that i used their code besides my own thereby that name also appears in my references will i get rejected for non anonymous submission any former neurips reviewer having faced this situation or anyone having made the same mistake as me
0,in your experience what were the questions the interviewer asked that made you realise you shouldn t work at this company or under the interviewer or do you have any questions to ask that help you decide whether the company interviewer is good
2,paper explained reward is enough full video analysis what s the most promising path to creating artificial general intelligence agi this paper makes the bold claim that a learning agent maximizing its reward in a sufficiently complex environment will necessarily develop intelligence as a by product and that reward maximization is the best way to move the creation of agi forward the paper is a mix of philosophy engineering and futurism and raises many points of discussion x200b outline 0 00 intro outline 4 10 reward maximization 10 10 the reward is enough hypothesis 13 15 abilities associated with intelligence 16 40 my criticism 26 15 reward maximization through reinforcement learning 31 30 discussion conclusion my comments x200b paper
2,evaluating a recommendation system i am still a beginner so please bear with me so i am currently creating a simple video game recommendation system using a content based approach for a data science course assignment however i am stuck with the algorithm evaluation and hypothesis testing my question is what are the best methods that i can use to test my hypothesis which is that content based recommendation systems provides an x accuracy
1,probability of choosing an expired pill i have n pills in a bottle the pills expire after t days where n is less than t i get a new bottle of n pills at some number of days that is less than n which is a proportion of n called p when i get the new bottle of n pills i fill that bottle with the p n pills from the last bottle what is the probability that i pick an expired pill from the new bottle how does that probability change if i repeat the process assume the bottles are infinite size what if p is variable over time i m most interested in how you approached this problem so please share your thought process as well as any solutions
0,applied mathematical methods are they useful i am in a graduate level program social sciences program and leaning towards data analyst data science fields when i am finished i am currently evaluating a course i would like to take on applied mathematical methods this particular course is taught in the economics college but the methods should be applicable in a broader socioeconomic context here are the mathematical methods listed matrix algebra differentiation unconstrained and constrained optimization integration and linear programming my question how much math do you use in your daily would knowing any of these concepts bolster your skills if not what mathematical methods would take your game to the next level in a data science role
0,does anyone mind sharing their professional githubs or passing along some ideas on how to build one s out hello i m still learning about this field and trying to figure out how best to build out my skills as well as create a professional presence my personal git has a handful of repos but nothing too exciting so i was looking to see how other people build theirs
0,do data scientists type as much on their keyboard as software engineers just wondering if i can ditch my ergonomic keyboard and use my gaming rgb keyboard for work if the typing is not as intensive
0,who here uses windows as their primary work os i come from cs but i have a friend who is in a data science masters program at a no name school and the program seems to be allergic to unix systems all their classes are taught assuming windows is the only os anyone is using and thus my friend never bothered to learn any unix i ve told he should be learning at least how to work around a bash terminal for when he needs to ssh into a server but i am wondering if that is all he d need to worry about coming from cs i told him he should just drop windows all together and install linux but i began wondering how many data scientists actually use or need linux proficiency since i know many need to use microsoft tools like excel so it is actually important for data scientists to be proficient on a unix system
2,what kind of tools for data annotation have you used i want to know what types of tools are available out there would be good if you recommend both desktop based and webapps also free or paid i want to explore data annotation tools for all types of labeling text image video audio other
1,how to rescale coefficients of lognormal regression to be change in y for unit increase of x i have fit a lognormal regression model after scaling and centering the independent variables only i did this using the brms package in r using the family lognormal argument which fits a model with the following regression structure log y alpha beta x to be explicit i did not scale and center the y variable prior to running the regression now thanks to this excellent explanation i understand to interpret the parameters for the model i need to exponentiate them if beta 0 0582 then exp 0 0582 1 059 which means i would see a 5 9 change in the value of y for every 1 standard deviation increase in x but how should i get the change in y for every unit increase of x do i divide beta by the standard deviation of x recorded when scaling and centering the variable and then exponentiate the result or exponiate beta and then divide that by the standard deviation of the variable i e which of these is correct a exp beta std b exp beta std also is there anything i need to do with the intercept alpha other than exponentiate it
1,measure the similarity between two datasets suppose you have two datasets each dataset contains continuous variables x y z and 1000 rows let s say the first dataset is from a hospital in california and the second dataset is from a hospital in new york are there any common ways to measure how similar the two datasets are thanks
1,how to find statistical significance of more than two variables in a b b tests i am looking for some advice on how to calculate statistical significance of more than two values it will basically be an a b test but more like a b b or a b b b etc so the control will be compared against multiple versions to find which creates the most of a desired result i came across this repo on one way and two way anova but i don t know if it is the correct way to get my desired result any advice would be appreciated
2,handwriting generator use machine learning to generate handwriting from example images in browser i m not the one who made this but it s fascinating how well it works my handwriting was imitated quite well anyone have a clue how it works especially because it is offline handwriting analysis they also had some interesting animation and export options x200b x200b
2,the ai grad school an industry analogy satire this is satire treat it as such don t take it personally i know everything is not like this terminology 1 professor ceo 2 phd student postdocs program product manager 3 master s batchelor s students code monkeys independent contractors unpaid interns 4 paper product 5 publishing paper in big venue product release 6 nsf amazon google facebook jeffery epstein other funding souces vc funds life cycle of a product 1 ceo thinks of some company vision and hires some program managers sells the vision of a product as equity to some dreamy eyed employees who become the code monkeys 2 ceo parallelly makes different code monkeys work on same vision while program managers are chilling 3 one code monkey from the army comes up with some new neural lego that works after months ceo becomes super happy with the outcomes of the lego other code monkeys are told to keep working on a problem when there is already a working solution that even they can know 4 ceo tells winning code monkey lets make a product out of the lego step in project managers no clue of how the lego was built start dictating how to productize it 1 winning code monkey asks to give spec on what is needed for product release project manager gives some requirements code monkey satisfies requirements one week later project manager changes the requirement the pattern keeps repeating and code monkey keeps getting frustrated 5 winning code monkey keeps working on the product but ceo and project manager all have their visions on how to sell it winning code monkey asks for code and detailed information to run lego along with all results be fully open sourced with the product ceo and project manager tell the winning code monkey to fuck off because exposing too much information is not good for the product 6 on product release the ceo goes to vc funds for more money to hire more project managers 1 if multiple products succeed bask in vc money if fail no more funds and no more project managers 7 on product release winning code monkey s contract is done and the project manager takes over the product and post release the product is shelved because it helped get new funding for the new project managers other code monkeys hopelessly play with neural lego dreaming of hopefully productizing one day with some other new vision of the ceo code monkey very upset that the code got no value in this product space code monkey also upset that working with many monkeys in an agile way could have made the product much faster but program managers and ceo didn t like that too much as equity sharing on product becomes the issue but what do i know i am just a code monkey with a tiny tiny product experience
0,open source work in data science for a newbie is there any open source work for someone who has no prior experience in making tools or any contributions someone who welcomes beginners and helps them or guides them to get better in contributing and improve collaboration skills
0,how to learn to analyze business data logic thinking not only tools in the field of data analytics there s a lot of learning materials out there covering the tools libraries or visualization techniques what i couldn t find is a guide on the logic or thinking process x200b for example if i have sales data what to look at what business questions to ask how to make compelling business recommendation from my findings etc x200b do you know of any online resource course book video on that matter
2,theoretical performance of machine learning algorithms on imbalanced datasets suppose you are working on a supervised binary classification task you have patient medical information e g age weight gender height blood pressure etc and whether they have a certain disease or not this is the response variable yes or no let s imagine that determining if patients have this disease is time consuming and costly so a machine learning approach is being considered let s assume that this disease is very rare in your data set only 1 of patients have this disease thus the dataset is imbalanced intuitively we know that any machine learning algorithm trained on this data will likely perform poorly that is the performance will likely be deceptive you might get an accuracy of 99 but misclassify all of the patients who have the disease mathematically speaking is there any mathematical explanation for this very logical concept e g if only study 1 hour for a chemistry exam i might only learn how to solve 2 3 types of problems thus on a true false style chemistry exam there will be many questions that i don t know how to answer because i never saw them before and i will be likely to perform badly on material that i have not prepared for do machine learning models work the same way for popular algorithms like neural networks xgboost and random forest can it be shown that for classification problems you need a minimum number of observations or a minimum proportion of the minority class to probabilistically achieve a certain model performance on a more abstract side i have heard that researchers are interested in trying to make machine learning models generalize without seeing thousands and thousands of examples e g a 5 year old child can learn what is an elephant after seeing a few pictures of an elephant e g it s perfectly reasonable to expect that a young child would see a picture of the cartoon character dumbo and identify dumbo as an elephant after coming back from a zoo but a machine learning algorithm would likely need thousands and thousands of pictures of elephants and likely require to see the same pictures upside down inverted with added noise different color scheme etc prior to be able to generalize and learn the concept of an elephant perhaps the same analogy applies to machine learning models struggling to correctly classify patients with a rare disease since there are so few of them does the above concept have anything to do with the bias variance tradeoff or is it just logic if there is not enough variability and information within the data the machine learning model just learns the noise within the dataset i am really curious to see if such a threshold for measuring minimum level of variability within the data has ever been studied ps in a 1 dimensional sense on a number line if you have a point at 3 and another point at 5 you could consider all inferences outside of 3 and 5 as extrapolation and all inferences between 3 and 5 as interpolation when dealing with higher dimensional data could you simply consider observations from the test set that have a smaller euclidean distance to other observations from the training set as interpolation and observstions that are farther away as extrapolation in reality can you just consider all prediction as extrapolation small scale extrapolation for closer points large scale extrapolation for further points thanks
1,can bonferroni or holm bonferroni be applied to a non independent collection of two way anova p values hello i m a computational biologist we have an experimental setup where we are testing effect a effect b and their interaction on the genes of response organism y y has many genes which we measure therefore if i think about it one gene at a time the analysis looks like a straightforward two way anova gene response a b a b but like i said the organism has many different genes in one replicate we measure all of these in similar studies people will do a straightforward multiple comparisons correction dividing alpha by the number of genes but in those similar studies there usually isn t the two way factorial treatment structure they amount more to a single t test per gene the result of which is corrected for multiple comparisons is it allowed to adjust p values from different anovas the same way you would from different t tests and as a follow up imagine i picked holm bonferroni would i line up the p values from each independent varianble i e all the a main effect p values then in a separate list all the b main effect p values then in a third list all the a b interaction p values or would i make one master list first thanks
0,floating an idea workshops for data scientists interested in contributing to conservation and the environment hey all i m an academic scientist with a background in data science ecological models for invasive species i m working with a non profit conservation group interest in hosting educational workshops in the new york city metro area we ve talked about organizing a workshop for career data scientists interested in finding paid or volunteer work in environmental science and conservation biology we re talking about 3 4 day course with speakers from government and non profit environmental groups plus some skills lessons on how we extract environmental data from government or private databases the goal would be to provide data scientists without a biology background some domain specific knowledge to break into this field there isn t much money but it is exciting and rewarding to be a part of environmental advocacy and protection hypothetically speaking if this was near your city would you want to drop by or even be willing to pay to attend we re not looking to make a profit on these workshops just cover the costs for speakers lodging etc
2,megatron lm annotated paper megatron lm provides a simple yet innovative approach on how to parallelize models to train large multi billion parameters language models and efficiently use gpu memory during scaling the key point is that it does not require any major modifications like compilation or an entirely new framework to implement this in the existing code it also suggested a small modification in the bert architecture which allowed bert to scale effectively to parameter sizes that did not perform well on before i will focus more on papers on model scaling techniques in the upcoming few annotated papers as i want to gain more idea about this area check out the annotated paper below annotated paper
2,ml in digital marketing company hi guys i was wondering if any of you was working as ds de data analyst in company specialised in digital marketing i would be curious to see how data can be used in this field increase impression share decrease cpl budget optimization etc
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
2,performance oriented monitoring and bottleneck detection in my previous post described a case how basic monitoring might help to save 10x on compute power for a specific nn model bert base uncased if you benchmark something setting up a monitoring is trivial for one host but might be an ache if you have a k8s cluster also there is no fun in setting this up every time especially if you lack devops experience and there is no point in manually parsing through the data and looking at graphs if you know exactly what you are looking for and what’s relevant for the performance cost with my team we do that kind of cost performance optimization service for our customers all the time so we have built a set of simple tools for ourselves over time after some feedback from my first post we’ve put together a public web app with some of the tools we use so others might use it too x200b x200b x200b basically what it does you can deploy well tuned telegraf based monitoring with one command on any vm aws gcp or on prem k8s cluster or bake it into an image and get basic visualization dashboards right away x200b you also get an automated detection of some relevant bottlenecks and idle times on your infrastructure and can zoom in on them right away including those i’ve mentioned in my previous post like one thread preprocessing bottleneck underutilized gpu idle times etc… x200b x200b so if you have benchmarking routines similar to ours and just need something to quickly glance at metrics without a world of pain here you go if not just stroll on you lucky bastard lmk if that’s useful or if you have any other ideas what might be ps if that catches any attention we are going to add an automated benchmarking runs the same container on a set of instances and produces a nice summary report with runtime price and metrics for each run pps we just decided to publish it so if anything is glitching email me at egor rocketcompute com
2,is a phd in ml worth it if one doesn t plan on becoming researcher professor i m finishing a 5 year diploma msc equivalent in applied math focusing on cs and ml i m currently working on my thesis and my advisor straight up suggested for me to continue with a phd in ml i ve read various opinions online some say a phd will only give you precious knowledge on the topic to continue your career as a researcher professor and it s not worth it if you plan on working in the industry others say it s a powerful asset for future jobs and will skyrocket your salary i see myself most likely working as software engineer data scientist or ml engineer rather than researcher or professor given that do you think a phd is something worth doing in my case
0,if you made a huge discovery to improve ai e g self driving cars what would you do next i ve sometimes wondered what i would actually do if i came across a discovery like this it s not like i can just send elon musk an email what would you do
2,depth in tree based algorithms vs neural networks on the expressive power of deep architectures 2007 bengio and delalleau i am reading this paper over here on the expressive power of deep architectures 2007 bengio and delalleau in this paper they make a statement one of the characteristics that has spurred much interest and research in recent years is depth of the architecture in the case of a multi layer neural network depth corresponds to the number of hidden and output layers a fixed kernel support vector machine is considered to have depth 2 bengio and lecun 2007a and boosted decision trees to have depth 3 bengio et al 2010 if i understand this correctly a neural network is said to have the same depth as the number of layers e g a n layered decision tree will have a depth of n whereas boosted decision trees are said to have a fixed depth of 3 no matter how splits the boosted decision tree is said to have does anyone know why this is why does a really deep boosted decision tree still only have a depth of 3 thanks
2,not all knowledge is created equal selective mutual knowledge distillation not all knowledge is created equal mutual knowledge distillation mkd improves a model by distilling knowledge from another model however not all knowledge is certain and correct especially under adverse conditions for example label noise usually leads to less reliable models due to the undesired memorisation 1 2 wrong knowledge misleads the learning rather than helps this problem can be handled by two aspects i improving the reliability of a model where the knowledge is from i e knowledge source s reliability ii selecting reliable knowledge for distillation in the literature making a model more reliable is widely studied while selective mkd receives little attention therefore we focus on studying selective mkd and highlight its importance in this work concretely a generic mkd framework confident knowledge selection followed by mutual distillation cmd is designed the key component of cmd is a generic knowledge selection formulation making the selection threshold either static cmd s or progressive cmd p additionally cmd covers two special cases zero knowledge and all knowledge leading to a unified mkd framework we empirically find cmd p performs better than cmd s the main reason is that a model s knowledge upgrades and becomes confident as the training progresses extensive experiments are present to demonstrate the effectiveness of cmd and thoroughly justify the design of cmd for example cmd p obtains new state of the art results in robustness against label noise
0,seeking recommendations for a model evaluation tool if one exists our team works on various models and prediction problems sometimes we get bogged down in discussion about better approaches and want to test new ideas in each team members specific individual scripts they have test and evaluation data which makes true side by side comparisons trickier than otherwise my question is is there a service open source server or even a paid tool where we could send our predictions to be tested against a single universal out of sample test set for model evaluation the only example i can think of is kaggle years ago i attempted to join a competition and you would submit a csv of predictions on some test data and your score would show on a leader board are there any tools servers libraries or platforms out there that work in a similar fashion that would allow our team to compare and compete with each other on some prediction tasks
0,what are your top things that are usually being overlooked but super important and can save lots of time and money my list 1 documentation 2 the summary of code logic as flowchart diagram format 3 code convention 4 grab some general significant number to check if each step output is making sense 5 prepare some common preproc function for the task with similar procedures 6 put the date time as prefix for each simulation testing
1,multicollinearity issue with fixed effects regression and dummy variable hi im doing a fixed effects regression on a panel data with n∗t 870n∗t 870 observations where n 290 and t 3 the entities are districts observed over a total of 3 time periods i want to interact my main independent variable with a dummy variable where the dummy variable takes the value 1 if a large district and 0 o w will this cause issues with multicollinearity since the entity fixed effects are essentially dummies for each entity ive posted the question with more details here ive posted the question here panel data multicollinearity with fixed effects regression and dummy variable cross validated stackexchange com x200b thanks
2,supporting the ml reproducibility challenge hey r ml i m one of the creators of dagshub as some of you might know an ml reproducibility challenge is happening now organized by papers with code with some of the leading ml conferences if you haven t heard about it or didn t consider joining i think that it provides an awesome opportunity for several reasons it s a chance to work on sota research with a clearly defined goal – i personally took part in the last round of the challenge and it was a lot of fun and very challenging i learned a lot from the other participants you can create a cool project for your portfolio you ll be helping to advance the field by making sure that the results are replicable i care a lot about ml reproducibility and making machine learning projects easily reproducible is one reason we started dagshub in the first place we ve decided to support the challenge by providing participants 500 per paper reproduced to help cover compute costs and incentivizing participation and providing our platform and support from our team and community for the challenge this is not a challenge we started and other organizations are supporting it but we thought this might encourage more people to participate and think that everyone would benefit you can read the full guidelines here would love to answer any questions you might have or if you have other ideas on how to incentivize reproducibility further
1,propogating standard errors in predictions from regression models hi all trying to remember my university stats classes if we are making a prediction with outputs of a regression model i m keen to work out what the standard error around the final predicted output would be for example from the regression output below i am trying to predict the utility of a patient with seizures twice per day and with type 2 seizures as its on the normal scale the predicted value is just intercept coefficient twice per day coefficient type 2 seizures 0 857 0 150 0 06 0 647 how do you calculate the standard error of this final prediction
2,relative robustness on adversarial attacks while testing several defensive models against carlini wagner cw attack i am wondering what would be a good way to measure relative robustness of defenses let s say that i have defensive models a and b when they are attacked by cw attack both models accuracy end up becomes 0 which means the attack is so effective that the defenses are not useful however is there any way that measure relative robustness between a and b against cw in this case although a and b are of no use against cw attack there can be difference on robustness providing information that a is relatively more robust than b or in other way in case of pgd attack amount of perturbation epsilon can be controlled therefore although a defense becomes useless when epsilon is large we can still see and compare robustness of defenses with smaller epsilon but for cw attack i don t think limiting the number of iterations or other parameters are valid ways to limit the degree of cw attack i wonder what you think about this
1,any tips suggestions on how best to utilize the rest of my summer prior to my combined ms in biostatistics bioinformatics i come from a biological sciences background but have taken calc i iii i ve been allowed to take linear algebra in my 1st semester of the program what topics subjects would you guys recommend i revise in this interim period before my program
0,is it okay to forget a language if you haven’t used it in a while
2,neural network based association rules has anyone ever looked into more advanced applications of association rules mining association rules can also be used for prediction classification purposes e g this allows you to obtain a fully interpertable algorithm that can provides a set of conditions for making predictions however these rules are usually not very powerful does anyone know if there are more recent spinoffs of this algorithm perhaps where a neural network or ensemble models can be used to learn these rules or in general does anyone know any machine learning based algorithms that provide rules thanks
2,deep clustering survery hi new here i am performing image deep clustering survey and i am trying to pinpoint impactful papers from recent years i am aware of the great survey here but i have a hard time discriminating which paper is impactful and which isn t checking number of citations is helping but i was hoping the community here could pinpoint me to important papers from the field x200b thanks
1,z transforming pearson correlation vs converting to mutual information they seem to be related but how given a pearson correlation coefficient ρ it is common to transform it using fisher s z transform z 1 2 ln 1 ρ 1 ρ i noticed that this is extremely similar to the formula for the conversion of a pearson correlation coefficient to the mutual information assuming only linear relationships i 1 2 ln 1 ρ 2 they are clearly of a similar form but it s not entirely clear how the relate to each other it seems like the fisher s z is doing something like turning into the correlation into a quasi mi does anyone have an intuitive explanation for what is happening here or know of any literature i can consult to dig into more of this
2,balancing doesn t seem to work with neural network regressors i am currently training a neural network regressor on a highly unbalanced dataset the huge majority 95 of samples are labeled 0 0 or close to 0 0 whereas other samples are continuously labelled between 0 0 and 1 0 i train a deep neural network in two different settings first i select a certain threshold build a binary classifier train it with bce and evaluate it on the f1 score in this setting unsurprisingly i find that balancing though oversampling highly improves my f1 score second i use the same architecture but train it as a regressor with mse and in this setting i find that whatever i do to balance my dataset or my training procedure in particular i tried oversampling and lds loss reweighting from delving into deep unbalanced regression icml 2021 my results are always much worse than when using no balancing at all this is true in terms of mse loss and also in terms of f1 score using the aforementionned threshold is this surprising to you would you have some insight about why this happens please thanks
1,help with call center work flow forgive me if i am in the wrong place i am looking for an analysis on work flow please let me know if i can provide more data i manage a call center with 8 hour shifts given no sales the daily requirements are 150 dials per day or 5 hours of talk time or 18 75 dials or 37 5 minutes talking per hour if agents close 3 sales they only need 115 dials or 4 hours 15 mins talk time each call with voicemail is approximately 90 seconds i am seeking an analysis of the maximum time that can be spent per call in order to still meet our quotas while still staying at or below the desired work pace of 18 75 dials or 37 5 minutes per hour with no sales my assumption is that by taking the time to make the sales agents are asked to work at a higher pace than with no sales my hypothesis is somewhere just over 30 mins per sales call will be detrimental to work flow as the pace of dials will exponentially increase as time decays and as people actually answer the phone unknown how any will answer if any at all say an agent makes 3 sales but each call takes 30 mins that is 3 dials and 1 5 hours gone and now there are 6 hours 30 mins to make 112 dials or complete the talk time minutes i know you statisticians can sum this up thanks for any help mods delete if necessary
0,freelance experience as a ds you may share hey there long time lurker first time poster i love this sub i learned quite a lot and i am very thankful for all the interactions about my post i work as a ds in the risk area of a bank nonetheless lately my boss has been taking away my projects and giving them to someone else also cancelling my meetings and engaging less with me i fear he is trying to make me quit management has used this tactic to force other employees to quit before i decided to look for other jobs no luck so far and i read a bunch of articles suggesting me to do freelance work my github portfolio is wanting and i am asking for any and all advice from the community what is your experience doing freelance i am setting my account on upwork atm how may i get noticed any experience you can give a n00b like me can i get a living wage from freelance much appreciated
2,disentangling medical image features using normalizing flows recently i found out about nfs and their property of being inherently invertible and also allowing conditional generative modelling what are some cool works that apply these conditional nfs to medical images are there works that try to disentangle features of images using these techniques
1,when do i reject my directional hypothesis when using a t test so let s say that i have two groups that undertook an english test one s and experimental group eg and the other s the control group cg in my directional hypothesis i stated that eg will perform worse than cg the alpha level is p 0 10 since it s a directional hypothesis my degree of freedom df is 37 because i don t have 37 in my t table i chose the closest that is 40 the t critical in this case is 1 68 left tailed since my hypothesis stated that eg will perform worse i calculated the t observed and i got 1 36 in this case do i reject the directional hypothesis or not i argued with my colleagues about it for god knows how long i think it should be rejected since there s no statistical difference between the two group so we can t say for sure whether eg performed meaningfully worse than cg if that makes sense am i wrong i looked all over the net but i couldn t find a satisfactory answer so i m giving you this specific example to clarify things once and for all
0,what s the culture like in data science progressive currently i m working in civil engineering and it has a pretty conservative culture i m a young queer lady and i had been closeted at work but i m about to get married and i can imagine uncomfortable questions i have the opportunity to get a masters in data science largely paid for i m wondering if a career pivot would increase the chances of having a queer friendly work place what s the culture like in a data science career and yes i know this should be protected but with at will employment laws its harder to build a case also i think i d enjoy working in an environment without racists transphobes climate change deniers and antivaxers
1,help in understanding the difference between pca and efa i m a grad student of psych trying to grasp how principle component analysis pca and exploratory factor analysis efa are different and having a hard time i ve read definitions online and watched videos of people explaining it but am still struggling i know both are used to identify variables that cluster together in a data set but don t see how to are distinct from there i would greatly appreciate help with this
0,advice on resume data collection during this summer season i hope to conduct a research project with my discord friend honoredtarget the goal of this project is to compile a giant database of resumes sent to fang companies and compare those that were accepted rejected to try and crack the screening process in other words we are essentially trying to figure out if there are certain keywords phrases or wordings that increase your chances of getting an interview however in order to do this we need a large pool of resumes then from this pool we focus on the skills section of each resume our initial plan was to create a form and post it on various subreddits but it seems that is not going well i have spoken to various moderators of other cs related reddits about permission to post the survey and they have flat out said no or just not responded so it appears i am in a bit of a predicament how do you guys think i should go about collecting a large amount of this sort of data i have looked online and have been unable to find any public databases of resumes let me know what you guys think i should do
0,advice needed po for data science team hey guys i recently got a position of a po of a data science team previously i always worked with classic engineering teams needless to say i quickly come to realize that 1 data science team engineering team in terms of way of working and approach to work 2 i probably could use some development of how to be a better po for such a team x200b what i would like to ask you guys about 1 could you recommend a good book training materials to become better in working with a data science team 2 could you recommend a good blog web site where i as a po can read updates in data science community that might be relevant for me by now i typically find resources which are more focused on tech issues updates which would be more relevant for the ml engineers data scientists thank you
2,gourdian free dataset download project sunroof solar electricity generation potential by census tract postal code hi there we ve just added a new dataset to gourdian this one courtesy of google s project sunroof this dataset essentially describes the rooftop solar potential for different regions based on google s analysis of google maps data to find rooftops where solar would work and aggregate those into region wide statistics it comes in a couple of aggregation flavors by census tract where the region name is the census tract id and by postal code where the name is the postal code each also contains latitude longitude bounding boxes and averages so that you can download based on that and you should be able to do custom larger aggregations using those if you d like this dataset seems like it d be interesting to cross reference with things like weather and perhaps electricity prices to find the best places for people to invest in rooftop solar if you have any other ideas of what it d be good to combine with let us know and we can try to prioritize ingesting those
1,how can i describe the following equation hello how should i name this type of statistic measure is this some sort of exponential moving average based standard error
0,need to go back to the basics what s your favorite stats 101 book hello i an looking for a book that explains all the distributions probability anova p value confidence and prediction interval and maybe linear regression too is there a book you like that explains this well thank you
1,question what are the different statistical techniques to study associations between different variables of a dataset i am new to the field of statistics and i came across this problem at work where we need to find associations between current penetration and a host of other variables all the data is numeric my first intuition is to use regression and see how the two variables may be related and to what degree my explanatory variables explain the response variable and then i also graph the data with the regression line to make it more visual to me and the reader i was also thinking about calculating the correlation between the variables maybe a correlation matrix am i heading in the right direction are there any other techniques that can help me out and yield better results thanks
0,is deep learning more computer science than it is data science please bare with me if this has been asked before but over the past 3 months as i have continued my job search i have picked up deep learning keras mainly as a ms of ds student i found it interesting that deep learning has its own vernacular around popular statistical concepts cost function dnn vs loss error stats is an example then i started to better understand cnn backpropagation and cnn in particular really have nothing to do with stats and everything to do with computer science representing pixels as matrices and analyzing these matrices accordingly i almost feel like deep learning is much more of a computer science while data science is focused on more traditional statistical methods is this fair to say thoughts on this
1,question popularity of blackhat tools what does the graph mean interpretation can someone tell me the behavior of the graph as shown in this image does this mean blackhat tools are popular or unpopular according to the graph
0,dwindling team hello my data team at work is slowly losing more analysts as they look for jobs at other companies i’m wondering if there are any guidelines when it comes to continuing good work while we actively look for analysts to hire onto the team my main concerns are the slowing down of typical turnaround times senior analysts being forced to do less exciting work yet again etc any tips would be greatly appreciated thanks in advance
1,does anyone have a citation for the equivalence of an interaction regression model between binary regressors and using dummy variables for each combination between the binary regressors it s pretty easy to show on paper that the two models must be equivalent but my professor who isn t the greatest at stats wants a citation anyway the problem is this is such a trivial result that i m having trouble finding an academic source that actually spells it out any recommendations on where to look
2,time varying covariates in survival analysis i m doing survival analysis in order to predict the time to failure of machines and some of my covariates are from sensors pressure vibration etc up until now i ve been building parametric models using a survival dataset in which i only have a single value for each sensorization covariate which is the value at the time of failure or at the time of maintenance in the case of censored cases i e i ve been treating these covariates as if they were a constant value throughout the whole period of observation however i just came to the conclusion that this would not work well in real life is there a way of estimating parametric survival models with time varying covariates if so how can i do it in r any help is appreciated because i have to finish my dissertation soon and i m running out of time thank you in advance
1,hypothesizing the ideal conditions for neural networks vs random forests is it possible to speculate what are the ideal conditions required for a neural network to perform well compared to a random forest for instance when dealing with image recognition tasks we know that convolution neural networks are generally favorable seeing as how the convolution operation is very effective at understanding images e g recognizing edges now suppose we look at a standard binary classification task suppose we have a smaller sized dataset e g 15 columns 5000 rows in general terms we know that a neural network works by approximating small regions of the target function with a collection of mini functions this is done by calculating a set of weights in the future data is passed through this network of weights and these weights are used to calculate the probability that a new observation belongs to a certain class is calculated in theory we could repeatedly pass similar points through the neural network and monitor how the probability of belonging to a certain class incrementally changes a random forest is quite different look at the decision tree for a second the decision tree works by randomly making binary partitions in the data a binary partition is made for a predictor variable such that this partition tries its best to cleanly separate the classes of the response variable when a suitable partition is made for the first predictor variable we move on to the second variable then the third variable etc so in the end if we imagine our data as a big box we create these mini boxes i e terminal nodes within the big box each of these mini boxes has an address i e the different partitions e g if var1 5 and var2 10 then mini box 1 each of these mini boxes is associated with a response label the random forest improves the decision tree by bootstrap aggregation thousands of randomized and smaller decision trees are combined together for improved predictive power and less overfitting all the trees in the forest are used to collectively decide which mini box a new observation should be placed in my question based on this very general understanding on how both these algorithms work can we try to hypothesize what kind of datasets are more suited for neural networks vs random forests for example in the random forest algorithm by using the gini index criteria it is relatively straightforward to make mini boxes for categorial predictor variables however a neural network would have to one hot encode these categorical predictor variables and as a result deal with more variables curse of dimensionality and as well these one hot encoded variables are likely to contain a greater level of sparsity furthermore it might be easier to make general mini boxes in sparse data compared to using gradient descent with missing values i know this is all speculation and the no free lunch theorems say that no machine learning algorithm is universally best but could we try to speculate and say that certain machine learning algorithms might be better suited for certain types of datasets just as convolution neural networks are better for image recognition and lstm networks are better at handling sequential data could we argue that bagging and boosting algorithms e g random forest gradient boosting might have an easier time at handling smaller datasets with mixed categorical continuous variables i would be interested in hearing some opinions and thoughts on this thanks
1,do any of you actually regret not doing a phd in statistics data science hello my goals are to work as a data scientist in industry at this point i’m just kind of unsure if i should pursue a phd or not my goals are to just be a data scientist in the industry for any of you in industry and i’m sure this is also based on the specific industry type do you guys regret not getting a phd in statistics for your job my plan is to get an ms in stats and work but for any of you did you regret not finishing through after your ms if you do have regrets what are they and how had the ms limited you if any
1,question which data program to use how to learn more about this topic dear statistics people we are doing a student project as part of our biology bachelor collecting data about hand features and collecting additional data about other traits factors which may or may not be correlate with those hand features now the question is how to process that data are there some free genius programs which may do the work also are there ways to put in the data and get out usable graphs charts diagrams i could imagine something like deviation of the norm reg hand features correlated to traits factor w x y and z where you see a larger corelation between the deviation of the norm and lets say trait w and x which belong to the category of disease and less correlation between deviation of the norm and trait y and z which belong to the category of character traits in one i d like to depict the correlation between the single factors w x y z and in a second graph the correlation between the categories disease character traits this is just an example in reality there will be much much more traits and categories to connect also using different groups male female and male female so this is going to be a huge mess that s why i m looking for a program and or some advice disclaimer you are not doing my homework by helping me our uni didn t ask for that sort of extra work but as i m interested in this topic i want to treat this project as a chance to learn about it thus increase its complexity just for the sake of it if you know any programs or can recommend great sources to learn about that topic or have some good advice that would be awesome i really hope you can help me somehow edit thanks a lot for all of your very helpful answers
0,shooting for kaggle competition prizes is it worth it i am thinking about joining kaggle competitions to try and win money prizes however i am unsure about the time and money cloud compute it would take to get a real shot at it did you try to join a competition with the specific goal to win money any advice
1,any intuitive resources for understanding rotations in pca factor analysis interactive websites videos or text less preferred would be really helpful thank you in advance
1,education p hacking your way to fame part 1 of 6 what is p hacking here s a new video by me about p hacking in statistics i d love your feedback x200b
2,best package in python to manually set up a neural network without layers purely for evaluation without back propagation training i have some decent experience using neural networks and for everything else before this i have gotten away with just coding up the neural network directly or a script that creates a nn based on some inputs i m about to start working on something that steps up the complexity so i feel like i should definitely leverage one of the big ml tools in python to do the heavy lifting everything i see online about keras tensorflow and pytorch all involve creating layers of a nn and using those tools do to the training which is not very helpful to me i m looking to get use out of a tool where i can specify which nodes are connected the exact weight values and just tell it to evaluate the network based on my inputs to get the output node values i can t use any standard layer creation methods since there is no simple geometry for the connections between the nodes in the networks so i guess my question is what python ml tool makes doing this the easiest bonus if the tool can handle time recurrent neural network evaluation in addition to just forward propagation
0,excel hate what is it with data scientists and being snobs about excel it s a great tool if one wants to get the texture of a data build simple one off things or even prototype logic for a workflow maybe i m weird but i like having stats and using that to flip around data in simple pivots then digging into the crosstabs i understand we like our cutting edge and bespoke tooling but excel is as if not more effective to walk a client through parts of their data in a familiar environment since i ve been in the field i had the idea in the back of my mind that it is a difference between people that tend to think in terms of functions vs tables i come from microbiology mycology research where every record was painstakingly recorded in a table so i probably put more value on dissecting the data much more as i moved from that career track in 2013 to where i am now that focus was on increasingly larger big data and greater distance from the data this may have just been the different between capturing hundreds of observations in the lab by hand to inferring risk from millions of user permissions or is it customary to dunk on excel now
2,can t reproduce paper what next for 3d object detection tasks i ve been intrigued by the recent papers focusing on processing lidar data as a range image in works such as lasernet and rangedet however neither paper shares code and there are no popular 3rd party implementations so i decided to work on one for lasernet it s basically just a resnet that includes some downsampling convolutions and upscaling transpose convolutions the original dataset is uber s private dataset so i chose the waymo dataset as a reasonable alternative however implementing it using the original training parameters batch and lr schedule i m getting loss plateauing early and garbage results when testing on the training set i ve even corresponded with an author to get more information on parameters not mentioned with no improvement when trying to reproduce papers without original code or data are there any tips for reverse engineering the results i ve ruled out any obvious bugs in the code using tensorboard graph to verify correct architecture my best guess is either i m missing an important detail from the original model architecture or training setup or the data is just bad
2,research could anyone get invariant risk minimization irm to work in a practical problem hello everyone i am interested in getting something like irm i e a model that i can explicitly optimize doing something better than empirical risk minimization my problem is a numerical regression tabular data problem and i would like something that learns from a few tables and generalizes correctly to other tables i liked the irm paper it introduces a relatively simple loss function and the code is provided however i did not fully understand whether it will work when using arbitrary nn there are some follow up critics and enhancements is this worth a try or should i stick to serious validation hyperparmeter tuning
0,anyone has experience on working with ‘fully remote team’ i know that we are all working remotely to an extent now but does anyone here have experience with working on a team which is fully remote i am in talk with a recruiter for an exciting position in a fully remote company well funded startup and recruiter promises a good work life balance i have had colleagues in the same location in all the places i worked before i very much enjoyed the social aspect of office so being in a fully remote team is something new for me and i am being a bit cautious
0,are linear models still useful at all if so please comment on their uses i occasionally use r and it feels weird that lm is the only builtin ml function and yet i have so far found no real uses for aside from perhaps simple forecasting and constrained optimization of a hidden function i ve heard that andrew ng seems to find them useful enough that it is almost always the first thing he tries to solve a problem probably for exploratory purposes can anyone comment on that
2,how to train your vit data augmentation and regularization in vision transformers paper link jax code pytorch code a study in order to better understand the interplay between the amount of training data augreg model size and compute budget for vits the authors have trained vit on imagenet 21k with augreg which either matches or outperforms their counterparts trained on the larger but not publicly available jft 300m dataset
2,kmeans on t sne in regards to the recent academica fraud thread am i right think that doing a kmeans or similar distance based clustering on t sne or similar embedding is utter bs because i just read a paper that did that claiming new insight into the domain of the data or what am i missing
1,hypothesis contrast simple doubt about what standard deviation type to use sample s one or population s hi i m doing some exercises about bilateral and unilateral hypothesis contrast and sometimes they give us the sample s standard deviation and other times the population s one so i was wondering which one do i really need to use in the formula to get the contrast statistic z x m s √ n where x is the sample s mean m is the population s mean s is the standard deviation i don t know if the sample s or the population s n is the number of tests any help is greatly appreciated
2,m1 macbook for machine learning i will be starting my phd in a few months have an ra for summer 2021 in machine learning i really need a new laptop within a month which can last for atleast the next 3 4 years i already have an ipad pro which is just amazing to use i hear similar positive reviews for m1 macbook regarding battery life compactness ease of use etc i can use the ipad as a secondary screen with macbook but i hear some negative reviews on m1 chip for machine learning applications and tensorflow sklearn external gpu connectivity giving a hard time for m1 users referring to comments posted here 5 months ago are there any interesting develpments since that post most of my model training will happen via ssh on university servers and i will need tensorflow pytorch docker anaconda on device either for practice or to develop poc architectures i am from india and am ready to go upto 2000 1 50 000 rs for a machine learning friendly lightweight laptop i am aware of the incompatibility of my requirements d i am super excited for m1 mac but i don t want to invest in something just for the aesthetics are there other really good laptops which satisfy my requirements please do help thank you
2,convolutional neural network for cell images hi i m not sure if this is the right subreddit for this but i wanted some advice on using a convolutional neural network i m doing a project where i want to detect and count cells in clusters within an image and was wondering the best way to do it in terms of frameworks or libraries are there any examples online that do something similar thanks for any advice
0,does anyone else feel like an incompetent programmer i m currently an engineering student and i started my data science journey about a year ago when i discovered my passion for data since then i have been self taught all the way from moocs and books i tried to apply what i learnt through personal projects and internships but the entire time i feel like i ve only been searching up copying code and modifying it for my own needs most of the time i don t write any original code except for some simple functions or if i m working on a platform with custom syntax i am mostly familiar with the libraries i need but only to the extent where i know which specific library can help me with a particular task i know it s still pretty early in my ds journey but i can t help but feel incompetent sometimes does anyone else have the same problem if not how do i overcome this and become better at writing original code
0,how does version control work for ml stuff i m a software developer first so getting into ai it s hard to get used to handling so much binary data how do people in this field do data and model versioning in general ninja edit my first instinct is git lfs
1,question free graduate student courses hey everyone i m a graduate student working on my master s in political science i m in the middle of my data collection but i ve never taken a stats class before and i have no idea how to analyze my data i know i ll probably need to use r or maybe i can get away with excel right now i m using thematic coding to look for trends in documents related to coastal management but i have no idea where to begin i looked at mit s analytics edge course but it s a huge time sink 15 hours a week i m not sure what i need but i doubt my analysis will require a mastery of stats or r just some basic applied knowledge however i could be totally wrong ideally i d like to find a relatively quick and free online course that i could finish by the end of the summer can anyone point me in the right direction thanks in advance
2,revisiting deep learning models for tabular data hi we introduce our new paper revisiting deep learning models for tabular data and the rtdl package that enables easy access to the main models from the paper paper code ft transformer tl dr we show that two simple architectures can serve as strong baselines for tabular deep learning 1 a resnet like architecture and 2 ft transformer an adaptation of the transformer architecture for tabular data the problems where gradient boosting dominates should be prioritized when developing dl solutions targeted at beating gradient boosting
0,what content would be useful to intermediate data scientist i’ve been working as a data scientist for 2 years now and i’ve noticed that the content on medium targeted towards intermediate data science professionals is limited and i wanted to fill the gap what kind of content would you guys like for me to write about
1,question prediction based on multivariate explanation hi i have a set of variables that when summed make up an aggregate variable when i regress the aggregate onto a non specified timeseries i get an insignificant result the same happens when i regress the timeseries onto the regressors individually that is in my understanding because the individual variables capture primarily the variability of the aggregate variable however when i use a multivariate regression where i include all fractional variables at the same time some of them yield significant results which also make sense interpretation wise the goal of this endeavor is to use those significant fractional variables as predictors for the dependent timeseries i am however unsure how to go about it specifically i do not understand how to employ the explicative power of those fractional variables as predictors the same way that i determined it ie through multivariate regressions i m not entirely sure how explanation and prediction connect in this case on a theoretical level because that really goes beyond what i learned in my stats classes greatly appreciate every comment
1,question books on more philosophical aspect of statistics hello i recently got interested in statistics and wanted to read some books that would give me a good introduction to it i m not looking for a book that goes more technical and teaches me about the different techniques but i was hoping to find some book that talks about maybe what exactly is valid data etc more like meta statistics or more philosophical statistics i hope my question makes sense thanks
2,research how i started in research and development in ai ml my interest in the areas of ai ml started when i was in the last years of university there i started to become interested in topics such as artificial neural networks signal processing and digital systems thus in addition to my undergraduate courses i took courses in control using fuzzy logic and neural networks where i learned important fundamentals of these areas after finishing university and concluding my english studies i applied for a 6 month internship funded by unesco and the government of poland because there was an area that caught my attention computer vision i applied and had the opportunity to be accepted a few months later in october 2016 i traveled to poland to start the internship there i conducted a research project on deep learning for hand gesture recognition and trying to optimize computational resources by using gabor filters at the end of the project we were able to publish a scientific paper in a top iberoamerican conference on artificial intelligence the iberoamerican congress on pattern recognition ciarp that was my first scientific publication returning to peru in 2017 i started to carry out projects on my own as an independent researcher and related to what i did in poland the results of these works were presented at national conferences such as intercon which helped me to get more involved in research later i looked for an opportunity at the universidad peruana cayetano heredia upch more precisely in the laboratory of bioinformatics and molecular biology led by the outstanding scientist dr mirko zimic i chose this laboratory because they were developing interesting projects in artificial intelligence and medicine thanks to my interest and my previous work i was able to get the opportunity to work in their lab there i started to help in projects where machine learning techniques were applied to make a faster and low cost diagnosis for several diseases such as anemia autism and tuberculosis in parallel to my work at upch i was doing projects independently thus i had the opportunity to have one of my papers accepted in the lxai workshop icml 2019 and also to win a travel grant from the icml organizers for icml 2019 in that conference i was able to observe more closely the advances and applications of machine learning and deep learning which motivated me to get more involved in the research and development of these interesting areas besides that i was able to establish contact with the latinx in ai community and meet several latin guys involved in ai ml subsequently some of my projects that i developed independently were submitted and accepted at conferences such as intercon and simbig in addition i submitted papers developed at upch to workshops at neurips 2019 and icml 2020 most of those papers were successfully accepted and exhibited i also had the opportunity to win a travel grant from lxai for neurips 2019 which helped me a lot to learn about the state of the art of ml ai and also to expand my network a few months later i won a fondecyt peru grant to do an internship in italy in the laboratory of dr lamberto ballan at the university of padua the work consisted in developing an automatic segmentation algorithm for tuberculosis cords in order to have a faster diagnosis this work was successfully developed and the results were accepted for an oral presentation at the ml for global health workshop at icml 2020 after finishing this internship i was accepted and won a scholarship to the pi school of ai where we had lectures on ai and developed a project for a company the project was related to automatic generation of text summaries using bert models this experience helped me to see the differences between developing research projects and developing projects in a company both of which present great challenges since i had already published in several local conferences and workshops of major ml ai conferences i applied and was accepted to review papers for icml neurips iclr cvpr workshops and at the same time i was invited to review papers for local conferences this helped me to strengthen my ability to critically analyze research articles then i had the opportunity to participate in important summer schools such as the lisbon machine learning school lxmls 2020 and the cornell maryland max planck pre doctoral school cmmrs 2020 where i learned a lot about ai ml fundamentals and the state of the art at the end of 2020 i was accepted and obtained a scholarship for the program in data science global skills which is organized by aporta and mit i am currently in the program which lasts a year and a half there we are learning more about the fundamentals along with hands on labs on statistics probability machine learning and deep learning as part of this program we will also develop a data science project with an ngo now in 2021 i am still involved in ml ai projects at cayetano heredia university i am also participating in a project for an opencv competition where we passed the first stage and at the same time i am waiting to participate in summers schools that accepted me such as the eastern european machine learning summer school eeml 2021 the nordic probabilistic ai school probai 2021 among others i hope what i have shared here can help you as a small reference on how to get involved in ai ml research and development you can visit my personal website www dennishnf com for more information references 1 universidad nacional de ingeniería uni 2 universidad peruana cayetano heredia upch 3 unesco poland co sponsored fellowship programme in engineering 4 latinx in ai lxai 5 pi school of artificial intelligence 6 advanced program in data science global skills 7 international conference on machine learning icml 8 conference on neural information processing systems neurips 9 international conference on learning representations iclr original post
0,as a beginner in this field is it normal to feel insecure after seeing people showing crazy ml projects on linkedin hi i m working on my first ml project at work needless to say i struggle very often in performing various data wrangling or any other tasks that i do for that project i don t open linkedin that often but whenever i do i come across people posting crazy machine learning projects that they build for fun passion this makes me feel i am struggling so much in performing tasks that i m paid to do whereas people are just building end to end so difficult ml models just for fun do you guys also feel like that sometimes or am i missing something here thanks
1,what are some ways or heuristics to estimate a distribution when only given the mean and the median if you only know the mean and the median of some data like some exam scores is it possible to imagine what the distribution looks like of course you have an infinite number of distributions that fit the values of these two summary statistics but some distributions are more likely in the real world so based on real world distributions what are some ways or heuristics to estimate a distribution when only given the mean and the median
2,askreddit what can you do with datasets with non commercial licenses recently i ve been looking into a number of publicly available datasets and some of them have non commercial licenses such as sku 110k and rpc my question is where should you draw the line with commercial use let s say we re doing some work for a profit seeking company training on the datasets for a model that they intend to monetise is clearly not allowed but what about transfer learning from the models that the authors trained that seems less clear cut to me but still quite sketchy and what about using the dataset to confirm a hypothesis before going out and collecting your own images and annotations any opinions comments or experience on this that you could share would be most appreciated
2,can an svm give you feature importance i thought tree based models like rfs and boosts are the only modern classifier that gives some sort of feature importance but i found a guide online taking the coefficients of features resulting from an svm as feature importances is this valid
0,is nonprofit salary negotiation a thing currently working at a nonprofit is it worth asking for a review if so typically how long before you should ask
0,forecasting out of stock items on a store item level hi all i work for a large wholesale company and i am trying to create a way to forecast when specific items will go out of stock at a specific store i have access to sales data by store item for the past 5 years as well the past 5 years of out of stock data also on a store item level how would you approach this fyi i code in python mainly and i m pretty familiar with most ml tools and models just trying to get a variety of ideas thank you
1,trying to choose between masters programs how important is the degree title i m trying to choose between 3 masters programs for statistics with the following titles 1 msc in mathematics concentration statistics data analytics 2 ms in applied statistics concentration data mining 3 msc statistics concentration statistical theory if one were trying to get a job as a data scientist would the title of any of these degrees have a noticeable impact on being hired generally speaking for example would an msc in math with a concentration in statistics be considered lesser to an msc in statistics i apologize if this is the wrong place for a question like this but any insight is appreciated
0,grappling with the social impact of data related careers i’ve been working in this field at a consulting firm for 2 years now and a question that always rests heavy on my mind is whether i should be applying my skills to a sector that has more direct social impact the need for data analytics science seems to follow the money trail as larger corporations have been able to collect massive amounts of customer data and thus are willing to pay top dollar to help make sense of it there are definitely options to do data science in a more impactful way e g non profits environmental data life sciences data but i have the overall impression that these fields pay less more importantly it can be frustrating to do ds for due to the lack of work done previously leading to messy and lots of unclean scattered data in other words because the majority of people take the higher paying jobs over the ones with higher social impact the high social impact data roles and ecosystems remain underdeveloped my question is how do you grapple with this reality do you grind at a corporate job and donate part of your income do you teach on the side answer a lot of stack overflow questions invest it in your children’s future some pro bono ds work for non profits in your free time or just ignore all of it because life is short and you worked incredibly hard to get to where you’re at
0,how much has probabilistic programming been adopted in industry hello this just merely an interesting thought i’ve had i’ve noticed there’s an interesting niche within the field of bayesian statistics that goes into probabilistic programming building bayesian models bayesian deep learning etc this area seems like a big topic in research as well my question is more so geared towards industry but what is the trend recently when it comes to using bayesian statistics and probabilistic programming in a company my intuition tells me that bayesian methods are really interpretable to stats ds math folks but to those outside of that in industry say stakeholders or upper level management it may not be as interpretable with most baseline statistics classes starting off at the frequentist perspective it seems that these are the methods which are really interpretable to management in industry and thus there is not much of a use case for probabilistic programming and bayesian methods other than research can anyone speak to this i’m curious to see how much of an acceptance there is to probabilistic programming in the industry and if it is really only limited to research
1,which 2x2 correlation test to use chi squared or mcnemar s i have a research data set that consists of a number of videos which we ve manually coded using themes and sentiments i e positive advertisement authority gov official medical professional etc each video can have any number of themes and sentiments coded to it we then for each theme did a pairwise binary comparison with each other theme to see if a theme has correlative value with another theme in different terms we took the set of videos that match a theme and the set of videos that do not match a theme and paired them with the same binary sets on a different theme here s a made up example matrix for one of these comparisons x200b authority authority positive 20 60 positive 30 240 x200b my understanding is that we can find either the phi coeffecient or the mcnemar s test statistic to compare this data i m confused on which to use my understanding is that phi coefficient is appropriate for unpaired nominal data mcnemar s is appropriate for paired nominal dichotomous data the comparisons above are nominal and dichotomous but are they considered paired data i m having trouble reasoning about why they might considered be paired or unpaired as far as results go mcnemar s is giving results that match our hypothesis for instance positive and advertisement show strong positive correlation when we take the phi coefficient we are getting much weaker results values closer to zero the direction of the correlation is not different between any of our sets for the mcnemar s and chi squared phi coefficient test x200b p s mcnemar s gives a directionless value from 0 1 and chi squared gives a directional value from 1 to 1 but we were able to multiply the mcnemar s result by a direction derived from our results to show positive or negative correlation i can explain how we did that if that s relevant to any answers here
0,how many of you are in a sales position but doing data science for your company i m head of sales for a relatively small company 20m annual sales and do all of the sales data analysis via power bi and i m learning python just curious how many other people in similar positions and how you re dealing with it any insight or guidance is appreciated i love working with data but have 20 years of sales specific experience previously wrote programs whatever you d like to call it in excel vba currently pulling from sql into power bi realizing i can t do what i want without python seaborn etc
2,which websites apps do you use for organizing your downloaded research papers some that i know of free for downloading arxiv papers with the title as filename mendeley first 2gb of storage free for organizing papers and storing them in the cloud zotero free for organizing papers locally
2,red flags that indicate that an elderly ml researcher has a hopelessly outdated mindset say you re considering a distinguished but elderly ai ml researcher as your future doctoral advisor you ve checked their twitter and decided against it because they wrote an opinion about ai ml that is so outdated there is no hope that their advise on your long term research directions will be of any use what opinion would be such red flag for you
0,ml visualization software what software did the author use to create the wonderful visualizations in this transformer writeup i would love to recreate his work
1,finite networks i m looking for pointers on literature of statistics of finite networks to be more concrete i d like to know what methods there r out there when my data are different graphs possibly of different sizes
0,deploy machine learning model on website hello everyone my field is not web development but data science with that in mind i have the following questions i have developed a machine learning model in python s sklearn i would like to build a website where users provide inputs for predictions e g their age and output some prediction e g expected income my objective is not just to make a dashboard i could easily use python s streamlit or r s shiny to deploy my model and make it accessible by anyone i want to use this as an opportunity to learn about web development and build a website from the ground up to solve this problem and have full control over it for instance i would like to be able to have ads on the website and control how many predictions are requested by each user per minute so what would be a step by step clear approach syllabus to do this what specific libraries and tools would i need to use in each step where should the model be stored where should i host my website and make it call the model for predictions how can i make it safe so that my model building code is private what are the most cost effective solutions further details i know some css html and js never created any api not much experience in deploying website or web app thanks in advance
2,anthropic is the new ai research outfit from openai’s dario amodei and it has 124m to burn techcrunch article company announcement from about page anthropic is an ai safety and research company that’s working to build reliable interpretable and steerable ai systems large general systems of today can have significant benefits but can also be unpredictable unreliable and opaque our goal is to make progress on these issues for now we’re primarily focused on research towards these goals down the road we foresee many opportunities for our work to create value commercially and for public benefit our research interests span multiple areas including natural language human feedback scaling laws reinforcement learning code generation and interpretability the easiest way to understand our research directions is to read some of our team’s previous work such as gpt 3 circuit based interpretability multimodal neurons scaling laws ai compute concrete problems in ai safety and learning from human preferences thoughts openai may have been divisive with some of their pr in the past but they definitely do have had some great minds working on policy and safety many of whom seem to be going to this so personally i find it cool
2,what really happened when google ousted timnit gebru i think this is a well written article about the timnit gebru firing
1,question is generalization of the statement correlation is not causation correct or not correct recently i stumbled upon some tweets which were critical of the latest book the noise by kahneman sunstein and sibony the criticism was mainly centered around the fact that the book had stated causation implied correlation however one tweet by caught my eye the author is perhaps hinting that sometimes correlation can be a sign of causation i hope i am not misinterpreting his statement below it the quoted tweet correlation everyone who i ve ever seen mention correlation is not causation in conversation has been a fool causation anyone who mentions correlation is not causation in conversation is a fool in my experience both statements are true that apart in my statistical practices too i often notice in my client meetings that some smarty pants makes the comment hey but correlation is not causation whenever i or my colleagues say x is strongly correlated with y somewhere i feel the over generalization of the statement correlation is not causation has caused more harm especially at the hands of layman the layman outright rejects any reports and analysis with the words x is correlated with y with a quip correlation is not causation my question is the generalization of the statement correlation is not causation correct also is the author in the tweet correct to hint that sometimes correlation can be a sign of causation p s i don t have a training in causality i understand that there is a huge body of work under the topic of causality so thought somebody in this group with a better understanding about this subject than me can help
0,dashboard for market research reporting hello everyone after a long time of research and many fruitless looks i would like to ask you a question that is burning under my fingernails i am currently working in a market and social research institute for a little over two years which would like to gradually finally dedicate itself to more modern solutions to deliver results before that you could find me as a market researcher in financial consulting trying to slay the spreadsheet headed hydra with python but i d rather tell that story another day anyway if you know market research you will also know the typical researcher type of guy who still throws around piles of crosstabs and powerpoint slides and prefers to create their postscript tables based on fixed column ascii data as our company still does in parts but this industry does not sleep either and so interactive reporting formats are gradually gaining acceptance now here comes the difficulty what solution can be found to create dashboards for external customers in ad hoc study projects without much infrastructural effort it needs an environment that i can give to less code savvy people if they want to set up at least rudimentary reporting that nevertheless offers a minimum of interaction the big difficulty here is that many solutions i find on the market typically handle metric data very well but in market research there is also a lot of categorical and ordinal scaled data generated by multiple choice and matrix questions where i need for example stacked column charts of 30 matrix items with their corresponding likert scales sorted by top 2 values from the respective scale which i also need to function after drill down filtering and as reproducible as possible open source solutions are preferred in our company tableau powerbi and relatives are less attractive due to licensing costs around the corner i d also thrown tools like metabase on a server hooked it up to a few sample databases and tried it but the internal query capabilities were too limited for the multi variable graphs i just mentioned it worked but ate up too much time for one graph prorietary tools like dapresy and displayr can inherently work very well with such data as they can even handle wide format input which of course makes importing data easier but again there are licensing costs that quickly scale along with the projects at the moment i m thinking about covering this use case with r flexdashboards and giving my colleagues a library that provides typical functions for us so that we can at least generate a viable product but i refuse to believe that there isn t a more scalable solution around i would be very happy to hear of any good solutions experiences or tips have a great day everyone and kisses on your eyeballs tldr need good dashboard solution for research projects a lot of categorical data need better solution than multiple union all queries in sql colleagues can t code but demand egg laying woolly milk sow excel smells death to pie charts help me i love you
0,non career focused data science subreddit is there a reddit for data science that isnt focused on career questions i m on r statistics but i didnt know if there was a similar home for ds specifically edit and would anyone else be interested in seeing this sub be more than a flood of career entering and movement threads if that s what is wanted great but i ve always thought that most of the content in this sub could be contained in a single pinned thread
2,good books or material on optimization algorithms i m trying to find a good book on optimization algorithms i m not referring specifically to the theory but a good resource on a list of differing optimization functions and their properties for example adversarial loss has properties l1 distance between 2 vectors is
1,question interpreting test percentiles bayes rule three variables hello hey everyone i ve been chewing on this for a few days and i m hoping someone here can help i m trying to find out the percentage of people who got a better score than i did on a standardized test the test reports results in the following way the test has three parts each part reports its own score the scores are supposedly t statistics with means at 50 and 10 point standard deviations so a 60 is one standard deviation above the mean and a 40 is one standard deviation below the mean no other information is given about the underlying distribution of the scores but let s presume the distribution is normal the scores are reported cumulatively let s say i got a 59 61 and 60 on each component not my real scores so i have a cumulative score of 180 my first thought was that because 34 of people are within 1 standard deviation of the mean in each direction and i did 1 standard deviation better then i m probably around the 84th percentile then i got to thinking that was wrong if i assume that the events are independent then i can simulate the number of people with a 180 assuming random distributions around the score 50 with standard deviations of 10 i don t know how to upload an image but in 1000 repeated simulations a score of 180 was always better than between 96 5 and 95 of other scores the tests aren t truly independent though so i would expect that someone who does better on test 1 would also do better on test 2 so i know this is some kind of bayesian equation but i don t know how to begin i found some people who volunteer their scores online and calculated the correlation between test components 1 2 and 3 comp 1 and 2 corr 2 comp 2 and 3 corr 18 comp 1 and 3 corr 33 160 obsv in the volunteered data per test component anyway i ve never done anything with bayes theorem before and i don t know how to start with three variables if anyone has a solution please let me know
0,ml optimizers from scratch using jax github link includes a link to a kaggle notebook to run it directly shreyansh26 ml optimizers jax implementations of some popular optimizers from scratch for a simple model like linear regression the goal of this project was to understand how these optimizers work under the hood and try to do a toy implementation myself i also use a bit of jax magic to perform the differentiation of the loss function w r t to the weights and the bias without explicitly writing their derivatives as a separate function this can serve as an excellent tutorial for beginners who want to explore optimization algorithms in more detail
1,4 1 ms stats summer after bachelors i am an undergrad doing math econ and trying to get an accelerated ms in stats this summer i am at an reu and loving the research world next year i will graduate with my ba and i want advice on what to do next summer are there similar programs for recent grads who plan to return to school after i am also interested in tech industry internships to learn about that world but research work is preferred
1,proof of working hotelling procedure confidence coefficient i tried looking online for a proof that the working hotelling procedure captures the entire regression line with the specified confidence coefficient but couldn t find anything is there a relatively straightforward proof of this
2,discussion fitting or filtering method for data points that seem to group into some volume sometimes gaussian like or triangle like but may have multiple peaks fitting or filtering method for data points that seem to group into some volume sometimes gaussian like or triangle like but may have multiple peaks particularly if it s changing whether it has one peak or more peaks or whether it s more gaussian or paraboloid then is there some general method to discover best fit in this kind of family of shapes
1,how do you compare a population subset proportion to entire population proportion i am working on a research project where i have collected data on cult members in another country i have the data organized by province and calculated the percentage of the total cult member population i sampled who are in each province i want to compare this to the overall percentage of the whole country s population that lives in each province to show that where the cult is recruiting is significant and difference from the overall national population distribution any suggestions on what tests or methods i could use i also have the average age of the cult members and the standard deviations calculated for each province and would like to see if provincial identity impacts age people join cults although based on my significance tests so far nearly all average ages by province fall within the 99 confidence interval for the overall cult population so i m not sure if this would be useful or not
1,rkhs and regression so in regression context given n observations of your partially known function f you want to obtain a prediction of f at new point x in rkhs framework by denoting k the kernel associated with the rkhs h without noise you are looking for the function which minimises the norm f h with respect to f x i y i for i 1 n my question is quite simple why is the function with the minimal norm a good choice what does it mean that the norm is minimal for the function thank you
2,dynamic mode decompositions for higher order systems with new operators and function spaces so this is probably to coolest project i ve worked on this is a totally different perspective on dynamic mode decompositions for time series and it is both deep theoretically and very satisfying the analysis of high order dynamical systems often involves state augmentations where an n th order system for a single variable is converted to a first order system of n variables if the analysis is done with respect to this data then what would have worked well with 10 samples for a single variable approximation problem now requires 10 n samples the curse of dimensionality dynamic mode decompositions are designed for first order systems so its use leverages these state augmentations we designed new spaces and operators to analyze these systems and recently posted a paper to arxiv the big change is that the space of functions act on continuous signals rather than points in state space this lets us analyze directly systems of the form d n dt n x f x that is we do require that the dynamics only depend on the state and none of the intermediate derivatives one example of a system of this form is the duffing oscillator ddot x x x 3 i gave a semi plenary talk at the southeast analysis meeting this year which is honestly one of the videos i am most proud of you can find the paper on arxiv at the code itself isn t really ready for dissemination but we will share it at a later time curious to hear everyone s take on this this has been a great community and i value the input i have received here
1,that troublesome coin toss i understand that the toss of a coin is a good example of probability but it bothers me that there is some chance of the coin landing on edge and i’ve never seen that mentioned i’ve just discovered a paper where a simulation determined the probability of a us nickel which has a flat edge landing on edge is 1 in 6000 or 0 0167 so perhaps all the examples that are careful to point out a fair coin should say a fair coin with a rounded edge 🙂
2,cvpr mobile ai workshop presentations from google samsung qualcomm mediatek huawei imagination and oppo free live on youtube the largest cvpr event on deep learning for edge devices will take place this sunday during the workshop you will see tutorials from all major mobile soc vendors including google samsung qualcomm mediatek huawei imagination technologies oppo and synaptics telling you how to efficiently deploy machine learning models on edge hardware an introductory talk from ai benchmark will additionally provide all basic concepts related to ml inference on smartphones mobile deep learning libraries and sdks acceleration options edge npus and their performance as well as will show how to run any tensorflow or pytorch model on any android smartphone in less then 5 minutes the event will start at 7am pacific time on the 20th of june 2nd cvpr date and will be streamed live on youtube for everyone workshop schedule pacific time 07 00 ai benchmark deep learning on smartphones an in depth dive frameworks and sdks hardware acceleration with npus and gpus models deployment performance and power consumption analysis 08 20 mediatek edge ai technology – from development to deployment 08 50 learned smartphone isp challenge results and top solutions 09 10 imagination technologies imagination technologies approach to overcame the challenges of deploying ai in mobile 09 40 smartphone image denoising challenge results and top solutions 09 50 samsung samsung exynos mobile npus and sdk hardware design performance models deployment and efficient inference 10 40 google android neural networks api what s new and best practices 11 10 quantized image super resolution on npus challenge results and top solutions 11 30 synaptics ai on the edge at synaptics hw and sw products and development 12 30 huawei ai deployment from hardware to software – challenges and opportunities 13 30 video super resolution on smartphone gpus challenge results and top solutions 13 15 oppo learning to see the world clearer 13 40 single image depth estimation on mobile devices challenge results and top solutions 14 20 quantized camera scene detection on smartphones challenge results and top solutions 14 30 qualcomm hate it or love it your sw stack defines application performance and reach
1,updated article on data types in r i posted an article on the data types in r yesterday and it s been edited after all the suggestions that i received check it out here hope this helps any beginners i ve verified what i ve written from multiple resources before posting it i can link those resources in case anyone is interested to know more hopefully it gives you clarity on vs and na null nan if you found the article useful give it a 👏 thanks
0,what is your official title currently
2,help with an optimization algorithm for stylegan2 interpolation i would like to interpolate two images using stylegan2 ada pytorch from nvlabs for the sake of simplicity it can be said that with two images of different persons i want to create a third image depicting a third person with a body from the first image and a head from the second i also have corresponding w vectors for the two source images ready at hand g is a generative model in line with stylegan2 trained to output 512x512 images latents shape is 1 16 512 g g eval requires grad false to device type ignore num ws g mapping num ws 16 w dim g mapping w dim 512 segmentation network is used to extract important parts from images segmentation dnn segmentation dnn to device source images are represented as latent vectors i use g to generate actual images image body image from output g synthesis w body noise mode const image head image from output g synthesis w head noise mode const custom function is applied to source images to create masked images in masked images only head or body is present and the rest is filled with white image body masked apply segmentation mask image body segmentation dnn select body image head masked apply segmentation mask image head segmentation dnn select head in order to compare similarity of any two images i use vgglos vgg16 is used as a feature extractor to evaluate image similarity url with dnnlib util open url url as f vgg16 torch jit load f eval to device class vggloss nn module def init self device vgg super init for param in self parameters param requires grad false self vgg vgg self criterion nn l1loss to device def forward self source target loss 0 source features self vgg source resize images false return lpips true target features self vgg target resize images false return lpips true loss self criterion source features target features return loss vgg loss vggloss device vgg vgg16 now i want to interpolate image body and image head creating image target to do this i need to find latent representation of image target in the latent space of stylegan2 crudely i can optimize for a certain interpolation coefficient query opt to partially include latents from image body and image head w target w body query opt w head w person query opt torch randn 1 num ws 1 dtype torch float32 device device requires grad true optimizer torch optim adam query opt betas 0 9 0 999 lr initial learning rate w out for step in num steps learning rate schedule t step num steps lr ramp min 1 0 1 0 t lr rampdown length lr ramp 0 5 0 5 np cos lr ramp np pi lr ramp lr ramp min 1 0 t lr rampup length lr initial learning rate lr ramp for param group in optimizer param groups param group lr lr synth image from w target using query opt this is an important step and i think that my math is messed up here w target w body query opt w head w person image target image from output g synthesis w target noise mode const image target body masked apply segmentation mask image target segmentation dnn select body image target head masked apply segmentation mask image target segmentation dnn select head loss vgg loss image body masked image target body masked vgg loss image head masked image target head masked step optimizer zero grad set to none true loss backward optimizer step logprint f step step 1 4d num steps loss float loss 5 2f save current w target w out step w target detach i can t figure out how to make my optimizer actually target query opt in such a way that combined vggloss is actually optimized for i must be missing something in my pytorch code or maybe even in the main interpolation formula
1,education grad school course recommendations hi everyone i m going into my second year of a ms biostats degree with the goal of getting a statistician data scientist role in a health related field after graduating here are some of the courses i could take next semester statistical computing fundamental algorithms rng mcmc em implemented in r techniques for working with big data hadoop spark unix programming databases version control programming best practices clinical trials longitudinal data analysis survival analysis python pandas numpy skl pytorch advanced applied stats mediation analysis ml bayesian techniques which of these would most help me get a job be a good statistician data scientist i see many job postings ask for skl so perhaps taking the class on that would be good but i think i could probably teach myself skl outside of school i think it would be much harder to teach myself statistical computing or survival analysis for example it seems the more methodological courses would have a longer term impact on my abilities and understanding of the field and the more applied courses would have the short term benefit of helping me get a job how do i balance these needs
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
1,extend to a 5th year just to get a math minor hello all i’m currently a rising 3rd year at my university in the us majoring in statistics i’m considering going to phd programs after college i’ve heard from people that my major isn’t the strongest for theoretical math prerequisites for top schools but i can get into a mid tier school with my current classes i will be taking intro real analysis not the rudin one linear algebra lower div and introduction to proofs logic class these are all part of my stats major i don’t have any topology measure theory or other stuff under my belt with regards to theoretical math i’m a stats major so i have stats classes but i’ve been told i’d be at a disadvantage for some top phd programs because i don’t have intense math rigor with that being said i do have the option to do a math minor in addition to my stats major to take more math but since i will be a third year in the fall this would require for me to do a fifth year how does a fifth year look to phd programs do you think i should do it if it will make me get into more top schools will doing a fifth year for the minor be a waste and should i just get into whatever program accepts me does the prestige of your phd grad school really affect job chances i want research scientist roles any advice would be appreciated thanks i will most likely have two papers published by the time i graduate from undergrad research but i’ve heard some grad schools don’t care about it and only care about your grades in theoretical math classes i will also have 4 profs who will give me letters of recommendation and currently have a 3 4 gpa screw general ed courses
0,best alternatives to shap package the shap package has been great when it works but i would like an alternative package that has similar functionality i mostly use gradient boosting so any package that can use the tree path methods interventional is nice too but not as important would be a life saver
2,pytorch wrapper of attention free transformer aft layer hello folks recently came across the paper titled an attention free transformer by zhai et al from apple inc here s my pytorch wrapper around the aft full layer you can pip install it as well more info in readme it s a plug and play module with existing attention based networks without major tweaks any prs suggestions amends appreciated the paper wasn t too clear for some implementation details so i had to essentially pull certain things out of thin air and somehow it still works tbf cheers
2,dataset of the political alignment of subreddits hi as a part of a course on network science i m currently taking i m doing a project where i want to analyze the political divide in the us and how it carries into the digital landscape in particular reddit i d like to see the relationships between subreddits of differing political alignments i d like to use this dataset which includes 6000 subreddits which links subreddits where a user posted a link from one subreddit to another my current issue is that i don t have any way to find out the political orientation of the subreddits well at least not in a matter which takes a reasonable amount of time does anyone know of a dataset of subreddits that includes the political orientation of each otherwise if any of you thinks up an idea of how to perform my analysis without massively enriching the dataset perhaps by only using a handful of political subreddits i d very like to hear what you have to say
1,maximum entropy method with random constraints i m working on a research problem that involves measuring constraints on a function f x c and then finding the distribution p x that has the largest entropy while respecting the constraint the problem is that when i measure c from the data there is uncertainty involved so i don t know exactly the value of c to put it in a bayesian context let s say the best i can determine with total certainty about c is a prior distribution for example it is well known that if f x is the expected value of x then p x is an exponential distribution with mean parameter c now what if all i know about c is that it s a normal random variable with mean mu and standard deviation sigma how would i find the solution p x that meets these criteria if it helps here are some important properties of p x i know with certainty 1 p x is a member of the exponential family 2 1 x 1 3 the sufficient statistic is a set of legendre polynomials 4 the function f x are the expected values of the legendre polynomials 5 the maximum likelihood estimate of the distribution p x is easily obtained from any given value of c 6 the mle obtained from 5 is also the maximum entropy solution because it is unique only one distribution p x satisfies the constraint any thoughts or relevant articles would be appreciated thank you
2,does anyone know of a github or notebook that classification nlp to predict what language a sentence is in i want to see how someone would use classification to predict what language a sentence is in does anyone know of a project that has worked on that ideally a simple one
1,would need some help to define a reasonable threshold for biological samples hi i just established a new experiment type in our lab and got the first data from a pilot study because this is a new experiment there is no existing standard data analysis workflow in the next paragraphs i will describe the experimental approach as well as the possible threshold formulars that our lab is using and why i feel they are suboptimal so lets start with some details for my experiment i have patients n 15 suffering from nosocomial infections caused by different common viruses virus 1 virus 2 virus 3 and a healthy control cohort n 15 i measured different protein values protein a protein b protein c for patients and controls our hypothesis is that virus 1 causes an increase in the concentration of protain a virus 2 in protein b etc however these viruses are extremely common and can also cause asymptomatic infections so it is somewhat likely that also in the control group some protein concentration could be mildly increased but in the patient the increase should be way higher i wanted to use the control group to set a threshold for every single protein a c as healthy background in the next step i would check which patients have higher protein concentrations and thereby predict the infecting virus since we also have the clinical data i would be able to compare my protein based prediction with the clinical diagnosis this made a lot of sense to me an my colleagues but when we saw the real measured data the definition of the threshold gave us a lot of headache and since we are no statistical experts our tools are rather limited 1 mean 3xsdnote way to prone to outliers as in prot c controls you can see there is one outlier that would screw the mean based approach a lot in addition we can expect a right skewed distribution for the data 2 median 3xiqrnote looks already way better but there are still a lot of data points in the controls that would be included and are thereby false positive i would favor to have rather false negative cases not able to predict the causing virus than false positive predict someone healthy as infected 3 trimmed mean 3x trimmed sdnote somewhat similar to 2 however the guy who gave us the advise to use the trimmed mean also said just go for some nice even number like 10 or 25 trimming what sounds really arbitrary to me but if this is the way to go what amount of trimming would you recommand overall i noticed that i get relatively good results with simply using 100 as threshold unless the control thesholds using 2 or 3 are higher in this case i would go for the calculated thresold but this is totally arbitrary and extremely unsatisfying it would be really great if someone could come up with a smart reliable and easy idea for the threshold calculations why easy first no one in our lab is able to write and run fancy r scripts and second our pi is hard unable to convince to publish complex statistics because they might be mathematically more correct but not the usually way in our field and yes statistics in biosciences are hmm blunt but maybe one of you has the solution i am looking for x200b example dataset prot a ctrl prot a patient prot b ctrl prot b patient prot c ctrl prot c patient 5 7 32 223 1 63 20 1 238 1 96 1 1 4 1 51 8 339 1 4 15 7 1 3 3 1 420 13 13 19 2 1 1 23 1 1 1 147 1 30 1 8 6 323 242 1 1755 1 33 20 71 78 9 25 3 4 7 59 1 84 1 147 11 99 8 71 2 25 3 45 4 8 3 1 2 542 4 14 1 59 4 196 4 415 4 4 3 14 1 7 x200b ps i did run a pca but since the original data set is way bigger and more complex there was no reasonable result
1,is there a noise resistant method for testing correlations of proportions i have a friend who is trying to analyse a large sanskrit corpus with many verbs in verbs can occur in the active or the passive and they can occur in the past tense or the present tense she wants to run a statistical test to see whether verbs which occur more often in the active than the passive in the present tense also occur more often in the active in the past tense here is made up data to illustrate present active present passive past active past passive run 102 50 130 seize 40 84 59 rebel 1 2 3 one method would be to compute active stats for a present tense and b past tense and then look at the correlations between these two present active past active run 67 seize 32 rebel 33 here for run 102 102 50 67 and so on this method is not ideal because some verbs are very rarely attested like rebel above and those will introduce a lot of noise into the system could anyone point us at a better method i feel like we need some hybrid of correlation and a chi squared test
0,are there any 3rd party tech data sci recruiting firms worth their salt i swear every time i decide to work with a 3rd party i m reminded of why i never want to work with 3rd parties even if they say they specifically place data science folks anyone here had luck working with a 3rd party recruitment firm and if so which one s
1,sample size too large election projections accurate within 15 minutes after closing voting booths please help a statistics noob hi everyone i m quite a statistics beginner so i d really appreciate some help i d like to figure out by when the election projections are accurate enough 1 error for me to regard assuming the voting booths close at 6 pm suppose i expect a party to gain 4 5 million votes 10 of all votes p 0 1 i also expect all votes to be counted within ten hours this gives me an average of 104 000 votes n 104 000 counted for this party per 15 minutes assume the projections are updated every 15 minutes based on this influx of newly counted votes assuming also that the votes follow a normal distribution i would have a standard deviation of merely 140 57 votes root104 00 0 1 0 9 surely that can t be right there s no way this accurately reflects reality right what am i doing wrong or how can i do this differently maybe i shouldn t assume a normal distribution it just seems to me that the sample size is too big and therefore the deviation too small perhaps there s a way to add more uncertainty i d greatly appreciate any help
2,rudimentary real time 2d to 3d media conversion and playback in vr hi everyone i m a developer working on a project for converting 2d media to 3d meshes using intel s midas model for playback in vr running custom on edge directcompute based inference engine in unity initial results and early prototype available here
1,sales trends c i have a list of registration years of vehicles that are bought and reregistered as used vehicles i am using the initial registration date and the used registration date to find the amount of time customers are holding onto their vehicles i am sorting the data to only give me n 20 and i have data going back 20 years if i project that most customers hold onto their units for a period of five years trying to extrapolate this to conquest customers based on average financing options what would be the best statistical test to use to determine this i want to be able to cross reference this with my known accounts to conquest accounts so we can use marketing money to throw at them when we can predict roughly their purchasing period sorry i haven’t taken stats since college and figured i’d ask here before start digging into my old notes or watching youtube videos any help would be appreciated thanks
1,what statistical analysis or statistical tool should i use for example i have an response variable of number days from a patient to fully recover from fever and my independent variables categorical values are age 60 to 69 70 to 79 80 to 89 90 to 99 bmi obese normal underweight overweight and blood type a b ab o i want to compare that ages 80 to 90 with a bmi of obese and a blood type of ab takes longer time recover than the those with age range of 60 to 69 with normal bmi that has a blood type of o what statistical analysis or statistical tool should i use
0,california consumer privacy act interview request graduate school hello i am writing a paper on california consumer privacy act or more specifically title 1 81 5 california consumer privacy act of 2018 1798 110 i need to interview someone in the workplace that has experience on this act statute how this statute has impacted your work etc let me know if you re interested for a 30 minute zoom skype interview on this topic i can send questions beforehand tia
1,would there be any point in trying to beat the betting companies betting companies employ many statisticians to calculate odds on various mostly sporting events of course they most likely have access to some really great data and have a lot of resources in general however betting companies will host odds on many different types of events do many statisticians try to beat the betting companies for particular events where they might be able to invest more time into creating accurate models to calculate odds in a sense try to out statistics the betting companies let s take football for example if a statistician were to invest a lot of time into trying to predict the outcome of arsenal games exclusively surely the statistician might be able to do this more accurately on average with enough time invested
1,can someone please help me find a reference source that explains this neural networks are weird in the sense that they both are and are not impacted by the curse of dimensionality dependent on the architecture activations depth etc so to reiterate the curse of dimensionality is the problem that a huge amount of points are necessary in high dimensions to cover an input space one way to interpret deep neural networks is to think of all layers expect the very last layer as doing a complicated projection of a high dimensional manifold into a lower dimensional manifold where then the last layer classifies on top of the next part so for example in a convolutional network for classification where the last layer is a softmax layer we can interpret the architecture as doing a non linear projection onto a smaller dimension and then doing a multinomial logistic regression the softmax layer on that projection so in a sense the compressed representation of our data allows us to circumvent the curse of dimensionality again this is one interpretation in reality the curse of dimensionality does in fact impact neural networks but not at the same level as the models outlined above source cab anyone recommend any sources papers articles where they explain how why all layers expect the very last layer as doing a complicated projection of a high dimensional manifold into a lower dimensional manifold where then the last layer classifies on top of in all the readings i have done i have never read any interpretations that describe how when data passes through layers a natural projection from high dimension to low dimension is taking place thanks
1,is a two way anova test suitable for my thesis analysis hello for my thesis i will be comparing the yearly income of type a companies between the years of 2010 2020 to the yearly income of type b companies between the same years i want to see if these yearly incomes between the two types of companies i have 50 companies for each category have progressed in the same way over the same 10 year period my roommate suggested i do a two way anova test doing some research though i am very much confused if this is the best way to go any suggestions thanks a lot
2,are resnets as good as it gets tldr for training from scratch on non classification tasks with humble computation resources are resnets roughly as good as it gets this has been my experience although the literature and the hype surrounding it does not seem to agree with me at all please prove me wrong in the past years visual models have transitioned from resnets to efficientnets and more recently vision transformers and temporal sequence models have transitioned from rnns to transformers while the latter in my experience constitute a clear step forward for almost all aplications and resource scenarios the former in my experience has never really worked for me while i m well aware that pre trained efficientnets perform extremely well on imagenet and are quite successful for finetuning on other classification tasks and even segmentation for me they have never worked well for new tasks such as generation from visual features or self supervised learning i have seen some threads about this here and there but it does not seem to make a dent on the efficientnet hype overall there are also other models such as regnets and vision transformers but these are still substantially slower during training time than normal resnets without massive performance improvements in fact for vision transformers this underwhelming behaviour was acknowledged in the recent dino paper so basically i am left with the questions 1 are resnets roughly as good as it gets or 2 have i not found the good new visual models or 3 has my extensive experimentation with these new models led to misleading conclusions the fact that very recent self supervised learning literature is still focused on resnets excluding very recent ssl vision transformer works and that most practicioners i know that do not work strictly with classification still use resnets leads me to believe that perhaps there is something to this and i am not totally deluded please prove me wrong cheers
1,how interpertable are regression models i was recently reading some articles on the importance of interpertability when dealing with blackbox models blackbox models like neural networks are said to have a very low level of interpertability because they don t allow the analyst to understand why the model is making a certain predictions for an individual observation on the other hand models like decision trees and regression models are said to have much higher levels of interpertability in a general sense i can understand why models like decision trees are interpretable because they literally provide the analyst with a set of fixed rules that explain how to classify an individual observation if you look at a regression model e g salary 5 3 height 2 weight 15 8 age a regression model can allow the analyst to understand how much each variable contributes to the prediction e g in this example age contributes more to the prediction by a factor of almost 8 times and you can also find out how statistically significant each variable is e g indivudal p value of each regression coefficient is this what is meant by the interpertability of a regression model thanks
2,what is considered samples in tensorflow object detection api in image classification it is obvious that images are considered as samples but what is considered as samples in tensorflow object detection api are the samples images or the individual objects in the images if an image contains 10 objects should i consider 10 objects as training samples or just a single image i need to know this to effectively set batch size and steps for training
0,data analytics with java currently i’m working for a customer which only allows visual basic and java the project i got is to make visualisation create a report in powerpoint automatically this should be done easily in python if using matplotlib and pptx python has anyone done data analytics in java like this before
1,real world problem hi thanks for reading this and helping me think it through i m an ee who doesn t do a ton of stats this experiment is the way it is for reasons i can t control the problem i have experimental data from two models of a device in several changing environments that are out of my control not every environment has both devices but some do the environments are dynamic in terms of magnetic and electrical fields present i can process data from each device and get total vrms noise over a bandwidth i care about per five second chunk of data i want to determine if one device is better than the other at shielding against that radiated noise a meaningful difference would be an order of magnitude or more the experiment run sometimes multiple times with each device environment four minutes of ambient environment noise data four minutes of data taken near a noise source that should be greater than the ambient environment but may not be the idea is that they ll add root sum squares style and the new noise will swamp the ambient contribution questions assumptions averaging the five second chunks ffts and then taking the area under the curve gives me total average noise plotting the average noise vs how often that amount of noise appears grouped by device and binned in some sensible way should give a normal distribution per device that i can then compare using a 2 sample t test i d like to use the delta between the noises in both parts of the experiment but it seems like that would be measuring the environment and not the performance of the device or maybe that s where a paired t test would be useful guidance here is welcome some experiments will be run incorrectly introducing conducted noise but i only have the data no observation of it being taken can i throw out data that shows very large noise the null hypothesis the shields are equally performant in defending against radiated noise alternative hypothesis the shields performance differs this is akin to an epidemiological study where everyone is vaccinated with one of two vaccines and they re in unknown environments but one environment probably has more of the disease and we want to know which is better and roughly how much better thanks
2,will jax be the next big thing i would like to know of you if you think that jax will disrupt pytorch in the future as the new „tensorflow“
1,trouble with understanding mixtures hello everyone i ve been studying gaussian mixtures for a project of mine and i can not for the life of me understand what they represent so far i assumed a gaussian mixture with 5 components was essentially a weighted 5 dimensional normal distribution however in some papers i ve read the number of dimensions of an observation is bigger than the number of mixtures is there a transformation of variables that i m missing or did i completely misunderstand the subject
1,adjusting probabilities based on historical data i ve searched the sub and found things that seem related but nothing that i saw anyway mdash it s possible i could ve missed it or searched the wrong terms that directly addresses this not being super into stats myself i figured i d ask to get some direct answers tl dr while probabilities are fixed i e 50 50 heads tails for coin flips can we adjust or weight probabilities based on historical data sampling and apply that as an assumption going forward for lack of knowing a better term i ll refer here to theoretical probability and historical probability to define how i use these terms theoretical probability is the probability on paper that is to say with a coin flip the theoretical probability of hitting heads or tails is 50 50 the historical probability as i ll be using it would be adjusted based on data over let s say 100 flips you hit heads 80 times and tails 20 times so in this scenario your historical probability of getting heads was 80 and tails 20 i know statistically no matter what the historical data shows every individual flip has a 50 chance of being heads or tails and as the sample size increases the results will trend that way after 1 000 flips you might see 700 heads and 300 tails for example and after 10 000 it might be 6 000 and 4 000 however is it fair to say for smaller sample sizes that an adjusted probability can come into play based on historical probability keeping with the 80 20 example for coin flips would it be fair to say that based on the 100 flip sample size future flips have a weighted probability skewed from the theoretical something like 50 80 2 65 for heads so dropping the simple coin flip example i m looking to apply this to the lottery as a fun exercise not with any hopes of beating the odds or hitting the jackpot i simply find that the lottery has some interesting data patterns i have been playing with the statistics of the lotto max national canadian lottery and have noticed some data points that i found interesting for example while theoretically every combination of odd and even numbers is just as likely as any other to my knowledge anyway could be wrong here the winning numbers tend mdash at a rate of about 30 84 mdash to contain 4 odd numbers and 3 even numbers see the table below odd even draws of draws 7 0 5 0 69 6 1 37 5 12 5 2 104 14 38 4 3 223 30 84 3 4 209 28 91 2 5 108 14 94 1 6 36 4 98 0 7 1 0 14 based on this while theoretically i think anyway but again i could be wrong you should be just as likely to have 7 even numbers as you should 4 odd 3 even the data shows that choosing 7 even numbers will probably not be the winning combination your best odds are with 4 3 or 3 4 then there s also the individual numbers the numbers run from 1 50 inclusive while every number has the same probability of being drawn as any other number at that same point in time 1 50 1 49 1 48 the data shows that the number 50 is the lowest occurring number having been drawn only 31 times the number 39 on the other hand has the highest rate of occurrence at 128 draws so then would it make sense to adjust the probability of 39 to say that it is more likely to be redrawn does any of this make any sense to anyone or am i way off base here
1,central limit theorem and normal law hi all so i understand that when we sample a distribution and take the means of sample and create a sampling distribution of means it will tend to a normal distribution and that it s at the basis of t test and all of mean test but what about the samples distribution and the population distribution of the phenomena we are studying when it is not normal ok we will have a sampling distribution that is normal but making a test based on means to it doesnt make any sense right and doesnt having the original distribution being normal isnt an assumption in order to make t test and others standard tests what s the point then
0,comparing a data analyst intern bank offer with it project analyst consultancy offer hi all i have two offers one is from a big4bank for a data analyst internship pays less but i believe they use state of art technology and there is a lot to learn the work is all for internal stuff the second offer is to work as a project analyst in which i will be working with clients the company is big but not in the top10 or anything the advantage of this company is that i’ll be working with clients which means more exposure and connections they also use near top tech i m not sure which one will provide most growth any advice appreciated
2,why do we pass the reward of the previous time step as input to the lstm in rl many recurrent based rl agents pass the action and reward of the previous time step as input to the model i was wondering about the intuition of this
1,i run 100 simulations and i record the wait times of every entity in each simulation if i want to know the median wait time do i take the median of all wait times combined or the average of individual median wait times edit some context i am simulating patients in a medical clinic patients arrive a random number of them at mostly random times and depending on if the necessary staff are available the patients have to wait for the next step of their appointment then leave i am interested in if the wait times are below a certain threshold i e if the xth percentile of wait times is below y minutes for example simulation 1 returns wait times 1 5 6 2 3 4 simulation 2 returns wait times 2 3 7 1 simulation 100 returns 10 3 5 2 3 3 3 if i want to know the median wait time of the system should i combine all of the wait times into a single list and take the median or should i take the median of each simulation and average those 100 medians what about the median of the medians would it matter if i wanted the 90th percentile instead of the median what if i wanted the mean wait times i believe this is called the grand mean the number of samples might not the same in each simulation but the number of samples should be around the same they come from the same distribution
0,future of data science hi guys i am sure all of you must have seen the popularity of the field these days from being stated as the sexiest job of the 21st century to having a variety of its courses programs online on platforms like datacamp edx coursera dataquest codecademy etc most of all recently google just launched their data analytics professional certificate so that anyone could get a job in the field having to have followed the program as there is a vast number of job openings worldwide my question is what do you guys think about the future of data science in the upcoming years is it going to become oversaturated in the upcoming years or is the job market in contrary going to increase how is it going to affect students enrolled or thinking of enrolling themselves in the bsc of data science as more more people are taking these courses online from these huge enterprises companies would love to hear your insights
0,neoj4 books hello there i m a data science student and i was looking for the neoj4 book for graphs databases and machine learning being a student i can t get the free copy can someone pass it to me or suggest another book thanks for the attention
0,lifetime reliability and performance testing looking for suggestions on either books courses websites etc that cover things like survival analysis reliability testing lifecycle testing would be great if implementations were based on python r
0,best way to politely refuse a job offer casually applied for a position interviewed and really liked the company eventually landed an offer however i m currently a year or so away from when i have a significant equity vesting with the current little company i m working for and would hope to not burn any bridges with this new company maybe eventually hope to work there after i finish my current stint tried asking for some time on the offer and the answer was no any other good way to politely refuse the job offer while possibly keeping the connection open
1,how does r s coxph function handle gaps between time spans hello all i am working on survival data with multiple observations per participant i m curious how the coxph function handles situations where for example a participant has a gap between successive start stop intervals a participant may be a no show for a period of time and suddenly re appear at a follow up visit participant 2 is an example of this where there is a 10 unit time gap between successive time spans the event and x columns are there for illustrative purposes x200b id start stop event x 1 0 5 0 3 1 5 18 0 4 2 0 5 0 2 2 10 18 1 1 will participant 2 be included in the risk set for events occurring during the span from 5 10 if so is there an argument i need to set to let r know this thank you reading my post
0,how do you know if you re writing a good code or a bad one hi few months ago i started working on a project that requires me to use r to fetch clean data do some feature engineering i m able to do whatever is required but i m not sure if my code or rather code snippets are good or bad i m not even sure what good or bad means but i ve seen these words thrown around can more experienced people of this sub explain to me what qualifies as a good code thanks
0,what s your approach to developing iterating on models when you have enormous amount of data if you have billions of rows of data sitting on a cluster and you need to develop a model that will then be used to make billions of predictions and it s intractable to develop your model i e run experiments tune hyperparameters compare models on all data because it s too expensive how would you approach developing a model i m curious if people have principled ways of approaching these kinds of settings mine would be take a stratified random sample of the data the stratification should respect the distribution of target labels and any features you consider important the sample should be small enough so that you can feasibly tune the hyperparameters of models you consider and use cross validation rather than simple train test splits once you ve gone through iterations of feature engineering model comparison analysis and identified your best candidate model re train on a larger dataset to plot the learning curve and see how important additional data is depending on how much your model benefits from additional data re train on as large a dataset as possible and use that model in production in terms of tools libraries i d imagine the stratified sampling would be done using spark the model development with libraries like scikit learn and pytorch i realize there are ml frameworks such as sparkml that allow you train models on spark but i feel like they aren t nearly fleshed out enough to support the iterative workflow described above however since these libraries are a lot more efficient you could train on a lot more data thoughts on the tradeoff between more iterations on smaller data vs fewer iterations on more data and for inferencing making predictions i assume there are ways to deploy scikit learn pytorch models in a spark environment
2,struggles when reading ai ml papers hey everyone i was wondering with what are you struggling the most when reading ai ml papers what do you find are the biggest obstacles in understanding those papers and what is your way of dealing with them
2,what best practices are folks using to make sure their data labeling guidelines are effective my team s helping computer vision and nlp companies label training data and it s quite common for team s to not have comprehensive guidelines issue here is that there s often a lag between deciding what type of labels you need and having them prepared while you hash out all the details x200b question if your annotating videos images texts how long do you spend preparing your labeling guidelines and discussing edge cases with your annotators x200b
2,latest tensorflow 2 5 0 optimized wheels with cuda 11 3 for python3 9 i built some wheels for the new tensorflow 2 5 0 with cuda 11 and cudnn 8 in case anyone finds them useful this includes sse4 x avx2 fma instructions i usually build these for skylake march or other architectures on request depending on my availability why is this useful for when you install the official binaries and see a warning like this your cpu supports instructions that this tensorflow binary was not compiled to use avx avx2 in case anyone finding these useful contribute to my coffee addiction ☕ and support these builds and related projects here or or just say hi davidelnunes on twitter
1,regression equation for testing 230 data 200 for creating the regression equation and 30 for testing how can i compare the actual value vs the predicted value and find the error percent will i average the 200 data to compare it to the 30 data and how can i get the error will subtract the 30 predictive values to the average actual values thank you for answering
1,research paper dissertation stock price reactions to climate change protests hi all i m studying my masters degree currently and am undertaking my dissertation on stock price reactions to climate change protests specifically stock price reactions for firms regarded as non environmentally friendly i ve done a literature review and found papers on protests from hong kong affecting the financial markets useful one of the problems i ve found is that climate change protests don t happen sporadically there is no shock due to the planning undertaken beforehand which affects the event window approach as a precise date cannot be used originally i was going to do an event study to regress the data of esg stocks against normal benchmark ftse or s p but my literature review has made me question that methodology would anyone like to weigh in on which method they would recommend to regress the data i am still deciding to do one event or use a range of events from different countries and pulling the results together i find statistics fascinating and before collecting my data wanted to see if anyone had anything to add regarding the methodology in order to make this a great project and paper kind regards james6006
2,paper explained xcit cross covariance image transformers full video analysis after dominating natural language processing transformers have taken over computer vision recently with the advent of vision transformers however the attention mechanism s quadratic complexity in the number of tokens means that transformers do not scale well to high resolution images xcit is a new transformer architecture containing xca a transposed version of attention reducing the complexity from quadratic to linear and at least on image data it appears to perform on par with other models what does this mean for the field is this even a transformer what really matters in deep learning x200b outline 0 00 intro overview 3 45 self attention vs cross covariance attention xca 19 55 cross covariance image transformer xcit architecture 26 00 theoretical engineering considerations 30 40 experimental results 33 20 comments conclusion x200b paper code
0,weekly entering transitioning thread 11 apr 2021 18 apr 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
0,data science projects and reporting results i am part of a data analysis r d team in my company that deals with developing ml solutions for our products large scall manufacturing equipments my background is not data analysis but electronics circuit design the problem at hand is that everytime i do a progress report about the project i am working my team lead expects me to elucidate every little background information of the project before entering into the topic being researched and the results his reasoning is that since he is management i can t expect him to know all these first hand knowledge about the project for example the things he never seems to remember the kind of data we are currently dealing with why we chose the method a to test the kind of parameter we are using to test the model efficiency and why etc it s not like i never tell him these i always explain things when they re brought up for the first time but he wants me to recap these things every single report we do progress reports every week and he wants me to spend the better part of report bringing him upto speed is this normal he handles 3 different project teams am i the one being unreasonable here i want to hear experiences of other data scientists analysis
0,looking for general advice how do you guys handle conflicts in data science projects with respect to peers managers and stakeholders
0,how much statistics ml knowledge does your manager supervisor have i’m about 1 5 years into my first roll out of grad school as a data scientist and i find myself frequently frustrated with my manager’s total lack of even pretty simple stats knowledge for example he has on multiple occasions tried to emphasize effect size by pointing to p value saying things like “this effect size is big and significant and the p value is super small so the effect size is super big ” is this standard for a little more clarity in my team my manager typically plays more of a herding and prioritizing role than actually recommending any sort of method or answers very much a “this needs done more than this” guy and the most hep he can typically offer is industry domain experience
1,name that analysis assume you calculate a rate of change across 1 10 trials of task then compare whether performance on trial 11 is or is not consistent with the trajectory you would predict based on the slope of change across trials 1 10 what is this analysis called someone suggested an analysis like this for a set of data i m working with where after n trials participants get new information on trial n 1 potentially impacting their performance however neither of us can remember what this time of counterfactual inference approach is called making it hard to get appropriate information before setting it up edit we believe my colleague is talking about an interrupted time series design
1,book recommendations for statistical analysis particularly hypothesis testing and analytical engineering hi guys i’m hoping that i could get some suggestions on statistics textbooks i’ve previously worked in data analytics involving energy modelling but i’ve since moved onto a career in engineering aligned with my degree i’m currently seconded to an analytical engineering department but like with my previous job i’m surprised how little statistical theory is actually used in the analysis that is done i’m by no means a mathematician but i’ve studied analytical engineering topics and have probably an above average interest in statistics for my profession one of the reasons i was successful in my previous job was because i managed to show them the power of using more advanced statistical analysis techniques and r based modelling most of which i self taught from various sources basically i want to put some of that into practice again but i need a bit of a refresher and supporting literature since it’s been a while since i did this stuff the topics i’m mainly looking at are hypothesis testing experimental analysis modelling regression or even machine learning doe to a lesser extent reference to r is a bonus too thanks in advance
1,how can i do regression with repeated measurements i have a list of hospital admissions of 396 patients and each patient has gone into hospital more than once i have also got the deprivation index of the area they live in is there a way to model the relationship between number of admissions and deprivation index
0,r datascience discord is there a data science discord would be nice to have one
1,grateful for help with a tricky mathematical problem hi guys i ve been very impressed with some of the answers i ve seen on the sub and was wondering if someone could help with my mathematical problem or at least signpost me in the right direction i did statistics at school about twenty years ago so my skills are extremely rusty the problem is as follows i have inherited a hundred classic watches that i want to trade eventually leaving me with no watches for the maximum profit available i strongly anticipate that there is going to be a spike in the value of the watch market however i don t know when the spike will be how long it will last or how high it will go it is also not clear whether or not i will be able to rebuy the watches once they are sold my problem is therefore to work out a formula that will give me the highest rate of return by selling the bulk of the watches as close to the peak of the spike as possible what i think i need to do is construct two models model a in which i can only sell and not rebuy watches i try and work out the optimal size of batches that i could sell watches for eg 10 watches in a batch and run a monte carlo simulation that takes account of the local volatility of the spike in price the peak and variance of the price movement x200b model b in which i could re buy the watches so that for instance if the watch price goes to 100 each i could sell say ten watches and rebuy them if the price drops to say 50 each in the anticipation that price would rebound above 100 or to sell them at a lower cost if the price continued to fall again i am assuming a monte carlo simulation would yield the best results x200b so how would i solve this issue is this a statistical problem or something more suited to computer programming x200b thankyou in advance for your considered opinion it s driving me mad
2,blind bipedal stair traversal via sim to real reinforcement learning my lab recently released a paper which was accepted to rss 2021 wherein we use rl to train a recurrent neural network to ascend and descend stairs on the cassie robot without the use of any vision or perception and then deploy it to the robot in the real world to climb staircases it s able to use proprioception alone to respond to large steps up and steps down without falling here is a link to the submission video with a few interesting clips of going up down stairs hills and over curbs here is a link to the arxiv preprint for the curious here is an uninterrupted five minute video of the policy climbing and descending a staircase as part of a reliability robustness test
0,what tools language and technologies to learn as a beginner in the field of data analytics i am a beginner and have learned python pandas excel and the basics of hypothesis testing and wanted to know what should i learn and what certifications i should prepare for if i want to get a career in business analytics or data analytics so please can anyone tell me what should i do and also what type of projects i should do
1,does the memoryless property actually apply to grocery store lineups i was reading about the famous memoryless property of the exponential distribution which can be used to approximate the amount of time you need to wait before being served at a grocery store for instance if you have already waited for s minutes and you want to know the probability that you will get served after waiting t more minutes probability getting served at s plus t minutes having already waiting s minutes probability getting served after waiting t minutes my question does this formula actually work in real life just using logic and subject matter knowledge can this formula really be applied to estimate waiting times at grocery stores suppose s 50 minutes and t 30 minutes it just seems unrealistic to me that the probability of getting served after waiting 80 minutes after already waiting 50 minutes can be the same as the probability of being served after waiting 30 minutes can this ever happen in real life thanks
1,what is this r computed from t statistic and df in the last row in the table in r sqrt t 2 t 2 df what is r is it pearson correlation coefficient r why does the relation hold is there some book explaining that thanks
0,datacamp vs edx which would you recommend and why as the title suggests there are a lot of good reviews on datacamp however i ve taken courses on edx before and they are amazing there are a few from mit and ibm etc for a beginner what would you recommend and why
0,are mobile neural nets still relevant is there any good idea of a mobile app which would require on device inference i can t think of any in my opinion mobilenet times have passed now creating mobile efficient models are only useful to surpass some benchmarks a weird flex and that s all in my view it seems all useful ml based mobile apps have already been implemented face recognition image filters sound classification voice recognition is there any need for deploying a ml model on a mobile device and not using an external server api
0,college feels like such a racket but every job requires a degree are there any other options pretty much what the title says i just started college last week but for the past half year of so before that i had been self studying math and programming every weekday by myself for a minimum of 6 hours per day i used to look forward to waking up and being busy the next day i feel like i taught myself a pretty solid foundation in python and made up for my almost nonexistent high school education now that i’ve started college i’m studying lower level cs and stats at the minute all the fun has been sucked out of learning for me i feel like i’ve got no choice but to power through if i want to do data science though given how many data scientists hold graduate degrees am i not seeing other options does it get better in grad school or is this going to be six years of slogging through needlessly repetitive busywork and inefficient learning i barely have time for personal projects anymore because of how long my homework takes to complete
1,statistics question ancova or linear mixed model we have a dv depression measured at 3 time points two ivs one is a binary iv based on therapy time using random assignment the other iv is a continuous variable measured only once called alliance score this measures how well the patient and therapist worked together we want to know whether depression is significantly related to the two ivs again we have 3 measurements for depression baseline t0 3 months t1 and 6 months t2 i am curious what analysis most people would run here i have two thoughts 1 a two way ancova method with no interaction where the dv would be depression in t2 and the ivs would be treatment alliance score and depression at t0 the idea is to see if there is a treatment effect or effect of alliance controlling for baseline t0 depression this would mean not using t1 or possibly doing 2 models where depression at t1 is a dv as well 2 a linear mixed model where time is nested with each person this way uses t0 t1 and t3 for depression as the dv and then the ivs would be treatment alliance score and time an interaction between time and treatment would tell us whether the two groups experienced different rates of change in depression over time i would prefer to the run the first given its simplicity i think either analysis could be justified and want to know what some experienced researchers think
2,trajectory transformer
0,plotly app to sharepoint hi i m a self taught data analyst currently working as an office clerk it s not that bad since i m still a huge beginner and the job leaves me enough freedom to keep learning the main problem is that i must end with the office package that my company uses i d love making some dash app be it only to train my python skills but since i can t find a way to export them in our holy sharepoint i just end up making boring excel dashboards do you have any idea that could work i can t make my own online website since our data must stay protected thanks
2,how interpertable are regression models i was recently reading some articles on the importance of interpertability when dealing with blackbox models blackbox models like neural networks are said to have a very low level of interpertability because they don t allow the analyst to understand why the model is making a certain predictions for an individual observation on the other hand models like decision trees and regression models are said to have much higher levels of interpertability in a general sense i can understand why models like decision trees are interpretable because they literally provide the analyst with a set of fixed rules that explain how to classify an individual observation if you look at a regression model e g salary 5 3 height 2 weight 15 8 age a regression model can allow the analyst to understand how much each variable contributes to the prediction e g in this example age contributes more to the prediction by a factor of almost 8 times and you can also find out how statistically significant each variable is e g indivudal p value of each regression coefficient is this what is meant by the interpertability of a regression model thanks
0,what is the best data analytics product that you have ever used what is the product or tool that analysed or summarised your data in a way that provided you with a memorable user experience eg very useful pleasant easy to use why was that i am interested in products that analysed your data for you such as google analytics for data of your website apple health for data about your sleep apple screen time for data about your device usage etc i am not interested in products or tools that allowed you to analyse your data like ms excel
0,dynamic model that predicts the best next input variable to ask based on the first two three inputs hi i m trying to construct a model that takes in the first two three input values from the user and based on those values decides the best 4th independent variable to ask the user to optimize the time needed to bucket the user into one of the 5 6 different buckets supervised clusters the initial approaches that i was considering were bidirectional lstm rnn s but after reading up on these two i m now thinking there may be more suitable clever approaches to tackles this thank you the deep learning gods of reddit xx
0,what is something you re absolutely not looking forward to once work from home ends and how do you plan to fix it hello i am personally loving work from home because it saves me a lot of time but eventually everyone has to go back to the offices so i thought maybe it s a good idea to talk about the problems we face when working from physical office and how can we make the experience better thanks
0,dank or not analyzing and predicting the popularity of memes on reddit a new study in one of my favorite academic journals internet memes have become an increasingly pervasive form of contemporary social communication that attracted a lot of research interest recently in this paper we analyze the data of 129 326 memes collected from reddit in the middle of march 2020 when the most serious coronavirus restrictions were being introduced around the world this article not only provides a looking glass into the thoughts of internet users during the covid 19 pandemic but we also perform a content based predictive analysis of what makes a meme go viral using machine learning methods we also study what incremental predictive power image related attributes have over textual attributes on meme popularity we find that the success of a meme can be predicted based on its content alone moderately well our best performing machine learning model predicts viral memes with auc 0 68 we also find that both image related and textual attributes have significant incremental predictive power over each other
1,q can we use pearson’s and spearman correlations for binary variables such as a correlation between race and gender
0,i am looking for a personalization project can you suggest what new things we can do with some open source data
2,evil geniuses data and tech in esports event hey everyone my name is sean and i’m an intern at evil geniuses working with the data and tech team we will be hosting a workshop this week 5 19 at 11 am pt 2 pm et focused on the use of data in esports the two panelists who will be running the workshop are soham “valens” chowdhury head of data science and zach kamran head of tech and analytics i have copied the eventbrite link below please free free to sign up the event is completely free if you have any questions please don t hesitate to reach out to me on twitter reegs191 eventbrite
0,the modern way to run notebooks on the cloud i have been working for a startup where we are a team of 5 6 data scientists we regularly have to make decisions regarding our infrastructure requirements and the tools that we would use for our analysis model building we have a powerful ec2 server where we run our computation and data heavy analysis using jupyter notebooks and python scripts however this probably would not scale and we already have situations where the ram runs out and we run code overnight to get some results is there an obvious solution on the modern cloud ecosystem that would preclude the need to have a server and allow us to use compute power and ram as and when we need it preferably within the aws ecosystem
2,eth zurich proposes a robotic system capable of self improving its semantic perception a research team from eth zurich combines continual learning and self supervision to propose a novel robot system that enables online life long self supervised learning of semantic scene understanding here is a quick read eth zurich proposes a robotic system capable of self improving its semantic perception the paper self improving semantic perception on a construction robot is on arxiv
1,question is phd right for me i ve read a bunch of posts here from people asking if a phd is right for them and i ve found that the circumstances of most posters aren t super relevant to me i have a bs in math with minors in stats and information science from a state university graduated with a 4 0 gpa stats was always my favorite subject though i also really liked the intro number theory and coding theory classes i took too i also competed in some university hackathons datafests and won a few for the past 2 years i ve worked as an actuarial consultant in the us the pay is fantastic but i m already feeling burned out from the boring work environment i thought actuarial work was heavy stats but in reality the 90 that all clients actually care about is basic accounting my day to day is something like a baby level data architect data engineer programmer automating away all the analyst level work and some of the statistical modelling work of different actuarial departments the real rub is that i m constantly messing around doing math problems coding challenges reading books doing what i can to feel more stimulated whereas my job demands more focus on accounting regulation i ve been applying to some data science jobs but it seems like the only ones available to me with just a bs would be similarly low level analytics roles anyway i think i chose this job because i was really eager to prove myself financially and i wanted stability as fast as possible with a safe secure job but now i have plenty of savings and my work just feels hollow the only part of my work that i really enjoy is a research project into mortality forecasting that we won from a non profit org i think i like discovering things more than building things but anyway even in this project i can see that typical industry methods of analysis just aren t going to do the job justice the assumptions we implicitly make are all so clearly off base and the more i google the more i realize we ve had better techniques for decades but nobody in our profession has the depth of education or the awareness motivation required to innovate so from where i m sitting learning more statistics and then getting to read new developments in the field researching to add more to the field etc sound like a dream as far as teaching goes i actually ta d a class in undergrad basically did everything lectures website homework exams etc and i enjoyed it so i don t think i would mind that much but part of me is also wondering if this interest of mine is just some grass is greener on the other side bias or maybe i just feel the need to flex now that i see some of my friends graduating from grad school while i really enjoyed writing an undergrad honors thesis i ve never done any actual research before and some of these papers on the topics i love the most seem a little intimidating 1 do you think i d be doing it for the wrong reasons 2 would i even have a shot at a phd program considering it s been 2 years out of school i did no research and i can only be absolutely sure that one prof would remember me and write me a great letter if it matters at all my gre practice scores are great but i don t know if it s enough 3 i keep thinking about trying out a masters program first but i see several drawbacks first they cost a pretty penny whereas phd is usually fully funded with a stipend second if i then get a phd would it be faster or just the same time if the same that seems like a waste of time third many programs stress that the masters in stats is for professionals not academics so i worry that i would not be able to try out real research in a masters program do you think all this is accurate 4 i m not too worried about money but whether it s academia or industry i think i will want a job where i m actually tasked to discover things it seems like stats phds actually do open up some industry doors in terms of quantitative research related roles is that accurate or would a masters suffice for those roles just as easily thanks all advice and feedback is appreciated
2,model that points to specific object in an image hi i would like to train a model that takes as input an image and the label of an object in text form e g apple and outputs a point x y coordinates that is at the center of the object in the image this seems like a sufficiently simple idea that i m hoping it already exists but i m not familiar with any paper proposing it i know there is work on image captioning and object detection but usually those are dealt with models that take just an image as input if the model outputs a bounding box rather than a point this should still be fine since one can easily recover the point from the box bonus points if the model doesn t take a label as input but a sentence about the object i e it takes an image of a car and the sentence this is a car because it has four tires and outputs a point on bounding box around one of the tires does anyone know of existing work similar to this
2,gpt 2 fine tuning 124 m vs 355m can someone please explain me what difference does it make to fine tune gpt2 124m and 355m i get it that 355m was pretrained on larger dataset but when i am fine tuning it how does the model size matter thanks
2,a theoretical review on rethinking attention with performers iclr 2021 oral i make some time to make a theoretical review on an interesting work from choromanski et al 2021 with the title of “ rethinking attention with performers which is presented as an oral paper in iclr 2021 i assume that you have skimmed the paper to focus on their theoretical analyses i hope it helps your reading part 1 preliminaries on attention mechanism generalized kernelizable attention work through of the proofs of lemma 1 and 2 part 2 theorem 2 states that if we choose orthogonal random samplings instead of gaussian random samples the mean squared error is improved with a specific margin lemma 4 gives the inequality for a tail probability using legendre transformation theorem 5 states the orthogonality provides smaller tails where we use a trick with canonical basis vectors to prove this lemma 5 gives an insight into “beautiful things” with gaussian distribution lemma 6 shows the upper bound on the analysis of chi distributions
2,compreface free and open source self hosted face recognition system free and open source face recognition system that can be integrated into any system without prior ai knowledge you may also subscribe to compreface news and updates to never miss new features and product improvements
1,question modelling of continuous data with lots of 0s i m not a statistician but i m having a conversation someone who apparently is i asked specifically whether data that appears continuous with lots of 0s e g hiv viral concentration in a subsection of population most will have concentration of 0 some will have some value could reasonably be modelled with linear regression mle rather than negative binomial or zero inflated models i have a very large dataset the central limit theorem came up in response but i don t see how that might apply anyone able to shed some light on whether linear modelled errors will do in this scenario or do i need to use a better fitting model and how is the clt related
0,neural search i did research on the topic and this is what i learned tl dr neural search is a new approach to retrieving information using neural networks traditional techniques to search typically meant writing rules to “understand” the data being searched and return the best results but with neural search developers don’t need to wrack their brains for these rules the system learns the rules by itself and gets better as it goes along even developers who don’t know machine learning can quickly build a search engine using open source frameworks such as jina table of contents what is neural search evolution of search methods rule based search vs neural search applications of neural search get started with neural search what is neural search there is a massive amount of data on the web how can we effectively search through it for relevant information and it’s not just the web where we need it our computers store terabytes of company and personal data that we need to work with we need effective search to get our day to day job done and what do i mean by effective search can we go beyond just matching keywords can we search using natural language just like we would write or speak can we make the search smart enough to forgive our minor mistakes can we search for things that aren’t an exact match but are “close enough” we can answer all those questions with one word yes to understand how we need to enter the world of natural language processing nlp is a field of computer science that deals with analyzing natural language data like the conversations people have every day nlp is the foundation of intelligent search and we have seen three different approaches in this field as follows evolution of search methods 1 rules 1950–1990s complex handwritten rules that emulate natural language understanding drawbacks handwritten rules can only be made more accurate by increasing their complexity which is a much more difficult task that becomes unmanageable over time 2 statistics 1990s — 2010s probabilistic decisions based on weights machine learning and feature engineering creating and managing rules was solved with machine learning where the system automatically learns rules by analysing large real world texts drawbacks these statistical methods require elaborate feature engineering 3 neural networks present advanced machine learning methods such as deep neural networks and representation learning since 2015 statistical methods have been largely abandoned and there has been a shift to neural networks in machine learning popular techniques using this method make it a more accurate and a scalable alternative it involves use of word embeddings to capture semantic properties of words focus on end to end learning of higher level tasks e g question answering x200b when you use neural networks to make your search smarter we call this a neural search system and as you will see it addresses some of the critical shortcomings of other methods note that the applications of neural search are not just limited to text it goes well beyond what nlp covers with neural search we get additional capabilities to search images audio video etc let’s look at a comparison of the extreme ends of search methods — “rules” vs “neural networks” rules symbolic search vs neural networks neural search while the neural search method has become more widespread since 2015 and should be the primary focus area of any new search system however we shouldn’t completely rule out symbolic rule based search methods in fact using a combination of neural search and symbolic search may result in optimized results let’s look at some of the powerful applications of neural search applications of neural search semantic search 🔍 addidsa trosers misspelled brand and category still returns relevant results similar to query “adidas trousers” search between data types with neural search you can use one kind of data to search another kind of data for example using text to search for images or audio to search for video search with multiple data types with neural search you can build queries with multiple query data types e g search images with text image get started with neural search for rule based searches apache solr elasticsearch and lucene are the de facto solutions on the other hand neural search is relatively new domain there aren’t so many off the shelf packages also training the neural network for such a system requires a lot of data these challenges can be solved using jina an open source neural search framework to get started with building your own neural search system using jina x200b references notes neural search term is less academic form of the term neural information retrieval which first appeared during a research workshop in 2016 i also found it useful to learn about how google search works
1,efficient ways of choosing number of layers neurons in a neural network i have been reading more about the theoretical backgrounds of neural networks e g universal approximation theorem and have seen several authors demonstrate that even a simple few layers many neurons neural network can theoretically approximate the variable of interest i e the response variable to a decent level of precision however the implication being that to use simple neural networks in order to achieve good results this would require a very large number of neurons therefore deeper neural networks have been developed over the years which attempt to provide good results with more layers but a fewer number of neurons this brings me to my situation i have never been able to successfully fit a neural network to any real world data that i have used i have always gotten really bad results with neural networks after trying all sorts of combinations of number of neurons number of layers learning rate activation function drop out regularization etc this seems to be a hyperparameter grid search problem ironically models like cart decision trees have good results on the same data supervised binary classification and random forest has produced even better this data is not small by any means contains around 30 columns and over 300 000 rows of data does anyone know if routines have been written e g in tensorflow keras that can assist in this problem of deciding the number of layers and the number of neurons is there a ground rule for deciding how many layers and how many neurons to begin with is there something around that can intelligently point you in the right direction for how many neurons layers to choose
2,unsupervised ways of comparing the quality of clusters between feature sets i m trying to compare the same clustering technique k means on different feature sets of the same data are there any ways i can perform internal validation on this
2,pricing of ml tools are you paying this much i m looking at some of the tools available to help out with different verticals within ml e g improving the workflow with logging w b comet ml i ve used them personally and they really are excellent but if you re not using it privately or in a small startup their list prices are around 200 usd month user their sales people keep saying that they re looking to solve more of the problems in ml but i m completely uninterested in their other stuff i ve had friends in other companies go through their sales process only to be told in the end that they d have to pay 250 usd user month that seems crazy expensive and makes a pretty big dent for our small but growing team that s yet to prove ourselves i ve seen the same thing for other products as well e g data labelling we d have a very hard time using tools that charge per active user and month in this order of magnitude are any of you in teams where you actually pay for this stuff and if so can you share how much you pay or do you use private accounts or similar
1,question how many random events do have to occure in a year for each day to have atleast one occurance i m no mathematician so i appologize if i don t phrase this right given a class of random events that occur at any given time let s say deaths or births how many such events would have to occur in total in any given year so that we have atleast one occuring each day of the year how do i calculate this this is not school work it s just pure curiosity from a debate i was having with a friend he asked me how many employees should a company have for it to have atleast one death every day of the year and i realized i have not really any idea how to calculate this correctly to rephrase in different words let s say we take the global population and for population of this size someone dies everyday holds generally true my question would be how small the population could get that the assumption someone dies everyday would still hold generally true say to 99 confidence
0,the best data science newsletters that you subscribe for
2,creating the handbook for visionai in production for our projects we were looking for a resource that summarizes the most important concepts for computer vision and focuses on the knowledge only which is relevant for the practical implementation of the concepts it shouldn t explain every theoretical deviation for a hyper parameter but rather provide intuition on how to set it and what happens when you change it we didn t find what we re looking for so we started to create it on our own we spent hours reading through great blog posts forums and just asking our senior ml engineers i m proud to share the first version of our wiki with you today i hope for some of you it ll be as useful as it is for us we re just getting started with the wiki so don t expect it s 100 complete yet but we ll keep expanding it over the upcoming weeks and if you d like to contribute to this handbook for visionai in production please reach out to me we re happy about anyone interested to help also if you spot any mistakes or have feedback please also let us know link wiki hasty ai
0,weekly entering transitioning thread 02 may 2021 09 may 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
1,could someone eli5 p values for me please i’m stuck on an infernal statistics sections in my psych textbook and it’s about p values they are introducing them to us in relation to experiment results and i just can’t wrap my head around it this is what i think that understand so far a p value can help decide whether it’s more efficient to use either the mean median or mode for the central score i think it will also tell if the difference in data between the experimental group and control group falls within the realms of chance or if there’s a probability that the difference was the result of the actions taken to influence the experimental group and here’s where i’m having trouble the problem with using a p value is that the curve ranges from “convincing to unimpressive” i think by convincing they mean that the deviation is probably a result of something done in the experiment another problem “is that we should care about the size of the effect and not wether or not it occurred given a large enough sample it would be possible to even for a tiny effect with no practical importance ” i don’t know why it’s just not clicking for me especially that last line math is not my strong suit i would greatly appreciate if someone could put this in dumber terms for me
2,arxiv miner a toolkit for scraping parsing and searching arxiv research papers since the past year i have been working on research projects around scientific literature mining a by product of the research projects was a small search engine named sci genie today i am open sourcing the core library that runs sci genie arxiv miner is a python library that helps scrape parse and mine research from latex source on arxiv the library also supports elasticsearch on the storage layer and provides hooks to quickly search indexed research records github repo documentation please note this doesn t parse pdf and but i am very happy to have contributors to help make it better i hope this helps anyone working in the field
1,automatic feature engineering during deep learning i have often heard that one of the reasons that deep learning methods are preferred over other machine learning methods is because algorithms like deep neural networks do not require the analyst to spend as much time selecting variables for the model i e feature engineering feature selection feature extraction apparently deep neural networks are able to intelligently in the background consider and create many different combinations of features that are conducive to the modelling problem naturally i was curious about this claim intuitively i understand that through the hidden layers weights and activation functions neural networks are making new combinations of features that are passed forward and are ultimately used for making predictions on new data beyond this i am not sure what to think are there any references papers that have documented either theoretically or empirically that deep neural networks are able to largely take care of the task of feature engineering compared to traditional algorithms like regression models decision trees and random forest have any experiments been done where many irrelevant features were added to a dataset and a deep neural network was able to ignore them
2,regarding training bert from scratch hi everyone this might seem to be a trivial question but any advice is appreciated while training bert from scratch makes sense why do we not use pretrained embedding as an initialization point for the model instead of training it from scratch i can understand why they didn t do it in first paper since they were trying to understand model effectiveness but doesn t it make sense now to see if these initialisations can do better given we have multiple variants i might be skipping one or more papers here feel free to correct me secondly if they don t work do we have an intuition for it if somebody has tried this and if it didn t work do we have an intuition for this is there a study for the inherent structure between the matrices that can tell about the similarity or difference in the matrices at different layers
2,project for my masterthesis first i’ll explain what my thesis is about it’s about quality control in car manufacturing currently a person with a tool needs to go around a finished car and needs to measure all the gaps between the metal sheets of the bodywork of the car now i want to write in theory if this can be automated with ai cameras are on the assembly line and take photos of the car while it passes by the photos are analyzed and checked if all the gaps are perfectly wide and parallel i’m aware that ai is not strictly necessary laser scanning 3d scanning of the car could do the same job i’m writing if the ai is helpful and can do it better or more precise my masterthesis is not coding this it’s just writing about it i want to research all the needed components though so someone could easily code it after reading my thesis i do all the research so ideally i need a dataset of car bodies or other objects like furniture where gaps are visible but i’m doing just prototyping so an ai that can just recognize lines and tell if they are parallel is enough thanks a lot if you have any questions feel free to ask if you have help comments criticism please tell me
2,what is binarization in the context of nlp and fairseq library i find the developers in fairseq library recommending the binarization of text before inference or training however i looked over the internet and i didn t find a clear explanation of what binarization is
0,what do you use to plot your multiple regressions see title i ve created a nice and simple multiple regression which works as intended x multi df drug offences public order offences y multi df homicides regr linear model linearregression regr fit x y y pred regr predict x i had a look online to see what was recommended for plotting multiple regressions but nothing stood out to me what do you use to plot your multiple regressions
2,how does adam fix problems from stochastic gradient descent i found this link over here that discusses the problems of stochastic gradient descent and various extensions of gradient descent e g adam which can be used to fix some of these problems there are newer extensions of gradient descent that use concepts like momentum that can better guide the neural network in finding out the minimum of the cost function are there any mathematical justifications that show why regular gradient descent is prone to get stuck and how adam is able to resolve these problems or is all this just based on empirical observation
1,weighting survey data for propensity score matching hi everyone i am using stata to analyze a pooled cross sectional dataset a certain percentage of observations are from simple random sampling but the rest comes from stratified sampling as they tried to oversample household living in areas with certain characteristics i have a dataset that oversampled certain areas since the dataset does not include a clear strata variable i created one by converting the oversampling indicator variable characteristics 1 1 characteristics 2 2 etc in short strata is areas prone to a certain kind of disaster did i create a correct strata variable after i use the svyset command svy works fine with common commands like regress and logit however it does not work with teffects psmatch teffects psmatch does not allow for the pweight option either i am lost and don t know what to do any advice will be appreciated
1,inductive biases in machine learning can someone please try to explain the concept and role of inductive biases in machine learning are inductive biases very basic and general assumptions required for machine learning algorithms to work e g birds of the same flock fly together unseen data can be predicted based on how similar it is compared to seen data
1,is there a name for using the difference between separate regression models for control and treatment to estimate causal effects this seems like a fairly intuitive approach to causal inference but i have never run across it and i am wondering why assume you have two regression models y1 e y a 1 x and y0 e y a 0 x where y1 is fit only on the treatment group and y0 is fit on the control group to get the average treatment effect you could imagine computing y1 y0 for everyone in the whole population and averaging this would be very similar to a standard regression except that the coefficients for x could be different between the two is this a technique that exists and has a name already
0,what are some tools best practices that causal inferencing teams use for experimentation would love some advice recommendations from causal inference data scientists we are trying to look for tools frameworks platforms that can help boost the productivity of data scientists in a causal inferencing team right now the data scientists are just doing their experimentation on ai platform notebooks but would like to try to standardize their methodology automate processes whenever possible and track their experimentation i believe the current workflow is 1 ds writes sql query to pull in treatments outcome variables data from snowflake 2 ds uses econml or dowhy libraries to get the causal estimate statistical significance etc 3 ds tracks experiments and different variables used on mlflow i m sure this current workflow can be greatly approved and was wondering if there are some established industry best practices that we can learn from with regards to causal inferencing and experimentation also we would love to leverage any open source tooling you would recommend that would help in this domain
2,simulated adversarial testing of face recognition models
0,the data cleaning vs analysis conversation seeing posts threads on this topic and i have a hot take the cleaning is the more interesting of the two so many posts on this topic and all seem to have an underlying premise that data cleaning sucks and that the modeling is what s interesting fun but the cleaning takes up so much time precisely because it s very challenging ambiguous it s the part of the process i think will be last to be automated if it ever is i get that modeling can provide deep insights and or value and so are certainly rewarding but i really think many of our conversations on this topic miss the point mapping the real world to noisy data is an inherently ambiguous task best to embrace that if it were otherwise there d be a lot less demand for data professionals in the first place
2,advice on dodging grad school poverty does anyone have any advice for how to make decent income while earning one s phd in ml i m open to all suggestions consulting year round company funding blogging etc personal experiences would be most appreciated
1,estimating parameters of a categorical distribution that changes over time i m modeling a scenario in which a subject is asked to choose between k different options repeatedly over time if it were the case that this subject s preferences didn t change over time i e the observations are iid i could model the probability that they choose each of the options as a categorical distribution with parameters p 1 p 2 p n and use bayesian estimation with a dirichlet prior to estimate these p i parameters or even just use the estimate p i times i was chosen observations however suppose that the subject s preferences can change over time if we had 1000 observations the first 10 observations may be less relevant in estimating these changing p i s than the last 10 observations my first instinct is to create some sort of a weighted model in which earlier observations have less impact on the estimate of the p i than newer ones an example would be to have a rate γ at which observations decay so that p i sum γ n t i x t i sum γ n t where each index variable t goes from 1 to n where n is the number of observations and i is the indicator function however i m not sure that this is necessarily the best way to go about this is there any literature on this type problem or related statistical techniques that could be useful am i even approaching the problem the right way i appreciate any pointers edit to be clear i am primarily interested in predicting x n 1 given the observations x 1 x n
2,python package for a r cran package hey everyone i am doing forecasting for retail data used custom model as forecasting model but now the main problem is updating the other lower level data which is called forecast reconciliation in r it was recommended to use foreco which has different reconciliation algorithms so if anyone found similar package in python else i am thinking of rewriting it in python in free time link to r package foreco thank you for helping out
1,question can i remove a significant variable from a regression model i have an assignment to build a regression model then analyse it etc now for my starting model i used all of the variables and got that they were all significant at 1 i doubt that would be it since i don t think we would get a dataset for this assignment where the basic model is perfect so my question is if i see that there is multicollinearity between variables would it be methodologically correct to remove them even if they are significant and then test if the new model will show better r 2 bic values than the starting one or if a variable is significant it shouldn t be removed at all
2,is supervised binary text classification considered solved given a sufficiently large dataset with binary labels based on a reasonable arbitrary human concept are we at a point where solving that is no longer considered to be of research interest and only interesting from an engineering and domain specific perspective
0,what server configuration should i get for organisational data analytics applications i head a bi department for a startup well not much of a startup we are almost a small enterprise now currently i run a suite of bi applications from my machine my main applications that are run regularly are ms office alteryx database postgres python power bi tableau its quite capable for what it does however we are looking to make the access for these tools organisational and hence we are going to invest in a server where all these applications will reside can the community please help me with the best server configurations for running the above applications seamlessly of course we are looking to have windows server as an os
2,coatnet marrying convolution and attention for all data sizes abstract transformers have attracted increasing interests in computer vision but they still fall behind state of the art convolutional networks in this work we show that while transformers tend to have larger model capacity their generalization can be worse than convolutional networks due to the lack of the right inductive bias to effectively combine the strengths from both architectures we present coatnets pronounced coat nets a family of hybrid models built from two key insights 1 depthwise convolution and self attention can be naturally unified via simple relative attention 2 vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization capacity and efficiency experiments show that our coatnets achieve state of the art performance under different resource constraints across various datasets for example coatnet achieves 86 0 imagenet top 1 accuracy without extra data and 89 77 with extra jft data outperforming prior arts of both convolutional networks and transformers notably when pre trained with 13m images fromimagenet 21k our coatnet achieves 88 56 top 1 accuracy matching vit huge pre trained with 300m images from jft while using 23x less data
2,ai researchers from mit lincoln lab developed rio reconnaissance of influence operations system that would counter the spread of disinformation by making use of machine learning disinformation manipulates accurate information deliberately to mislead the masses and the spread of such information is not new it has been in practice for quite some time right from the imperial war propaganda and now in this digitalized world social media has brought with it its perils and one of them is its usage to spread false information it has the power to change opinions altogether for example the entire dynamics of the public elections however it is now being claimed that artificial intelligence systems could efficiently detect and simultaneously counter the spread of this disinformation on digital platforms the reconnaissance of influence operations rio program built at the mit lincoln laboratory promises to do just the same it would automatically detect and analyze all the social media accounts used to spread disinformation across a network paper
1,r language loading library i ve recently started using r for a new project because of an interested pacakage called cmaverse for mi applications a lot of issues came up as i was installing it according to instructions i received a lot of missing dependencies that this library requires and when i tried installed some requirements those reqs gave more missing dependencies etc is it supposed to be like this or am i possibly doing something wrong any suggestions would be appreciated running r using vim r plugin on manjaro linux not sure if that has any effects
2,new models announced in google i o 2021 lamda next generation conversational ai agent mum an encoder decoder language model model that can understand images and is trained on 75 different non english languages trained on more conversational data to improve the search queries this is like t5 but more powerful sounding more like a human and can do downstream tasks in non english languages without the need to fine tune it can also understand images opinions
2,adversarial reprogramming of neural cellular automata link authors randazzo e mordvintsev a niklasson e levin m opening paragraphs in a complex system whether biological technological or social how can we discover signaling events that will alter system level behavior in desired ways even when the rules governing the individual components of these complex systems are known the inverse problem going from desired behaviour to system design is at the heart of many barriers for the advance of biomedicine robotics and other fields of importance to society biology specifically is transitioning from a focus on mechanism what is required for the system to work to a focus on information what algorithm is sufficient to implement adaptive behavior advances in machine learning represent an exciting and largely untapped source of inspiration and tooling to assist the biological sciences growing neural cellular automata and self classifying mnist digits introduced the neural cellular automata neural ca model and demonstrated how tasks requiring self organisation such as pattern growth and self classification of digits can be trained in an end to end differentiable fashion the resulting models were robust to various kinds of perturbations the growing ca expressed regenerative capabilities when damaged the mnist ca were responsive to changes in the underlying digits triggering reclassification whenever necessary these computational frameworks represent quantitative models with which to understand important biological phenomena such as scaling of single cell behavior rules into reliable organ level anatomies the latter is a kind of anatomical homeostasis achieved by feedback loops that must recognize deviations from a correct target morphology and progressively reduce anatomical error in this work we train adversaries whose goal is to reprogram ca into doing something other than what they were trained to do in order to understand what kinds of lower level signals alter system level behavior of our ca it is important to understand how these ca are constructed and where local versus global information resides
1,control spc charts question non normal distributions and time series i am trying to measure process time in relation to material that is being processed by various different machines at the company i work at i may also be interested in doing the same but with the combined wait time before moving on to the next machine i e dwell time in almost all cases there is a non normal positive skewed distribution so my first question is how do i handle this without further transformation in the context of spc charts first because the average lay person will be confused when they see numbers that don t represent the actuals and second because most text book examples assume the underlying data is normal what choice of spc chart should i pick and should i only show the upper and lower control limits given that the data is not normal i read somewhere that an individual chart with only these two control lines accommodates non normal distributions relatively well my other question is how and what kind of spc chart might offer the ability to see process time month by month some further questions here might be around sampling grouping etc thanks
2,episodic curiosity eco with flat observations i m trying to solve a simple 2d maze with eco by using ppo s mlppolicy the environment is simple a mujoco maze with actions space of two float from 1 to 1 x and y force and continuous observation of 4 x y v x v y the algorithm cannot reach a good policy has someone a suggestion or experience with this
0,creating a data warehouse and dashboards for my business hi i m looking to set up a new centralised system for aggregating all the data my business has and then perform further bi queries with it including building dashboards that inform of things i need to know here are some of my concerns i have had troubles with many big name cloud players in the past such as facebook paypal aws where they disabled my business accounts rejected without reason common issue all around even if you re paying them and google is no exception not even doing anything controversial nor are we in any high risk industries their algorithms just act up i m mostly skeptical about anything cloud where i don t control it directly and the partner can cut you off anytime so i lean towards on prem unless you can convince me that cloud data warehouse providers are different there s also the cost of setting it up so a set up and then a disabling later on is far too costly what guarantees can one get any legal contracts or terms what on prem data warehouse stacks do you suggest for smaller volumes not doing big data but etl functionality would be nice to have what analytics and bi softwares are flexible enough i used to use sas eg in my earlier days to cut data and that was the best for me so far like excel for data warehouse i would say but am wondering if there are better i tried metabase and grafana and they don t query well without much more tweaking haven t tried tableau but am wondering about their costs prefer open source but licensed software can be considered if costs are suitable was looking into apache but only briefly small business so nothing too fancy and enterprise grade prefer web dashboards so i can view it anywhere prefer gui where possible and minimal code unless it s for a special custom query data ownership and control over the system is important lots of ground to cover here but i appreciate all your opinions and how you unleash your passionate idea of an ideal setup d
2,graph embeddings of wikidata items i m trying to use pytorch biggraph pre trained embeddings of wikidata items for disambiguation the problem is that the results i am getting by using dot or cosine similarity are not great for example the similarity between the python programming language and the snake with the same name is greater than between python and django does anybody know if there is a wikidata embedding that results in better similarities x200b wiki item 1 wiki item 2 dot cosine q28865 python language q271218 python snake 17 625 0 64013671875 q28865 python language q10811 reptiles 8 21875 0 300048828125 q28865 python language q2407 c 25 296875 0 919921875 q28865 python language q842014 django python 11 34375 0 409912109375 q271218 python snake q10811 reptiles 11 25 0 409912109375 q271218 python snake q2407 c 12 5390625 0 4599609375 q271218 python snake q842014 django python 6 05859375 0 219970703125 q10811 reptils q2407 c 4 76171875 0 1700439453125 q10811 reptils q842014 django python 0 60009765625 0 0200042724609375 q2407 c q842014 django python 11 53125 0 419921875
0,how would you go about building a content based recommendation system with reinforcement let’s say we’re a company like spotify and want to recommend songs to you based on songs you’ve listened to say we have a constraint that we don’t want to funnel all of our impressions to a small of songs which is likely to occur with user based collaborative filtering but rather would prefer broad coverage of viewership across our whole library we want this to be personalized to each user and we want to iteratively learn as you continue to listen to new songs how would you go about building this architecture what does it look like what models do you opt to use everything i’ve thought of seems to have some sort of hold up so i feel i’m missing something i assume we can’t train an rnn for every user due to likely limited sample data they’ve only listened to so many songs themselves and computational cost of maintaining hundreds of thousands or millions of nns traditional content based recommendations using like simple cosine similarity may not be able to capture some of the more complex nonlinear relationships without exceptional upfront feature engineering e g i like electronic songs but only if they’re between x and y bpm with female vocalists what am i missing do we have a good solution for this type of problem
2,can someone please explain what the white color shades mean in this picture these pictures are supposed to show the decision boundaries of different machine learning algorithms on a binary classification task there are two classes for the response variable red and blue shouldn t all the decision boundaries either be fully red or fully blue what do the shades of white mean does this mean an overlapping decision boundary thanks
0,plotting in r s ggplot2 vs python s matplotlib is it just me or is ggplot2 way smoother of an experience than matplotlib i came up in the space using r for ad hoc plotting and eda and i d like to check to see if it s my home base bias warping my perception or if matplotlib really is a more cumbersome experience for plotting in my experience ggplot2 s chains make plots easy to manage in the code functions corresponding to plot elements are simple and take care of all of the customization i could want matplotlib on the other hand makes me feel like i need to write whole separate programs to build and style my plots am i missing something in matplotlib that makes it especially powerful for plotting
2,discussion correlated outputs from deep learning mutli output regression i m dealing with a problem at work right now that involves taking a non linear signal generated from 5 continuous variables and training a denoising autoencoder to reconstruct the signal signals were generated by randomly scanning over the vector space all variables are normalized between 0 and 1 and their histograms are flat across this space the main goal is to ensure that the latent representation of any signal is equal to the 5d vector that generated it i enforce this constraint through a simple weighted sum of two mse loss functions loss alpha mse 5d label latent embedding 1 alpha mse signal reconstructed signal i am able to obtain great reconstructed signals but i find that i always obtain some type of correlation between the latent neurons as seen in the correlation matrix attached furthermore i find that the same feature always becomes mis represented scatter plot attached shows each feature plotted against its embedding along with the r2 value in the legend does anyone have any intuition as to why this would be occurring i ve even tried overfitting the model to absolute hell and i cannot for the life of me get close to perfect predictions i ve also tried using a simple small dense nn for this and still no luck any insight would be insanely appreciated
2,why is the baseline for fairness and bias zero when we talk about ml models accuracy and other metrics nobody cares whether they are better than random or not i am exaggerating a little bit what people care about is whether the model is better than humans performance for some reason this doesn t seem to be the case for bias and fairness whenever people talk about controversial potential algorithms for instance ai immigration officers or ai hiring such as hirevue people get very defensive and say that the algorithms are going to be biased of course but are they going to be more biased than humans i highly doubt that i bet an ai wouldn t ask me to open my luggage so frequently after i land in london just because i have a beard and am not too white in any case i don t understand why the baseline for fairness bias for ai algorithms is zero and not human performance now i recognize that the expectation of ai from users e g interviewees from hirevue are much higher than of humans and they might also feel more uncomfortable so yes i do see why we want a fairer less biased ai than humans but the conversation needs to go deeper than this there are costs and benefits to introducing ai if it s more accurate than humans less biased than humans and cheaper than humans chances are it should be used my point is simply saying ai is biased and putting a period in my opinion is not the right way to go about it a human is also biased and in many cases significantly more biased than the ai
2,a practical guide to counterfactual estimators for causal inference with time series cross sectional data this paper introduces a unified framework of counterfactual estimation for time series cross sectional data which estimates the average treatment effect on the treated by directly imputing treated counterfactuals its special cases include several newly developed methods such as the fixed effects counterfactual estimator interactive fixed effects counterfactual estimator and matrix completion estimator these estimators provide more reliable causal estimates than conventional two way fixed effects models when the treatment effects are heterogeneous or unobserved time varying confounders exist under this framework we propose two sets of diagnostic tests tests for no pre trend and placebo tests accompanied by visualization tools to help researchers gauge the validity of the no time varying confounder assumption we illustrate these methods with two political economy examples and develop an open source package fect in both r and stata to facilitate implementation liu licheng and wang ye and xu yiqing a practical guide to counterfactual estimators for causal inference with time series cross sectional data june 21 2020 available at ssrn or
2,machine learning wayr what are you reading week 114 this is a place to share machine learning research papers journals and articles that you re reading this week if it relates to what you re researching by all means elaborate and give us your insight otherwise it could just be an interesting paper you ve read please try to provide some insight from your understanding and please don t post things which are present in wiki preferably you should link the arxiv page not the pdf you can easily access the pdf from the summary page but not the other way around or any other pertinent links previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 week 1 11 21 31 41 51 61 71 81 91 101 111 week 2 12 22 32 42 52 62 72 82 92 102 112 week 3 13 23 33 43 53 63 73 83 93 103 113 week 4 14 24 34 44 54 64 74 84 94 104 week 5 15 25 35 45 55 65 75 85 95 105 week 6 16 26 36 46 56 66 76 86 96 106 week 7 17 27 37 47 57 67 77 87 97 107 week 8 18 28 38 48 58 68 78 88 98 108 week 9 19 29 39 49 59 69 79 89 99 109 week 10 20 30 40 50 60 70 80 90 100 110 most upvoted papers two weeks ago u dl updates intriguing properties of vision transformers u au1206 besides that there are no rules have fun
0,how many hours of actual work do you do everyday hi i was just wondering if i was on the low side of number of hours people work a day i talked to a friend who works at amazon and they said that they do 8 hours of work by work i mean when you re sitting on your desk and doing stuff not including the meetings although i understand meetings are also part of work i realized i do maybe 4 hours of actual work rest is just thinking about some stuff for work lunch break etc it s hard to imagine how can someone just sit and do 8 hours won t they be burnt out how many hours do you put in thanks
0,happy pi day 🥧
2,ieee publishes comprehensive survey of bottom up and top down neural processing system design an ieee team provides a comprehensive overview of the bottom up and top down design approaches toward neuromorphic intelligence highlighting the different levels of granularity present in existing silicon implementations and assessing the benefits of the different circuit design styles in neural processing systems here is a quick read ieee publishes comprehensive survey of bottom up and top down neural processing system design the paper bottom up and top down neural processing systems design neuromorphic intelligence as the convergence of natural and artificial intelligence is on arxiv
0,first two weeks of my first internship today i got my first paycheck from my first internship and i am shocked about the entire situation i come from a poor family i am the first of my family to college and grad school and the first to have a real professional work experience i honestly feel blessed to be able to improve on my data science abilities and get paid for it i have been working with the lead data scientist and have learned so much in these past two weeks i enjoy coming to work and even more so now that i saw the paycheck sorry for the weird post but i am just in a good mood right now p s my boss asked me if i want to continue my internship for the fall
0,little analyst in a big data pond hey folks i’ve been working as a sales data analyst for the past two years just kind of worked my way into this role despite the fact that my education is in the soft sciences as in some light stata use the problem is that now i’m the crm admin and we’re about to go from a company of 500 people to 5000 people thanks to a dozen or more acquisitions coming up i look at our infrastructure and all i see is a looming train wreck most of these companies keep their sales data on excel files in a shared drive some have salesforce my current company does not my goal is to best prepare myself and my company for these small but numerous data integrations i want to keep things organized and have a clear input output system for all data sources and then of course i need a way to easily access all of these to compile company wide kpi reports reading the wiki i think this counts as “data science” and i think i should start with learning r anything else to add
1,does it makes sense for my null hypothesis to be mean 2 5 and alternative hypothesis mean 2 5 i ve read that the null hypothesis will definitely have a form of equality but if the case above is possible is it possible to do a t test with 2 5 as the test value
2,gan training aren t the double discriminator calls wasteful i ve noticed that a lot of gan codebases runs the discriminator d on the gen d data once for training the generator g and then again on the same data for training d i guess this is mainly as a convenience because g and d s losses have opposite signs is there another upside or do we accept 50 ish extra processing time just to avoid a small hassle two ways around it i can think of 1 use a minimizer on g and maximizer on d a k a negative learning rate 2 somehow flip the sign of the gradient when backprop ing from d to g
1,question about this chi square test result hi my friend is in an entry level statistics course and they had a chi square question on the exam usually when someone comes to me saying i think the professor is wrong here i take it with a grain of salt but i actually think the professor is wrong about this i just want to make sure before i tell her to argue the point here is the question on the exam and the work she did this has already been submitted and graded as you can see from the professors note on the first picture the professor says that this chi square should be 3 76 however my friend says it should be 2 74 she says that the difference in the answer comes from the fact that the professor gets the expected number of people in each cell by simply dividing the row totals by 2 so the expected number of people over 40 who like chocolate would be 90 2 in the professors version i think this clearly isn t right
0,how to use product matching to create product bundles i am working on a product matching model goal a store has many products like creams perfumes other beauty products based on product properties i have to cerate bundles of it so we can sell more product at once while giving a small discount to the customer i can not make a project based on collaborative filtering because the goal is to have similar styles matched together ex 1 hugo boss perfumes and creams 2 summer lipstick collection any brand that is what is our customer base is all about that we have big sales and nice discounts currently these products are matched manually project plan 1 product matching clustering algorithm unsupervised learning question 1 1 it has about 70 metrics for product type lipstick cream perfume at unsupervised learning clustering should i a label encode this or b one hot encode it question 1 2 what unsupervised learning algorithm you recommend if i don t know how many products groups ther should be we have currently 4000 products i am currently using agglomerative hierarchical clustering question 1 3 how can i get an all product x all product matrix with 1 number in each matrix cell that defines the closeness of the 2 compared product question 1 4 what should be x and y axis of a plot when i want to see product groups like in simple k means clustering i have more than 70 product properties 2 salles representative want to define some parameters of the products price product types my goal if they pick 1 product they copy the product id to the cell that an excel sheet looks up for them that they want to define things like product type quantity price margin review score at least 4 5 star on our site wishes it is on people s wishlist ex the sales person can filter it that he only wants lipsticks in an excel sheet so he get the closest matching lipsticks 3 supervised machine learning regression predicts sales based on previouse product bundles that the salles representative created even before this algorithm
1,has anyone heard of the stein paradox in statistics there is a famous statistical paradox discovered by the researcher stein which is said to describe a very paradoxical phenomenon popular articles have appeared hailing the james stein estimator a paradox one should use the price of tea in china to obtain a better estimate of the chance of rain in melbourne is this really true can you really use anything to estimate anything does this work for quantities with completely different units e g years vs micrograms and the more important question if the stein paradox is true why don t weathermen in melbourne use the price of tea to predict the weather as far as i know they only use meteorological satellites thanks sources
1,is there any downloadable or browser program which can make more detailed stats so recently we won a tournament and i want to analyze the data mark up the bad rates what was the distribution etc is there a good site program what not relies on coding to do this rather than using excel
1,what s the correct test to analyze my data hello i would like to know your opinion about what test s i should use to analyze the data from my experiment i want to compare the differences in a specific exercise between two different sub phases of the menstrual cycle let me know if you need more details i am a mres student with limited statistical knowledge trying to learn sample size 5 subjects performed the exercise in both sub phases of the menstrual cycle exercise protocol 5 sprints with a short recovery 6 sprint 24 recovery design every subject had to perform the exercise protocol in two different phases of the menstrual cycle in randomized order someone started in phase 1 someone in phase 2 before this they all did a familiarisation session parameters 1 sprint parameters those are parameters analysed during after every sprint and therefore for each exercise session i have 5 values for example power velocity 2 anthropometric data parameters collected once before the exercise protocol height body mass circumferences 3 post exercise parameters collected immediately after the exercise 3 minutes after and 5 minutes after the exercise ended x200b what tests would you use to compare familiarization sub phase 1 and sub phase 2 in these parameters my supervisor suggested a two way anova with rm for the sprint parameters and the post exercise parameters with a tukey s multiple comparison to then see if there are significant differences between sub phase 1 and sub phase 2 due to a few missing data the software graphpad suggested to use a mixed effects model restricted maximum likelihood reml to take care of the missing data for anthropometric data a one way anova with rm and tukey s multiple comparison was suggested x200b do you agree with these choice i think the idea is correct but i would use the non parametric versions of these 2 tests i would use friedman instead of the one way anova with rm for example but i am not quite sure what to do with the two way anova any help
1,question conditional probability of several variables i am wondering what is the joint probability of several variables that do not follow each other as in the following way p a b c p b c p c p a b c in other words i wonder what would be the joint probability of p a b p c d is it going to be p a b c d on the other hand if we have a case such as p a p b c and p a c p b c i think that they are going to have different joints but i am not sure how can they be written differently in a joint distribution notation my question partially arises due to bishop s pattern recognition textbook where he expresses p w a b t t p t w b p w t a b p a b t i interpret this as first multiplying the last two terms and getting p w a b t which leads to p w a b t then multiply that with p t w b however i am not sure how can we multiply two conditionals namely p t w b and p w a b t given that p t w b does not contain after the given notation all of the terms from the p w a b t before the given notation any help is much appreciated
2,facebook transfer learning method boosts code autocompletion accuracy by over 50 a research team from facebook shows how the power of transfer learning can enable pretraining on non ide non autocompletion and different language example code sequences before fine tuning on the autocompletion prediction task to improve model accuracy by over 50 percent on very small fine tuning datasets and over 10 percent on 50k labelled examples here is a quick read facebook transfer learning method boosts code autocompletion accuracy by over 50 the paper improving code autocompletion with transfer learning is on arxiv
0,starting my masters in ds this fall any advice on my first class after much thought i have decided that getting a masters in data science is the correct career choice for me i begin my studies this fall and my first class is python for data analysis does anyone have advice they’d like to share i know the basics of python and i took an online udemy course for python with data analysis back in 2019 my goal is to maximize what i learn in this course because i’d really like to increase my use of python for data science and just in general
1,what sort of things would you try to get on your cv if you wanted to make a statistics phd application context right now i have an undergraduate degree in mathematics since graduating i ve been working as a programmer but i have an offer to do a master s degree in statistics next year i m thinking i d like to do a phd after my master s but i m a bit worried that i would have no research experience and generally not much to put on a phd application compared to other candidates any thoughts on how you can put yourself into a good position for when it is time to start making applications for example would it be prudent to start working on my own statistical projects i don t really see how else it would be possible to get research experience thanks in advance
0,can somebody give an example of their model being audited i keep hearing concerns about reproducing old models while i might see the reasons behind rolling back due to technical needs some bring forward auditing reasons so i m interested in who gets to audit your models what does that process look like what do you need to present what are they specifically looking for thanks
2,feasibility of building a reliable gun detection model discussion i m trying to understand the barriers to building a computer vision model for detecting a gun in a person s hand in real time i ve played with a dozen or so repos for gun detection and they all share the ability to detect a gun held directly in front of the camera ideally in profile however they perform poorly when tested with a typical security camera setup is building a reliable model to detect a gun in hand possible and how would you approach it would you optimize training data for a consistent camera angle and distance ie 45 degrees and 20 would you include pose detection finally is there a framework that lends itself best for this application ie yolo v4 x200b
0,how do you save and manage code for reuse hey everyone after doing online courses i m working my first guided project and i hope that i can transfer to unguided ones in anaconda soon my current project for practising is guided i find myself not remembering some solutions which i had in the courses eventhough i know i ve solved a similar issue before i know it s common to have some kind of library for code for different problems how do you store and manage these i was thinking to maybe make a onenote windows while studying but i can imagine there are better ways especially when i switch to linux i will need a new solution
0,have any of you taked the qualified sql challenge as part of interview process for zoom i got this assessment today and just wanted to get an opinion on the difficulty level is it similar to leetcode sql easy medium hard or is it a totally different level trying to get a feel for what to expect in the 75 minute period tia
1,is there a way i can run data through something that will give me many many different statistical model results i m only familiar with people able to get 1 result out from something at once i use r but i ll learn something else or use a different tool i d want to see if i can run data through something and let it crunch numbers for a bit and give me info on all sorts of different models
1,the state of the job market in bay area i saw the post today where a lot of people with ms in stats share their stories but i can t help but notice that all of those stories are from the people with 2 years of experience what is the situation on the job market rn i have a feeling that ds positions are flooded by from zero to hero and cs l know a bit of stats people biostats jobs are reserved for biostats people or am i in a bubble and that s not a case at all i have an ms in statistics from san jose state university and a ds internship in bay area recruiters automatically assume you are proficient in sql and python i am and that s all they are interested in no one asked me a single stats question so far is someone interviewing right now what are your experiences
0,what is the hardest topic that you encountered in data science when you studied it when i studied ds for mastery i noticed that people came from different backgrounds consequently people were struggling in different classes according to what they studied before for example people that did statistics as bachelor mentioned that they had a lot of problems with the programming aspect of ds but were much more comfortable with modeling i had a lot of trouble with bayesian statistics but was much more comfortable with programming i was a software engineer before so out of curiosity what was your experience like what topic did you find difficult and which ones did you find easy
1,could i use reweighting to turn a cross sectional dataset spanning multiple cohorts into a simulated lifetime earnings model for a single cohort say i have a cross sectional survey with demographic characteristics and wages of people across different cohorts could i group people by cohort and reweight the relevant demographic variables towards cohort a to create a simulated lifetime earnings model for this cohort
2,boosting monocular depth estimation models to high resolution via content adaptive multi resolution merging x200b paper project page
1,do principles of sufficiency and likelihood mean functional dependency casella and berger s statistical inference lists formal sufficiency principle consider experiment e x theta f xltheta and suppose t x is a sufficient statistic for theta if x and y are sample points satisfying t x t y then ev e x ev e y does it mean that ev e x h e t x for any x for some function h formal likelihood principle suppose that we have two experiments e1 x1 theta f1 x1 theta and e2 x2 theta f2 x2 theta where the unknown parameter theta is the same in both experiments suppose x1 and x2 are sample points from e1 and e2 respectively such that l theta x1 c l theta x2 for all theta and for some constant c that may depend on x1 and x2 but not theta then ev e1 x1 ev e2 x2 does it mean that ev e x g e l x for any x for some function g
1,is the following statement about statstics correct a friend told me that all analysis in statistics including machine learning algorithms supervised learning unsupervised learning reinforcement learning hypothesis testing bayesian methods regression models survey sampling time series clustering etc can either be considered as causal analysis and counterfactual analysis is this statement somewhat correct
1,computational costs of machine learning algorithms can anyone recommend any sources that discuss why kernel methods e g svm are computationally cheaper compared to neural networks
2,ml basic technology i had this idea of creating low impact ml inventions that are like the most basic way to use ml in every day life eg a ml door a ml window a ml chair etc what do you guys think about this
2,life expectancy of a 1080ti buying decision used 1080ti or rtx 2060 it goes without saying that gpu prices are insane right now but sometimes we still have to spend i m setting up a machine to train cnn and lstm models in keras and need to buy a gpu but budget constraints limit me to about 600 and i m a bit torn between getting a new rtx 2060 or a used gtx 1080ti speed wise i don t think there s any question that the 1080 is the better choice but aside from being more energetically expensive i am also worried about how much more life i am able to get out of a used card physically i d imagine that a large number of 1080ti s in the used market may have come from cryptominers who decide to retire these cards for whatever reason and while capacitors and fans can last a while are used miner cards going to be a significant problem in a machine that will inevitably also power the cards nearly 24 hours a day 7 days a week i guess i can avoid the issue of getting a secondhand miner card by only looking for private sellers and going by their words instead of going through big box stores but then i lose the safety net of being able to return the card if it s a clunker although honestly it s not like i d be able to tell if a card is going to die suddenly within a week right that and the aspect of not having tensor cores make me worry about how future proof the 1080ti will be well what would you do i guess buying a used rtx 2060 is an option but retailers are slowly restocking their shelves and new 2060s can go for less than used ones
1,mann whitney u corrected for ties hi guys i’m currently working on my thesis’ result section and i planned to use an independent t test for one of my hypotheses unfortunately the data wasn’t normale distributed which brought me to the mann whitney u test i never heard or worked with this test but i think i get most of it now i reported the p value and put ‘corrected for ties’ next to it in the feedback my supervisor gave me she commented ‘what does this mean ’ on the ‘corrected for ties’ part i don’t know if she is asking herself me this question or that i have to elaborate this in the result section but that is something for me to figure out later my question for you is what does it mean since i have to defend my thesis at the end of next month and i think it would come in handy if i could proper explain what the test does why it doesn’t assume normality and what the ‘corrected for ties’ means could someone give me an eli5 of this test or at least an eli5 about the ‘corrected for ties’ part thank you in advance
0,best way to evaluate two competing recommendation systems i have a music recommendation survey and i have developed a music recommendation algorithm a and a simplified control algorithm b the users are given a list of songs to rate 1 6 based on how much they like them my question is should each user either listen to songs from the same system exclusively so aaaaa or bbbbb or should the recommendations from both systems be interlaced abababa randomly in order for me to be able to compare the average ratings for each system the first approach means that some users will never hear my recommendations and given that i do not expect a huge amount of respondents that is a bit of a concern for me also i expect a large chunk of the respondents to be from family and friends thus i worry that some of the responses will be biased by that fact so if the user gets the fake system they would still rate it relatively highly since they are being nice what should i do
0,what is going on with sas and how do you use it my company is insane and despite investing in cloud technologies and building out a team of python data scientists they are silo ing us to deploy all our products in sas to facilitate collaboration with non coding engineers for context we re in the energy industry so lots of meche and ees who are competent at data analysis but don t really program i have about a thousand questions i am praying the community can help me with as you know sas puts all their products behind enterprise licensing so stack overflow is failing me and i don t have any influencers on youtube to walk me through the mechanics like there is with python r if you are one of these people know that i love you forever and always what are good resources to learn sas fundamentals i have access to hours of videos from sas that are all a series of sales pitches for how great the product is but don t actually tell you how it works how does the data architecture of sas work all our data is stored in the google cloud platform and i m expected to pipeline this into sas is this even possible how can i explain to my boss that this isn t like the myriad other products we are expected to adopt we re a growing branch of our organization so it is common for us to trial various sas software as a service products but they give me a git repo i can dig into along with a support engineer i can ticket for elaboration not the case with sas should i be updating my resume now because my company is run by morons who make python r data scientists use sas
1,what testing problems are the outlier detection methods using z score and on mahalanobis distance based on i heard that there are two ways of detecting outliers for univariate use z scores with an alpha of 001 to eliminate very extreme values this corresponds to a z score of 3 for multivariate use mahalanobis distance which is is distributed as a chi 2 distribution with an alpha of 001 to eliminate very extreme values are the two outlier detection methods based on some statistical hypothesis testing problems looks like yes to me because there is level alpha of significance involved if yes what are the testing problems thanks
1,question how to approach pairwise comparison of many values to a standard with r and achieve a tabular output hobbyist newcomer here i have a dataset consisting of responses to the census bureau s american community survey which looks like this x200b zip totalresponses hispanicresponses moe 01331 2000 211 20 18774 785 523 102 93571 52000 1455 444 x200b i would like to compare the percentage of hispanicresponses to the national percentage by zip code so i end up with a table of results showing me how much each zip s percentage differs from the us percentage does anyone have suggestions on how to achieve that using r
0,is there any difference in operational definitions between predicting vs forecasting is one a subset of another other or are they fundamentally different i have heard forecasting is different than predicting primarily due to forecasting having a time component but i am not sure if this is true
1,stochastic block model vs standard community detection algorithms has anyone ever come across the stochastic block model all in all this seems like a community detection algorithm for graphs i e network clustering does anyone know in which circumstances it would make more sense to use stochastic block models compared to community detection algorithms such as louvain clustering thanks
0,market basket analysis help hi i just started to learn about market basket analysis with fp growth what is the amount of data do i need to create an accurate association rule is there a minimum is it pure arbitrary thanks for the help
0,in the spirit of mental health month imposter syndrome many of my data science candidates and coaching client s face imposter syndrome i compiled some resources on what is imposter syndrome how to recognize and combat it here is a link to the full article with youtube videos imposter syndrome “it seems like whenever i have a problem and i go to stackexchange i almost always get a response like “well obviously you have to pass your indexed features into a regix 3d optimizer before regressing every i th observation over a random jungle and then store your results in a data lake to check if your normalization criteria is met ” it’s like where are these guys learning this stuff ” link characteristics of imposter syndrome some of the common signs of imposter syndrome include reference self doubt an inability to realistically assess your competence and skills attributing your success to external factors berating your performance fear that you won’t live up to expectations overachieving sabotaging your own success setting incredibly challenging goals and feeling disappointed when you fall short what is imposter syndrome youtube video the imposter syndrome imposter syndrome is loosely defined as doubting your abilities and feeling like a fraud it disproportionately affects high achieving people who find it difficult to accept their accomplishments many data scientists question whether they are deserving of accolades their job recognition or the like you do not have enough time to learn something you want to learn you look around and see that there are other people that know that thing you don’t have time to learn you feel incompetent why do so many data scientists have it data science is an extremely broad field of study there are core competencies required to have a successful career in data science but there is also a lot of industry specific and technical knowledge that is ever changing data science is a career which has many job options all of which require a high level of expertise and knowledge if the broad seemingly confused data science job postings show us anything it is that many companies do not really understand what a data scientist is how they compare to a data engineer or software engineer and how to train or support them within an organization to add to this the labor market for data scientists in predominantly new graduated or early career professionals when challenge is high and expectations are unknown it encourages people to fall into high arousal anxiety and worry you can see this from psychologist’s mihaly csikszentmihalyi flow model these feelings are compounded by a lack of support feedback and mentorship provided within a company this is not generally intentional but a product of small data science departments business executives licking their wounds from years of poor data quality and technical deficit and increasing demand for better data driven outcomes how can data scientists deal with imposter syndrome according to the american psychology association if you recognize yourself in the description of the impostor phenomenon take heart there are ways to overcome the belief that you don’t measure up in a nutshell there are three ideas that you need to get in your head in order to get over imposter syndrome you are a generally competent person there are always going to be people that know more about a certain area of data science than you and that’s ok and expected even more importantly you’re not the smartest person in the planet so if you look hard enough you’re going to find people that are better than you at everything you do and that’s ok you have a finite amount of time to learn things and your goal shouldn’t be to learn the most but to learn the things that maximize your specific goals – generally this is going to be career advancement but for some it may be something else when the imposter syndrome feeling comes up 1 remind yourself that you are a competent person – if you weren’t you wouldn’t have gotten to the position you are in right now whether that’s graduating from college or leading a data science team yes even ds team leaders catch the ‘drome from time to time 2 remind yourself that when you look for people who know more than you about a specific area you are guaranteed to find them – that’s just how it works people choose to specialize in certain areas and if you only focus on that area of expertise you are going to feel inadequate but even more importantly recognize that if you run into someone who is better than you at literally everything you do that doesn’t diminish your value – it just means you have run into someone that is pretty special 3 get back to prioritizing what to learn do you need to learn that or do you just want to learn it to feel better about yourself if the latter learn to let it go and focus on the things you need to learn – and save the things you want to learn for when you have the time which will come u dfphd – phd head of data science ecommerce youtube what is imposter syndrome and how can you combat it talk to your mentors “the thing that made so much of a difference was supportive encouraging supervision” many have benefited from sharing their feelings with a mentor who helped them recognize that their impostor feelings are both normal and irrational though many will often struggle with these feelings you must be able to recognize personal or professional progress and growth instead of comparing myself to other students and professionals recognize your expertise don’t just look to those who are more experienced more popular or more successful for help tutoring or working with younger students for instance can help you realize how far you’ve come and how much knowledge you have to impart this can be a great way for a data scientist to give back to the industry as well as set a more realistic benchmark of your perceived value remember what you do well psychologists suzanne imes phd and pauline rose clance phd in the 1970s impostor phenomenon occurs among high achievers who are unable to internalize and accept their success imes encourages her clients to make a realistic assessment of their abilities “most high achievers are pretty smart people and many really smart people wish they were geniuses but most of us aren’t ” she says “we have areas where we’re quite smart and areas where we’re not so smart ” she suggests writing down the things you’re truly good at and the areas that might need work that can help you recognize where you’re doing well and where there’s legitimate room for improvement realize no one is perfect clance urges people with impostor feelings to stop focusing on perfection “do a task ‘well enough ” she says it’s also important to take time to appreciate the fruits of your hard work “develop and implement rewards for success — learn to celebrate ” she adds change your thinking “let the challenge excite you rather than overwhelm you ” people with impostor feelings must reframe the way they think about their achievements says imes she helps her clients gradually chip away at the superstitious thinking that fuels the impostor cycle that has best done incrementally she says for instance rather than spending 10 hours on an assignment you might cut yourself off at eight or you may let a friend read a draft that you haven’t yet perfectly polished “superstitions need to be changed very gradually because they are so strong ” she says avoid all or nothing thinking just like a standard distribution most data scientists fall within the center if you find yourself comparing to outliers then you are going to continue to feel like a fraud which will in return stifle your career in data science youtube how you can use imposter syndrome to your benefit mike cannon brookes talk to someone who can help for many people with impostor feelings individual therapy can be extremely helpful a psychologist or other therapist can give you tools to help you break the cycle of impostor thinking the impostor phenomenon is still an experience that tends to fly under the radar often the people affected by impostor feelings don’t realize they could be living some other way they don’t have any idea it’s possible not to feel so anxious and fearful all the time
1,looking for standard statistics probability bayesian book hey i am taking a course in bayesian stats now and the book bayesian data analysis 3 we are using is really good but it is very text heavy i am wondering if there is any statistics book with the format definition theorem proof example that focuses on bayesian statistics or at least that it cover bayesian methods as much as it would cover standard frequentist statistics i seen a lot of recommendations for the book statistical rethinking but it seems to be very text heavy as well
2,is it legal to create images with an open source nn implementation and sell them i want to use the recently published vqgan clip implementation of transformers to generate images based on a text description and then sell those images in any way i can maybe through a website i would use very specific seeds and text so these images would be almost impossible for anybody to replicate with the same network is this legal as i understand vqgan and clip are both open source thank you
2,modifying open sourced matrix multiplication kernel i ve spent the past few months optimizing my matrix multiplication cuda kernel and finally got near cublas performance on tesla t4 in the past few weeks i ve been trying to fuse all kinds of operations into the matmul kernel such as reductions topk search masked fill and the results are looking pretty good all of the fused kernels are much faster than the seperated versions while using much less memory runtime of fused minbmm vs torch bmm torch min edit unit of time in this plot should be seconds not milliseconds runtime of fused topkbmm vs torch bmm torch topk runtime of fused mbmm vs torch bmm torch masked fill i also wrote a blog post about the motivation applications and some implementation details of these kernels the source code can be found in this repo
2,huggingface bartforconditionalgeneration model bias tensor remains zero hey guys i m looking at the source code of bartforconditionalgeneration where the language model head has a bias per each vocabulary token however this is a non trainable element and never changes from zero i don t think this has been the intended behavior any thoughts x200b
2,what is the smallest dataset you stylegan2 trained i hear somebody on twitter tried successfully to train a 300 sample dataset gan is it even feasible training with stylegan2 pytorch under 1k image dataset
1,i was flipping a coin and got tails 7 times in a row how likely is this question im not sure if this the right subreddit to ask but this has been eating me up lately one day i was deciding what i wanted for dinner one was something i didnt like but shouldn t have the other i liked but is unhealthy so i decided to flip a coin when it said to eat the thing i didnt like out of frustration i flipped again and again and one more for good measure until i felt like the coin was rigged for some reason and got tails three more times until i finaly got heads this cant be very likely at all right am i crazy does this mean im going to die or the world will end anyone please help me this not a joke or exaggeration
2,on the robustness of vision transformers video paper and code on the robustness of vision transformers to adversarial examples tl dr an analysis of the transferability of adversarial examples between vision transformers and cnns and how this transferability can be leveraged for security introductory video paper link github code
1,two teams are decided by each player flipping a coin using a strategy can you make the chances of being on the same team as a friend greater than 50 in ultimate frisbee teams of 7 are often decided by flipping a frisbee heads is team a tails is team b if one team fills up before the other the rest of the players don t need to flip because they just go on the other team because of this is there a strategy where you can increase your odds of being on the same team as another player if you both use the strategy the rules are that you can decide to flip whenever you want so to give one example if 6 players have flipped and they all landed heads it would obviously be beneficial for you and your friend to not flip since you re most likely just going to be defaulted to the tails team since ay any moment you can decide to flip your worst case chance is 50 but by waiting and seeing how the flips go can this be increased
1,trying to see if there s a practice effect across 2 blocks of a task hi i m an undergraduate student working on a psychology dissertation just wondering if i anyone could help me out in my experiment i ve measured average reaction time in 2 blocks of a task from participants in 4 separate conditions a b c and d first i took the raw data and ran a paired samples t test on rt block which showed that overall mean rt improved in block 2 in general now i am trying to identify whether this was purely due to a kind of practice effect or whether e g participants in condition a improved more between blocks than participants in condition b did if there was a practice effect i d like to isolate it and factor it out to find any condition effect i have already run a one way anova on rt condition for each block which showed that in block 1 rt was signficantly faster for conditions c d than for conditions a b but in block 2 rt did not significantly differ between any conditions i have also run a paired t test on rt block for each condition which showed that in condition a rt was significantly faster in block 2 i m now kind of stuck on what to do next it seems like rt improved in block 2 for all conditions but condition a may have caused it to improve even more i want to know a way to confirm test this if it helps overall rt difference between blocks 295 4847 and rt difference between blocks in condition a 865 939 is it sufficient to just say that because condition a s difference was higher than the overall mean difference a had an effect or should i subtract the mean difference from a s difference or something apologies if this is unclearly written or if i m missing anything glaringly obvious this is my first time posting here and i m pretty stressed and tired any help would be very much appreciated thank you also i should say i m doing all this in rstudio
0,has anyone ever made a html presentation instead of a powerpoint presentation i saw this video here that shows some of the advantages of a html presentation vs a powerpoint presentation e g smaller size interactive graphs has anyone ever done this before is there a straightforward way to make a html presentation what software do you use for this
2,need help with normalizing flow 🙇‍♀️ i m reading a normalizing flow survey normalizing flows an introduction and review of current methods and get confused with section 3 4 2 regarding autoregressive flows maf and iaf the conclusion is absolutely correct i e maf supports fast density estimation and iaf supports fast sampling however it seems to me that the remaining equations descriptions and figures are all opposite i suppose x is the base distribution and y is the target distribution i agree with this blog post eric jang blog normalizing flows tutorial part 2 modern normalizing flows but the blog post and the survey paper seem contradictory anyone familiar with normalizing flow pleas ping me 🙏thanks 😉
0,ml advice needed hi not sure if this is the right place for this but just stressing cos i m getting to the end of the sprint and i ve got no results to show for my work x200b so basically i started at my first ds job just over a month ago and i m currently working on developing a small object detection model to pick out various symbols in these complex noisy diagrams crowded with lots of superfluous lines and text x200b the data set we have is pretty small and i ve had to label it myself i ve chosen to break the symbols into around 8 classes that are all pretty distinct from each other and i ve picked out 600 data points of all symbols that we need and 400 cases of the general background cases x200b i ve trained a cnn model to classify each case and the confusion matrix looks pretty good but when i run the model on the sliding windows it just performs terribly x200b basically it s ok at picking out these things in the small image sections that i ve picked out but is terrible at picking them out in the random windows generated by the sliding windows technique that i m applying x200b this is basically my first time building a nn model and my first time labelling my own data i think i have all of the implementations working fine but i just don t know how to boost the performance x200b i feel like this is mostly just a lack of experience on my part but it just feels like i ve hit a wall in terms of what i can do with this model i just don t want to have to say to my team that the project i ve been working on all sprint is a no go i d also mean that i d have to come up with an alternative strategy to identify these things cos we kind of need this in the long run x200b sorry for the rant any advice or resources would be really appreciated x200b thanks
0,zindi s official response to claims of unfair treatment by u maroxtn an official response to this post my name is paul kennedy i am zindi’s community and communication manager linkedin profile for reference i and my colleagues at zindi were disappointed to read this comment and i would like to take the time to address the principal claims made in the above post we take cheating on the platform very seriously as it erodes trust for all our users claim 1 i opened my email thinking it was some sort of a mistake i found an email sent by them stating that they banned me under the pretext of collaboration outside of team i responded explaining to them that i single handedly worked on the solution of my problem telling them i m ready to provide proof if they want they didn t respond the user in question and the author of this post was banned from the ai4d icompass social media sentiment analysis for tunisian arabizi challenge for contravening rule 2 2 of the platform please see our rules page these rules are also replicated in every competition page 2 2 multiple accounts per user are not permitted and neither is collaboration or membership across multiple teams individuals and their submissions originating from multiple accounts will be immediately disqualified from the platform the user was given a first offence warning and removed from the challenge on 19 march 2021 as per rule 7 1 7 1 first offence no prizes or points for 6 months probation period if you are caught cheating all individuals involved in cheating will be disqualified from the challenge s you were caught in and you will be disqualified from winning any competitions or zindi points for the next six months claim 2 then today out of sheer luck i discovered that the team that took my place on the leaderboard when i got banned work as a data scientist for zindi which is quite preposterous to say the least this is an utterly false and baseless claim as per rule 1 of the zindi platform employees of zindi are not allowed to participate in any competition on zindi further any contracted individual may not participate in any competition that they worked on and helped to prepare 1 participation 1 all members of the public are permitted to participate in any zindi competition unless 1 they are a full time or contracted employee of zindi or an immediate family member of a full time or contracted employee of zindi 2 they are a full time contracted employee or intern at the host company of the specific competition 3 individuals may not participate in a specific challenge if they have helped design the challenge source and prepare the data or if they are will be involved in code review lastly i’d like to express my sadness and regret at the awful sentiments expressed in the post above regarding ‘everything wrong with african countries’ as a company we distance ourselves from such bigoted remarks zindi is dedicated to growing supporting and uplifting africa through the transformative potential of data science and machine intelligence we are at our heart a company dedicated to making ai accessible to african and other developing countries we strive every day to promote fair and ethical data science as a career choice a technology for disruptive positive change and a movement that will reshape the african landscape for decades and centuries to come we have hope and optimism for africa and for african data science we are constantly inspired and motivated by the amazing work of our african and non african users anyone who shares these sentiments is welcome on our platform and is invited to join us in building ai together for everyone and for good
2,what is the science behind artbreeder i recently encountered artbreeder website i am wondering whether there is any description of how it works behind the scenes i know what biggan and stylegan are and i read artbreeder uses them but these are the things i want to know more in detail 1 how these gene knobs are created given the datasets i understand you train the model on a big dataset of face images then how do you create these gene knobs 2 how do you merge two images with each other do you interpolate between their latent representation thanks in advance
1,q help interpretting regression i ran a logistic regression for he variables not in the equation variable a is not significant but black race is i e black race has a significant relationship to the outcome and their interaction is also significant for the variables in the equation none became significant however the model is significant in this case do i say that any are significant associations is there an interaction why is the model significant if none of the variables are i also ran a regression with a different outcome variable that is continuous and for variables not in equation only race was significant not variable a and not their interaction but for variables in the equation race remained significant yet the model is not significant would appreciate any insight into interpretation of whether an interaction exists
0,need advice on the best web scraping tool approach for this job hey everyone i first posted this on r analytics but realized it doesn t fit very well there i need to scrape some elections data from a website it has javascript and around a 1000 individual pages that all have the same format and variables stored in a table i m new to scraping but have read a bit today and it seems like python is my best bet i was wondering if this is the type of thing i should use a full crawler on like scrapy the urls for the pages i need all have this format where xxxxx seems to be an id code made up of digits for each page i don t know which 5 digit codes actually correspond to the pages i need but its certainly not all 5 digit combinations because the number of possible pages is much smaller than 99 999 should i just get started with learning scrapy in python or do you think there is a better tool for this task
2,video deep learning with dynamic graph neural networks i m a phd student studying machine learning and applications in transportation systems and autonomous systems think rl and robotics while there are several gcn made easy videos out there on youtube i feel like these videos often miss the forest for the trees especially since gcn is just 1 algorithm that was developed in 2016 and videos often don t cover the broader historical context of how gnns were developed and don t cover how different variations of these models allow them to model new types of systems this is the fourth video in a series i m making about graphs graph neural networks and the application areas where they have the potential to make big impacts please let me know what you think of the video and if you learned anything new from it
2,are there any conferences journals where reviewers evaluate focused on new applications areas of research rather than mostly vain architectural algorithmic improvements that usually don t translate to anything except the extremely specific task they were designed for i would like to write papers where the focus is on applying existing techniques to new domains tasks and why the task is important or how different methods perform in this task rather than having to justify why my method is novel or performs some improvement using a new module loss function to appear novel to the reviewers is this even a possibility
2,object detection is it detrimental to retrain but only focusing on one object type hi i m currently working on an object detection model using gluoncv autogluon my model is at about 80 for some objects but much worse for others when i m updating retraining my model is it harmful to only train with certain image types so if i know i have a set of images which contain objects a and b will only tagging and retraining with images that tag object b increase accuracy for b and or decrease accuracy for object a thanks p s if it helps here are some of the current model parameters amp false base network resnet50 v1 data shape 512 filters none nms thresh 0 45 nms topk 400 ratios 1 2 0 5 1 2 0 5 3 0 3333333333333333 1 2 0 5 3 0 3333333333333333 1 2 0 5 3 0 3333333333333333 1 2 0 5 1 2 0 5 sizes 30 60 111 162 213 264 315 steps 8 16 32 64 100 300 syncbn false transfer ssd 512 resnet50 v1 coco
2,does code for relational networks related to physical system dynamics actually exist i m talking of course about the paper a simple neural network module for relational reasoning the paper itself does not have any corresponding code through some folks seem to have implemented certain parts of it x200b the code i am looking for is related to their claim of being able to model physical dynamical systems are there any sources that have actually implemented this part thanks
0,recommended resources for designing an item rating system hi there i am building a website with a rating system and i was wondering if there are any highly recommendable books that could help with designing it i m interested in anything ranging from details such as designing algorithms to deal with items with fewer ratings having higher average scores as in an item with a single 10 rating being ranked higher than an item with a thousand 9 votes or the arguments behind different rating systems 10 stars 5 stars thumbs up down subcategory ratings etc to even more disperse topics such as adapting the math to the user psychology behind it all such as how to adapt the system to nostalgia votes or how to separate evaluating how impactful or revolutionary a product was when released from how good or recommendable it is today i am also really interested in the implications that those different rating system designs would have for a inter user recommender affinity system but i understand that might be a tad specific x200b i am ok with technical reads and know some machine learning i ve done the stanford course on coursera but i am not a data scientist so i d prefer it if the books resources were reasonably accessible though if a dense book is an absolute staple it would be good to know about it as well cheers
1,mutivariate linear regression vs multiple linear regression i started learning multivariate linear regression in a multivariate statistics module and i m struggling to understand the difference with the following would a multivariate model with k target variables be equivalent to running k multiple linear regression models one for each of the target variables in the multivariate model
0,what are the best data science bootcamps hi all could i get your opinions on the best value data science bootcamps out there it s something i ve looked at for a long time but i ve been hesitant to pick one because there seems to be so many of them the first one that comes to mind is general assembly and i also see springboard let me know your experience with these and any others out there thanks in advance
0,uses and limitations of data science in finance what are the best applications of data science in finance particularly in investment banks trading and investing in the markets in general then what are some limitations i ve read a comment by a data scientist that predicting stock prices through data science machine learning is a pie in a sky problem and hence is not realistic
2,in bert what is the purpose of the sep token when segment embeddings are already included in the input during pretraining of bert two sentences a and b are given transformed using wordpiece and position embeddings and are made distinct from each other by including two extra features into the input the sep token which is inserted between sentence a and b the predefined segment embeddings which are added to the the embedding layer what i cant figure out is why are both of these included shouldn t either one by themselves do the job the sep token in particular seems redundant to me as it isn t even used for the next sentence prediction pretraining task which instead trains on the cls if i ve understood everything correctly
1,glivenko cantelli theorem vs law of large numbers has anyone heard of the glivenko cantelli theorem in statistics before this theorem states that the empirical distribution function of a variable will converge to the cumulative distribution of this variable i am not sure and do not understand what kind of convergence this is but the way i understand it the cumulative distribution and the empirical distribution will be equal to each other when the number of observations increases is the glivenko cantelli theorem similar to the law of large numbers do both of these attempt to describe a similar concept the relationship between the observed and the theoretical thanks
1,difference between cox regression and glm with exp log link function i m trying to learn survival analysis and the model specification for cox regression looks very similar to a glm with exp log link function x200b i understand that there needs to be a distinction between censored and non censored observations but can t that be achieved by modeling the conditional probability with the glm i feel i m either missing some critical thing about cox regression or just forcing a similarity between it and exp glm
1,beginner question what are good resources for learning about the robustness of a technique to its assumptions i’ve been learning the rudimentary stats procedures anova regression etc and that these procedures have varying assumptions for the data you are using e g normal distribution linearity my professor keeps saying some procedures with varying degrees are robust to certain assumptions they have depending on sample size when i ask for guidelines in this regard he tells me to find literature on the topic ive looked long and hard but cant seem to find guidelines for understanding how much i should expect my data to meet assumptions for various procedures before choosing an alternative one any help or suggestions would be appreciated
1,question what layman example can one give to convince a person of their wrong interpretation of null hypothesis i recently saw a data science guru s video explaining null hypothesis testing he wrongly kept saying we accept the null hypothesis when i tried to correct him and said you can only fail to reject the null hypothesis one can t accept it he retorted and said how does it matter it is just a word play i understood that he was not getting the seriousness of the issue how would you try to explain the seriousness of the issue with an layman example
1,are the functions that neural networks are trying to approximate conceivable based on the universal aproximation theorem neural network are said to be able to approximate any function reasonably well from a very basic definition in math here is what i think of when i hear the word function suppose you have a task in which you have pictures of cats and dogs you want to train a neural network to recognize pictures i guess pictures can be considered as large matrices of numbers in this case what is the function that the neural network is trying to aproximate like in this picture can you say that the orange circle are the numbers contained in the pixels corresponding to cat or dog and these are mapped to the yellow circle containing cat or dog in this example is the function an abstract concept that can not be directly observed numbers contained in pixels dog or cat
2,make use of validation data without label when predicting values lets suppose a model where i have a train dataset labeled which i can use freely can only predict values in batch you have to feed it n validation tests and the model will attribute a label to all of them which means the model cannot predict each validation case but need to predict all of them at once taking into consideration their values but obviously not their label which may be unknown would you guys consider this model biased i really would like a discussion about this or maybe even some literature references i honestly dont think this should be considered biased
2,lstm for fault detection diagnosis with time series classification i m working on a fault detection and diagnosis using an time series data set which has both normal and faulty data 7 faulty types where i will predict the the class label of a given sequence of inputs a selected window of current incoming data i used sequential model with kerasclassifier and implemented a nn model which gave me an predicted class label based on the input sample one array of data sample but here i have used only a single input of data the scenario i want to achieve is providing a specific window of latest incoming a window of 10 inputs within 10 seconds and predict a class label based on the provided sequence of multiple samples to the predict function
2,stanford ai ml course i’m considering the stanford professional certificate in ml ai based on the graduate program but worried it might be too theoretical not completely new to ml eg did andrew ng’s coursera course before read aurelien geron’s book but looking to improve and mostly become better more efficient in python coding with sklearn and tensorflow any experiences to share how does it stack up to other programmes out there eg mit oxford
2,fun and dystopia with ai based code generation using gpt j 6b gpt j was released a few days ago and in testing i noticed it did code generation rather well so i ran it through a few more tests and the results are chaotic to say the least some example generated code def is bird img check whether an image is a bird with tf name scope get bird a vgg16 select roi feat img h w tf image extract image patches a 7 7 4 4 1 1 b tf reshape h 1 7 7 256 c tf reshape w 1 7 7 256 bird img tf subtract b c return bird img def is cake cake check whether the cake is true if not cake print it s a lie return else print it s a true fact return true def is ai generated text check whether a text was generated by an ai language model e g gpt 2 see gh 196 for details about why we do this note this relies on the fixed set of standard ai terms and the logic that standard ai generated texts share certain words return re search r a za z text or re search r generated by text or re search r all your base are belong to us text or re search r text
0,is it ok to have a coefficient greater than 1 hello i m new and working on a logistic regression model i have two variables that are what i would consider abnormally large at 10 2 and 30 7 i read online that it s usually a bad sign if this is the case and that this may signal multicollinearity however the vif factor on these is 1 2 for both under the 5 0 that i ve read signals multicollinearity is it ok to leave these in should i remove them thanks
2,morphological operations that use template shapes to find desired shapes from voxel spaces how to i was reading this paper that discusses methods to extract tree stems from lidar point clouds around page 9 they discuss morphological operations applied on voxelized point cloud that use template shapes in order to figure out what shapes to look for this seems intuitive but i m unable to grasp what the method of matching to a template means it sounds like doing a model on the templates and then maybe inputting parts of the cloud there but i am not sure does one compare the entire cloud to the template or parts of it what kinds of parts how does one get them
2,machine learning wayr what are you reading week 113 this is a place to share machine learning research papers journals and articles that you re reading this week if it relates to what you re researching by all means elaborate and give us your insight otherwise it could just be an interesting paper you ve read please try to provide some insight from your understanding and please don t post things which are present in wiki preferably you should link the arxiv page not the pdf you can easily access the pdf from the summary page but not the other way around or any other pertinent links previous weeks 1 10 11 20 21 30 31 40 41 50 51 60 61 70 71 80 81 90 91 100 101 110 111 120 week 1 11 21 31 41 51 61 71 81 91 101 111 week 2 12 22 32 42 52 62 72 82 92 102 112 week 3 13 23 33 43 53 63 73 83 93 103 week 4 14 24 34 44 54 64 74 84 94 104 week 5 15 25 35 45 55 65 75 85 95 105 week 6 16 26 36 46 56 66 76 86 96 106 week 7 17 27 37 47 57 67 77 87 97 107 week 8 18 28 38 48 58 68 78 88 98 108 week 9 19 29 39 49 59 69 79 89 99 109 week 10 20 30 40 50 60 70 80 90 100 110 most upvoted papers two weeks ago u hateredditcantquitit autodidax jax core from scratch besides that there are no rules have fun
0,startup went bankrupt need career advice good afternoon i am facing a challenging situation on my career now and i would like some advice i will provide a very brief summary of my experience first of all i am based in portugal europe i have a a mechanical engineering degree focused on energy systems with a bunch of optimisation ml and numerical simulations mixed in including the master thesis as well as years in extra curricular activities revolving around python and ml projects i have 2 years and a couple of months experience in data science in the following roles 1 data science consultant for 1y and 2m role included developing a genetic algorithm time series analysis with lstm prophet and arima as well as a big data project on databricks and sql i started feeling like i wanted to be part of internal data science team so that i could really own my projects and that is why i left for company 2 2 data scientist for 7m in an energy related company me and another person were brought in to start a data science team this company was not a technological company and there was not much to do really we were supposed to get data to start working on some use cases but after 7 months we did not have any data nor any indication that we would have in the future during my time in there i explored outlier detection methods like isolation forests and lof did a local web page using flask javascript and bootstrap and also worked on some data cleaning and process automation pipelines i had an offer via a recruiter that contacted me on linkedin that i felt would make my job much more meaningful and at the time it was very hard for me to justify not taking it so i took it 3 7m fintech startup this was pretty much what i wanted in a data science role build a model using h20 ai for client grading and well as lot of other analysis for managing client and portfolio risk i felt like my job truly mattered and i was extremely satisfied with it and super excited for all the possibilities that i would have but due to a very unexpected issue with financing we ended up closing after 7m on the role more and more i have been craving time to truly dedicate myself to learn and develop some personal projects mostly related to crypto and financial markets my question is can i afford to have a gap on my cv assuming i fill it with personal projects or do i need to start looking for jobs immediately also since i left university i wanted to start an international career and move to germany sweden switzerland austria uk i have been learning german for 2 years would this be a good time to truly try to move do i even have a chance if i just send cv s from portugal i am wiling to reallocate and get a job on any of this countries before i move or would i really have to move there before i start trying to get a job thanks all
1,discussion such thing as too any events of interest in a data set for survival analysis i m familiar with the challenges when there are too few events of interest when analyzing data using survival analysis for example having 10 variables in your model but less than 100 events or 10 events per variable but i am curious if there is a problem with too many i m comparing a few different models on a dataset and the event of interest i created has over half of all patients experiencing this event strictly looking at the concordance indices of the models there doesn t seem to be a drop or improvement in performance when compared to a previous event that had only 1 4 patients experiencing so i am just curious peoples thoughts x200b tldr if covariates events is high dimension survival data and it brings challenges how about having too many events per variable
1,confidence interval question hi i am an economy grad who is trying to get around this confidence interval problem i am self taught so i need someone to tell me if what i did is correct thank you sorry if my explenation is kinda messy i lack the english terminology to express the problem properly a sample of 20 people watches a certain tv show with an average time of 180 minutes and a deviance equal to 150 find the confidence interval for the average watching time of the population with a 1 error i have the deviance which means i have to calculate the variance equal to dev n 1 150 19 7 89 that means the sample x follows a normal distribution with e x 180 and var x 7 89 the exercise wants an interval which implies an 1 error the formlula is x z a 2 sqrt v x n x z a 2 sqrt v x n given that a 2 is 0 005 i have to find on the table the z value which corresponds to 0 995 which is 2 58 so according to the formula the interval is 179 638 180 362 is that correct thanks
0,data scientists who have moved to a lcol area and work remotely what have been your biggest learnings i just got a dream job with a bay area company who is allowing me to work remotely i’ve been living in the bay area for 5 years and now want to move to a more peaceful less stress area that is closer to family fortunately my salary won’t be adjusted so that’s not an issue what i’m wanting to ask this network is those of you who decided to move away from hcol areas to lcol areas and work with a distributed team what have been your biggest learnings what advice would you give do you have regrets how would you have modified your decisions with hindsight i’m particularly curious about zoom fatigue culture shock communication effort stability security etc
2,cpu amd and gpu nvidia rtx for machine learning is it ok i need to build a pc with the following set up and i like to know if there is any issue regarding the machine learning frameworks etc cpu amd ryzen 9 5950x processor motherboard asus rog x570 crosshair viii hero wi fi atx am4 motherboard gpus 1 nvidia rtx 2070 super windforce 2 nvidia rtx 3090 until now i am using intel based cpu and supported motherboard but as i now want to use amd based cpu and supported motherboard i am a bit confused about if it is ok for machine deep learning frameworks such as tensorflow pytorch any compatibility issue or anything like that
0,statistician vs data scientist what are the differences is one just in academia and one in industry or is it like a rectangles and squares kinda deal
0,how might you display overlapping binary data other than a venn diagram i’m plotting data in r that consists of which countries have a which countries have b and which countries have a and b i feel like a venn diagram is the most obvious choice but was wondering if anyone might recommend any alternatives
1,is this a bilateral or a unilateral hypothesis contrast so i have to do this exercise and i don t know if it is a bilateral or an unilateral hypothesis contrast calcium is normally present in the blood of mammals at concentrations of about 6 mg dl of the total blood a series of 9 tests on a patient revealed a sample mean of 6 2 mg dl with a standard deviation of 2 mg 100 ml is there any evidence for a level α 0 05 that is the patient s mean calcium level higher than normal if it is unilateral h0 μ ≤ 6 h1 μ 6 if it is bilateral h0 μ 6 h1 μ ≠ 6 so it would be bilateral wouldn t it
0,how do i think of data science projects i d really like to learn more about data science through practice and maybe build a portfolio but i m really uncreative and can t think of any projects to try does anyone have a suggestion for how to approach this or project ideas for someone with intermediate python experience maybe something w a machine learning component
2,using cma es to train model on dataset supervised hello folks i m looking a way to train a rather large model 50m params that s not gradient descent friendly during training all gradient based methods are failing horribly with a shabby increase in accuracy i m looking to use cma es to optimise the params in the model i ve read up on the basics of implementing it but want a second opinion i came across this post on stackexchange and found it useful for my problem i ve also been reading u hardmaru s attention agent paper from gecco 2020 and his article and lillian weng s article on the various evolutionary optimisation algorithms for very very large models i feel this blows up the total parameter count of n model copies children by a large extent i m training on a colab tpu but can move to gcp tpu v3 if needed i managed to snag a few from tfrc i m using pytorch xla as the framework can i get some advice on how to go about this is there a better way of training my network i m aware of hill climing and simulated annealing but have heard cma es outperforms them and is effective any thoughts and possible directions welcome thanks again
1,possibly noob question sorry in advance so i was thinking is getting heads on a coin twice in a row always 1 4 if i predetermine ill get tails twice in a row then it s 1 4 but if i get tails once then decide to predetermine i ll get tails again is it technically still 1 4 or something else i think but i m probably wrong hence the post the chances of getting another tails is just 1 2 at that point another example lets say there s 100 balls in a bag each one is labeled with a number from 1 to 100 basic example if the first ball i pull is 97 and i put it back in the bag then predetermine ill get 97 again and i actually do get 97 again was it really a 1 100 000 chance or just 1 100 by that point
2,why normalization in initial layer of discriminator leads to unstable gans according to the dcgan paper directly applying batchnorm to all layers however resulted in sample oscillation and model instability this was avoided by not applying batchnorm to the generator output layer and the discriminator input layer i have observed the same thing when i use other normalization layers like instancenorm or layernorm but not able to understand why that could be the case why is it that using a normalization layer in the initial layers of discriminator leads to unstable gan training i hope i can get an intuitive answer thanks
0,geo spatial analysis and visualization techniques i m just curious what tools and processes and analysis practices you ve used i m relatively new to granular geo spatial analysis i m looking into uber s h3 hexagonal map basically translating longitude and latitude coordinates into hexagons of different sizes 1 12 that cover the globe the grouping allows for categorization and logical analysis is there a tool that can quickly visualize a table of long latitude coordinates into these hexagons i can of course map to the hexagons i m researching myself rapidly here but wondering if anyone has any insight say i have a bunch of hotel lat long in the city of nashville what would be a rapid way to translate these into say uber h3 hex size 10 or frankly any size they did a lot of the legwork already and then map said results
2,serverless gpu i need to deploy a vision model that will run a small inference job 1 5s per request is there a serverless offering out there that does this there are other previous posts on this subreddit but i know this space is moving fast so thought it worth asking again so far i’ve seen aws sagemaker kind of allows for a situation like this but would rather not deal with all that config algorithmia and nuclio are too enterprise focused neuro is new and looks great but from my understanding i would still need to create a lambda instance myself that then calls neuro’s servers too indirect is there a total solution out there for this ideally something that is as straightforward to use as neuro but also handles http requests
0,what is the difference between the coding skillset of a data scientist and that of a swe i honestly think that the line of difference in coding skillsets is blurred for instance are you supposed to be grinding leetcode to become a data scientist or is that only for swes
2,reviewing for workshops what is an appropriate standard of an accepted paper i am reviewing for an ml workshop at icra for the first time have reviewed for conferences before while giving the rating for a project that is clearly wip i am confused as the how strictly leniently i should grade the paper i can provide appropriate subjective insight but i am finding it difficult to translate it to an appropriate objective rating any help would be appreciated
1,when to use t vs z in confidence intervals ap stats teacher said the following question would be graded wrong if we wrote z instead of t why is this i always write z question activity trackers are electronic devices that people wear to record physical activity researchers want to estimate the mean number of steps taken on a typical workday for people working in new york city who were such trackers a random sample of 61 people working in new york city who wear an activity tracker was selected the number of steps taken on a typical workday for each person in the sample was recorded the mean was 9797 steps and the standard deviation was 2313 steps a construct and interpret a 99 percent confidence interval for the mean number of steps taken for a typical workday for all people in new york city who wear an activity tracker answer x bar ± t s √n
0,how to handle a potential career misstep tl dr i was working building cool stuff at a big company but didn t have opportunities to build my skillset so i took a job with a smaller company that indicated i could do the same thing but have more resources at my disposal i took the job and now i m building tableau dashboards based on requirements lists hi everyone i am curious to know if anyone else has found themselves in a similar position i work in healthcare analytics and recently left my previous role after 3 years at the company overall i liked my previous job but i was the only person with a data science skillset in my area this came with big trade offs on one hand i was seen as an expert and had good discretion on project selection what started to concern me was growth i didn t have others around me to hold me to a higher standard and there were very high barriers to truly productionalizing work as in deploying containers to a production environment working on real time issues was basically impossible most projects had to be run monthly on personal computers and it support or access to it tools was non existent so i started looking passively this was a publicly traded top 50 us company but my area was only responsible for about 400 million in revenue i was contacted by a smaller private company 300 million in revenue in my area and had a few rounds of interviews with them the director of analytics and the manager i would be working for were both very knowledgeable people i expressed the areas i wanted to be able to push forward with and what i had done in the past i discussed that i was an end to end consultant i worked directly with stakeholders got an idea of their needs prototyped ideas on how i could develop solutions and did all the ai ml work sometimes i would work with an analyst to package the results into tableau they seemed really interested in my work i built predictive engines for deal modeling fraud detection competitor price estimation churn etc and i asked them what problems projects they had on the horizon they shared some ideas that seemed interesting i asked about whether they get it resources and if they can deliver solutions and they explained that they have recently started working in aws so i started there just over a month ago and to some extent i feel like i was sold a bag of goods they have me working a jira queue for projects that get submitted with requirements lists for dashboards that business users want created they don t seem to give much thought to what projects could make money for the company or save operational costs at the time of the interview they did ask me a few questions about tableau which i answered fine but explained that i didn t use it much in my role they also asked some fairly straightforward questions about models like what parameters would you tune in certain libraries differences in regularization techniques etc the types of questions i would ask a candidate just to make sure they weren t lying on their resume i got about a 20 increase to leave so i m paid well but honestly i have no idea why they would hire me for this type of work i had several viz focused colleagues that i m sure would be happy to take double the money for a role like this i thought i did a decent job interviewing them but it really feels like i ve made a huge mistake i ve met with my manager to ask about strategic priorities and how we manage the project queue his examples of strategic priorities were just bullet points from a project that had been submitted by the sales team i m sure things could improve but it has been a serious shock to the system thus far the most charitable view i can take so far is that they want to take on new styles of project work but haven t gotten there yet and are just staffing up for the future sorry for the long post i tried to keep it brief and left out a lot of other strange tidbits what would you do in my position
2,where the ml research is headed and a thank you to the community that made it possible mark writes i increasingly feel like conferences matter less than twitter github by the time a paper hits conference the state of the art is two papers further acceptance by u wightmanr into timm and hitting u karpathy’s arxiv sanity trending is arguably more impactful than a neurips talk x200b andrej karpathy writes there’s a few other prestigious venues like ykilcher youtube paperswithcode ak92501 al tweet streams etc but yes i rather like the emerging hybrid model where the new cheap low latency async distributed consensus layer coexists with the legacy “layer 1 chain” pubs x200b arguably this subreddit is also playing a role in this trend without being part of the traditional academic community i feel like i have learned more about ai and ml without the need to do a ph d or attend a conference and i am deeply grateful this is possible thank you what are the thoughts of others on this trend
0,how can i make this workflow better hello i am trying to automate some manual tasks prone to human error i have some ideas but they are all so janky and i hope some one smarter than me can offer an elegant solution the workflow is like this i have a series of binary files in a proprietary format on a redhat server on this server is a c binary application that can decode this file into csv format i log into the linux machines run this application on the binary files then copy these files over locally to a windows laptop and run a python based data analysis putting python analysis software on these linux servers is not an option i have mounted the linux server s home directory as a mapped drive on my windows machine for easy copying of files but i still have to log in to the linux host to run the binary decoder application i want to automate the whole process where i can run a python script on my windows laptop and point it to a file or folder on the mapped linux drive have it execute the decoder script remotely then read in the csv files and perform the data analysis the problem is that the decoder script is compiled in the native linux env so it needs to be ran via its native environment is there an elegant way to execute a binary remotely on linux from windows x200b p s my current proposed solution is to have python launch a cygwin application and then have cygwin try to ssh user host command to remotely execute the decoder binary
0,40s mid level manager in fintech with interest in ds should i do a part time ms degree mid level director in early 40s working in large financial data company making a decent living responsible for change an risk for a product within operations group also have a small team of 3 data scientist engineers and double the headcount next year with focus on new data integration data source scraping rpa ai nlp modeling and advanced analytics visualization with tableau pbi while this is 1 3 of my responsibilities its where my interest and curiosity lies my background is more finance and operations intermediate sql my involvement in all this is more around coming up with ideas push on execution project manage it all while tasking my guys on execution etc but because i know nothing about python the different packages etc i always wanted to learn more since i m on an operations team and not technology are use cases are fairly basic etc but it s still fun to find opportunities to apply such tech to automate and reduce risk etc our company offer tuition reimbursement of 20k per calendar year so i can pick any msc degree over 3 calendar years but in 2 full years and it would be free i wouldnt do it for the starting salary since i make above any msc or mba starting salary it s more for trying to further my career sharpening the toolbox in prep for any potential career company changes if at all and staying relevant question for you guys who have done such degrees and are working in such field would this make sense for me should it be data science or would data analytics make more sense based on what i do and what level i m at is ds too nitty gritty too close to the actual work would it further it further my career considering i have a team of data scientists or data engineers etc does it make sense to learn it all from the group up thoughts
2,about the neurips 2021 author survey neurips is in a really interesting mood this year first we had the checklist now we have this author survey all authors are asked to estimate the probability that each of their papers will be accepted in the neurips 2021 review process authors who submitted more than one paper will additionally be asked a second question to rank their papers in terms of their own perception of the papers scientific contributions to the neurips community according to the survey invite the purpose of this survey is to evaluate how well authors expectations and perceptions of the review process agree with reviewing outcomes i suppose it s interesting to get an idea of authors confidence in their work but it feels weird i also question what sort of genuine insight they can get from it as for me i got stuck in a dark fantasy of reading the rejection email along with an attachment of my acceptance estimate just to twist the knife a bit now i am extremely confident in the work that i submitted but i am much less confident in its chances as it goes through the increasingly random process of peer review so i had to be realistic when i filled out my estimate which showed it being little better than a coin flip to get in how are you handling it
0,is there a sub discipline of data science that focuses on the physical sciences i’m looking to see if there is a data science category that focuses on physical phenomena i e physics math chemistry etc instead of data science methods used when analyzing human behavior like voting tendencies or purchasing habits i know that both areas can use the same machine learning models but i m curious to know if the physical sciences tend to benefit from a certain category of data science methods am i right in assuming that our approach to data science in the physical sciences can be different than our approach to data science in the softer sciences like psychology and sociology if so i would greatly appreciate your thoughts and any potential references to already existing literature that relates to the topic thanks in advance
2,training tips on diffusion models diffsinger for a tts hi all i m currently playing with diffsinger which is a tts system extended by diffusion models for the naive version it consists of encoders for embedding text and pitch information and a denoiser where the encoders output is used to condition the denoiser everything is similar to diffwave including denoiser s structure and prediction but the neural net to predict epsilon would be changed to epsilon noisy spectrogram encoder outputs diffusion step compared to diffwave s epsilon noisy audio upsampled spectrogram diffusion step while i m successfully training encoders i got an issue during training denoiser i used ljspeech here is what i did 1 first of all as a preliminary experiment i try to check all modules to work well by setting denoiser as epsilon noisy spectrogram clean spectrogram diffusion step to predict the noisy spectrogram 2 after the model converges i went back to the denoiser of epsilon noisy spectrogram encoder outputs diffusion step to predict clean spectrogram i detached the encoders output from the auto grad when the input to prevent from updating to the denoiser to fix the conditioner for model convergence the model was broken when i didn t detach allow the encoder to be updated during denoiser training 3 i found that when the range of the conditioner encoder outputs values is smaller then the model shows better evidence of successful training bellows are the results i ve got so far the upper one is the sampled synthesized mel spectrogram and the lower one is the ground truth on each image 1 i can see the model converge during the primary experiment x200b 2 when the encoder s output directly input to the denoiser value range 9 xxx to 6 xxx x200b 3 when the encoder s output is multiplied by 0 01 to shrink the range x200b for case 2 it shows any clues on training on contrary the case 3 shows some levels of training but it is not what we expected i double checked the inference part reverse part but it is exactly the same as that of 1 and diffwave so i just want to know if you have any idea on the successful conditions of the input conditioner of the denoiser why does the model show such an unsatisfying result above do i miss something to process the conditioner i will appreciate all suggestions or sharing of your experience thanks in advance
0,ds take home assignment requires building an entire project using skills i don t have hi everyone i have been a lurker in this community and it has been super helpful in more ways than i can count recently i spoke with a company for a ds position and they sent me a take home assignment a couple of days ago it involves building an full fledged ml web app from scratch the steps include 1 loading tables in a sql database 2 training a model that predicts an outcome and 3 building a rest api that would receive data and post predictions based on the model i trained above in addition they state that it should take only 3 4 hours to complete this really i do not have any meaningful background in building web apps and servers this is pretty clear from my resume also the job description did not mention any such requirements or skills for this particular position although the company has an interesting product i feel i would be wasting my time working on this assignment given my lack of skills i wonder if i should rather spend my time working on other applications assignments interviews rather than doing this i feel really uncomfortable and honestly a little angry that they ve asked me to build an entire project from scratch would love to hear if y all have any recommendations and thoughts about what i should do thank you
2,do you feel in control while working with ml im finishing bsc in ee this year and i have worked on a pretty standard deep learning project in uni doing semantic segmentation using a published model with a couple of tweaks and a new data set i generally love a subject that i can fully understand and make sense of which made me really love math what felt weird to me is how well the project worked and how little i felt in control of it even while having a solid understanding of the math behind it i did not actually know why in practice stuff work or didnt work and it was more of a trial and error rather than a calculated path i know im not the first person to point it out i think ml is fascinating and i m not even close to understanding 1 of it so i genuinly want to hear how it feels for people who dealt with it enough time and have the proper education how much do you feel in control when dealing with a model how well can you predict how things are gonna go does it still feel like a black box to you
1,error term in nlme lme lme4 lmer function i cant seem to find this info anywhere what do the different sides of the in the lme or lmer in lme4 functions represent i e 1 id vs time id essentially why is the 1 to the left of the and why is the id to the right thanks
0,yoe to go from entry level ms data scientist to mid level senior data scientist usa i m a new grad ms working as an entry level data scientist most of the phds at my company are either at the senior level or managerial but most also have a few years of experience too in general how much years of experience do you need to go from my current level to a senior data scientist and then to the principal managerial level i know a few new grad colleagues in swe who got to senior engineer shortly after one year but it seems like that trajectory is not exactly the same for ds i know this will also vary by company with some tech companies requiring a phd for data scientists at the lowest level
0,imposter syndrome and prioritizing what to learn imposter syndrome comes up in this sub a lot and as someone who feels like he has mostly learned to manage it i wanted to share my experience with it and what was ultimately my major breakthrough in a nutshell there are three ideas that you need to get in your head in order to get over imposter syndrome 1 you are a generally competent person 2 there are always going to be people that know more about a certain area of data science than you and that s ok and expected even more importantly you re not the smartest person in the planet so if you look hard enough you re going to find people that are better than you at everything you do and that s ok 3 you have a finite amount of time to learn things and your goal shouldn t be to learn the most but to learn the things that maximize your specific goals generally this is going to be career advancement but for some it may be something else in that order i think that generally imposter syndrome shows up in a thought cycle that goes the opposite direction that is 1 you don t have enough time to learn something you want to learn 2 you look around and see that there are other people that know that thing you don t have time to learn 3 you feel incompetent so when you feel that flip it 1 remind yourself that you are a competent person if you weren t you wouldn t have gotten to the position you are in right now whether that s graduating from college or leading a data science team yes even ds team leaders catch the drome from time to time 2 remind yourself that when you look for people who know more than you about a specific area you are guaranteed to find them that s just how it works people choose to specialize in certain areas and if you only focus on that area of expertise you are going to feel inadequate but even more importantly recognize that if you run into someone who is better than you at literally everything you do that doesn t diminish your value it just means you have run into someone that is pretty special 3 get back to prioritizing what to learn do you need to learn that or do you just want to learn it to feel better about yourself if the latter learn to let it go and focus on the things you need to learn and save the things you want to learn for when you have the time which will come as an anecdote my first encounter with this scenario was a professor that literally did everything i liked doing but better he was a tenured professor at a top school he had come this close to being a professional soccer player and he was a classically trained musician was in incredibly shape for his age and was a generally charming dude i was a fumbling grad student who played recreational soccer poorly and played in a shitty metal band that no one ever went to see play out of shape and generally a not so charming dude it made me incredibly self conscious for about a minute until i realized wait up this guy is just an abject abnormality of humanity i shouldn t feel bad about myself i should just be impressed by how smart and accomplished this guy is because 99 99999 of the population would be looking up at him too that helped me later in life when i would encounter people who i felt were just fundamentally smarter people than me in particular i remember hiring someone for my team that was so smart and thinking there is a better chance that i am going to be working for her in 10 years than the other way around and being ok with that
1,non parametric outlier test on gaussian data why i am reading an academic study re data cleaning of high frequency data in finance as hf data is very noisy and not normally distributed due to fat tails the author uses a standardization procedure to normalize the data in his words the transformed data is “virtually normal” the author then uses median absolute deviation tests to detect purge outliers from his data my question is why if the transformed data is now approx normal why use robust statistics why not use a parametric approach what was the point of the transformation in the first place in case the reference is needed
1,how to compare 2 sample means when the samples are highly left skewed and have outliers hi friends i have few questions that i had in my mind for a long time was hoping someone can help 1 how to compare 2 sample means when the samples are highly left skewed and have outliers is non parametric test the only way 2 how to compare 2 sample means when the samples are both approximately normal but with few outliers may i simply remove the outliers and compare all data points 99th percentile in each sample i imagine if we were to compare salary of men women and i randomly sampled bill gates i can remove him 3 what is the application of central limit theorem on this question my understanding is that clt will return us a normal distribution after we get the means of the resampling let s say i am working on 2 samples that are highly left skewed have outliers after resampling getting the means i am left with 2 samples that are normal may i use the 2 sample t test now to compare sample means
1,question effect size clarification my understanding of effect size is kinda weak and maybe as well as my general statistics knowledge if it would not be too much to ask i would just like to ask if comparing the effect sizes stated here is valid since they have the same units in effect spearman s r 2 from student s t test on spearman s r cramer s phi 2 from x 2 test on contingency table zmax 2 n from kruskal wallis anova dunn’s post hoc test concept is form this paper
2,train model 3x as large with dynamic tensor rematerialization in deep learning you can trade space for compute by recomputing activation in backpropagation phase known as gradient checkpointing classical gradient checkpointing algorithm is great but they dont work for eager execution dynamic tensor rematerialization dtr is a gradient checkpointing algorithm that work with eager execution and is implemented at megenine a deep learning framework read this blogpost to learn more
1,seeking help with anova formula and bootstrapping thanks for the help on my previous post i ve refined things a bit ten participants in two conditions some movements were bad so unequal sample sizes and measurements are related but not paired i e movements in one condition don t match with a movement in the other condition i think the formula would look like this dv condition montage phase subject condition and i think to bootstrap this it would look like so bsaov function formula data indices d data indices fit aov formula data d return something results boot data nfvf statistic bsaov r 1000 formula mean power abs condition montage phase subject condition i m not sure what to return from bsaov to be summarized so to speak by the bootstrapping i could also be completely wrong about anything here reasons for bootstrapping unequal sample size non normal distribution shapiro wilk p 0 001 heterogeneous variance between conditions f test var test p 0 001 two groups multiple ivs so no kruskall wallis i appreciate any suggestions
0,do you guys with all honestly love your job and this occupation in general i m interested with becoming a data scientist or something very related ever since i was 15 but to be completely honest 90 of the posts here are ranting about the profession about how it s not like what everyone s thinking and how much 99 of your job basically isn t fun are you happy with what you do would you take this profession again in hindsight is it worth it or is it just the big money that s driving it
1,history of non parametric models i have been reading about the use of non parametric models in machine learning e g kernel methods like svm kernel regression decision trees gradient boosting random forest and i tried to contextualize the reasons why these methods emerged in my head this is the conclusion i reached 1 parametric models like standard regression models are tricky parametric models require certain assumptions about the data to be true also require the analyst to manually specify interaction terms within variables but the biggest drawback regression models tend to require more beta coefficients to capture more complex patterns within the data a regression model with many beta coefficients behaves similar to a higher order polynomial function and higher order polynomials are notorious for behaving in very unpredictable ways outside the range of observed data runge phenomenon this basically explains why higher order regression models have a bad reputation of overfitting training data and generalizing poorly to new data this is all related to the bias variance tradeoff 2 non parametric models do not require interaction terms between variables to be manually specified e g in a decision tree you don t need to specify this the decision tree will try to recover these interactions by itself and have less stringent assumptions about the data e g choice of kernel the appeal of non parametric methods was an attempt to defy the bias variance tradeoff the idea of trying to make a less complex model with the ability to make predictions comparable to a complex model with the hope that the lack of explicit complexity leading to better generalization on test data 3 the popularity of neural networks a parametric model is due to the fact that researchers found out ways to make these explicitly complex models generalize to unseen data e g effective regularization methods is my interpretation of the history correct thanks
1,any apps or websites that can help me copy and paste symbols into word
0,normalizing merging two different data signals hi all i am analyzing our companies transaction data visits data we have lot of user actions before making a transaction i am trying to segment the user base on the activity signals they are generating but was confused on how to merge multiple data signals distribution for example i have user performing actions such as view an item list an item buy an item since all of these activities have their own distributions i was thinking to normalize each of them for a user last 12 months activity but was not sure if it would make sense to sum up these individual normalized signals in some way so that i have just one score against which i can benchmark users current activity like if i see that the user has x value combined score across all actions in last 12 months and this month he has x 12 i can estimate that he is decreasing so on so forth
0,hired at a small company my job is shaky first ds job working remotely started about 3 months ago it s a very small company in an industry that i was unfamiliar with so there was a distinct learning curve in getting acquainted there was also no onboarding as is to be expected for very small companies i m not sure they needed a data scientist and now i m kind of scrambling to try and show value along with that the part of the business that i m working on does not generate any revenue and has very little data about 3000 sparse data points maybe 1200 good ones very few updates maybe 5 per week and very few insights that i am being asked to run on these actual data points with that i m being involved in the business development side of things higher ups do not know how this current 6 year old project should generate revenue and every week my task changes usually it s about finding open source datasets but my boss has very little focus patience so each week is different i struggle to maintain focus in my day to day data work as it becomes clear that what my boss wants is usually not doable within a reasonable time frame i e investigate a causal question that would be great for an entire econometric paper and will likely not generate revenue anytime soon are there any other ds folk who have been hired into small places with very little data being the only ds how did you handle the situation how long does it take you to do open source data collection i don t mind the field the work nor wearing many hats but i m worried that i won t be generating enough value to justify staying on the team
0,is it common to use the zip code as a predictor variable in statistical models suppose you want to make a model that predicts if a student will drop out of university you have historical information about many students and whether they dropped out or not you also have access to the zip code postal code where they lived 1 is it common to actually the zip code as an input variable probably not since there are too many categories or maybe use the first 3 numbers of the zip code as an input variable 2 i was always told to avoid using a predictor variable that has too many categories is there a mathematical reason behind this from a mathematical standpoint if your data has 1000 rows and one of your predictor has 450 categories mathematically speaking why might this harm your statistical model i can understand it intuitively having too many categories means too much information and your model might get confused but is there a mathematical explanation note i know you can just take students from different cities and make a sepperate model for students in the same city but i am not interested in doing this clarification i am using zip as a categorical variable
2,best cloud dev tool hi all what is the best cloud development setup you ve come across for industry dl research python gcp pytorch i m talking about using a cloud sever as a work station for both development training eval jupyter is great because connection problems don t stop the run but it doesn t have modern tools like debugging auto import etc pycharm vscode in remote dev require connection to sustain through out training dev session but they have amazing tools that make jupyter look like childplay code server ultra buggy anything else preferably paid off the shelf solution
1,what can network analysis tell you that factor analysis can t and vice versa
0,some beginner friendly tips on landing data science internships from a recent college grad hello all from 2019 till today may 2021 i ve submitted roughly 550 applications for various tech roles in california software engineering internships data science data analytics internships machine learning deep learning computer visions internships entry level ds swe positions non internships my background 3 0 4 0 gpa recent graduate with a bachelor s in physics a top 10 public school and a background in data science disclaimer i have never interviewed with a faang company so this certainly is not a guide to landing your dream faang job as a non cs major with a mediocre academic standing i hope some of you guys will be able to relate to or learn from my intern searching experience starting off the resume for beginners in my 2nd year of university i started taking interest in data science and signed up for some online courses after 3 months i quickly realized that my resume was lacking in work experience skills and just an overall description of my career objectives moreover my degree in physics was worrying how can i compete with undergraduate students studying cs or data science the solution open source data projects this is key to kicking off your data science resume building journey and in my opinion the most efficient way to learn data science learn a programming language for data science r or python and stick with it for at least a year you should start with 1 basic programming syntax coding logic data structures 2 data cleaning and wrangling 3 data visualization 4 basics of statistical analysis 5 linear algebra 6 regression modeling more advanced but a great way to start learning machine learning i found dataquest to be very digestible and enjoyable in the early stages of learning data science there are also tons of other free resources that you can use to practice and learn your basic data wrangling skills kaggle uc irvine s repository in a few months try to formulate a project idea that can be solved with data science i wanted to analyze visualize time series data for a company s drone flights and luckily they were kind enough to send me some data files to work with using python to do some cleaning and visualizations i discovered that drones were able to fly further in a specific air temperature range an engineering team could potentially use this information to implement a cooling system in their drones to maintain internal temperature and improve flight efficiency even if your project doesn t feel insightful or creative try to stick with it for the sake of learning even if you don t make an astonishing finding you will develop very strong ds fundamentals through these data exploration processes tldr if you feel that your resume is lacking experience work on data science projects the more unique and personal the better these will become great talking points in your interviews building your experience networking research for those lacking experience by 2019 3rd year undergrad my resume contained 2 simple eda projects exploratory data analysis and i felt ready to start applying to internships for the next 365 days i applied to 207 positions during this time i received 6 interviews and 1 offer for an unpaid data science position remote with a 3 0 hear back rate i felt very discouraged the one offer i got was hardly an internship this percentage told me that my resume and skills were probably not competitive enough which drives me to my next point take advantage of your academic resources while you are in school i did not attend any career fairs while i was in university shame on me but to compensate for this i reached out to several professors in my department offering to do research work for them for free up to 20 hours a week thanks to a colleague i landed a data analyst researcher role for my university in the summer of 2020 during this time i picked up some shell scripting real time programming and data modeling skills i know unpaid research is not as flashy or alluring as a google internship but trust me anything that adds 2 6 months of data science experience to your resume is huge regardless i wished i had focused on networking more when i was still in university would you rather spent 6 months applying to 200 positions or immediately land several interviews in a matter of weeks due to connections yep i would have preferred the latter tdlr reach out to professors colleagues or even cold email recruiters be resourceful general interview job searching tips if you did poorly on an interview or got turned down learn from it you should quickly discover what you are lacking after several rounds is my coding logic poor or rusty practice more leetcode while talking out loud is my skill set not desirable enough research valuable ds skills for the current year what sort of skills are desired in this specific industry tailor your resume accordingly stuttering or awkwardness record yourself or practice behavioral questions with a friend know your script your resume projects experience strong points weak points while i do have a google doc containing scripts for what i should say i never actually read from it an interview script should be more of a rough blueprint for rehearsing and practicing try to talk about something personal or outside of the job don t force it but if can laugh with your interviewer connect on a more personal level you will leave a strong impression tailor your resume to what you are applying to if you are targeting computer vision roles orient your projects around that if you are focused on machine learning roles your resume should reflect expertise in modeling and training prediction techniques an interview is mutual after 20 rejections i know it is easy to place immense pressure on yourself to land that dream internship remind yourself that you are interviewing the company as well embracing this mindset will turn your interviews into meaningful conversations which is what you want ask good questions if you feel that the bulk of your interview was not excellent try finishing strong by asking great questions some examples what are some of the biggest challenges you ve faced in your role a favorite project that you ve worked on where do you hope to see your data science department in x y years how is your company utilizing machine learning techniques to improve sales customer retention do some brief research on linkedin before you meet with your interviewers might find something in common or help you come up with good questions it s also a good idea to read into the company their products how they generate their you will likely hear back from companies in la irvine oc more often than ones in the bay area 63 of my hear backs interviews were from companies located in southern california my job hunt statistics rough breakdown 50 of applied jobs were related to data internships ds da ml deep learning data eng 25 of applied jobs were related to software internships front end back end etc 15 of applied jobs were related to full time entry level positions in data analytics ds 10 of applied jobs were related to marketing analyst business analyst or other non tech analyst positions 8203 year 2019 2020 2021 total apps 207 57 257 516 interviews 6 0 5 11 offers 1 0 2 3 my three offers consisted of 1 remote unpaid part time data science internship 1 digital marketing analyst position and 1 paid full time data science internship currently pursuing tdlr don t give up this field is competitive but through sheer numbers extraordinary connections interview skills or luck you will find a suitable internship i believe in you resources my favorite job searching platforms linkedin indeed glassdoor my favorite data science resources kaggle uci repo tech with tim youtube channel 3blue1brown youtube channel amazon s datasets cracking the coding interview a painful book to grind through but it is very helpful o reilly s hands on machine learning with sk learn keras and tensorflow 2nd ed my favorite book reach out to me in 12 months i hope to be enrolled in a graduate program for data science i am no expert in data science and i still have a lot to learn nonetheless the past 2 years have taught me a lot about the data science job market if you any questions or a strong desire to talk about data science please feel free to reach out to me i am happy to share my project source code resume and additional tips upon request
2,how can machine learning help improve seo can machine learning models help improve seo performance in any way for example google analytics is able to track data without cookies with the aid of machine learning any other ways in which seo can be revolutionized by machine learning
2,extrapolation vs interpolation while using statistical models i have started to read more about the background of some of the popular statistical models used for predictive analytics and i find some of this information troubling although there is apparently no such thing as a universally best algorithm i e there will be always some dataset where algorithm a performs better than algorithm b and vice versa apparently some algorithms are inherently better at untangling complicated patterns within the data the logic being other algorithms are inherently incapable to capturing complex patterns e g non linear patterns and as a result this can impact their abilities to extrapolate and to interpolate x200b here is an example i thought of x200b suppose you have a dataset with weight and height measurements of individuals and whether they have a certain disease or not this is a binary response variable red no disease and green is disease the goal of this problem is to predict whether a new individual will have the disease based only their height and weight measurements however little do you know the majority of the data comes from children and only a few of the observations come from adults as a result most of the observations have low values of weight and low values of height as typical in children and only few of the observations have larger values of weight and larger values of height as typical in adults thus one could regions of the data with large weight and large height are underpopulated naturally one could expect that it might be more difficult to make accurate predictions for adults given that most of your data reflect information among children but let s imagine that when we are working on this problem we are not told which point corresponds to an adult or a child we can just infer this information using logic and only notice that large weight and large height are underrepresented in the data we are only given 3 variables height weight and disease status x200b now the question is about choosing a statistical algorithm for this problem given that one of the regions within the data is underpopulated x200b 1 the first question i have suppose we want to make a prediction about whether a new adult large height large weight has the disease or not in the context of this problem would this be considered as interpolation or extrapolation and suppose we want to make a prediction about whether a new child low weight low height has the disease or not in the context of this problem is this extrapolation or interpolation x200b just using logic based on the distribution of the available data i would say that making a prediction for a new adult is relatively more of an extrapolation task compared to making a prediction for a new child is more of a interpolation task is my logic correct x200b 2 the second question i have is about preemptively understanding the limitations of certain algorithms when dealing with this kind of problem i have read about some inherent problems of decision trees such as it can be theoretically shown that a decision trees can not generalize to unseen data b decision trees require an exponentially large number of data points to maintain a certain level of accuracy c decision trees can only make boolean splits in the data further limiting their ability to make inferences beyond the data they have seen x200b on the other hand algorithms such as neural networks do not require an exponentially large number of data points to maintain the same level of accuracy and neural networks do not make boolean splits within the data when neural networks pass information from layer to layer information becomes represented as a function of all variables within the data thus instead of linear separations characterized by boolean splits neural networks decision boundaries are able to be non linear and capture a higher level of complexity within the data but apparently the main advantage of non boolean splits is that neural networks are able to extend the information and influence of their data to regions that are located further away apparently this theoretically gives neural networks to have better extrapolation abilities compared to decision trees x200b i tried to draw a sketch of this below click the magnifying glass symbol x200b can someone please confirm my logic is this correct thanks
1,what’re some good free cheap online resources for learning stats specially regression methods multivariate analysis and bayesian stats p s i have a strong mathematical and comp sci background in case that’s relevant
1,depth in tree based algorithms vs neural networks on the expressive power of deep architectures 2007 bengio and delalleau i am reading this paper over here on the expressive power of deep architectures 2007 bengio and delalleau in this paper they make a statement one of the characteristics that has spurred much interest and research in recent years is depth of the architecture in the case of a multi layer neural network depth corresponds to the number of hidden and output layers a fixed kernel support vector machine is considered to have depth 2 bengio and lecun 2007a and boosted decision trees to have depth 3 bengio et al 2010 if i understand this correctly a neural network is said to have the same depth as the number of layers e g a n layered decision tree will have a depth of n whereas boosted decision trees are said to have a fixed depth of 3 no matter how splits the boosted decision tree is said to have does anyone know why this is why does a really deep boosted decision tree still only have a depth of 3 thanks
0,retrieve similar images i am trying to build a similar image retrieval system where given an image the system is able to show top k most similar images to it for this particular example i am using the deepfashion dataset where given an image containing say a shirt you show top 5 clothes most similar to a shirt a subset of this has 289 222 diverse clothes images in it each image is of shape 300 300 3 the approach i have includes 1 train an autoencoder 2 feed each image in the dataset through the encoder to get a reduced n dimensional latent space representation for example it can be 100 d latent space representation 3 create a table of shape m x n 2 where m is the number of images and each image is compressed to n dimensions one of the column is the image name and the other column is a path to where the image is stored on your local system 4 given a new image you feed it through the encoder to get the n dimensional latent space representation 5 use something like cosine similarity etc to compare the n d latent space for new image with the table m x n 2 obtained in step 3 to find retrieve top k closest clothes how do i create the table mentioned in step 3 i am planning on using tensorflow 2 5 with python 3 8 and the code for getting an image generator is as follows image generator imagedatagenerator rescale 1 255 rotation range 135 train data gen image generator flow from directory directory train dir batch size batch size shuffle false target size img height img width class mode sparse how can get image name and path to image to create the m x n 2 table in step 3 also is there any other better way that i am missing out on thanks
1,include intercept or not in bayesian linear regression question i m running a very simple bayesian linear model with categorical predictors as follows y dnorm mu sigma mu a bx i a dnorm 0 10 bx i dnorm 2 1 sigma dexp 1 where x is a categorical variable with 4 categories which i indexed 1 through 4 in the model there is one b parameter for each category if that was not clear from above i m debating whether to include the intercept a or not in this model when i do i get standard deviation values for each b parameter which are quite larger than when i do not include the intercept to be honest i m also a little confused about the meaning of the intercept in this case with the categorical variables and i know with these models some people use an intercept while some also don t see last couple models i would appreciate if maybe someone could help me out thanks
1,why do polynomials e g polynomial regression have a bad reputation of overfitting we all must have heard by now when we start learning about statistical models overfitting data the first example we are often given is about polynomial functions e g see the picture here we are warned that although higher degree polynomials can fit training data quite well they surely will overfit and generalize poorly to the test data my question is why does this happen is there any mathematical justification as to why higher degree polynomial functions overfit the data the closest explanation i could find online was something called runge s phenomenon which suggests that higher order polynomials tend to oscillate a lot does this explain why polynomial functions are known to overfit data i understand that there is a whole field of regularization that tries to fix these overfitting problems e g penalization can prevent a statistical model from hugging the data too closely but just using mathematical intuition why are polynomials known to overfit the data in general functions e g the response variable you are trying to predict using machine learning algorithms can be approximated using older methods like fourier series taylor series and newer methods like neural networks i believe that there are theorems that guarantee that taylor series polynomials and neural networks can arbitrarily approximate any function perhaps neural networks can promise smaller errors for simpler complexity but does anyone know why polynomials are said to have a bad habit of overfitting to the extent that neural networks have largely replaced them interesting paper
1,does increasing sample size increase test size as well as power power and test size of a test procedure are computed in the same way based on the probability of rejection under each parameter value increasing sample size can increase power does that also increase test size and therefore increase the risk of test size exceeding the level of significance how do people manage to increase power without risking test size exceeding level of significance thanks
0,what do you use to run jobs on a schedule i have to run some computationally intensive jobs and i m not sure what the best practices are right now we manually run ec2 instances in the morning but i m looking to automate this we ve looked into apache airflow simple scripts aws lambdas kubernetes cron and rundeck can you run jobs with spark rundeck seems the most promising what do you guys think
1,arizon election fraud statistics crank found this link since someone told me it was “mathematical evidence” that the election was rigged in the state of arizona 2020 i started watching it hoping to follow along with this guys analysis and i just don’t understand what he is talking about when he talks about “correlations between 1 vs 2 hardline republican vs slightly red 1 vs 3 hardline republican vs moderate 1 vs 5” “and the average correlations are 63 for men ” i don’t know what he means by correlation or what he is measuring between groups his “tedious analysis” seems like it would be at most 5 lines in r to accomplish separating and summing into a bin for age gender and voter type i was wondering if anyone else might be able to share some insight on this guys “analysis” since i can’t understand it
1,question about r lmer formula syntax i have a group of subjects that participate in two conditions condition within those two conditions they do the same things phase i measure one thing dv in each phase i m confused about the proper formula syntax would it be this because subject is the grouping factor dv condition phase 1 subject or would it be this because subject is nested under condition or is that vice versa dv condition phase 1 subject condition or something completely different
1,i have many observations outcomes generated using specific parameters and i m wanting to find the optimal combination set of parameters that maximizes the probability of a good outcome and not necessarily the set of parameters that maximizes the outcome for the given observations this might be a simple question but admittedly it has been a while since my statistics courses in college i have a set of inputs a b c and d that produce output e some positive some negative i m wanting to find the specific combination of inputs that maximize the probability of a positive outcome for a while i ve been trying to find the set of parameters that maximize the outcome but i ve realized that a lot of this could be pure luck or down to the specific circumstances at the time which my model might not be accounting for and so i m wanting to rather find the parameters that would maximize the probability that i d arrive at a positive outcome i e where outcome e is more than some value x i ve performed multiple regression on my observations which produces what i believe to be a sufficient adjusted r square 0 6 and so therefore i have a feeling that the solution to this problem would involve multiple regression and a line of best fit but i m unsure as to the exact approach to get to the optimal set of parameters here s an example of the data x200b a b c d e 0 15 0 9 3 475 790 0 15 0 7 2 4 925 780 0 15 0 9 3 525 778 0 15 0 9 2 3 825 729 0 15 0 8 2 5 725 725
1,calculating probabilities in clue cluedo in the game clue there are 21 cards and 1 of each type weapons 6 cards rooms 9 cards people 6 cards is randomly selected and placed in an envelope the remaining 18 are distributed among the players the goal is to determine which three cards are in the envelope in a game with 3 players each player will receive 6 cards i’m trying to probabilistically determine which cards are in the envelope based on known information throughout the game i’m convinced that estimating the likelihood of each specific card being in the envelope can be calculated independently and the sum of all the unknown cards should sum to 1 before the first turn 6 cards are known because they are in my hand i have w1 w5 r2 r6 p1 p3 so the probability that the unknown cards are in the envelope are w 2 3 4 6 25 r 1 3 4 5 7 8 9 143 p 2 4 5 6 25 scenario 1 i suggest w2 r1 p2 and the next player shows me w2 now the card is known so the envelope card probabilities are w 3 4 6 333 r 1 3 4 5 7 8 9 143 p 2 4 5 6 25 scenario 2 player 2 suggests w3 r1 p3 to player 3 and they show them a card knowing that one of those three cards is not in the envelope must decrease the probabilities for those three and increase them of other unknown cards but by how much scenario 3 player 2 suggests w6 r5 p4 to player 3 and they don’t have any of the cards knowing that none of those three cards is in their hand and therefore are more likely to be in the envelope must also increase the probability that one or more of those cards is a winning card but how much thanks
2,help in collecting a dataset for building a neural network hi i m a junior developer and i m creating a machine learning project so please fill out the form below you will really help me in the development the data is completely anonymous and depersonalized so no need to worry about it it will take you no more than 5 7 minutes please distribute this form so that i can finish my project faster
0,which dimensions are you probing when talking to a company about a new job hi interviews are not only a way for the company to test candidates it is also an opportunity for the candidate to decide if the company can be a good fit or not i am going to have the first round of interviews next week with a medium sized financial service company they re opening a new medior senior data scientist position besides questions around the job content i am thinking about probing the following meta dimensions to detect potential orange or red flags size seniority background of the team company culture how mature is the organization in terms of data storage and management where does the data science team fit in the organigram how close you are to key stakeholders where does the company see the data science team in 5 years and you what are the key dimensions that you are probing when you talk to a company about a new job x200b edit thank you so much for all the valuable answers they are incredibly helpful
2,random forest vs gradient boosting out of distribution hello everyone i m working on a classification task where i have data from a certain company for years between 2017 and 2020 trying to train different models random forest xgboost lightgbm catboost explainable boosting machines on separate data with one year at a time from 2017 to 2019 and looking at the results for 2020 i see a curious behavior and i would like to understand whether it is a normal one in the literature or dependent on the particular data in particular while training with data from 2019 all the boosting algorithms obtain better performances than random forest 0 78 0 79 auc vs 0 76 this dramatically changes when i train a model on 2017 or 2018 data for 2020 this data is slightly out of distribution as there is for sure label shift and data is quite different and the learned models feature importances pdp are quite different between the years but here random forest still learns to generalize decently for 2020 data we have a auc of 0 704 if trained on 2017 and 0 706 if trained on 2018 while the boosting algorithms have on average worse performance with a big difference for lightgbm between the two datasets for 2017 xgboost 0 567 lgbm 0 565 catboost 0 639 ebm 0 521 for 2018 xgboost 0 661 lightgbm 0 734 catboost 0 639 ebm 0 685 provided i have not performed extensive hyperparameter tuning and further testing and this might be a really particular case dependent on data and hyperparameters still i was wondering does there exist some literature i cannot find on the robustness out of distribution of random forest vs boosting algorithms which might explain this behavior because intuitively it might make sense that the variance reduction obtained by bagging would help even out of distribution as some learners might still have learnt something relevant but i am not sure it is enough ps as a sanity check i also tried with a logistic regression and a gaussian nb which have the same consistent decrease in performance 0 7 to 0 45 0 6
1,controlling for year effects using dummy variables i am currently reading the master thesis of a friend of mine the aim of the thesis is to measure the impact of the fund size of a private equity fund on the performance of the fund using a regression analysis among other control variables the year in which a given fund was launched is included my friend argues that this allows to control for differences between economic conditions which are related to the specific year what makes me wonder is the implementation of this control variable for each year he creates a dummy variable in the following way 2010 1 if the fund was launched in 2010 and 0 otherwise 2011 1 if the fund was launched in 2011 and 0 otherwise 2012 1 if the fund was launched in 2012 and 0 otherwise my understanding is that the variable year is an interval scaled variable is it legitimate to control for year effects in this way what would be an alternative better way to control for possible effects emanating from the launch year of the fund i have no experience in panel data regression but i thought that this may be a more appropriate approach to use here thanks
1,question calculating conditional sensitivity hey all consider a population that i am attempting to create a cohort from the population is described by 2 binary variables a and b my cohort will be composed of people who a 1 and b 1 i have 2 imperfect models a and b which may have correlated errors which i will use to select my cohort i can measure the sensitivity of each prediction individually i e sensitivity of a and sensitivity of b my question is how would you go about measuring how the performance of one model affects the performance of the other model when selecting the cohort x200b some thoughts i currently have 1 sensitivity of a conditioned on b 1 and b 1 i e sensitivity of a among people who should be selected into the cohort this approach however seems to ignore incorrect classification of b 2 sensitivity of a conditioned on b 1 i e sensitivity of a among people selected into the cohort this approach does not account for the correctness of b classification 3 sensitivity of a condition on b 1 i e sensitivity of a among people who should be selected into the cohort this approach doesn t account for the correctness of b as well and does not describe the cohort of interest a 1 and b 1 x200b perhaps i am approaching this with the wrong tool so i am certainly open to other suggestions thanks
1,would i face any pitfalls if i only used inter and intra cluster distance for determining the quality of my clustering hi all i currently have a function in r which generates a distance matrix from some data where i have a pre defined grouping of it my goal is to evaluate the performance of a parameter in this function by going from 0 to a high number where i believe the evaluation is possible because i already know the correct clusters beforehand i m thinking of doing this by calculating the inter and intra cluster distances generated by each increase in the parameter where the optimal point of the parameter would be the point where the inter cluster distance is the highest and intra the lowest i was wondering if this strategy is discouraged or not by statisticians however or if better strategies of doing this might exist especially if they are implemented in r thank you all for your help in advance
2,need help understanding the usage of rolling window sequence in a research paper i m currently trying to implement this paper in the paper they calculate subsequences of the original time series data using a rolling window method see the image below x200b but what i don t understand is that how do you calculate the rolling window sequence at time step k 0 when there are no values behind it do you just pad the sequence with values of zero or the mean or think of it as negative indexing to me that sounds a little stupid
2,face tracking for musicians x200b usecase while digitalization is transforming our lives 99 of musicians are still playing with physically printed scores in contrast to printed books this is not because they like them they are quite a pain actually you must buy print them and therefore you cannot just spontaneously try out a piece if you play longer pieces you must interrupt your playing to flip the page or ask someone to do it for you they often fall off or get blown away by the wind if you are outside but sad truth is that there isn t really an alternative yet of course there are apps but there you must interrupt playing all the time to flip the page or preset a speed that is super annoying which is why musicians have always stuck to printed scores concept by flipping the pages via facial expression e g by opening the mouth or turning the head rapidly in the end there have to be several options to accommodate different instruments preferences digital scores become super handy and easy to use all one needs is an app for smartphones and tablets that can recognize facial expressions via the front camera and flip the page building on that one can implement lots of premium features e g a pedal to make the app profitable as well technical solution i have already built a little python prototype video with opencv facial landmarks where one flips pages by opening the mouth obviously it s very vulnerable to changing light conditions and from what i could find out online the process should be too tough on computing power for most smartphones tablets therefore i m open to taking a different approach to the development of the actual product one approach that came to my mind 1 take a dataset with e g 50 000 human faces and use open cv or something comparable to label them according to the degree of mouth opened and later blinking head turned etc label the ones where landmarks don t work manually and ideally check all the other ones as well 2 train a neural network on the labeled dataset assuming that running it later requires less computing power than landmarks 3 implement the neural network in the mobile app assuming that python wouldn t be efficient enough i am thinking of pwa for example xamarin or react native context i m myself a recent architecture graduate therefore my coding skills beyond python and c are slightly limited i did the course on machine learning by andrew ng and did a few projects like the prototype still i wouldn t consider myself advanced in the field even worse i haven t ever written a mobile app yet i m looking for 1 your feedback and your input particularly on the technical solution 2 you i d be more than happy to share this adventure feel more than free to contact me cheers julian in case there s trouble with the vid
1,question when one can claim a variable is a risk factor suppose we have conducted a study and it turns out variable a like being high on trait neuroticism predicts a specific outcome like depression but can we claim a is a risk factor for b or do we need a causal framework design to claim such a thing
1,stupid question why is it called deep learning instead of wide learning today my friend was telling me about something called the universal approximation theorem which apparently contains the mathematical foundation behind neural networks work after watching a lot of youtube videos on this topic i think i have some understanding on this topic supposedly a 2 layer neural network can approximate any function and the error of this approximation is proportional to the number of neurons within the neural network the more neurons there are the better the approximation will be the only problem with this is that a 2 layer neural network will likely require a very large number of neurons to produce a decent quality approximation apparently so many neurons that the computations will become very inefficient conceptually you can imagine a 2 layer neural network as being very wide looking supposedly the solution to the above problem is to create neural networks that are deep instead of wide so instead of a 2 layer neural network with a very large number of neurons it s said that a neural network with fewer neurons and more layers is more computationally effective and these deep neural networks also have the universal approximation property the question i have if wide neural networks require a very large number of neurons to adequately approximate a function why don t deep neural networks require a very large number of layers to approximate a function at a similar level of accuracy what explains this mismatch between the number of layer and number of neurons equivalence suppose a wide neural network with a relu activation function 2 layers and 1000 neurons is able to approximate some function with an error of 0 05 clearly we believe that a deep neural network also with a relu activation function with 1000 layers with each layer having 2 neurons is not necessary instead we believe that the number of layers does not need to be so deep what explains this mismatch why are deep neural networks typically able to maintain a relatively low number of neurons and layers compared to wide neural networks where only the number of layers can be kept small
1,cronbach s alpha question i have 5 measures of structural disadvantage and was seeing if i could combine them into an index 4 of the measures are percentages of social and economic variables and the 5th measure is median household income an alpha test on the 4 percentages alone gives me a cronbach’s α 699 but when i include median household income the value drops drastically to 006 i am not going to be combining them into an index anymore but i am interested is why the values are so different from adding one variable that should be correlated with the others is this because median income is measured on a different scale or is it simply that median income is completely unrelated to my other measures
1,why is expected frequency in a chi square test for independence calculated with row total column total grand total i know how to calculate expected frequency but i am having trouble understanding why it is calculated that way if we are checking for independence then why do we not assume the observations will be evenly distributed again i know the method i m just trying to understand the reasoning behind it
1,non inferiority for diagnostics how is this done this field is too damn big i didn t touch a lick of this sample size planning diagnostics in my master s and google shows nothing any tips on how i can plan a study for checking say is this covid test any less accurate for males than it is females it depends on the chosen metric of course but let s say i want to use auc and i want to make sure i show that auc for males is no more than 3 points less than auc for females should i treat it is a non inferiority proportion problem because the auc is the probability a randomly drawn positive has a higher score than a randomly drawn female which is like a proportion
0,jira roadmap backlog anyone else has to pedantically update jira tickets for every task subtask with story point estimates i m having trouble communicating the fact that even i don t know how much time an analysis for a client might take it s not a developer journey where i know where the insight function i have to write pre hand i don t know where the insight might lie i can t give you an estimate that s accurate to the hour or am i being naive here
1,how do people even come up with neural net architectures as a beginner to dl coming from stat i find the conceptual and math understanding of how stuff like dense conv layers activations dropout regularization pretty straightforward but these architecures like resnet lenet unet insert whatever net the latest greatest transformer how the hell do people come up with this stuff it seems like total shooting in the dark there are infinitely many possible architectures and hyperparameters to try like how do you just stumble upon these im also wondering bc i am taking a dl class but the project throws you on the deep end no pun intended and you have to do something novel applied to a dataset that includes modifying the architecture and im like i don’t even know where to start
1,what does it mean to orthogonalize a variable and how can i do it i have to replicate a paper in which the authors “orthogonalize each variable against a set of macroeconomic conditions to reduce the likelihood that the proxies are connected to systematic risk” after the orthogonalization the authors take each of the orthogonalized variables to form an index using pca i have tried to google it but i couldn’t understand how can i orthogonalize each variable any help would be really appreciated edit link of the paper
0,project lifecycle i am still in academics and most of my activities and projects have had limited scope and it s nothing like what i may work like in industry i would really appreciate if someone can share their experience of working as a data scientist in the industry how do you go about finding a problem challenge idea of what to work on what steps does a project go through do you follow sdlc kind of methodologies for a ds project do you participate in model deployment what tools do you use i know this is not a structured list but even though i am working through school it is really difficult for me to absorb anything without actually understanding industry practices
1,mplus question how to get more decimal places i m trying to figure out a way to get more decimal points for my estimated correlation matrix for the latent variables i m using tech4 to call up the results but i need 10 decimal places not the default 3 any help thanks so much in advance estimated correlation matrix for the latent variables in dj we pj djcpjc in 1 000 dj 0 622 1 000 we 0 247 0 061 1 000 pj 0 643 0 400 0 262 1 000 djcpjc 0 010 0 016 0 135 0 006 1 000
0,has data science failed to answer criticisms over bias and discrimination we ve all seen multiple articles along the lines of lack of diversity in data science leads to algorithm being biased against it s an interesting and important topic but am i alone in thinking that most of the articles written about this show a complete lack of understanding about what machine learning is how it works and even what bias and discrimination are and that data science and data scientists are generally pretty silent about this or even seem to go along with the accusations so as not to be seen somehow as pro discrimination i watched something i think from google about this recently where one of their senior ds mles was talking about how an algorithm was biased because it was more likely to predict men earned 50k than women but isn t it a fairly established fact that men on average earn more than women and so you d expect a higher of men to earn above 50k and so any good algorithm should be picking that up to fix this problem you d essentially have to doctor your data until you get the result you want which is just nuts i m not saying there s not any ethical issues around these kinds of topics at all ds should be aware and responsible for not reinforcing discrimination that already exists but if the world and the data it produces have biases discrimination and even just plain old differences between groups built into it ds should be reflecting that i mean if you were building a salary predictor and you found that it predicted higher salaries for 50 year olds than 19 year olds who in their right mind would think that was a result of ageism
1,a b testing book recs hi all i’m about to graduate from college and was lucky enough to have gotten a position as a financial analyst however my interest is not finance but data driven marketing i’d love to work as a marketing analyst or business analyst so was wondering if anyone know of any great books for a b testing and other methods also if anyone is in the marketing analytics field would you mind sharing what your day to day looks like thank you
1,question should we not be careful in our usage of the terms errors and residuals especially in context of linear regression assumptions as far as i know errors pertain to the true data generating process residuals are something that happens because of us fitting a model basically the difference between observed and predicted values however i have seen many statisticians data scientists use the terms synonymously they often use it in the context of assumptions of linear regression i have seen many use the wording the errors must be normally distributed i think it actually should be the residuals must be normally distributed since there is no way one could know for sure about the errors and whether they follow normal distribution or not so which is the correct terminology errors must be normally distributed or residuals must be normally distributed
1,thinking about approaching people in the street surveys i was just thinking about a job i did for a fortnight or so some years back what we had to do was patrol a relatively small area e g the main street of a town or this desolate wasteland and approach absolutely everyone we could we d then ask them if they wanted to be in our survey if they said no we counted that on a tally sheet if they said yes we then asked them an exclusion question if they weren t excluded we then took their email and sent them a link so they could do the survey at a later date i think there was a prize draw there was also a bit where we had to determine the age of the respondent those under 15 were excluded and we couldn t go inside any properties not even shops which was an issue since the exclusion test was geographically defined and very strict like 10 113 people who wanted to be in the survey could actually do it is there any inferential validity to this kind of survey like how do you do probabilities of selection
1,q statistics book for university recommendation hi everyone i asked about a good probability theory book and got a suggestion for basic probability theory by robert ash which is a great book i need one for statistics the semester course consists of 1 statistical structure sample statistics ordinal statistics variational series sample moments and sample distribution function their properties 2 point estimate unbiasedness consistency optimality the uniqueness theorem for the optimal estimate 3 likelihood function sufficient statistics complete statistics factorization theorem 4 rao cramer inequality effective estimates 5 the rao blackwell kolmogorov theorem optimality of estimates that are a function of complete sufficient statistics 6 method of moments properties of estimates obtained by the method of moments 7 maximum likelihood method properties of maximum likelihood estimates 8 confidence intervals central statistics methods and the use of point estimation 9 testing hypotheses neumann pearson lemma 10 kolmogorov s goodness of fit criteria and x square 11 criteria for the homogeneity of smirnov and the x square if anyone has any good books to learn this i d be very grateful thank you
0,anyone here working for a non profit or any organization that isn t obviously evil i guess jokes aside i would really appreciate if you could share what role you play in your org and if it s fulfilling work this is the setting i m determined to work in and i need an idea of what roles are given to people with data science skills tia
1,how to customize my mstat program hello i was recently admitted into the mstat program at my local university and am starting this fall i’m really excited because i majored in math and found that the statistics and probability courses i took were really interesting to me whereas the pure math stuff ugh real analysis was not a ton of fun anyway i’m reaching out to the community today to get some guidance on what would be the best way to tackle my program where i’m at is typically the degree requirements include 1 one section of introductory probability 2 two sections of introductory statistical inference 3 two graduate sections of linear models 4 one graduate section of mathematical statistics 5 four graduate electives either from the math department or “scientifically oriented” sections from other departments so when i completed my bs i already worked through items 1 and 2 and so that opens up three additional electives for me so i am pretty much in a boat where i can tailor this degree to be just about whatever i want since the only statistics i’m required to do are the graduate sections of linear models and mathematical statistics i have seven electives who would have thunk it so i’m trying to figure out what to do with all of this freedom the thing that immediately came to mind is that my computer programming is definitely lacking and so i thought i would go through the cs department’s prerequisites to eventually take two graduate sections of software development and then some combination of a few different graduate sections of probabilistic machine learning and artificial intelligence my question is are there areas where my electives might be better spent i have a very naive idea of what machine learning and artificial intelligence are but based on what i’ve read it seems like some experience with those ideas would be helpful when i enter the workforce it could be that i am just selecting these because they are particularly popular areas in the industry right now and i recognize i am biased by that and of course i still want to use some of the electives to pursue more mathematical sections stochastic processes and time series analysis come to mind but where i have all of this freedom to cater the program to my needs i immediately felt like this was a great opportunity to really apply myself and learn how to make computers do cool things while also bolstering my statistical know how any insight you can offer to me is appreciated and obviously i would really prefer that responses come from folks that have been through mstat or phd programs themselves
2,cmu researchers propose ratt randomly assign train and track a method for guaranteeing ai model generalization the approximately correct machine intelligence acmi lab at carnegie mellon university cmu has published a paper on randomly assign train and track ratt ratt is an algorithm that uses noisy training data to put an upper bound on a deep learning model’s actual error risk model developers can use ratt to see how well a model generalizes to new input data the researchers demonstrate mathematical proofs of ratt’s guarantees and conduct experiments on various datasets for computer vision cv and natural language processing nlp models in their publication when a trained model gets a high error rate on randomly labeled or noisy data but a low error rate on clean data the model is assumed to have a low error rate on new data full summary paper
2,enigma gpt 2 trained on 10k nature papers and an interactive game where you have to tell the difference between real abstracts and generated ones project enigma stefan zukin a grad student trained gpt 2 on 10 000 nature papers and created an interactive game with the model where you try to distinguish between a real abstract and one that is computer generated some of them are quite hard
2,selecting the right algorithm i am a student learning more about ml so please excuse my limited knowledge while learning about how to select the right ml model i understand there are hundred different factors involved on this i found a kdnugets article which includes size of the data as a factor in selecting the right algorithm the article says it is usually recommended to gather a good amount of data to get reliable predictions however many a time the availability of data is a constraint so if the training data is smaller or if the dataset has a fewer number of observations and a higher number of features like genetics or textual data choose algorithms with high bias low variance like linear regression naïve bayes or linear svm if the training data is sufficiently large and the number of observations is higher as compared to the number of features one can go for low bias high variance algorithms like knn decision trees or kernel svm in this explanation i don t understand why or how did they come to this conclusion of choosing these algorithms for such a dataset size can someone please explain
0,how do you create storyboards hi all i’m working on a project that requires a story or vision i don’t want it to look crude but don’t need to spend a ton of time on it what’s a good compromise to start with i figure a good starting point is a step up from powerpoint no disrespect to ppt but… thanks
0,thoughts on julia programming language so far i ve used only r and python for my main projects but i keep hearing about julia as a much better solution performance wise has anyone used it instead of python in production do you think it could replace python provided there is more support for libraries
2,intel said they don t plan on releasing their work on gta v enhancing photorealism enhancement as a mod as it is research only what efforts are currently being made if any to turn their research into a working mod can we get people started on this please video side by side comparisons paper
0,exploratory data analytics does anyone know of any research papers that have an extensive exploratory data analytics phase i am looking to see how professional researchers document the exploratory data analytics phase and i am trying to learn from them so far all i find are research papers with minor exploratory data analytics nothing too extensive can someone please recomend something a bit more detailed e g researchers extensively doing exploratory data analytics on their data before making a predictive model for corona virus thanks
1,difference between inductive bias and model assumptions recently i came across this term inductive bias i am having difficulty understanding what is the difference between inductive bias and model assumptions an example of an inductive bias stackoverflow link says in logistic regression there exists a hyperplane which separates negative positive examples wouldn t this be the same as a model assumption can someone please explain the difference thanks
1,bayes’ theorem precautionary principle hello i’m currently reading up on bayes theorem in preparation for handling environmental statistics next semester and i have some questions about the information is the line drawn much like walking a tightrope when we discuss bayes rule and the precautionary principle is bayes rule theorem the form of statistics best suited to tackle questions of precautionary principle i e when we discuss pollution’s impacts on human health is bayes’ theorem best suited to answer questions of where the invisible ‘line’ between too much restriction on a pollutant and too little restriction based on metric of human health and negative impacts on it and the environment or is the very application of precautionary principle a further “bias” when handling data
2,would it make sense to use a rnn to approximate a linear system i ll start with a very simple example let s say i want to compute something similar to exponential moving average of time series data but with only n previous input values being averaged i can make a vector of exponentially decaying weights and compute dot product of this vector and n previous inputs getting the result i want but of course it s inefficient and for large enough n it doesn t give much of a difference from a standard exponential moving average computed using feedback so i can just use ema as a good approximation of my desired result in some sense i approximated a large memoryless linear system using a small linear system with memory i was wondering about a more general version of this idea what if my weights were different but with magnitudes still decaying exponentially and i was only interested in approximation being good on a small subset of possible inputs an example of this could be an artificial reverberation which also works by taking a large weighted average of shifted copies of an audio signal to me this seems similar to the way rnns work i e taking advantage of internal state and restricted input space to find a good approximation of a non trivial transform of time series data but since they re typically nonlinear i m not sure if it s a good idea to use them here on the other hand i couldn t find much information on rnns with purely linear activation functions has anybody here seen something similar to this are there issues or complications that i m missing thanks
0,tell us you’re a data scientist without telling us you’re a data scientist best answer becomes a meme
2,is there a point to having layers with just a linear activation i know in some contexts you can use a linear layer with no activation to increase or decrease the size of a vector however i have been looking at wavenet implementations and i noticed that they apply a linear layer with no activation function before adding to the residual bus the size of the output convolutions appears to be the same as that of the dilated convolutions so the linear layer is not needed to resize the outputs and they could be directly added to the bus is there really any point to doing this the 1x1 conv on top eventually has a relu applied to it however the bottom one is simply added to the resnet bus and goes on to the next layer of dilated convolutions could i just remove the bottom 1x1 conv or is there a good reason for having it gate 1x1 conv skip output input filter 1x1 conv dense output if you look at the following code i got from notice that res x and skip x have no activation also it appears it has the same number of filters as tanh out and sigm out why are these filters needed x200b def build model residual block self x i s original x x todo initalization regularization tanh out causaldilatedconv1d self filters 2 atrous rate 2 i border mode valid causal true bias self use bias name dilated conv d tanh s d 2 i s activation tanh w regularizer l2 self res l2 x sigm out causaldilatedconv1d self filters 2 atrous rate 2 i border mode valid causal true bias self use bias name dilated conv d sigm s d 2 i s activation sigmoid w regularizer l2 self res l2 x x layers multiply tanh out sigm out res x layers conv1d self filters 1 padding same use bias self use bias kernel regularizer l2 self res l2 x skip x layers conv1d self filters 1 padding same use bias self use bias kernel regularizer l2 self res l2 x res x layers add original x res x return res x skip x x200b
0,explanation of enterprise data architecture vs tools hi all i was hoping for some insight i haven t worked with red hat or openshift before and so i was wondering if you could give me the cliffs notes version of how the platform differs to a traditional enterprise data architecture or is it meant to slot in to the gap to provide cloud services for the other tools being used would appreciate a bit of insight there hope that isn t too silly a question i was also wondering if there is an open source enterprise data architecture system with all the components that i could play with as i develop a set of tools for my company to get a feel for what the various components do and how they fit together
1,question would a glm binomial model be appropriate for this outcome variable hello i have an variable in a repeated measures dataset who s distribution looks like this raw and like this after doing a groupby participant id python3 seaborn would a binomial model be appropriate or a bimodal one my hypothesis stats in r is that the mean variable can be predicted some demographic variables and an interaction for my variables n 31 mean variable outcome continuous percentage or binomial thats the question w continuous float x continuous float y continuous int symptom measure z continuous float medication days ordinal either 3 or 6 it is a covariate to control for having 2 datasets where in one the participants were treated for 3 and in the other for 6 days my code in r r res binom glm mean variable bool w x y z days data demos mn family binomial res null glm mean variable bool 1 data demos mn family binomial r squaredglmm res binom anova res null res binom test chisq
1,principal component efa and max likelihood cfa is there any particular methodological issue with doing a principal component analysis with a varimax rotation to tease out variables from a scale and then to confirm that model with a maximum likelihood confirmatory factor analysis versus say using maximum likelihood from start to finish
1,question regarding an example of texas sharpshooter fallacy hi consider the below example explaining the texas sharpshooter fallacy there are a certain number of cancer cases in a region area say 36000 every year and we divide all the area into 1000 sub areas parts for analysis of cancer clusters we notice that the highest no of cases is 48 in a year belonging to a particular sub area so to analyse we run a simulation may be 1000 times and try to find the probability of that particular sub area getting more than 48 cases in a year now as per texas sharpshooter fallacy we should be checking for the probability of the highest no of cases exceeding 48 in all of the area region in totality and not just that particular sub area my question is this in what scenario we should be checking that particular sub area only then and how i could possibly think of a scenario where we are looking for the historical record of that particular sub area making a distribution out of that data and then checking whether 48 is an exceptional value or not but i think that we will reach the same conclusion as we reached above so where am i wrong in the framing of my scenario or something else thanks
0,how to give effective feedback without sounding like an asshole you’ve probably heard of toastmasters the organization helping people improve their public speaking i noticed the level of detail with which they approach giving feedback think about it if you fail to properly articulate the why behind someone’s subpar performance they’ll likely just ignore your comments because they’re not actionable on the other hand if you’re a bit too blunt you risk offending someone and ironically this leads to most people not doing anything because instead of focusing on what you told them to improve they’ll go into defense mode i’m convinced that the toastmasters feedback guide can be applied to our day to day as data scientists let’s get into it here are 6 actionable tips that you can use to give feedback that doesn’t suck 1 setting clear and concise objectives before you give effective actionable feedback you actually need something to measure it against these performance expectations and goals then need to be clearly communicated to others around you the new intern at work is not going to have the same level of expectation as your veteran data scientist that doesn’t mean the intern can do crap work it just means that what you expect from both individuals is going to naturally vary 2 provide immediate feedback right from the toastmasters guide the passage of time diminishes the effectiveness of your praise or criticism if the feedback you give is too late then your co workers won’t be able to act on it because they’ll have forgotten what they did there’s a reason why back in high school your teacher would give you feedback on an essay before you wrote the next one it sounds simple but in a fast moving workplace it can often be forgotten 3 be specific if the feedback you give is too general or broad your message is going to be hard to interpret and when what you’re saying can’t be distilled down into something specific it’s going to be ignored here’s an example instead of saying “the product presentation you gave to the sales team was too technical at times ” say “on slide 7 i noticed how you went in depth into how our bidding algorithm works but it was a bit hard to follow because the sales team didn’t have context on the algorithm a slide earlier explaining why it’s needed in the first place would have helped make it more digestible ” 4 use the “i” technique my personal favorite avoid using “you” too much when giving feedback it’s one of the main reasons why someone might get a bit defensive remember this is feedback coming from your perspective it’s not some kind of label you’re trying to stick on someone say i felt like you didn’t spend enough time crafting next steps to follow up the presentation we gave yesterday rather than you were unprepared yesterday 5 start and end in a positive manner a good rule is to start off with something positive this disarms the person and allows them to ease into the conversation once they’ve settled down you can share more constructive criticism studies have shown that the ideal praise to criticism ratio is 5 1 meaning for every negative comment you make you need to share five positive comments as well five might seem like a big number but i’ll get to this in a bit finally try to end on a positive note too give them a roadmap to aim for and express your confidence in them 6 feedback is a continuous process some of you might have gotten scared when i stated that the ideal praise to criticism ratio is 5 1 five seems like a big number but if you’ve put yourself in a situation in which you’re just stacking negative comments on top of negative comments there’s likely a huge communication gap that has led to the low level of performance perhaps a project or presentation aim wasn’t clearly communicated or more likely you’ve just waited too long to give feedback if the only time you give someone feedback is during their quarterly performance review then they’re going to be surprised they won’t know where they stand and will blame you for not being more transparent with them sooner and that’s the last tip make it regular giving someone feedback shouldn’t be some type of formal ceremonious event it should be frequent and simple that’s it folks i hope that was useful if you liked this post you might like my newsletter it s my best content delivered to your inbox twice per month cheers
2,secure collaborative analytics and ml on encrypted data using mc² hi all our team uc berkeley has been working on a platform for secure analytics and machine learning called mc 2 and today we are excited to announce the initial release v0 1 of the platform with mc 2 you can take encrypted data and run various analytics and machine learning workloads at near processor speeds while keeping the data confidential mc 2 also enables secure collaboration mutually distrustful data owners can jointly analyze train models on their data but without revealing their data to each other github we just published a blog post that provides more information on mc 2 the project currently provides a docker quickstart and also supports deployments on azure confidential computing please check out our project would love feedback contributions
2,what is your preferred way to access columns in pandas in a pandas dataframe it is possible to access columns as a class member if the column is also a valid python identifier or by a dict like interface what is your preferred way to access it view poll
0,graph module reccomendations any easy graph model packages libraries that i can use for a quick modeling project use both python and r with some prefrence for python trying to model and optimize for min time in an automated elevator from one floor to another i e set path no pressing buttons so i could theoretically pack it all into a matrix but i feel like graph theory is applicable here thanks
1,how to compare metrics qwk mse r i have two machine learning models trained on the same task using the same data call them a and b i have calculated the qwk mse and r for both models using their predictions on the test set i want to use these metrics to determine how much better or worse a is than b how should i go about this my supervisor has led me to believe i can just subtract qwk b from qwk a and that the resulting qwk delta will represent the gain or loss of a over b but is that correct and what about mse or r i’m pretty sure literally subtracting the metrics in those cases is not valid in which case i don’t know how to compare them any advice appreciated tia
0,whats the difference between marketing data analytics and martech checked google but its only showing adtech vs martech would be great to know what the difference is between marketing data analytics and martech
1,pca applicability hello i posted this in r bioinformatics originally also posting here to get a breadth of opinions i tried to cross post but the tags weren t compatible i ve got an idea that is centered on using pca to group genomes based on the frequency of specific genetic features i am a complete stats noob but am aware that the applicability of different techniques is generally dependent on the data set your analysing with that in mind are there any things that i should be aware of for general applications of pca for example from my understanding pca is commonly used in transcriptomic data analysis to look at differential gene expression where the dimensions being reduced are the transcript counts for each gene fyi my use case isn t transcriptomic it s just an example so for this you might have 1000 s of genes features describing perhaps 2 4 samples would trying to apply pca to a set of samples with far fewer features e g 20 be a fundamentally bad idea would any issues be easily diagnosable with e g a scree plot how many features per sample is the minimum in your opinion in general are there other things that need to be considered cheers your friendly bioinformatics padawan
0,faang interview prep a b testing please be merciless in your reply here s the hypothetical from the interviewer fb launched a zoom like feature it was generally well accepted and its usage is growing you work at instagram how would you evaluate if ig should add that zoom like feature in other words a synchronous communication app within a heterogeneous network fb is being evaluted for launch in an otherwise asynchronous homogenous network ig my response clarifying questions can some people use the fb zoom feature with a higher different access level than others what requirements minimums or thresholds must be achieved in order to obtain higher access such as o a facebook business page o a facebook business page with 1 000 followers o a facebook user with 500 edges relationships since facebook can process one trillion edge graphs which the people nodes desiring higher access must have people higher access might include the ability to invite more than 20 people 20 being the number the hypothetical provided the ability to place other companies’ advertisements in the zoom invitation the zoom meeting and or the zoom follow up notification the ability to place advertisements of the host’s company in the zoom invitation the zoom meeting and or the zoom follow up notification a longer zoom meeting time 60 minutes for example clarifying questions partially answered but mostly deferred i would review the data from the a b experiment on this feature s usage and adoption at fb i imagine this a b test would have a dependent variable control group training set in machine learning ml user behavior before zoom feature and an independent variable test group test set in ml user behavior after zoom feature user behavior here would include the following variables which would be available in sql tables for further analysis frequency did it increase or decrease after using this feature in what ways did it increase or decrease likes comments shares marketplace buying or selling direct chat w other nodes users creation or removal of certain edges relationships i d also analyze datasets which tracked how many invitees became hosts and set up their own fb zoom meeting within 1 2 3 4 and 4 weeks how many invitees purchased one of the host’s products that were advertised in the zoom meeting how many invitees purchased one of the non host products that were advertised in the zoom meeting the a b experiment i d design at ig dependent variable control group training set ig user behavior before zoom feature independent variable test group test set ig user behavior after zoom feature user behavior here would include the following variables which would be available in sql tables for further analysis frequency did it increase or decrease after using this feature in what ways did it increase or decrease likes comments shares marketplace buying or selling direct chat w other nodes users creation or removal of certain edges relationships last i d run an sql inner join of the networks of people who used fb zoom and the networks of those same people on ig i d use clustering ml entropy weight k means to then look for trends or patterns which might explain why the zoom feature was used more or less on fb than ig which gender uses the zoom feature more which network ig or fb leads to more zoom invites being sent out do ig or fb zoom meetings lead to more purchases what type of purchases category what price range how does price category and frequency of purchase vary based on gis or location data of zoom host and location of invitees do the data suggest this might be growing into something like the influencers feature
1,i calculated an iota coefficient of 0 899 when comparing scores of two judges is that good or bad what are the cutoffs of a good iota coefficient in this study we have several trainees performing surgery one surgeon is assessing each trainee in the operating room and another surgeon is assessing them through a video recording and reviewed later on they assess them on around 5 points with a single digit indicating their score for each point with the help of u ticketstothepunshow i got an iota coefficient of 0 899 between the two judging surgeons on all 5 criteria is that a good iota that indicates the two judges were similar in their scoring also when i compared the means of their scores on each task a particular task scoring jumped out as the cause of the reduced iota how do i prove that it s the culprit but when i calculated the iota coefficients 5 more times but without one task each time the coefficient went up and down is this a better way of knowing which task ratings were the most and least overlapping thank you
1,pursuing phd with coursework part time masters i hope you don t mind another phd question i m becoming interested in pursuing higher education and am looking for some opinions my current situation working full time as an actuary studying msc statistics part time at the university of toronto no research experience outside work school already consumed most of my time i ve always been interested in genetics big data but not biostatistics in general i d like to work in the industry for a research role hopefully in biotech after graduation i m hoping to apply to a statistics phd program in a school with warmer climate i know it s kind of petty but toronto is very cold my grades are not stellar given that i m squeezing school work together so i m okay with not shooting for top schools but i d still try to apply for reputable places thanks again
1,statistics of data pooling are there any statistical methods for qualifying the combination of two datasets i e data collected by two different research teams with their own experimental setups this is obviously quite a common occurrence in medicine trials and in engineering the most obvious example i can think of is the moody diagram which was constructed from pressure drop experiments by a wide variety of researchers would the justification be based on verifying that the same experimental conditions were used by both teams if so this seems a bit ‘loose’ since it would neglect any influence uncontrolled variables say ambient temperature might have on the results alternatively is there some quantitative statistical method of proving for example that the relationships between the response and independent variables are not statistically different
1,eu proposing to regulate the use of bayesian estimation i hadn t paid close attention to the news about the eu wishing to regulate ai thinking it would be mostly focused on deep learning it turns out that ai techniques does include statistical techniques from gelman s blog c statistical approaches bayesian estimation search and optimization methods roughly skimming through the document i see some things i can comprehend agree with on the data side having rules around pii data and logging apart from that it isn t quite clear to me how this influences day to day work in modeling
1,looking for statistics about the comprehension of sexual violence sorry if this isn t the right sub but i m looking for statistics about the comprehension of sexual violence whose participant isn t grouped by gender what s your understanding about sexual violence what do you regard as sexual violence do you know sexual violence is kind of items thank you
2,should i use cnn or mlp for my project hi i m working on a project that has an 8x8 matrix as input and it has 4 classes as output should i use a convolutional neural network to preserve the relationship between my input data or should i flatten my data and use a simple multilayer perceptron
2,call for papers icml 2021 workshop on distribution free uncertainty quantification icml 2021 workshop distribution free uncertainty quantification this will be a virtual event see website here no reformatting of papers required no page limit rolling deadlines with 1wk casual review workshop will be beginner friendly and all are encouraged to submit speakers include michael i jordan emmanuel candes rina barber vladimir vovk leying guan jing lei larry wasserman and kilian q weinberger submission deadline up to june 14 for spotlights and july 1 for posters topics include conformal prediction risk controlling sets calibration tolerance regions and heuristic notions of uncertainty like platt scaling organized by myself stephen bates aaditya ramdas sharon yixuan li and ryan tibshirani
1,what sort of distribution will help me with my problem not sure if links are allowed but i was playing this game warning its about matching the crime to mugshot and was wondering what kind of distribution will allow me to calculate the probability of getting a certain number 0 5 guesses correct its kind of like a binomial distribution but without replacement i guess any help will be greatly appreciated
0,currently working as da company doesn t have any cloud platform or servers hi right now i m struggling to pull data in order to make reports or any analytics everything is scattered around either in excel files or in a bunch of platforms i just started so i don t have much working experience and i know you guys might be able to give some tips that could help me on this really appreciate it
2,gpt j 6b jax based transformer lm ben and i have released gpt j 6b jax based transformer lm performs on par with 6 7b gpt 3 performs better and decodes faster than gpt neo repo colab free web demo trained on 400b tokens with tpu v3 256 for five weeks gpt j performs much closer to gpt 3 of similar size than gpt neo tweet article repo colab demo
0,in the early stage of your career how many hours per week should be dedicated to upskilling hello i am about to finish my first year of a data job after grad school in the past year i ve found myself not being able to upskill much outside of new things i learnt at work reasons or excuses could be wanting time for my hobbies not wanting to study on the weekend i know i need to put some hours every week to learn new stuff outside of my job but i am kind of struggling i was wondering how do you guys go about your upskilling and what are some tips would you like to give to someone like me thank you p s i can t say my skill set is the same as it was a year ago but not doing much outside of work makes me a bit insecure
2,on the role of knowledge graphs in explainable ai very interesting paper on the role of knowledge graphs in explainable ai by freddy lecue the current hype of artificial intelligence ai mostly refers to the success of machine learning and its sub domain of deep learning however ai is also about other areas such as knowledge representation and reasoning or distributed ai i e areas that need to be combined to reach the level of intelligence initially envisioned in the 1950s explainable ai xai now refers to the core backup for industry to apply ai in products at scale particularly for industries operating with critical systems this paper reviews xai not only from a machine learning perspective but also from the other ai research areas such as ai planning or constraint satisfaction and search we expose the xai challenges of ai fields their existing approaches limitations and opportunities for knowledge graphs and their underlying technologies x200b link to paper
0,how to perform churn analysis for a business that does not have recurring revenue and is not subscription based
2,paper explained decision transformer reinforcement learning via sequence modeling decisiontransformer by lili chen et al transformers are everywhere so why not add them to reinforcement learning rl as well yeah that s right the researchers at uc berkley just did that they approach rl as a sequence modeling problem and use an autoregressive transformer to predict the next optimal action given the previous states actions and rewards so that it maximizes some reward function perhaps surprisingly this simple decision transformer approach achieves state of the art performance on atari openai gym key to door tasks check out the full paper digest to learn about how offline rl can be turned into a sequence modeling problem represent simulation trajectories for the transformer to learn from and most importantly apply transformers to ace offline rl tasks meanwhile check out this paper poster presented by casual gan papers decision transformer full explanation post arxiv project page more recent popular computer vision paper breakdowns dall e vqgan dino
0,in ds field is it possible to get a remote job working from another country i d like to know how often do companies look for players from another country if any of you were able to achieve this considering that earning in dollar would be incredibly benefitial for me even on a jr analyst position
2,how to do multi task learning intelligently we have a new article out how to do multi task learning intelligently that may be of interest to you it covers the concept of multi task learning and provides a summary of some cool recent papers about it adashare learning what to share for efficient deep multi task learning end to end multi task learning with attention and which tasks should be learned together in multi task learning would love feedback
0,why was my career question on the data science industry removed hi i posted over the weekend in r datascience about how someone might navigate the data science field without masters or a phd within 15 minutes my question was removed and i’m hoping someone can help me understand why this happened thanks
2,great paper forbidden knowledge in machine learning refections on the limits of research and publication forbidden knowledge in machine learning refections on the limits of research and publication by dr thilo hagendorff certain research strands can yield “forbidden knowledge” this term refers to knowledge that is considered too sensitive dangerous or taboo to be produced or shared discourses about such publication restrictions are already entrenched in scientifc felds like it security synthetic biology or nuclear physics research this paper makes the case for transferring this discourse to machine learning research some machine learning applications can very easily be misused and unfold harmful consequences for instance with regard to generative video or text synthesis personality analysis behavior manipulation software vulnerability detection and the like x200b linkt to paper
2,measuring similarity between datasets so i have two different datasets x and y obtained in two different settings but using the same sensor systems and i want to quantify how a model trained on x will perform on y without evaluating the trained model on y basically i am trying to quantify the generalization capability of the trained model one answer to this problem is to measure the similarity or distance between x and y looking at the literature only this paper geometric dataset distances via optimal transport seems like a good fit do you guys have any experience or ideas along these lines thanks
1,why was the kaplan meier method considered so groundbreaking the kaplan meier method was developed in the 1950 s and is still considered to be quite groundbreaking and influential it is used to estimate survival rates and mortality however it seems that similar methods to estimate similar concepts had already existed for hundreds of years before for example life tables were recorded by edmund halley in the 1600 s what was so special about the kaplan meier method compared to its predecessors thanks side question can force mortality be considered as a hazard function
0,pro cons of contract vs fte i am weighing the pros and cons of taking a contract position which i haven’t done before what are things i should consider all i know about contract work is they don’t take any taxes out no benefits no health insurance less networking not a part of team culture may or may not turn into a job less stable this is for a data analyst role they are offering 40 hr but my current annual salary is 75k with great health coverage what factors should i consider what do you wish you knew before taking your first contract role why do people generally look down on contract offers i don’t get it any help would be greatly appreciated
2,paper overview barlow twins self supervised learning via redundancy reduction video paper code abstract self supervised learning ssl is rapidly closing the gap with supervised methods on large computer vision benchmarks a successful approach to ssl is to learn representations which are invariant to distortions of the input sample however a recurring issue with this approach is the existence of trivial constant solutions most current methods avoid such solutions by careful implementation details we propose an objective function that naturally avoids such collapse by measuring the cross correlation matrix between the outputs of two identical networks fed with distorted versions of a sample and making it as close to the identity matrix as possible this causes the representation vectors of distorted versions of a sample to be similar while minimizing the redundancy between the components of these vectors the method is called barlow twins owing to neuroscientist h barlow s redundancy reduction principle applied to a pair of identical networks barlow twins does not require large batches nor asymmetry between the network twins such as a predictor network gradient stopping or a moving average on the weight updates it allows the use of very high dimensional output vectors barlow twins outperforms previous methods on imagenet for semi supervised classification in the low data regime and is on par with current state of the art for imagenet classification with a linear classifier head and for transfer tasks of classification and object detection
0,pdf search another project i built using jina ai search framework source code on github in this project i am using jina to search a repository of pdf files the project allows a user to query the data by providing text or an image or both simultaneously how to use it clone the project and run following commands install requirements pip install r requirements txt start the server python app py t query restful query via rest api curl request post d top k 10 mode search data jina hello multimodal h content type application json what s included in this example search text image pdf all in one flow or in separate flows speed up indexing time with parallel peas use customized executors to better fit your needs provide detailed docstrings for yaml files to help you understand the example let me know your feedback and what would you use this project for i d love to help
2,raquel urtasun founder and ceo waabi podcast x200b we ieee soft robotics podcast going to have raquel urtasun the founder ceo of waabi and professor at university of toronto if you have any questions please let us know through the following link x200b
0,for those of you working under with executives that are completely ignorant to their own company’s limitations with data how do you all deal with their demands for results came down the pipe at work today “ceo was at a conference and another ceo was talking about how they ‘stole’ customers from bigger industry players by looking into customer transactions and tailoring products and services to them to attract them over ” financial industry so these are transactions in and out of one company that are being munged the thing is we don’t have decades of preserved customer data we don’t even have a data warehouse of any sort just some old clunker of a legacy hierarchical database that’s meant for acting as the data layer of a transactional system now our ceo wants to basically know who of our customers has certain account types at other companies based on our “data” because of something he heard at a conference that another company was doing there is no way we’re mining our trivially available data and coming to any conclusions or assumptions about our customers financial business with other companies this isn’t the first time our ceo has done this a year or so ago he wanted a full travel prediction system built like chase or cap1 has for his credit cards there that automatically know when he’s traveling so he claims everyone else in the company licks his boots except me and i’m always the one coming off as the ass shooting these ridiculous demands down yes i’m looking for a new job no i haven’t had an interview in 2 years now while looking i’m tired of people telling me this as the answer to my problems the other day it was “we’ll use prebuilt ai and just give it data and see what it finds ” how do you all deal with absolute pie in the sky demands from executives
1,if you have f x y where x y are distributed by two different normal distributions and f is a function that combines the two what is the distribution of f
0,cluster analysis for customer segmentation what do you think about a cluster analysis to segment customers i feel like a manual segmentation is often times better especially when it comes to more personalized marketing i think that clustering makes sense when there are distinct groups in the population however i think that these groups can easily be identified by eda finding thresholds of certain variables in most cases there is a possibility for identifying relevant groups only through cluster analysis but i think that a those cases are rare and b the identified clusters are more complex and not suitable for a segmentation with the objective of a more personalized communication does anybody have a success story where unsupervised clustering led to a customer segmentation that offered a business value e g because of more personalized communication i am struggling to imagine a scenario where unsupervised clustering comes up with better clusters for personalized communications compared to manually building clusters by thresholds criteria for clusters
1,what regression methods are similar with nadaraya watson kernel
2,introducing skim platform to help skim through papers in this fast moving research world youtube video after seeing arxiv sanity down for more than half of the time whenever i visited and going though lot of tweets i decided to create a better platform which not only solves the problem like arxiv sanity but gives lot more features which allows ml folks to stay on it for almost everything skim aims to be spotify of the ml world but here papers are your tracks racks are your playlist of papers we got search you can follow unfollow users and conferences like unlike racks and papers currently we re planning to bring conferences related data like deadlines visualisations of acceptance rate statistics a list of papers maintained year wise for that conference in a rack more features like images of papers pages to skim through finding similar papers recommending papers notifications are in our roadmap this platform is built in just 2 months and at very early stage of beta we are open for all sorts of feedback website
1,question power transformations i m using a yeo johnson transformation but i don t get how it works i read that lambda is chosen in a way that it fits to my data and then the data is transformed like yeo johnson says okay cool but how exactly is lambda determined
1,a modern version of the jackknife and bootstrap shao tu hi all i was wondering if any of you know a book like the jackknife and bootstrap in the sense of mathematical rigurousity for resampling techniques but that is a bit more current this book is great but 2 things i dont like the notation is a bit cumberson for my taste but more importantly is lacking in other resampling techniques like permutation test x200b also since is more than 20 years old theres probably some outdated content there any help would be cool thanks
0,how to start preparing for aws cloud practitioner certification i’m a beginner and i’m planning on creating a career in cloud services i currently have little to no knowledge on cloud platforms and i am wanting to learn more as a first step i’m planning to prepare for the basic certification offered by aws i e cloud practitioner it would be great if anyone can provide me resources to begin with i’m more of a video person than book person so if you have any online mooc course that will be perfect for my goal thanks
2,acm siggraph on discord siggraph on discord hello all i m trying to find the channels on discord in which members of siggraph post topics chat collaborate respond and who s hosting
0,neural network optimization x200b hi i used a feed forward neural network on keras to approximate a concave one dimensional function i would like to find the argmax and the max of my neural network what would be the easiest way to solve this should i implement something myself or does keras already have some buil in function for that thanks
0,maybe this question has been asked before but how important is it to have a github account with your independent out of work projects i am in the market right now and i see that many companies are asking for github url information i m assuming it is so that the interviewee can provide some work that they have done on the side in ml outside of work requirements so how important is that because i enjoy ml and data analytics as my job but after work i enjoy other activities not necessarily doing data analytics and projects in ml so is not having a github full of ml projects something frowned upon by employers now
0,what data science specializations do you think are worth looking into in the future i wanted to go down a route that may not be traditional a field where people can stand out what fields niches do you think those might be and how could one enter and also how can one gain these skills
0,given a hopelessly awful data project made no progress in 3 months is it time to give up i ve been working on an incredibly simple yet absurd and stupid data project for 3 months with my 30 minutes or so of free time i have per week outside of my current business analyst duties we use this program called anaplan which has a power bi api to get data but the api of course is garbage and doesn t work if your organization uses single sign on which basically every single organization does so the api is therefore useless and you have to download massive text or csv files with all the data that you want and important manually into power bi but here s the real issue with the data it has a stage or process step column that duplicates every single primary key five times to capture data about each step in the approval process so you have five different rows for each and every single primary key where the approval date or the date that it moved into that specific stage is located meaning that if you try and pivot the columns so that each stage is its own column now you have literally five different columns where dates are not all in the same row neatly like they would be in excel i have tried to figure out how to get them all in the same row in power query or power bi but it s hopeless it s like there s no way to do it it s senselessly stupid i m expected to be some sort of data scientist even though i m just a regular business analyst with no data science education training working on this absurdly stupid dashboard that i have to manually pull the data for instead of using the api that they worked on to develop that doesn t work because it s single sign on
0,similarity between datasets suppose you have two datasets each dataset contains continuous variables x y z and 1000 rows let s say the first dataset is from a hospital in california and the second dataset is from a hospital in new york are there any common ways to measure how similar the two datasets are another application if you train an ml algorithm on a dataset and then the you get new data then if the new data is really similar to the old data you can be more confident about the performance of the ml algorithm on the new data thanks
0,data driven web frontends looking at react and beyond for crud hello fellow community so while we might love jupyter and all our fancy tools when getting results into the hands of customers webapps seem to be the deal currently i am developing a few frontends calling them “data driven” for now whatever that means but it’s trendy basically they are crud interfaces with a lot of sugar collapsible lists with tooltips maybe a summary row icons colors basically presenting data in a way that people will like to pay for currently i decided to go with a django backend and a react frontend overall i have to admit i hate frontend dev almost as much as i hate webapps still i thought react was a reasonable choice for a great user experience with a modern toolset right now the frontends authenticate against the backends and fetches data using graphql instead of traditional rest which sounded like a great idea at the time but actually i feel like this was a terrible approach when fetching data there needs to be a ton of transformation and looping over arrays done in the frontend to bringt the pieces of fetched data together in a format suitable to render tables which in my opinion is a mess fiddling with arrays in js while there is a python backend at my fingertips that could use pandas to do it in the fraction of the time but that seems just how this works i also got fed up with react it provides a lot of great advantages but honestly i am not happy having tons of packages for simple stuff that might get compromised with incompatible versions and stuff down the road also i feel bad about the packages available to create those tables in general it just feels extremely inefficient and that’s coming from someone usually writhing python overall what i like beautiful frontend great structure single page applications just feel so good easy to use mainly what i just can’t stand anymore way too much logic inside the frontend way too much data transformation inside the frontend well all of it too much packages that don’t feel reliable in the long run sometimes clunky to debug depending on what packages are used i somehow never get the exact visual results rendered that i want i somehow create a memory leak daily that i have to fix then call me incompetent but i can’t figure out why this always happens to me so i have been talking to a few other ds and devs and graphql and react seem to be really popular and others don’t seem to mind it too much what are your experiences similar problems do you use something else i would love to ditch react in favor of something more suitable overall i feel like providing a crud interface with “advanced” stuff like icons in cells tool tips and collapsible rows tree structure tables should be a common challenge i just can’t find the proper tool for the job best regards and would love to hear your thoughts
2,can a re implementation of stylegan2 be released with mit license i have seen the amazing work of lucidrains implementing an easy to use version of stylegan2 however i am puzzled seeing that the license is mit which includes commercial use while the original algorithm from nvidia has a very restrictive license that explicitly states it may be used or intended for use non commercially does anyone know if this actually possible or we shouldn t trust that mit license statement
2,efficient learning what direction is best to invest how should someone approach the problem of efficient learning there is many ideas and they all seem legit gans to generate more data weakly supervised learning self supervised learning fine tuning and others what should someone choose if no time is there for testing and evaluating all of them
2,image generators with conditionally independent pixel synthesis cips by anokhin et al generative models have become synonymous with convolutions and more recently with self attention yet we yes i am the second author of this paper yay 🙌 ask the question are convolutions really necessary to generate state of the art quality images perhaps surprisingly a simple multilayer perceptron mlp with a couple of clever tricks does just as good if not better as specialized convolutional architectures stylegan 2 on 256x256 resolution check out the full paper digest reading time 5 minutes to learn about the architecture of our mlp based generator the two types of positional encoding used to increase the fidelity of generated images and how cips can be used to generate seamless cyclical panoramas without ever training on full panoramic images meanwhile check out the paper summary poster by casual gan papers cips conditionally independent pixel synthesis full explanation post arxiv project page more recent popular computer vision paper breakdowns dall e vqgan decision transformer
2,deepmind presents neural algorithmic reasoning the art of fusing neural networks with algorithmic computation a research team from deepmind explores how neural networks can be fused with algorithmic computation and demonstrates an elegant neural end to end pipeline that goes straight from raw inputs to general outputs while emulating an algorithm internally here is a quick read deepmind presents neural algorithmic reasoning the art of fusing neural networks with algorithmic computation the paper neural algorithmic reasoning is on arxiv
2,openai announces openai startup fund investing 100 million into ai startups it does not appear to be explicitly gpt 3 related any type of ai is accepted but hints very heavily toward favoring applications using it
0,dealing with imbalanced datasets suppose you are working on a supervised binary classification task you have patient medical information e g age weight gender height blood pressure etc and whether they have a certain disease or not this is the response variable yes or no let s imagine that determining if patients have this disease is time consuming and costly so a machine learning approach is being considered let s assume that this disease is very rare in your data set only 1 of patients have this disease thus the dataset is imbalanced intuitively we know that any machine learning algorithm trained on this data will likely perform poorly that is the performance will likely be deceptive you might get an accuracy of 99 but misclassify all of the patients who have the disease mathematically speaking is there any mathematical explanation for this very logical concept e g if only study 1 hour for a chemistry exam i might only learn how to solve 2 3 types of problems thus on a true false style chemistry exam there will be many questions that i don t know how to answer because i never saw them before and i will be likely to perform badly on material that i have not prepared for do machine learning models work the same way for popular algorithms like neural networks xgboost and random forest can it be shown that for classification problems you need a minimum number of observations or a minimum proportion of the minority class to probabilistically achieve a certain model performance on a more abstract side i have heard that researchers are interested in trying to make machine learning models generalize without seeing thousands and thousands of examples e g a 5 year old child can learn what is an elephant after seeing a few pictures of an elephant e g it s perfectly reasonable to expect that a young child would see a picture of the cartoon character dumbo and identify dumbo as an elephant after coming back from a zoo but a machine learning algorithm would likely need thousands and thousands of pictures of elephants and likely require to see the same pictures upside down inverted with added noise different color scheme etc prior to be able to generalize and learn the concept of an elephant perhaps the same analogy applies to machine learning models struggling to correctly classify patients with a rare disease since there are so few of them does the above concept have anything to do with the bias variance tradeoff or is it just logic if there is not enough variability and information within the data the machine learning model just learns the noise within the dataset i am really curious to see if such a threshold for measuring minimum level of variability within the data has ever been studied ps in a 1 dimensional sense on a number line if you have a point at 3 and another point at 5 you could consider all inferences outside of 3 and 5 as extrapolation and all inferences between 3 and 5 as interpolation when dealing with higher dimensional data could you simply consider observations from the test set that have a smaller euclidean distance to other observations from the training set as interpolation and observstions that are farther away as extrapolation in reality can you just consider all prediction as extrapolation small scale extrapolation for closer points large scale extrapolation for further points thanks
2,looking for an ml website i saw a while ago change a face based on sliders of facial properties i saw a website maybe a year ago where there was a face made by a gan and you could control what it looked like with a few sliders underneath the face such as facial width eye size skin color etc does anyone have any examples of this
2,improving genomic discovery with machine learning original source x200b similar ai ml data science articles in a form of a newsletter here
2,extracting job skills and yrs of experience required from job offers assuming the preprocess is done removing all html css and js to start analyzing a text how can i extract the job skills and experiences needed from the offer e g 5 years of experience in managing personnel to return 5 years management being experience in yrs if any skill to extract i tried with nltk but got to the point where i can pos the whole text sentence by sentence but i m stuck here i do not know how to determine the appropriate skill as there s not always the same grammar structure or they list the skills literally i m thinking on using a token classification model from huggingface transformers trained with custom tags but i am really new to all of this and would like to ask for opinions on my approach or some kind of guidance on how to achieve something like this
0,what are some of the most complex difficult data science concepts that you ve struggled to grasp in my role data scientist at an e commerce company i focus primarily on building recommender systems we recently implemented a variational autoencoder based system and while the architecture implementation is fairly straight forward unpacking the nuance and derivation of the kl loss function and understanding variational inference in general i found to be quite challenging i m wondering what projects concepts you have worked with that you found challenging to understand implement
2,should i use locality sensitive hashing or md5 to check if two datasets are the same i need to compare if two datasets of type audio text image etc are the same or not should i use locality sensitive hashing or split the dataset into smaller chunks and hash using md5 sha 1 i m leaning towards the second step locality sensitive hashing is more about finding similar items but i only need to check if two datasets are exactly the same or not both run on o n time complexity so i don t think speed will be a difference
1,calculate sample size hi my assignment is to find out how big of a sample size i need to show that the anticipated incidence from a preliminary examination is existing irl but i have no clue which formula to use i have used this website to calculate the sample size but they dont show the calculation any kind soul that can help the significance is supposed to be 0 01 power 90 and ratio of sample 142 311 118 849 1 197 counties preliminary examination entitled to vote kronoberg 22 142 311 blekinge 25 118 849
1,basic questions about linear regression 1 often we are told that linear regression models with many beta coefficients are undesirable e g 100 beta coefficients 100 predictors in general these models are said to be unstable high variance and likely to perform poorly does anyone know why this is 2 this is a common sense question suppose you have 3 variables 1 response 2 predictors you make a plot of these 3 variables and you notice that the plot of this data is clearly not linear therefore you would decide not to use a linear regression model is there any mathematical logic that shows why a linear regression model is unable to well represent non linear data for 3 variables i guess you could show this visually but for higher dimensional data what is the mathematical justification used to understand why a linear model can not capture non linear data 3 in the previous question i asked if linear models are too rigid to capture non linear patterns but what about the opposite suppose you take the same example with the 3 variables 2 predictors and 1 response this time you have new data and make a plot and the data appears to have a strong linear patterns in this example if you had still chosen to use a non linear model has their been any mathematical research that examines the ability of a non linear model to capture linear patterns in this example would a linear model have some advantage at recognizing and capturing linear patterns compared to a non linear model or in general are linear patterns completely within the domain of non linear patterns and as such non linear models are not expected to have any disadvantages at recognizing linear patterns compared to linear models 4 are non linear patterns more likely to occur in bigger datasets more columns and more rows could we not say that if there are more data points there exist more geometric configurations that these data points can be arranged in making non linear arrangements more probable e g any 2 points can be connected with a line but 3 points can begin to be arranged on nom linear arrangements
1,including gender male female nonbinary as a covariate in anova spss hey guys i am running a repeated measure ancova with life satisfaction as my dv i have a significant influence from the gender variable male female non binary on my dv so i want to use it as a covariate all the necessary conditions for running an ancova are satisfied but i am unsure of one to check if the interaction between the iv and the covariat is non significant i ran an normal anova with my iv and gender as my between subject factors and life satisfaction as my dv the interaction between the iv and the covariate age turned out non significant is this the correct way to check for the assumption also i am unsure whether i can simply include the gender variable as a covariate in spss i know that categorial variables can be included if they are dummy coded but since the gender variable has more than 3 categories i am not sure if i can just do that note there is only one person in the non binary category thank you guys in advance
0,i quit stories i am happy to announce that today i am quitting my current position of 3 years i built my current team of des and dss from the ground up but i m always the last priority in terms of pay and advancement and on top of it non technical colleagues get all the credit for my work so while they advance i ve been stuck for quite a while our leadership team is non technical and yet they dictate our development cycles causing 36 hour work days and to top it off we have a terrible it department that has built a horribly configured stack every day can be a nightmare for some of us due to various factors what s your story why did you take the job in the first place and when did it become too much did the experience lead you to land your dream job or was the whole transition a total disaster
0,classification regression just from a distance matrix i have a dataset that is pre computed into a distance matrix fully connected between all points this made sense as it was originally intended for clustering but now i am wondering if i could use it for classification i e given unlabelled points and their distances to all labelled points assign soft labels being able to do regression in the same manner would also be a plus i tried googling this but i could not come up with the appropriate terms to search the best i could find is graph based semi supervised techniques like label propogation in sklearn but i was wondering if anyone had any insight into additional models techniques i could look into thanks
0,how good at r or python do you have to be before you add it to your resume so i m actually applying to data engineering jobs so may not even be relevant however i m never sure when it s okay to actually add something to your resume in the case of r i only spent maybe a couple months on it i learned the tidyverse package including dplyr i also created a single project which included various graphs along with decision trees linear regression and clustering algorithms using the caret package but i wouldn t say i m proficient i don t think i m just good at googling stuff i don t know if it s still okay to just stick it in with my list of skills
1,question formula for calculating character life probability in a video game hello i play a video game called tierras del sur its a small 2d indie game with an active comunity of around 1000 concurrent players i usually do some small math on my head but i dont know how to calculate probability for the value of characters and i need some help so the game works like this a character levels up from 1 50 levels 1 25 are quite easy and get progressively harder with the maximum player right now being level 40 from 40 50 it gets incredibly slow and will take years before someone reaches 50 characters have life and mana both increase with each level but mana increases on a fixed value while life increases based on an average depending on the class mage is squishy so higher average paladin is tanky so higher average the averages are for example a human mage 6 5 so each time you level up you roll the dice for a life increase of 5 6 7 or 8 this value determines the worth of your character by being higher or lower than your expected average for each level this can go down to a defined cap of 10 and up indefinetly so theoretically you could become a level 50 mage with 75 points above your expected life if you rolled an 8 on each level example you create a character it has a base life of 15 every time it levels up that life can increase by for example in mage 5 6 7 or 8 all rolls are equal 1 4 chance so say your character levels twice and both times it rolls a 5 it will have a life of 25 but the expected life is 28 since the average of rolls is 6 5 that character would be 3 if it rolled 8 both times it would be 3 if it rolled a 6 and a 7 it would be an average character most valuable characters hover between 10 and 20 with some exceptional charactes reaching 25 above level 40 the way people go about this is create a character up to level 15 which takes about 2 3 hours and if the values are above 8 they will continue with a character my question is to evaluate the value of time to character worth what formula i could use to calculate the probability of a character above 10 at a selected level say i wanted my mage to be 15 at level 15 how many characters i would need to create on average and if it would be more worh it for me for example to create a character to level 7 and try to roll 4 perfect 1 4 dices sorry for bad english
0,marketing analytics how do i track where customers are coming from and how far on the website they go to this is a little confusing so i ll add all the details that i know so far we have facebook ads running for a product that is displayed on amazon but my manager wants me to create a website for this product and then analyze which ad people are coming from at the same time there s influencer marketing for this product as well unfortunately adobe analytics would be an easy way to track where people are coming from but my company doesn t use that there are a lot of clicks on the ads but no one is buying the product so i m trying to see exactly where on the website they stop and how to optimize the site layout my manager really wants me to create a website in order to track all of the information instead of using amazon and i m not so sure if that step is needed at all the product started out as really successful but now almost no one is buying it so it seems like a weird project the main issue is that i m not sure how to even start collecting data after i m able to collect the data i can then analyze it and make a model but i don t have a background in marketing or marketing analytics so i m pretty lost
2,using two optimisers for large model with two parts need advice hello there i have a very large model 51m params that comprises of two sections one non differentiable and the other differentiable i tried training the whole thing using gd like adam sgd etc but the results were very subpar i have tried using derivative free optimisation algorithms like genetic algos but the results were even more abysmal i am aware of cma es ars pgpe and other blackbox optimisers but they only work on smaller models with 10k params and that s stretching it too lol similarly simuated annealing and hill climbing also failed my new idea is to use two optimisers one gradient based and the other gradient free during each epoch i hope to tune the parameters separately while using the loss at the end as the only feedback signal i want to optimise both subsections in a disconnected fashion without entangling their parameters together that way both modules can be trained while being oblivious to the other s performance ideally forcing them to become better and not rely on the other to improve the network as a whole would like some advice on how i can go about this and if there are any alternatives i should consider i m open to all options appreciate any help d thanks
1,level of agreement finding a way to justify why only a certain level of mean was analyzed hey people i m currently writing my first ever thesis and i m a bit overwhelmed by the statistical part in my research i asked students to fill out a survey in which they had to indicate how important 34 different attributes were to them based on a 5 point likert scale i did descriptive statistics in spss to see the mean and std deviation of all 34 attributes the outcome was a table that included all 34 attributes and i was able to sort them from highest to lowest based on the mean my problem is now that i don t wanna analyze all 34 attributes but i have no idea how can i justify that i only focused for example on the ten most important attributes or on the 5 most important attributes my question is now if there is some way to calculate some sort of highest level of agreement that i can use to justify that i only looked at a certain amount of attributes for example saying that 12 attributes are the most important ones m 4 10
0,what separates the jobs and fields in and related to data science depending on boss or colleague they introduce me various ways data scientist researcher statistician those are the main titles i m introduced as but there are more on paper i m titled as a statistician according to hr but i think the title of researcher is probably the best fit the majority of my duties design research studies experimental and survey collect data from a research study and or public data clean structure analyze and visualize data create papers decks and reports present findings what separates the jobs and fields in and related to data science statistician researcher data analyst data scientist et cetera
1,what do you guys think of this curriculum from an undergrad degree in statistics hello i m thinking about getting a bachelor s degree in statistics and i m wondering if the curriculum below will be enough to give me a good knowledge of the field i m not in the us so i translated the names listed from my country s language if any name seems weird please let me know here it is calculus i analytic geometry and vectors calculus ii calculus iii linear algebra computer algorithms and programming notions of statistics statistics laboratory r probability i probability ii database manipulation inference scientific method and research techniques sampling techniques stochastic processes computing applied to statistics time series regression analysis planning and research bayesian inference statistical consulting i discrete data analysis methods in multivariate analysis statistical consulting ii suplementary math numerical methods numerical analysis i beyond that there is a bunch of electives to be chosen among subjects like computer science biology engineering data science math what do you guys think edit so i really thought i would hear i would need more cs but it seems like what is really lacking is math that was a huge insight also a degree is probably not enough i appreciate all the answers
1,how would grocery shopping spending be modeled where spending events are dependent on each other and the money spent is correlated to the time inbetween events i want to be able to use my past time stamped spending to make a simulation of what may happen in the future like a series of future dates and amounts
0,how to compare metrics qwk mse r i have two machine learning models trained on the same task using the same data call them a and b i have calculated the qwk mse and r for both models using their predictions on the test set i want to use these metrics to determine how much better or worse a is than b how should i go about this my supervisor has led me to believe i can just subtract qwk b from qwk a and that the resulting qwk delta will represent the gain or loss of a over b but is that correct and what about mse or r i’m pretty sure literally subtracting the metrics in those cases is not valid in which case i don’t know how to compare them any advice appreciated tia
0,question on survey data results and writing my apologies if this isn t the proper group to ask the question but any assistance would be greatly appreciated my company does weekly surveys to our membership base we have approximately 4 years worth of data results that we were looking for someone to assist in interpreting the data and possibly tying it all together for a comprehensive report sort of speak i am struggling to find a company that offers this service or even what this service would be called anyone here know of a company or consultant that could help
0,alternatives to jupyter pyhton i find that jupyter is a little buggy occasionally and also along with all my other stuff open in chrome i d rather have a specific window main tasks day to day condition based based analysis data collection and cleaning graphing time frequency angle domain analysis data sets vary from 1gb to 50gb rarely more what do you guys use edit update vs code is the one i like it a lot though i did have multiple issues with numpy due to conda environment already moved over my current project did some housekeeping and all looks great also switched to miniconda which is much better again
0,why are data scientists in the sports industry underpaid i’m currently a undergrad sophomore whose very much into data science i myself have no idea as to what industry i want to go into but i have thought of sports analytics as being one of them moneyball was ultimately the first movie that go me into data science and after getting deeper into the languages statistics ml i have noticed i do enjoy centering my data science projects around sports analytics i have considered sports analytics as a possible route and dream of being a data scientist for a professional team nba nfl but i have read that data science in these industries are heavily underpaid compared to others from what i’ve read on this sub and others that sports analytics is fascinating but the work u do is underpaid and you are better off doing it as a side hobby than as an actual job industries like tech and finance pay more for data scientists compared to sports my question is why why do data scientists in sports get underpaid if there are currently any people out there who are data scientists on sports teams or have worked in the past what are your experiences and do you agree with these claims what advice do you have for a student whose interested in the field should i pursue it or am i better off going to a tech finance related industry
1,how to create a score between twitter sentiment scores positive negative neutral and performance statistics for athletes hi this is my first time posting here so i hope this is the correct space for this question i scraped twitter and analyzed a year s worth of posts for their sentiment i made a count for each type of sentiment positive negative and neutral i would like to create a type of index score when cross referenced with a player s performance during that same year the performance will have positive statistics goals 5 points assists 3 points etc and negative statistics owngoals 2 yellow cards 1 etc my goal is to determine which player deserves praise jeers based on their performance for the given year can someone help me out i ve never done statistical analysis before and am not sure where to begin
0,predicting when a product will be sold out is survival analysis appropriate use case a particular product has x remaining items in stocks predict when the product is sold out given both time invariant and time varying covariates is a multivariate cox regression model appropriate for this problem where the event is the product being sold out what about if the items can be restocked i e by users issuing returns effectively increasing the stock in the process is there a more sensible approach i tried to look up something like arimax for non negative discrete forecasting to predict remaining stock but it seems unnecessarily complicated
1,question anyone know a good website platform for hosting a pairwise comparison survey
1,question creating linear regression channel hello friends i am attempting to recreate the linear regression channel in excel but struggling with the deviation i’ve ran the regression and have the slope intercept best fit line how do i get the deviation away from the slope say i want 2 standard deviations away is it the 95 upper limit the slope how would i increase it to 2 5 with the regression output or is there another mathematical way i can’t seem to figure out the answer and maybe i’m asking the wrong questions which is why i can’t find the answer online it’s been a long time any help would be appreciated
0,unseen data prediction i have a dataset for a bank where the objective is to predict whether a loan will end up as good where it s completely paid off or a bad loan the dataset has the target variable consisting of three labels open the loan is still ongoing good loan and bad loan the dataset is heavily skewed imbalanced where majority of the rows data points behind to good loan and the bad loans are a minority what i have done is filter the dataset for good and bad loans and trained different ml models on it which gives too good to be true performance this makes it a binary classification accuracy precision and recall are all 1 00 or 100 for the validation sets then i want to use it to make predictions for the open rows data points in the csv file but the problem is that since these loans are still ongoing we don t have the ground truth for them and therefore whatever predictions i get cannot be compared against the ground truth to get model metrics such as accuracy precision recall etc suggestions help thanks
2,help on hardware for pose estimation i don t know much about tech so would be very valuable if you could help i m in early planning for a golf ai app at the driving range using 3d pose estimation i m trying to understand the price of the hardware set up for around a golf bay i ve come to the conclusion that i would need cameras probably 2 webcams a nuc wiring and a wifi booster would this setup be correct any information would be much appreciated i will be happy to pay someone if they could give me helpful information
1,the null hypothesis is p 0 77 if the alternate hypothesis is two sided what must be done to the p value if you look up the s score on the z table i’ve been looking through definitions and whatnot for two sided alternate hypothesis and nothing so far has helped come to answer for this problem also that number is different than the one in my problem cause i wanna try and do it myself i’m just completely stuck
2,a list of available datasets for machine learning in manufacturing i have created a list of manufacturing related datasets for machine learning you can find it here i hope you find it useful
0,how do you go about negotiating a contract hourly rate position any tips pointers lessons learned i got a job offer from a faang as a remote data engineering contractor for a 6 to 24 month contract now that i have the offer i would like to get an idea if the compensation makes sense enough to leave my current job and take on some of those risks as a contractor what were some lessons learned from your negotiations as a contractor what were you able to negotiate on and what do you wish you did differently for further background i have only worked fte positions and my current total compensation salary bonus benefits retirement is 127k to get this position i worked with a staffing agency called insight global to get the contract during the initial conversations when salary was mentioned they said max hourly pay is 75 hour which translates to an annual 156k by multiplying by 2080 hours 52 weeks 40 hours week i am not sure if this max rate is determined by the faang or by the recruiting agency since my understanding is that i will be a direct contractor to the faang one perk is that the faang allows 15 days pto but that s about it i have heard the best way to determine hourly pay is to increase your base salary by 30 to cover benefits and retirement so in my case salary bonus 105k 1 3 136k so the hourly rate seems more than enough i plan on taking the offer since it looks like a challenging and interesting opportunity for growing my skills and learning my wife and i will probably get a cheap health plan off the exchange since we will be without benefits i would like to negotiate since it seems like i m in a good spot already have a full time job so i have that leverage any advice is much appreciated
2,research looking for an article or paper similar to data augmentation by pairing samples for images classification hello everyone i remember an article from more than a year ago which trained a neural network on a single or a handful of images the main idea i am basing this on mnist as i remember it from the article was to build new pictures that were made up of the original training images one training image for example was 30 three and 70 five by using this method you would be able to learn mnist with 5 images or less if you include more classes in a single training image i think this is doing pretty much what i am looking for just not for mnist if anyone can point me in the right direction that would be great
2,what do you think about this „don‘t learn deep learning“ post here is an interesting post i have found i am grad student by the way i thought about taking a course on dl after two ml courses but now i am not quite sure if it wouldn‘t be better to take something else…
1,book recommendations for college supervisor hi everyone i am looking for book recommendations i am finishing my masters degree soon and i would love to give a present to my supervisor as he was amazing to me and helped a lot i thought about sending him a book but i am not really advanced in fields of statistics machine learning or data science and it s hard for me to choose anything advanced enough to buy anything that would be special so my question is are there any books that you would love to have or have and absolutely love thank you all in advance
1,with possibly too few data points post intervention for an interrupted time series study should i try a different test hello currently i am analyzing the change in a trend line pre and post the covid 19 lockdowns april 2020 i have two separate datasets one with monthly data points from 2013 q4 2021 q3 dfa and the other from 2015 q4 2021 q3 dfb on each of these datasets i have different points plotted on the same graph ex monthly energy used for the top 10 vs bottom 10 of income for dfa i have plotted both monthly and yearly averages of the data i want to analyze if there was a change pre and post april 2020 between these two groups if being in the bottom 10 caused this group to use more less energy compared to the top 10 but am scared i do not have enough data points to confidently come to a conclusion with the interrupted time series study especially based on this paper zhang f wagner ak ross degnan d simulation based power calculation for designing interrupted time series analyses of health policy interventions j clin epidemiol 2011 64 1252 61 which says you ideally should have close to an even amount of points before and after the intervention so my question is does anyone here possibly have an idea of what other study i could use to analyze this difference or how i could alter the interrupted time series equation of y b0 b1t b2d b3p e to fit my analysis better any help is greatly appreciated and if anything confuses you on this long winded explanation i would be happy to delve in further thank you in advance
1,reservoir computing echo state networks vs lstm s and rnn s has anyone ever heard of reservoir computing or echo state networks does anyone have any idea in what situations they should be used compared to models such as rnn and lstm
1,question why is qualitative research not always considered as statistics i was in an argument with my sister where she was telling me that qualitative research is not statistics she was saying that statistics requires quantitative data now i am a statistics graduate and i know that you can perform statistical analysis on qualitative data so that isn t exactly true but to my surprise from some research there is something called qualitative research that may use no numbers and is not part of statistical analysis i thought the definition of statistics was the analyzation of data so therefore this should be considered statistics in a way but my sister insists it isn t and i have found multiple things on the web saying it isn t considered statistical analysis further research also told me that this form of qualitative research can t be generalized to a population but only be used to explain things about the sample so i have a couple questions 1 why isn t this considered statistics 2 if it can t be used to generalize to a population what questions could it possibly answer 3 how do they analyze data with this method without using numbers or statistics
0,is data science too broad to ever feel prepared for an interview i m a data scientist that does data engineering i get data science interviews from my job title alone does anyone else think data science is too broad of a field to ever feel prepared for the interview for example i feel data science jobs can be broken down into the following types of roles 1 the typical data scientist this is what we typically how we imagine a data scientist the role involves a bit of data exploration ml model building presentations to management etc 2 the deep learning data scientist this is kind of like the previous example but with a greater emphasis on deep learning over traditional ml the role is more likely to ask for a phd this role looks at more interesting problems in my opinion such as computer vision and nlp 3 the data engineering data scientist this is like my current role i work on etl pipelines and bring new data to data scientists in the previous categories for ml model building because of my job title i might be asked to do some data analysis work i work a lot with python sql and aws 4 software engineer data science this data scientist is in reality a software engineer attached to a data science team this is not as common but definitely exists 5 the data analyst with a data scientist job title with this type of data scientist there is less python and ml and more sql excel and presentations hiring managers typically look at non technical skills over technical skills those are all the roles i can think of and i am sure i am missing some but assuming you fit one of the categories it s pretty hard to prepare for all other data science interviews some roles only leetcode you others might ask sql questions others might ask math stats trivia others might give you a take home presentation to prepare
1,discussion free webinar automating data annotation with micromodels automating processes within the workflow to improve efficiency guarantee high quality
2,transformer model which predicts probability distribution hi everyone i m working on a problem where i need to predict human activities in a time window i have a context which is a sequence of human activities and objects and i want to predict the human activities in the next 20 seconds the prediction should be a distribution like 0 2 0 6 0 1 0 05 which can be interpretated as the predicted time window consists of activity a 20 0 2 4 seconds activity b 20 0 6 12 seconds etc so far i have fed the activities in the form of word embeddings into various neural networks e g lstms and used softmax to give me probabilities that has worked well so far i was now wondering if there are pre trained transformer models that i can use to achieve something similar i have already found multi label classification models but these always have a target vector consisting of 0 or 1 per class and are not able to predict a distribution like i want i am grateful for any advice
1,trajectory analysis with shannon entropy i am working on a small project involving the detection of anomalies in aircraft trajectories i rely on historical datasets to estimate a pde of two variables x y of a trajectory and then use it to calculate the shannon entropy of input data as a measure of uncertainty and compare it with a threshold calculated from the initial data higher entropy means anomaly my problem is that a lot of my pde bins end up giving a probability of 0 for a pair x y and by convention the value is ignored in theory the entropy of an anomalous trajectory described by a set of datapoints x1 y1 x2 y2 xn yn could amount to 0 and subsequently fail to be detected by my algorithm is there a way to handle such cases mabe there is a better variant of shannon entropy for this case or a major flaw in my logic
0,working in a bubble how do i know how i m actually doing i think some of us are either solo ing a lot of ds efforts in our current roles orgs or work with teams that may be inexperienced and rely on you as a subject matter expert for data science analyzing modelling structured data that s my situation at least so how do i know how i m doing comparing to kaggle is fine but honestly most notebooks on kaggle are quite poor quality and even the better ones are more cs focused than deep thorough analysis my team is happy with my work but that isn t saying much they don t know better appreciate some insight here suggestions etc thanks
0,resources for learning data engineering hi i have cs bachelors and am interested in going thru a curriculum or learning the core fundamentals of data engineering and gaining practical experience doing projects i m at stage 0 right now have reasonable familiarity with python pandas basic experience with sql relational does anyone have any resources articles online classes courses or tracks books etc they can share so i may learn the core fundamentals for data engineering starting as a beginner a lot of what i ve seen online is like cloudera google aws etc offering some classes tailored to their proprietary systems but idk which softwares programs are most relevant in data engineering so its hard to tell what is a good resource for learning core fundamentals versus company a wanting to grow users in their proprietary ecosystem which may not be used much by many companies in short i d like to know what the most widely used skills softwares programs are for data engineering and what the best resources are to learn apply said learning pragmatically ideally cheap free but is not necessarily a requirement i appreciate any help in advance
2,silas a high performance machine learning foundation for logical reasoning and verification silas is a generic data mining and predictive analytics software toolkit built upon advanced machine learning automated reasoning and artificial intelligence techniques it deals with any type of structured data and performs tasks such as classification regression segmentation anomaly detection prediction and more the below paper introduces silas as a high performance machine learning tool which is built to provide a more transparent dependable and efficient data analytics service we discuss the machine learning aspects of silas and demonstrate the advantage of silas in its predictive and computational performance we show that several customised algorithms in silas yield better predictions in a significantly shorter time compared to the state of the art another focus of silas is on providing a formal foundation of decision trees to support logical analysis and verification of learned prediction models we illustrate the potential capabilities of the fusion of machine learning and logical reasoning by showcasing applications in three directions formal verification of the prediction model against user specifications training correct by construction models and explaining the decision making of predictions journal paper preprint pdf educational version download related work
0,working environments in the real world i m beginning to learn data science and in most courses and programs i m seeing so far students are often asked to start with learning environments like jupyter notebooks spyder or some sort of text editor like g edit or something i m just curious as to how things happen in the real world do people still work in these environments i use jupyter notebooks and sometimes spyder but i understand that one can work in terminal command prompt as well although i m not sure i understand why because editing code and stuff seems a lot easier otherwise just curious
0,silly question let me first say that i am not a data scientist or even a college degree holder i m just someone who thought of something but has no idea how to do it which i am going to ask how to do here pretty much i run a minecraft server and want to show player activity over the course of the last 6 months that i have had it every 50 or so seconds the console log outputs the following line day of week numerical day month year hour minute second utc info there are of players of a max of max of players players online player names seperated by x200b for example fri 14 may 2021 05 15 34 utc info there are 0 of a max of 15 players online fri 14 may 2021 14 51 30 utc info there are 1 of a max of 15 players online player1 fri 14 may 2021 03 40 22 utc info there are 3 of a max of 15 players online player1 player2 player3 x200b is there a program out there that i can use to look at those lines of text and convert them into a graph that can show for how many minutes or something of the sort in a single day a player was on for each day over the last 6 months all the while ignore every other line and if possible how i can make the program do that any and all help is appreciated
2,ways to stop ai from recognizing your face in selfies we upload so many personal photos on the internet so we might have questions like who else would have access to them what would they do with them—and which machine learning algorithms would be trained with this data clearview ai an american facial recognition company has already provided a facial recognition tool trained on millions of such photos scraped from the public web to us law enforcement agencies but that was likely just the start it’s easy for anyone with basic coding skills to develop facial recognition software thus it’s easier to abuse tech in everything from sexual harassment and racial discrimination to political oppression and religious persecution to address this issue there’s a requirement to develop ways to make sure ais can’t learn from the personal data people upload emily wenger at the university of chicago and her colleagues developed one of the first tools to do this called fawkes summary paper project codes
0,portable laptop wifi hey everyone so i m about to update my laptop that is long overdue to hit the trash and i m thinking about how logistics wise i do my schoolwork at work the issue though is i can t use that laptop to connect to the internet by go to makeshift solution was to google or search stuff through the work computer and use my personal computer to do jupyter notebook stuff which doesn t require the internet to work but that doesn t work for things such as using stackoverflow where they want screenshots or wanting to see the exact code used so my question is is there a way to bring wi fi with you so that you can use your laptop on the go is there anything i can buy thanks
1,software how do i open an spss file in graphpad prism hi i really need some help with this i have a large data file in the spss format which was painstakingly typed now my pi wants me to use graphpad prism instead as the graphs look better is there any common format that can be used to transfer my data from spss to graphpad or can the spss file be opened directly on graphpad
1,can you find individual significance from t test if i do a paired two tailed t test p 0 05 on two different sets of data and get p values for two groups is there a way to determine which sets from the group were a significant change to determine efficiency change for example if there were two store groups walmart and kroger selling apples i could get p values for their change from week 1 to week 2 is there a way to find out which individual walmarts or krogers more efficiently sold apples as determined by the p value
0,feature selection for large datasets to begin my question i would like to quote a paper by ishawaran et al on random forests for survival analysis data in which the authors very concisely outline the difficulties of feature selection i e which variables to include in a statistical model in classical regression models and how this problem is somewhat alleviated with more advanced models further because these methods i e classical regression models e g cox ph regression even though it s semi parametric are often parametric nonlinear effects of variables must be modeled by transformations or expanding the design matrix to include specialized basis functions often ad hoc approaches such as stepwise regression are used to determine if nonlinear effects exist identifying interactions especially those involving multiple variables is also problematic this must be done by brute force examining all two way and threeway interactions e g or must rely on subjective knowledge to narrow the search in contrast these difficulties are handled automatically using forests we illustrate the ease with which rsf can uncover complex data structures through an in depth case study of the prognostic implications of being underweight overweight or obese and having severe but stable coronary artery disease investigators have noted complex patterns surrounding possible reverse causation in underweight individuals interactions with smoking and an unclear inflection point at which point increasing body mass confers increased risk some have identified a possible obesity paradox among patients with established heart disease in which increased body mass predicts better survival to clarify these issues we analyzed a large cohort of patients with coronary artery disease undergoing isolated coronary artery bypass surgery using rsf random survival forest we identified a complex relationship between long term survival body mass renal kidney function smoking and number of internal coronary artery bypass grafts we believe our novel findings help explain some of the apparent contradictions previously reported source essentially the authors claim that traditional regression models struggle with feature selection and the newer models e g bagging random forest are able to better deal with feature selection i do remember from an intro stats class the somewhat tedious process of determining which variables to include in a multiple linear regression model as the authors described i remember there was something called cp mallow s criteria in which potential variables were repeatedly included and excluded in the regression model and the value of cp mallow s criteria was monitored a final selection of variables for the model was decided on the basis of this criteria however this selection process becomes inefficient for large datasets if i understand correctly this means you would have to refit the model for many different combinations of variables resulting in a combinatorics explosion for a large number of variables like the authors mention you can also manually hard code interaction terms in the model e g log var1 var1var2 var1 var2var3 var1 var2 var3 etc and there an infinite such number of potential interactions improper feature selection can also result in unwanted effects such as multicollinearity the last point i would like to bring up although my knowledge of mathematics is not strong enough to fully substantiate it is that classical regression models are said to have a tendency to overfit i don t know why i have seen visual demonstrations of this but i don t know if there is a mathematical explanation behind this or if it s just an empirical observation and poorly generalize to new data again i don t know why and that classical regression models are only able to recognize linearly separable patterns in the data intuitively i can understand this e g draw a circle of red points and a smaller circle of blue points that fits in the red circle a single line can not separate the two colors but i don t know if there is a mathematical explanation behind this this brings me to my question about feature selection for large datasets with the advent of technology data is becoming bigger and bigger everyday convolution neural networks are the go to method for analyzing pictures a standard black and white picture is said to have 786 variables whereas dna is said to have even more in such instances it surely must be impossible to address feature selection as done in conventional statistical modelling please excuse my poor understanding of math but my understanding is that newer statistical models have built in methods of handling the feature selection problem for instance random forest randomly chooses different combinations of variables and sees which combinations result in better model performance the exact randomizing mechanism uncorrelated trees is said to also prevent against multicollinearity i ahve heard that the creator of the random forest algorithm leo breiman claims through theoretical statistics that random forest by definition can not over fit and has some desirable error bounds and convergence properties is this true meanwhile i have read on data science blogs i m not going to lie that deep neural networks are able to automatically learn and consider useful combinations of features for approximating the target function am i correct all in all what i want to ask here for large datasets where sometimes the features don t have any immediate meanings e g a patient s blood pressure vs the information contained in the 231st pixel of a photograph is there any real way to handle feature selection or is this usually taken care of by the statistical model itself e g random forest and neural networks i have seen examples online where people attempted to write a massive for loop in which they train the same model with thousands of variable combinations but i am not sure how feasible this is can someone please provide a comment on this thanks
0,causal inference in data science does anyone have experience implementing causal inference in data science what exactly did you use it for and how effective was it did it actually provide some value
0,recent graduate in data science i graduated with a degree in ml datascience 2 years ago and since then been working for a small company as a ml engineer in their r d team unfortunately i’m not getting the growth i expected i would in this role it could be because i’m the only ml engineer no other data scientist or anyone working on the things i’m working on additionally there’s also no data engineers and no data pipelines no cloud the data is all scattered in files and folders as mentioned the company is not in cloud yet so no computing power either working as an ml engineer right out of bachelors and not having a team or even a mentor has been kind of rough to say the least so here i am asking for advise i really am not sure what to do next whether to get an ms in a similar field data analytics ml and then switch my job or to switch first and get some more experience and then think about doing ms i can tell you where i want to be though currently i’m doing core algorithm building and prediction modelling on moving parts but i think i want to venture towards the customer facing stuff – more on the lines of market analysis or product analysis i guess i’m here because i need some advice any advice helps tell me about your journey into ml data analytics what has worked for you what hasn’t any certifications or online programs you recommend to thrive and get noticed for more opportunities and grow in the field any ms programs that are good and worth exploring any advice that allows me to figure out what to do next helps thank you for taking your time reading this and leaving a comment
1,the population of earth is more than 7 billion is the sample size to do any statistical study concerning entire world should be big enough i will explain here as the population is 7 billion some studies come out saying this thing causes that thing in humans but their n size is like 100 200 or even 1000 is that size good enough to make a statistical decision about 7 billion people or should n be bigger comprising of people from different countries to have a better statistical understanding of the entire world i haven t conducted any study but my general opinion is n size should be bigger
1,issues in jamovi help i’m trying to input data into jamovi and it’s not giving me any information i’m trying to find chi square for goodness of fit i’m not sure what i’m doing wrong it’s not allowing me to put in a ratio for expected counts if you need me to explain more or think you could help pls send me a chat
1,differences between univariate and multivariate models hello suppose you have an outcome y and covariates i j and k and you create four models y β 1i y β 2j y β 3k and y β 1i β 2j β 3k if β 1 is not significantly different from 0 in the univariate model can it be significantly different from zero in the multivariate model my experience is that variables often become non significant when going from univariate to multivariate but i don t think ive seen the opposite except it did occur in a cox regression model i found in an article pmid 31838169 figure 3a histology is this more common than i thought rare and not often seen or should it be impossible and i should suspect there is something wrong with the reported model thanks
1,question what analogies have helped you understand statistical concepts i was confused about multilevel hierarchical modeling until i read the following analogy in statistical rethinking suppose we program a robot to visit two cafés order coffee and estimate the waiting times at each the robot begins with a vague prior for the waiting times say with a mean of 5 minutes and a standard deviation of 1 after ordering a cup of coffee at the first café the robot observes a waiting time of 4 minutes it updates its prior using bayes’ theorem of course with this information this gives it a posterior distribution for the waiting time at the first café now the robot moves on to a second café when this robot arrives at the next café what is its prior it could just use the posterior distribution from the first café as its prior for the second café but that implicitly assumes that the two cafés have the same average waiting time cafés are all pretty much the same but they aren’t identical likewise it doesn’t make much sense to ignore the observation from the first café that would be anterograde amnesia so how can the coffee robot do better it needs to represent the population of cafés and learn about that population the distribution of waiting times in the population becomes the prior for each café but unlike priors in previous chapters this prior is actually learned from the data this means the robot tracks a parameter for each café as well as at least two parameters to describe the population of cafés an average and a standard deviation as the robot observes waiting times it updates everything the estimates for each café as well as the estimates for the population if the population seems highly variable then the prior is flat and uninformative and as a consequence the observations at any one café do very little to the estimate at another if instead the population seems to contain little variation then the prior is narrow and highly informative an observation at any one café will have a big impact on estimates at any other café in this chapter you’ll see the formal version of this argument and how it leads us to multilevel models this helped me a lot and the whole thing clicked for me so i wonder what other analogies you ve encountered that helped you understand what s going on underneath a statistical concept
2,project text classification for item matching best setup hi there i am building a text classification model to match the name and description of a customer s item e g name suction press nip category paper machine parts to a list of 10k basic items name steel unalloyed category metals i have some initial matched data to test and i will get more and more hopefully i ve build a sentiment analysis program in the past this is a good example of what i used spacy scikitlearn this current problem is more complex though it s 1 to 10k match and not binary or max 5 6 values the string for the item is short and absolutely at the discretion of the source client item log which reads tutorials examples would you suggest to take a look at in python please
1,can anyone help me interpret and create an f statistic for a joint hypothesis test removed
2,testing a new feature for a model in production discussion i work at a startup and we use ml in production for most parts like training deployments and serving we leverage aws sagemaker we have a nice pipeline written in python which gets the data from the warehouse applies transforms and initiates the training job everything automated however the difficult part comes when we have a new feature idea and the decision has to be taken whether or not to add it to the pipeline the idea is to get a quick analysis for the new feature or features currently we do this in a notebook where we randomly sample the actual training datasets train two models one with new features added and one without and compare the metric on the test set we do not initiate a complete training job as we think that might be an overkill i would like to understand from others int the community firstly is this a common problem across industry secondly what is the best way to solve or get around the above problem and thirdly what are some of the best practices to handle a new feature from idealization to production
1,difference between a distribution and a process i can understand what a gaussian distribution or a poisson distribution is but what is their relationship to a gaussian distribution and a poisson distribution is a gaussian process basically a series of gaussian distributed variables suppose you want to model some data using a gaussian process or a poisson process how do you determine if the data is well suited for using a gaussian process or a poisson process it s relatively straightforward to determine if an individual variable has a gaussian distribution e g a 100 height measurements of basketball players you can see if this height variable follows a gaussian distribution with a specific mean and a specific standard deviation e g kolmogorov smirnov test but how would you check if this variable follows a gaussian process
2,generative adversarial network for tabular panel data i want to generate data using gan but my data is the panel with time and unit dimensions does it exist gans for this kind of tabular data
1,ma vs ms in stats for finding a job my school binghamton university has a 4 1 mathematics bachelors statistics ma program the program looks pretty solid plenty of challenging and interesting classes and i can complete it for cheaper and more quickly that if i were to get a masters at another school however i’m worried that it might be disadvantageous to get an ma rather than an ms in terms of finding a job what do you all think about this also for context my undergrad is a bs in mathematics and i have a 3 98 gpa here is the curriculum for the ma as you can see it certainly looks solid and i know all the grad statistics courses will be rigorous and challenging
1,recent statistics graduate unsure what jobs to apply for i graduated this year with my bachelors in mathematics and statistics i am 32 and do not want to pursue a graduate degree as i am wanting to begin my career i am open to returning to school later but right now i want to work i have completed courses in linear and abstract algebra calculus real analysis differential equations ode pde data analysis lm glm glmm gamm etc non linear optimization mathematical statistics and probability for computer languages i am most familiar with r but i am also a capable user of python i m not sure what jobs i should apply for i have applied to a few entry level data analyst and data science positions i m curious as to whether there are other types of jobs i should be looking into i am also concerned with my resume and what to include in a cover letter i have zero work experience in the field i have completed assignments for school but no real world experience i have been and still am working retail to pay the bills while i completed my degree any advice would be greatly appreciated
1,textbook recommendation for fundamentals of statistics on edx i am busy with one of the earlier courses at the moment and am going to buy the textbook as its giving me a hard time and i want to understand the material more fully the amount of work required caught me a little off guard it definitely puts the moocs that i m used to to shame from the reviews it seems the stats course will be even more work so i also want to get a textbook to follow along with it when i get there i couldn t find any universal recommendations by searching this sub they are all context sensitive or just random for all i know so can anyone make any recommendations for a book that would be a good companion to the course my background consists of an electrical engineering degree completed around four years ago and a few months of refreshing my calculus and linear algebra knowledge
0,looks like the expected number of dating app matches one can get in iceland is 187 i m working on a dating app and was crunching some numbers and would love some feedback to see if there s any holes in my assumptions and calculations thank you
1,education modeling for optimal bet size in a dice game currently taking an self study game theory elective and there seems to be a practice question right out of statistics probability you begin with a starting balance of 10 a dice game where a 100 sided dice is used a roll with a value greater than and or equal to 55 results in a payout of 2 2x your initial bet you get your initial stake back plus an additional 1 2x of your stake back as reward a roll with a value lesser than 55 results in a loss of your bet you may increase your bet size by a multiple of your choice only after winning or losing a bet e g 1 1 1 2 1 3 and so on this choice is systematic and must apply to all your wins and or losses calculate the optimal bet size to achieve an ending balance of 30 what is this question even asking i m guessing that optimal in this case means minimizing the probability of losing your 10 before reaching 30 i know some python so i can build a model to simulate this but i have no theoretical idea i think this is a perfect play question of where to start with something like this
2,iccv reviews are out what did reviewer 2 did do this time
2,purchasing a 40k gpu server for a new lab i was tasked with finding our lab a good gpu server that is built by an outside company i know it is much cheaper and more cost effective to build it ourselves unfortunately the outside assembly part is a requirement so far i found lambda labs thinkmate and system76 but i m sure there are more based on tim dettmers helpful blog i think we should combine 67 rtx 3080 and 33 rtx 3090 but i welcome other opinions do you have a recommendation for any company that builds small 40k servicing 2 5 students working mostly on cv and generative models gpu servers and how should such a server look like
0,does netflix use jupyter notebooks in production i love jupyter notebooks but never thought of them as a tool to put code into production so i was very surprised by this article beyond interactive notebook innovation at netflix found thanks to u yoursdata recent post introducing what it seems a very interesting newsletter this is a 2018 article anyone can confirm whether this philosophy continues at netflix any other companies out there doing this
2,jax learning resources i have been hearing a lot of great stuff about jax lately and from what i understand google s not gonna give up on this project anytime soon so i wanted to have a look at it but unfortunately i could not find many learning resources for jax found a couple of blog posts here and there and that s it so can someone kind enough drop a couple of links to learning resources down in the comments
1,do resampling and exact tests control for false positives in multiple comparisons i was reading the following article when i encountered this there are other methods for controlling the probability of false results when doing multiple comparisons including familywise error rate methods e g holland and copenhaver 1987 false discovery rate methods e g benjamini and hochberg 1995 resampling methods jackknifing bootstrapping— e g efron 1981 and permutation tests i e exact tests—e g gill 2007 page 3 box note i never heard of using the aforementioned procedures for controlling false results in the multiple comparisons situation i do know that sometimes we do control the fdr fwer bonferroni holm and use specific tests like scheffé dunnet etc how can these procedures control de false positive rate in multiple comparisons if they do of course thanks
1,help with choosing test i am comparing average algae heights cm under a factorial treatment design 2 levels of herbivory 2 levels of nutrients over 4 months at 3 different sites i believe my treatments are the between subjects variables because i am looking for the differences between the subjects used for each level i also think my sites are between subject variables because i am not measuring the same algae plots at each site because they are in different locations i am not moving them would the three sites be three different levels then i believe month would be my within subjects variable because i am testing the same subjects every month for 4 consecutive months
0,weekly entering transitioning thread 06 jun 2021 13 jun 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
0,kaggle and burnouts i am interested to know how often do people participate in kaggle competitions and when do they get to work on the competition s code i have a full time data science research position so i am usually occupied during the weekdays in the weekends i just like to take sometime off the screen and do outdoor activities hiking cycling chilling out i am keen on getting more engaged in kaggle competitions so i can have a broader knowledge and more career opportunities in the future however i am concerned that i will burn out having to stay coding behind the computer screen 7 days a week my position is a priority for sure and i don t want my work to be affected because i was not properly taking a time off when i am supposed to how do other people handle such a situation tl dr i am interested in becoming more active on kaggle but i am concerned that i will burn myself out what are your tips
0,ds to find kids who read good i work at a network of charter schools we have 50 schools thousands of kids and to keep it simple let s call it 4 main reading tests per year plus the typical everyday grades kids get on reading assignments how do we aggregate all this info to get to one measure of a child s reading ability in my mind we could standardize the scores then weigh them based on what we feel are the most valuable assessments and output one measure a reading kpi but is there a way to mathematically calculate these weights or just rely on smes to guide us would a regression model help us isolate the features in this case assessments that are most important to predicting their reading kpi can we use ml to predict their score on an upcoming test and take action if they are 1 sd below it should this metric consider the child s progress over time or only compare their performance against their peers
1,would you describe this data as biased an uk based feminist made an anonymous online poll about women subjected to male violence and concluded that 99 3 of women living in the uk were subjected to sexual violence at least one time in their lifetime and average woman will experience violence both physical and sexual at least 37 times in her life image full study can be found here her recruiting tweets for poll specified that she wanted women experienced any violence in their lifetimes at least one time example1 example2 she doesn t specify this in the study itself
1,trying to remember the name of a strategy for reducing the overall error of multiple estimates for the life of me i can t remember name of it but there s a strategy that allows some metric of improved accuracy lowered error when making multiple estimates like some kind of regularization technique whereas intuition would suggest that squared error is minimized by using the mean of each variable there is a strategy that lowers the overall squared error by slightly adjusting each estimate even when the variables and estimations are otherwise independent does anybody remember the name of this technique edit it s the james stein estimator i was looking for
1,help understanding the limits of chi square test for analysis as far as i understand a chi square test tells us if the overall distribution of frequencies differ in what is observed vs what is expected is there a way to discern which categories specifically seem to deviate from what is expected
0,how to do daily check up i want to start sending a list of tasks for the day to my manager in the morning and at the end of the day give a status update on all the tasks is there a good way to do this other than sending out emails mainly want to keep myself accountable in the wfh situation we don t do daily standup meetings so i ve found myself slacking more and more
0,association model used as causal models hi now i’m sure i’m not the only one but although causal models are the holy grail due to time and cost and laziness constraints all my models are just association models not necessarily causal but observational in nature easy so i then communicate the associative nature of model and no do operator or intervention has been modelled at all e g recently and elasticity model i modelled observed price change against qty sold change but then business takes a mental leap and wants to use they models as if they are predictive of ‘doing’ something e g promoting a product in a given week so my main question how do you manage to internally reconcile the somewhat illogical leap from associative model to that model being used as if it is instead a causal model obviously if we say ‘oh no you can’t do that but it sure was a fun model to generate’ we probably wouldn’t have jobs but then creating causal models can range from impossible to very slow and costly when business isn’t prepared to wait this long do we just accept the mental leap from association to assuming some causal relation at times in that intervening now will somehow get similar results as merely observing did and keep collecting our pay checks with a smile i’m interested to hear more on this by the ds reddit community as it seems like a pretty large gaping logical hole in our day to day lives
2,popup online conference machine learning for quantum this is quick post for whom it might be of interest there will be an online conference about machine learning for quantum july 5 9 2021 there will be talks from different experts from all around the worlds in the fields of quantum computing machine learning math chemistry etc there are 4 main topics for poster presentations machine learning for chemistry and materials discovery machine learning for the development of quantum computers machine learning and quantum for industrial applications quantum learning algorithms x200b ps please remove this if it is against the rules i thought some folks around here would be interested in registering abstract submission is not mandatory but it is welcome if you want to present your research
0,survey for data scientists issues faced around documentation on boarding kt hey all i setup a short survey print to learn more about problems that other developers data engineers data scientists face when it comes to documentation it would help me out if you d be willing to give some feedback about the challenges that you face this is a problem that i face when working on new projects and on new teams and something that i want to build and solve for challenges could include like time that it takes to create knowledge transfer of what you work on having to explain to non technical people vs new developers
1,education cmu professor me intuitively explains type 1 vs type 2 errors in statistics false positives and false negatives
2,what does lstm compared to fc do for sac and td3 policy gradient methods and when to use them i came across an implementation for continuous actions where sac uses 1 fully connected layers and 2 lstm this is the same for td3 here is the example my questions are a what advantage does lstm give for policy gradient methods such as sac and td3 b when should you use them i am currently training an agent to control its attitude given external physics simulation as forces i am not sure whether to try and use lstm for it or no
2,indian vehicle dataset to use object detection and classification problem for indian vehicle dataset which contains robust data i ll keep updating the dataset as more images come through here the sample images are given here
0,tools for creating interface models which tools do you use for creating interface models i am currently working on something which would need doing model driven system engineering
2,how did you implement papers with models that required a lot of gpus to train i m self learning ml and trying to implement the papers listed here but i don t have access to hundreds of free gpus like those corpos do edit i have rtx 2060
0,time series forecasting of sales data no trend only seasonality urgent greetings ds community x200b please help me out here i have a dataset provided by a company which is a daily tabulation of sales data over 6 months i want to create a program which would predict and showcase the next n values n as an input the data has no trend although it shows seasonality on a weekly basis the data rejects the adcf test and is stationary without any differencing even if i difference it over a shift of 1 the p value just drops heavily below 0 05 furthermore i have applied autoarima for understanding the best parameters for the sarima model but i still can t get good predictions i even used fb prophet but i don t know how to actually maximise the accuracy of these models can someone please help me understand what exactly should i do to get tangible results if someone can spend some time i ll reach out to them via pm and share the pacf acf and the decomposition plots please help me the deadline is tonight
1,intuitive explanation as to why some models can t handle categorical variables with too many categories i am working with an example similar to this i am using a random forest model and one of the categorical variables i am using has too many categories around 80 when i run the model i get the following error i am working in r error in randomforest default m y can not handle categorical predictors with more than 53 categories however this error goes away when i remove that variable and everything works fine i was wondering why does this error happen is it because the software is unable to handle this problem or does it have anything to do with the math behind the random forest algorithm for example random forests are made up of many decision trees decision trees themselves use the gini impurity or entropy to make these splits e g is it possible that the mathematical formulas don t work when there are too many categories for example maybe too many categories lead to a sparse matrix or one of the intermediate calculations results in a division by zero or results in all possible splits becoming statistically identical to each other
1,measuring the influence of three factors within a larger construct i am currently conducting a research dissertation into a given area much of my progress has went very smoothly and i have reddit to thank for such support i may use incorrect jargon so i will explain everything in full i have three scales which together form construct x i e these three scales measure different aspects of the same concept now what i want to find out is which of these three scales is of most importance or which of the scales has the most influence over the overall construct lets take the basic idea of anxiety i m spit balling here you measure stress sadness and loneliness on three separate scales together they form the construct anxiety is there a test which will display which of these variables is of most influence the data i have are on 10 point scales so the three scales which make up the construct each are on a 1 10 scale where 1 is low and 10 is high my n is small as this is student run research n 220 i have tried pca efa and a few other techniques i am either misreading the results or i am correct in assuming these only reduce constructs to core values not tell you which of the core values are most influential
0,people you work with just wanted to gauge how much data scientists like working with their colleagues i ve been interviewing for various positions and have been disappointed in the people interviewing me most of the time i m not treated respectfully and subsequently haven t been able to see myself working with the interviewers it s turning me away from joining the profession
2,simple implementation of 2d convolution neural network cnn i am looking for a good reference implementation code of 2d cnn convolution neural network using simple math operations instead of framework s high level tensor operation this is for inference only many open source code implementations out are quite complex and utilize high level operations
2,can someone suggest an open free face recognition dataset i m looking for a facial recognition dataset which is free public to use for research purpose i need a dataset which includes a relatively high number of images per identity 200 images per identity for at least 30 identities with labels do you know any
0,any ds freelancers out there hi i m thinking about exploring ds in the freelance world i have started working full time in january but the position is not very challenging or time consuming and i need more projects i m wondering if anyone out there has any experience as a freelancer consultant or part time and what your tips are to be successful thanks
2,recommended stack for deploying pre trained machine learning models hello guys so i have a pretrained pix2pix gan pytorch model that takes an input edge drawing and from one folder and output the drawing colored in essentially im currently trying to build a web application that would allow for users to upload there own drawings and receive the generated results off the top of my head a webserver hosting this model and building an api to handle the post request to talk to the server to process the images would be one way to go about this but im not really sure if this is best practice the application is built in react and node js i was wondering what is a recommended stack for trying to implement what i am aiming for x200b i ve come to the conclusion that training and running models locally vs deploying them are 2 different beast 😅
1,how would i go about checking if one group s results come from chance or fraud for example i have a bunch of groups which recorded 1000 for a and 500 for bbut i d like to check one of the groups which recorded 100 for b and 50 for a how would i know the chances for this to occur by chance or luck or if its most likely that this difference is caused by fraud in theory there should not be population differences between the groups they should all come from the same population for example if i have 5 groups which record 100 a 50 b 90 a 48 b 110 a 45 b 80 a 57 b 10 a 100 b in this case i would be kinda sure that the 5th group most likely came from fraud and is not a real result what math would i use to figure out the confidence that its fraud or check that its at least 99 9 sure its fraud
1,does anyone here study use models from functional analysis i came across this wikipedia page on functional data analysis are these kinds of models still being used nowadays alongside deep learning models does anyone use them in their work research
1,methodology for estimating expectations by dividing and conquering partitioned data suppose i have a set of iid observations y1 y2 yn which are partitioned in m blocks a1 a2 am for m n my goal is to estimate e f y the most natural thing to do is to take the sample mean 1 n sum i f yi as an estimate this would be unbiased and converges to the true expectation by law of large numbers alternatively i could estimate a1 s sample mean i e ybar1 1 a1 sum j in a1 f yj and similarly ybar2 corresponding to a2 so on and so forth then combine these estimates ybar1 ybar2 ybarm via some function h ybar1 ybar2 ybarm to produce another estimate h is some arbitrary function so h could be defined in such a way that it recovers the original sample mean is there a name for this methodology i m interested in learning about the construction of h that could reduce variance at the cost of introducing bias
0,data science in vc pe firms hi everyone those who work as data scientist in venture capital private equity firms can you share what kind of work do you do what do you solve for etc
1,how to find correlation between two independent variables i am very new to statistics and i am trying to do a statistical analysis on these two independent variables this might be very basic question but could someone share what could be a good way and also what online resources i could use thanks
1,how to report a kruskal–wallis test to apa student guidelines i seem to find many articles using different methods to report the statistics any idea or link to an article reporting it correctly that i would then be able to reference i am attempting to report the distribution of the scores did not improve when transformed using a logarithmic scale and therefore the data was analysed using a series of kruskal–wallis tests and reported here using similar statistics to relationship lsns r score significantly differed between the drug groups x2 7 01 p 030 with posthoc pairwise comparisons indicating that after conducting a bonferroni correction the mdma median 90 15 group scores were found to be significantly higher than the polydrug group scores median 71 41 p 037 see figure 1
0,documenting code in data science my team does not do much documentation they tend to think it s not worth the time to add docstrings or other documentation and figure the code is readable and anyone can just figure out what the code is doing by looking at it we all tend to work on different models but often we need to understand how other models work and i think it s worth the time to have better documentation since what seems obvious to the person writing code might not be clear to another team member i m just wondering how other teams think about enforcing documentation i don t think it s a waste of time and there will most likely be someone taking over someone s codebase eventually and it s painful to try to understand some functions when that person isn t around to ask questions to
1,can anyone remember a youtube tutorial for bayesian statistics that used measurement of the length of a football field as its teaching scenario i have a vague recollection that the video had a black background with neon coloured virtual pen being used by the presenter i watched it a few years ago and cannot find it anywhere
2,has anyone tried combining self supervised learning with active learning when talking to ml engineers using active learning we often heard that they had issues with the active learning queries selecting many similar examples for example a cat and dog classifier might have low confidence with the classification of small dogs such as chihuahuas doing active learning at this stage only with model predictions might result in a query of many chihuahuas essentially oversampling this class since we were already working on self supervised learning see our open source repository lightly we wanted to see whether features learned from recent self supervised models could help we did some early tests and the results look quite promising the model predictions can be used to find images where the model is uncertain the embeddings can be used to diversify the queried images we wrote two tutorials summarising this workflow active learning using detectron2 nvidia tlt active learning tutorial we would love to hear your thoughts and whether anyone is working on something similar
2,advertisements in this sub a while back there were a couple of posts post 1 post 2 on the increase of self promoting blogs and youtubers on the sub i won t beat a dead horse and repeat what they ve said but more so than self promoting blogs and youtube channels what concerns me is the increasing number of self promoting products on this sub there are posts trying to convince people to use a product from their startup here are some examples just from today s feed example 1 example 2 example 3 example 4 example 5 i am not sure i like the idea of a research oriented sub being overrun by promotion of products even if they are related to ml rule 5 prevents non arxiv link posts on weekdays but in my opinion this should be expanded to include text posts which are only promoting a startup or their product thoughts
2,dynamic view synthesis from dynamic monocular video chen gao ayush saraf johannes kopf jia bin huang project abstract we present an algorithm for generating novel views at arbitrary viewpoints and any input time step given a monocular video of a dynamic scene our work builds upon recent advances in neural implicit representation and uses continuous and differentiable functions for modeling the time varying structure and the appearance of the scene we jointly train a time invariant static nerf and a time varying dynamic nerf and learn how to blend the results in an unsupervised manner however learning this implicit function from a single video is highly ill posed with infinitely many solutions that match the input video to resolve the ambiguity we introduce regularization losses to encourage a more physically plausible solution we show extensive quantitative and qualitative results of dynamic view synthesis from casually captured videos
0,after a year of remote work what does your wfh setup look like for those that are still working remote how has your wfh setup evolved over the last year i m currently using a work provided 16 mbp with a single 27 monitor the monitor has a built in kvm which makes it easy to swap between my work laptop and personal desktop but the screen real estate is a bit lacking i m curious to see what setups r datascience use to maximize productivity anything you guys really like hate upgrades you wish you made earlier
0,when is it imposter syndrome i ve started working as a data analyst at a company where i used to work in a non techical role they didn t have an established data team instead myself and my manager we both graduated last year great opportunity for me but i m being asked to write a pretty big app in python and i m really struggling with it from a technical know how standpoint i feel like maybe i m not cut out for data work but also understand that imposter syndrome is a thing just looking for advice from people who have spent a longer time in the field are there moments where you re completely over your head do you just try and work through it thanks team
1,too many observations in lme4 model to calculate df s hi i fitted a mixed effects model using lme4 lmer the data has around 11 000 observations model is great but when trying to retrieve emms using emmeans i get into trouble the df s cannot be calculated but i need them for reporting they are returned as inf even when letting emmeans run with options set to do your thing my computer runs out of memory calloc error i could aggregate over one factor before fitting the model to get it down to 2 000 observations but that would throw away information do i have any other options
0,data scientists in leadership positions what are your strengths what do you focus on now and how do you approach new ideas do you have a framework to protect yourself and your team from risky work
2,dataset management what is the best tool for a team to collaborate over an image audio dataset our team is doing several audio and image detection projects and we have not found a great tool for the whole team of varying levels of technical expertiese to collaborate in so far we have ended up going with dropbox because it s easy enough for our collection team to view and review images audio but also has a robust api to plug into our pipeline seems like some options would be scale roboflow v7 labs thanks
0,i’m a pm a and need data science advice lead product manager here who recently joined a new company i have about ten years of experience but this is the first time i’m working on a heavy data science product with a junior data science team 😔 i am new to the org and on my second day the lead of the data science team put in his two weeks but didn’t stick around to provide institutional knowledge or any time for me to understand what’s been going on the data science is a small team of 4 and they are very very junior with that said i’m really trying to get them involved in our daily meetings so they can understand the vision and product we are building so ultimately be a individual contributor and ultimately point out where data science work is needed the problem is they don’t say a word it’s been 8 weeks and the data science really does not give any input at all not even on our designs which is super important and when asked to give feedback they stall and take it off line but i really think they just don’t have proper leadership i just can’t get them to give any input so i am not able to understand backend implications and such i imagine at a certain point the data science team will have to build “something” to power be efforts for example but if they don’t tell me that up front when discussing designs or requirements i can’t plan or gather solid data science requirements which is nuts what can i do to get more data science input to fuel our mission 80 percent of requirements have a data science implication from what i see and i need the data science team to step up more what can i do to make sure we have proper data science requirements how can i set up a successful data science strategy
2,randomized image augmentation during training tl dr is it a good idea to apply random augmentation during training every image is only seen once instead of training multiple epochs on the same fixed training set of images randomly augmented beforehand so i m training an ocr classifier to recognize chinese characters of a specific font in a specific newspaper i have a very similar font as a ttf file but very little labeled data 1000 manually cropped characters from the newspaper which i m using as a validation set so i generated a png image of about 4000 glyphs the ones i want the ocr classifier to be able to recognize from the font file and applied different randomized augmentation to it random translation different intensities of random noise random patches of increased brightness and morphological opening closing after random noise application in all combinations i get 36 augmented images from every original glyph image a fixed synthetic training set of 36x4000 images training googlenet on this yields about 20 accuracy when evaluating on the real life validation set not good but it s a start with the training error going down to almost 0 so i was wondering if since the augmentation methods are all based on randomness it might make sense to not have the model overfit on a fixed selection of them but instead take the original 4000 glyph pngs as a training set and apply a randomly chosen combination of the augmentation methods described above as i load the image if i m not mistaken it will make it more of an online learning approach it makes a lot of sense to me but i can t find any papers on this with image augmentation being so wide spread and much talked about why does everybody do it beforehand and train on a fixed data set trying to combat overfitting afterwards instead of serving any of an almost infinite number of combinations of different randomized augmentation methods during training improving generalization and avoiding overfitting is it for reproduciblility
2,does knowledge distillation really work nyu google study provides insights on student model fidelity a research team from new york university and google research explores whether knowledge distillation really works showing that a surprisingly large discrepancy often remains between the predictive distributions of the teacher and student models even when the student has the capacity to perfectly match the teacher here is a quick read does knowledge distillation really work nyu google study provides insights on student model fidelity the paper does knowledge distillation really work is on arxiv
1,how to get into career in data analysis i’ve become more and more interested in learning statistical and data analysis skills i worked in psych research so i have a bit of a background but i’ve found myself leaning towards policy analysis and research positions i worry that i don’t have the skills necessary for these fields and i’m wondering what i could do to gain more experience now any thoughts are welcome thanks
2,knn vs linear regression discussion when should i use knn and when should i use linear regression like gradient descent for regression how is knn better than linear regression and how is linear regression better than knn
1,question how to derive newton s forward interpolation formula from lagrange s formula how to derive newton s forward interpolation formula from lagrange s formula
1,what statistical tests to perform when i have low sample size n 16 and all binary categorical variables n 16 i have binary variable diseased non diseased and several binary variables like particular mutation present non present i know that i can do simple frequencies of mutations for diseased non diseased is there any other particular statistical tests that i can perform with this sample also any particular plots that can be useful here to present data
1,will do r coding free for practice hi i m a rising sophomore in college who wants to major in statistics i m looking to gain more real world experience in using r after taking a few r classes in my first year i think i have the skills necessary to accomplish most basic tasks and want to learn more using various online sources to complete more difficult tasks if anyone has some coding they need done in r you can message me and i d love to work on it for you to gain some experience obviously i would do this for free as the goal is to improve my skills thanks
0,anybody using a m1 apple product for local modeling work hi not sure if this post violates the on topic rule it is ds related in the applied sense in terms of being a practicioner sorry if it is considered ot i usually work locally on my machine which has always been a laptop with an i7 or i9 with a lower end nvidia gpu for small to medium sized modeling tasks i m going to start a new job soon and will have my choice of work laptop big compute tasks can be performed on the cloud however for prototype poc work with limited datasets that don t require very intense hyperparameter searches i typically work locally i ve been reading some interesting things about the performance of ml libraries on m1 machines and it looks like deep learning packages as well as low level vector libraries and libraries built on top of them such as numpy are very quick these days with the m1 is anybody using an m1 machine these days for ds i won t have time to mess around with complex builds and such i m generally somebody who just relies on anaconda to install what i need and make sure all of the packages work nicely together is the m1 there yet in terms of being ready to hit the road for ds work with minimal fuss my other question concern is memory allocation for the gpu cores when using dl libraries since the memory is unified if i have 16 gigs how is that split between general system use and gpu use thanks
1,pairwise comparison organiser does anyone have access to a free piece of software i can use to organise my pairwise comparisons i have run a one way anova with a tukey post hoc on some data i have the trouble is is that i have 15 different groups which means i’m having trouble sorting through the comparisons and writing the correct subscript letters on my bar graphs any help here would be greatly appreciated thanks in advance
2,is there any feature for ranking most popular arxiv org papers it would be useful to see which papers were most read or downloaded in arxiv org which has been a de facto publishing platform for cs related research so i was wondering if there was any portal or platform that ranked popular arxiv org submissions or maybe there s some feature in arxiv org itself that i don t know about if there isn t any feature of this sort then perhaps it should be
1,where to get room rent data so i m looking for historical data on rents for renting rooms in northwestern europe mainly the uk netherlands germany belgium and france i will generate insights for students based on this data i have tried to find it in eurostat statista and cbs but couldn t find anything on rented rooms rent m 2 yearly or monthly could be a suitable metric or just average room rent as most likely i could look at the average room size to get to my preferred metric as long as i have some data to work with studio s apartaments rents would also be a possibility but not preferred i hope somebody knows where to find this
1,are statistical models able to make predictions about individuals are statistical models in theory able to make predictions about individuals suppose you have an individual with observed covariate information x a y b z c in theory can a regression model trained well on some data predict the expected value of this individual s response variable i heard today that statistical models are not designed to make predictions about individuals they are only designed to predict the average behavior of a large group of individuals and in theory should not be used to make predictions about individuals is this correct does this mean that any time statistical models are used to make individual predictions this is going against the intended use of statistical models if i understand correctly this means that when a statistical model makes a prediction about an individual with observed covariate information x a y b z c it s making a prediction for the behavior of all individuals in the universe with observed covariate information x a y b z c is this correct does this mean by definition the idea of making predictions for individuals is a fallacy thanks
0,will i be disappointed in my first industry job after i do a phd hello i’m an undergrad student whose considering a phd in statistics my goal is to go into industry after ideally as a data scientist or in a research data scientist position i would do an ms but i personally would feel as though i should have gone all the way to do a phd and have regret if i just got an ms however there’s a lot of people with ms who have jobs in industry my question is if i get a phd will i be disappointed by my actual job in industry will i feel as though i could have just gotten a job with an ms it’s a complex question i guess because i first need to figure out if i want to do one or not because from reading this you guys may feel i’m not entirely sold on a phd but what i’m asking is that doing all that research in a 5 year program will the opportunities i get in industry be not what i expected to be my expectations cool ml projects statistical analysis etc
0,is it a primary objective to establish cause and effect in data science like the title says is data science like other sciences in that it’s important to determine cause and effect or is it enough at least in the business world that you have a model that seems to work
0,is it better to pivot data before working on it or to work with it unpivoted i m getting into a lot of data projects lately and the data i m being given is unpivoted there is a primary key column that has repeating values in my case company id which is a id number assigned to each company the sales team sells to a process step column which shows where sales is in the sales process five different steps and finally the process step change date if i leave the data unpivoted if i want to do any sort of analysis on how long it takes to go from stage to stage then i have to figure out a way to subtract dates at different stages from one another for a specific company id if i pivot the data then i will have repeating company id values and have the process step in their own columns with the process step change date as the values in those columns there will obviously be blanks because there are multiple company ids that i then have to delete which creates another problem we can sell to a company more than once so if i m removing duplicate companies i could be deleting data unnecessarily any recommendations here
0,looking for open source model serving framework with dashboard for test data quality do you know any open source framework for model deployment but with the following features it should have a dashboard for test data quality monitoring ideally with alarms from the great expectations framework should support a b testing of models it should be cheap to deploy and run the live data quality dashboard is important to me maybe there are other ways to monitor new data quality
0,create a model to improve a business issue i have a great opportunity in a company it s a b2b company the ceo of this company really want to hire me but he wants to test me one day to be sure this company use a call center and the ceo ask me to find a way to increase the pourcentage of people accept to take the call in this moment the call center get 30 of people taking the phone call the compagny wants an increase of 5 so i need to build a model what kind of variables should i take according to you to build it sexe age professional activity i have no work experiences but just theorical knowledge i need some advices thanks ps sorry for my frenchglish
1,glmer and convergence would there be any problems with running a glmer model with logrt instead of just rt i e glmer logrt i tried to use just rt but it basically gave me convergence issues for all models even the simpler ones logrt on the other hand produced working models that pass allfit and all is it just a problem for interpretability or is there any reason why one would be very cautious about doing this
2,fuzzy learning vector quantization for classification hello i m in the middle of my final project creating classification model using fuzzy learning vector quantization karayiannis 1997 but i m having a problem regarding turning it into a classification karayiannis created the method for clustering but some people have been using it as a classification method by measuring the euclidean distance between test data and the final cluster which generated from the training phase the problem is i couldn t find the perfect value for the initial fuzzy number mi and final fuzzy number mf also everytime i changed the initial cluster for the first iteration the accuracy seems to change but never reached past the point of 0 40 40 accuracy i couldn t seems to find the reason between low accuracy result if anyone could help me understanding the issue would be nice i ve been using human activities recognition dataset from uci uci har available at kaggle adding pca with n component is 200
1,french language beginning statistics textbook hello all please forgive me for an off the wall question here i am learning french but obviously most of the language instruction won t cover topics in things like statistics i wanted to enrich my learning by reading through a familiar topic in the language i m learning does anyone have a recommendation for an introductory statistics text that is published in french thanks
0,which topics from calculus do you use regularly in your work i’m curious which big concepts from calculus do data scientists need to know and use regularly do you use any of these concepts often things like integrals polar coordinates single variable calculus or multi variable calculus fundamental theorem of line integrals what concepts from calculus is used heavily in data science work thanks for your time
2,improving language model behavior by training on a small curated dataset interesting research results by openai it seems possible to improve the behavior of a gpt 3 language model by fine tuning it on a very small dataset of course we are talking about undesirable biases hateful agressive racist sexist etc they only used 80 texts on the other hand they neglect to say that someone can very well adjust the generated texts to favor biased texts with again a very small corpus the scientific paper pdf
0,powerpoint graphics visualisations tips are there any recommended tools templates for presenting my data neatly in my presentation slides i can produce neat python and r graphs and plots and paste them on my slides however i have seen many presentations by companies where they use nice and fancy layouts for graphs and also other graphics and icons i would like to ask the fellow data scientists in this sub how they produce elegant designs and how much time and effort it takes from them i am working in research so presentations are not as important as the methods and the results for me nevertheless generating nicer presentations would be a great skill to acquire
0,do you have a separate devops projects for data science data engineering data management and separate scrums or is eveything kept in one devops backlog just curious how things are tracked curently in a small team that will be steadily growing into more formalised functions over the next few years thing ds de governance architecture bi etc and some people want to integrate all backlogs into one and have one sprint for everything whilst i have reservations about how this would work in practice when some workflows are just different even if they need to communicate from time to time i guess i think separate devops processes ans backlogs that integrate into jira would work best and data scientists need only join ds stand ups on the regular and other stand ups on ad hoc basis what s your experience and view of this thank you
2,what would be the best approach to use embeddings extra features from a tabular dataset i already know how to create embedding however the other day i was thinking what happens if you have text in one feature but you also have other features that can help your model performance how can you use your embeddings and extra features to create a new model for example if you have 20k of vocabulary x 60 dimensionality embedding matrix how can you add extra features to the model it could be in any framework tensorflow pytorch etc
1,times running out which courses do i actually need to qualify hello all i will be a junior in the fall at my university and i’m currently majoring in statistics my goal has been grad school and even a phd since i really want to dive deeper into unsupervised learning methods clustering dimensionality reduction mixture models etc it had come to my attention from reddit that my major course work will not suffice for qualifying for phd programs i was also misguided by my advisor as he told me the major they had was intended for phd programs but now i’m coming to realize this is just a bunch of bs and i’m underprepared currently my major offers a whole host of stats classes calculus up to multivariable linear algebra not even upper division and two proof based math classes and one of them is a foundation of proofs and the other is an introduction to analysis the links provide syllabi to those courses and you be the judge of its rigorous enough but i have absolutely no class which is just real analysis i’m at a junior and at a point where i should be doing gre and such i now have to jam pack my latter two years of school with the requirements my major poorly fails to include in fact didn’t even tell me i needed a math minor when i asked them anyways this post isn’t about bashing the curriculum it’s about asking what courses do i need to take before i graduate i’m at a point where time is running out and i don’t think i can fit a whole host of classes in my schedule anymore so i need suggestions on what are math courses i should make a priority of taking for a phd before graduating please keep this in mind before commenting if you were in a situation where time is running out what classes would you recommend taking to qualify for phds thanks
2,finally actual real images editing using stylegan in the last years i have been interested in different technologies that enable facial editing one of the promising directions was editing faces using stylegan nevertheless each method that came up while succeeding in editing a small number of celebrities always failed to edit my face and many of the faces i wanted to edit i assumed the problem was with an inherent bias inside stylegan and decided to wait for its third version which just came up see so excited about the new opportunities it will bring to the world of graphics and editing in the meanwhile a very interesting paper called “pivotal tuning for latent based editing of real images” was released with many papers stating that they can edit real images i was not much optimistic about this paper as well but boy was i wrong for the first time i could actually edit facial images using stylegan the authors provide an inference notebook which i used to edit 2 machine learning legends see the results by yourself… the notebook the github repository what do you think will this paper and the advances in the field will affect our lives hollywood deepfake etc so much potential x200b x200b younger original image smiling x200b younger original image rotation
2,any recommendations for image annotation software i ve been using supervisely enterprise at work and it s been going really really well but my work has been paying for it meanwhile my grad research work is requiring a large scale annotation effort and my advisor is queasy about forking over grant money just to annotate we can t use the community version because hipaa i ve been looking at other free annotation software and i m trying to make a decision on which to use any suggestions on what has worked for you
0,mentorship i am a current college senior with several internships and research fellowships under my belt i am currently working for the summer for a large insurance company in my area as a data science intern but we have no practicing data scientists in my organization i am looking for someone who can serve as a mentor to me during the end of my education and into my career does anyone know of any us groups or resources that connect professionals who are willing to mentor to mentees i know just enough to know that i don’t know a lot but i just lost my mentor to an international job thanks for reading
2,the thesis review podcast with tomas mikolov statistical language models based on neural networks the creator of word2vec describes his work on language modeling and recent projects about complexity and cellular automata in this episode of the thesis review podcast tomas mikolov recently left facebook to move back to academia apparently also an interesting discussion about persistence in research and the difficulty of working on non mainstream ideas e g rnn and statistical lm in the 2000s link to the podcast
1,survey tool with a time limit first and foremost i m sorry if this isn t the right subreddit for my enquire but a similar post got auto deleted from academic psychology and i couldn t find a sub dealing with methodological issues explicitly so as the title states i need your help in finding a survey tool as for a class we have the task of conducting a basic scientific research long story short i need a tool that can split the whole survey into three different parts with one of it having a time limit of 25 minutes i would ve usually done it in google forms but i think the time limit eliminates it as an option so if anyone has any recommendations i d be really thankful
2,65 of execs can’t explain how their ai models make decisions survey finds from this venturebeat article in fact only a fifth of respondents 20 to the corinium and fico survey actively monitor their models in production for fairness and ethics while just one in three 33 have a model validation team to assess newly developed models how should companies responsibly assess deployed ml systems what metrics make sense for evaluating bias and assuring regulatory compliance in these systems once they are in the wild edit that’s what i get for using the article’s clickbait title… no one read past the title what about the other aspects of the survey
0,any library that can help me with identifying the correct data type for a given column i m looking for anything that can help me with this type of problem i basically need some help identifying the type that a given array is intended to be given its values titles etc for example i would need some code that realizes that this 45321 is intended to be numeric or that a given column is intended to be date even if some values are misformatted any help in the right direction is appreciated
2,improving bart text summarization by providing key word parameter or topic aware component hi all i am experimenting with hugging face s bart model pre trained by facebook on the large cnn daily mail dataset below is an example snippet of code that achieves the task from transformers import bartforconditionalgeneration barttokenizer model bartforconditionalgeneration from pretrained facebook bart large cnn force bos token to be generated true tok barttokenizer from pretrained facebook bart large cnn article text to be summarised batch tok article return tensors pt generated ids model generate batch input ids tok batch decode generated ids skip special tokens true i am now thinking about how i could insert an intermediary layer or keyword parameter which would indicate to the model to focus on particular words and words associated with the keyword for example if i insert a block of text which talks about different countries and the cars commonly found in those countries and specify the key word cars i d expect the summary to talk about which cars are found and in what quantity rather than information on the different countries for this i see one of three potential approaches which i have embedded into the overall process flow 1 bart encoder transforms text into a tensor of numbers 2 one of three options for this step a run a topic aware model e g top2vec genism tf idf other on the encoded article to produce a topic aware summary in tensor form b run a bart model on the encoded article to produce a summary in tensor form then run a topic aware model e g top2vec genism tf idf other on the bart model summary to produce a topic aware summary in tensor form i e a two step model pipeline c run a bart model on the encoded article to produce a summary in tensor form train the bart model to be biased to select topics or introduce a global parameter in an attempt to have the bart model focus on summarizing the article based on the topic 3 bart decoder transforms the tensor output from step 2 into a text summary open to discuss which would be a good path for implementation and also discuss ideas on how to actually write code for this
0,starting out as a data analyst to move into data science this is a unique situation let me start out by saying i am a “it support analyst intern” at my job part time what i do however is not all that complex i use pivot tables and excel as forms to show company spending at several locations i don’t recommend anything i simply show the bills in the best way i can currently it’s a pivot table from the previous employee my career goal is data science and starting out as a data analyst to get there perhaps getting a masters while being a data analyst currently my higher ups told me if i can learn python and how to somehow implement it in my job i can use it for resume building purposes so i’m reading “automate the boring stuff” since it has parts about python with excel and pdfs allow me to also note i am a cs major specializing in data science this does have a class for python with data science but i’d rather learn it sooner for experience purposes this has nice a machine learning class too i won’t be able to take for another year of course sql is in the database class next semester my question is what else should i be doing now to help get an actual data science internship sooner or data analyst if not since that’s not my current job title would using python with excel to show bill amounts count as a “data analytic” experience i would think not because it really doesn’t cover the broad strokes of the full job position “data scientist analyst” unless there’s a way i can visualize excel data i’m missing apart from python is there any key skills i have to learn asap even with a class coming up like sql and during this what actual data science skills should i be looking at right now to aid in actually getting a possible data science internship is there any key skills i’m missing are there any good resources to learn these skills like python if not my current book sql spark etc
1,how to test if the differences of a subgroup significantly different from the total group i am ashamed to admit how much time i have been searching for some answer on this i feel like it it is simple and i am just missing something i am looking to compare whether the gender split in a subgroup is significantly different from the gender split of the full group technically those not in the subgroup for example lets say i want to see if the hr depts for companies in a state have a gender split that is significantly different than the rest of their company the null hypothesis would be the gender ratio of the subgroup is the same as those not in the subgroup could i use an independent t test to compare
1,why is the beta distribution used to model the number of times a river floods let s start with an example problem case say we measure two variables that are non normally distributed and correlated for example we look at various rivers and for every river we look at the maximum level of that river over a certain time period in addition we also count how many months each river caused flooding for the probability distribution of the maximum level of the river we can look to extreme value theory which tells us that maximums are gumbel distributed how many times flooding occured will be modeled according to a beta distribution which just tells us the probability of flooding to occur as a function of how many times flooding vs non flooding occured source does anyone know why the beta distribution is used here to model the number of times a river floods thanks
1,which statistical test should we use we are trying to analyze data collected from an experiment that we conducted for our graduate thesis research and we need some help choosing the right statistical test for context we are ux design students doing our master thesis so we don t have much statistics knowledge our experiment involved letting participants play an fps game with 6 different user interfaces and 6 different levels and measure their performance in terms of reaction time accuracy etc the combination of level and user interface were randomized meaning there are 36 possible combinations and each participant played on 6 combinations of those so we have 2 independent variables that are categorical and 9 continuous discrete dependent variables the data we got could look something like this table below for one participant with 2 of the dependent variables user interface level reaction time accuracy a 3 0 5 0 5 b 2 0 6 0 6 c 1 1 5 0 2 d 6 0 7 0 5 e 5 1 9 0 4 f 4 0 5 0 3 our goal is to see how much each user interface affect the player performance but with this experiment design the performance could be both affected by the user interfaces and the levels therefore we want to find a way maybe to remove the impact of the levels on the data or isolate the impact of the user interfaces we have looked into anova manova multilevel models repeated measurements and some other tests but haven t found what we exactly need some suggestions with explanation would be appreciated
2,intelligent graphic design adobe’s directional gan automates image content generation for marketing campaigns a research team from adobe proposes directional gan dgan a novel and simple approach for generating high resolution images conditioned on expected semantic attributes greatly simplifying the image content generating process for marketing campaigns websites and banners here is a quick read intelligent graphic design adobe’s directional gan automates image content generation for marketing campaigns the paper directional gan a novel conditioning strategy for generative networks is on arxiv
2,gpu render farm machine learning hi i m building my own gpu render farm on nvidia rtx 3080 and 3090 i rent out my systems to 3d designers to render their projects it s interesting to me if i can provide ai developers with powerful systems to do ai tests and so on do they need such rigs for calculation what do you think
2,transformer encoder and temporal sequence encoding is there any way of including temporal data instead of positional data as the input for a transformer encoder
1,what sports postseason has the most unpredictable results edit i’m talking about the big 4 usa sports leagues nhl nba mlb and nfl what would be the most random sports postseason what playoffs do lower seeds constantly beat higher seeds we know in tournaments typically the higher the difference between the seeds between two team gets the better chance the high seed has to beat their matchup while this is true i feel like in the nhl it just about does not matter many times the best team in the league bows out in the first round or second round sometimes rarely ever can you predict an nhl playoff going according to the regular season good teams can get worse and bad teams can get good more years than not the team that wins the cup is kind of unexpected good teams no doubt win but it’s like they changed when the playoffs started if anyone could get an answer to this thatd be great i love the nhl and all sports in general but it seems like nhl playoffs just about have no way of predicting the winner just a random note during the regular season for nhl teams the best team in the league in the last 30 years has only won the stanley cup 8 times
1,finding and optimising a three group classification criteria for a 0 1 response with one independent variable i want to create a grouping algorithm that groups my data into 3 classes now imagine that we have one independent variable x which can be either discrete or real and a dependent variable y that takes values in 0 1 in general the x and y are on average monotonically increasingly related higher x higher the probability of y and class y is imbalanced around 9 1 i have the following problems that i need to address as they are closely related 1 create a cutoff criteria for x so that it can be used to classify cases into the three groups i can transform x if needed as in logistic regression 2 compare the three groups and find out if there is a significant difference between them based only on the response y which can only take values 0 or 1 3 optimise the cutoff from point 1 so that the groups are optimally divided by the above criteria for the 1 point the most crude criteria that exists is just setting the cutoff based on percentiles of x the second idea i had was using logistic regression and then classifying based on the response in other words the probability i guess i could also use linear regression and classify based on that response does this seem ok or is there any other method that i should try for the group comparison i am currently using a chi squared test but the problem arises when trying to optimise the cutoff should i just vary the cutoff criteria values from point 1 optimising for the largest test statistic i have also thought about using anova but the data aren t normal and i would again be stuck optimising the test statistic is there something else i should minimise maximise minimise the within variance of groups maybe or should i use a completely different comparison the problem for me here is that the response is either 0 or 1 meaning the only things that i can compare are the ratios or variances within groups as a bonus problem group 3 should be the smallest comprised of a minimal percentage of cases say 1 so the group size will absolutely not be the same i know this might seem trivial or futile but it is what it is and would appreciate some advice on the procedure
0,how useful is knowledge of data structures and algorithms and how to learn them best disclaimer not a data scientist but a policy and urban affairs researcher consultant that uses ds da tools to do his job better thus honest question sort if it sounds stupid the job i applied to required only excel knowledge but right now i m using sql and python 100 self taught almost every day as my company is doing less traditional consultancy and more analytics stuff most of the times it s just data cleaning and wrangling and getting insights or designing the process when it involves geographic data so that the data engineering interns can do the proper etl process if i m doing something more advanced clustering distance matrices facility location i usually make do with out of the box solutions i keep reading about data structures and algorithms in this sub and how important they are however i m a bit mystified by the terminology and can t really see how it s useful i know spatial indexing often uses trees and is used to make searching and performing operations faster union intersection other than that i m a at a loss and i m a bit worried that whenever i need to do more advanced stuff or eventually interview for a more data oriented role i ll just make a fool of myself thanks so much
1,how to compare the success rate in different states hello let s say i have 5 states a b c d and e each state has 3 sub categories a1 a2 a3 and so on i performed some kind of tests and it looks like state a and b have a higher success rate than the other 3 states how can i validate that statistically thanks
2,pytorch widedeep deep learning for tabular data deep learning vs lightgbm a thorough comparison between deep learning algorithms for tabular data using pytorch widedeep and lightgbm for classification and regression problems v1 of pytorch widedeep coming soon
2,skim platform to help people skim through papers in this fast moving research world 🎉 feature update 🎉 we are extremely excited to announce a new feature skim now shows acceptance rates of more than 50 conferences 📊 take a sneak peak into the updated conference page 👇 also we are sending out 100 invites this week request an invite by filling this form
1,deciding how much data to use for a statistical model this is a concept i always struggled with in statistics is more data always better suppose you 50 years of data about hospital visits you are interested in supervised classification you have predictors such as age height weight blood type salary etc you are interested in predicting if the hospital stay will be less than 1 day or more than 1 day this can be easily solved using random forest my dilemma is using all 50 years of data might be able to capture a wide variety of patterns but since we are interested in predicting future information maybe some of the older data is less relevant and might surpress more current trends how do you deal with this problem i thought of multiplying older observations by 0 5 to reduce their influence but i am not sure if this is an acceptable approach does anyone have any suggestions thanks
0,no idea how to train using a custom coco format dataset first off i m really really new to practical data science i ve taken a course in ml before but this is my first time doing any dl and i don t even remember half of what i did in that course in any case i m trying to t rain a model on the pklot dataset obtained from the problem is i have no idea how to train the model on a custom dataset i wrote this code modified slightly from i uploaded it as a notepad trial1 txt x200b trial1 txt x200b x200b i also tried emphasis on tried to modify the actual coco py file thinking that would make it easier it s uploaded as trial2 txt x200b trial2 txt x200b any help is appreciated thank you i don t even know what i m looking at half the time haha note that both obviously didn t work
0,rant if your company s interview process can be practiced for it s probably not a very good one the data science interview process is something that we have seen evolve over the last 5 10 years taking on several shapes and hitting specific fads along the way back when ds got popular the process was a lot like every other interview process questions about your resume some questions about technical topics to make sure that you knew what a person in that role should know etc then came the well google asks people these weird seemingly nonsensical questions and it helps them understand how you think so that became the big trend how many ping pong balls can you fit into this room how many pizzas are sold in manhattan every day etc then came the behavioralists everything can be figured out by asking questions of the format tell me about a time when then came leetcode which is still alive then came the faang product interview which has now bred literal online courses in how to pass the product interview i hit the breaking point of frustration a week ago when i engaged with a recruiter at one of these companies and i was sent a link to several medium articles to prepare for the interview including one with a line so tone deaf not to be coming from the author of the article but to be coming from the recruiter that it left me speechless as i describe my own experience i can’t help thinking of a common misconception i often hear it’s not possible to gain the knowledge on product experimentation without real experience i firmly disagree i did not have any prior experience in product or a b testing but i believed that those skills could be gained by reading listening thinking and summarizing i ll stop here for a second beacause i know i m going to get flooded hate i agree you can 100 acquire enough knowledge about a topic to pass know enough to pass a screening however there is always a gap between knowing something on paper and in practice and in fact that is exactly the gap that you re trying to quantify during an interview process and this is the core of my issue with interview processes of this kind if the interview process is one that a person can prepare for then what you are evaluating people on isn t their ability to the job you re just evaluating them on their ability to prepare for your interview process and no matter how strong you think the interview process is as a proxy for that person s ability to do the actual job the more efficiently someone can prepare for the interview the weaker that proxy becomes to give an analogy i could probably get an average 12 year old to pass a calculus test without them ever actually understanding calculus if someone told me in advance what were the 20 most likely questions to be asked if i know the test is going to require taking the derivative of 10 functions and i knew what were the 20 most common functions i can probably get someone to get 6 out of 10 questions right and pass with a c it s actually one of the things that instructors in math courses always try and it s not easy to accomplish giving questions that are not foreign enough to completely trip up a student while simultaneously different enough to not be solvable through sheer memorization as others have mentioned in the past part of what is challenging about designing interview processes is controlling for the fact that most people are bad at interviewing the more scripted structured rigid the interview process is the easier it is to ensure that interviewers can execute the process correctly and unbiasedly the problem the trade off is that in doing so you are potentially developing a really bad process that is you may be sacrificing accuracy for precision is there a magical answer probably not the answer is probably to invest more time and resources in ensuring that interviewers can be equal parts unpredictable in the nature of their questions and predictable in how they execute and evaluate said questions but i think it is very much needed to start talking about how this process is likely broken and that the quality of hires that these companies are making is much more driven by their brand compensation and ability to attract high quality hires than it is by filtering out the best ones out of their candidate pool
1,panel data and variable domains hi in the context of panel data analysis time domains of variable widths are often a problem in social sciences or biostats traditional techniques such as resampling or modelling summary statistics have the disadvantages of information loss and potentially spurious results somewhat recently a paper by gellar et al for handling variable time domains in regression appeared and has been cited 31 times according to google scholar what is your opinion about that paper the technique is quite seductive imo and it appears worth of promoting more widely among biostatisticians
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
0,just failed an interview but i have a feeling that the interviewer is wrong so i had a technical take home challenge due to having to do machine learning on a laptop and having 100 million records i took a random sample of the data or more accurately only 1 because that s all my laptop can handle i proceeded to do eda train data and fit a few models that looked well fitting this is retail data and my interviewer immediately told me that my random sample approach is wrong he said that i should have taken a few stores at random and then used all their data as in full data for all the stores picked to train the models according to him you can t train the model unless you have every single data point for a store i think that he doesn t seem to understand the concept of random sampling i actually think both approaches are reasonable but that his claim of needing every single data point for a store or you are not getting the full picture is incorrect i failed the challenge due to this issue and that was literally the only thing that was wrong with my solution according to feedback i asked for to add data set contained 100000 stores in the same chain the goal was to fit a model that will predict total sales for those 100000 stores
0,is real time processing worth it for your analytical use cases real time technologies are powerful but add significant complexity to your data architecture find out how to reap the benefits of real time processing with the least architectural changes and maintenance effort
0,how to extend a text classification ml model to work with more than one language we are using in production an ml for text classification we trained our model using some custom english text corpus currently the model is working acceptable level of accuracy for our purpose now we want to extend it to handle french language as well we are planning to investigate the following two approaches 1 we have a french language corpus therefore we would like to train a new model for handling french text 2 use the same model trained with english corpus but use a third party language translation service such as google translator to translate french text to english before inputting it into the ml model so i would like to know your thoughts regarding these two approaches
2,new work linking partial differential equations and graph neural networks hi everyone sharing some work from the graph ml team at twitter showing how a new class of gnns can be constructed by discretising diffusion pdes was on arxiv today and it will be at icml21 thinking of gnns as partial differential equations leads to a new broad class of gnns that are able to address in a principled way some of the prominent issues of current graph ml models such as depth oversmoothing bottlenecks and graph rewiring many popular gnns can be formalised as discretised diffusion pdes with explicit single step euler scheme with a time step of 1 where an iteration corresponds to a convolutional or attentional layer of the graph neural network and running the diffusion for multiple iterations amounts to applying a gnn layer multiple times in the neural pdes formalism the diffusion time parameter acts as a continuous analogy of the layers—an interpretation allowing us to exploit more efficient and stable numerical schemes that use adaptive steps in time blog post paper code
1,what would you include in a basic data literacy program edit target audience is people whose role uses data but is not data centric hi everyone what concepts would you include if you were training a group of people on basics of using interpreting data i can think of the following could you please add to the list thanks for your time a averages when to use median b importance of removing outliers c what is standard deviation why is it important in interpreting an average d importance of segmenting and not looking at aggregates simpson s paradox e what is a dashboard f kpi vs metric g correlation not causation h cumulative totals why are they used how to interpret i moving averages why are they used how to interpret j how charts can be misinterpreted comparing charts with different scales for example k when to use a line chart when to use a bar chart
2,hypothesizing the ideal conditions for neural networks vs random forests is it possible to speculate what are the ideal conditions required for a neural network to perform well compared to a random forest for instance when dealing with image recognition tasks we know that convolution neural networks are generally favorable seeing as how the convolution operation is very effective at understanding images e g recognizing edges now suppose we look at a standard binary classification task suppose we have a smaller sized dataset e g 15 columns 5000 rows in general terms we know that a neural network works by approximating small regions of the target function with a collection of mini functions this is done by calculating a set of weights in the future data is passed through this network of weights and these weights are used to calculate the probability that a new observation belongs to a certain class is calculated in theory we could repeatedly pass similar points through the neural network and monitor how the probability of belonging to a certain class incrementally changes a random forest is quite different look at the decision tree for a second the decision tree works by randomly making binary partitions in the data a binary partition is made for a predictor variable such that this partition tries its best to cleanly separate the classes of the response variable when a suitable partition is made for the first predictor variable we move on to the second variable then the third variable etc so in the end if we imagine our data as a big box we create these mini boxes i e terminal nodes within the big box each of these mini boxes has an address i e the different partitions e g if var1 5 and var2 10 then mini box 1 each of these mini boxes is associated with a response label the random forest improves the decision tree by bootstrap aggregation thousands of randomized and smaller decision trees are combined together for improved predictive power and less overfitting all the trees in the forest are used to collectively decide which mini box a new observation should be placed in my question based on this very general understanding on how both these algorithms work can we try to hypothesize what kind of datasets are more suited for neural networks vs random forests for example in the random forest algorithm by using the gini index criteria it is relatively straightforward to make mini boxes for categorial predictor variables however a neural network would have to one hot encode these categorical predictor variables and as a result deal with more variables curse of dimensionality and as well these one hot encoded variables are likely to contain a greater level of sparsity furthermore it might be easier to make general mini boxes in sparse data compared to using gradient descent with missing values i know this is all speculation and the no free lunch theorems say that no machine learning algorithm is universally best but could we try to speculate and say that certain machine learning algorithms might be better suited for certain types of datasets just as convolution neural networks are better for image recognition and lstm networks are better at handling sequential data could we argue that bagging and boosting algorithms e g random forest gradient boosting might have an easier time at handling smaller datasets with mixed categorical continuous variables i would be interested in hearing some opinions and thoughts on this thanks
1,regression models vs random forest i tried fitting a glm style regression model to some data and it resulted in all the regression coefficients being estimated as 0 i e model failed yet when i tried a random forest model on the same data the model worked well and i was even able to get 70 accuracy on the test set my dataset has continuous and categorical variables as well as a lot of naturally occurring zeros i was just wondering is this a common problem i spent a whole day trying to tweak the regression model to work but the random forest instantly outperformed it thanks
2,alzheimer s detection and prediction of conversions from mild cognitive impairment using deep learning hi guys thought you may find this interesting this was my university dissertation and i thought someone from here may want to build on this for their university project or just check it out it was coded in pytorch and it is a multi modal model which takes in both mri images and clinical information what the model does is predict conversions from mild cognitive impairment to alzheimer s diseases within 3 years for a given subject
1,education cmu professor me explains confidence intervals in an intuitive way no formulas
1,which statistical test should i run hi all i m working with an awkward set of data and i d appreciate some advice the study between subjects design uses a task that measures mean reaction times and scores of correct incorrect responses to number prompts there is one independent variable and the two dependent variables of reaction time score there were 16 usable sets of data split into 3 groups corresponding to the 3 categorical levels of the independent variable one group has 9 sets of data the second has 5 and the third has 2 for each of the data sets reaction time is measured in milliseconds and correct incorrect responses are measured in whole numbers i originally planned to use manova but the sample is smaller than anticipated and the 3 groups are extremely unequal i m not sure how to proceed with this analysis without running into issues with statistical power and error would it be better to conduct t tests between the three groups 9 with 5 5 with 2 and 9 with 2 or a multivariate regression instead i m lost
1,is there a forum or website where i can post job offerings for stats e g i have a very straightforward stats job offering basic chi squares only with no corrections that would take a max of 5 6 hours to do but as i m always at work i would prefer to pay someone to finish things up i wouldn t mind stats grad students is there somewhere appropriate where i can post an offer like this thank you all so much
0,euro 2020 predictions i remember andrew gelman had a nice little stan model for the world cup since the euro starts tomorrow i thought i would try my hand at forecasting some of the games i m not a sports data scientist i m more of a biostats guy but i do like bayes and i do like international football so i thought what the hell to keep myself honest i d like to post a few predictions here to keep in the spirit of the sub i am willing to discuss data models critiques of either with you all here are my predictions for the group stage since its on github you can see if i cheat and push predictions after the fact i plan to make predictions for each major stage and then update the model after that so the group stage will pass i ll calculate my brier score and cross entropy loss i ll refit the model and go from there but the real point of the post is to let you all know you re free to piggy back off my efforts clone the repo and try your hand i d love to learn a thing or two about these sorts of modelling problems from the community
1,which family function to use for my glmer hello i have plotted out my response variable and i m not sure whether i should use poisson negative binomial or a gamma function to model this response variable it is a discrete count variable i actually filtered the response variable when it was equal to 0 because i was interested in only those who have received one or more promotions otherwise the response looks like this and maintains the same pattern i will be using the glmer package because there are random effects in my model
2,cvpr 2021 progressive self label correction proselflc for training robust deep neural networks keywords entropy minimisation maximum entropy confidence penalty self knowledge distillation label correction label noise semi supervised learning output regularisation x200b proselflc is the first method to trust self knowledge progressively and adaptively proselflc redirects and promotes entropy minimisation which is in marked contrast to recent practices of confidence penalty 42 33 6
0,data science in a non tech company vs data science in a tech company hey all i am currently working as a data scientist in europe at a non tech company a global brand in fast moving consumer goods fmcg for a year and currently got an open offer with a tech company which would bring an increase of 17 in total compensation that is quite popular in my country in the future i would like to work for one of the big tech companies or start my own company with this information i am wondering staying at my current employer is the best thing to do or moving towards the tech company is the best thing to do the fmcg company overall has less technically skilled people less challenging problems from a ds perspective but a lot of emphasis on delivering business value which is a plus and is only at the beginning of transforming into a data driven business while i prefer to maximize the value we deliver with our solutions i am afraid that i ll progress less on my technical skills on the other hand at the tech company i d be less business facing and have more advanced use cases that also involve more engineering have you guys faced similar thoughts or a similar situation what are your thoughts
1,are these three models for simple linear regression the same in casella and berger s statistical inference 11 9 2 best linear unbiased estimators a statistical solution gives a model at the beginning of the section on p544 545 11 3 13 y i alpha beta x i eps i i 1 n where eps 1 eps n are uncorrelated random variables with 11 3 14 e eps i 0 and var eps i sigma 2 and then show the least square estimators are the blues under the model 1 at the end of the same section on p548 the fact that least squares es­timators are blues holds in other linear models also this general result is called the gauss markov theorem see christensen 1996 lehmann and casella 1998 section 3 4 or the more general treatment in harville 1981 isn t the model at the beginning of the section the model for gauss markov theorem if not what is the model for the gauss markov theorem or what is the difference between the two models 2 is the model at the beginning of 11 3 3 the same as the model 11 3 1 and more given at the beginning of 11 3 simple linear regression on p539 thanks
0,best libraries for distribution based modeling hey folks looking for some good repos to read over documentation for purposes of distribution based modeling for example which repo allows me to use poisson for queue building etc i haven’t used distributions for modeling so new to this any repos that also profile data frame and provide distribution recommendations thanks
2,ai generated music video what happens when openai s clip meets biggan i used openai s clip model and biggan to create a music video that goes along with the lyrics of a song that i wrote the song lyrics are made from imagenet class labels and the song itself is performed by me on a looper x200b outline 0 00 intro 1 00 ai generated music video for be my weasel 3 50 how it was made 7 30 my looping gear 9 35 ai generated music video 2 12 45 outro credits x200b code and references
0,is upward of 1 hour in runtime a normal occurence when querying data from large tables using pyspark i m currently doing a sql query select from where using spark sql and then writing the results into a table using df saveastable however this take upwards of an hour or more to run the table i am querying from has 1 billion rows of data and it is in hive is this to be expected or is there something inherently wrong with my configurations of spark that lead to the slow runtime
1,can you do a phd part time i wanna start working after my bachelors and pursue a master part time which i know is very common however i really wanna take it a step further get a phd too is it possible to do so while still working perhaps i could do it part time and then switch to full time for the final year also i heard most phd programs don’t allow you to work at all is that just conditional to when you’re getting financial aid from them i would appreciate any help in case its relevant i would wanna pursue statistics or data science thanks
1,let x normal 0 4 is 4 the standard deviation the variance or god forbid the precision likewise we summarize random variable y as 3±0 4 does this interval represent the mean ± 1 standard deviation a standard error of the mean some z confidence interval nobody can say certainly not the text edit and by text i mean as it exists in some applied context not in a textbook or methods paper what other cases of notational ambiguity do you often see how often do you reckon the above described cases map to each of the options presented or perhaps to other options
2,h5records store large datasets in one single files with index access recently i tried using tf record in pytorch however a lot of heavily used features by pytorch such as data length index level access isn t available although the storage and performance is great i really wanted something like tf record with build in compression support and the ability to scale well for pytorch some projects do use hdf5 as an alternative to tfrecords biggan pytorch however defining the data type isn t particularly user friendly so i written this abstraction layer to ease the use of hdf5 as a dataset for pytorch so far i have tested it on datasets of size around 200g without any major issue i hope this would be useful for the community when dealing with extreme large datasets github link edit repost due to inappropriate tag
2,online machine learning or how to automatically update your model in production i m trying to find resources to learn more about online machine learning the idea is beautiful and powerful your model in production trains itself with new latest data to react to changes faster however the classic ways to build the mlops infrastructure and algorithms maths won t do the job here so i m eager to learn more i ve found this post by standford s ml lecturer chip huyen to be a great introduction to the concept of online learning i ve found river to be a promising python library for online learning apart from that i don t know many resources out there could anyone help i m especially interested in finding a titanic equivalent a simple problem to get going the way i see it i would need a problem from where both historic and real time data can get imported i would train my beta model with historic data and then i d use real time data to both track the performance and keep evolving the models so the classic benchmark databases won t do the trick here there s historic data of the titanic but luckily not real time feed of it i was thinking in 1 trading both historic and real time data available 2 bike sharing both historic and real time data available through apis for certain cities any further ideas
1,casella berger in pdf does anyone has casella and berger statistic inference as pdf without being images i would like to use the search tool since it s a 600 page book i didn t read anything about asking for book in rules so i hope this doesn t goes against them
0,how do you deal with data pipeline engineering sales people on linkedin title says it all i accidentally accepted too many friend requests without thinking now i m getting bombarded with do you have a moment to chat about our awesome etl product messages do i just ignore them unfriend them or politely decline
0,anyone here use confluence or something like it to document their projects how is your space setup configured
0,how do you feel when you have nothing to do at work what do you do when you have finished most all your tasks for the day i have been wondering how much actual work data analysts do per day on their 9 5 job i know that some days will be very busy but do you frequently have calm workdays how should do you feel about it when you re supposedly working but have nothing to do is that a bad thing or is it normal to most companies average company not a faang or something
0,recruiters reaching out i’m a data scientists at a very large corporation and after hitting three years at the company i’ve noticed i get at least 5 emails linkedin messages from recruiters about different ds positions available normally i ignore them but lately recruiters from the companies themselves have been reaching out rather than contracts or recruiting companies and i am tempted to respond has anyone else worked with recruiters for data science positions and if so what was your experience i’m happy at my job but i’m getting so curious about these thanks for any insight
0,hostile members of an interview panel how to handle it i had this happen twice during my 2 months of a job search i am not sure if i am the problem and how to deal with it this is usually into multi stage interview process when i have to present a technical solution or a case study it s a week long take home task that i spend easily 20 30 hours on of my free time because i don t like submitting low quality work i could finish it in 10 hours if i really did the bare minimum so after all this i have to present it to a panel usually on my first or second slide basically that just describes my background someone cuts in first time it happened a most senior guy cut in and said that he doesn t think some of my research interests are exactly relevant to this role i tried nicely to give him few examples of situations that they would be relevant in and he said yeah sure but they are not relevant in other situations i mean it s on my cv why even let me invest all the time in a presentation if it s a problem so from that point on the same person interrupts every slide and derails the whole talk with irrelevant points instead of presenting what i worked so hard on i end up feeling like i was under attack the entire time and don t even get to 1 3 of the presentation other panel members are usually silent and some ask couple of normal questions second time it happened today i was presenting kaggle type model fitting exercise on my third slide a panel member interrupts and asks me so how many of item x does out store sell per day on average i said i don t know off the top of my head he presses further but how many guess i said umm 15 he does that s not even close see someone with retail data science experience would know that again it s on my cv that i don t have retail experience so why bother the whole tone is snippy and hostile and it also takes over the presentation without me even getting to present technical work i did i was in tears after the interviews ended i held it together during an interview i come from a related field that never had this type of interview process i am now hesitant to actually even apply to any more data science jobs i don t know if i can spend 20 30 hours on a take home task again it s absolutely draining why do interviewers do that also how to best respond in another situation i would say hold your questions until the end of the presentation here i also said that my preference is to answer questions after but the panel ignored it i am not sure what to do i feel like disconnecting from zoom when it starts going that way as i already know i am not getting the offer
2,introducing hyperlib simple deep learning in hyperbolic space project we have just released a new open source python library that makes it easy to create the next generation of neural networks in the hyperbolic space as opposed to euclidean we re calling it hyperlib the hyperbolic space is different from the euclidean space it has more capacity which means it can fit a wider range of data hyperbolic geometry is particularly suited to data that has an underlying hierarchical structure there’s also a growing amount of research documenting the benefits of modelling the brain using hyperbolic over euclidean geometry we found that existing hyperbolic implementations were less ready to be applied to real world problems hyperlib solves that abstracting away all of the complicated maths and making hyperbolic networks as easy as a pip install we hope it will inspire more research into the real world benefits of non euclidean deep learning you can install hyperlib using pip install hyperlib
1,why is mean considered better average than median why is it the case that we consider mean better average than median when we are aware of the fact that mean is affected by outliers and also it s not something we can locate by inspection on a graph
0,grocery store purchase records i m interested in a side project that looks at grocery store shopping habits and health outcomes for the shopper history i was hoping for loyalty number based transaction data that would give me a good picture on how healthy their trips look i m not as interested in price as much as types of food i e fruits vs candy haven t come across how to buy these data but this tweet thread made me think a data aggregator may have it has anyone purchased grocery shopper transaction data before
1,question about probability sorry if this is the wrong place to ask hi i would like to ask this question if i have 15 probability of something happening and it takes 200 tries to happen what s the probability of the event sorry for the bad wording i m not great at math
1,given the current longevity data on people’s life span could statisticians be able to accurately predict the year that we’ll reach the longest age of 150 human years on earth at what year will the first human ever reach 150 years of age given they still remain on earth
0,is the subscription for towards data science medium blog worth it i must say from the start that i’m a novice so advanced articles definitely won’t help me as far as i know there are many articles written in a tutorial style some of them suitable for beginners and i would be interested in them however i’ve heard that medium is not a reliable source of information and the quality of articles might be poor over there i can’t judge by myself whether this is true or not because of my lack of experience what do you think is it worth it
2,is there any way to use multiple google colab accounts gpu for distributed training i wanna train the roberta language model for nepali language from scratch but neither i have money to get gcp with powerful gpus nor my college has a powerful computer therefore i was thinking is there any way to get collective power of multiple google colab machines any help or suggestion is appreciated
2,using machine learning to decipher the regulatory code of gene expression a review excited to share that our review is out for those interested to learn more about the regulatory code of gene expression and how its modelled we i review the latest developments that apply classical ml or deep learning to quantify molecular phenotypes and decode the cis regulatory grammar ii list published studies and results in tables iii build from the ground up first focusing on the initiating protein dna interactions then specific coding and non coding regions and finally on advances that combine multiple parts of the gene and mrna regulatory structures achieving unprecedented performance and iv provide a quantitative view of gene expression regulation from nucleotide sequence by putting a number on the main processes in the central dogma of molecular biology
0,do you tend to over think data science projects and in your experience is this characteristic valued in data science roles when faced with a data science task i often find that i naturally start critiquing the data i sometimes list a couple hundred questions why are the multi time series not equal in length was the data unavailable why are some values missing are they missing at random across a particular class how can we fix this in the future why are there zero values what does zero mean in this context is zero real or a default database value for not available why are there so many duplicate rows in the data are they duplicates or are they real values if no id field is available then two independent rows may have the same values etc i list assumption after assumption if we assume this we can do this etc etc i ve found mix responses when working with people so people love my pragmatic and thought provoking approach to solving problems i often end up uncovering something that nobody even thought to ask others would rather i just glossed over it all and not bothered to ask such questions what is your experience with this i feel like a lot of the time people don t want you to overthink data science i m curious to know what others have experienced
1,question about testing i have three distributions of variables that are more or less normal these variables measure whether i think someone is cheating at a video game for var1 and var2 the lower the value the more likely i think you are to be a cheater for var3 the higher the value the more likely i think you are to be a cheater no measure alone is evidence but if you do really poorly on either both var1 and var2 or on all three metrics i think that person s probably a cheater i want some sort of statistical test using the three of them to identify the cheaters with some degree of confidence like if i can say 5 of the population is cheating with 95 confidence or these 10 players we are 99 sure are cheaters that would be amazing does anyone have a good idea about how to approach this is there a way to say like “this person is 2 standard deviations below on all three metrics and there’s only a 5 chance that’s a statistical coincidence” or something like that if this is relevant i have var1 and var3 data for all players but only var2 data for about 1 3 or 1 2 of the players
1,relationship between topological data analysis and dimensionality reduction methods like principle component analysis manifold learning and data visualization tsne recently i came across this topic of topological data analysis has anyone ever used this methods from topological data analysis is this more for data visualization or dimensionality reduction suppose if i am working on a supervised binary classification problem how can topological data analysis be used in the context of this problem
1,how do you explain someone who does not know statistics that sometimes you need to remove the outliers after completing data analysis courses i was wandering how could i explain someone sometimes we need to remove the outliers inorder to get better result with simple to understand examples
0,what are your thoughts on quantile random forest hello i am doing a project at work to predict on time delivery percentage in a manufacturing process i recently discovered quantile random forest and i like the idea of it i am thinking of using quantile 0 5 as a point estimator and 0 1 and 0 9 quantile as prediction interval so far the results have been good but since i m new to the real world project setting and new to quantile random forest i was wondering is there something i should keep in mind while using this algorithm i read an article at medium where they showed a use case of qrf at instacart to predict on time delivery percentage but i was thinking why this algorithm is not so popular maybe i just don t know about it what have your personal experience been using qrf thank you
1,pluralsight statistics course recommendations for machine learning projects i m beginner who s signed up in plural sight and i m keen to learn r enough statistics to work on data analysis projects in kaggle etc i ve used python before in university looking forward to your replies thanks in advance
2,confidence intervals for classification models the idea of creating confidence intervals in regression models is quite straightforward for example but do confidence intervals carry over to classification models 1 for example in this picture here confidence interval for an roc curve corresponding to a classification model can the lower limit of the confidence interval from this roc curve be used to gauge how well this model might generalize to new data 2 for a classification model can we make a confidence interval for the prediction of an individual observation for instance i wrote some r code you can directly copy paste the code for a particular example where a random forest i e classification model is used to predict whether if an observation will be approved or rejected see here for the code thus for each observation the classification model predicts the probability that this observation will be approved or rejected whichever probability is higher the model selects that outcome for the given observation also the higher one of these probabilities are this means the classification model is more confident with its prediction e g for two observations the ratio of approved rejected 0 9 0 1 vs 0 6 0 4 the model is more confident about the first prediction thus is there anyway to apply the notion of confidence intervals to the probabilities for individual predictions thanks
2,gary marcus and luis lamb discussion of agi and neurosymbolic methods pod professor gary marcus is a scientist best selling author and entrepreneur he is founder and ceo of robust ai and was founder and ceo of geometric intelligence a machine learning company acquired by uber in 2016 gary said in his recent next decade paper that — without us or other creatures like us the world would continue to exist but it would not be described distilled or understood human lives are filled with abstraction and causal description this is so powerful francois chollet the other week said that intelligence is literally sensitivity to abstract analogies and that is all there is to it it s almost as if one of the most important features of intelligence is to be able to abstract knowledge this drives the generalisation which will allow you to mine previous experience to make sense of many future novel situations also joining us today is professor luis lamb — secretary of innovation for science and technology of the state of rio grande do sul brazil his research interests are machine learning and reasoning neuro symbolic computing logic in computation and artificial intelligence cognitive and neural computation and also ai ethics and social computing luis released his new paper neurosymbolic ai the third wave at the end of last year it beautifully articulated the key ingredients needed in the next generation of ai systems integrating type 1 and type 2 approaches to ai and it summarises all the of the achievements of the last 20 years of research we cover a lot of ground in today s show explaining the limitations of deep learning rich sutton s the bitter lesson and reward is enough and the semantic foundation which is required for us to build robust ai
1,books references for learning about newer machine learning models a lot of books that are currently available do not contain a lot of material on newer models like transformer models e g in the context of time series can someone recommend some book source that introduces transformers thanks
0,do pandas and other python data science libraries change a lot i took a data science with python course on udemy that was created in 2017 i’m wondering how much do pandas and other classic ds libraries change matplotlib scikitlearn seaborn etc should i take a refresher course
2,how does anyone get any work done on cloud vertex ai etc looking at the vertex ai stuff has my back up again sorry not sorry to all the mlops workers but for years i ve never understood how fast iteration happens on cloud what with waiting for machines to spin up latency accessing s3 buckets docker craziness and the general from 10 000 feet view of code and data it s like trying to perform surgery wearing mittens really how does one get any work done if you can t even grep a logfile
1,least absolute deviations to a power and robust regression i ve been thinking a little bit about least squares and how one method of getting a robust regression alternative is to use least absolute deviations one of the things that seems to be potentially problematic is that there could be multiple solutions with the example on wikipedia given as such linked from would a fairly simple solution to this be to use a near 1 power that is instead of minimizing absolute residuals you could instead minimize absolute residuals 1 01 or 1 1 or 1 0001 or some other value less than 2 when you get to 2 of course you re at least squares and no longer reducing the effect of outliers so i m thinking of values between 1 and 2 but most likely near 1 is this common and i m just not aware of what this is called would this be a reasonable approach to robust regression while getting unique solutions vs the lad approach
0,weekly entering transitioning thread 20 jun 2021 27 jun 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
1,i have a dataset of some groups which answered questions what statistic should i use thank you for reading i have a dataset where some people answered 10 questions with a value from 1 to 6 integer depending on the agreement of the statement each person has 4 categories gender age range profession and years of experience i want to check for each question if there are substantial differences between each group the thing is that there are many combinations possible since each person belongs to 4 categories what statistical test should i use i have done t tests between each group of each category pair by pair is this correct should i perform other kind of test like anova x200b thank you so much
2,unreal engine trick with vqgan clip when you generate images with vqgan clip the image quality dramatically improves if you add unreal engine to your prompt more context on this twitter thread this is so cool and funny the network as a byproduct learned upsampling based on language conditioning lol i may be wrong about my stated implication but the results i am seeing are quite fascinating
1,can you calculate effects size from prevalence when doing a meta analysis i m new to using stata16 trying to run a meta analysis comparing prevalence data from 21 observational studies is this possible
0,question mcmc chain not sure if this is the correct place to ask but i am doing a bayesian analysis using mcmc i have 100 000 samples and i am planning to use iat integrated autocorrelation time to determine my thinning factor yet i don t know if i should calculate my iat before or after my burn in does anyone with experience could tell me thank you
0,confused on different job titles roles of data science after reading through bunch of articles about the different roles i am still confused on specific jobs within the data science community i really want to focus on the coding model creation model deployment and monitoring maybe where to ml models are already predefined but my job would be putting it into production and hooking everything up what would this role be called mlops ml engineering dev ops the sense that i get is no one seems to know what to call these guys or they are sort of interchangeable titles that might mean something totally different to each company you apply for also would this include making data pipelines or is that a data engineer s job
1,correct application of survival analysis i have been struggling to understand this concept for some time can you use create a survival analysis model for old patients and then use this model for prioritization and decision making for new patients imagine this example you have a historical dataset that shows patients coming into an emergency room you have covariates associated with each patient such as age gender etc and the time at which they left the emergency room call this the event or the time at which they passed away call this censored suppose you build a survival model for these patients and you want to use this survival model to triage new patients so you can decide who to treat first this model can tell you the probability of surviving past a certain point and the rate at which an instantaneous hazard can occur for each new patient based on the covariates of a new patient and the estimated hazard and survival function of each patient i want to try and use this information for triage i know that you could probably use a standard supervised classification model or regression model for this problem but classification regression models can only provide a point estimate i want to do an analysis that shows how risks evolve with time for each new patient this is an example i made up it might not be very realistic but i am trying to illustrate an example where survival models can be used for triage and decision making in survival analysis the cox proportional hazards regression model is the most common model but i want to use a newer approach called survival random forest like a standard random forest the survival random forest is made up of randomized boostrap aggregated survival decision trees each survival tree passes observations through a tree structure and places them in a terminal node a kaplan meier curve is made for all observations in the same terminal node then the survival random forest performs an ensemble voting using all trees and produces an individual survival function for each observation see here for more details the advantage of the survival random forest is to combat the common problems associated with non linearity and complex patterns in bigger datasets traditional cox proportional hazards regression models would require the analyst to manually consider different potential interaction terms between covariates these can be potentially infinite the survival random forest uses bagging theory developed by leo breiman to overcome this problem going back to my initial example for using survival analysis for triaging i tried to illustrate this example using r code adapted from here in this example i train a survival model survival random forest on a training dataset the lung dataset that comes with the survival library in r and then use this model to generate the individual survival curves for 3 new patients this can be seen here based on this analysis after generating confidence intervals for each survival curve can we say that the patient associated with the red curve is expected to survive the longest therefore we should first begin to treat the patients associated with the blue curve and the green curve the formatting on reddit was giving me a hard time so i attached my r code over here can someone please let me know if this general idea makes sense thanks
0,how long do non competes last i am a partner in a data science company as well going to work in another company both in data science non compete says it s lifetime is this viable
2,behavioral trees on ai agents for soccer other sports hey there i have been looking into some ai algorithms and i ve come across behavioral trees bt and i was captivated i wanted to see some examples of bt being used in ai in games and such but all i found was ai for an enemy player or some sort of pve game i want to see how would a bt look like for a simple ai agent in a soccer game or any others sports game in general would each agent have their own bt or would there be a central control system with a bt that tells each ai agent to what to do like form formations etc in my search i unfortunately have not seen or came across any good examples and i wonder if there is any i d be more than happy if you could provide an example or tell me where to look at or at least provide some sort of insight to what may it look like if there were a bt driven ai sports game i am curious yet it sucks that all i got from my research are dead ends and prediction algorithms for betting
1,interpreting ols coefficients to a percentage chance hey all i m gonna illustrate my question with an example of my output of an ols regression model independent variable coeff 0 02 constant term 0 05 is it safe to assume this implies a relative decrease of 40 compared to the model s baseline probability
1,homework question regarding ols regression hello all i m taking an advanced stats class for this graduate program that i m in and me and some classmates are stuck on the current homework assignment we are using spss the homework requires us to run ols regression on some variables and then take a quiz using the results that we get we followed the directions clean variables transform certain variables into logged versions of themselves and others into squared versions but still can t get the correct results actually the results we keep getting are a little outrageous we are getting coefficients in 5 digit numbers and the answers are only 3 digits we ve started over and followed the instructions several times but still can t get any results even close to the correct answers the worst part is that the class is asynchronous and the professor doesn t have constant office hours so we can t even ask him for help most of the time i know that this is a difficult question for you guys to answer because you d have to see every step we took before running the regression model so if you need any additional information please dont hesitate to message me i d be happy to provide screenshots and stuff please and thank you
2,is anyone aware with the working of mmdetection framework for object detection how exactly do i use my epoch pth files for inference
0,opinion on choice of model recommender system hey guys how are y all doing hope you are well and safe i ve been doing a small project on recommendation systems for board games i just started the project this week so i am pretty lost of course i want it to be personalized item user interaction instead of making a recommendation based on stock of the store and initially i m trying to go with collaborative filtering maybe in the future i will try doing a hybrid system x200b i ve tried some really simple memory based methods using pearson and cos similarity then i tried svd but i got an rmse of 1 5 the rating is ranging from 1 10 that doesn t look awesome and i don t even fully believe that the model is running right explanation for that is i ve engineered the data to be like the model asks for it user item rating int int float and then when i try to predict the rating for a random game with svd predict user id game id i could use any number at all even if the user game is not in the dataset which i think is weird x200b then i tried to find some more advanced models and i found this really good list and in there i found the microsoft one so it s where we are now which a bunch of different models and not a documentation tutorials out there x200b does anyone know where should i start what are the possible things i am missing thanks a lot
2,can a cycle gan work with data imbalance i wanted to know if anyone has tried training a cyclegan where target distribution has 10 times the number of samples than source distribution does it work in practice
1,contrast estimator for t student is calculated with standard deviation s or with standard quasi deviation ŝ i m doing some hypothesis contrast exercises where i have the value of the sample s standard deviation but not the one of the population so i ve read that in these cases i have to do the hypothesis contrast following a t student distribution to calculate the t value for the contrast i don t know if i have to use the standard deviation s or the standard quasi variation ŝ which is s √ n n 1 so the way to calculate t is t m μ s √n where m sample s mean μ population s mean s standard deviation n sample s size or t m μ ŝ √n where m sample s mean μ population s mean ŝ standard quasi deviation n sample s size any help is greatly appreciated because i have the global exam next week and i m so lost
2,what is the most relevant sota in deep nn interpretability hello i am currently working on a project in which i would like to find which parts of an input are most important for the forward propagation in a quite complex nn regressor i know that grad cam is quite popular in cv but it is also a bit limited since it is mainly relevant for images i found only people using it for classification although i don t see a reason why it wouldn t work for regression and you have to select a convolutional feature map in particular for it to work as far as i understand so my question is what is the current sota alternatives to grad cam aiming at doing similar things but for more general settings e g mlp regression please thanks
1,do you report simple regression equations i m currently finalizing a paper that i plan to publish in a peer reviewed journal before i reach out to my supervisor for such a small thing i thought i d reach out to the community first in my paper i run a pretty standard fixed effects regression with only few variables this is part of the results but the regression results are only a small part of the work from the perspective of a reviewer would you complain if i don t report the regression equation i ve seen examples in journals like review of financial studies etc both with and without reporting the equations and was just wondering if i m overthinking this or if that s a common complaint if not handled correctly thanks in advance
1,sample dataset resources hey guys not sure if this is the right place to ask but i was wondering where i could find sample right censored datasets currently doing research on survival analysis and looking to test some code out cheers
1,life table vs survival analysis i was looking at this article on how actuaries use life tables is this not closely related to the kaplan meier method from survival analysis
2,generating discrete encodings for music i posted about this a few weeks ago but since then i have made some progress and thought i might share and get some feedback the ultimate goal of my project is to generate discrete encodings that compress sound music in the time domain using these discrete encodings i would like to make another model to generate new music in the same way an rnn can be use to generate text since these encodings are compressed in time the rnn can learn music longer sequences than if i used raw audio the training data i am using is about 30 hours of random music mashups i downloaded from youtube i resampled it to 22050 hz and converted it to mono channel the input for the network is the raw audio quantized to 256 values via mu law encoding the network consists of three components encoder the job of the encoder is to compress the audio down 64x i would like to go larger but so far this is the most i can do i start by running the raw waveform through two strided convolutions to compress the time axis 64x and then i run it through a highway like network of dilated convolutions i say highway like since i did change gate convolution to look at the value it is gating instead of the inputs def create encoder input audio keras input none dtype int32 x layers embedding 256 16 input audio x layers conv1d 64 8 strides 8 use bias false x x layers batchnormalization x x layers leakyrelu 0 01 x x layers conv1d 256 8 strides 8 use bias false x x layers batchnormalization x x layers leakyrelu 0 01 x x layers conv1d 512 3 padding same use bias false x x layers batchnormalization x x layers leakyrelu 0 01 x for d in 1 2 4 8 1 2 4 8 f layers conv1d 512 3 padding same dilation rate d use bias false x f layers batchnormalization f f layers leakyrelu 0 01 f g layers conv1d 512 1 activation sigmoid bias initializer keras initializers constant 2 f x g f 1 g x x layers conv1d 512 3 padding same use bias false x x layers batchnormalization x x layers leakyrelu 0 01 x x layers conv1d 8 1 x return keras model inputs input audio outputs x name encoder the output of this encoder is a vector of length 8 for each 64 chunk of input audio however i wanted one discrete integer value per 64 samples not 8 this currently would only compress it 8x with a continuous encoding what i need is for each of those 8 floating number to represent bits in an 8 bit integer i actually used 1 and 1 instead of 1 and 0 to do this i use the sign function as the activation this function is normally not differentiable however i have found that i can apply the sign function on the forward pass but use the derivative of tanh on the backwards pass tf custom gradient def sign with gradients x def grad dy return dy 1 tf square tf tanh x return tf where x 0 0 1 0 1 0 grad i apply this activation in my custom training function between the encoder and the expander layer i also add some activity regularization to the outputs of the encoder before applying the sign function not sure if this is necessary but i wanted to keep the value from getting too far from zero e encoder r training training reg loss 0 0001 tf reduce mean tf square e e model sign with gradients e e expander e training training expander the purpose of the expander is to take the discrete encodings and expand it back out 64x to the size of the original audio the output of the expander is then used to condition the decoder to recreate the sound the architecture of the expander mostly mirrors that of the encoder i have debated whether it makes sense to make the expander a larger model than the encoder but most auto encoder architectures i have seen seem to be symmetric def create expander input data keras input none 8 x layers conv1d 512 3 padding same use bias false input data x layers batchnormalization x x layers leakyrelu 0 01 x for d in 1 2 4 8 1 2 4 8 f layers conv1d 512 3 padding same dilation rate d use bias false x f layers batchnormalization f f layers leakyrelu 0 01 f g layers conv1d 512 1 activation sigmoid bias initializer keras initializers constant 2 f x g f 1 g x x layers conv1d 512 3 padding same use bias false x x layers batchnormalization x x layers leakyrelu 0 01 x x upsample x 64 return keras model inputs input data outputs x name expander decoder the decoder takes the output of the expander and the prior audio sample of the encoded sequence as inputs and tries to predict the next sample currently i am using an rnn architecture but may try something like a wavenet i kept the decoder simple since i want it to use the input from the expander more than what it has learned from the prior sequence currently i am having an issue with it inserting other sounds into the output on top of the original music need to find a way to remove these artifacts i have found smaller models produce less of this def create rnn decoder stateful false batch size none prior audio input keras input none batch size batch size dtype int32 expander input keras input none 512 batch size batch size x layers embedding 256 16 prior audio input x cat x expander input x layers gru 512 stateful stateful return sequences true x x layers conv1d 512 1 use bias false x x layers batchnormalization x x layers leakyrelu 0 01 x x layers conv1d 256 1 x return keras model inputs prior audio input expander input outputs x name decoder i have been training this on sequences of length 512 which are randomly sampled from the output of the expander which is currently 2 14 in length i need to do this since i don t have the time or memory to train on the full length outputs currently i am using sequences of length 2 14 input to the encoder which compresses down to just 256 when run through the dilated convolutions if i made this much small enough that i don t need to sample the dilated convolutions would be encountering padding on both sides in all cases which would not generalize well to when i use it on longer sequences i trained the network overnight and it seems to still be improving on the out of sample data so i will leave it running for now i did generate an output sample you can listen to the underlying song can be clearly heard i just need to figure out how to get rid of some of the annoying artifacts if you have any suggestions please let me know previously i was using a gumbel softmax in a similar way to to how i used the hard value on the forwards pass but a soft value on the backwards pass i found this to be less than ideal since i prefer my encoder to be deterministic when i tried not applying gumbel noise the encoder would only ever activate a small portion of the nodes in the softmax in generating the audio in the link i checked the values it was using it used 237 256 possible combinations of bits each of the 8 bits also got activated close enough to 50 50 that i am happy with it so please let me know what you think and if you have any suggestions x200b update the validation data stopped improving around iteration 240 000 this is the final output of the above clip
1,can someone please explain what the white color shades mean in this picture these pictures are supposed to show the decision boundaries of different machine learning algorithms on a binary classification task there are two classes for the response variable red and blue shouldn t all the decision boundaries either be fully red or fully blue what do the shades of white mean does this mean an overlapping decision boundary thanks
2,facebook ai mila propose alma anytime learning at macroscale a research team from facebook ai research and mila mcgill university explores deep learning model accuracy versus time trade offs in anytime learning which they term anytime learning at macroscale alma the team evaluates various models to gain insights on how to strike different trade offs between accuracy and time to obtain a good learner here is a quick read facebook ai mila propose alma anytime learning at macroscale the paper on anytime learning at macroscale is on arxiv
1,is there any reason to use phi over proportion tests hello all i am currently learning about different tests of association and ran across phi for testing two nominal dichotomous variables i was curious however in what sense this might be applicable given that a proportion test seems more readily interpretable when it comes to reporting about two dichotomous variables e g testing the proportion of males who smoke to women who smoke and that outcomes in the statistical significance do not differ between these two tests when you use chi square to test for significance with phi when would one favor using phi in a practical context
1,can a stratified random sample be paid to take part in studies specifically im talking about the 5k people sample used in sbv intelligence test money may result in higher motivation and have impact on score found in some studies couldn’t find info about this and dont want to assume anything
1,method for linear regression analysis of 1 relation between 2 variables but based on data of different samples with different strengths had trouble formulating the title but i hope my explanation here will do for research i will be analyzing the relationship between precipitation and mass of deer antlers naturally i would use a linear regression analysis but i have a limited amount of data available i know the mass of antlers over 10 years for multiple deer so to illustrate for deer a h i know the antler mass from 2011 2020 and of course i also know precipitation in those years and since genetics and body mass of the different stags matter i cannot just treat my data as 1 sample but i have to make data sets of 10 values for each stag that would then yield multiple possibly linear entries for my analysis with varying strength but in essence i want to know how strong the relationship in general is between antler mass and precipitation so i am wondering whether there is a method in which i can do an analysis for this 1 relation based on these different individual datasets i hope this is clear enough would really appreciate some help
0,weekly entering transitioning thread 09 may 2021 16 may 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
1,how can i politely describe why having a censored event every day for every subject in tte is incorrect a friend of mine is new to tte and is trying to rerun a random survival forest that their boss provided them with updated data they came to me to ask for a quick rundown of how survival analysis works and we quickly came to a sticking point their data is daily observations for each subject with a censor if they are still going at the end of the day and an event if the subject died it looks like this subject time event 1 1 0 1 2 0 1 3 1 2 1 0 2 2 1 they are then feeding this into the model with no control for repeated observations i have never worked with a random survival forest before but from standard cox survival analysis my first reaction to this is to gargle incoherently before shouting that is not how tte works am i wrong when it comes to random forests i am having a lot of trouble convincing them that this is not how a tte model typically takes data when i try showing examples of other tte datasets i get a response of but that does not say we can t do it this way the closest i have come is on the repeated observation approach but they just dug up another method for tte with recurring events and said well this is like that so we can adapt rsf to take into account recurring events i think that one of the sticking points here is that they are using a few variables in the model that also change over time and they really want to be able to keep those variables in their predictions even though when using the model in the real world those data would not be available at the time of prediction am i nuts or are they nuts any suggestions for better arguments
2,facebook ai releases ‘hubert’ a new approach for learning self supervised speech representations many ai research projects have been striving to improve their ability to detect and interpret speech merely by listening and engaging with others much like babies learn their first language this needs not just assessing what someone says but also a variety of other clues from how those words are delivered such as speaker identification emotion hesitation and interruptions furthermore the ai system must recognize and interpret noises that overlap with the speech signal such as laughter coughing background vehicles or bird tweeting to fully comprehend a situation as a person would do article paper github fb blog
2,searching a paper agent navigates a grid world with text queries of symbols and colors hey guys x200b so some months ago i stumbled upon an interesting paper blog post i simply cannot find anymore it has two parts the first part is about a part of an ai system that generates increasingly harder grid worlds pictures of the grid worlds were black and white for another ai to navigate through the second part was about communicating what the agent should do the reward with text queries for example go and stand next to the blue triangle above the red square i know this is a very vage description but maybe anyone here knows the paper i m talking about d
2,paper explаined dall e zero shot text to image generation wouldn t it be amazing if you could simply type a text prompt describing the image in as much or as little detail as you want and a bunch of images fitting the description were generated on the fly well thanks to the good folks at openai it is possible introducing their dall e model that uses a discrete visual codebook obtained by training a discrete vae and a transformer to model the joint probability of text prompts and their corresponding images and if that was not cool enough they also make it possible to use an input image alongside a special text prompt as an additional condition to perform zero shot image to image translation to learn how the authors managed to create an effective discrete visual codebook for text to image tasks and how they cleverly applied an autoregressive transformer to generate high resolution images from a combination of text and image tokens check out the full explanation post meanwhile check out some really awesome samples from the paper dall e samples full explanation post arxiv project page more recent popular computer vision paper explanations comodgan
2,how does it sound if paperswithcode also supports you to implement the paper s code in gui assuming that they also have a service for you to execute the paper s code in gui without coding you only need to upload your dataset and run the code just by clicking what i am feeling is there are many papers i would like to implement but it definitely takes time to code it and check its reproducibility so i usually end up picking one or two papers to implement for time i am thinking it would be nice if i can also run the code more easily somehow is there anyone who feels the same about this
1,is it common to use the zip code as a predictor variable in statistical models suppose you want to make a model that predicts if a student will drop out of university you have historical information about many students and whether they dropped out or not you also have access to the zip code postal code where they lived 1 is it common to actually the zip code as an input variable probably not since there are too many categories or maybe use the first 3 numbers of the zip code as an input variable 2 i was always told to avoid using a predictor variable that has too many categories is there a mathematical reason behind this from a mathematical standpoint if your data has 1000 rows and one of your predictor has 450 categories mathematically speaking why might this harm your statistical model i can understand it intuitively having too many categories means too much information and your model might get confused but is there a mathematical explanation note i know you can just take students from different cities and make a sepperate model for students in the same city but i am not interested in doing this clarification i am using zip as a categorical variable
0,can someone explain use cases for apache spark so i vaguely understand what spark does at a high level right now i m working as a data analyst all i do is write sql queries conduct exploratory python analysis build tools etc i don t really do heavy analysis ml etc if my company transitioned to spark what would be the impact for me i would have faster processing times for my sql and python scripts access to real time data rather than waiting on etls
1,test for large dataset i have an extremely large dataset of hourly temperature over almost two years the temperature is for the differences under solar panels and are under the same panel but in different areas i want to see if there are significant differences in the temperature between these solar panel areas is the willcoxon signed rank test an appropriate test my data is generally not normally distributed there might be a month or two where it’s close to normal but on a monthly basis it’s extremely skewed right
2,outstanding performance of nvidia a100 on hpl hpl ai hpcg benchmarks
1,theoretical vs applied stats programs and biostats vs regular stats program so i m planning on getting a masters in stats to go with my degree in evolutionary biology in order to be a more attractive candidate for jobs in and out of academia and i m uncertain whether to get an applied or theoretical stats degree in addition while i m not super interested in public health and medicine i do want to target my statistics towards areas of ecology and animal behavior working for fish and game or university animal behavior labs that sort of thing i still don t know if i should apply for biostats programs still even though they focus on public health any advice would be enormously appreciated
2,thoughts or opinions on serverless model deployment i ve been working on building a serverless ml deployment platform for a few months now and i eventually realized that there s actually a lot of similar platforms out there but for some reason there s still a lot of divide in the mlops community between using a serverless solution and using a dedicated server for model hosting any ideas why this is the case is there anything that would make these serverless solutions more enticing view poll
2,text to image gans text rendering not being interpreted why when working with text to image gans e g clip and biggan does text from the prompt sometimes render in the image as text instead of interpreting that text is the word not understood is the prompt too short or is there another reason
0,professor cuts me off mid presentation and dismissed my idea how common is this in industry with management hello i’m an undergrad student whose been working with my college team doing baseball analytics work current role with a few other students had been to look at their pitch sequencing data and to find insights i had an idea today that i was presenting to them and i made sure not to overwhelm them with technicals provided insight into methodology didn’t state a value proposition which is where i probably messed up but as i’m going midway through my 30 45 sec breakdown the professor cuts me off and starts talking about another students idea doesn’t even let me finish i was pretty disappointed because i was pretty confident in my idea but he didn’t want to continue listening my question is does this happen a lot in industry when presenting to management do they sometimes dismiss your ideas like this also maybe give me some advice on how to pitch something too i am planning on scheduling a meeting with those two professors again to give them a full run through of my idea and why it’s useful
0,how do i gracefully exit an interview i am not qualified for i applied for a data science job recently thinking i was ready i ve been a data analyst for a few years now i had an phone interview over the phone and the job is way way over my head i don t know python much just basic syntax and slight use of pandas yet they want me to be able to code there i meet the sql requirements for sure however the recruiter over the phone did not sound confident when i told her i did not expect python to be used so heavily per the job description i also told her i am aware of a models that would be used in the role like linear regression k mean clustering etc but i have never actually coded them still she wanted me to progress to an in person interview next week i really don t think i can do this job some of the things she mentioned i have never even heard of i d love to work for this company in the future but i d like to bow out of the interview gracefully for now tips
1,appropriate use of the cloglog link function in binomial glmm hi i have boolean data from human participants per 4 conditions indicating whether a given response was correct i used a binomial distribution and compared logit probit and cloglog link functions all things equal cloglog gives the best fit with aic but is this link function appropriate for my data my data are quite asymmetrical i e more true events than false thank you
0,weekly entering transitioning thread 14 mar 2021 21 mar 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
0,leaving corporate data science i somehow landed a job right out of undergrad as a data scientist over the last few years i’ve spent a ton of effort to get up to speed with the masters phd level ds that i work with and about a year ago i feel like i got to their level and started teaching them new things the issue is i’m really starting to hate working in the corporate environment they’re so much pressure to perform perfectly and little room for experimentation i love the data science but i end up spending most of my time finding data fighting with it to get access finding smes to work with who have business knowledge of the data and so on i really like building models and working with data but the time i spend coding is so little it doesn’t feel worth it it also doesn’t help that i’m not the career driven type i care about how much money i make but couldn’t care less about what title i have so i guess my question is does any one have any suggestions of alternatives to working in corporate as a data scientist i’ve looked into consulting work but that takes some years of building a network considering i’ve only been in the industry a few years i have some room to grow thanks everyone
1,meta looking for a quick reference text i need to put a stats textbook on my desk that i can pick up and quickly look up fairly general statistics problems things like say how to do a chi squared association test or a t test or what a binomial beta distribution looks like etc just the standard set of foundational problems most people learn in their undergrad but then forget later with some examples and enough background to see the main points of the theory a small pocketbook would be ideal a lean textbook would work does anyone out there have a favorite book they d recommend e thanks for the suggestions but yes i know about the internet i m looking for a reference text a physical document i can hold in my hands written by a reputable expert e2 favourite so far
1,is the relationship between power significance level sample size and effect size representable by a formula 1 power significance level sample size and effect size are related to each other given three we can calculate the other other than some power tables and sample size tables is there a closed form or formula that represent their relationship the books i searched in have kept this vague 2 is their relationship derived in a completely deterministic way without calculating any probability i suspect their relationship might be purely deterministic and can be derived without referring to probability 3 the power is a function over the alternative hypothesis so what does a power which i guess is a scalar mean in relation to effect size in mathematical statistics books i don t find they mentioning effect size and the relationship between the four concepts but instead i find the definition of power function is the closest but still different the power function can be defined from level alpha test statistics t sample size n for each theta in set theta 1 of alternative hypotheses beta theta p theta t n x c where c is computed from sup theta in theta 0 p theta t n x c alpha thanks
1,discord servers for statistics statistics related topics does anyone know of can anyone recommend good discord servers related to statistics and similar topics maybe there s one for this subreddit that i can t manage to find for example thank you
1,are there any other glm other than loglinear model and logistic model i m studying for my exam of statistical models we re covering simple and multiple linear regression models and poisson s loglinear and bernoulli s logistic models i was wondering are there any other glms other than these two
1,would there be any reason to take a ratio stat and convert it to “by capita” i understand that for one variable stats dividing by the population can be critical in providing context but when a stat is in a ratio i don’t understand why anyone would divide by the population for example let’s say we want to know about eye color over time if we have historical data on the ratio of brown eye people to blue eye people for example 2 1 this year and 1 5 1 ten years ago what is the benefit of dividing all data points by the population what would dividing a ratio by the population even do isn’t the issue of relativity already accounted for because it’s a ratio
0,i’m struggling with data structures and algorithms hello all i’m currently a rising junior majoring in statistics my goal is to become a “data scientist” prior to being a statistics major i was a data analytics major and took two courses of software development one of which focused on data structures and algorithms i switched majors because i wanted to learn more of the math and i just genuinely liked statistics and wasn’t all into the systems design courses later on in the major long story short my naive young self a few months ago kept thinking that as a stats major i could get away with working with machine learning algorithms and big data with a masters degree in statistics and that i would never face data structures and algorithms again after reading this sub i came across several people who mentioned it’s important and i should learn it because i need a software side and they ask in interviews i am now spending my summer trying to get good at them for interviews but man am i struggling i’m sitting here learning linkedlists being asked to implement a function for like getting the position of an element or deleting an element and my mind goes blank i don’t know where to start not to mention i can’t even do leetcode easy i’m just really frustrated because i can wrangle data make dashboards know when to use specific models and have an understanding of statistical learning but it is this leetcode data structures and algorithms that is holding me back i don’t know how to learn this stuff other than mindlessly memorize leetcode problems any suggestions
2,trash garbage dataset for waste detection hi guys we ve published trash garbage dataset which can be used for waste detection and sustainibility based projects check then now if you like it you can give upvotes into our kaggle platform we re trying to make custom dataset and open sourcing on kaggle to make ai models more robust thanks
2,improvement on model s inference from deepspeed team how is jax compared great work again from the deepspeed team on model optimisation now not just during training but also inference step 1 multi gpu inference with deepspeed for large scale transformer models 2 compressed training with progressive layer dropping 2 5x faster training no accuracy loss 3 1 bit lamb 4 6x communication volume reduction and up to 2 8x end to end speedup x200b i was wondering how much of all the deepspeed s gain in optimisation can be done automatically in jax s pmap and xmap would a deepspeed for jax project even make sense
1,question simple sample question hello all i had a question on how to properly calculate the sample for a research project that i am helping with if i have an overall target population of 8 000 and i want to sample those within the districts of a given country do i calculate the sample size based on the target population within each district just want some clarity on this thanks
0,job frustrations data science analysis x200b so i have been working overall for around 6 months as a data analyst the first 4 months were a full time internship at a big4 and the last 2 months have been a full time position at a 6 years old startup the experience at both firms is amazing and enjoyable however the tasks themselves can sometimes be frustrating problem the frustration comes mainly from hard to clean datasets data inconsistency or tasks that are not at my level of experience yet aka challenges we also receive in some cases datasets that are slightly modified for no reason which causes the script to give an error context we receive generally sales data from retailers we clean them and push them to the database so we can later on visualize analyze create predictive models discussion i would like to initiate a discussion on your personal experiences with similar problems and how you dealt with them stackoverflow asked your supervisor i wrote a small blog here about my thoughts and stories so far a 2 mins read however i would be much more interested in the real stories in the comment section than the traffic to my blog because it will help me deal with the situations in a better way and with less frustrations on the long run
0,environment management skills after a slightly frustrating evening after work with vs code and virtual environments and all that good stuff i’m switching over to spyder which came with anaconda and therefore will find the packages i installed for my pet project a question off the back of this how important difficult is it to get a good handle on juggling these virtual environments it looks doable but i have a list of things i want to do background 6 months into my first job out of college doing non cs so there’s a lot of random tech skills i need to develop
0,calling out excel pro users dear excel geeks we do have it exceltips group but the mode is not active anymore so i have created another one if you are a pro excel user you can share excel tips and trick in this group if you starting with excel you can join as well to follow the pro users reson i created because if it wasn t for the people who share their knowledge absolutely free i wouldn t have survived and able to make a career as data analyst i still remember vividly when i first got a job as social media analyst in shanghai hardly had any knowledge about excel mike girvin channel on youtube name excelisfun absolutely saved my ass if you are an absolute beginner in microsoft excel i would highly recommend checking him
0,remote work do you think that remote working for data science or analytics roles will be a thing post covid too i know that it cannot be like the past year that every job was remote but will many jobs keep the remote status in the following years regardless of covid
0,recommender system for medicines code example i am looking for python coding example for recommender system for medicines the closest i have come to is this can you suggest something better thanks
0,is there software that can build a model of disease research findings with what s normal abnormal this might be the wrong sub so please direct me to the correct one if it is but is there software that can create a model for research findings for a disease preferably free cheap wishlist organized per subtopic immune system metabolome genetic etc you can mark if something was found normal abnormal with color with more than 1 option per thing because some research is contradicting researcher a found thing x was higher than normal but researcher b found it was lower with a link to the pubmed article and maybe a field for a pop up with notes when you click on it click a button and only show the abnormal results fade out the rest searchable something like this maybe where you can create a giant visual mindmap web of research results and zoom in on the different areas what do medical researchers use for this anyway is there some kind of central model database per disease or is everyone starting from scratch with their own research area
1,question what is the appropriate way to analyze baseline characteristics so say you have group 1 n 60 3 37 15 2 3 group 2 n 66 5 50 10 4 1 should i use x square to test for the overall group or should i test to compare each specific variable for example for the first doing 3 yes 57 no for group 1 and 5 yes 61 no for group 2 what s the appropriate way to conduct the test
0,product cannibalization would affinity analysis help we send an email to our customers to advertise 3 4 products every day supposing that today s email is advertising three products product a product b product c the customer is given four options 1 purchase a 2 purchase b 3 purchase c 4 make no purchase each choice is mutually exclusive i e each customer can make at most one purchase per day edit this is because of the nature of the business which is subscription based every day we advertise different products and some products are advertised with more frequency than others we want to test if certain products cannibalize others for example if customers that like a also like b it would be wise not to advertise both products on the same day i am exploring affinity analysis to measure support confidence and lift for each product pair the issue is however that some products are more likely to be purchased simply because they are advertised more frequently than others any suggestions on how to go about it
2,paper explained ddpms diffusion models beat gans on image synthesis full video analysis gans have dominated the image generation space for the majority of the last decade this paper shows for the first time how a non gan model a ddpm can be improved to overtake gans at standard evaluation metrics for image generation the produced samples look amazing and other than gans the new model has a formal probabilistic foundation is there a future for gans or are diffusion models going to overtake them for good x200b outline 0 00 intro overview 4 10 denoising diffusion probabilistic models 11 30 formal derivation of the training loss 23 00 training in practice 27 55 learning the covariance 31 25 improving the noise schedule 33 35 reducing the loss gradient noise 40 35 classifier guidance 52 50 experimental results x200b paper this paper previous code
0,julia vs r python recently i found a programming language called julia that is used for data science statistical analysis and machine learning does anyone actively use this software for their work how have your experiences been with julia does it offer anything that r and python can not is it worth learning julia e g some companies still have a loyal sas user group does the same happen with julia
0,what is a good technical interview machine learning question i am giving a technical interview later this week my background is in statistics and i will be a question with coding on it but i also want to ask a machine learning question our company usually uses machine learning algorithms for prediction as an alternative to logistic regression my own understanding is just that decision tree has a decent interpretability but less accurate than random forest or xgboost i don’t think it is fair for me to ask the candidate their algorithms in details since i am not an expert myself does anyone have any suggestions thanks
2,recommended interpretable machine learning fundamental principles and 10 grand challenges by cynthia rudin chaofan chen zhi chen haiyang huang lesia semenova and chudi zhong 10 technical challenge areas 1 optimizing sparse logical models such as decision trees 2 optimization of scoring systems 3 placing constraints into generalized additive models to encourage sparsity and better interpretability 4 modern case based reasoning including neural networks and matching for causal inference 5 complete supervised disentanglement of neural networks 6 complete or even partial unsupervised disentanglement of neural networks 7 dimensionality reduction for data visualization 8 machine learning models that can incorporate physics and other generative or causal constraints 9 characterization of the “rashomon set” of good models and 10 interpretable reinforcement learning this survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning paper
2,is this a typo in resnet s paper in page 4 of resnet s original paper it says when the dimensions increase dotted line shortcuts in fig 3 but looking at the network s architecture the dimension of the input is decreasing due to the strides 2 and the depth is increasing instead so i wonder if this is a typo or whether i missed something
0,spark ml stringindexer vs onehotencoder when to use which confused as to when to use stringindexer vs stringindexer onehotencoder the onehotencoder docs say for string type input data it is common to encode categorical features using stringindexer first in what situations would i want to take the extra step of transforming stringindex ed output to one hot encoded features i can find a lot of resources on how to use which but not in which cases the onehotencoder would be better
2,using ml to identify a particular object in a large collection of gsv image data last month a six year old boy was fatally shot in a road rage incident on his way to kindergarten the police have released an image of the killer s vehicle but thus far there have been no arrests because the crime took place around 8 am there s a good chance the killer lives or works somewhere near the crime scene according to this 2020 research paper pdf it should be feasible to use the google streetview api for python to bulk download gsv images within a specified area in this case orange county california then automate the process of searching for this vehicle our search is aided by the fact that this particular model of volkswagen is relatively uncommon x200b the author s combination of gsv data and cnn w tensorflow computational backing can result in an application that can be used to find different objects located in space a neural network could be trained to find a particular object and find it in the area further it can be trained to detect an anomaly in the vast data set which is quite dull and tedious work for a human gsv data isn t free roughly 6 7 usd per 1000 images but with a 450 000 reward for tips leading to an arrest and the potential of putting a child s killer behind bars could this be an interesting project for someone in the ml community is it even technically feasible
0,how do you guys remember syntaxes i ve been working as a new data scientist for 3 months and noticed some people could just jump straight in and type away on jupyter notebook without referring to anything while i have to maintain a cheatsheet of syntax need to bin categories refer to my cheatsheet need to plot a few bar charts with annotations refer to cheatsheet need to do a random forest err refer to cheatsheet usually there s nothing wrong with referring to notes or getting some help from google of course but as a professional data scientist it gets really embarrassing when i m having a discussion with my colleagues on a project i m working on and they re like can you create these new features and let me see how their plots look like real quick and i m like let me refer to my notes on how to do that
0,any freelancer looking for an appretice assistant hello everyone i am currently self learning data analysis and data science from scratch and i am really enjoying the process i am still a begginer at the moment and still have a long way to go i have just familliarised myself with python and the numpy and pandas libraries just now diving deaper into data cleaning i was wondering if any freelancer out there would be willing to connect for the prospect of a future apprentice assistant to help with freelance work i could help by handling simple parts of projects at the start and escalate to harder ones as i go along helping to speed up your work i am not looking for any salary just some mentorship and a reference in the future anyone interested feel free to dm me so i can adress any questions
1,discussion decision theoretic reasons for cluster analysis cluster analysis seems to be relatively popular for purposes such as separating out different market segments to aim for with different strategies however i have found it a bit difficult to find a reason in decision theory economic logic that this makes sense i was wondering if anyone could precisely articulate logic where this would be the analysis that really tells you what you want to know even under limited circumstances going back to the example of market segmentation one thing that would maybe make a bit of sense to me is if the clusters were actually the result of some latent categorical variable that you can t measure directly and you had a reason to expect due to other research that people with different values on this latent variable would respond optimally to different messages in this case maybe you would perform a cluster analysis on predictor variables to decide the cutoff for using different messaging strategies it seems like maybe this would be something close to optimal if your cluster analysis was based on sound assumptions about the distribution of the latent variable and how it relates to your observed predictor maybe on the other hand it seems like classification would be more useful than clustering in this situation at least if you had the right data to make a classification model alternatively without the assumption of some latent categorical variable maybe the point of the cluster analysis would be to find the coordinates of the center of the cluster the idea here is that people with similar characteristics on your predictor variables would respond similarly to messaging and a message optimized for the archetypical member of the cluster would perform pretty well for a lot a people since after all there is a cluster of people centered around that point i m not fully sure of my reasoning on either of these examples does someone have a better example is there a more logical statistical analysis to use for basically the same purpose
2,offline policy evaluation run fewer better a b tests does anyone here use offline policy evaluation at their company very interested in learning about real use cases for it and how well it performs at predicting a b test results
0,simple ways you use data science to improve your day to day job i’m curious what various ways this group has found to use data science or data science like skills to make their day to day job better i’ll start i wrote a script using primarily regular expressions to turn code log files back into code i don’t use it a lot but every now and then i’ll get my hands on a log file without access to the code and this lets me easily back door my way into the code
2,has anyone transitioned into computational neuroscience from ml research i m thinking about doing a phd in computational neuroscience after previously researching in machine learning i enjoyed building models in ml but realized i enjoyed thinking about how humans think more has anyone gone down this path what are some good subfields to apply my quantitative skills to not necessarily training nets what are some good labs to work in i m open to travelling internationally what s the fastest way to get a lay of the land dm me if you re a current comp neuro student and you d like to knowledge share i can provide lay of the land for ml
2,why the limitations in quantum ml i was reading about quantum ml and found out that the number of qubits one can have on a quantum circuit is limited that is we can not put in 1000s of qubits without quantum annealing onto the circuit this in return limits the kind of models we can train on the circuit for example images which typically have 32 32 3 input size can not be used my question is if we consider qubits analogous to neurons why is running a 1000 neuron neural net on a 32 bit machine a possibility but the same can not be done in quantum ml i am not entirely sure if the analogy is correct and might be having certain gaps in my knowledge on how actually a 1000 dense neural net can be processed on a modest 32 bit machine would love your insights on this
2,statistics refresher bengio goodfellow bishop or murphy all these books have a couple of chapters devoted to statistic which one would you pick as an introduction to statistics
0,shipping data science projects together with other teams in the last two companies i worked for we had a separate data science team and we had to work together with feature teams to deliver models into the product in my experience in many cases there is a misalignment between what the data science team is doing and the roadmap of the feature teams that makes it super hard to ship stuff i think one of the main reasons is that data science teams have to work with more uncertain timelines which makes it hard for pms to allocate resources to ship the ds projects once they are ready do you depend on other teams when it comes to shipping features if so how do you collaborate with them in these terms have you experienced issues when having to ship models before starting to work on a particular project how do you choose which project to work on and how do you make sure that the feature teams will have the resources to integrate the models in production
0,what is your approach to building a time series model before coding to after deployment i want to know your process of building a time series or other predictive model from scratch sources how do you keep track of the data used stakeholders interviewed assumptions made sql codes python r codes markdown wiki etc code reviews how do you convince other teams to use your model what output do you give them the data csv web app power point deck api quantifying success how do you measure success outside of rmse do you revisit your projections and measure your errors and revise your model is it built to do that automatically team how big is your team just you who else is involved in the stages of work research data sourcing review deployment follow up do you follow crisp dm or other framework loosely or rigidly my main fear with my own project is building a model no one uses i am sure the project s success will depend a lot more on soft skills like relationship building and cross department communication and less on my code or if i use arima over ets or whatever
2,how to combine auxiliary data and image data in deep cnn i have an image regression task for example to predict a person age given an image of his face but i have some more auxiliary information for each image that is known to be correlated with the prediction for example weight gender race etc how can i combine the auxiliary data with the image data so naturally i can quantize the auxiliary data into a finite number of options and train a neural network for each dataset but this is a poor option for many reasons a main one is because i am using only a portion of the data for each network can you refer me to a paper or architecture that use auxiliary information with a cnn
2,bootstrap your own latent why does it work so this discussion is for people who read know about the paper byol if not you can watch an explantation here so after reading the paper and watched some videos there are stuff that are still unclear to me 1 an inuition for byol was shown when the authors showed they can use a random freezed network which will never change not by sgd and not by moving average and they trained an online network to match the representation of the random network the same way they do in byol the represntation created by this got accuracy of 18 in linear evalution on imagenet much better than random i don t get why this would work if the freezed network would recieve the un augmented image and the online network would recieve an augmented image i can see why it would work because the online network will learn each image has one anchor in the latent space and each augmented image would have to get the same latent represntation and that would make the online network to ignore the augmentation and give it some semantic knowledge but because the random network can recieve augmented images each image in the dataset will have multiple anchors and it seems like the online network will just have to learn noise 2 why do they need to use the predictor head in the online network what is it s purpose couldn t the projection head just have the same role why can t the networks be symmetric
0,is there such a book about data science that covers the numericals of algorithms for eg like the one in above video is there a book or some notes made by some teachers available in the internet or even paid udemy courses that give examples of these solved numericals like in video i am asking this because these videos by indian guys are very inaccurate and i am not sure if i have learnt correctly or not
2,what matters in adversarial imitation learning google brain study reveals valuable insights a research team from google brain conducts a comprehensive empirical study on more than fifty choices in a generic adversarial imitation learning framework and explores their impacts on large scale 500k trained agents continuous control tasks to provide practical insights and recommendations for designing novel and effective ail algorithms here is a quick read what matters in adversarial imitation learning google brain study reveals valuable insights the paper what matters for adversarial imitation learning is on arxiv
2,has anyone worked with physics informed neural networks pinns i m trying to decide on my final year project and one of them is supposed to be on geometric deep learning utilizing pinns i ve skimmed through the paper introducing this and it all seems very theoretical the project aims to improve upon an existing biomedical classification task using pinns and i m not sure about how practical that will be could someone please enlighten me on this it d be much appreciated thanks edit the biomedical classification task happens to be trying to predict heat transfer fluid flow and patient specific tissue properties tumor sizes and location by feeding in thermograms 3d scans as well as bioheat transfer navier stokes equations i m not very good with phrasing my sentences so apologies for any confusion in advance
2,discussion fixed sized input to variable sized output hello everyone this is a crosspost from r learnmachinelearning but since i got no reply there i will try my luck here the input is a fixed size image let s say the image contains k objects say dogs that i would like to detect and in addition the relation of these objects to the other objects in the image say two dogs are in relation with each other if their respective leashes are held by the same person hence i would like to return a vector of size k 2 or matrix of size k x k where k is the number of detected objects is there any architecture that can achieve such a thing best regards and thank your for your time
2,weka dl4j image iteration i posted this in r learnmachinelearning first but didnt get any responses so trying here i am hoping someone is familiar with image classification using the dl4j library in weka using the gui i can train and save my model just fine but i am having trouble figuring out how to point the model to new data to classify my training arff and newdata arff are in abc123 jpg class format for training the model i set the image iterator location to the folder with all of my training images however when i try to run the model on the newdata arff it seems to want to look in the test folder for images and i am failing to understand where i can set the filepath for the newdata arff i m positive that i am overlooking a simple setting but frustration has set in and any help would be greatly appreciated
2,how does google s showing results for work if i search i love to eate my food on google then google will show results for i love to eat my food how does this algorithm work
1,is ward s method inappropriate for dissimilarity metric data i m assessing the results from hierarchical clustering using different linkages single average and ward i came across this post made in stackexchange and it stated the following last 6 methods require distances and fully correct will be to use only squared euclidean distances with them because these methods compute centroids in euclidean space the data i am working with is dissimilarity metrics regarding single nucleotide polymorphisms snps and other genetic distance dissimilarities as such based on my understanding it would not be appropriate for me to use ward linkage on this data this is my first time discovering this my previous coursework didn t mention this nuance but i suppose i may have learned about this upon review islr
2,graph neural network fails at generalizing on unseen graph topologies hi everyone x200b i m using pytorchgeometric to train a graph convolutional network for regression over nodes problem the graph models physical phenomena in the network of sensors in the training dataset there graphs with different topologies i e different edge index tensors and each of which has input and label tensors which consist of float values for each node in the graph the training curves look good the loss curve is converging to a small value and there are no exploding nor vanishing gradients x200b there are 1000 different graph topologies in the training set and around 2000 training samples so when the trained model is tested on graphs whose topology occurs 2 or 3 times in the training set the results are great almost the same as the test sample labels for each node the input values of nodes are different only the topology is already seen when the trained model is tested on graphs whose topology occurs one in the training set the results are slightly worse but when the model is tested on the unseen but similar graph topology the results are completely wrong since the graph models physical phenomena in the network of sensors i would expect that the gnn should be able to learn how sensor information impacts the neighboring variables even for the unseen graphs i ve tried going deeper into the graph and adding the convolutional layers x200b did someone have a similar problem are there some gnn models that are better at generalizing on unseen graph topologies x200b cheers
1,interpretation of kaplan meier curves in this picture here it appears that the survival curve for male patients seems to descend steeper than the survival curve for female patients based on this information could you argue that in this example males are at higher risk of experiencing some unfavorable event e g let s assume that this example is about surviving some medical condition and therefore should be initially paid more attention to compared to female patients thanks
2,can an ai learn political theory interesting paper can an ai learn political theory by stephen j decanio abstract alan turing’s 1950 paper “computing machinery and intelligence ” contains much more than its proposal of the “turing test ” turing imagined the development of what we today call ai by a process akin to the education of a child thus while turing anticipated “machine learning ” his prescience brings to the foreground the yet unsolved problem of how humans might teach or shape ais to behave in ways that align with moral standards part of the teaching process is likely to entail ais’ absorbing lessons from human writings natural language processing tools are one of the ways computer systems extract knowledge from texts an example is given of how one such technique latent dirichlet allocation can draw out the most prominent themes from works of classical political theory x200b link to the paper springer
1,mental preparation for real analysis hello all i’m a statistics major whose taking real analysis soon and since i had a whole summer i was going to start getting a head start on at least trying to get my mind to think in the direction of proofs does anyone have any advice on how to tackle such a course or how i should prepare i feel like jumping into a book right away will be ineffective as i won’t have any prior knowledge on “thinking” to do analysis does anyone have any advice on how they prepared for an analysis class tips or books i should read before hand i’m going through abbots “understanding analysis” book anything is appreciated thanks
0,til random forest randomness til that changing the order of features when calling the fit method of sklearn random forest regressor can lead to a different model even if the seed and random state is the same for ex 1 function call 1 randomforestregressor random state 42 fit dataframe1 feat1 feat2 feat3 2 function call 2 randomforestregressor random state 42 fit dataframe1 feat3 feat2 feat1 the models you will get from the above two functions will be different this could be down to the random nature of the algorithm and how it indexes features and considers them for making splits i spent an entire day trying to figure out as to why were my experiments not repeatable even when i set the random state and numpy seed at the top of the notebook and at the end of the day it was this small thing
2,icml 2021 accepted papers list so many papers by google
0,data science training company worth it i ve been in the data analytics industry for over 15 years now and i have been thinking about starting a company that teaches data science did research and found a lot of competition between coursera udemy universities and boot camps i d be targeting people who are considering a career change or professionals that need to retrain upskill i really have a passion for training teaching others and i have credibility from my experience working in the field i was thinking about how to differentiate myself personalized training teaching adjacent skills such as project management for data professionals and coaching in the job search process do you think it s worth pursuing or is the field just too saturated
0,app for project management i am a statistician and i work as consultant in lots of projects i would like an app to keep track of my jobs track progress schedule deadlines and the like most of the time i work alone so no need of collaboration features would be nice that the app can have some kind of connection to github since all my work is stored there not really a must do but desirable support for linux ubuntu windows and android is a must i would appreciate any suggestions for this
2,reverse attention for classification task discussion are there any papers except for dranet where reverse attention ra is used for classification task there are couple of papers which used ra for segmentation but hardly any which i could find that are used for classification purposes any leads
2,openpose caffe dependancy links down anybody have a backup i was trying to install openpose today and was encountering a lot of difficulity it seems like the links that they configured with cmake to download the caffe and opencv are down meaning that installation isn t going to work does anybody have a version of caffe opencl caffe3rdparty and opencv configured for the openpose framework to share
1,has anyone here studied association rule mining beyond the classic grocery store example has anyone ever looked into more advanced applications of association rules mining association rules can also be used for prediction classification purposes e g this allows you to obtain a fully interpertable algorithm that can provides a set of conditions for making predictions however these rules are usually not very powerful does anyone know if there are more recent spinoffs of this algorithm perhaps where a neural network or ensemble models can be used to learn these rules or in general does anyone know any machine learning based algorithms that provide rules thanks
0,how do you remember how statistical theories work to me it s like learning a language that you instantly forget if you don t move to the host country and use it daily i ve learned the basic stat theories 100 times apiece in my career because they are completely absent my brain every time i need them binomial distributions gaussian distributions central limit theorem on and on somehow i can remember bayes but figuring out why will require more introspection on my part i learn them as a means to some end like a machine learning course instructor asks us to make sure we know them before we move into our first modeling exercise i say oh shit i completely forgot everything about those so i deviate for a day brushing up on those theories i make it through the course go back to work with my new skill and then somehow never even think about those basic theories again until the next time i take a course that is make sure you know how binomials and gaussians work before you continue oh shit part of it might be that discussions of stat theories never occur in my professional life in fact nobody i know ever wants to hear about them so maybe i don t get practice in the same way that we don t get practice with foreign languages without moving to a country where they re native it also might have something to do with theory being taught with games in a recent binomial distribution lesson the instructor focused entirely on a game with coins and truth tables i ve been able to memorize the pattern in a way that helps me understand the density function but it s all so abstract that i don t think i ll be able to derive the density function myself in a year when i need it again how do you remember this stuff is there a good way to get past the memorization encouraged by instructors and into robust understandings of these theories
2,live demo analyze product hunt data using plain english questions nl to sql hello everyone as part of our product hunt launch we just released a live demo that lets you explore and analyze product hunt data using plain english questions which get automatically translated into sql using a semantic parser that we ve built you could ask for instance what were the top 10 posts in crypto this year what s the average number of votes for analytic posts which weekday has the most posts show me the top 10 hunters in analytics live demo product hunt post x200b looking forward to hearing your feedback
2,getting a good p value and confidence interval for cross validated auc in the case of data being not so big the auc can be relatively random that is why in this kind of cases you want to do cross validation and average the out of fold aucs to have something close to the real one however in order to have an idea of how accurate this auc is it would be best to have a p value for it i did research and learned interesting things one way would be to use boostrapping and it should work not too bad however best would be to use actual stats like you can find here fact is that there is good stats to actually model the auc get an estimate of the variance and a confidence interval however in the case of the average of aucs like in the case of cross validated auc i don t rly know about stats that much as is the case with most ml engineers i guess what would happend with the p value confidence interval i think its a very interesting topic and thanks for your help
0,techniques for anomaly detection across different time series most anomaly detection techniques packages focus on anomaly detection within a single time series ie take some sort of steady state average and alert if the data suddenly goes above or below some threshold my problem is totally different however i have a hardware device that performs the same operation repeatedly most of the time it succeeds but sometimes it fails i have sensors that measure position and angle 6dof and i have a large data set of each attempt whether or not it was a success or failure and the sensor data and first and second derivatives from a few seconds before the event to a few seconds after what i m looking for is a technique or python package that can analyze all this time series data and given label of success or failure identify if there are any anomalies that typically lead to a failure i ve done quite a bit of googling and stack overflowing but keep coming up with typical anomaly detection packages maybe i m using the wrong keywords or language here to describe what i m looking for any suggestions or pointers in the right direction would be greatly appreciated
1,inevitable manual work required in statistics projects i have feeling that not many people are willing to admit but ultimately is a significant part of many data mining projects e g checking data quality parsing through data etc still done manually for example here is an example i just made up relating to supervised nlp natural language processing classification suppose i have 1000 medical reports of patients containing unstructured text made by a doctor during a hospital visit for a given patient each report contains all the text notes that the doctor made for that patient for visits between 2010 and 2020 these reports make mention of the patients bio data e g age gender medical history etc and the details of the symptoms that the patient is experiencing over a long period of time e g let s say that these reports are 2000 words on average the problem is different doctors have different styles of writing each of these 1000 reports is different from another if a human were to read the report the human could figure out what happened to the patient did the patient have a serious condition let s call this class 1 or a non serious condition let s call this class 0 this is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients the problem is there is no clear and fast way not that i know of to take the 1000 medical reports that are available and label each report as class 1 or class 0 for example for class 0 one of the doctors could clearly write at the end of a report all medical tests were conducted and the results and were all negative and another doctor could end the report by saying the patient should seriously consider changing their lifestyle and eat healthier food benign in this example how would someone assign labels to all these 1000 cases without manually reading them and deciding if the information in the report corresponds to a serious condition or a non serious condition i was thinking of using something like sentiment analysis to capture the mood of these reports and use sentiment analysis a method to informally gauge if the tone of the report is dark serious condition or light non serious condition but i am not sure if this is the best way to approach this problem is there a way to do this without reading all the reports and manually deciding labels in the end this is what i am interested in doing suppose a new patient comes in and on the first visit the doctor makes some quick notes e g patient is male 30 years old 180 cm 100 kg non smoker frequently complains of chest pains no high blood pressure works a construction worker and takes daily medicine for acid reflex just based on these quick notes and the 1000 reports available note i am trying to illustrate a point here that the medical notes for the new patient and the 1000 reports do not have the same format can a researcher predict supervised classification e g decision tree if this patient will have a serious or a non serious condition ps suppose the doctors have a very detailed medical encyclopedia on their computers can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results
0,if ds is evolving into sub specialties what are they exactly i know about ml but what else is there
1,education signal vs noise in statistics are we in a record no hitter season in the mlb
1,question most appropriate statistically analysis i am looking for assistance in finding the most appropriate statistical analysis for my data set i am investigating the effectiveness of different survey methods in the detection of burrows each method is tested against the same burrow across six grids and properties of those burrows are recorded as the additional predictors version 1 of my data set looks similar to this method 1 method 2 method 3 vegetation activity 1 1 1 grass active 1 1 1 grass inactive 1 1 1 shrub active 0 1 1 shrub inactive 0 1 1 canopy active 0 0 1 canopy inactive as you can see the predictors are categorical and the responses are binary i ran logistic regression using r with the following code and the results are not what i have expected lm1 glm method 1 vegetation activity data logistic family binomial anova lm1 test chisq summary lm1 it basically stated that grass has a statically significant effect on burrow detection okay but i am more interested in whether the method did not detect the burrows i may be confusing the output data and interpreting it wrong for example i would have thought canopy would have a significant effect as the method didn t detect any burrows in the example set only roughly 20 in my actual dataset but it did not come out as significant 0 005 so i transformed the data to a continuous response based on the grids to look something like this burrows method vegetation activity grid 6 m1 grass active 1 4 m1 grass inactive 1 2 m1 shrub active 1 6 m2 grass active 1 5 m2 grass inactive 1 3 m2 shrub active 1 and ran an anova on that dataset although i m not sure if this is correct as there were only 14 burrows in grid 1 and there is overlap between burrows and methods basically i would like to know the best way to determine if vegetation and or activity affect the detection of a burrow within a single method then potentially determine the difference between methods within the predictors eg is method 1 better at detecting burrow in grthe ass method 2 is better in shrub etc then if there is an overall difference between burrows detected by the three different methods sorry if this is all over the place not sure how to explain my problem in writing thanks in advance
1,how to justify taking these courses in a statistics program i just got accepted into a math ma program with a statistics concentration as my university doesn’t have a masters in statistics i have already been assigned an advisor and we were discussing the courses i should take per the guidelines i am able to take up to 12 credits outside of the math department speaking to my advisor he is not very fond of that he wants me to take all the courses in the math department well i don’t want to i want to learn stats but that’s not all i want to learn my intentions were to take a grad course in machine learning and a course in micro and macro economics well he is very much against that we have a data science program and he stated if i wanted to learn machine learning i should have applied to that program and he also believes the econ courses are useless i personally don’t see it that way i want to learn whatever i wanna learn and i don’t wanna strictly focus on math courses which is allowed so he states i need to justify a need to take those courses and i honestly can’t think of a much better answer than the fact that i want to learn them
2,identifying products based on their weights i m trying to develop a system that can classify a product based on its weight to double check if the operators are using the right setup on the balances we use in our production lines how is this where i work we measure the final product s wight to control the consumption of our raw material and quality control the balance s operator set a certain product id based on the production schedule and this product is measured and its weight is pointed on the system the problem is the operator can input product a while we are manufacturing product b and thus we will have a divergence between what is the expected weight and what is being measured i m wondering if i can use some i a to learn what product it is based on its weight to avoid this type of problem the thing is my only input are the measured weights but every product tend to be normally distributed
2,paper analysis negative data augmentation iclr 2021 here is my analysis of the paper negative data augmentation published at iclr 2021 x200b x200b
1,how to predict the likelihood of the next number in a sequence where do i learn about this i m a noob trying to figure something out to keep it simple let s say i have a sequence of just 0s and 1s and each has approximately a 50 chance to be the next number when taking the whole sequence into account like this 0001011101011110001010010101010111100010000111 forever but the numbers in the sequence are not random they are based on some variable unknown to me and just happened to end up almost 50 50 so some patterns start to form that would not be likely if each number truly had a chance to be 50 50 each time is there a way to come to conclusions such as if 0 occurs x times in a row a 1 being the next number is likely to happen x of the time just using statistics and not actually knowing the variable which affects the next number is this possible and if so how would i do it x200b here s the actual sequence in question turned to 1s and 0s to fit the example it starts from the bottom and goes up 01001101001101011101110011001001011111010001001100000110011111101001011100110111101101110001110000101011010110100110101011110100011010100011110101101110100001110100011011111110110010111111101010011110110101011111010110101110001101111011101011100111011001011010011010110101001011100101010010001010000000101011101111010100110111010110101101101110111110011100011110001111101110110101111010101011100110010010001110011101110111010010111010011100000010111010011111001111100001111010011111001001001001110100001001110011101011101111111101001001101110000010001101110111100111110011101111001111101101100001000101000110111010011000001110111001000000101000000101010000111011111000010111100111101000011110100011100110110111101110101010001011101111010001010010111101100101110100111010110111010010001010000110111000111001111110010001001101110101101111110110010011001111000010101011010010010111110110101111110110011111111010111101110010111111001100101100011011000110001101110110111010110001100100110011001111111001001011101101011010010001010100111000100001011100010101110110111111101110000110101010101011000110011010101111110010111101010011110000000001001101010101100101100111001001000110001000100110101010111001101010101011111010111100101101000001110110101101010100011100010011010111011101010010111000111100111011111001101001110000011001101011010101100010001001110011101000100101010101010001000011001001100111001111011111000001001101101011001110000001101010010001110000011101111010011000011011100111000101001001001110001100110010111010011011101100110000101010100101001000101001110100101101001011110000011000001101111100010001011001011101111011111001101011011110100010011000101000111001100100010111011110110110101001010111010101101001110101001111110001111011101111011001110101001110101111110011000111100100101100100110101110101011101111010010100011001101101000100111101010000110000011011000111011010110011110111110111100100100100000111101111111011001101000010010001101011001111011111111010111010011010001100101010001010111101111101101111100100111101010101010100011011111110110100101110101101001011111111001100101100000101100111111001110011111000110011011010010011101000011110100000100111101100101001010011100111111000011000011101000000111011010011110101011100010111100000010010001011110100100110000011000110001110111110001011110111001011101110000101111101111011101011111011000101110011100000001011011011001110111010101110111001011100001111100100011110111001011101000100000001101010100011011111000111101101000000111100011000101110000111101110111000010110110111011100010010011011000111011101111011011111001101100010100111011111011111101110001001110000100111110101111101101111001010010100011010111101111010100001110000100101100011101101011111100000100001101011010011001010100010010100001011001101101111110111011100110011101110111111010011110101010011001010001011011111101011111101011110010111010100011101011111101000010010110111111000010001010111011111000011101111001100100111100111011011110100101010111011000110100101111010000101000101011010011010111011100111100110100110111101010000010000001101110100011110101101001010111011001001010101101101111100100100010010110100100101001000000001011000110101110110000111011100110011011000110101101110001010101100101010011100101000111101001111010101011000111001111001001110110001110100101000101000111010101110100011011011000001011010 n a0010111101001101011001111010101001000010101011001100100100101100101100101101111001011010010110110001011101101001010100011101111100100010101110110001111110001101001010111110001001011111011111101001101110110111010001000001011100011011110001101001111010100011000101110110011011100110111111011100011011111110111011011100111100101101111001101110101011111010000110010011001100001101010110100100011000100011101110010001000101101011100011101101001110010010100111111010000101101001110101010010111011010010101111100101110000111100101101011111110011101111011000110101100001000011111110001100010110110101001101010101000111011101011011111110110010001001111101001011011011011110010100101101010101100011011110010101000100101001110101110101111101010111011100001101010100001010111100111011001010011010110011000111111111001101100010010110110100101101111010111011110101111001100001111100101000011101000110001101011010111011011111101000110001110001011101110010001110110100000110100001110111100000100110110110100100101110101111011110110111011001010010111101000010001001101110101101011010111110100010010101100101111111101011110111110101000101101000110011011001010100110111001111101111100010011001010011001101011010011011000010111111110010100110101001100010000110110010000010 n a1001011100011000100110100000101011001101000111101001010111011110100001001100100011001110100000101101011001111000111101000010001000110001001000110010100110100001100110110010000110000001001010101010001110010101000110101101011010001011000001100111001010010001110111010111101000011001001100101110110111100011101100100110010110110110000010000100001010101001100100111011100010101100100010100001110000100101111000100111111000001011011100011101011010001001110001010100111101001000001001000010101100111011101110001001111100100001100111010110100011000001010111001011001100000110001000000100010001001100101101010011001111111000100100111110110110101001010111000100110100100000111100010001010101011000001100110100111101110001100111110011010110101001011000100010110011110
1,is introduction to mathematical statistics hogg craig supposed to be hard hi all i am self learning using the hogg craig mathematical statistics textbook it s challenging but i m understanding 98 of the subject material and i can get most of the questions right one of my worries is that i find the practice questions quite challenging and i often have to look at the answers i can get most of the application questions right but often regarding the proof esque questions i get stuck and give up after 45 minutes of trying i understand the answers to the questions and often have why didn t i think of that moments but sometimes the answers are stuff that i think i might never would have come up with i m also in grade 10 i know calculus important parts of multivariable calculus and linear algebra what i m wondering is is this textbook supposed to be that hard for someone using it for a statistics course in university or does most of my difficulty come from the fact that i may not have formal training in these subject matters also a follow up because i m getting a ton of imposter syndrome from trying this textbook does the fact that i m struggling with the questions this much mean my learning is shallow thanks a bunch a
2,goto approach for incorporating global knowledge for semantic segmentation i m working on a segmentation project for volumetric images the data is too large to be fed into the model 3d unet as a whole so instead i have been cutting out subregions this is of course very limiting since information outside of the cutout is lost what are methods to incorporate global information in such a task is google deeplab s approach using dilation and spatial pooling the way to go or has something new come up thanks
1,what kind of t test to use when comparing mean price of one stock in two points in time hello i would like to ask you the question from the topic i am writing a bachelor thesis on stocks and want to compare 30 day mean stock price in two points in time i was suggested to use a unpaired two sample t test and then a beta coefficient t test to compare the sensitivity the beta coefficient in the financial meaning of the stocks however i think that when comparing 2 mean values of the same stock in different points in time a single sample t test would be better as these two means come from dependent samples just like e g grades before a test and after a test from one student in that case the beta coefficient t test would not be performed as it requires the betas to come from independent samples which approach is right in this situation thank you for the answers
0,thank you r datascience r dataisbeautiful you guys helped me get my dream job ❤️ context i used to love working with technology when i was younger i did computer science at school worked at apple at 17 had work experience at toshiba research europe everything was going great until i got my gcse grades back and realised my coursework was terrible it wasn’t my fault but rather the teacher had taught us the complete wrong thing to do and only 1 person managed to pass he was fired but when it came to a levels i didn’t end up picking computer science as much as i wanted to i was anxiety riddled as a teenager and i didn’t believe in myself to do it i ended up going to university dropping out because of severe depression going into bookkeeping then lockdown happened i had so much free time that i ended up doing programming for fun i got reddit to try and find fixes to syntax errors when i’m programming but reddit recommended me this subreddit data is beautiful and i would check it everyday just because i found it interesting it was the perfect blend between number crunching and technology leading me to learn python get better with excel fast forward to a few days ago and i manage to get an interview with an amazing employer to work as a junior data analyst i was really worried because i didn’t know who or what the competition was but i did my best i mentioned that i followed these pages on reddit turns out they only interviewed one other person and i had the edge as i used reddit taught myself in my spare time showing huge enthusiasm thank you to everyone on this page you are all legends ❤️❤️❤️ tldr i fucked up computer science when i was a teen even though i loved it so much taught myself over lockdown and got a job partly because i read these subreddits in my spare time
1,factor analysis hey i need to perform a factor analysis via spss for a student project assignment to look at whether the items of my questionnaire measure the same construct i collected data from participants that needed to answer the same questionnaire consisting of 15 items after each task 10 tasks for each participant therefore 10 times the questionnaire i see in my data set every item 10 times now it seems to me a bit messy that is why my question is how can i make my data set more readable should i take the average for each item or should i perform 10 individual factor analyses
1,question regarding the use of dummy variables in regression models in order to incorporate categorical variables into a regression model these variables need to be recoded into a new set of dichotomous variables however does the same apply for ordinal variables for example i have a variable which measures the highest level of education for each participant the values range from 0 indicating lowest possible level of education to 5 indicating highest possible level of education should these values still be recoded into dummy variables
2,any paper formally pointing why softmax based neural networks don t return proper confidence scores i ve been reading about uncertainty estimation and almost every paper says that softmax is not a suitable certainty score so we need other methods to calibrate or correctly estimate uncertainty measurements my question is if this statement is purely empirical or if there exist some paper formally proving doing the math that softmax is not a uncertainty score the paper that i found closer to this problem is why relu networks yield high confidence predictions far away from the training data and how to mitigate the problem thanks is advance
0,python developer role disguised as data science i m a cs student and i ve recently been interviewing for several data science internships one of the companies was a walking red flag keep in mind that this wasn t a small startup this was a huge company 10k employees it started when i applied for the position online they responded after over a month once i already had several offers what was interesting was that they were the only company that called me as their first response and did not send a mail which was a surprise but not a deal breaker to me they wanted to make a short teams interview which i accepted since i wanted to have the interview training but would only consider them if they would be a really great fit so far so good we arrange an interview for the following monday at 1 pm in the call about 3 hours after the call they send me a teams link via mail which was written pretty unprofessionally and looked like they they hadn t put much thought in it what i immediately notice was that the teams link they send me was for 11 30 am on monday not 1 pm like we had agreed upon which was really strange so i send them a mail asking when the interview really is and that i would be fine with both timeframes but would like to know when i get a response back about 2 hours later saying that the other employee that would be present can t get to it at 1 pm so they decided to move the interview i immediately see the red flag they had just moved the interview without talking to me first and didnt even think of telling me i shrug it off and told them it was fine since i was not planning on taking the job anyways fast forward to monday 11 30 am i join the teams meeting they start out with a little small talk chit chat and we proceed to the interview they talk a little about what they do at max 2 minutes and then ask me to introduce myself which i do and then we just do a standard interview which goes great for the most part what they really emphasized at multiple points was that they wanted to have an intern for a long time so they were really straight forward asking me for how many years i will study cs probably so they can know for how long they can get away with paying me less and proceed to talk about how they keep their interns for a long time then one of them starts to list all the people in their branch which was weird since he didn t list a single data scientist the role that the internship was for which made me suspicious then in the middle of the interview i get to ask some questions so i ask what they will expect from the intern and what kind of task he be responsible for when they answered i immediately thought wow that sounds alot like a python developer role not data science which made sense since they kept asking for my python skills which as itself is not a problem since i am very comfortable with my python knowledge but it made me really sceptical so i keep asking more specific questions until im am certain of it since i was 100 sure that this wasn t a ds role and i was not worried of not getting the job i ask why did you call the job listing data science when it sounds alot like a python developer role the answer was not what i thought one of the interviewers straight up told me to my face that they knew it was not a ds role but decided to call it that anyway since its such a great buzzword and they would get a lot more applicants that way this is as close to a 100 quote as i can recall so this is not exaggerated which really made me speechless after that i just decided to play nice until the interview was over but had already decided that i was not going to take their offer if they would make me one fast forward 2 weeks after they had told me that they would get back to me by the end of the week they didn t and after two weeks i receive an email that they had decided for another applicant that poor dude my guess is that they smelled that i didn t want to take the job anyways and if i did i would not stay for long since it became pretty clear that i knew they were full of shit x200b has this happened to any of you how common is it that companies try to disguise a python dev role as ds
0,can anyone list a successful project that was actually ds ml and fully delivered in their company having one of those days where it feels like every project we attempt fails or fizzles out seems to be that 90 of the time there is a lower tech solution analysis that would deliver most of the value that hasn’t been explored yet looking online much content is toy box example of projects algorithms kind of showing how they work but not much content is around about things that have actually been production used or provided real value i sometimes feel as though the interesting ml ds things people think of when you say ml or ds have already been done and an out of the box solution would suffice bespoke solutions are not reached again because the low hanging fruit hasn’t been explored or the data doesn’t exist plus where do people even go to learn about developing the useful stuff there’s the same content in 1000 different forms explaining the basic stuff we all know but where to go from there i feel as though this has turned into a bit of a rant sorry about that i suppose i’m just looking for confirmation that ds is still relevant and useful outside of the walls of amazon google etc
0,best way to deploy a model into production let s say i ve built a model using past bank data that predicts which customers will apply for an auto loan with us i now have a potential email list with our current customer data that i want to check which specific members who are most likely to apply for a loan and a predicted agg count of how many will apply for a loan what would be the best way to do so can i have it read a csv file or connect to a sql table to do this i ve read a little bit about flask but i haven t seen anything like i m wanting done in flask not looking for a step by step or for someone to do the work for me more of a yes no this can be done and maybe a link that might assist me thank you
1,does personal laptop choice matter hey everyone so i’m heading into my first year of grad school for stats in the fall and planning on getting a new laptop i just wanted to ask if personal laptop choice really makes a difference in terms of having “laptop a vs laptop b” i’ve read around here and there that at the end of the day if i need to do any sort of heavy work it won’t even be through my own personal laptop along with grad school i’ll be interning doing data work with python r sas and some entry level algorithm stuff if that helps so does it really matter as long as it does what i need for daily stuff ie my school work basic productivity etc is there anything i should avoid and does os matter i’m only familiar with windows mac so fine with either “mainstream os” thanks for any help
2,project semantic text search in 42 lines of code searching a mourning man in the poems an example project made using jina search framework this is a short 42 line script to index 800 poem verses from the huggingface poem sentiment dataset and uses a transformer model to index them and performs a knn search using faiss module the example output for the search phrase a mourning man looks like following 0 sat mournfully guarding their corpses there 1 dearest why should i mourn whimper and whine i that have yet to live 2 taught by the sorrows that his age had known 3 the love that lived through all the stormy past notice many of the results don t even have the word mourn or man but they still are about a mourning man and thus appear in the results source code on github what do you think about this how can we make it even better and shorter
0,what are your thoughts on analytic app frameworks in python e g dash etc do you miss r’s shiny hi i am wondering what’s your opinion on frameworks for building dashboard analytics apps in python e g dash streamlit panel voila etc in python there seems to be some fragmentation for example people say that dash is more customizable but has a verbose syntax while streamlit is easy to start with but not so customizable this is interesting because in r there seems to be a clear winner which is shiny i heard multiple people say that they either miss shiny in python or that they even go back to r when having to develop an analytics dashboard app kudos that they are so fluent both in r and python what’s your opinion on this which framework do you prefer
0,i m fed up of seeing continuous data from tests in the time domain it offers very little value this drives me nuts unless you are looking for a relationship between time and your variable graphing in time is useless i so often see two parameters plotted on the same graph x axis time where people are trying to establish a correlation this isn t limited to the fresh faced grad who s just discovered r but experienced technical experts in a field typically engineering data in the fields i work data logging etc if you want to visually assess correlation between variables plot them x vs y and then lets have a look where time is not a critical factor
2,understanding of batch renormalization hello i was going in details through paper about batch renormalization arxiv link i don t quite understand two things there maybe there is anyone who faced similar issues knows the answer and could give me some hints 1 why in the formula here we don t multiply by the derivative of sigma b over mi b the standard deviation is not a constant but it s a function of the mean mi b however the final formula for dl dx i it s the same as i get so the equations are overall fine x200b 2 also i have a problem with the underlined sentences it would be nice to know at least what should i calculate step by step to reach this conclusions x200b my idea is that we have to find the ortogonal basis of the kernel of a matrix with 2 columns p0 and p1 and later project scaled dl dx dashed onto it or maybe there is better way to do it the next sentence is also not clear to me
1,question resources with step by step explanations for conducting statistical analysis on likert scale item data hellow r statistics i m in the middle of a project and i have to conduct some analysis on likert type data i ve searched online for days for any guide as to how to perform any analysis but i can t find anything specific can anyone recommend some resources like books or articles for non parametric tests kruskal wallis mann whitney ordinal regression etc preferably something with a step by step explanation because i need to understand the reasoning behind the tests
0,choosing the best parameters in an optimization hey forum i wanted to get some help and potentially collaborate hire someone to help me find come up with a solution to the below problem and the correct methodology on how to go about it here is the background x200b the use case is algorithmic trading to make a simple example i use indicators to trade forex stocks etc i perform backtests on data over 10 to 15 years and my goal is to determine the best performing indicator input parameter combination i do my backtesting using a process called walk forward analysis walk forward optimization the goal is to determine how robust the trading algo is the system when it runs on out of sample data the goal is to select the best performing parameters indicator parameters like stochastic if that s what you are using and carry that forward and use those input parameters on your out of sample data so here is what a in sample results look like x200b param1 param1 cagr avgdd 27 140 10 661 27 160 10 236 29 145 9 633 31 150 12 927 33 155 3 952 35 140 3 214 37 145 5 977 x200b cagr avgdd is my performance metric it can be anything really profit or profit factor etc basically parameters 1 2 x are inputs to the system during an in sample optimization run for each input parameter that i want to optimize i pick a start end and a step so if there are 3 parameters and each has a start value of 1 the end value of 4 and a step of 1 then you have 4 4 4 combinations passes and each pass will generate a performance metric value my in sample runs have a statistically significant amount of data for example a run over 4 years in sample period will have at least 5000 to 10000 trades for each pass parameter combination x200b so here is the problem that i want to solve how do i know which single pass parameter combination to select to run on my out of sample data set how do i choose the best performing input parameter combination i know it won t always be the one with the highest performance metric value for example my highest value can be 10 and the one right below it can be 4 3 clearly here 10 is an outlier what algo or method should be used some people have said knn is that true can this be done with more than just 2 parameter inputs what if i am optimizing 3 or 4 is there a downside to optimizing 3 4 5 etc different parameters x200b i m also looking for someone to help automate this for me looking for someone in math data science background to help me with this
2,how much memory does intel s gta5 image enhancer use for inference last week intel presented a neural network that enhances the graphics of grand theft auto 5 to photorealistic level the results are impressive video here paper here and the researchers state that they were able to perform the transformation at interactive rate which i assume is near real time i m trying to figure out how much memory horsepower the model needs to perform the transformation in real time the question is what size of a gpu you would need if this would become a real feature the paper provides information about the structure of the neural network but very little in terms of implementation details below is the key components of the model that are used during inference there are more pieces to it but those are used during training x200b i tried to figure out the numbers based on what information exists in the video and paper the image enhancement network is a modified version of hresnetv2 paper here on a 1024x2048 input the hresnetv2 uses 1 79gb of video memory on a size 1 batch x200b intel s model has made some modifications to hresnetv2 including the replacement of batch normalization layers with rendering aware denormalization modules which add more learned parameters to the model so i suppose this makes a minimal addition to the model s size also the g buffer encoder takes six different maps normals depth map albedo glossiness atmosphere segmentation and encodes them into a 128 component feature vector there are no details on how many convolution layers are used and i m not sure what the input size is so i don t know what the model size will be but it is a lot smaller than the main image enhancer network x200b here s an interesting quote from the paper inference with our approach in its current unoptimized implementation takes half a second on a geforce rtx 3090 gpu since g buffers that are used as input are produced natively on the gpu our method could be integrated more deeply into game engines increasing efficiency and possibly further advancing the level of realism given all we know about the image enhancer how much do you estimate its memory consumption to be also considering that gta5 a game that was released in 2013 can consume up to 3 5g of vram in 1080 resolution how realistic is it to see this kind of enhancement become available for gamers who don t have the highest end graphics card
2,do successful models defy the bias variance tradeoff in statistics we are always warned about the bias variance tradeoff simple statistical models are reliable but are generally unable to sufficiently capture the complexity within the data i e high bias low variance complex statistical models are able to capture complexity within the data but are generally not as reliable when generalizing to new data i e high variance low bias this leads me to my questions 1 are successful statistical models able to defy the bias variance tradeoff as a simple example consider the famous iris dataset kaggle competitions have shown us that statistical models can be made that perform well on both the training data as well as the test data are these statistical models defying the bias variance tradeoff now let s imagine a far more complicated problem and dataset but suppose that we are still able to create a statistical model that performs well on both the training data as well as the test data are we again defying the bias variance tradeoff 2 i have seen proofs that show how the mse mean squared error can be decomposed into a bias term and a variance term thus for a given statistical model for a fixed value of this model s mse if the variance is high then the bias must be low in order to compensate and vice versa my question relates to the following when people discuss the variance in the bias variance tradeoff they are generally interested in the variance of a statistical model s performance when dealing with unseen data since this unseen data might not even exist at the moment how is the bias variance tradeoff able to make claims about unseen data is the bias variance tradeoff a general idea with some theoretical foundations or is it mainly empirical 3 finally how does the bias variance tradeoff apply to real world models such as the self driving car alpha go and computers playing tetris or in the case of reinforcement learning models the bias variance tradeoff does not apply the same way it does in supervised learning models thanks
2,the 2021 gradient prize for overviews and op eds about ai tldr submit an article to the gradient from now until september 1 2021 for a chance to win a 500 prize full details here in case you don t know the gradient is a digital magazine covering research and trends in ai and ml we are a non profit and volunteer run effort run by researchers in the ai community we basically don t make any money but have recently received a grant that enabled this initiative while it s not a huge prize fund we hope it provides an incentive for anyone who has considered writing with us before happy to answer any questions but hopefully most should be addressed here
2,phd studentship for uk international applicants in ai driven population health study phd studentship in ai driven population health study improving medication verification for cancer patients applications are invited for a three year phd studentship the studentship will start on 1 october 2021 or as soon as possible after that project description medication errors including those in prescribing dispensing or administration of a drug are the single most preventable cause of patient harm they have a significant impact on the efficiency of the workflow in pharmacy raise safety concerns for patients and result in a financial burden on the healthcare systems within cancer treatment emphasis on reducing the number of medication errors has been an active research area for many years with understanding that interdisciplinary approaches are vital to assure continuous improvement opportunities created by the reduction of transaction times for complex computational processes and use of machine learning to support clinical decision making create a potential catalyst for the development of tools for reduction in medication errors this phd studentship offers an exciting opportunity of exploring ai and machine learning with large clinical data sets residing within electronic health records to create methods to assure the effective use of systemic anticancer treatment including traditional cytotoxic chemotherapy immunotherapy novel oral therapies etc without compromising patient safety the studentship will require application of interdisciplinary skills to enable cooperation between the research clinical industry and patient communities in the development of a novel approach which could enhance clinical outcomes supervision team x200b professor shang ming zhou shangming zhou plymouth ac uk mailto shangming zhou plymouth ac uk x200b dr edward meinert mailto dr 20edward 20meinert edward meinert plymouth ac uk mailto edward meinert plymouth ac uk x200b mrs andrea preston andrea preston uhbw nhs uk mailto andrea preston uhbw nhs uk this phd student will be academically advised by professor shang ming zhou and dr edward meinert research scientists with research interests in applied artificial intelligence and machine learning computing science in health and care the student will also be advised by mrs andrea preston a macmillan divisional lead haematology sw cancer commissioning pharmacist this supervision team will assure the execution of a world class phd embedded into the wider digital health ecosystem at the university of plymouth eligibility · this phd studentship is offered for uk and international applicants · applicants should have 1 a first or upper second class honours degree and a relevant master’s qualification in computing science data science statistics health informatics medical informatics bioinformatics or any areas related 2 interest in working with real world problems and large data sets 3 excellent proficiency in english and outstanding communication skills 4 strong analytical and programming skills 5 a “can do” positive attitude with an aspiration to change the world · experience in machine learning is advantageous · experience in publication of peer reviewed literature is desirable international students international applicants should meet the english language requirements please see the details from the university’s website ielts academic 6 5 or above or equivalent with 5 5 in each individual category is commonly required by the university’s doctoral college how to apply to apply for this position please visit please clearly state the name of the studentship that you are applying for on your personal statement a research proposal is required please see for a list of supporting documents to upload with your application enquiry if you wish to discuss this project further informally please contact professor shang ming zhou shangming zhou plymouth ac uk mailto shangming zhou plymouth ac uk dr edward meinert edward meinert plymouth ac uk mailto edward meinert plymouth ac uk or mrs andrea preston andrea preston uhbw nhs uk mailto andrea preston uhbw nhs uk for more information on the admissions process please contact doctoralcollege plymouth ac uk mailto doctoralcollege plymouth ac uk closing date the closing date for applications is 30 july 2021 shortlisted candidates will be invited for interview
0,at what experience level do coding tests take home and onsite go away i m considering taking an algorithms course next semester i doubt the content will add much to my knowledge considering the other courses i ve taken but it would surely be useful for interviews i m curious when the entry level coding tests stuff like dynamic programming longest increasing subsequence etc i m used to will start to disappear in interviews this is assuming someone with significant experience won t be bugged with these type of interview questions could be wrong
1,are there any careers in data science and plant research i’m a junior in high school and i’m think of pursuing a career in statistics data analytics i’m passionate about plants and permaculture but it’s not really feasible career wise so i’m wondering if there’s a way to merge the two into one career are there any of you who works with data and plants if so what led you to choose this pathway and how did you enter what are the pros and cons of your job what do you recommend to someone trying to enter this field thank you
1,creating correlation matrix for multiple combinations of variables i hope this is the right reddit for this i have a csv file with 10 columns and i ve used python to output a correlation heat map simple to do but what i want next is to identify correlations using combinations of columns for example a simple correlation matrix would compare a to a a to b a to c a to d etc but i want to combine columns in every conceivable way such as a b to a a b to b a b to c etc a b c to a a b c to b etc and be able to highlight any noticeable correlations between certain combinations is this possible at all are there proprietary applications that can do this or does anyone know how to do it in python thanks
2,disrupting model training with adversarial shortcuts it’s not always great that people can train machine learning models on your data in this new work we create adversarial shortcuts to prevent neural network training adversarial shortcuts are hand crafted modifications to images in the training set that exploit simplicity biases in models to prevent them from capturing the semantics of the dataset adversarial shortcuts are also easily ignored by human perception x200b while this idea is more broadly applicable we begin its study in the context of a well known machine learning problem supervised classification adversarial shortcuts all share a common idea fixing a pattern for each image in a particular class encourages models to fit that pattern over anything else it turns out that even fixing a few pixels prevents the model from fitting the semantics here is an example of an imagenet sized image with a pixel based adversarial shortcut this is neatly illustrated by these plots of imagenet validation and training accuracy progression notice how with the adversarial shortcuts applied the training acc 1 reaches close to 100 while the validation is stuck close to 0 x200b x200b of course the pixel based pattern may be easily disrupted so we also explore more complicated patterns watermarks with the class index made up of mnist digits and brightness modulations x200b read more including ablation studies and comparisons to related work in our new arxiv preprint joint work with ian covert aditya kusupati and tadayoshi kohno
2,are aaai and iaai tier i conferences are iaai innovative applications of artificial intelligence conference and aaai conference on artificial intelligence considered as tier i conferences in ai both of these seem to be quite popular but i am not sure if these could be called tier i how can i find out if a particular conference is a tier i conference
0,can anyone recommend some comprehensive blogs tutorials projects for text mining nlp in r can anyone recommend some comprehensive blogs tutorials projects for text mining nlp in r i have come across a lot of basic stuff but is there anything more detailed and comprehensive thanks
1,can you do a pca that includes continuous categorical and continuous data need help with direction for data analysis edit whoops title should say “ordinal categorical and continuous data” hello all i’m a graduate student trying to analyze data for my dissertation i have ordinal data for my dependent variable abundance of frog calls on a scale of 0 3 and a lot of too many independent variables things like site size water quality metrics soil type distance from other sites that are mostly continuous but with two categorical variables i want to know if the abundance of frog calling is related to any of the independent variables from conversations with my advisor and others i think i need to do a correlation matrix and cut out variables that are highly correlated then i should do a pca to pick out variables to use for further testing then i should do a multiple ordinal regression to test the significance for the selected variable’s effect on the dependent what i don’t understand can i do a correlation matrix and a pca with ordinal categorical and continuous data together i did read about doing polychoric pca but i’m confused about if that’s only for if all of your data is ordinal any insight is much appreciated
2,predicting human randomness with machine learning this is my first post to machine learning so i m not completely sure what i should write recently i have been experimenting with neural networks and machine learning and i wondered how accurately a neural network could recognize patterns in human generated random sequences of numbers so i tried as hard as i could to create a randomly generated sequence and i inputted around 1200 numbers the sequence was made of numbers 1 9 no ten or zero so randomly guessing the numbers would give you an 11 11 accuracy to my surprise my neural network guessing them with up to 27 accuracy i was shocked by how accurate the neural network was with relatively little test data this discovery opens many other possibilities and i have many ideas for how this can be applied using this neural network you can measure how random a number sequence i have a done a test with my sister and the accuracy of the ai varies depending on who s number sequence inputted why do people have different degrees of randomness is it correlated to intelligence this is one of the many questions i had while working on this experiment if you would like to help me with this project programming generating sequences please pm me any help would be appreciated thanks for reading this post the link to github is here humanrandomness
2,what is the successor to convolution autoencoders obviously from the title i am quite behind in machine learning literature and the sota works last time i checked cnn autoencoders were sota so a long time ago however now what would be more recent accurate autoencoder architectures gan autoencoders with a discriminator for more accurate results autoencoders with transformer layers if possible could someone link papers code
1,interpretation of covariance matrix i am trying to get my head around principal component analysis in the context of statistical risk models so i have a set of stock return time series as inputs can someone explain to me what from a practical perspective it means if my matrix is not positive semi definite and invertible und what in the data could causes that so i could pre condition the data and also why would these two criteria cause issues when running the pca thanks
2,nlp researchers reaction wanted this paper claims that eq 2 approximates eq 1 effectively the concept of timestep in this autoregressive model mdp is not defined based on the input sequence as in most nlp models afaik but rather an ad hoc decision my question 1 is this the first paper that has done this 2 is there any justification for it thanks in advance for any pointers
2,drop clause boosts tsetlin machine accuracy up to 4 and training speed up to 4x x200b cifar 10 interpretability with and without drop clause tsetlin machine tm with drop clause ignores a random selection of the clauses the key learning elements of a tm in each epoch inspired by dropout to explore the effects clause dropping has on accuracy training time and interpretability we conduct experiments on imdb sst 2 mnist and cifar 10 by visualizing the tm model we uncover that drop clause patterns seem more robust providing pixel level interpretability for images and word level interpretability for natural language open challenges improved booleanization techniques for color images and self supervised learning of interpretable tm based language models
0,make sure to test code that you pluck from github becase it can be really terrible sometimes i m trying to make my life easier by snooping around on github to see if i can steal code for my own work i ve found that a lot of ds open source projects are done by people who don t test their functions classes properly and it almost always gives me a headache sorting through the mess making me regret cloning the code in the first place how come so many data scientists don t know how to write a basic unit test hell even a bunch of assert statements would fix so many preventable problems with whatever you re trying to do i m no software engineering rockstar by any standards but it is really appalling what i find sometimes people will genuinely write a function where they describe a relationship as e ln x fucking really i m resisting the urge to link a few repos that i ve found because i don t want to call individual people out as it s a broader problem is this something that i m alone in or do you guys see the same thing
2,ensemble methods for one class classifiers i m working on an anomaly detection project we have normal datapoint and datapoint that are anomalous so as you can imagine its a very imbalanced dataset for the supervised pipeline my team has decided to use oneclass classifiers like oneclass svm elliptic envelope local outlier factor and isolation forest sklearn has pretty standard libraries but the syntax is not fit x y its fit x where x contains all the normal datapoints so any new point that doesn t fit its definition of normal will be classified as anomalous it all worked fine until we started looking for ensemble methods because these classifiers alone didn t give us enough performance where we can combine the performance of 4 classifiers we have in mind the ensemble methods on sklearn don t work because of syntax so we re wondering if there s a different library we can work with we considered majority voting which seems good but we are looking for a sequential ensemble method instead of parallel i ve been looking at adaboost and gradient boost adaboost seems direct enough from what i learnt about it since we can declare define our own weak classifiers in the meta algorithm gradient boost on the other hand uses residuals and decision trees are used as classifier i m unable to find a way to replace decision trees with any other classifying method in the meta algorithm so my question is this 1 apart from majority voting is there any other way to ensemble multiple one class classifiers 2 if there s a library out there that can help us out with this that would make my life so much easier thanks in advance
1,i have two correlated metrics and want to use one to forecast the other but the relationship isn t linear i have two metrics sales backlog and customer deposits i know they are correlated over 95 correlation coefficient according to excel as the backlog increases the time between making a sale and delivering a product so will our customer deposits money from customer set aside from a sale until we deliver it but the relationship isn t 1 1 they don t rise and fall at the same rate meaning the ratio between the two isn t constant how can i predict what the deposits number will be in the future given a specific backlog number i ve got a few years history for the relationship between the two but am not sure how to turn it into a forecasting equation
1,q spc hi i doubt i ll ever fully understand when i should use spc charts to monitor process i get figures each month from a region which are an amalgamation of several hospital figures in the region i want to know if i can and should be using spc on this data for example this month i got for the region 22 which is 4 hospitals that submitted figures 2 10 5 5 so my boss has asked me about doing an spc taking previous month s data i m just not sure if it should be used here he wants to see if when they are constantly reporting high if this is just their normal any help much appreciated
1,could you give me suggestions on how to get better at statistics this is kind of a broad question i m not in the stats field but i need statistics for my degree social science i wanna improve in interpreting data that includes making sense of the math and stats but i m not good at either of those i ve been recommended some textbooks on statistics e g gravetter levine etc i don t have a copy of the books yet before i get them i wanna ask do you have any recommendations for textbooks or even online courses coursera etc for stats recommendations for math stuff like calculus if that s helpful or necessary would be good too
2,can we begin to understand possible mathematical reasons as to why algorithms like xgboost and random forest win kaggle competitions instead of neural networks could there be any mathematical reasons behind why algorithms like random forest and xgboost are known to win kaggle competitions i e perform well for medium sized tabular datasets compared to deep neural networks and linear regression models heuristically here are my general conclusions 1 glm general linear models perform best on smaller sized datasets provided certain statistical assumptions are met 2 boosting and bagging algorithms e g random forest and xgboost perform best on larger tabular datasets and do not require many statistical assumptions 3 deep neural networks perform best on very large datasets preferably on non tabular datasets e g tensors pictures audio computer vision text nlp but can there be any mathematical reasons that try to explain these general conclusions provided these conclusions are correct for instance suppose there is one response variable and one predictor variable and when graphed together they look like a sine wave it seems unlikely that a linear regression model could perform well perhaps this is because a linear model can only capture a linear trend perhaps it is too hard to understand the exact assumptions required for glm models to work on real world data or they are too prone to overfit on complex data the same way is there any math that explains why alphago self driving cars and google s bert nlp model are all based on neural networks and not using random forest and xgboost is this because there is some mathematical property of random forest and xgboost which severely hinder their performance on very big and complicated datasets perhaps it can be shown theoretically that random forests require an exponentially large amount of trees to model complex data which is just not computationally possible or would surely result in overfitting and the same way is there any math that explains why deep neural networks aren t as successful as random forest and xgboost on medium sized tabluar datasets do deep neural networks simply require too much effort to select the right combinations of hyperparameters and its just not worth it for medium sized datasets when random forests work well given significantly less effort are deep neural networks to prone to overfit medium datasets of course all of this comes to down to trial and error if a certain model fits the training and test data well then use that model but just using mathematical logic and intuition can we develop some general guidelines that tell us which conditions and types size of data are favorable for specific algorithms this could potentially save us a lot of time by directly trying better suited models for the task at hand e g not even trying to use logistic regression for alphago so in the end beyond empirical results could there be any mathematical reasons behind why random forest and xgboost are chosen in kaggle competitions compared to deep neural networks and beyond empirical results could there be any reasons why random forest and xgboost are not chosen for the imagenet competition thanks
1,why are there so many variations on stochastic gradient descent i have been reading about variations on the stochastic gradient descent algorithm such as adagrad rmsprop and adam i am trying to understand the motivations behind the need to develop these extensions just by reading online it seems that these variations have an adaptive learning rate which can assist the neural network by optimizing the cost function 2 questions i had about this 1 how exactly does having an adaptive learning rate help to in trying to find the optimum of the cost function people wrote online that incrementally reducing the step size prevents from overshooting the optimum point but is there a more mathematical explanation as to why the basic stochastic gradient descent algorithm tends to overshoot the optimum point or is this just an empirical observation i e there is no real reason but it has been observed many times 2 i am trying to understand the relationship between gradient descent and regularization perhaps i am this incorrectly it seems that the neural network is trying so hard to find the optimum point of the cost function and so much research is being done has been done in trying to find this optimum point but as it turns out this optimum point has often been observed to produce a set of weights that result in overfitting this is where regularization comes into play and nudges the algorithm away from this optimum point in hopes that the new point obtained from the regularization will not overfit isn t this kind of ironic there is a possibility that the true optimum of the cost function will likely result in an overfitted model x200b thanks
1,where to download gmhdif software from
1,what is your opinion on sports statistics hi there first off i apologize if this is the wrong place to post considering that i ve seen a few posts about an education in stats i thought that this may be the place to ask for your opinions i m currently 15 years old in grade ten and have been asked in one of my courses to plan for post secondary school i have a few questions and would love if you could give me your opinion on it do you think a career in sports statistics is realistically possible to get into i know that sounds like a stupid question but i have been seeing mixed answers when i look online some saying it is very hard as there are only a few positions available whereas others are saying that it is in high demand and that a candidate should have a ms in statistics to have a good chance of getting the job and if you are someone who has a career in sports statistics and you re willing to share what tips would you recommend to someone who has an interest in it thank you
1,moderation v mediation every time i look up the difference between moderation and mediation i understand the basic examples they give however i still don t think i have it right in my head and my supervisors agree using this specific example can any one explain the difference between self compassion mediating the link between social support and mental health and self compassion moderating the link between social support and mental health my previous thoughts were that the former would mean self compassion is necessary for this link to exist and the latter meant self compassion would increase the strength of this link thanks so much in advance
2,prior image constrained reconstruction using style based generative models icml 2021 paper code obtaining a useful estimate of an object from highly incomplete imaging measurements remains a holy grail of imaging science deep learning methods have shown promise in learning object priors or constraints to improve the conditioning of an ill posed imaging inverse problem in this study a framework for estimating an object of interest that is semantically related to a known prior image is proposed an optimization problem is formulated in the disentangled latent space of a style based generative model and semantically meaningful constraints are imposed using the disentangled latent representation of the prior image stable recovery from incomplete measurements with the help of a prior image is theoretically analyzed numerical experiments demonstrating the superior performance of our approach as compared to related methods are presented
0,getting a head start when switching to a completely new domain i have so far mostly worked in adtech as a data scientist soon in about a month i will be switching to electric vehicle fleet management which i have never worked with i think the aspect which i mostly improved upon while working with adtech was handling huge amount of data but the underlying models weren t so complex it was almost always logistic regression i did deal with some deep bayesian models can anyone who has worked in similar field or related fields give me some pointers regarding the problems and the methodologies being used there what i could gather so far was that the general problem is about vehicle routing and fleet management what are the underlying methods being used there what algorithms models should i brush up on
0,how to use training data in python keras sequential metric edit the title says training because i m an idiot reddit doesn t let you edit titles but i meant to say testing i ve got a python keras sequential model that i would like to use early stopping on as soon as the test mse gets out of hand to prevent overfitting but i m not seeing any way to feed keras the test data and tell it to calculate a metric off that
2,project paddleocr awesome multilingual ocr toolkits（practical ultra lightweight ocr system support 80 languages recognition provide data annotation and synthesis tools support training and deployment among server mobile embedded and iot devices） hi all technical article code： i am glad to share that my team are working on an open source repository paddleocr which provides an easy to use ultra lightweight ocr system in practical comparing to the other open source ocr repos the performance of paddleocr is much more accurate but also the cost inference time is much shorter furthermore the performance is comparable to commercial api solution in some scenarios the visualizations of the general ppocr model the visualizations of the general ppocr model at present the stargazers of padleocr have exceeded 12k and increase continuously we hope that more people can benefit from the project thank you and looking forward paddleocr r d team
2,highlights of pytorch ecosystem days recently pytorch held an ecosystem day event that brings developers and practitioners together to share how they integrate their recent software packages with pytorch original post is here how one of the most popular dl frameworks builds the whole ecosystem of software and packages around itself when i first saw it i registered straight away but boy i must say the schedule and organizational part disappointed me the info and schedule were in google docs and yes google develops a direct competitor framework it was big overloaded and hard to understand in comparison google created a simple easy to use and fancy webpage with the schedule of their similar google i o event as a pytorch fan i feel a bit ashamed still the event is full of content it’s great that pytorch and the community support each other in better tools development to the content summary first of all i see that products based on pytorch are growing rapidly and packages are becoming more and more domain specific there are wrappers over pytorch like pytorch lightning ignite fastai catalyst they meant to make high level api with lots of sota features implemented the level of specification of pytorch ecosystem goes deeper each year we now can find not only cv nlp packages but also biomedical imaging audio time series reinforcement learning 2d 3d augmentation libraries mlops solutions some packages help to diagnose your models like finding model drift mitigate unfairness compress them i even saw a separate mlops package for automated driving the whole ecosystem is here and it quite interesting to dive into it there are in total 60 tools libraries and packages there soon they will need an advanced filtering categorization system it is amazing how fast this ecosystem grows curious to see the next step of this evolution posters they are here i decided to start from the posters and not from the videos since imo posters are more interesting there are posters for such wide known products like huggingface pytorch lightning etc i won’t mention them below is the list of something new interesting that i personally want to mention pytorch development in vs code they have profiler and tensorboard integration i am using pychram right now but thinking more and more about switching to vs code upcoming features in torchscript ai model efficiency toolkit aimet about model compression enabling pytorch on amd instinct™ gpus with the amd rocm™ open software platform nice to see that amd is catching up finally torchstudio a machine learning studio software based on pytorch yes pytorch focused ide product is not ready but looks interesting high fidelity performance metrics for generative models in pytorch upit a fastai package for unpaired image to image translation compressai a research library and evaluation platform for end to end compression pystiche a framework for neural style transfer videos there are opening talk videos for emea and apac morning emea opening talks evening apac opening talks they are more or less the same to be honest i would recommend skipping those and watch cuts from them instead my journey to pytorch by piotr bialecki nvidia you probably know piotr bialecki if you are using pytorch he is the guy who answers most of the questions in pytorch forum piotr is the technical lead of the pytorch team nvidia he speaks about his path in ml dl how he started to use pytorch he reflects on the past and looks forward you should watch that if you need a little inspiration youtube comment about piotr pytorch release by joe spisak you should watch this video if you want to learn more about latest pytorch release features from pytorch product lead u facebook ai joe speaks about python code transformations with fx it is a toolkit for pass writers to facilitate python to python transformation of nn module instances not sure everyone will need this torch linalg provides numpy ish linear algebra operations support torch fft that cover discrete fourier transforms and related functions pytoch native profiler yay with tensorboard plugin distributed training including support of amd sic gpus pytorch partner collaborations by geeta chauhan geeta leads ai partnership engineering at facebook ai nice lecture for the ones who want to know recent collaboration features she talks about more details on profiler with use cases integrations into partnering frameworks etc scaling in production with torchserve meant to be model serving framework for pytorch that makes it easy to deploy trained pytorch models performantly at scale without having to write custom code mlops with kubeflow building pipelines mlops with mlflow from model artifact serving to auto tracking of pytorch training metrics hyperparameters search disney s creative genome by miquel farré miquel is senior technology manager u disney he speaks about creative genome a project aimed to provide curated time based metadata on top of this metadata they are building their models to recognize their characters in movies animations comics series detect certain activities and events capitan alice in wonderland community updates by suraj subramanian pytorch developer advocate faceook ai well the name speaks for itself suraj gives some statistics on contributions into pytorch and also speak a bit about the ecosystem applications of ai and pytorch in asia pacific by ritchie ng ritchie ng is the ceo of hessian matrix an ai systematic global hedge fund based in singapore he provides an overview of apac speaks about cv in e commerce and retail nlp in finance more resources contributor newsletter includes curated news including rfcs feature roadmaps notable prs editorials from developers and more to support keeping track of everything that’s happening in our community contributors discussion forum designed for contributors to learn and collaborate on the latest development across pytorch pytorch developer podcast beta edward yang pytorch research scientist at facebook ai shares bite sized 10 to 20 mins podcast episodes discussing topics about all sorts of internal development topics in pytorch good luck diving into that
1,what to know before becoming a data quality analyst i am interviewing for a data quality analyst position at a global clinical trial company the position expects relevant knowledge of statistics data management sql excel and or clinical trial design what statistical concepts are used in data quality and what should i know regarding excel sql and data management i also have no knowledge about data quality or clinical trial data any resources courses would be helpful i have a bachelors in statistics and am fairly confident in using sql excel and r
2,just a fun idea project this is not super high on my list of priorities but if nobody tries it i’ll give it a shot at some point i’d like to see a manifold or gradient 3d printed with support structure from an actual ai model you’ve fit or a function that’s been optimized and then a marble may be involved it would be kind of next level awesome to compare two optimization methods like adam and sgd i would like to see how the path an object rolling down the gradient would differ between optimizers if you send me an ndarray of some project you ran that you could represent the fitting of in euclidean space i’ll happily turn it into a mesh and print it
0,mainframe limitations has anyone dealt with the limitations of data science against a mainframe and if so what was the key to success as background i recently took over a team with the mandate to start developing analytics and predictive models however a large portion of the data we might be interested in is stored in mainframe files we can access subsets of the data in downstream data lakes but those tables are highly curated or transformed without clear source to target mapping we also have the capability to use easytrieve to extract mainframe files but that tends to be a manual and tightly controlled process i’m currently researching an api or other connection that would allow for use of python power bi etc against the mainframe files i’ve also toyed with the idea of creating regular batch jobs to push entire mainframe files into an sql server for our purposes any thoughts would be greatly appreciated
0,paid nlp tools vs building own model i have started to work for a small start up company we are only 2 data scientists i am trying to understand consumer satisfaction by analyzing reviews sentiment analysis and ner will be methods i will go for as initial step my company doesn t have an nlp pipeline yet i wonder which one is better using paid nlp tools like google could nlp ibm s watson nlu or a self build nlp model i would be happy to hear what you think pros and cons google s service seems a bit expensive but pricing is still confusing for example if i have 5 million reviews how much am i expected to pay for sentiment analysis and ner services an estimate does google charge me again whenever i run sentiment analysis is there any cheaper but still effective cloud computing nlp tool that you can suggest i would be happy to hear your insights
2,confusion about pytorch loss functions and discussions workarounds say that for a single example my target is a probability distribution over 3 classes e g soft target 0 2 0 7 0 1 let s call these kind of targets soft targets i want to minimize the cross entropy of predictions relative to soft targets mathematically this problem is equivalent to minimizing the kl divergence of predictions relative to soft targets pytorch s crossentropyloss unfortunately doesn t support soft targets so i thought the simple work around would be to use pytorch s kldivloss instead i started looking up how pytorch users would do this task just to make sure i m thinking straight there s a lot of discussion about training with soft targets in the context of label smoothing see this github issue and this so thread there few people suggest using kldivloss for some reason one reason is spotted in the github issue there s a comment saying that we can t just naively input the same x into crossentropyloss and kldivloss i think people may have been misled by the switch in notation the line kl x y sum k y k x k y k log y k should be corrected to kl x y sum k y k log prob x k y k log y k assuming x is a still a vector of unnormalized scores not probabilities after making this correction i derived the expected result note that ls x y is defined as the cross entropy of x relative to y which was smoothed kl x y ls x y entropy y i m not sure why that math was useful as it s easy and numerically stable to add a log softmax to the final layer in order to use kldivloss so why are there all of these semi complicated workarounds why isn t everyone just using kldivloss for their soft targets what s so impractical about the entropy term in the special case of label smoothing isn t it much simpler and more abstract to just smooth labels as a preprocessing step and then train with kldivloss there s one comment on cross validated see the motivation part that implies some practical challenge with kldivloss probably about numerical instability but it doesn t directly say what the problem is so i m very confused about why the simple solution of using kldivloss doesn t seem to be popular in pytorch
1,if i am building 2 different types of models to predict the same thing does my partition need to be the same to train each model basically the title but for clarity let’s say i am building two models to predict some binary response one model will be a boosted logistic regression and the other an artificial neural network hypothetically when building and evaluating training models do i need them to use the same exact data partition in order to make a comparison later on or does the partition only need to be validated thanks
2,neural style transfer applied on classic retro video games in real time have you ever seen a 𝗠𝗼𝗿𝘁𝗮𝗹 𝗞𝗼𝗺𝗯𝗮𝘁 𝗳𝗮𝘁𝗮𝗹𝗶𝘁𝘆️ painted 𝗶𝗻 𝗩𝗮𝗻 𝗚𝗼𝗴𝗵’𝘀 𝘀𝘁𝘆𝗹𝗲 by a 𝗗𝗲𝗲𝗽 𝗡𝗲𝘂𝗿𝗮𝗹 𝗡𝗲𝘁𝘄𝗼𝗿𝗸 this snapshot short video clip here has been 𝗲𝘅𝘁𝗿𝗮𝗰𝘁𝗲𝗱 𝗳𝗿𝗼𝗺 𝗼𝗻𝗲 𝗼𝗳 𝗼𝘂𝗿 𝗹𝗮𝘁𝗲𝘀𝘁 𝗧𝘄𝗶𝘁𝗰𝗵 𝗹𝗶𝘃𝗲 𝘀𝘁𝗿𝗲𝗮𝗺 we implemented a 𝗗𝗲𝗲𝗽 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴 model to perform 𝗻𝗲𝘂𝗿𝗮𝗹 𝘀𝘁𝘆𝗹𝗲 𝘁𝗿𝗮𝗻𝘀𝗳𝗲𝗿 𝗶𝗻 𝗿𝗲𝗮𝗹 𝘁𝗶𝗺𝗲 testing it on video games currently interfaced in 𝗗𝗜𝗔𝗠𝗕𝗥𝗔 𝗘𝗻𝘃𝗶𝗿𝗼𝗻𝗺𝗲𝗻𝘁 𝗳𝗼𝗿 𝗗𝗲𝗲𝗽 𝗥𝗟 github here in the full video you’ll see 𝗮𝗹𝗹 𝟮𝟯 𝗱𝗶𝗳𝗳𝗲𝗿𝗲𝗻𝘁 𝘀𝘁𝘆𝗹𝗲𝘀 taken from very famous paintings in action the 𝗲𝗳𝗳𝗲𝗰𝘁 𝗶𝘀 𝘀𝗼 𝗰𝗼𝗼𝗹 𝗮𝗻𝗱 𝗳𝘂𝗻 experimenting with these technologies is really exciting️ 𝗳𝘂𝘀𝗶𝗻𝗴 𝘁𝗼𝗴𝗲𝘁𝗵𝗲𝗿 𝗱𝗶𝗳𝗳𝗲𝗿𝗲𝗻𝘁 𝗲𝗹𝗲𝗺𝗲𝗻𝘁𝘀 𝗮𝗹𝗹𝗼𝘄𝘀 𝘆𝗼𝘂 𝘁𝗼 𝗲𝘅𝗽𝗹𝗼𝗿𝗲 𝗻𝗲𝘄 𝗱𝗶𝗿𝗲𝗰𝘁𝗶𝗼𝗻𝘀 𝗲𝘃𝗲𝗿𝘆𝗱𝗮𝘆 to know more twitch channel discord server website neural style transfer applied on classic retro video games in real time
1,book recommendation on snps i work as a statistician but need to know a lot about biology current i need to know about snps variants does anyone have any suggestions for books articles or something i can use to study
2,what are the active fields of research in bayesian ml i have only a vague idea that variational autoencoders are quite mature already and there is a lot happening about making variational inference usable for larger datasets can you give me some more detailed and broader view of the topic thanks a lot
1,what is the best way to normalize crowd sourced nature observation data from this post yesterday and got a lot of comments suggesting i normalize the data to the population i e per capita however someone was saying that they think log observation would be a better approximation doubling the number of observations wouldn t double the number of species and another user was suggesting i normalize my data to total observations edible occurrences total of observations any help would be appreciated i am not a statistician or gis guy or anything just curious about this stuff
1,what s the difference between a qq plot and a normal probability plot
2,ml to train on billions of csv file rows what computer do you use to train ml on a two column two billion rows csv file i checked thinkmate servers but i thought i need an entire server room
1,engineer looking for advice for learning predictive modeling i am a civil engineer who will be starting a master s in geoinformatics engineering focusing on geospatial data science in the near future i took a probability and statistics for engineers course in undergrad a very practical course without much focus on theory and the math behind concepts i know i will eventually have to grasp statistical learning and predictive modeling my question is should i bother reading a more theoretical book like mathematical statistics by wackerly or can i jump directly into books like introduction to statistical learning and applied predictive modelling thanks
1,graphing continuous and ordinal data situation i m working on a scoping review and would like to visualize longitudinal data from multiple studies on a single figure however some authors have presented continuous data i e measures taken at each age while others have presented ordinal data i e from ages 6 9 10 13 14 18 etc i m struggling with how to do this one idea was to layer a line graph continuous on top of bar charts ordinal but this will be messy as the individual studies used a range or ranges another option might be to force the ordinal data to be continuous by plotting the same value for each age within that range i would greatly appreciate any suggestions or links to resources thank you in advance
2,tabular data deep learning is not all you need interesting paper a key element of automl systems is setting the types of models that will be used for each type of task for classification and regression problems with tabular data the use of tree ensemble models like xgboost is usually recommended however several deep learning models for tabular data have recently been proposed claiming to outperform xgboost for some use cases in this paper we explore whether these deep models should be a recommended option for tabular data by rigorously comparing the new deep models to xgboost on a variety of datasets in addition to systematically comparing their accuracy we consider the tuning and computation they require our study shows that xgboost outperforms these deep models across the datasets including datasets used in the papers that proposed the deep models we also demonstrate that xgboost requires much less tuning on the positive side we show that an ensemble of the deep models and xgboost performs better on these datasets than xgboost alone by ravid shwartz ziv and amitai armon x200b linkt to paper
0,tips on leaving a job that you like for a better opportunity i just landed a job at a big tech company and need to put in my 2 weeks to my current boss and team virtually i like my boss and i like the team i learned a ton on the job in the year and a half i ve been here the new job i m guessing will be a better career move in the longterm more learning opportunities and room for professional advancement how did you resign from previous jobs and what are some things you wished you did differently any advice on what to say
2,discussion unsupervised vs supervised learning i was recently exposed to unsupervised and supervised learning and from my understanding the main difference between the two would be whether labelled datasets provided however during the implementation of the code the models used are largely similar such as the resnet or osnet models i would like to ask how the code mainly differs for these 2 methods or does it only differ for the input provided thanks
2,paper explained vqgan taming transformers for high resolution image synthesis it is a lucrative idea to combine the effectiveness of the inductive bias of cnns with the expressiveness of transformers yet only recently such an approach was proven to be not only possible but extremely powerful as well i am of course talking about taming transformers a paper from 2020 that proposes a novel generator architecture where a cnn learns a context rich vocabulary of discrete codes and a transformer learns to model their composition as high resolution images in both conditional and unconditional generation settings to learn how the authors managed to create an effective codebook of perceptually rich discrete image components and how they cleverly applied latent transformers to generate high resolution images despite severe memory constraints check out the full explanation post meanwhile check out this paper poster provided by casual gan papers paper poster full explanation post arxiv project page more recent popular computer vision paper explanations comodgan
2,predicting the class of an event or its probability in n days after i have a dataset with time series and for some of them test sample dates are known maybe from zero to several pieces per time series which we will call target events the occurrence of which i want to predict also for each rserie several heuristics have been calculated that catch some kind of local change in the serie different types of drops in the serie level and fix in themselves whether a change has occurred on such and such a date or not i want to take advantage of these heuristics and so to learn how to predict the occurrence of targeted events three metrics are of interest forecast accuracy forecast completeness recall and forecast of interval between heuristics date and target event for example that a target event will occur n days after a set of heuristics is triggered for each date i can calculate how many target events will occur on the test sample after each of the heuristics is triggered for n days by this calculation i want to evaluate the usefulness of each of the heuristics but there are several questions how to relate the triggering of the heuristic on date n with the desired event and for example not with the next one this is necessary to assess that the accuracy of the heuristics is it possible to somehow generalize the triggering of heuristics to predict the occurrence of a target event exactly after the calculated number of days maybe there is some way to predict the class of the event in n days whether it will be target or not or the probability that it will be target this is necessary in order to ultimately generalize the triggering of heuristics and additional known time series features
1,question want help understanding the model in a paper on climate change and lyme disease x200b trying to understand the model behind this paper it s a paper examining the link between climate change and the incidence of lyme disease can anyone help me understand the model used and give me a basic overview of how it works
1,any good survey creation tools to show undergrad stat majors i m looking to do a tech demo of survey creation for an undergrad class or stat majors in about a month they won t be any big project around it but i want them to have familiarity with one survey creation tool for future work ideally i want a system that is free or cheap for small time users or has a community edition lightweight enough that the basics of it can be demonstrated in less than 2 hours of lecture capable of producing a csv or something similar for analysis capable of some rudimentary skip or display logic available in places like china and iran that have had past difficulties accessing american software services is there anything you know that ticks all these boxes thanks in advance
1,technical analysis so i ve decided to finally have a vaguely more exciting savings strategy and have decided to start trading some stocks as part of it i ve started learning about the different indicators and whatnot now there s the obvious stuff like reading a balance sheet etc that i m all good with makes sense to me and i understand the value of then there s technical analysis you know i have written some pretty mean stuff on this subreddit making fun of six sigma and ill do it again but technical analysis makes six sigma look like some hard core academics it is so god damned absurd its like a six year old son of a stats professor puts on his dads suit and got out his fisher price chalk board and pretended to be him teaching which is super cute of course but finding out that the stock market trades on this is absolutely fucking bonkers nuts to me all of this i would be ok with to a point except for one issue it is self reinforcing the more people believe in it the more true it becomes people trade on these mickey mouse signals so they do affect the market so i actually have to learn this i have to sit down with the 6 year old in the suit and have him explain to me how the ema crossing the sma means this or how the fast stochastic bla bla bla means this when its just a fucking average and everything has these unnecessary bullshit names fucking bollinger bands ffs its an ema with k sigma bands what kind of a twat invents something like that and puts their name on it he actually trademarked that shit i mean wtf anyways technical analysis is the most mickey mouse bullshit i have ever seen and i have to fucking learn it so now i m salty
2,an overview of some available fairness frameworks packages i ve created an overview with some available fairness toolkits i hope it can be helpful for one or the other content 1 the linkedin fairness toolkit lift 2 fairlearn fairness in machine learning mitigation algorithms 3 ai fairness 360 4 algofairness 5 fairsight visual analytics for fairness in decision making 6 gd iq spellcheck for bias code not available 7 aequitas bias and fairness audit toolkit 8 certifai a common framework to provide explanations and analyse the fairness and robustness of black box models 9 ml fairness gym google’s implementation based on openai’s gym 10 scikit fairness 11 mitigating gender bias in captioning system link an overview of some available fairness frameworks packages
1,help with if then statements in spss hi everyone this might be a very stupid question but i am having a lot of trouble figuring this out i am using spss i am trying to categorize people into one of two groups 1 or 0 based on their attraction ratings to various ages i want to assign people to get a 1 if their score for male female 18 25 years olds is greater than all other groups i e male females 26 35 36 45 46 55 56 65 66 so basically to get a score of 1 you need to have a higher attraction rating for males or females 18 25 than all the other possible groups so far i thought i could go to compute variable if include if case satisfies condition plug in 18 25m f all my other group variable names and then once i continue back to the compute variable section i would just input 1 else 0 this doesn t work i have also tried it a different way by just putting it straight into the compute function but i get the same error which is 127 execute during execution of the indicated command one of the operands of and or or had other than a valid logical value the valid logical values are 0 1 and missing the invalid value has been treated as a system missing value i appreciate any help with this thank you
0,how to remove data from a scatterplot i have a list of x y coordinates that represent the location of cells in a slice of tissue i can generate a scatterplot of that data and make a map now i want to remove a bunch of those points because they are not trustworthy i know how to do that 1 point at a time but i have thousands of points that need to be removed on each of hundreds of scatterplots do you know if there is any way to highlight a lot of points at once on a scatterplot and then just delete them from the data set it doesn t matter what program platform is used to do this it seems like a straightforward question but i can t find any way to do it nor anyone who does
0,prescriptive analytics subtopics hello i’m currently in a undergraduate data science club and we had a chief analytics officer from a company come in and speak to us about linear programming and optimization it was towards the end where i had asked him how optimization techniques could be used in predictive modeling and machine learning he told me that the concepts he was telling us about actually didn’t fall under predictive analytics but prescriptive analytics the difference being predictive analytics is using previous data to predict new outcomes and prescriptive analytics being using previous data to find what the most optimal solution is that peaked my interest because thus far i have had a rather narrow view on data science with only predictive analytics and machine learning as my focus of interest however i wanted to learn more about the prescriptive analytics side for those of you who work with prescriptive analytics what are some math topics or sub topics that could help me explore this i’m a statistics student and naturally my tendencies have gone to predictive modeling with machine learning so i’m wondering what other math concepts would be useful for prescriptive analytics linear programming optimization graphs
2,collusion rings noncommittal weak rejects and some paranoia hello everyone i will start by saying that i am a fourth year doctoral student in a small lab within a not so famous university who thus far sent about 10 papers to various ml cv conferences and had peers in my lab who also sent bunch of papers which i had the opportunity to have a glance to the reviews they received i know there are many people in this community who have more experience than i do i am creating this post to have a discussion on the topic of academic reviewing process and collusion rings the topics of academic fraud and collusion rings recently gained traction with the blog post of jacob buckman 1 and the follow up video of yannic kilcher u ykilcher 2 many thanks to both researchers for talking about this topic 1 2 x200b according to the professors in my lab in recent years they have seen a dramatic increase in the number of low quality reviews with noncommittal weak rejects these rejects often do not provide enough feedback to address in the rebuttal process and have conflicting outcomes compared to their reviews as well as other reviewer s who provide decent well written feedback following up on the spirit of the aforementioned media i wanted to share the experience i and fellow doctoral students in my lab had with the reviews x200b two examples from the reviews from recent papers we submitted reviewer a strengths of the submission the investigation of this paper is valuable and interesting weaknesses of the submission the article uses xyz method i don t think this method is useful for this type of analysis the article is unstructured and hard to follow outcome weak reject reviewer b strengths of the submission this work is interesting which reveals the shortcomings of the methods used in abc field weaknesses of the submission in my opinion contribution is maybe not enough i suggest authors to use the methods in these papers paper1 paper2 paper3 paper1 paper2 paper3 three papers from the same university of which two of them were written by the same person and he is the author of the third paper outcome weak reject noncommittal weak rejects like these do not provide nearly enough feedback to make any improvement on the paper but this kind of reviews ensures that the submission will be rejected since the number of papers being submitted is increasing every year and acs are under pressure to reject as many papers as easily as possible x200b i recently had a pleasure of having a conversation on this topic with another senior professor who is also working on computer vision but another subfield he said that i should try to get acquainted with researchers from a certain country and make sure to have a couple of them as co author in my papers to improve my chances of acceptance when i told him that the reviews are double blind and that having them as a co author wouldn t matter he confidently said that the double blind is just a facade x200b we are working in a niche field of computer vision with not many participants compared to other subfields i am starting to have deranged thoughts thinking that we are being somewhat unlucky with the reviewers and that our papers are being rejected by people who are part of a group that try to reject papers coming from outside their circle in a noncommittal way so that these reviews do not raise any suspicion x200b i hear left and right that collusion rings are far more rampant than what people believe they are am i being paranoid what is your experience with the reviews in recent years please do share
1,confusion about fwer and bonferroni correction when tests are not independent so as someone not really as familiar with frequentist stats i have a question about the statement that a bonferroni correction is appropriate in the context of posing multiple independent hypotheses in contrast to statements like one of the most widespread of these misunderstandings is that the method would be based on an assumption of independence between p values this misunderstanding comes from a frequently used but deficient motivation for bonferroni saying that the probability of making a false rejection if all m0 p values of true hypotheses are independent and we perform each test at level by this reasoning bonferroni seems like a method that only provides approximate fwer control and that requires an assumption of independence for its validity in fact the method of bonferroni provides exact fwer control under any dependence structure of the p values the context of my problem involves linkage disequilibrium score regression and specifically estimating the extent to which heritability for a collection of non independent features phenotypes is conditionally enriched across overlapping so not independent sets of loci surrounding overlapping sets of genes the genetical non independence of traits can be seen in this genetic correlation matrix i estimated the non independence of gene sets can be seen in a jaccard similarity matrix i estimated but don t have on hand but it wasn t too bad on the order of 0 1 0 2 the usual approach here is bonferroni correction since the gwas used tend to have high n and so are pretty highly powered seeing as at the end of the day there probably aren t any true nulls but this seems like the exact context where the bonferroni would not yield the desired fwer where it might be overly conservative if the former wishy washy statements were the case to explore this problem i wrote a small r script where i simulated multivariate normal data according to some correlation matrix sampled from a flat lkj estimated sample correlations among that data and computed the corresponding p values i then estimated regression coefficients between these samples x and independently generated data i e from y a 0 x e and then computed the corresponding p values after bonferroni correction the fwer was at the desired value which is unsurprising given that a pearson correlation is iirc just the ols estimate of the regression coefficient when both predictor and outcome have been rescaled to unit variance this would also apply if the correlation structure non independence were in y given the symmetry between y x and x y neither of these situations quite maps to my original problem where it s not so much the individuals in the gwas that covary though they do given population structure but rather the coefficients themselves given pleiotropic effects but then i m not quite sure how to simulate that since the whole point is to simulate from a null nil model where the true value of those coefficients is set to 0 otherwise my vague intuition about this non independence multiple testing correction thing is that the whole independence non independence idea applies to the sampling distribution of model parameters across different tests maybe but i m both unsure of how to specify that sampling distribution and also unsure of how exactly to accommodate it in hypothesis testing any help or insight would be appreciated ps i m actually not 100 sure how what sort of hierarchical model would appropriately regularize estimates of pairwise correlations in the bayesian case you could maybe use an lkj or its implied marginal distribution r ij 1 2 beta η−1 k 2 η−1 k 2 w η to tune but that would center things on r ij 0 using a plain beta would get you off that but then your estimate of the matrix as a whole would not respect any sort of psd constraint any thoughts there would also be nice
2,game on mit allen ai microsoft open source a suite of ai programming puzzles a research team from mit allen institute for ai and microsoft research open sources python programming puzzles p3 a novel programming challenge suite that captures the essence of puzzles and can be used to teach and evaluate an ai s programming proficiency here is a quick read game on mit allen ai microsoft open source a suite of ai programming puzzles the paper programming puzzles is on arxiv
0,how to be more structured in formulating queries for sql interviews when i work on sql queries i usually start with a base query and edit as i go depending on the requirements sometimes running the query and debugging however after being in a sql interview it seems like this approach doesn’t work the best for interviews as i frequently go down the wrong path before correcting myself and especially not in situations where i can’t actually run intermediate queries what’s a good approach for working out a sql query in a structured way
1,question how do you respond to what is statistics i was wondering how do people respond to questions like what is a statistician why did you choose statistics what is statistics i have friends who often express curiosity of my major and i feel like i don t give it justice in explaining it i ll mention something about data analysis creating models or something like applicable to many fields i was wondering how do others explain what statistics is and make it sound interesting has anyone ever explained it during a conversation with a positive response if so how did you describe it i feel like statistics is definitely useful but its difficult to explain how cool it is unless you re someone who is using it
0,how to encoding nan values with meaning the two most common cases i ve seen nan values appear in datasets are either because the data was simply not collected and or is just missing for no meaningful reason or that a response is not applicable to a feature due to the nature of that specific data point as an example id has arthritis arthritis limits ability to work 1 yes yes 2 yes no 3 yes nan 4 no nan in the table above the values are missing in rows 3 and 4 for different reasons seemingly the value in row 3 is missing because most likely the data was not collected however in row 4 the missing value is due to the feature not being relevant i e we do not need to ask if a patient s arthritis limits their ability to work if they do not have arthritis it would thus seem that in order to make the most accurate model we should not treat these two cases the same what are some methods for dealing with these types of situations
1,when applying for phd programs do i need to explain a c grade on my transcript in an unrelated class so i just finished my ms in statistics i got a s in all the statistics courses required for the degree however in the last semester i decided to take a java programming course for fun as i was just curious about more intensive computer science theory and java the course was not required for my degree and was unrelated to the field of statistics i ended up getting a c in that class when i apply to phds in statistics a closely related field like biostats for example in the future do i need to explain the c in that java class or i can just leave that out and the straight a s in the grad level statistics courses will speak for itself
2,how do you use rl on multi modal embeddings for image2text retrieval for my mini project combining computer vision nlp rl interests me i ve come across this paper recipe1m a dataset for learning cross modal embeddings for cooking recipes and food images where the main task is trainingg a neural network to learn a joint embedding of recipes and images that yields impressive results on an image recipe retrieval task it also has an image to recipe retrieval where theye evaluate all the recipe representations for im2recipe retrieval given a food image the task is to retrieve its recipe from a collection of test recipes it also includes some embedding properties like word2vec they basically use cnn for encoding the image and rnns to encode both the recipe and the instructions and then have a joint embedding for the recipe and instructions their embedding is created using a cosine similarity loss and one semantic regularization loss for introduction of rl to image captioning i ve seen the they incorporated rl by having their deep q network to learn through action the next word of the imagecaption state the current words on the caption on time t and reward being some score i was wondering how do i introduce deep rl for this scenario on embeddings hopefully you can help guide me
1,is there a way to untransform points in an ordination forgive me if i m using the wrong vocabulary this might explain why i haven t been able to successfully google the answer yet here is what i have going on i have created an ordination using principal coordinates analysis pcoa based off the first two axes i have plotted a set of multivariate data in 2d my question is there a way to untransform the x and y coordinates of a specific point to return the original multivariate data what is that called and how easily can it be done in r thanks
0,lowball offer or am i being greedy hi all i apologize for the forthcoming long post but i’d appreciate any insights regarding negotiating walking away from my recent offer i recently got my first data scientist offer after spending the last few months actively interviewing for various analyst and data scientist positions for some background i’m in the midwest area of the us i have 2 yoe in data analytics and i’m currently a lead data analyst managing a team focused on building internal nlp forecasting and other machine learning models along with various reporting dashboarding responsibilities using r python tableau etc i also have a b s in stats and i’m working on my m s i’ve been excited about this position as it is very involved with building and deploying customer facing machine learning models i was even more excited to get an offer but i’m now feeling fairly disappointed after receiving an offer of 65k in my interviews with other companies and from my own research i’ve never even discussed seen a salary this low for a data scientist position it seems to match what i’ve seen and been offered for other analyst roles and i know this isn’t a case where the position is just named data “scientist” while actually being more of an analyst role am i being lowballed or does this offer make sense for someone starting their first “real” data scientist position i’m typically interviewing 3 times a week so i don’t want to undersell myself but since this is my first offer i’m worried i might be missing out on a good opportunity am i being greedy expecting more than 65k regarding my experience or what range should i be expecting tl dr first ds offer of 65k 2 yoe and current lead analyst was i lowballed or am i over evaluating myself regarding typical data scientist compensation edit thank you all for the great comments and advice you’ve helped ease my mind and i’ve followed your suggestions to try and negotiate but they don’t seem interested in budging i’ll take what i’ve learned from you all on to my next offer thanks
0,how to explain your projects hi i am switching from pmo to a data science role and got one interview today for a part time role the manager asked me are you working on any ml projects i told them i am working on 2 projects one is with a company and another one is with a university manager asked me what models you use and i told linear regression to predict the values and we are still in data preprocessing like cleaning the data imputing the values and stuff then later manager didn t ask any question and asked me whether i have any question for them then the interview was done in 15 mins now i feel i screwed up what to do in future to prevent this scenario please help me
0,how do you pronounce epoch i ve always pronounced it as eeee pock which is how my comp sci professor who first taught me neural nets said it but i hear people say epic or eh pock all the time and it really irritates me for some reason how do you think it s supposed to be pronounced in a data science context edit i ve learned from some commenters that the american pronunciation is supposed to be eh puk like epic and the british pronunciation is supposed to be e pock but i swear i hear some people sort of meet in the middle and use eh pock as well
0,research or internship in prep for phd applications hello all i’m currently an undergraduate stats major who will be a junior in the fall my goals is to apply to phd programs in senior fall if i wanted to look at opportunities for the summer prior to it should i look into doing research within say the stats dept or should i be trying to look for an actual internship at a company my research interests are within statistical learning so i was thinking a research role would be better suited for me than some data analytics position at a company i feel that it will be hard for me to get on any papers or do research because i won’t have a ton of theory knowledge but i’m hoping i can get on something more applied so what do you think i feel like trying to get a research role would be better when applying then doing sql all day at a company for a summer chances are even if i expressed my case as i want to do some more data science and quantitative sort of internship it wouldn’t be as good as if i did research with a prof i will be applying to stats phd programs
1,career msc statistics vs msc in business analytics hey guys i am a recent graduate of statistics and economics and i am wondering which masters program you would recommend my undergrad curriculum was very intense like 20 125 graduated and included all maths prerequisites for calculus up to iii and linear algebra econometrics stochastic models time series not economics based survival models multivariate analysis statistical machine learning predictive analytics survey sampling and bayesian statistics all at masters level our undergraduate classes were the same level as the masters programs same resources classes attended just slightly different assignments the only important modules in the msc statistics that i haven t done are data mining non parametric statistics and data programming in python sas i can only code proficiently in r but i am currently learning python sql and tableau on my own the business analytics msc is much more qualitative with an emphasis on visual analytics data management operations research and business intelligence i am not sure if it would be worth it getting a an msc in stats if i have already covered so much already which degree do you think would be the most versatile for obtaining a high paying interesting job i am interested in consulting ecological statistics finance and data science would it be more worthwhile doing a conversion degree for computer science thanks
1,can i use one way anova here hi for different sets of data 4 groups i have been asked if assumptions of variance equality were met since i used one way anova to test for statistical significance i am not an expert on this at all so would appreciate any insight you might have for me for some analyses variances were unequal based on bartlett s test using graphpad prism based on my research and limited understanding this may not be an issue and i could still use one way anova if the samples have the same size is that correct does same size mean that samples have the exact same number in my experiments the sample sizes are roughly the same as in 45 54 or 30 45 so can i still use one way anova or would i have to change my statistical test
1,some help understanding mixed model in r i have very simple data y dependent variable continuous x1 independent variable 1 continuous x2 independent variable 2 continuous x3 independent variable 1 discrete my hypothesis is x1 x2 influence y additionally x3 not only influences y but also changes how x1 x2 influences y is the following code in r correct for above model lmer y x1 x2 1 x1 x3 1 x2 x3 data data
2,the difference between a blurring matrix and a psf in image reconstruction i m working on a research project related to deblurring images and i don t fully understand the difference between a blurring matrix and a psf i understand that to apply them to an image you use two different operators but why is that what is the difference between them is a psf not a matrix how are they related to each other do they affect the image the same way
0,how hard is it to switch from data science to software engineering let s say someone mastered python out of necessity for the data science field wouldn t it be too easy for them to just learn other frameworks technologies software engineering related which would eventually land them software engineering jobs edit i m not moving to or from anything i m really interested in data science and technically i only have the basics in both fields se from my university courses it s just that i saw a lot of people saying how data science is becoming so saturated and in my country the opportunities for se far outweigh the data science ai ml ones so i was kinda worried that if i didn t find something data science related abroad that won t be the case hopefully i would be able to switch to se easily
1,what is the culture of a statistics work environment would anyone be willing to share their experiences preferably looking for those who have worked in the uk to share but i would still appreciate experiences in different countries hi i have posted this on a few subreddits already but i would love to hear as many people s opinions as possible i m looking into to studying statistics or applied statistics i was wondering what the work culture was like is there a lot of pressure the pressure stress of the job is something that i am particularly interested in hearing people s opinions on so any input would be appreciated or does it vary by employer assignment and the field you are working in thank you
0,can anyone recommend a higher quality version of this picture i drew this picture a regression model is being used to estimate the y value for some points and error bars that look like mini normal distributions are shown for each point can anyone please recommend a higher quality version of this image something from google images etc i spent some time looking but i couldn t find anything that exactly matches what i am looking for thanks
2,similarity of decision tree and bagging decision tree most of peoples know the advantages of decision tree and bagging decision tree random forest 1 but what is the advantages of decision tree that bagging decision tree rf still retains after implementation 2 what is the similarity in terms of characteristics between the bagging rf and it base classifiers
0,technical interview timed task no pandas just had an interview task for a data engineer role where i had to filter by date find some codes in different columns etc i read after that i was not allowed to use pandas or any external library is this a ridiculous ask i feel like this doesn’t represent the working environment at all
0,what should i expect for the first week or so at a ds consulting firm i am starting in a new role as data scientist at a fully remote consulting company next week this will be my first full time position as a data scientist previously worked as an independent contractor doing ds work for a couple of different startups after my phd does anyone have any tips for starting out in a role like this anything you wish you had known before starting
2,convect instant serverless deployment of ml models i’ve recently launched convect and would love for folks to try it out convect deploys machine learning ml models to instantly callable serverless api endpoints using convect jupyter notebook users can deploy trained models from their notebooks and share them with the world in seconds no web development or infrastructure experience is needed convect simplifies the process by being more opinionated than other model deployment workflows to give convect a try visit you can also try out models without signing into an account on the demo page i would love your feedback some background context deploying ml models to be used in production entails a different set of skills than training models in a sandbox environment and can get pretty complicated depending on what you’re trying to do for many data scientists this “sandbox” environment is a jupyter notebook one common approach to “deploying to production” that i’ve seen is turning a model trained with scikit learn in a notebook into an api endpoint from my experience there are a few ways to deploy a model to an api endpoint and all of them involve a nontrivial level of effort and time examples of some of the steps in the process include pickling a model and uploading it to cloud storage dockerizing a model’s prediction code and environment deploying a flask app or getting set up with an ml framework e g mlflow or platform e g sagemaker so you can use the deployment feature in their sdk while complex workflows make sense for deploying complex models i haven’t seen any dead simple deployment solutions for simple models and that’s what i am working on building with convect in this case simplicity comes at the cost of flexibility i e you give up the ability to customize your infrastructure and runtime environment in exchange for a simple one click workflow my hypothesis is that this tradeoff is worth it in many situations and i’m curious to see what people are enabled to build when this aspect of the ml workflow is drastically simplified under the hood convect creates two artifacts by serializing 1 the model prediction code and 2 all the variables that are in scope in the python session at deployment time i’ve made this part of the deployment code public here these artifacts are then loaded and executed in an aws lambda function upon invocation by an api gateway endpoint i’ve talked with 50 data scientists at small to medium size companies 2 300 employees and many have identified deployment as a pain point in their workflows i’ve also spoken with a few data scientists who have indicated that this would help save time on their after work weekend side projects i’d love for people to try out convect and to hear about how you use it or how i can improve it to make it useful i’ve also put together a gallery of examples for training and deploying models to make it easy to quickly get started and provided example endpoints that you can use query or even build ml powered apps on top of find out more at thanks for having a look
0,what is with the assumption that if you are using the jupyter you are a noob i have usually read heard this on platforms and all that if you are using a jupyter notebook you are a noob why the assumption is it because the code is not production ready
0,julia has rendered pandas obsolete change my mind
2,looking for tips on training language models on large datasets i m phd student currently working with a small research group that doesn t have deep experience with my research area i have set of experiments i want to run on bert like models where the training data won t fit into memory and will require preprocessing most of the work i ve done in the past it was possible to do the preprocessing up front and load the entire dataset and run multiple training epochs are there any resources e g blogs posts or guides on tools and best practices for training language models on large datasets my definition of large here is like 20 million or so sentences currently debating if i ll frame the task as a masked word mlm prediction task which is expensive or discriminator task like electra which may be more efficient to train with my resource limitations but is completely new to me i m trying to figure out how to store the data is a database necessary or should i write multiple text files that can be lazy loaded how do i monitor the model training should i checkpoint every 1000 steps the experiments would be run on a gcp instance with a couple of t4s and i m working with pytorch lightning to handle some of the more complicated aspects of the training loop as a starting point any advice or pointer to resources would be greatly appreciated
0,do successful models defy the bias variance tradeoff in statistics we are always warned about the bias variance tradeoff simple statistical models are reliable but are generally unable to sufficiently capture the complexity within the data i e high bias low variance complex statistical models are able to capture complexity within the data but are generally not as reliable when generalizing to new data i e high variance low bias this leads me to my questions 1 are successful statistical models able to defy the bias variance tradeoff as a simple example consider the famous iris dataset kaggle competitions have shown us that statistical models can be made that perform well on both the training data as well as the test data are these statistical models defying the bias variance tradeoff now let s imagine a far more complicated problem and dataset but suppose that we are still able to create a statistical model that performs well on both the training data as well as the test data are we again defying the bias variance tradeoff 2 i have seen proofs that show how the mse mean squared error can be decomposed into a bias term and a variance term thus for a given statistical model for a fixed value of this model s mse if the variance is high then the bias must be low in order to compensate and vice versa my question relates to the following when people discuss the variance in the bias variance tradeoff they are generally interested in the variance of a statistical model s performance when dealing with unseen data since this unseen data might not even exist at the moment how is the bias variance tradeoff able to make claims about unseen data is the bias variance tradeoff a general idea with some theoretical foundations or is it mainly empirical 3 finally how does the bias variance tradeoff apply to real world models such as the self driving car alpha go and computers playing tetris or in the case of reinforcement learning models the bias variance tradeoff does not apply the same way it does in supervised learning models thanks
0,salaried ds no work kept on the bench i’ve been working as a data science data engineer hybrid for about 5 years i would consider myself a practitioner – i’m not looking for cutting edge methods to address common problems rather i’m just trying to deliver as much business value as possible to a client currently i’m in a consulting role but previously i worked for a big enterprise i’m in a situation where my consulting group does not have work for me at the moment obviously what you do in the consulting world is heavily dependent on what sales can pull in and they don’t seem to be doing a great job i’m salaried so even if they don’t have work for me i’m still getting paid lack of work does affect my bonus but overall not such a big deal i am content with my level of pay even without my bonus it’s been like this for a month or so but there are no clear signs of a big project in the future so this could continue for many more months while i’m on the bench they literally have nothing for me to do and my boss has told me so himself he said you can do trainings if you want to make good use of your time but otherwise there’s not much else to do i’ve been taking him up on that i’ve come to the realization that selling data science is a difficult chore for salespeople for a variety of reasons lack of technical knowledge can’t guarantee roi in many cases difficulties in pricing etc… for this reason i sympathize for them but it makes me worry my questions to the community 1 have you experienced anything like this at your work is this common and what should i do 2 should i be thankful to have this sort of “problem” or is this a worrisome position my company has made it clear that they have no intentions of laying me off any time soon and seem to have a history of keeping people on salary even without work 3 do you agree with the assertion that selling ds is hard or could this just be an issue i’ve seen with the companies i’ve worked for edit i do currently help with sales in a pre sales role sales generates leads and i m brought in to talk about the technical details and help sell the service
0,has anyone here had the experience of what i call an air raid style of interview where the panel of interviewers seem to be constantly trying to attack and poke holes in you i ve been back on the job market for the last month since being laid off and i have what may be a final interview on monday for a job i really want its kind of a dream job for me director of data analytics for a company i ve always wanted to work for i didn t even apply a recruiter sought me out personally i have the experience and qualifications i ve been in this business since i graduated in 2011 i have management experience and a successful track record i ve already had multiple calls with hiring managers and those have all been great hence why i have advanced so far however my next interview they said they are bringing in 2 outside guys from a data firm they contract to gauge my technical experience this immediately brought in fear because i ve been interviewing for many jobs over the past month and i ve already had 3 interviews that i considered to be air raid interviews where it seemed like i was under attack from the panel rather than being interviewed by the panel the first air raid interview i was facing a panel of 3 in a final interview setting and all of a sudden we get to a part where it was rapid fire trivia style questions i felt like i was on the fast money round of family feud because i was only given 30 seconds to answer each question the thing is it was pure python trivia a lot of questions over stuff i ll automate anyway that i didn t know off the top of my head and would have to reference my work the guy leading the interview the entire time during this had a smug skeptical look on his face like that blonde dude from napoleon dynamite who is always making fun of napoelon afterward the 2 other interviewers were trying to apologize without apologizing as you could tell they werent a fan of that part of the interview i did fine on everything else but that was a train wreck and i got the rejection email the next week then about a month ago i had another panel style interview going up against 3 guys who i felt were just trying to knock me down from the get go asking a lot of booby trapped questions that i was clever enough to figure out but there were a couple that got by me questions obviously designed to trip people up it didn t help that the microphones one of the interviewers had was horrible and i kept having to ask him to repeat questions but these guys were very smug and hostile and played off of each other as if it were some kind of game to ruin the dreams of anyone applying i got rejected by these people too and actually got a customized rejection email saying that my skillset was below their standards no shit if they are judging my skillset based on that interview they won t find anyone that meets their standards which is the case because i still get emails from linkedin telling me to apply for that job as they are still actively recruiting for it good luck finally i had a take home task that took me about 5 hours and i had to present it to a panel and they were really unimpressed not because of my conclusions findings work but because it wasn t visually appealing enough they actually wanted me to present my findings in powerpoint i used r markdown with fancy infographics and such i didn t realize i was being hired for a graphic design job i did what i was tasked to do and infographics were not in the task instructions it s not like my r markdown graphs and charts were ugly or off they told the story perfectly and accurately but its clear they were looking for someone who wanted to be an artist as well anyway i feel like these 3 experiences have prepared me for what i may be facing and i suppose there is a chance that these 2 guys they are bringing in to gauge my technical skills won t bombard me and will be totally cool but i am not getting my hopes up these two have already reached out to me and asked for my github account something i was not prepared for since i was never asked about it prior nor was it in the job description i sent it over anyway but a lot of my code was written for a one man audience me and i wasn t able to save all of my work from my past job as they blocked access to my work onedrive as soon as i was laid off so i could only save my work samples that i had on my own machine therefore i don t feel like my github is truly putting my best foot forward i had a good panel interview experience 3 weeks ago i made it to the final 2 but was not selected but it was still a great experience and i let the recruiters hiring managers know it they didn t try to pepper you with trivia or kaggle or leetcode and instead just probed you about your conceptual understanding of concepts when i was a hiring manager and was hiring people for my own teams i always hired on potential not pure ability i didn t hire them based on what they could do now but what i felt like they could ultimately become i feel like a lot of these interviews are structured around finding candidates with high floors rather than high ceilings anyway just kind of ranting as i mentally prepare for what could be another air raid interview anyone have any tips in case it gets to that point
1,the book of statistical proofs hi all i just stumbled upon this incredible book which holds a very large number of proofs and definitions i thought that was interesting and potentially useful cheers
1,predoc programs in statistics raship for students i am currently exploring the option of pursing a phd in statistics motivation mentioned at end i am currently doing masters in india at one of the top institutes i have good grades both in my bachelor s and also in master s till now no research experience but going to work on a research project in summers which may lead to a paper if i can prove some results my question to the community is that are there any predoc programs like those present in economics for students these programs allow students to work with an instructor for a year on a research project and also take some papers at the institute before ultimately going for a phd alternatively referred as raships i searched on google but didn t find anything will i have to contact an instructor personally to get such an opportunity is it possible to get funding for the same in us or anywhere else my reasoning for searching for a raship is to get admission into the top places for bayesian statistics duke columbia etc i am definitely not at that level right now but would like to give my best efforts to get there and work with the best minds alternatively what do i need to do to get admission in thes top programs for bayesian statistics also alumni from my course have been placed in top 50 statistics phd departments but none in top 20 in last few years all went directly for phd ps motivation i took a course on bayesian modelling this semester and really enjoyed it it is something in which i would like to work further i talked with my instructor and he said that the best way to work in this area is to do a phd and then search for opportunities in academia or corporate research
0,roadmap plan documentation for ds projects i just started a lead role with my company and have was curious if anyone else has done something similar basically i will be leading and helping develop efforts towards curating a use case list choosing of that list what we ll be working on based on business value and executing those solutions this effort started a couple of weeks before i began the new role so i am catching up our executive leader has some passion around developing and using some tools we do not already have so there is a bit of focus on graph analytics right now but other ml and ai solutions are not out of the question i just need work with my team to decide on the use cases and create a document for our plan i ve done this for other projects that were not ds related but was curious if anyone has any sort of template or guidance for a format that worked really well for them maybe even an outline of what should be included and how it should be organized i don t have a lot of direction since i am being asked to run with this so i thought this community might have some good suggestions thanks so much
2,anyone attended the oxford machine learning summer school has anyone attended the oxford machine learning summer school in the past i got accepted and am curious what the experience is like it s a little bit difficult for me to attend since it s occurring in a difficult time zone for more than two weeks and i am wondering if it s worth it to attend to those who have attended what was your experience like is it more than just a bunch of seminars are there hands on tutorials also overall is the summer school well known or prestigious and would it be valuable to a graduate student who is interested in a career in academia
0,how important was is work life balance in your mid 20 s and what did you do to maintain or destroy it hi i m 26 and work as a bi developer data analyst at a fortune 500 company my job pays well and i live comfortably but sometimes i crave a change a change of company a change of tools i use at the current job using outdated technology right now is kinda the only reason i want to switch then i think if i switch job it might be a better paying job but could be bad for my work life balance right now my work life balance is super my manager is absolutely fantastic knows his boundaries doesn t check my performance in terms of how many hours i m sitting on my desk i can stop working at 4 4 30 or 5 i won t be asked any questions i can work till 6 and i don t have to put effort in showing that my hobbies are in check to the seniors of this sub or people of my age what do you value the most in a job thanks
2,5 minute paper digest towards real world blind face restoration with generative facial prior gfp gan by xintao wang et al have you ever tried restoring old photos it is a long tedious process since the degradation artifacts are complex and the poses and expressions are diverse luckily the authors from arc tencent came up with gfp gan a new method for real world blind face restoration that leverages a pretrained gan and spatial feature transform to restore facial details with a single forward pass read the full paper digest reading time 5 minutes to learn about the degradation removing module generative face prior and channel split feature transform meanwhile check out the paper digest poster by casual gan papers gfp gan full explanation post arxiv code more recent popular computer vision paper breakdowns cips simswap gans n roses
1,question who prefers bayesian stats over frequentist stats and why i just found bayesian stats but dont fully understand the idea yet i understood the basics what they are doing and what´s the difference why would you prefer either bayesian or frequentists do you know any good technical textbooks on bayes the more math the better
2,defect passed the inspection from machine vision hello all i m new member here i solely create reddit account for gaining knowledge in manufacturing area need your help guys as i am curious about machine vision cognex keyence basler etc place where i work have several machine vision for ic packages defect detection for molding leadframe marking what i don t understand is sometimes the vision system cannot detect the defect even though when we try to do offline test or verification on the machine it could detect the defect on ic package fyi we use basler as 9 main camera and 3rd party vendor for software plus machine what do you think guys the common cause of this inconsistencies is it common problem in manufacturing industry as well
0,is it normal to feel guilty when you don t work much on a work day hi work from home has been wonderful ever since it has been implemented but i ve found myself not working much on days like today i just wasn t feeling like it i m not sure if it s a good thing or a bad thing about work from home do you guys have days like this too not sure if it helps but i m not missing out on any targets deadlines manager is quite happy with what i m delivering and i might even get promoted next year but today i didn t have much to do and i just felt like relaxing and listening to a podcast instead of upskilling or working on left over small tasks at work also i m a junior just finished my first year after grad school thanks
0,how do you organise your reading materials recently i ve been working in a research and development capacity and have found myself taking time out of my day to read more papers i m just wondering if anyone has any suggestions of software apps you use to keep your reading materials organised right now i m using notion and it s pretty good does the job i can create pages to take notes of each paper i go through but does anyone use anything perhaps more specialised to do this
1,example of berkson s paradox i was discussing berkson s paradox with someone and they used the example of an old baseball superstition they had if you were on a winning streak you do not wash your socks it is well known that if you wash your socks in the midst of a winning streak you will lose the next game obviously whether or not the team collaboratively or as individuals plays with clean or dirty socks has nothing to do with the outcome of the game all i know is if we were on a hot streak our socks could stand on their own he went on to say p loss dirty socks same logical value as p win clean socks isn t this more of the gambler s fallacy than anything i ve been learning about berkson s paradox recently and this example just made me doubt my understanding wouldn t comparing score and cleanliness of socks make for a better example of berkson s paradox
0,how do you handle large datasets hi all i m trying to use a jupyter notebook and pandas with a large dataset but it keeps crashing and freezing my computer i ve also tried google colab and a friend s computer with double the ram to no avail any recommendations of what to use when handling really large sets of data thank you
2,what do you think of boundary matching networks trimming videos on actions i have been looking temporal action localization these are methods that automatically trim longer videos into smaller videos where the smaller video solely contains a specific action that happens additionally they can also provide a class label for the action that is happening in the trimmed video example a 1 minute video of a driveway filled with snow where at some point a person starts to shovel snow for 5 seconds gets trimmed to just the 5 second video where a person is shoveling snow x200b while looking at all the different methods available i saw that the current most popular one already dates back from 2019 namely boundary matching network i also came across some other more recent methods like muses and temporal context aggregation network tcan x200b when i m looking at these methods they all seem fairly complex i do not mean that i cannot understand their workings i mean that they generally require a complex architecture tcan claims that it only needs 201ms to process a 9 minute video clip on a 1080ti this sounds pretty fast to me x200b what i would like to have some input on are there any other methods that i should consider for temporal action localization does anybody have experience with using this on fine grained actions example actions within a domain a soccer player running on the field should not be trimmed but a soccer player performing a tackle or a soccer player shooting the ball should be trimmed afaik this all just depends on how we label the training data of what actions are important i couldn t find any google aws ibm apis that allow you to do a task like this does anybody know whether they do exist or are my findings correct
1,is it bad to leave outliers in when you have different measures and different outliers for all of them i had to conduct an experiment and do a write up for my undergraduate degree so it doesn t really need to be scientifically valuable i have around 8 measures and some of them have 3 4 outliers but all of the outliers are different between them i already have a small sample size n 16 so removing them all would leave me with half of that when i try to run an anova
0,i just got offered a data science internship with amazon i ve been lurking on the sub for 3 years and just wanted to thank the folks who put together stats ml cheat sheets this sub really motivated me to take my undergraduate degree in biomathematics statistics and turn it into a masters in data science i use to think i wouldn t have the programing background or that i wouldn t have the technical skills people wanted it took a lot of my moving past my imposter syndrome as a woman in stem and working on my skill set but i ve gotten this far thank you all so much edit just came back to this post and saw all the support for any one interested i have been applying since september to internships and have since then applied to 83 positions reworked my resume twice ended up making my own website for my projects just to look better on paper and got 5 interviews at the end of march i have gotten offers so far from every place i interviewed at and used the smaller offers to ask amazon to give me a decision earlier which ended up working i only did 2 interviews with amazon before i got my team and offer which from reading online isn t common as they usually have a 3rd or 4th interview for interns its been a long process and a battle at every stage just 2 weeks ago i was resigned to the idea of a summer with no internship but here we are now
2,should we trust cloud ml platforms aws azure gcp hi all i am in the early stages of a deep learning project with medical images and videos due to the large size and quantity of these the use of cloud computing platforms such as aws azure gcp is imposed however in my team we have the well founded suspicion that at least google and probably the others are very interested in our data for this reason we are reluctant to use them do any of you know how are the contractual policies of these big platforms with the data deposited in them should we trust them are some of them more trustable than others thanks
2,could machine learning help to improve cheat detection on chess platforms hello everyone i have been wondering if it might be possible to create a neural network that helps with cheat detection on online chess platforms for example one trains a neural network with games played by humans and games played by engines there can be found enough training data on the net then when the engine is given a new game it gives out an information like this white black while the is the probability the ai thinks the side was played by an engine you think this could be become an effective way to detect cheaters in online chess
2,if you finetune gpt 3 are you cheating in its annual build conference microsoft announced its first gpt 3 powered application a code generator for the power fx data query language according to the microsoft blog “for instance the new ai powered features will allow an employee building an e commerce app to describe a programming goal using conversational language like ‘find products where the name starts with “kids ”’ a fine tuned gpt 3 model emphasis mine then offers choices for transforming the command into a microsoft power fx formula the open source programming language of the power platform ” the question is wasn t gpt 3 supposed to be an all purpose language model that can perform multiple tasks without being finetuned from the gpt 3 paper s abstract “here we show that scaling up language models greatly improves task agnostic few shot performance sometimes even reaching competitiveness with prior state of the art finetuning approaches ” there isn t much detail on what microsoft means by fine tuned gpt 3 model but a few scenarios come to mind 1 gpt 3 can t perform code generation on par with models that have been specially trained for this purpose its performance is not stable enough to release it as a product 2 the vanilla gpt 3 can perform quality code generation but it is computationally expensive wastes too much resources and can t turn in profits a smaller optimized version of the model can perform the same tasks at a lower cost this led me to the following questions 1 can the fine tuned gpt 3 model perform other language related tasks text generation question answering etc 2 if not do you really need gpt 3 for this in the first place i mean there are several other transformer based architectures that can perform specialized tasks 3 and finally if you finetune gpt 3 are you cheating in the sense that you re defying the original goal of the model on a separate note i think microsoft s gpt 3 generator raises a few other important issues 1 in its raw form is gpt 3 a scientific achievement or a product ready for commercialization again from the microsoft blog “this discovery of gpt 3’s vast capabilities exploded the boundaries of what’s possible in natural language learning said eric boyd microsoft corporate vice president for azure ai but there were still open questions about whether such a large and complex model could be deployed cost effectively at scale to meet real world business needs emphasis mine ” 2 does it even make sense to build products on top of the gpt 3 api given that microsoft has an exclusive license to the technology hosts the model and can basically do whatever you do with the language model faster better and at lower cost i m interested to know what the community thinks about this you can find my full analysis here
2,we need a new reviewing system nowadays there are two reviewing systems that coexist and both are broken in their own way the formal conference journal reviewing system it is broken because 1 only a small fraction of papers gets accepted so that the conference journal remains prestigious but there are many more papers that deserve to be published 2 bad reviews are rarely read and debunked by the community even when they are public 3 the reviewing cycle is too slow the informal social media reviewing system people post their opinions about new papers in the form of blog posts tweets reddit posts these reviews often lack the formalism rigor and completeness of a real peer review with some exceptions and many papers that would deserve the attention of reviewers are never reviewed what would be nice is a reviewing system that combines the best of both worlds reviews are posted directly on twitter and reddit where the community can read and discuss them reviews are moderated to make sure they are formal and not just baseless opinions or disrespectful rants anyone can write a review anonymously or not anyone can ask for a review about any paper of interest authors themselves or others this is what we are proposing to do at flying scholars what do you think
1,question what r 2 values are typical in engineering design of experiments developing a model of a physical process think how a material property changes with respect to temperature time etc and i m trying to find what range of r 2 values i should expect to be reasonable for this field i know alone it s not a good indication of fit so i m looking at s too but i don t even have a gauge for what a reasonable r 2 value looks like 0 4 0 8 edit brain fart moment changed to proper values
2,what are the best practices for fine tuning resnet i m trying to fine rune resnet 50 for a multiclass classification task i have around 10k samples and 10 classes about galaxies but they re not balanced i ve currently done the following steps with some decent results added on top of resnet 2 dense layers and a 0 5 dropout unfreezed the last 3 layers of resnet added weights to the loss function to account for the imbalanced dataset some data augmentation rotatation and flipping mostly what else could improve the results i m still new to this so i might have missed something important
0,how to pipe data efficiently i have a mariadb that i need to pipe data from and into a clickhouse database it appears that clickhouse does not have mariadb replication available only mysql what does it take to do this in a budget friendly way no vendor lock ins or proprietary software licenses as much as possible it has to have automated scheduled replication using cdc bin log alternatively can you suggest an easy to manage olap dwh other than clickhouse
2,i m new and scrappy what tips do you have for better logging and documentation when training or hyperparameter training hi i m relatively new to the field i m self taught like most and have a pretty scrappy approach to logging and documenting i want to have a more organised approach i m looking for tips resources or a discussion on what methods are available to be more systematic with the way i document my ml training and hyperparameter optimisation for example when training a model for a specific project do you save each hyperparameter configuration and output in a json or should i use a txt is one of these formats more suitable for hundreds of separate fittings how do you save your learning curves also a json file is there any reason to save each model you train thanks in advance edit i use sklearn mostly is there a universally accepted way to log or is it package dependent
2,a new baseline and codebase for self supervised learning ssl with vit swin transformer microsoft research github tech report highlights the first work to i nclude down stream evaluation for ssl using transformers provide results of the transferring performance on down stream tasks of object detection and semantic segmentation light tricks lighter additional tricks than in previous works such as moco v3 and dino high accuracy on imagenet 1k linear evaluation 72 8 vs 72 5 moco v3 vs 72 5 dino no multi crop aug using deit s 16 and 300 epoch pre training 75 3 using swin t and 300 ep training
1,is there a standard for measuring how noisy a signal is over time with respect to a ground truth so let s say i have a machine learning application and i want to compare two models let s say i have a signal output that ranges between 0 1 for 100 timestamped samples the ground truth correct answer would be 0 5 model 1 outputs 0 for 50 samples then 1 for 50 samples model 2 outputs 0 1 0 1 alternating for all 100 samples both models will have the same average error but one is less stable is there a way to measure this stability i m guessing the naive approach is just to sum the absolute difference between each subsequent sample is there an industry standard way of measuring this and what if the timestamp between samples is not uniform as in some samples have a different time delta between subsequent samples
0,data scientists extinct within 10 years i read an interesting article on medium that claims the data scientist title will go the way of the dodo in a decade the tldr is data science tools will become as ubiquitous as ms office products and everyone will be expected to be skilled in using them titles will transition back to reflecting domain knowledge rather than skills will data what are your thoughts do you agree will this impact how you approach your career path it reminds me of chandler making a lot of money for simple data entry i never understood why something so easy would pay so well but maybe the next generation will say the same of us
2,minimum requirements for loading gpt2 xl edited for clarifications hey quick question first of all sorry if this is not the right place for this question i m a beginner so i m wondering which would be the minimum requirements for loading the gpt2 xl 6 gb pytorch model ram is the key thing here i guess note that i am not talking about fine tuning but rather just load it i am using this repo to load the large 3 gb version via google colab and it works by changing a few parameters google colab has 13 gb ram but when trying to load the xl in the ram the python process is just directly killed for example would this vps configuration be enough 6 vcpu cores 16 gb ram 400 gb ssd 400 mbit s port thanks
1,what are the odds of rolling 6 dice and getting a 6 on all of them you get 2 re rolls and you can save dice basically yahtzee rules but with 6 dice first post so i hope this follows rules i just don t know how to calculate it
1,help with chi square for a research paper i ll try to keep this brief i m writing a research paper for psychology however it is being written in future tense because we aren t actually conducting the research covid i still have to put something in the results and data analysis portion and were being told we would use chi square i did fine in stats class because we were always given data and a study to work out but i never made my own my study consists of observing 60 total people walk by two different people begging on the street so 30 people each my hypothesis is that more people will give to someone who appears unable to work than someone who could work testing bias can anyone help me figure out the best way to word how i would work this out with chi square i think what s throwing me off is having two separate random groups of 30 i had to come up with something that didn t involve recruiting participants
0,what is the best tech stack data pipeline for apache superset
2,just discovered a new 3blue1brown styled quality ml youtube channel i m reading jax s documentation today and in there was a link to a quite accessible videos to get a deeper sense of automatic differentiation and it s actually very good what is automatic differentiation the video style is 3blue1brown inspired explains the topic from bottom up very accessible though not shy away from maths i see that the channel is still relatively small but already got some great videos on normalising flow and transformer if you like those too please go there and subscribe to encourage the authors to create more high quality contents
1,three way mixed anova bbw sig vs no sig interactions what now hello everyone i m desperately trying to have an answer for this but no luck so far my rq do students experience increased anxiety prior to their first laboratory yes can a short intervention reduce this initial anxiety is there a difference between gender males vs females or laboratory experience short vs long n 300 year 65 females 35 males 70 attend long laboratory 30 attend short laboratory study design anxiety measure 1 continuous variable intervention anxiety measure 2 same survey attend first lab anxiety measure 3 same survey post lab i figured three way mixed anova bbw the dependent variable is anxiety score the 1st between subjects factor is gender two groups male and female the 2nd between subject factor is lab experience two groups long and short the within subject factor is time which has three time points t1 t2 and t3 problem when i follow laerd statistics step 1 do you have a statistically significant three way interaction no step 2 do you have any statistically significant two way interactions no laerd if no – you do not have any statistically significant simple two way interactions – end analysis and write up if i listen to laerd there is nothing to write however by looking for significant simple main effects however i found that males reported lower anxiety levels when compared to females at t1 p 0 015 by further looking into simple simple main effects i found that at t1 males in the longer lab show higher anxiety than females in the longer labs and at t2 males in the longer lab show higher anxiety than females in the longer labs ofcourse the most important aspect is overall anxiety regardless of gender or laboratory experience significantly decreased across all three time points suggesting the intervention works t2 t1 and initial heightened anxiety was present t3 t1 my confusion is if i follow laerd s stats i only talk about not having three way interactions and two way interactions however as you can see i have quite a bit of findings i am facing this issue across another cohort as well dif year where another intervention showed no three way interaction but a two way interaction however according to laerd i can t look at simple simple comparisons for some reason any help would be truly appreciated
0,senior data scientist salaries in eurozone what s the latest senior data scientist salary range in eu especially countries like germany france and the netherlands in us tech companies based here high end consulting such as bcg and also the average european company glassdoor is so outdated and anyway does not have sufficient data points for senior roles
1,question how air car travel safety statistic is calculated always hear that air travel is safest but how exactly it is compared to other forms of travel distance traveled time spent in an airplane car
2,google s launchpad programming framework simplifies the distributed computation learning process a research team from deepmind and google brain proposes launchpad a programming model that simplifies the process of defining and launching instances of distributed computation here is a quick read google s launchpad programming framework simplifies the distributed computation learning process the paper launchpad a programming model for distributed machine learning research is on arxiv
2,combining images and other numeric features in a cnn i have been working a little bit on a traffic problem i have been given a dataset of intersections with a bunch of numeric features with crashes as the response variable however in the dataset i was also given the latitude and longitude so i wrote a script to query the google maps api to generate images of each intersection once that was done i went about trying to combine the features i already had with the features i already had but when i sat down to write up the cnn i realized i had no way of combining the image with the other features i searched around for a bit but didn t find anything concrete which was surprising so from what i did read you have two options 1 somehow imprint the the other feature data into the image and run it through a normal cnn 2 run just the image through the cnn to compress it down to a 1d array of information then combine that array with the other feature data in another nn has anyone had any experience with this
1,can someone explain effect size in a practical sense hi all i feel like i understood the concept but after doing some googling i am not so sure anymore suppose i fit a linear model y a b1 x1 b2 x2 e and my null hypothesis is h0 b1 0 nbsp i had thought that my effect size in this example would be the estimated b1 but when i did some googling around this subject most explanation says the effect size for a linear regression is f2 where f2 r2 1 r2 if i am only interested in the effect of x1 wouldn t it make more sense that the effect size is b1 nbsp not sure if it matters but this question stem from a power calculation which i am trying to perform thanks
2,georgia tech microsoft reveal ‘super tickets’ in pretrained language models improving model compression and generalization a research team from georgia tech microsoft research and microsoft azure ai studies the collections of lottery tickets in extremely over parametrized models revealing the generalization performance pattern of winning tickets and proving the existence of super tickets here is a quick read georgia tech microsoft reveal ‘super tickets’ in pretrained language models improving model compression and generalization the paper super tickets in pre trained language models from model compression to improving generalization is on arxiv
1,looking for a statistical method that allows me to spatially distribute a value within a cell to it s surrounding cells in a geometric series like way i have a grid with some cells containing vehicular activity and some cells that don t i d like to fill these cells that lack information according to their surrounding cells such that if the cell is next to it it gets a fraction of the cell s value for example o o o o o o o x o o o o o o q o o y o o o o o o in this example o and q are empty cells and x and y are cells with information in this case i will use 0 5 1 2 i to determine the maximum fraction the filled cells will distribute to their surrounding cells where i is the ring surrounding the filled cell i 1 is closest so the cells in the first second and third ring will get 0 25 x 0 125 x and 0 0625 x respectively since q is in the first ring of x it should get 0 25 8 times the value of x since there are 8 cells surrounding x in the ring that has allocated a maximum of 0 25 times the value of x and each should get an eighth of this maximum value also it should get 0 0625 24 times the value of y since q is in the 3rd ring from y and there are 24 cells in this ring in this way the filled cells should keep at least 50 of their initial value and distribute the other 50 ad infinitum to their surrounding cells i clearly plan to truncate this but that s besides the point i don t know if this procedure i have described has a name or if there is a better way to do it so i d appreciate any guidance here
1,sample trimming for supwald formulation hello i am currently working with some ivx theory in some part of a paper a supwald formulation is used to detect structural breaks where the breakpoint is unknnown and also the threshhold parameter is unidentified under the null hypothesis so far so good now they state that they used 10 trimming at each end of the sample i cannot find a good source explaining to me why this trimming is needed can anyone explain to me why we need sample trimming in the case of sup wald formulation if i recall correctly trimming is not needed for standart wald tests thank you in advance
1,is there a regression method that can estimate the beta of a stock that takes into account its debt equity ratio over the period the formula for estimating the beta using linear regression is r j a b r m that is you have the stock returns versus the market returns over many years and want to estimate the linear relationship the slope b is therefore the estimated beta but as the textbook points out the beta estimated this way reflects the average financial leverage of the company over the period so if you simply use the formula levered bata unlevered beta tax rate debt equity ratio the obtained unlevered beta does not represent the current leverage is it feasible to find the best fitting slope if i rewrite the relationship like this r j a b unlevered tax rate debt equity ratio r m so you have one more column in the data debt equity ratio
2,research preferential temporal difference learning hello everyone i am excited to announce preferential temporal difference learning a td style algorithm to learn the value function in the presence of preferences a joint work with my supervisor prof doina precup and it will appear at the icml this year the algo is very simple and it has many interesting properties i am happy to answer your questions here
1,how do you take notes while reading a statistics book hi it s been a year since i finished grad school and it feels like i need a refresher on the concepts i started reading islr and i felt like i need to take notes so that next time i need to refresh on the concepts i can just go through the notes instead of reading the entire book the note taking is primarily because of potential future interviews do you guys just do it old school by taking notes in a notebook or do it differently now also is there anything else i should do in order to prepare notes for interview prep any other advice is welcome thanks
2,large dataset overfitting epochs iterations and batch size i have a very large dataset and i want to make a statement on how of that dataset i need to attain a certain accuracy for my predictive model which is a neural network i use early stopping criteria observing validation loss with a particular patience all this is implemeneted on keras i started observing that as i increased my dataset after a certain point my test error on an unseen dataset started increasing after thinking a second about it i came up with the thought for each epoch we do n iterations given by training set size batch size which then means that as i increase training set size i do more iterations per epoch thus i do more gradient updates per epoch empirically i observe that as i increase my training dataset my early stopping criteria kicks in faster i e i use less epochs overall does it make sense to think that i am converging faster to a minimum when i have a larger dataset because i do more updates per epoch and if i am not careful e g have a too large patience for the large dataset can i actually accidentally start overfitting because i use a larger dataset
1,will effect size estimates be more accurate with a larger sample my question is if i run 50 studies with 100 participants and 50 studies with 200 participants all examining the exact same effect in the same way will effect estimates be more uniform in the second case i know that effect size is supposed to be independent from sample size but it makes sense to me that with a larger sample size we should get a more accurate estimate of effect size this seems to mean that these estimates will vary more so across smaller studies is this true and then as a kicker will they vary more as a function of the effect size that is if there are 50 studies examining an effect with an effect size of cohen s d of 1 and 50 studies examining an effect size of 5 does it make sense to expect more variation in effect size estimates in the studies on the smaller effect
1,beginner question regarding chi squared i have a couple questions about the chi squared statistic 1 i have seen around that the chi squared is given by sum residuals 2 sigma 2 in a fit of a line to data how do i calculate sigma if i don t know the errors on the data points i m fitting my function to 2 i have also seen that the chi squared is just given by sum residuals 2 for example here what is the difference and which is more acceptable edit this specifically applies to the levenberg marquardt method for fitting i want to know how one would calculate the chi squared after having found best fit parameters through the lm method
1,need suggestion on distribution selection given that x is a iid rv that are positive real numbers 0 1 2 3 where p x 0 0 7 p x 1 0 1 p x 2 0 05 and e x 2 7 no more information was given what discrete distribution would you fit this data set with
1,does anyone know how to make the second graph from the first graph in this example here what are the steps required to convert graph a into graph b how do you decide the shape and size of the oval contours how do you decide which points end up in which contours thanks
2,a mathematical guide to complex variable optimization in the field of ai the adaptability of imaginary numbers is sometimes overlooked when contrasted to their real valued equivalents the added domain information contained in these numbers can enable substantially richer representations i m here to announce the release of the first of two reports featured on weights and biases which delves deep into the math underpinning complex variable optimization and includes a regressive example in tensorflow to demonstrate its utility you can find it here the goal of this series is to encourage ml researchers and practitioners to indulge in complex numbers and representations in their research any feedback or suggestions are most welcome
1,appeal of occam s razor in statistics and machine learning people often refer to occam s razor in statistics that simpler models are preferred to complex models provided both models have similar performance why should we prefer a simpler model does this have to do with the variance bias tradeoff
2,implementation of kernelshap on numpy i implemented kernelshap on numpy i also recorded a video on details i tried my best at least
0,how does one become a better communicator hi all as the title suggests off late i have been struggling to communicate with non technical stakeholders efficiently for example there was this one time where one of the stakeholders asked me to explain what a model did and i ended up combining a few technical terms together in a sentence which ended up confusing the stakeholder even more it is not that i suck at communicating in general if i were to have a conversation regarding a simple analysis explaining some viz i do it pretty well but when it comes to explaining something more technical to someone with a non technical background like the inner workings of the model i mess up big time my initial thought was that i might be messing up the explanations because i don t understand the models well enough and i have been reading up all the basics from scratch in hopes that it would help me become better at explaining concepts while i do continue my re reading of the basics are there any other ways that i could improve my technical explanations thanks
1,is using a rate per 100 000 people bad practice when your population set includes populations under 100 000 i m looking at various covid statistics and something like 0 00105 deaths per capita isn t a very user friendly number 105 people per 100 000 is easier to conceptualize if i want to be able to compare all of europe i have to apply that per 100 000 to germany but also san marino and liechtenstein is this fine or is it exaggerating the small countries statistics
1,what’s gone wrong with my linear discriminant analysis lda so i have 4 groups each with 3 dependent variables vectors and different sample sizes in each group i firstly performed a manova which involved calculating the matrices for the sum of squares between e and within h and found an f value more than the critical value so i know there’s a significance difference in the variables across my groups what i want to be able to explore is which variables seem to be the main differences between groups so my next step was trying to do lda i already had the matrices so just needed to calculate the eigenvalues of the matrix e 1 h this gave me 3 eigenvalues and eigenvectors which is fine and i chose to keep the first 2 the strange thing is my eigenvectors were v1 0 999 0 001 0 002 and v2 0 999 0 001 0 001 so the “x component” of the eigenvectors seems to dominate when i then project my original data in terms of the first two linear discriminants v1 and v2 and plot ld 1 vs ld 2 all the data lies on a line i expected the groups to break out into different areas of the plot but instead they all lie together it’s almost like it’s plotted in 1d even though i’m making a 2d plot any ideas what’s gone wrong here
1,omitted variable bias in non linear binary outcome models i am trying to analytically derive the omitted variables bias for binary outcome models in particular i am trying to get a deeper understanding of scaling bias do you all have any sources that would help with this
1,recommendations for a bayesian stats intro textbook meant to rewire my frequentist mind this had been asked probably many times in this sub but i swear this is a special use case i’m a undergrad stats student who has taken a stat inference course just this past semester and it was all frequentist i want an intro bayesian stats textbook which really is an approach to remove the “brainwashing” of my frequentist classes something that really motivates ideas not a lot of theory yet does have some math for reference i’ve taken calculus up to multi variable probability and i have some background in linear algebra but very little as i’m taking linear algebra next semester but really a textbook which just really makes it a point to rewire someone with a frequentist mindset
1,please help with primary data methodology in research proposal so i’ve got a business writing course where i have to do a research proposal with a survey or interview attached i bit off a little more then i could chew and designed it as a one group pretest post test intervention study to see what effect a meditative practice called yoga nidra would have on university students in terms of anxiety depression and academic performance our professor in the primary data methodology section asked those of us doing surveys to post the number of participants needed for the study to be statistically significant the method given is to find this out by using a online tool and plugging in a 95 confidence level confidence interval of 3 and total undergraduate student body i came up with the following in my proposal “there are 13 620 current students at the university using a confidence level of 95 and a confidence interval of 3 it is determined that 990 students will need to participate in order for the study to be statistically significant” because of the nature of my pretest post test intervention study i’m under the impression that this isn’t sufficient and i’ll need to do more complicated statistics math involving power analysis i have no idea how to do this as i’ve never taken a statistics course will that be necessary the teacher i don’t think will be expected something that complicated as this is just a business writing course should i just leave it as is and not work this out if i should how do i even begin thanks any help is much appreciated
0,sql vs pandas why bother mastering sql when you can simply extract all of the data using a few basic select commands and then do all of the data wrangling in pandas is there something important i’m missing by relying on pandas for data handling and manipulation
0,what is one single essential tool program skill that a new person absolutely must master when transitioning into a data science analyst role view poll
2,cvpr panels with richard socher olga russakovsky huggingface w b anyscale msft google etc what should we ask them hi r machinelearning cvpr is starting this week we re holding two cvpr panels on the future of datasets and next gen ml infrastructure if you could ask one question to one of those attendees what would it be comment below if you want to ask a question on the topics mentioned and we will do our best to include it in the discussion 1 cvpr pre game the future of datasets when tomorrow june 18th at 12 pm edt 9 am pst clubhouse link topic currently when companies train their ml models they focus on optimizing their models rather than the actual data but data sits at the core of a good model how can we be more data centric in ml guests include olga russakovsky imagenet challenge co author princeton richard socher imagenet co creator ceo you com jeff boudier huggingface chief of product joseph gonzalez uc berkley riselab jianing wei google ai siddhartha sen microsoft research 2 cvpr panel next gen ml infrastructure for computer vision when monday june 21st at 3 pm edt 12 pm pst clubhouse link topic there are few existing solutions for data centric ml in this discussion we explore tooling and infrastructure to get the most out of data guests include tobi knaup ceo co founder at d2iq lukas biewald ceo at weights biases waleed kadous head of engineering at anyscale glenn jocher yolov5 creator ceo at ultralytics tianqi chen cto at octoml dillon erb ceo at paperspace josh tobin ex open ai ceo at gantry davit buniatyan ceo at activeloop btw clubhouse is now available both on android and apple if you d like your question to be asked please comment below and we ll pick the most voted questions try to cover as many of them as possible also let me know if you d like to attend and need an invite see you at cvpr and thanks for the tips
1,what does a negative r squared value mean how do i interpret this i ran a glm model in r my r 2 value is 0 001431
2,collaboration for ml research project i m a data scientist at nalagenetics a biotech startup in indonesia and we are looking for a corresponding author with expertise in ml to guide us for a high impact and value project for healthcare workers in indonesia please dm me if you would like to learn more and collaborate with us
2,pytorch to desktop app deployment i have trained a model through pytorch and wish to bring it into production through a windows desktop app for real time inference my current plan is to convert my pytorch model to a torchscript then in actual c inference i ll then use libtorch to call the torchscript formatted model my goal is to integrate this into a desktop app and run it in real time on a stream of data also i plan on only running on cpu not gpu does anybody have any advice on how to best decrease latency and throughput i really want to optimize efficiency and i m wondering if i m missing anything is there any hardware cpu specific optimizations i could look are there any benefits going to onnx or even tensorflow over pytorch thanks
0,what jobs and sectors can i get with a data science degree hey guys so i’m undecided yet on what i want to major in but data science has caught my eye i was just wondering what specific entry level jobs could i get out of college is it so in demand that you can work in pretty much any sector is there such thing as an associates in data science im not very good at math but i plan on taking remedial math in college assuming they have that will i be fine what kind of internships can benefit me to set myself up for success
1,is survival analysis an appropriate application if i want to study likelihood of community college students graduating in a particular quarter what i want to do determine the likelihood that someone graduates with an associates degree after a particular number of quarters in community college what is the likelihood they graduate in their 8th quarter at school what is the probability that someone graduate any time before their 8th quarter data i have demographic information and graduation quarter of each student going back 20 years what i imagine this looking like correct me if i m wrong 1 correcting for censored data such as drop outs and type 1 censoring 2 reference distribution plots to find the appropriate family 3 survival analysis summary table and plot am i going about this correctly thank you for your help
2,project rpg learning recursive point cloud generation want to have a lightweight and efficient point cloud generator with many additional features for free check out our recent work rpg learning recursive point cloud generation our rpg starts from a single point and gets expanded recursively to produce the high resolution point cloud via a sequence of point expansion stages similar to the idea of 太極生兩儀兩儀生四象四象生八卦 where you can not only perform the interpolation between point clouds easily but also unsupervisedly obtain the hierarchical semantic segmentation of a single point cloud or across many project page paper
1,why do we believe stochastic processes are not actually deterministic is it because most processes cannot be studied in strict isolation and are always subject to secondary drivers what we call noise and thus influence the underlying process itself or is the prevailing belief that there will always exist some randomness to every process no matter how strictly we isolate it the former implies that given sufficient computational power and insight we could simply model these conditionalities via some extremely multivariate model and or additional models to explain the noise does it not that we could conceivably drill down the uncertainty noise down to a series of deterministic models that when run in unison could perfectly predict any process i know this is of little practical importance but i am just curious
1,question how to calculate the standard error of the difference between regression coefficients of different regression i am performing analysis for my master thesis and found a paper that gave me a good idea on how to analyze my data in this i am performing an ff3 regression in order to check whether the 2 portfolios are significantly different from each other in this i ve got excess returns for 2 portfolios for the same time frame the thing i would like to create now looks like this alpha rmrf smb hml high esg portfolio 0 0029 1 1882 0 0069 0 2420 0 0015 0 0376 0 0698 0 0582 low esg portfolio 0 0013 1 086 0 0725 0 1324 0 0016 0 0392 0 0728 0 0607 difference 0 0016 0 1017 0 0795 0 1096 what i am looking for now is how to calculate the standard error of the differences between these coefficients based on the regressions outputs how would one go about this to see whether the difference in returns and risk factors is significant
1,my mom is big on astrology and i just watched a guy on the cut on youtube guess 4 12 people s signs right so as the title says i just watched an astrologer guess the signs of 12 people and managed to get 4 of them right given 12 people with 12 different signs none repeat how many are you likely to get right accidentally
1,question how differencing a time series leads to stationarity when we talk about stationarity of a time series the properties we attribute are constant mean constant variance constant autocorrelation structure and no seasonality i came across several articles and books which says differencing makes a series stationary the illustration normally begins with taking an ar 1 model as example yt ɸyt 1 ɛt also it is assumed that the above equation has a unit root i e ɸ 1 then they difference the series and represent it as below δ yt yt yt 1 ɛt they then claim that the error ɛt in ar process is white noise white noise by definition has constant mean finite and constant variance and no correlation structure they then go on to take expected value of the error term and say e ɛt 0 thus implying that mean is constant ok so far we have ticked one of the check boxes to prove it is stationary the the other check box i e to prove constant variance remains unchecked the next part is what is very unclear and often left unproved they say the variance of the error term is var ɛt σ2 then without proving that variance is constant they say differencing has lead to constant mean and variance my question is how can one prove that differencing leads to constant variance would be grateful for any explanation
2,czech speech to text hi i need speech recognition from microphone for czech language i m looking for either pretrained neural network model or usage of online api preferable from google but i cannot find good example how to use it i only found this but it didn t work
0,favorite podcasts and other audio resources for learning and staying up to date what’s everyone’s favorite data science related podcasts youtube channels and audiobooks i spend a lot of time in the car and would love to learn while i drive thanks
0,weekly entering transitioning thread 28 mar 2021 04 apr 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
2,collusion rings in cs publications this issue is huge what do you think about it and how to cope with it
0,knuckle dragger seeking advice on what courses to take so that i can analyze data on the sp500 what i am looking to do 1 scrape data of sp 500 company names and prices at specific date and end date 2 compile the data 3 organize companies by return over a 10 year period my goal see what of all sp 500 companies end up with different returns for example what of companies end up being 10x or being removed thanks in advance i haver zero skills related to this
2,does anyone know of coreference resolution tools where you can specify the entity hi let me elaborate on the title i m currently working on paragraph level data and want to perform coreference resolution i ve tried working with spacy s neuralcoref and although it works great it receives a string as input and returns all entities and mentions it deems appropriate rather than that i m looking for something where you can specify the entity and the model will return all such instances for that particular entity does anyone know if something like that exists thanks
1,weekly r statistics discussion what problems research or projects have you been working on please use this thread to discuss whatever problems projects or research you have been working on lately the purpose of this sticky is to help community members gain perspective and exposure to different domains and facets of statistics that others are interested in hopefully both seasoned veterans and newcomers will be able to walk away from these discussions satisfied and intrigued to learn more it s difficult to lay ground rules around a discussion like this so i ask you all to remember reddit s sitewide rules and the rules of our community we are an inclusive community and will not tolerate derogatory comments towards other user s sex race gender politics character etc keep it professional downvote posts that contribute nothing or detract from the conversation do not downvote on the mere fact you disagree with the person use the report button liberally if you feel it needs moderator attention homework questions are generally not appropriate that being said i think at this point we can often discern between someone genuinely curious and making efforts to understand an exercise problem and a lazy student we don t want this thread filling up with a ton of homework questions so please exhaust other avenues before posting here i would suggest looking to r homeworkhelp r askstatistics or crossvalidated first before posting here surveys and shameless self promotion are not allowed consider this your only warning violating this rule may result in temporary or permanent ban i look forward to reading and participating in these discussions and building a more active community please feel free to message me if you have any feedback concerns or complaints
2,preventing index collapse in vq vae i keep experiencing index collapse wile trying to train a vq vae on raw audio more often than not the model just converges on picking one or just a small subset of the codes what are the ways to prevent this i was thinking of using something like batch norm but my batch size was too small
2,final deadline extension call for papers kdd 2021 workshop on bayesian causal inference for real world interactive systems august 14 15 2021 x200b submission deadline extended may 27 2021 anywhere on earth format 3 page extended abstract references appendices acm proceeding template submission website x200b increasingly we use machine learning to build interactive systems that learn from past actions and the reward obtained theory suggests several possible approaches such as contextual bandits reinforcement learning the do calculus or plain old bayesian decision theory what are the most theoretically appropriate and practical approaches to doing causal inference for interactive systems we are particularly interested in case studies of applying machine learning methods to interactive systems that did or did not use bayesian or likelihood based methods with a discussion about why this choice was made in terms of practical or theoretical arguments we also welcome submissions in the following areas offline evaluation of recommender and interactive systems comparison of bayesian off policy and other heuristic approaches for offline metrics probabilistic approaches applied to contextual bandits and reinforcement learning approaches probabilistic approaches to incrementality and attribution non bayesian approaches and trade offs with bayesian likelihood approaches bayesian methods in a production environment x200b organizers nicholas chopin ensae mike gartrell criteo ai lab dawen liang netflix alberto lumbreras criteo ai lab david rohde criteo ai lab yixin wang uc berkeley
1,anomaly detection i work in the industry where people are often interested in knowing if the things continue to work normal when monthly report from some sort of the site comes through they would like be able to use that data to assess if things are normal there i am studying statistics and in last couple of semesters we have been focusing on model selection and variable selection but that is pretty much where things stop so lets say that i have some response that is a proportion lets say it is reasonable to use logistic regression after doing some analysis we end up with a good model based on 3 factors from a historical monthly reports lets say logit p 0 30 0 5xfactor1 0 26xfactor2 0 12xfactor3 i wonder what would be next steps for moving to further analysis focusing on anomaly detection first of all is the above method of using historical data to build a model acceptable start for further analysis focusing on anomaly detection first thing that comes to my head is that i can build a prediction interval based on the above model and check if the new observations are in the interval if it is outside i would mark it as an anomaly it sound reasonable to me is it would there be any downsides to this approach how can i check if the interval is of any practical use aka what in the modeling process could indicate that the interval will be for example too wide for any practical use are there any better methods for anomaly detection what is a good read on the topic thank you for your time any comments welcome especially the one that could point out where i might be wrong or how to look at the problem from a different perspective
0,sales pitch for ds within australia large companies have something called grad programs these grads rotate through the business 3 or 4 times in 6 months stints these grads have to apply to an area of the business to work i need to write a pitch to get these new grads to apply to my area of the business question is if you are new to analytics data science why did you want to get into it what did you hear about it for you to think yeah that’s what i want to do
2,how do you work through a modeling project in a notebook do you follow specific steps or handle it free style i m currently helping to onboard some interns and was hoping to put together an in depth guide for working through modeling pipelines in a jupyter notebook style environment i m hoping to crowdsource a bit with understanding differing practices on these kinds of problems imagine a hypothetical situation in which you re provided a dataset for the purposes of trying to build a predictive model here s my current outline if you do something different in your day to day or if you think i m wrong about something lemme know 1 clearly establish the problem statement is there a model and metric already being used or is this a new project try to understand whatever limitations there are surrounding the current methodology is there a source of irreducible bias do stakeholders insist on a particular metric 2 load the dataset and do some exploration understand the types of features in the dataset categorical numerical otherwise and overall data quality is there a lot of data missing important things to find out if this is 4 data preprocessing feature engineering i e clean up text fields create dummies as necessary perform bucketing if necessary treat null fields missing values etc etc etc 3 data splitting devising appropriate train test dev splits is stratification important is data covariate shift apparent in your dataset note that some types of preprocessing can only take place after this step e g standardization normalization else you experience data leakage 4 put forth some models that you d like to try based off the problem type data provided and other concerns if you have to defend your results in front of a client interpretability might be important some models are known to work particularly well with e g text like naive bayes 5 tune parameters when you re ready use cross validation in tandem with gridsearch bayesian optimization to find parameters that optimize for your target metric 6 evaluate results on the dev dataset is your model experiencing high bias high variance take steps to improve your model s performance this can involve further feature engineering introducing regularization adding more data or any number of other steps 7 when you re happy with your model and the parameters chosen evaluate the test dataset to get a best estimate of your model s performance in the real world
0,this is going to be a rant so sit back so i started my engineering journey with electrical engineering but somehow got interested in data science i learned most of the things on my own i studied about things required to be in the cs field but felt like i knew nothing and hence asked a few of my cs students to do a project with me so i can learn they weren t that motivated so i did somethings myself i was learning and going good doing my own project though i didn t get any jobs in the field because i wasn t having a cs degree to be sitting in interviews plus due to pandemic the company wants better at a lesser price understandable to me somehow i got some research projects in the field of nlp and work independently suggesting my own ideas depending on the problem statement and data peovided now i am in an mnc as a data science and analytics intern with two other young team members who are very excited giving ideas and just talking they don t know how to do it ultimately increasing my work because they don t need the job as they are just students and i need it i don t want any bad impression from my side on top of that the project they gave us is of making a recommendation engine they are not giving any idea everything is on us i can make a recommendation engine like that item based user based anyone and can try to build a hybrid one depending on the data we have they asked us to look for the domain yourself it should have a unique feature too because at the presentation you have to tell that why the product is different i mean 8 weeks of time no data nothing need an mvp in 2 weeks data has to searched and it should have a unique feature plus this one intern irritates that heck out of me now i am thinking i was better working alone but i have seen some great teammates please advise me anything on this i don t know the feeling is right or not or is it the lockdown eating my head i am willing to correct my thought process but i need some advice from someone in the same field without being judged update just got the problem system even broader they need a personalization system not specifically a recommendation sytem
0,mlr vs caret for training statistical machine learning models in r are there any advantages of using the caret package compared to the mlr package do you guys have any preferences for instance i noticed that caret does not easily allow you to perform grid search on the max node size hyperparameter in the random forest model does anyone have any advice thanks
0,generating new data points with smote there is a well known algorithm in statistics called smote synthetic minority over sampling technique which is often used to balance and imbalanced data set if i have understood correctly the premise of the smote algorithm is as follows suppose you have a dataset containing information for medical patients that are healthy and not healthy but let s assume that the majority of the patients within your dataset are healthy the composition of healthy not healthy being 95 5 if you want to make a statistical model for this data the data does not contain enough information for not healthy patients and it will be very challenging to build a reliable statistical model that can make accurate predictions for not healthy patients thus the smote algorithm can fix this problem by 1 rebalancing the data set e g after smote your data set can have a composition of 70 30 2 creating new data points from the existing data as i understand this is done by multiplying a given vector corresponding to a randomly selected individual observation by some random number between 0 and 1 this leads me to my question suppose you have already have a balanced dataset e g the healthy not healthy has a composition of 60 40 but let s assume that you have a relatively small dat set to begin with e g 500 rows can you use the smote algorithm to create new data points so that your dataset is bigger i understand that no algorithm can magically compensate for data quality issues but at the same time i don t see any major flaws with using smote on already balanced data for reference i illustrated this process below using r i would be interested in hearing a second opinion thanks load and install libraries remotes install version dmwr version 0 4 1 library dmwr create some fake data and put them into a data frame called f var 1 rnorm 100 1 4 var 2 rnorm 100 10 5 var 3 c 0 2 4 var 3 sample var 3 100 replace true prob c 0 3 0 6 0 1 response c 1 0 response sample response 100 replace true prob c 0 3 0 7 put them into a data frame called f f data frame var 1 var 2 var 3 response declare var 3 and response variable as factors f var 3 as factor f var 3 f response as factor f response smote algorithm simulate new points from the first class smoted data over smote response f perc over 100 simulate new points from the second class smoted data under smote response f perc under 100 combine everything together into a final new data file final rbind f smoted data over smoted data under
0,team with no data science infrastructure knowledge crawl walk run i m in my first real data science job at a f500 med device company the team i am supporting is looking to implement smart features for a web application the team is all software developers with zero experience understanding of data science the previous work proof of concept for the work was a bunch of juptyer notebooks using static log data as inputs and we are working through which features to implement i m working to frame the steps of using data science ml in production to crawl walk run i e start small and work up from there considering there is currently zero infrastructure anyone been in a similar situation and have advice on how to frame the crawl walk run steps for a team with zero experience
1,masters with low undergrad gpa i graduated from fsu with a bs in statistics in december i immediately got a pretty well paying job out of sheer luck i feel around 85k after bonuses the company offered to pay for me to do a part time masters starting next spring the issue is that my undergrad gpa is incredibly low at 2 6 i did have some serious health issues throughout college since i have crohn s and did not have insurance for about a 5 year period i just kinda marched through the pain and constant bathroom use for those years which was incrediblly difficult for me but i m not sure if any college would even care i really do believe i can finish coursework now since i have my disease under control and i m fully in remission what are my chances of getting into a masters program with 1 year of work experience is there anything i can do to help my case when talking to graduate admissions in the mean time
1,taking the median of datasets hi all i’m working in the idl programming language which has a median function which i’m using on my data obviously taking the median of an odd amount of numbers gives you a middle value whereas taking it of an even amount gives you two in the middle which i believe you’re supposed to average for some reason in idl the default option is that doing e g median 1 2 3 4 returns 3 median 3 4 5 6 returns 5 i e it returns the number just past half way you actually have to specify “even” as a keyword so that it averages the middle 2 returning 2 5 and 4 5 in the above examples my question is is what idl does with even sets of numbers by default “wrong” if so why would it not automatically average the middle two numbers i’m trying to work out whether i need to reprocess all my data or not
0,ideal text analytics data structure one document at a time or one row at a time i just got tossed an interesting problem by management there s a lot of a certain type of document laying out agreements between labor mgmt at the local branches and a poor understanding at the top over what topics occur regularly in these documents it s over a thousand txt documents up to about 15k characters in each so there s a lot to look through and therefore text analytics to the rescue i just got finished reading them all into hadoop and turning them into a big table of the form row lineoftext originaldocumentname x200b before i get too far into the modeling part i m wondering if this is the best form to have the data in or whether i d do better with a format of row entiredocument originaldocumentname x200b my tools will be sas viya s text analytics primarily because i like the concept modeling part and some r package like tidytext and whatever i can find for concept formation r is my real go to language but i m a bit of a noob in text analytics and don t fully understand those packages yet i ve done some with python s nltk and similar packages although i m expert level in r and struggle some in python any thoughts about what s the best general format to use one line at a time or the entire document at a time by the way the documents come from a variety of sources like word pdf and ocr input and are pretty low quality in terms of misread or mangled words white space and weird control characters i m not sure there s an obvious way to skip the line feeds in a hive import to get to the second document as a row format but i can cross that bridge later and have a few databases whizzes on my team that could help me figure it out if needed
0,how much of a role should assumptions for statistical methods of analysis play in data science tldr as a data scientist how important are the assumptions for the statistical methods of analysis currently i am working on my ms in data science and i ve been thinking a lot about how my data science cs courses practically ignore basic statistical practices that i was taught in undergrad as a stats major for example the last two semesters i have taken two courses offered through the college of computer science taught by the same professor one data preparation and analysis and two data mining having taken a data analysis class in undergad a different university and offered under the college of mathematics as a statistics course i was very well equipped for this course but i noticed that the professor overlooked a lot of things that i was told is critically important to statistics and statistical analysis specifically assumptions and tests to see if those assumptions were met i didn t think much of it this course was designed for grad students and there are prerequisites that were needed that cover assumptions for various methods though not all of the methods in this course are addressed in the prerequisite courses the next semester i take data mining an undergraduate level course like i said same professor and a cs course i understand that data mining might not be heavily as heavily based in stats as cs with its basis in machine learning and ai but the stats is a piece during our final project i was discussing with some friends the trouble i was having meeting assumptions with the dataset given same project and dataset for everyone and asked them how they were handling it my friends no stats majors could not understand what my issues were when i was explaining to them the assumptions for a principle component analysis pca a large part of the project they said that i was making it a bigger deal than it needs to be and i should just run the pca with no check on the assumptions and move on like the example the prof provided us unable to get any help on my problem i did just that turned in my project and to my surprise i got a 100 i couldn t believe that i got no points deducted for not checking the assumptions the previous semester i had a project too my partner dropped the class in the middle of the semester so i did the project assumption checks and all on my own with little problems so there wasn t an issue as a statistician running analyses without checking assumptions raises huge ethical flags how can i know that the method of analysis and resulting prediction responses were right without knowing the assumptions were met i went on kaggle to look at other people s code to see how they handle assumptions in their various projects and competition submissions and not many had the assumptions addressed it made me wonder if in data science it was enough to run the analysis and predict as long as there was a good prediction accuracy no need to worry about the steps to verify the method of analysis was the right method for the data
2,mlp singer towards rapid parallel korean singing voice synthesis hey r machinelearning i m excited to introduce mlp singer towards rapid parallel korean singing voice synthesis it s k pop singer in the making a baby step forward in that direction paper demo code motivation many singing voice synthesis svs models use an autoregressive design in which acoustic features produced from the previous time step are fed into the model to generate the next set of mel spectrogram frames while ar models have their advantages they are prone to exposure bias and can be time consuming to train and sample solution we were inspired by mlp mixer an architecture exclusively composed of multi layer perceptrons introduced in the cv literature for efficient image classification we experimented with the model and found that mlp mixer can also be used in a generative context in the audio domain since the mixer block works with transposed latent features it gains a receptive field equal to the size of the input chunk this can be helpful for generating context aware representations advantages mlp singer is a non autoregressive parallel svs model hence its generation time is much faster than conventional ar svs systems we found that the inference latency of mlp singer on a cpu is comparable to that of an ar model on gpu mlp singer achieved a real time factor of around 3400 on an nvidia rtx 6000 gpu limitations remedies since mlp singer is a parallel model that generates mel spectrograms in chunks audible artifacts can be produced when generated chunks are sequentially put together to mitigate this issue we employ an overlapped batch segmentation method that gives the model more frames to look at near frame edges detailed explanations and experiment results can be found in the paper looking forward to feedback and discussion
2,the encoder of this deepfake model makes the input larger instead of making it small this and this deepfake model recieves an input of shape 64 64 3 or 12288 values and the encoder outputs of shape 8 8 512 or 32768 values the size of the output is larger than the size of the input but the job of an encoder is to compress the input but this encoder made the input larger instead of making it smaller and the deepfake still works why does it still works
2,binary survey data imputation thanks is advance developing a ml model from multi year survey data and am having trouble trying to determine the best way to impute missing values all the data features have been binarized except year what adds to my trouble is that not all questions were asked every year leading to consistent missing values for some years here are some of the options i came up with 1 i don t want to simply remove items with missing data as that would remove a significant portion of the data where questions were not available for that year so i could take that data as is and utilize a decision tree based approach similar to xgboost that handles missing data 2 i could fill the missing nan values with a large constant i e 9999 which would never appear in my binary data 1 0 then i can use any classification based approached 3 search for a data imputation technique for binary survey data am i missing anything are there other options available and what about the options that i have proposed above thanks again
0,a lot of people entering this field are like over fitted models no disrespect to ph d s just an interesting analogy lots of internal validation and creds but poor performance in the wild
1,solid textbooks on probability theory advanced hello i’m a statistics major who will be a junior in the fall i took a probability class already but i felt it was too vanilla and was only a surface level class for probability theory to be honest with you i think the class was just not all that i could have learned from a first probability theory class does anyone suggest any textbooks which go more in depth into probability i guess it could go into stochastic processes if it does but it doesn’t have to necessarily just something which goes deeper into the “theory” side of probability theory
0,weekly entering transitioning thread 25 apr 2021 02 may 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
1,t test help which tail and type this is probably a super easy question but im struggling to get my head around it i m currently investigating the effects of a strain on fungus on plant growth so i have tested the same species of plant under 7 different conditions 1 control no fungus 2 3 4 fungus a at 3 different concentrations 5 6 7 fungus b at 3 different concentrations i am going to undertake t tests to compare my results i just want to make sure i have the tails and type of t test correct i have undertaken a 1 tailed paired t test because i want to know if the mean of 1 strain is statistically greater than the other is this correct i ran an unpaired unequal variance t test to see if the results are different and it gives me massively different results so i want to make sure i ve picked the right test before i start my full write up asking reddit because my dissertation supervisor is scary and doesn t like simple questions lol x200b edit i know anova is suitable for comparing multiple but i have been specifically told by my supervisor to use t tests to compare the cons against each other separately
1,question would it be right to say normal distribution is popular but not the most prevalent in statistics and probability people often assume that the data generating process follows normal distribution most of the assumption about error especially errors in linear regression is assumed normal i have worked in data science and nlp i often find that the process does not always have a normal distribution even talking to data scientist statistician friends from different industry i get to know that normal distribution is perhaps a theoretical assumption but in realty it is never the case however detractors always point to central limit theorem and law of large numbers would it be right to say normal distribution is popular but not the most prevalent what examples can one give from their line of work or from statistics itself to prove this point
0,specialize in one area or be a generalist i’m wondering if i should stay a generalist there’s always stuff to learn or if i should hone in one niche area nlp computer vision causal inference or bayesian stats it seems like the ideal data scientist is an expert in all of those areas even though in reality it’s quite difficult we all sorta learn what we need depending on the problem we’re tryna solve update seems like most of y’all are recommending be a generalist unless i’m passionate about a specific area
2,project my own open source automl library my name is daniel and i m excited to introduce you my and another great developer school diploma project fully open source automated machine learning library we are beating built in automl in sap famous product github repository web application for users who don t want to code
0,euro 2020 predictions update edit u pedrosorio makes good points i ve changed the prior rankings to reflect the team elos from the start of qualifying and have also included uefa nations games what i really should do is model the team ability as a random walk in time or add some sort of competition tournament effect the model is good enough for me for now i appreciate all your comments though so please share last week i posted some predictions for the 2020 euro now that the first round is over we can examine some of my performance my predictions and results for the first round are shown in this table sorry it isn t prettier i achieve an average log loss of 0 92 where assigning all outcomes as equiprobable yields an average loss of 1 1 my multiclass roc for predicting the outcome is 0 77 in short in the first 12 games i perform slightly better than random guessing which is honestly fine for me however most people who have watched international football wouldn t assign all match events as equally likely is italy drawing turkey really as probable as italy losing to turkey no its hard for me to measure against a reasonable guesser my work pool records all our guesses and so at the end of the group stage i can use that as a sort of ensemble method to compare against we ll see here are match predictions for the remaining group stage games conditioned on the results of the first games the model is not perfect and still makes some weird predictions for example portugal is given higher probability to beat france than they are to beat germany even though france beat germany in the first round if you subscribe to some sort of sports law of transitivity this may sound weird my predictions for the second round and the results of the first can be found here
1,stata sas or r certified courses hi all i m looking to learn more than my very limited experience with r i was not overly talented as it seems like it s going to be required for any job i do in the future i m wondering if there are any online courses free or paid i m in canada if that matters that grant a certificate of completion or something along those lines is one of the three better to learn for biostatistics public health related uses i ve seen all three mentioned fairly equally so it would be great to have an idea of the best place to start thank you
2,will conferences like nips iclr aaai acl ever be in person again with covid and border restrictions slowly easing in the us and europe whereas getting worse in india do you guys think that all these top machine learning and nlp conferences will be in person any time soon what is the community view regarding in person conferences do people think virtual conferences are the new norm and more productive less carbon footprint more visibility etc biggest con no free travel
2,suggestion for ml data engineer related projects in c i m looking for a new job as an ml engineer i saw there re many job descriptions showing that they want the candidate to know python c java my current team builds production systems for ml models and we only use python i know some c and i have some small school type projects memory management system usage monitor concurrent programming in c but i don t have any ml or mlops related projects in c i saw there s a course on linkedin for teaching you how to build a neural network from scratch in c if i put that on my resume does it sounds fancy or strange i mean most people companies use pytorch or tensorflow to build nn architecture models now so i don t really see the point anyway i really want to know if i need to put a c project on my resume to make it competitive if so any suggestion for an mlops or data engineering related project in c thank you
2,research what are recommended frameworks libraries for live image segmentation basically the title which libraries frameworks that offer object detection instance segmentation tensorflow 3d mask rcnn opencsv etc are recommended and is it live through the camera feed as in not just a static image but live
1,advice for building a strong profile as a statistics grad student i will be starting my m s in statistics this fall and plan to start applying for internship positions by the end of the year i wanted to utilise my time before grad school to work on my profile what kind of and how many skills projects papers should i include in my profile to be a strong candidate for internships as an analyst in health finance or chemical industry
0,is it impolite to ask a hiring manager to change the job title i currently work as a data scientist currently interviewing for a data analyst role at a company i am interested in despite the job title the job description sounds like more of a data science job than a data analyst job would it be impolite to request a title change for the job as part of the hiring process i have heard that job titles in data science don t mean much and people often do go between data science data engineering and data analyst jobs as part of their career how true is that
0,how do you plan estimate and communicate data science projects i m currently working on all deep learning based projects and tasks involve reading a fair bit of research papers then implementing them experimenting etc however i m struggling to estimate the timelines especially for research oriented tasks and communicate them effectively to the management any advice on planning projects better do sprint cycles work for ds projects if not what works are there any data science related design documents etc that are publicly available which goes into more details into the planning of tasks
0,1 month of free time looking to utilize it well hi i have an upcoming month where i will be completely free and want to build up a strong foundation for data science in the future i have a month free in the upcoming weeks before i start my postgraduate degree and would like to utilize it fully i have a fairly good understanding of logistic regression decision trees random forest svm etc a course which does a deep dive though of these would be amazing i am primarily looking to enter the field in terms of neural networks deep learning here please treat me as a total novice i don t mind paying for courses if they re worth it should i go for free courses or certified courses which ones would you recommend
2,karpathy cvpr 2021 workshop on autonomous vehicles video here full workshop stream here karpathy discusses how radar bugs were holding autopilot performance back and how removing it from the stack to go vision only ultimately improved performance though it seems they did use radar to some degree in automatic labeling triggering events to train on in an active learning style he also discusses tesla s supercomputer for training 5 760 a100 gpus and some data engine and team management processes at tesla still nothing new about their dojo project nn training accelerator chip it s still unclear to me to what extent they use all these predicted semantic features like object detection other agent kinematics to plan with classically and how much they use things like the predicted future path or how these are merged to actually act in the fsd beta
0,does anyone do event tracking at their job does anyone work in event tracking and designing them i was wondering if any other data scientists work on eventing designing schemas and qa ing the events gtm google analytics snowplow etc and how much of the job is doing said task
1,dividing each observation by series mean hello folks a time series data has n observations is there a special name for a series where each observation is divided by the original series mean thanks
2,highest resolution gan available hi everyone for a private film production experiment am seeking for the highest quality resolution image generation gan available for public i am searching for those that are trained on landscape and architectural datasets not those for human faces alternatively if there is a hi res gan that could be retrained it would also be appreciated generation consistency is also a big factor for me as plan to animate a lot of generated frames what providers are out there that can do this my manual search has only pointed me to tech papers andl am an artist not a ml scientist thank you very much for your help 3
2,what happened to the arxiv insights youtube channel the arxiv insights youtube channel was one of my favourite ones for deep learning but he stopped posting videos over a year ago does anyone know what happened
0,do any of you do modeling with pymc3 or other bayesian oriented packages trying to get a sense of example industry problems where a bayesian perspective would be relied upon
0,is learning web development worth it i have seen that sometimes the results obtained from the data science process have to be displayed to the end user in some sort of analytics web tool should i add web development to my data science toolbox
0,game development how to structure data hi i m a game developer and i have a question about how to best structure my data as a former hammer accountant everything always looks like a nail spreadsheet to me i m wondering if there s a better way here are a few actual examples of the desired output in lua format apologies if the formatting isn t right i did my best these are a few lines describing maps in an rpg m map info hash forest tree h1 script require maps map codes forest tree h1 display name biome forest max level 10 min level 1 store level cap false player tint add vmath vector4 0 0 0 0 player tint subtract vmath vector4 0 0 0 0 cloud vmath vector4 0 15 0 20 0 10 0 border true map zoom true music id force false ambient indoor empty ambient multiplier 1 rest time 120 m map info hash rusty fortress h1 script require maps map codes rusty fortress h1 display name biome rusty max level 10 min level 1 store level cap false player tint add vmath vector4 0 0 0 0 player tint subtract vmath vector4 0 0 0 0 cloud nil border true map zoom true music id force false ambient indoor empty ambient multiplier 1 rest time 120 m map info hash rusty huts h1 script require maps map codes rusty huts h1 display name biome rusty max level 10 min level 1 store level cap false player tint add vmath vector4 0 0 0 0 player tint subtract vmath vector4 0 0 0 0 cloud nil border true map zoom true music id force false ambient indoor empty ambient multiplier 1 rest time 120 here are a few lines from a space game about alien genetics skill name log log 4 skill id 1089 race ids 4 4 category hash hash 4 4 texture hash skill icons flipbook hash 4 4 description add 97 crew limit is mutation false is variant true can combine true show in skillcheck false texture string skill icons flipbook string 4 4 default unlocked false skill name log log 5 skill id 1090 race ids 4 4 category hash hash 4 4 texture hash skill icons flipbook hash 4 4 description remove 38 damage is mutation false is variant true can combine true show in skillcheck false texture string skill icons flipbook string 4 4 default unlocked false skill name pow pow pow 1 skill id 1091 race ids 1 1 1 category hash hash 1 1 1 texture hash skill icons flipbook hash 1 1 1 description add 21 damage is mutation false is variant true can combine false show in skillcheck false texture string skill icons flipbook string 1 1 1 default unlocked true skill name pow pow pow 2 skill id 1092 race ids 1 1 1 category hash hash 1 1 1 texture hash skill icons flipbook hash 1 1 1 description add 68 crew limit is mutation false is variant true can combine false show in skillcheck false texture string skill icons flipbook string 1 1 1 default unlocked false this ends up being sometimes hundreds of lines of data with a lot of stuff strings numbers tables in tables etc since the data is live during development i end up wanting to change it a lot add and remove things etc it obviously gets unwieldy doing it right in the editor editing hundreds of lines previously i’ve made google sheets to handle it i love spreadsheets but even i will admit this is cumbersome m map info hash b3 script require maps map codes b3 display name e3 biome c3 max level p3 min level q3 store level cap r3 player tint add vmath vector4 s3 t3 u3 v3 player tint subtract vmath vector4 w3 x3 y3 z3 cloud m3 border n3 map zoom o3 music id h3 force i3 ambient k3 ambient multiplier l3 rest time j3 generally speaking tables might look something like this a number 2 a string string here subtable 3 a number 4 a string another string subtable a number 8 a string yet another string subtable 3 5 7 11 13 17 19 the main stumbling block ends up being the subtables which can have none one or multiple values i haven t figured out a good way to handle this in a spreadsheet i can either begin entering comma delineated values manually this is bad because it’s manual makes it hard to keep track of the data and eventually begins resembling the reason i implemented the spreadsheet in the first place or i have x number of columns for each individual data point in the subtable this is bad because it is cumbersome to set up and implies a maximum number of entries in a subtable does this make sense is there a better way to handle this some kind of tool free is great but i ll pay to make my life easier thank you in advance and please do let me know if anything needs to be clarified
0,is this a common problem i tried fitting a glm style regression model to some data and it resulted in all the regression coefficients being estimated as 0 i e model failed yet when i tried a random forest model on the same data the model worked well and i was even able to get 70 accuracy on the test set my dataset has continuous and categorical variables as well as a lot of naturally occurring zeros i was just wondering is this a common problem i spent a whole day trying to tweak the regression model to work but the random forest instantly outperformed it thanks
1,how do i find crime statistics relative to race relation to proportion of the population i want to find murder statistics by race for a given country relative to the population of that race within the total population of that country races can include white black hispanic asian etc for example if there was 2500 murders in a given year and 1000 were by white murderers and 1000 were by black murderers i would want that divided by their proportion of the population for example if the population was 75 white then you would divide 1000 by 0 75 to get roughly 1334 if the population was 10 black you would get 10000 the point of the statistic is to mitigate total numbers of crimes to the relative population of the race of the murderers
1,aic questions hi there i m new to statistics and have two question about the aic 1 why is it that i see a lot of people say that the max of the likelihood function can be approximated as the sum of the square residuals i understand that this might be a simple questions so a link to a paper or textbook would be helpful too 2 i have seen that the aic is calculated using aic 2k ln l yet when i look at the lmfit module in python for example i see they use something like 2k n ln sum of residuals 2 n what is the difference and is the second still useful thanks for the help and patience
1,education should i pursue a biology major along with statistics i hope this is an okay place to ask i’m a rising sophomore majoring in biology and statistics and was wondering if there would be benefits to keeping the biology major i was originally just majoring in biology decided to take on statistics hoping to go into biostats my concentration in statistics is applied statistics and biometry i spoke to students at my university who said the most overlap between the biology and statistics majors would be taking the computational biology concentration is there a concentration that would be both applicable to stats and expand fields career options for reference the concentrations within the biology major include biochemistry computational biology general biology genetics genomics development microbiology molecular cell biology neurobiology these are the ones i’m strongly considering sorry for the broad question any guidance would be greatly appreciated— thank you in advance
2,researching with no affiliations to any universities academic organizations naive high schooler here i had a pretty solid idea or so i think of a particular experiment i want to do which involves an intersection of 2 fields ml signal processing it hasn t been done before surprisingly and there isn t much research into that area i wanted to write a research paper on my idea but obviously lack the skill and resources needed to experiment more with the idea i know universities and professors often secure research grants to fund their work but i don t have any affiliations with any universities is there any way i can still provide my idea some of the solutions to the problems created using it and implementation coding help to a professor uni and get myself listed as a co author of the paper or can i go down some other path to help me publish the idea note even though i am publishing primarily for my c v i think my solution is highly novel and might benefit the industry but it appears i am stuck without an actual professor
1,prediction variance using gpr hello in gpr framework one can get at an unobserved point not just the value of prediction but also the variance one knows that this variance only depends on the position of the observations the point where we want to get the prediction and the kernel here is my question if you have 2 different kernels you can get two very different variances so can you really use this information
1,can you use principal component analysis to compare data sets i am working with spacecraft data where i’ve split my data into 4 subsets categories within each category there are a number of data points flow “events” and each flow event has a number of measurable parameters an analogy might be that you have 200 cars split into 4 sets of 50 based on manufacturer and each car has parameters that you measure like engine power wheelsize etc what i’m trying to do is use pca on each subset including all my parameters to understand find out if there are true distinguishable differences between my different “subsets” of flows i have written the code to do this but i am having real trouble interpreting the results i have plotted principal component vs variance for each subset and they are all very similar is this telling me that based on my chosen parameters alone there is little distinguishable differences between the datasets in each subset the first principal component only contains around 20 of the variance what does this mean physically what i want to be able to say is “ah so statistically that is why this flow falls into category x because it has a high y parameter value” thanks
2,which possible approaches for comparison of two data time series transformations without real metrics imagine that there are many time series and the problem of finding significant drops in the level of each series is being solved if there are heavy drops in it we also have two methods for detecting such level of series drops however there is a problem we make the decision about the catastrophic fall in the level of the series on the basis of the series itself but we do not know how much the real business metric corresponds to our finding for one of the methods it is possible to evaluate the business sense of the found falls in the series after the fact and we have a certain amount such assessments how theoretically can such two methods be compared i would like to come to the basic metrics accuracy precision recall errors of i ii types etc but unfortunately the feedback is either completely absent or given delayed and indirectly and also not accurate for example one method signals that a catastrophic fall the level of the series happened on may 1 and in the feedback to it there may be information that according to other data inaccessible to me problems have been observed say since april i think that useful metrics could be confirmation of the happened fall of the series according to the later data of this series for example the level of the series fixed at a lower level or continued to fall for the next n days also other useful metrics could be the qualitative characteristics of the method the conditions in which it can be applied and the restrictions on the data that it imposes but all of these methods of assessment and comparison are designed around the method itself and yet i need to somehow tie to the business metrics but they are clearly not given and they need to be constructed somehow
2,what algorithms beat deep learning and in what application hi there deep learning is taking over a lot of other machine learning algorithms in industry i was curious in what applications do other algorithms still outperform deep learning and what algorithms are they i am mostly curious on this over in the industry world if you could provide in the comments 1 the algorithm 2 the application and 3 the industry it would be awesome any comments are welcome
0,on the job training and education hello i was just curious what everyone s on the job training and employer training program experiences have been like i ve really worked at some smaller startups where a senior data scientist would supervise and give advice i recently got a job offer at a much bigger company so i m curious about whether or not is normal for larger companies to have more organized training opportunities thank you
2,advanced python nlp introduction course we are publicly releasing all class material for our advanced python nlp introduction course at the budapest university of technology and economics feel free to share it all feedback is welcome course page on github
1,could you recommend some standard books in statistics that use r i found some standard text books in statitics and some in r both at introductory and theoretical advanced levels i was wondering if you could recommend some standard books in statistics that uses r at introductory and comphrehensive or advanced levels
2,multimodal deep learning practical project hi guys i have been reading into multimodal deep learning and came across several blogs which explain it from a bird s eye perspective without diving into implementation details also i browsed through the following research papers 1 multimodal deep learning by jiquan ngiam et al 2 multimodal deep learning for activity and context recognition by valentin radu et al promising 3 multimodal deep learning models for early detection of alzheimer’s disease stage by janani venugopalan et al 4 improved multimodal deep learning with variation of information by kihyuk sohn et al i am interested to implement a personal project where the problem statement is a need for fire detection this is usually handled by computer vision object detection models yolo faster r cnn etc however i was thinking about using multimodal dl for this to take inputs from heat thermal sensor etc apart from video feeds can you provide me any practical blog tutorial which shows implementation codes thanks
2,comment on this binary image classifier architecture pairs of images as input i m looking for some feedback on an architecture that i m experimenting with it seems to be performing well but i really want to make sure what i m doing is fair my problem is a simple binary classifier i have 80k labled images they are often subtly different think foggy or blurry images because of that traditional 4 6 layer conv2d networks perform no better than random chance i ve tried dozens and dozens of variations on that theme but it never even showed a hint of training even with augmentation traditional transfer learning didn t work because these images really don t share the feature space of vgg16 and other existing models besides being challenging images i also have a class balance problem i have 30 of class a and 70 of class b in short i don t have enough images to train from scratch class a is the important class and it s expensive to get more images here s my new approach i take pairs of images in my use case i only care is class a present i noticed that if i concatenate two images and treat that as one training image then my possible inputs are aa ab ba bb then notice that corresponds to probabilities of 0 3 0 3 0 3 0 7 0 7 0 3 0 7 0 7 if i combine those into class a is present in either image and only class b my outputs are suddenly balanced at 0 51 vs 0 49 and also i am now drawing randomly from a pool of 80k 80k images 6 4b images all of my original candidate models immediately went from coin flip garbage to an auc of 0 85 consistently on fairly generated holdout data that the model had never seen i have to defend this does the defense s case hold water if you are a fan of my cousin vinny is my approach valid
0,python or r which programming language is better for data science and which has better scope in the future
1,how to appropriately deal with a single outlier than can t be deleted normal distribution hello everyone i am currently writing my thesis and have encountered the following problem i have sample of 150 countries with their respective scores for further tests and z score calculation i ll need to check whether the data is normal distributed the problem is that north korea s score is an outlier which leads to an excessive kurtosis how do i deal with this appropriately i have no legitimate argument for excluding the data point from the sample do i transform my entire dataset via log transformation or what is the usual approach here any help or advice appreciated thanks in advance
0,my first data science client experienced data scientists am i on the right track i have no one more senior to ask and this is the only job i was able to get to break into the field basically client is a procurement company that wants to use ai to make data driven decisions they don t really have much experience or idea of what they want i have asked them to send me some invoices and other types of data that they are currently capturing if they don t have enough data i will simulate something they do understand that they will have to get their data management in place before they do any machine learning or ai my idea is to take couple of weeks and use open source tools to show them what s possible like a prototype and hopefully they will sign a longer term contract with my company i was thinking of doing some anomaly detection predictions which is fairly easy to implement and some classification ml to classify their invoices into different categories this doesn t sound too impressive though any ideas what more i can do in the short time frame like some kind of an easy win
1,new to regressions and i need to pick correct variables hello i m new to econometrics and this is my first term taking an applied econometrics course i have a few question regarding running regressions and i m given a practice data set first off i m trying to measure the demand for biking conditional on pollution and weather there are multiple variables such as weather humidity windspeed bushfire rain and air pressure my question is how of many of these variables am i supposed to include in my regression isn t temperature correlated with humidity air pressure windspeed i noticed when i include those 3 variables temp the effect of temperature increases what does that imply would including temperature only and not the rest yield in biased estimates since the covariance between error term and xi is bigger than 0 i m sorry if i m all over the place i m just trying to understand how could i efficiently pick my variables
1,how much of an emphasis is there on “software engineering” skills in non tech industries hello i’m a undergrad whose looking to do an ms in stats or applied stats i have read a lot of posts about ms applied statistics vs ms statistics and how the mas can prepare you with software engineering skills more than ms can etc however something got me thinking that a lot of these programs which have an emphasis on software skills feels like it’s just prepping students for a job in tech which makes sense hence why data structures and algorithms is so common as a requirement however my question is outside of tech say in banks or other companies is there as much of emphasis on software skills as opposed to stats knowledge i know that coding is always there but outside of tech is there as much of an expectation for ms stats or as candidates to have extremely good coding skills like the tech industry or is it more in the emphasis on the stats side
0,i got my first internship as a data scientist at amazon i m graduating next year from a cs engineering school in france i was contacted by a recruiter on linkedin for a business data analyst role i applied to it had a few tests in sql excel and video interviews got the internship then i had my first call with my manager he told me that we would be doing some descriptive analytics but also predictive and prescriptive basically ml analytics if i was willing to he then asked me about my expectations for the internship and i told him that i would like to do a lot of ds ml work instead of ba da luckily since he s the one responsible for ds in the team he agreed and offered me to do a ds oriented internship instead of a ba da one x200b and that s how i got an amazon ds internship without even completing ds specific tests i m starting my project on monday i feel kinda scared lol i don t have much experience and have only implemented a few clustering algorithms and linear regressions but i have some theoretical knowledge on more complex stuff since it s going to be my first real internship i don t know how it s going to work but i guess i ll just google anything i don t know when i need it x200b i still can t realize the opportunity it is and hope it s going to launch my career in data science all of my friends are mad jealous
0,multimodal deep learning hi guys i have a problem statement where there is a need for fire detection which is usually handled by computer vision object detection models yolo faster r cnn etc however i was thinking about using multimodal dl for this to take inputs from heat thermal sensor etc apart from video feeds any practical blog tutorial you can point me to thanks
1,summary song 10 the rock curve stats parody foo fighters the pretender hello i m back with the latest and final parody in my professor s series hope you like it link summary song 10 the rock curve stats parody foo fighters the pretender
2,is there a difference between composite class prediction or merging multiple classes into one hi all so i’m not sure if it’s the right place to ask this but i don’t know any better places for it either might be a bit silly as i’m not a machine learning engineer myself we are currently annotating data and building models for object detection from images and our solution at the moment is that first we would want to predict the object itself and then its different properties like size color material etc depending on the domain of the objects and we recently had a bit of a disagreement which one would be more accurate 1 having the objects’ classes as high level and generic as possible like “trousers” and “dresses” for garments or “tables” and “chairs” for furniture and then have “style type” as a separate property with values like “suit pants” and “jeans” and “sundress” or “office table” “dining table” “coffee table” 2 having the objects already more specific so basically already defining the style type in the first level of labels by just having “dining table” and “coffee table” as object labels and then predicting properties like material and leg count etc one valid argument for the second option was that object detection happens based on the whole image but as this process also selects the area of the object the property prediction only works with that area so for example if you can’t see from the table itself that whether it’s an office or a dining table it shouldn’t be a property but those should be separate objects because during object detection the model is working with the whole image and can consider the context like the table being inside the kitchen for example although i wouldn’t know how the model would know about this context if this isn’t present in the training data in any shape or form but other than that from the annotation point of view it made more sense to me to have the object classes as generic as possible and separate everything that somehow describes this specific object in a separate property i was told this composite prediction might grant lower accuracy because you need to do 2 predictions instead of one but for me this sounds like bs because we would still have different properties that need second prediction otherwise we’d just create all combinations of properties to form a single level set of classes for example merge “gender” and “material” as well while we’re at it “male cotton suit pants” “female cotton suit pants” etc i don’t want to believe this separation if objects and properties would somehow cause lower accuracy during prediction any professional experienced feedback on this topic
1,how to select appropriate linear regression equation transformation in case of violation of linearity assumption looking at the residuals vs predicted plots of my models it becomes apparent that the models do not fulfill the assumption of linearity most articles i found recommend applying some sort of transformation to the model adding a polynomial interaction or logarithmic term to independent variable or transforming the dependent variable but i am having real trouble understanding which transformation is appropriate for which type of relationship observed on residuals vs predicted plots and to which variables should i apply these transformations i have about 15 independent variables one of the models resemble a line with negative slope other starts out as a line with a slope of zero but has two humps above y 0 in the middle and then turns into a line with negative slope another model once again starts out as a line then exhibits a hump below y 0 but ends as something which looks like cube root function
1,datasets hello i was wondering if you know some datasets where they compare or can be adjusted for comparing two populations sorry if my english is bad it isn’t my native language
2,does directly optimizing the soft f1 loss make sense hello i am currently training a nn on a binary classification task with an unbalanced dataset and trying to get the best f1 score i found that some people optimize it directly i have tried this approach both with and without balancing through oversampling and using the mean between positive and negative f1 scores as done in this article what i found is that my model would very often get stuck in the regime where it always predicts 1 and never 0 for some reason although the point of using the mean between the positive and negative f1scores is supposed to avoid this issue as far as i understand so i wonder why this happens and whether there is a grounded reason why people usually optimize binary cross entropy and not this
1,question delong test if i only have auc and standard errors confidence intervals is there a way to perform delong comparison between two roc curves without having the raw data i only have aucs and standard error confidence interval moreover is there a calculator or a simple way to do it in spss or medcalc it would help me a lot thank you in advance
2,looking to hire hey guys i am looking for a team of fte of ai and cv specialists that can help automate our process we are in the fashion space and need to segment clothing from photos as well as fill avatar bodies with classical computer vision algorithms shoot me a message if you or someone you know may be interested
2,what are your challenges when improving ml models hi i am very enthusiastic about ml dl i worked in this space for 4 5 years myself and wanted to help fellow ml practitioners with building their models please can you help me to validate the problem that i am trying to solve specifically do you find the model improvement slow right now does it take you long to identify edge cases and re train your model on those
0,data science tattoos rediculous topic but my friend who is also a data scientist got a neural network tattooed on him and it got me wondering if anybody else had data science machine learning tattoos or ideas
2,what s the best ml method to predict weekday performance i have historical win loss data per weekday from a game my goal is to find a correlation between gaming performance and a certain weekday or the weekends how can i best extrapolate the win loss ratio i e what is the best statistical method to use to predict future weekday performance
0,data wrangling multiline python statements trying to learn best syntax for python coming from r hi i have worked with r as my primary language dealing with everything from econometrics ml applications gps analysis using apis nlp contract analysis and dealing with unstructured data i would say if i have a problem and can use r then i can likely devise some way to solve it x200b however i want to move into a new role and i am finding despite my 5 years with r as my primary language that python is more preferred when i talk with recruiters so i have picked it up and i was hoping to get some guidance with good examples of code in r it is very easy and code flows between statements using the pipe operator but python i am finding it less intuitive from trial and error to do multistep aggregations data summarization etc x200b anyone have a good guide of how to use python for complex data summaries i want to be able to something like ifelse col1 in phrase1 phrase2 col2 na and get a count of how many unique values pass that logic and i might have a half dozen conditional things like this as i have to evaluate many similar statistics for different time periods quite often kaggle has some data cleaning but that data is perfect compared to some of my sources as a consultant x200b x200b obviously this below chunk of code should be broken up across multiple lines correct df borstate df df borrowercity san francisco groupby borrowerstate borrowercity borrowerzip agg tot amount currentapprovalamount sum sort values tot amount ascending false x200b is something like this standardized df borstate df df borrowercity san francisco groupby borrowerstate borrowercity borrowerzip agg tot amount currentapprovalamount sum sort values tot amount ascending false
0,worst mistake anyone’s made while handling data hello currently a sophomore doing some classical stat analysis at a lab on campus it’s a driving simulation laboratory so it’s mainly doing analysis on the data they get between two different scenarios and doing some hypothesis testing anyways the lab manager told me how my analysis was completely wrong and how i mishandled the data because apparently i joined two datasets which weren’t supposed to and ended up with duplicates i’m kinda bummed that i made this mistake cause i felt pretty confident about and proud of my results this had me thinking what mistakes have you made at your work when handling data
0,i was wondering about this problem and wanted to know if it makes any sense lol so suppose you have different data sets that are not connected to each other via a primary id for example df 1 numerical data of medical records of patients e g bp sugar etc df 2 objective and subjective records by the patient now we have to match records from the df2 to that of df1 how can we match these two dfs without having any id x200b it can be data of anything website data or exam data etc let me know how would you approach this problem
2,of moments and matching a game theoretic framework for closing the imitation gap x200b when attempting to mimic an expert a learner could learn by a rolling out their policy and comparing generated trajectories to expert trajectories b producing actions on expert states and attempting to match action conditionals or c performing rollouts and attempting to match corrections provided by a queryable expert we provide for each of these settings bounds for how well the learner can do reduction based algorithms for efficiently finding strong policies and simple yet competitive practical instantiations that can scale to high dimensional tasks paper code videos
2,review of can vision transformers learn without natural images x200b paper can vision transformers learn without natural images kodai nakashima hirokatsu kataoka asato matsumoto kenji iwata nakamasa inoue tl dr an intriguing study showing that a computer generated database of fractals can replace imagenet to pre train vision transformers for small datasets it would be interesting to try this pre training method on more challenging tasks such as imagenet with 1 labels link to full review in the comments
1,choosing between a mann whitney test linear regression or both currently i am in the process of writing my master thesis for which i have conducted a survey the variables that i have are as follows independent variable a yes no question determining two subgroups dependent variable a interval variable 7 point likert scale moderator continuous variable for these variables i am curious how you would approach this a mann whitney test similar to t test but for non parametric variables makes sense for comparing two groups however it seems odd to use a mann whitney test for the main variable and subsequently use a linear regression for the moderator x200b in this case would it make the most sense to use a linear regression for both the main relationship using dummies for the independent variable and the moderating relationship to have consistency would it make sense to use both tests to strengthen your findings i d appreciate some input in this
1,what s your favorite concept rule theorem in statistics and why what idea s in statistics really speak to you or you think are just the coolest things why what should everyone know about them
2,extensive n 1250 survey on ml ds salaries in israel thorough analysis disclaimer unaffiliated with mdli although i am an active member of that community arabic and hebrew versions available through the link from the link as in previous years this year we ran the mdli community’s annual survey so as to map various trends among those who work in the data science and machine learning fields this year an exceptional number of respondents completed our annual survey – 1 250 people – a respectable achievement by all counts omri goldstein an algorithm developer data scientist and creator of the “data driven” blog analyzed the survey’s findings we used his analysis to generate the mdli community’s 2021 annual payroll report we also developed a dedicated salary calculator for data professionals in israel
0,i found a research paper that is almost entirely my copied and pasted kaggle work i did some work a couple of years ago on w h o suicide statistics here s my kaggle project from april 2019 and here s the research paper from january 2020 it was immediately clear from me seeing the graphs that the work was the same but most of the findings are entire paragraphs lifted from my work this isn t the first time this has happened but it s probably the most egregious my work is obviously not mentioned in the references is there anything i can actually do here i don t care about people using or adapting my public work as long as credit is given but copying most of it and giving no credit really isn t cool edit thanks for all the help and advice i contacted the universities of the authors this morning no response yet and i can t help but feel like i m not going to get one
0,what is data philosophically speaking is it interesting to study onto itself like particles in article physics or computation
2,paper overview vicreg variance invariance covariance regularization for self supervised learning video paper abstract recent self supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image a trivial solution is obtained when the encoder outputs constant vectors this collapse problem is often avoided through implicit biases in the learning architecture that often lack a clear justification or interpretation in this paper we introduce vicreg variance invariance covariance regularization a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually vicreg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization and achieves results on par with the state of the art on several downstream tasks in addition we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements
1,how would you proceed in panel data modeling with countries as groups and only two years to compare so i m in a bit of a pickle in attempting to do panel data modeling i m trying to model mortality from a certain medical condition so dependent variable is a percentage value of the deaths within the population with said condition i am doing a global comparison and attempting to indicate that there are disparities between rich and poor countries i have various independent variables such as the prevalence of said condition undiagnosed of population in country with said condition the prevalence of people at the borderline of getting the condition also in of population out of pocket spending scaled as of gdp percentage of population with the condition that is currently undergoing medical intervention in data exploration i have compared these dependent variables in income groups low mid upper mid and high and i did consistently see differences for all of them i am now doing a panel data model 65 countries only two years to see if these variables which behave differently depending on wealth could be able to predict mortality within a condition or if they just so happen to behave differently due to wealth i know i could simply use the hausman test to choose from fixed or random effects models but what would be the logical model to choose given what i have just said in a theoretical sense which between fixed and random effects models makes more sense for my scenario and supports my theory of disparities among countries due to wealth furthermore i am concerned with adding a dummy variable to indicate one of the two years a dummy year variable does it make sense in doing so for either fixed or random effects what logic does doing so give all i just do is base it on the sign of the coefficient of the year dummy and just say something along the lines of mortality is higher lower on this certain year
2,possible research topics in time series analysis using ml hello there i m a master s student and i have an opportunity to publish a paper this summer my advisor wants me to propose some possible research project ideas to get started i m interested in time series analysis as there seem to be many job opportunities in this area so can anyone here suggest me some interesting research ideas within this domain i know anomaly detection is a pretty hot topic these days but i d also love to hear more
0,interest in a puzzle solving community hi everyone many members of this subreddit want to brush up on data science or keep their skills sharp would anyone be interested in starting a community where we write each other challenge problems and get in the habit of solving problems daily think probability puzzles coding problems and questions about ml techniques research shows daily problem solving can help you learn much quicker boost recall and prevent you from forgetting key concepts even with a small community of 20 members writing 1 question means 20 questions to practice with every week feel free to comment or dm me if you re interested
2,anonymous walk embeddings graph ml research paper walkthrough this research talks about using random walk inspired anonymous walks as graph units to derive feature based and data driven graph embeddings in an unsupervised fashion 🔥
2,research extreme face inpainting with sketch guided conditional gan link recovering badly damaged face images is a useful yet challenging task especially in extreme cases where the masked or damaged region is very large one of the major challenges is the ability of the system to generalize on faces outside the training dataset we propose to tackle this extreme inpainting task with a conditional generative adversarial network gan that utilizes structural information such as edges as a prior condition edge information can be obtained from the partially masked image and a structurally similar image or a hand drawing in our proposed conditional gan we pass the conditional input in every layer of the encoder while maintaining consistency in the distributions between the learned weights and the incoming conditional input we demonstrate the effectiveness of our method with badly damaged face examples x200b using edges as sketch condition using the hand drawn sketch as a condition x200b we were supposed to publish this paper in 2019 but due to unforeseen issues we had to delay to ei2021
2,how can c4 5 algorithm split numerical attributr how can c4 5 algorithm split numerical attributes i had encountered one journal paper where the c4 5 decision tree has same attribute with different ranges multiple of time in same rules for example if age 60 and wealth poverty and age 75 then sick can somebody explain
1,ms in statistical finance or ms in data science for quantitative finance i am currently a rising senior majoring in math with a cs minor
2,model to detect abnormal behaviour in animal cctv videos horses hi we have cameras to monitor our horses in stables or in fields all fed to r blueiris software hosted on a i7 windows box blueiris app works like a charm i m looking for a way to detect abnormal behaviour and alarm about it not simple motion detection not person detection but something that would trigger a something is not going as usually things like a lot of movement compared to usual sleeping resting for too long i would image a model that would learn on like a week worth of video and triggers if something is different do you folks know about something similar or where i could look for this running on windows ideally locally to keep cost to minimum happy to discuss edit thanks for the answers so far i understand that it s actually a way harder problem that i initially thought i also thought something would exist already unfortunately not competent enough in ml to dive into some solutions proposed
1,preferable to use mad or standard deviation is it generally better to use mean or median absolute deviation as a measure of spread compared to standard deviation
0,most important reporting monitoring tools for your business reporting and monitoring there are two important types of tools in bi one is for reporting one is for real time monitoring for reporting power bi tableau qlik sense looker etc for real time monitoring i believe there a few categories based in what i have read in various articles cliff ai for real time monitoring of business metrics montecarlodata com for real time monitoring of data what others tools for reporting real time monitoring tools you use or know about
2,reconstituting t cell receptor selection with neural decision trees because t cell receptor tcr antigen recognition can drive cytotoxic and other t cell responses autoreactive tcrs that recognize autoantigens can drive the destruction of healthy cells and tissues with those autoantigens autoreactive tcrs are removed by t cell selection which protects against autoimmune diseases we predict which tcrs are removed or retained by combining high throughput tcr sequencing and machine learning our trained machine learning models may therefore be able to identify autoreactive tcrs we use neural decision trees ndts for the machine learning with ndts we encounter scaling issues reminiscent of the vanishing gradient problem we introduce an architectural change to correct the scaling issue achieving strong results see materials and methods paper genes immunity paywalled pdf
0,what kind of education do you need for ai drug discovery i have a bachelor s in chemistry and i ve been learning python to try to get into data science what i generally see on this sub is that a master s in statistics or ds is a good education for this field would this also be true for ai drug discovery or is that a different animal is this something that requires a phd in say biochemistry programming experience or does it fit more into the purview of general data science
0,what kind of mindset a person should shape in order to bocome a good data scientist some context i m gonna be speaker for a meetup and i want to touch the fundamentals for people who wants to start their career as data scientists rather than talk about the usual stuff develop math biz and coding skills i want to people reflect on the mindset they need to build so they can become successful data scientists right now i have for things to share with them 1 curiosity data scientist are people with lot of curiosity and they are willing to research if your stay curious you will have the mindset for ask questions look for data formulate hypothesis and test them 2 perseverance usually data science is a rough path there is no magic bootcamp or 6 month course to become a good scientific you need to stay focus and practice your skills pretty often even by just listening a podcast or reading a paper 3 emphaty you have put yourself in other scenarios rather than just thinking inside the data science side often you need to collaborate with other disciplines and understand the context of unknown situations you might analyze social events business metrics human behavior etcetera 4 participation as data scientists we need to get our hands dirty no better way to learn imo than building real applications solve real problems what other characteristics do you perceive in a successful data scientist
0,why you re bored at your job and how to fix it this is a post especially relevant for those of you transitioning into data science from a non traditional background so i hope you find it especially helpful in the 1950s frederick herzberg developed a theory that states there are two dimensions to job satisfaction motivation and hygiene hygiene factors can minimize dissatisfaction at work but they can’t make you love your job these are factors like salary supervision and working conditions when you look back at the best moments of your career they won’t really include the perks or the free lunches you got instead you’ll look back and remember the motivators these are factors like recognition and achievement they mean that your work is challenging and that you’re learning about topics that you’re intrinsically interested in these are the factors that’ll be the predominant source of your work satisfaction and what contribute to your personal growth here’s the thing though if the hygiene areas aren’t properly addressed you won’t feel satisfied regardless of how fulfilling your work is no matter how challenging and exciting your work is if you’re not getting paid what you deserve you’ll constantly have a nagging thought at the back of your head telling you to leave on the other hand only having hygiene areas resolved is the reason why you constantly think something’s missing you’re puzzled over why you’d be unhappy you have a high status job plenty of cash and great coworkers but we need challenge and growth to drive us forward and that’s why the motivators are integral without the motivators we go to bed at night dreaming about what we’d be doing in an alternative world just look at these hacker news posts link the reason this can be hard to identify in our day to day is because we wrongly assume that just because we’re not fully unsatisfied we must be satisfied and when we inevitably don’t get that resounding feeling of congruence with our work we get puzzled one of my favorite examples of someone who prioritized her intrinsic motivators over factors like money or status is kristina lustig she quit her high paying director of design job to retrain as a software developer it might not have made sense to others around her but only kristina knew what motivated her intrinsically loss aversion let’s assume you realize you want to make a career change into something more rewarding your brain is going to freak out it’s going to start screaming what if i don’t like my new job as much as my current one what if i don’t end up happier i can’t change if i don’t make as much money the key to overcome this thinking is to separate short term losses from long term losses so here are a few examples short term in the short term my salary will drop long term but 5 years from now why can t it exceed what i m making right now short term i might have to take an entry level role which feels like a big drop from my current position long term but 5 years from now won t i not only be in a more senior position but also a few steps closer to doing work i enjoy short term i might have to give up the stability of my current role long term but 5 years from now won t i have stability and a new skillset i can leverage the next thing it’s really easy to fall into the trap of thinking that the nicer office the next pay raise or the more prestigious title is what will make us happy after all it’s what your friends and family see it’s the labels that stick instead we should aim to ask a different set of questions is this work meaningful to me is this job going to give me a chance to develop am i going to learn new things will i have an opportunity for recognition and achievement am i going to be given responsibility these are the things that will truly motivate you the rest is just noise i hope that was helpful if you liked this post you might like my newsletter it s my best content delivered to your inbox once every two weeks and if twitter is more your thing feel free to follow connect with me here
2,bert metadata new to bert and wondering if there are ways to incorporate additional information like for example font size of words in bert models would appreciate pointers
0,visualizing graph2vec i’m a data science n00b and was recently asked to look into graph2vec i ran the ex program and it outputs a csv file with 128 dimensions is their anyway to visualize this so that one can roughly see what is happening thanks
1,any summer online multivariate statistics courses that give academic credit hello i’m a graduating senior and unfortunately my school doesn’t provide the multivariate data analysis course during the summer which is when i have to graduate however my advisor says that i am eligible to do a “course equivalency” meaning i can take this course at another private institution school etc as long as the program gives academic credit does anyone know any programs this summer that provide this thank you
1,i discovered riot games lying about the existence of losers q in league of legends by using statistics at least read the funny introduction introduction matchmaking in league of legends apparently uses algorithms to put all their undesirables who have soaked up a high ratio of reports together on a team this is known as loser s q if this was not the case you would see feeders on your team 50 of the time and 50 on the other team yet i played over 3100 games in the past two years and 80 of the time the feeder was on my team calculating coin flips can be done in this link we have 2480 heads out of 3100 coin flips and it ends up being 1x10 261 and one of the cover up agents said this is nothing more than common statistical variance lol you know how cover up agents are right they deny censor insult push misinformation redirect the conversation discredit etc etc but the truth wins out read if you want to see the breakdown of the last 51 games and discussion of the previous 3050 read if you want to see the attempt at coverup that was more than revealing that moment when they try and hide stuff so well that they just show their cards to you
2,inductive biases in machine learning can someone please try to explain the concept and role of inductive biases in machine learning are inductive biases very basic and general assumptions required for machine learning algorithms to work e g birds of the same flock fly together unseen data can be predicted based on how similar it is compared to seen data
1,need help understanding joint probability of three variables hello all i am describing the environmental conditions at the site of an offshore wind turbine the sea state at the site is described by three stochastic variables wind speed wave height and wave period or frequency the code for designing offshore installations dnv os j101 demands wave height according to joint probability distribution of wave height wave period and wind speed in other reports on basically the same issue the following is done wind speed governs the wave climate the wind speeds are divided into say 2 m s intervals and the probability of the wind speed falling within a given interval is calculated the corresponding wave heights and periods are calculated example say there is a 0 1 probability of the wind speed being between 2 and 4 m s the wave height and wave period with a probability of 0 1 is calculated from their respective cdfs as i understand the joint probability is given as p x x y y z z where the capital letters are stochastic variables in the above p windspeed 2 4 0 1 and then the other two variables are isolated from p waveheight y 0 1 and p waveperiod z 0 1 i am doubting whether this is a joint probability distribution and i suspect i may be missing a major part of understanding what joint probability means i do have the cdfs and pdfs of all three variables so the question is really what i should do to obtain the wave height according to the joint distribution as in the quote above any insights will be much apprechiated and please ask questions if i should include more info
2,machine learning games hey guys hope you are all good i m teaching machine learning fundamentals to young kids and i m looking for cool ways to do it maybe games videos or so on i would really appreciate it if you could help me out with this thanks
2,what should be the next alphago alphafold moment in ai what milestone does ai research need to tackle next that would be similar in scope to what deepmind did with alphago or alphafold
0,anyone ever get fired i got canned from my first job in the industry joined a tech startup where devs ran the entire show and did wtf they wanted not the management i wasn t the extrovert personality the ex consultant management seemed to want client work didn t come in they nit picked on small stuff in my 3mo review like not responding to slack messages immediately on a sunday and canned me a week before christmas seemingly nothing really to do with the work i did didn t even get to go past my desk to get my stuff i now work for one of their clients but 1 5 years on i struggle to let it go of the shame that i got fired from a job
1,controlling for confounding of unmeasured with a correlated but non causal covariate howdy x200b tl dr i have an outcome variable a and a continuous treatment variable s observational not experimental variable a has a set of potential confounders u that i have not measured i also have variable b which has a similar relationship to confounders u as variable a and where s 0 is it plausible to use variable b to control for confounding by u on variable a and isolate the effect of treatment variable s i think a causal dag would look something like this s a u b x200b x200b full context i m working on a marketing measurement model to estimate roi for ad spend something akin to a media mix model the marketplace is entirely digital and we only use digital ads we can generally tell in aggregate how much we ve spent on certain kinds of ads video display etc and how many times those ads have been viewed each day as well as if someone who viewed an ad made a purchase and the revenue from that purchase we aren t able to tell specifically which individuals saw the ads and what their outcome was only aggregate daily numbers we also have in aggregate the total sales for the marketplace by subtracting the sales with at least one ad impression from the total sales i can separate the sales into two streams x200b ad attributable sales customer saw at least one ad prior to purchase these sales are possibly caused by the ad impression but it could also be that we showed an ad to someone who would have purchased anyways completely organic sales customer definitely didn t see any ads prior to purchase these sales are definitely not caused by an ad impression as we know the customers didn t have any ad impressions x200b a media mix model would generally be fit as a log log linear model with the ad spend in different buckets as the covariates as well as indicators about seasonality economy etc with the total sales as the outcome to reason about the causal effect of ad spend on sales this high level approach is used because it often includes ad channels such as radio tv etc where you can t really know who saw the ad or if they purchased or not in my case while i can t say specifically that customer a saw this ad and then purchased i can say that 1000 customers saw this ad and then 50 of them purchased x200b we can follow the media mix approach and regress the ad spend against the attributable sales to get some kind of measure of roi but without controlling for seasonality economic factors etc the roi will be overstated and further from the causal truth however sourcing the data for the potential confounders particularly economic is time consuming and adds a lot of complexity to the model and project x200b i am wondering since i have the ad attributable sales and the completely organic sales if it would be reasonable to use the completely organic sales to control for all of the above the completely organic sales would also be affected by seasonality economic factors promotions etc in a similar way to the ad attributable sales without the obvious potential influence of the ads x200b thoughts
0,question regarding freelancing i ve been contacted by an acquaintance to do some freelance work and i m unsure how to bill it should i just set an hourly rate and charge according to time invested until completion they are interested in the final product not on the code and since i would be using some personal libraries of mine on the one hand i feel that being too fast is counterproductive with an hourly rate little money however if i just set a flat price and there s unforeseen issues i might have to work more ending up with an effective hourly rate that is too low how do you deal with this and what do you recommend
2,plotting a decision tree where each node is a figure does anyone know how i can plot a binary tree where each node is a figure eg seaborn graph in python i know dtreeviz can do something similar but i’d like to specify what each image will be thank you in advance
1,risk of overfitting in extremely large data sets so the risk of overfitting small datasets is obvious as each individual observation can exert undue influence on our results but i have read that as data sets get larger and larger the risk of overfitting approaches near certainty as we are bound to find spurious relations just due to the sheer quantity of the data i am aware of many of the best practices to avoid overfitting on moderately sized data sets but i am wondering if there is any guidelines or literature on how to know when a dataset is so large that the risk of overfitting becomes an increasingly pressing issue
2,practicality of a machine learning classifier for driver vigilance through computer vision and machine learning is it possible to build a neural network that can classify if the driver is engaged focused or vice versa what are some potential challenges and what kind of data would be needed best for training this model let me know what you guys think
2,nyu facebook cifar present ‘true few shot learning’ for language models whose few shot ability they say is overestimated a research team from new york university facebook ai and a cifar fellow in learning in machines brains raise doubts regarding large scale pretrained language models’ few shot learning abilities the researchers re evaluate such abilities with held out examples unavailable which they propose constitutes “true few shot learning ” here is a quick read nyu facebook cifar present ‘true few shot learning’ for language models whose few shot ability they say is overestimated the paper true few shot learning with language models is on arxiv
0,do you often find hyperparam tuning does very little in python sklearn most of the time the defaults produce the best or very close to it performing model f1 score and doing a gridsearch over 6 000 combinations or whatever rarely improves anything the only thing i ve found to be helpful is building new features is this typical
2,has anyone heard of zaslavsk s theorem of hyperplanes has anyone heard of zaslavsky s theorem on hyperplane arrangement supposedly it says that there are only a finite number of ways that hyperplanes can be arranged does anyone know why this is important apparently it has implications to decision boundaries of machine learning classifiers thanks
1,estimating class attendance given cumulative registration howdy x200b i am working to estimate attendance for a class in the future we open up registration up to a year prior but registrations can occur up to the day of the classes not everyone who registers will actually attend i m looking to get an estimate each day in the week before the class starts for example i might know that 7 days prior to class 1000 students are registered 6 days prior to class 1100 students are registered 5 days prior to class 1150 students are registered some cancelled 1 day prior to class 1200 students are registered on day of class 900 registered students actually attended x200b additionally students go through several different steps in the registration before they are eligible to actually attend such as gathering requisite materials which can be seen as a signal of intent to attend we have the counts at each time step for students who have completed each step x200b i am planning to use a bayesian approach with pymc3 as we know from experience that typically around 30 of registered students don t attend my current thought for how to approach this is to model like y p rₜ where y is the attending count rₜ is the registration count at t days before the class and p is an unknown proportion that will actually attend then develop a linear model to estimate p x200b i wanted to ask this sub first though if this sounds like a problem that already has a general solution or framework to estimate
0,using data skills for activism so i work as a full stack data professional handle both engineering and science and have recently become very interested in doing some side projects around using data for activism my first thought was to look at some examples of people doing this or maybe some blogs or online communities to get ideas around what i could do to help my local area however my google searches are really only turning up articles around data activism in the workplace or are ads for cloud data platforms does anyone have any good resources on the topic know some better terms to search or just have their own ideas in general
2,tabular data deep learning is not all you need not that surprising xgboost still rocks when the underlying data is in a tabular form original article here more hard to find independent stuff related to ai data science here
1,methodology check psm looking to see what is missing from my methodology idea i am trying to develop a plan to take a look at some healthcare claims data i have two types of physicians in my data type a and type b i plan on propensity scoring their patients and then either matching or weighing the patients so that i can compare a plethora of outcomes between type a and type b physicians this would be done over the same time period of claims if i just do statistical tests on the outcomes my understanding is that what i am saying is that there is some type of difference between group a snd group b physicians that is not occurring by chance if i want to quantify the impact of being in one of these groups i would need to take the analysis further to something like difference in diff i have some outcomes that span across time too and one of the questions i am trying to answer if over time this outcome is different between the two groups and within the subgroups of each group could i just use an anova to answer this after propensity matching i have a very huge ever evolving list of outcomes and question types which honestly confuses me any help appreciated thank you
2,discussion looking for a very particular data set hi i m kinda new to ai and don t know all of the best places to look for datasets but from searching a bit i can t seem to find what i m looking for does anyone know of a data set that maps human descriptions to raw data for instance if i have a graph of vertical height which varies with time of a ball being tossed into the air and it forms a parabola a human might say the graph looks like a hump or the graph forms an arc or even more specific the graph peaks at x time in addition if a human is asked the behavior of a graph at a particular point in time they might say it s increasing or it s peaking or it s rapidly falling depending on the context any datasets relating to this sort of topic would be great i can t really find any that come close and frankly i don t know what to search for exactly thank you for reading
0,are there any data science applications in the nonprofit space or any companies products aimed in the nonprofit direction i’m just curious if due to the cost of data science any nonprofits employ it in any way
2,scaling vision transformers wow much sota paper the title would suggest that the paper is just about training large vision transformers but i think the main takeaways are the scaling characteristics larger models do better especially with fewer examples
0,data engineering what is the future and what to focus on today we have loads of companies who either have their data foundation run in the cloud or hybrid solutions where some business processes are in the cloud and some on premise i have been a data engineer building and maintaining an on premise datawarehouse for quite a long time now but i am very intrigued by learning more about cloud datawarehousing and cloud solutions in general as i think its a very practical and promising technology for the future i have been getting my certificates and want to start working with cloud technologies now as well my questions to you guys do you think that cloud solutions will be the future or will on premise data systems be there for a while and how alike is working as a data engineer in on premise vs cloud solutions please share your thoughts
1,what fraction of data to trim in trimmed mean for normally distributed data the sample mean is the best estimate of the mean and for the laplace distribution the median 50 trimmed mean is optimal when the distribution of the data is unknown how should the fraction of data to trim in computing the mean be chosen in general higher kurtosis suggests trimming a larger fraction of data but is there a quantitative rule
1,parametric unpaired t test or non parametric wilcoxon rank sum test for relative values i have to groups n1 13 n2 9 each having a set of values for cell sizes unfortunately the calculation to get absolute values didn t work probably due to an uncalibrated flow cytometer which i used for measuring sizes my supervisors and i agreed that we d still use the relative values to test for differences between the groups both groups are normally distributed and both t test and wilcoxon rank sum give non significant differences in mean median respectively so the results are actually quite clear i just want to be sure i m using the right test so the question is what is theoratically correct i know the t test checks for differences in means but could the relative values affect the results from my understanding the wilcoxon test modifies the data to pure ranks anyway and i imagine this will be completely robust for relative vs absolute values greatly appreciate any tips
1,what is the likelihood of three siblings having their golden birthdays in the same year question so my three cousins who are siblings all have their golden birthdays in the same year what is the likelihood of this happening if we assume that the parents don’t give birth before 18 or after 35 we got a few different numbers some incredibly low some incredibly high 1 in 34 000 and 1 in 101 000 000 were the two answered we came up with personally i think it’s the latter but i figured i’d asked some people who are more skilled in the subject
0,can we begin to understand possible mathematical reasons as to why algorithms like xgboost and random forest win kaggle competitions instead of neural networks could there be any mathematical reasons behind why algorithms like random forest and xgboost are known to win kaggle competitions i e perform well for medium sized tabular datasets compared to deep neural networks and linear regression models heuristically here are my general conclusions 1 glm general linear models perform best on smaller sized datasets provided certain statistical assumptions are met 2 boosting and bagging algorithms e g random forest and xgboost perform best on larger tabular datasets and do not require many statistical assumptions 3 deep neural networks perform best on very large datasets preferably on non tabular datasets e g tensors pictures audio computer vision text nlp but can there be any mathematical reasons that try to explain these general conclusions provided these conclusions are correct for instance suppose there is one response variable and one predictor variable and when graphed together they look like a sine wave it seems unlikely that a linear regression model could perform well perhaps this is because a linear model can only capture a linear trend perhaps it is too hard to understand the exact assumptions required for glm models to work on real world data or they are too prone to overfit on complex data the same way is there any math that explains why alphago self driving cars and google s bert nlp model are all based on neural networks and not using random forest and xgboost is this because there is some mathematical property of random forest and xgboost which severely hinder their performance on very big and complicated datasets perhaps it can be shown theoretically that random forests require an exponentially large amount of trees to model complex data which is just not computationally possible or would surely result in overfitting and the same way is there any math that explains why deep neural networks aren t as successful as random forest and xgboost on medium sized tabluar datasets do deep neural networks simply require too much effort to select the right combinations of hyperparameters and its just not worth it for medium sized datasets when random forests work well given significantly less effort are deep neural networks to prone to overfit medium datasets of course all of this comes to down to trial and error if a certain model fits the training and test data well then use that model but just using mathematical logic and intuition can we develop some general guidelines that tell us which conditions and types size of data are favorable for specific algorithms this could potentially save us a lot of time by directly trying better suited models for the task at hand e g not even trying to use logistic regression for alphago so in the end beyond empirical results could there be any mathematical reasons behind why random forest and xgboost are chosen in kaggle competitions compared to deep neural networks and beyond empirical results could there be any reasons why random forest and xgboost are not chosen for the imagenet competition thanks
0,does a classical job as a statistiacian still exist i ve graduated a few years ago and i always like my courses in stats and econometrics i found a job in another field which i very much enjoy working in a big company i sometimes take a look at the internal vacancies often there is a a vacancy for a data scientist data analyst or similar but i ve never seen a vacancy that s looking for a statistician for these kind of jobs they re generally looking for someone with a background in it or engineering and the job requires more than excellent knowledge of stats programming and the like i ve only seen vacancies for a statistician econometrician for government and central bank jobs which are rare and attract many candidates do these kind of jobs where the main tasks are statistical analysis still exist
1,data issue investigation experience market timing hi all i m doing research into whether more experienced fund managers refrain from raising funds in recessions basically i proxy experience with the number of previous funds a manager has had and created a recession year dummy variable my aim is to create a simple graphic that shows whether experienced managers wait for the recession to be over which would lead to a lower average experience in such a year current approach however based on this approach i see little to no difference in experience between recessionary years and non recessionary years my concern is that there may be a change in the distribution but not a change in the average to complicate things the average number of experience increases over time and since recessions are spread out i need to somehow compensate for this i feel stuck so i am happy to hear any suggestions or alternative approaches
1,generating new points with smote there is a well known algorithm in statistics called smote synthetic minority over sampling technique which is often used to balance and imbalanced data set if i have understood correctly the premise of the smote algorithm is as follows suppose you have a dataset containing information for medical patients that are healthy and not healthy but let s assume that the majority of the patients within your dataset are healthy the composition of healthy not healthy being 95 5 if you want to make a statistical model for this data the data does not contain enough information for not healthy patients and it will be very challenging to build a reliable statistical model that can make accurate predictions for not healthy patients thus the smote algorithm can fix this problem by 1 rebalancing the data set e g after smote your data set can have a composition of 70 30 2 creating new data points from the existing data as i understand this is done by multiplying a given vector corresponding to a randomly selected individual observation by some random number between 0 and 1 this leads me to my question suppose you have already have a balanced dataset e g the healthy not healthy has a composition of 60 40 but let s assume that you have a relatively small dat set to begin with e g 500 rows can you use the smote algorithm to create new data points so that your dataset is bigger i understand that no algorithm can magically compensate for data quality issues but at the same time i don t see any major flaws with using smote on already balanced data for reference i illustrated this process below using r i would be interested in hearing a second opinion thanks load and install libraries remotes install version dmwr version 0 4 1 library dmwr create some fake data and put them into a data frame called f var 1 rnorm 100 1 4 var 2 rnorm 100 10 5 var 3 c 0 2 4 var 3 sample var 3 100 replace true prob c 0 3 0 6 0 1 response c 1 0 response sample response 100 replace true prob c 0 3 0 7 put them into a data frame called f f data frame var 1 var 2 var 3 response declare var 3 and response variable as factors f var 3 as factor f var 3 f response as factor f response smote algorithm simulate new points from the first class smoted data over smote response f perc over 100 simulate new points from the second class smoted data under smote response f perc under 100 combine everything together into a final new data file final rbind f smoted data over smoted data under
2,forecating problem i m working on a binary classification problem where the goal is to forecast the demand and dispatches powers encoded as class labels demand dispatches encoded as 0 and demand dispatches encoded as 1 i have a year dataset every hour i already selected the features more relevant to the problem and i tried two methods decision tree and random forest the problem is that when i split the train and test sets randomly 70 for training and 30 for test i get 80 of accuracy but when i consider 180 days for training and try to predict the next 24 hours with a sliding window process i only get 60 of accuracy has anyone solved a similar problem thank you
2,using google cloud or paperspace for u net image segmentation and tensorflow possible hy all is it possible feasible to train a tensorflow u net model für image segmentation over cloud services like google cloud or paperspace currently i am using my tower pc even though i am not utilizing the gpu right now to build a u net based image segmentation model following this tutorial i will move to another country in a few weeks and there i will only have access to my laptop i worry that my laptop might not be powerful enough therefore i am looking for other solutions for training would google cloud or paperspace be a solution in general i am missing a feeling of what is a big computational expensive model if i would train with about 10 000 images with u net would this be considered something big i appreciate all answers as well as links for sources where i can read up on those topics thanks a lot
0,glassdoor reviews work life balance i always see amazing glassdoor ratings for small 500 employee companies and wonder if this is genuine the general story seems to be that work life balance is a lot better at large companies like fortune 500s but glassdoor seems to disagree is this a coincidence with the companies i m looking at or this there something i m missing is it just that start ups expect no work life balance so they give it 5 also any tips on finding a job with work life balance currently an overworked data analyst
0,environmental work hello peepos i’ve started an internship with territorial bureau of statistics that has me doing broad statistical work it is my intention to get into the environmental field with a strong statistical background when i’m done i’m wondering if any of you beautiful folk work for mother earth and could offer some guidance or any “green” company of sorts d for now i’ll be be honing my gis skills dashboarding database management and intensive math like survival rates and differential equations any big topics i should be studying as well ty
0,data scientists who moved to ml data science roles how did you get senior level skills i ve been a data scientist for a bit over a year and a half but i don t feel senior at all if anything i feel like what i assumed data scientists who got entry level roles felt like i look at the interview questions for uber amazon msft and others and i don t really see myself learning them from my peers i ll need to study islr or other materials but it s the other materials where i m a bit stuck wondering if folks who made it to senior roles or faang roles not counting analytics product roles could share tips on how to learn to be a better data scientist
2,facebook ai open sources augly a new python library for data augmentation to develop robust machine learning models facebook has recently open sourced augly a new python library that aims to help ai researchers use data augmentations to evaluate and improve the durability of their machine learning models augly provides sophisticated data augmentation tools to create samples to train and test different systems augly is a new open source data augmentation library that combines audio image video and text becoming increasingly significant in several ai research fields it offers over 100 data augmentations based on people’s real life images and videos on platforms like facebook and instagram article github facebook blog
1,decline of traditional state space models it seems that recurrent neural networks have overtaken traditional state space models for time series models is this because traditional state space models require the analyst to make certain assumptions about how the system transitions between different states whereas a recurrent neural network can consider a wide combination of states through hidden layers and deep architecture
0,struggling to find a data job after being laid off in august hi all this is really stupid but i ve been struggling to find work i have 3 years experience as a data analyst in a research institute and a data analytics and modeling degree from an accredited university in the beginning i wasn t applying as religiously because of how overwhelmed and burnt out i was but i have consistently been applying to jobs since january i have experience in matlab sql r python i also have a publication under my name starting to feel a little bit helpless i have gotten to the technical interviews at 2 different companies which obviously i didn t pass for these i prepared doing coding problems and basic machine learning i d love some study recourses if available more than anything i have had a hard time even getting the initial interview i always get the second interview after a behavioral one really any advice would be great i can black out parts of my resume and post it i ve always felt really insecure job hunting after being in abusive relationships where my partners have put me down and told me i wasn t smart enough
1,question which statistical test to use log binomial cox something else i am wanting to perform a multivariate analysis of a binary outcome participants entered a testing period at different times and were tested at random some were never tested other were tested many times most tests were negative only 1 5 of all participants ever returned a positive test all testing periods were abruptly ended at the end of the study there are a few covariates including the number of times a person is tested i would also like to include the length of the testing period in days what would be the most appropriate test to perform a multivariate analysis where the outcome is whether the participant tested positive 1 or not 0 logistic regression log binomial to get risk ratio or would a cox model be more appropriate thank you for any help you may offer
0,data scientist responsibilities in a data analyst job description i have an interview for a data analyst position coming up soon and have several questions in the job responsibilities it mentioned typical responsibilities for a data analyst except for predictive analytics isn t this a data scientist s responsibility since it would require knowledge a typical data analyst would not know would it be fair to mention in the interview the salary range was 60 70k assuming i can convince them it s a data scientist position and switch the title would i be able to negotiate a salary above the range x200b thanks in advance
1,question what is the essence of combining ar and ma models into arma or arima i have always wondered why ar and ma are combined to form an unified arma or arima model my thinking is that a time series comprises of the below yt signal noise eq1 the ar part models a lagged version of the dependent variable there by increasing signal of finding any correlation structure perhaps a weak casualty too thus ar amplifies the signal in the above equation eq1 the ma part models the error or white noise i e to predict a future value it kind of course corrects by factoring in previous errors thus ma reduces the noise in eq 1 is my intuition or thinking correct if not why are the ar and ma terms merged to form a unified model would be grateful for the comments or clarification
2,research discussion roadmap for a research scientist position hello everyone i m currently in my 4th year of a phd in cse ai ml i m very much interested in the position of a research scientist since i have hardly one year for graduation i would like know the roadmap to achieve a good role after graduation i would like to ask suggestions advices and recommendations from the experts
1,have you ever had to derive a new estimator or test in industry i m curious under what circumstances a statistician would have to derive a new estimator or test in practice i would love to get a chance to use what i learned in math stats someday but i can t think of a situation i ve encountered where i d need to derive something new if i have i didn t recognize it and found an existing approach that was acceptable are there certain application areas where this is more likely to happen
0,nlp for contract review hi everyone i m in unfamliar territory honestly but i ve been given an opportunity to work on an nlp project for contract reviews and i d rather give it my best shot than anything else so here s what i can share i need to redline contracts where a prospective customer has violated one of our pre defined rules prepared in a separate file i have a corpus of contracts that are 50 standardized meaning paragraphs are numbered in order in a specific format the other 50 of contracts are in an unfamliar format unique to that particular customer engagement however the content of interest is in there what i need to do is sift through these contracts and redline specific parts of the contract that violate the pre defined rules x200b here are a couple thoughts i have validate that the specific rules i ve been given are actually representative of the examples in the corpus so that i will be looking for specific group of words text matches easy option try to segment the document into sections easy with the ordered ones little trickier with the non standard ones and then based on section identified looking for words features of interest x200b appreciate any insight or even books articles for further reading x200b thanks
2,how deep have you stacked rnn layers when it comes to convolutional networks i see a lot of techniques for creating deeper and deeper models these include relu activations batch norm between layers residual networks highway networks etc however when it comes to stacking rnn layers on top of each other all i ever see is maybe a few stacked layers and a dense output i don t see any reason why for instance you can t build something like a resnet out of rnn layers x skip x x lstm 512 x x lstm 512 x x x x skip x skip x x lstm 512 x x lstm 512 x x x x skip so what i am asking is how deep have you ever stacked rnns has anyone tried seen something like what i have above would there be much point in doing so
2,understanding the application and the relevance of the representor theorem in machine learning i have often seen the representor theorem mentioned in machine learning but i have never been able to fully understand the application and relevance of this theorem in machine learning can someone please try to explain this in a simpler way how is it different from mercer s theorem thanks
2,launching jupyter notebooks on aws with a single command after my last post on nimbo a few people asked if we had an alternative to google colab but for aws well we now do d if you setup nimbo you can just run nimbo notebook and it will sync your code data and environment and launch a notebook on a remote instance which you can access in your localhost now you can make use of your aws credits to get a colab like experience or dare i say better than colab because we do all the setup for you i hope at least some of you find this useful and as always i m happy to receive feedback
1,careers for ms statistics without coding what careers are there for people with ms in statistics that doesn’t involve coding i feel like for any good high paying job these days you basically need to be a great coder
2,how to do project planning for research heavy projects in the private sector assume you are being tasked coming up with a project plan e g creating jira epics for an application built around machine learning how do you come up with etas this is already guess work for traditional software engineering but how do you handle this when the machine learning part will require prototypes and some research before you can even start speccing out the subtasks for its implementation i do have a phd i know how to plan and carry out a research agenda but their time estimates are always fuzzy when looking more than 1 2 months ahead you might also have to adjust directions and shuffle priorities i have found repeatedly that my non technical supervisors just don t understand it is not like front end where you can just compile a list of buttons features to add not research research more like investigating different methods proposed in the scientific literature
1,how do you get the z score out of only a percentage let s say we re planning on constructing a confidence interval we haven t done any research yet so we don t have the mean the standard deviation anything we want to set the confidence level at 95 2 5 tail probabilities without the use of a calculator or a z score chart how would we get the z score out of 95 i m guessing there s an equation for this if so what is that equation a solution i see is rearranging the equation for the bell curve to isolate x but i don t see how i could do it since i m not good enough at math to do that i m sorry if somebody has posted this question before but i can t find anything on this anywhere on the internet
0,are there publications you can write for other than tds just curious if towardsdatascience is really the only publication you can write for nowadays
1,education uncertainty in statistical modeling explained intuitively here s a video by me a professor at cmu about how to think about uncertainty and confidence in statistical modeling i d love your feedback x200b
0,cdp xcdp for beginner data scientist hi everyone did anyone work as a data scientist in ecommerce industry i am currently working with bloomreach which can provide full customer experience without really knowing advanced programming i am looking for something cheaper alternative tool for data science in ecommerce tracking data analysis predictions single customer view omnichannel communication recommendation etc what tools combination of tools are best for you i tried almost all cdp demos but i want to know some experiences from e commerce segment for info we are small agency focused on automation and data
1,what prevents automated exploration of alternative model parameterizations in mcmc i have a q regarding the use of mcmc to sample from the target distributions of varying geometries specifically wrt the automated exploration of joint posteriors of models with both centered and non centered parameterizations i feel that when implementing e g hierarchical glms in stan i must often rely on vague heuristics re the prior magnitude of the parameters themselves for whether one one form or the other will improve sampler efficiency and sampling of some parameters in the same model may benefit from the non centered form where sampling for other parameters may not often the final arbiter of what’s appropriate where is observed performance since occasionally the non centered form will sample more efficiently since reparameterization is such a straightforward rote procedure why not incorporate it into something like the warm up process a user could specify the model in its fully centered form which read aloud is often much easier for me to parse at least and an algorithm could toggle on and off different parameterizations and try to adaptively identify which transformed target distributions are easiest to explore according to convergence mixing diagnostics like e fmi or w e is the reason this is not done just that warm up would have to start from scratch with each parameterization but would it even so it seems like something that would be fairly straightforward to implement even at the cost of lots of early redundancy certainly faster than doing everything by hand just something that enumerates some subset of possible non centered parameterizations launches however many independent samplers and then prunes the ones that are sampling poorly might offer considerably improved convenience and efficiency alternatively if there are hard and fast rules re when one parameterization is more efficient it seems like those too could be caught and implemented automatically am i missing something here for why this isn t done
2,i created a page to compare cloud gpu providers i use cloud gpus sometimes when i need a lot of gpus for a short period of time and found it quite tricky to compare prices and features across them so i just created this page which shows a summary of all the main ones i know about it s a small add on to a project i was already running which aggregates ml competitions from across multiple platforms everything is open source the file that drives it all is here if you spot any inaccuracies feel free to send me pm tweet ml contests submit a pull request i d love to hear your feedback suggestions
1,what is the definition of effect size in statistical theory what is the definition of effect size in statistical theory i have searched in many mathematical statistics books but haven t found one that mentions the concept they do clearly define power level of significance is effect size not a well defined concept i was curious that how the minimum sample size for a test is computed from a desired power level of significance and effect size in mathematical statistics books the minimum sample size is computed from desired power and level of significance so how is effect size involved then thanks
1,trying to examine if there are statistically significant differences between multiple variables in 4 groups so i have 4 groups of data grouped by me and in each group there are 5 measured variables these are all vectors with different lengths different numbers of samples between groups i have tried using tests like kruskal wallis and anova but these only allow me to compare 1 variable at a time between the groups i have also tried manova and linear discriminant analysis but hit a bit of a dead end with that because my data re projected into 2d space didn’t show that it was split into clear regions of the plot by group and isn’t it meant to what i want is to be able to do is 1 examine if there are statistically significant differences between the groups in terms of the measured variables 2 given a value for each of the variables predict which group that data point would belong to can anyone offer any suggestions
2,stylegan2 implementation with side by side notes implemented stylegan2 model and training loop from paper analyzing and improving the image quality of stylegan code with annotations this is a minimalistic implementation with only 425 lines of code and lots of documentations and diagrams explaining the model github paper on arxiv
1,question best statistical test to use for interaction of two independent variables i am doing a study on the effects of obesity and socioeconomic status on patient outcomes i would like to demonstrate whether socioeconomic status drives the historically poor outcomes in obese patients so which statistical test would be best since i am looking at the interaction between two variables and their effect on patient outcomes my first thought was to use a two way anova but i am very new to statistics and would appreciate any help
0,sell me on your physical input set up keyboards mice accessories
2,conv2d filter progression grow or shrink i m experimenting with simple conv2d based cnns these are typically 4 6 layers i believe the conventional wisdom is that the first early layers typically encode primitive features color gradients lines etc and later layers encode more complex features like the eye of an animal i want to compare two hyperparameter strategies relating to the number of filters per layer one is growing filters an the other is shrinking filters for a 5 layer cnn i might pick 16 32 64 64 128 and for comparison i d pick the opposite 128 64 64 32 16 i run these two models using the same dataset kaggle cats and dogs dataset and configuration of data generators empirically it seems that i get better performance with the second approach shrinking filter size maybe my data has a larger number of primitive features and fewer complex features is this a data dependent observation or is it generally true or is this just a matter of try it and see what works
1,pls help i am so high if you take 50 red m ms and 50 blue m m’s in a bowl and mix them all together what are the odds of randomly picking both a red and blue m m at the same time i hope this makes sense edit you are picking 2 m ms at the same time thank you
0,i m looking for a simple tool that i can color 100 dots symbols that represent 100 people i m not a data scientist so sorry if this is inappropriate i want to have 100 symbols small circles to visualize demographics i can t find such charts in excel or other simple chart tools what s the easiest way i can achieve this and if this is inappropriate here which sub should i ask for it
0,reccomendations on vulnerability scanners we are running a kubernetes based development environment where data scientists are free to work on anything they deem appropriate we need to ensure they are not introducing vulnerabilities does anyone have any tools they recommend for continuous scanning and reporting
1,how do heatmaps work i am interested in learning about the mathematical and statistical formulas used in making heatmaps choropleths such as the one below made in r does anyone know if it is possible to understand the exact math formulas behind the coloring shading in this map i tried consulting the official documentation for the functions used to create this map but no where does it explain how the colors are finalized i suspect that this somehow might be related to kernel density estimation furthermore i have a feeling that the max and the blur options are actually parameters used in the final coloring of this map does anyone know if it is possible to understand the exact formula used behind the coloring thanks
1,introduction to bootstrapping book giveaway dear all a new book about statistical bootstrapping has been published aimed at students practitioners and researchers the book provides an introduction to the technique besides the theoretical foundations practical examples are given in python and stata this book was written for very beginners and only the very basics stats i maybe ii are required to understand the content i am happy give away a few digital copies pdf i would very much appreciate a short review online personal blog website webshop etc bootstrapping an integrated approach with python and stata isbn 978 3110694406 please pm me with your email and a very short summary of your status student researcher etc many thanks edit due to the large interest i will send out the book about saturday evening european time thanks for the interest edit2 done if you pmed me but did not receive an email please check your spam folder as well message me again and please make sure to include your email thanks a lot edit3 i just realized many people contacted my in the messenger in reddit i did not see this until now my apologies
2,how to turn minecraft maps into photorealistic 3d scenes explained did you ever want to quickly create a photorealistic 3d scene from scratch well now you can the authors from nvidia in their paper gancraft unsupervised 3d neural rendering of minecraft worlds proposed a new neural rendering model trained with adversarial losses without a paired dataset yes it only requires a 3d semantic block world as input a pseudo ground truth image generated by a pretrained image synthesis model and any real landscape photos to output a consistent photorealistic render of a 3d scene corresponding to the block world input check out the full paper explanation on my channel here is an example of the model outputs looks like something out of a ps3 game but still very impressive full explanation post arxiv project page more recent popular paper explanations dino mlp mixer vision transformer vit
1,pros and cons working as a contractor for pharma company i am expecting an offer for a long term contract as sr data scientist for a medium size pharmaceutical company it is a unique position that i really like however i have never worked as a contractor so i want to be informed here are my thoughts pros 1 big salary and title increase from my current academia job 2 get my foot in the doors of pharma industry i found it is almost a norm to start as a contractor cons 1 minimal benefit bad insurance plans and very few pto days no more long vacation and may have to postpone family planing 2 the company is unlikely to invest in me for further career development i had a zoom interview with the hiring manager for 40mins and he decided to give me the offer 3 uncertain whether or when a permanent position will open 4 being treated as second class citizen which is likely to happen anyway since i don t have a phd or md anything else
0,how interpertable are regression models i was recently reading some articles on the importance of interpertability when dealing with blackbox models blackbox models like neural networks are said to have a very low level of interpertability because they don t allow the analyst to understand why the model is making a certain predictions for an individual observation on the other hand models like decision trees and regression models are said to have much higher levels of interpertability in a general sense i can understand why models like decision trees are interpretable because they literally provide the analyst with a set of fixed rules that explain how to classify an individual observation if you look at a regression model e g salary 5 3 height 2 weight 15 8 age a regression model can allow the analyst to understand how much each variable contributes to the prediction e g in this example age contributes more to the prediction by a factor of almost 8 times and you can also find out how statistically significant each variable is e g indivudal p value of each regression coefficient is this what is meant by the interpertability of a regression model thanks
0,rmsprop algorithm in machine learning why square the gradients i’m learning about rmsprop and have read about it quite widely around the web but am finding the explanations lacking on one key detail tl dr in the title lol it’s clear that the whole point of rmsprop is to replace a static learning rate with a dynamic learning rate that is a function of the size of the derivative because in rmsprop you divide the learning rate by the square root of the moving average of the square of the gradient the effect of this on the learning rate is to grow it when the derivative is small and shrink it when the derivative is large this helps to slow down learning i e weight update steps as the model approaches local minima because the function is flatter and thus gradients smaller in these regions that’s all fine and good but why square the gradient looking over the equation wouldn’t the same effect happen if we just used the gradient without squaring it is it just that we want to make small gradients even smaller or is it that positive and negative gradients would cancel each other out and squaring fixes this if the latter i e squaring to avoid cancellation why not just take the absolute value afaik were not differentiating the rmsprop equation so the discontinuity at y 0 characteristic of absolute values shouldn’t be a problem relatedly why take the square root when dividing the learning rate interpretability of the units isn’t important here like it is for variance vs stdev in statistics and the dynamic growing shrinking of the gradient would occur just the same whether the took the square root or not so what benefit does taking the square root bring
2,genetic algorithm for feature selection there are many ways of improving machine learning model performance one such is feature selection amongst many feature selection techniques genetic algorithm is one i have created a python library that helps you perform feature selection for your machine learning models it helps you identify the best set of features for your model feel free to use it pip install evolutionaryfs example notebook pypi page with documentation
2,does it make sense to generate sentences with transofmrer s encoder quite a few vision language papers pretrain bert based model with image text data and finetune for image captioning task but there is no decoder involved to generate sentences does that make sense and what s the main difference between using t s encoder to do the sentence generation and do it with a t decoder
1,this may be a noob question but how do i interpret the confint function how can i see if the conf int contains 0 this is what comes out of the confint function
0,thoughts on approaching multiple time series i have the order dates of a 100 different products going back more than a decade and want to do forecasting to the best of my ability the data is clean and consistent though some items aren t ordered very often the two approaches i ve thought of is 1 to do a monthly aggregate of how much product was ordered across all items and do a basic sarima with 1s for all values and a time lag of 12 months 2 to use the prophet library to predict the future though these results have been poor due to the items having a greater volatility and growth as of late due to covid product output and other outside factors avenues i m considering 1 hyperparameter tuning for sarima 2 using the monthly aggregate of all items to help forecast each individual item as building sarima models for each individual item may not be as useful due to a lack of data as some items may not be ordered at all some months 3 read more of the prophet documentation to try to adjust for the extreme volatility in my data 4 try messing around with ml time series forecasting libraries and see what can be done thoughts anything else i should try links or topics i should look into for finding the best solution
1,literature on monte carlo simulation i am looking for a standard work on monte carlo simulation can anyone point me towards some books you enjoyed the book should function as a reference for most common topics relating mc and maybe explain some statistical concepts needed on the way i have studied theoretical physics so i have heard about a bunch of things but i want to develop a deeper intuitive understanding of the matter if you can recommend a statistics book that is written didactically well i m interested too thank you in advance
0,is the market for data engineering significantly better than data science specifically for entry level apologies if this isn t the correct sub i believe i ve seen similar posts on here before so i figured it d probably be fine title says it all thus far i ve been focusing my search on data science and to be honest data analytics positions for those of you within data engineering do you feel that the entry level market is significantly less saturated or about the same would really like to hear from people who have experience in that field rather than guesses from those who ve exclusively had experience with ds
0,discovering column mappings i have a challenge to work on at work and am trying to figure out the approach we have an internal system that stores transactional data in a tabular form we receive daily files with data from the same domain transactions metadata but the column names are not standardised and the data fields are not always the exact same e g the amount field may have 3 digits behind the comma where our system expects 1 digit or what our system calls amount might be called quantity1 in the incoming files etc we have a manual mapping and transformation defined for each incoming file but the volume of different formats and sources is ever increasing im looking for a way to take any input file and to train a model that predicts for each column what the most likely corresponding column in the target file is i ve been looking into a few things using nlp spacy to train a model that recognises patterns in the column data e g numeric period comma is likely to correspond to amount i ve also looked at modeling the data and extracting an rdf representation using a open source tool called karma to see if i can train a model on a network graph but really struggling to see how to implement this is anyone aware of the formal name of this type of problem and if there are tried and tested approaches implementations out there that i could build upon
0,what are some applications of data science in digital marketing hi there i am a student pursuing a master s degree in applied statistics analytics i don t have any domain knowledge of digital marketing i wanted to get some hands on experience so i got in touch with a digital marketing agency through one of my contacts i had a short meeting with one employee of this agency and he was interested in knowing how i can help him fine tune campaigns for the clients x200b if you can share some use cases of analytics in this field or guide me to any resources for learning the same it would be a lot helpful thank you edit this agency only runs google ppc ads
2,different angle estimate i was wondering if any work has ever been done on utilizing machine learning to generate different angles of a photo e g if i take a photo of a dog from the front it could generate different angle views
2,huggingface transformers now extends to computer vision huggingface just released version v4 6 0 of their huggingface transformers framework with support for three vision transformers vit by google deit by facebook research and clip by openai these three architectures can now be loaded from pytorch and load either original checkpoints contributed by the model authors or any checkpoint uploaded by the community on the hugging face hub with support for inference widgets like the image classification widget for vit vit and deit get state of the art results in image classification and clip can be used for a flurry of tasks including image text similarity and zero shot image classification see the release notes for version v4 6 0 vit and deit heavily benefited from ross wightman s timm framework which offers a number of great vision models it is released alongside a few notebooks to play with the models inference with vit and training vit
0,disillusioned with the field of data science i’ve been in my first data science opportunity for almost a year now and i’m starting to question if i made a mistake entering this field my job is all politics i’m pulled every which way i’m constantly interrupted whenever i try to share any ideas my work is often tossed out and if i have a good idea it’s ignored until someone else presents the same idea then everyone loves it i’m constantly asked by non technical people to do things that are incorrect and when i try to speak up i’m ignored and my manager doesn’t defend me either i was promised technical work but i’m stuck working out of excel and powerpoint while i desperately try to maintain my coding and modeling skills outside of work i’m a woman of color working in a conservative field i’m exhausted is this normal do i need to find another field are there companies types of companies that you recommend i look into that aren’t like this this isn’t what i thought data science would be edit thank you for the responses everyone i’ve reached out to some of you privately and will try to respond to everyone else based on the comments and some of the suggestions which were helpful but already tried i think it’s time to plan an exit strategy being in this environment has led to burnout and mental physical health is more important than a job to those of you suggesting this as an opportunity to develop soft skills or work on my excel ppt skills that’s actually exactly how i pitched it to myself when i first started this role and realized it wouldn’t be as technical as i’d like but being in an environment like this has actually been detrimental to my soft skills i’ve lost all confidence in my ability to speak in front of others and my deck designs are constantly tossed out even after spending hours trying to make them as nice as possible to anyone else reading this that is experiencing this you deserve better you do not have to put up with this in the name of resilience at a certain point you are just ramming yourself into a wall over and over again others in my organization were getting to work on data science work so it wasn’t a bait and switch for everyone just some of us coincidentally all women i’m not going to leave ds yet i worked too hard to develop these skills to just let them go to waste but i think an industry change is due
1,is there a way of working out prevalence with multiple variables and over time hello i m trying to extrapolate prevalence data for a medical disorder that impacts both upper and lower extremities and how that prevalence has changed over time is this possible n 1 736 baseline prevalence upper limbs 30 lower limbs 41 5 year follow up prevalence upper 36 lower 56 this particular line has confused me time trends were as follows the proportion of residents who had no contractures on admission remained contracture free after 5 years was 59 7 for upper limbs and 39 8 for lower limbs while the proportion of residents who had developed new contractures after 5 years was 15 1 for upper limbs and 26 5 for lower limbs because there was no data available on those who were affected with both upper and lower i m uncertain as to how to continue thanks in advance
1,is age considered as a numerical or categorical value hello everyone should i apply pearson s correlation coefficient for trying to find the force of the relationship between age and fee of a soccer player in other words is age really to be considered a numeric value in this case or is age more categorical my model does show a non existing correlation see below when using pearson but i feel like perhaps i shouldn t be using pearson for this link of graph
1,career current phd stats student seeking advise i m currently at the end of my phd in computational statistics it s been a wild ride although i love research i wouldnt want to continue at university so i wanted to ask if you have any names tipps of eu based outer university research facilities or even straight industry ones i just dont want to stay at uni with all its politics but hope to somehow be able to stay in research any ideas would be greatly appreciated
1,m s in statistics at sdsu vs sjsu i have some insight into the program at sjsu since i ve taken a class there and it seems like a solid program i like a few of the profs i m wondering has anyone done the ms program at sdsu or if there are math stats undergrads at sdsu as well what is was your experience like how good are the professors and how challenging is the program i m currently living in san diego but trying to decide if i should pursue one program over another thanks in advance
1,can a model satisfy the gauss markov conditions blue if there s omitted variables bias
2,feedback for the first edition of the gradient s newsletter hi there i am one of the editors from the gradient which i think or hope many on here have seen and like having been around for more than 3 years now we ve decided to branch out beyond what we ve been doing and just released the first edition of our newsletter the update though admittedly i am making this post partially to promote it since it s the very first one i also really want to ask for your opinions on the format length content of it and whether you find it compelling we discussed how to create something we thought was different from other newsletters out there and actually interesting worth reading but it s hard to tell if what we came up with is actually good so would appreciate your feedback
2,before i re attempt docker gpu pass through on windows promising signs from wsl 2 gpu support for docker on windows is tensorflow gpu acceleration now possible running in windows based docker containers before i revisit this and sink huge amounts of time trying to do the impossible again has anyone had success with this the learning experience was ‘fun’ the first time but probably much less so the second
1,creating even levels based on number of patients using r hi everyone i wanted to ask a stats wiz here if they can help me and my lack of stats training i am conducting research with hospital data and pulled socioeconomic variables of the patients from american community survey acs with this i created an ses index with principal components analysis pca method giving each patient an ses index ranging from 10 to 10 question problem i have around 6000 patients in the group i am aiming to create 5 separate socioeconomic levels based on the ses index think 5 low ses 4 moderately low 3 average 2 moderately high 1 high ses i want to split the groups evenly so that roughly the same number of patients are represented in each group would anyone have a statistical approach they recommend to do this let me know if the question isn t clear thanks in advance
0,does your company make use of the skills they expected from you when you applied i have noticed more and more data analytics engineer and science roles require extensive knowledge of tableau powerbi and of course sql but i want to know how many companies actually make use of these skills and how many just list them as requirements because they know they are supposed to
0,experienced data scientists what advice could you give to a junior data scientist working at a start up i ll keep this short the start up were impressed with me when i was a software developer for them i have a mathematical background and my masters was on a data science working with ai phd students i have a lot of knowledge in algorithms cloud and best ci cd practises and i was previously a phd student in a i didn t finish for valid reasons x200b the company has asked me to take the lead on projects despite my lack of experience and due to the nature of startups there s not much structure and mentorship involved but they believe in me i m extremely motivated to do well and constantly learn for me and my company so i was wondering the data science journey might be more smooth if i make this post x200b i know this is kind of vague so if you need more information about me let me know x200b thanks a lot everyone
0,visualizing multidimensional data 128 dimensions 2d or 3d i am using programs like node2vec graph2vec doc2vec etc to build a knowledge graph the output of all of them comes in 128 length vectors is it feasible to try and compress the length to 2 or 3 dimensions so that i might visualize what any of the above methods have accomplished if it it feasible than can i get pointed in the right direction for python libraries packages etc does not need to be perfect just needs to give more than a csv file lol thank you
0,modern infrastructure and data science i ve spend the last years mostly in dl and there i m am quite happy with our current tooling and infrastructure however lately i ve noticed that there is another area of data science where i now find myself lacking knowledge now it seems i ve missed too many trends while i only looked into dl in our company there is an undisputed trend away from larger standalone machines towards lots of small instances in particular we have set up some on premise k8s clusters with the possibility to launch plenty of pods on many worker nodes however individual worker nodes support very limitted amounts of ram since this transition was managed by people with much deeper knowledge about infrastructure and modern software engineering than me i ll trust they know what they are doing now what i observe in our data science team there are still lots of applications that require huge amounts of ram sometimes unncessarily and simply because they re written by novices but mostly for good reasons often large amounts of data has to accessed with fast random access hashmap like access still is a great way to do this and i ve been seeing everything from very basic solutions à la just put it in a large dict over intricate special pupose indices into compress data to well known data structures tries btrees bloom filters used just like your algoirhtms and data structures prof would have wanted them to be this means we re still using a lot of legacy hardware running programs that use 30 40 or even 60gb ram at times such instances are not supported by the new kubernetes could an obvious pattern to resolve this would be to just move data into third party software and frameworks that allow horizontal scaling deploy them into kubernetes and use them as data backend for everything large pandas df spark large dictionaries redis mongo whatever and so on that said i see a few downsides 1 i really dread the management overhead for all these thrid party things new versions incompatibilities etc 2 it would take a lot of convincing to make all data scientists embrace the new paradigm and get comfortable with kubernetes at all i hope juypter enterprise gateway could help us here 3 i would imagine the new solutions would sometimes be slower than the status quo after all ram is pretty fast and some projects even condiered cache efficiency in the design of thier data structures outsourcing this to horizontally scaled off the shelf solutions is porbably only worth it once we scale it further than the sattus quo for which there is no imminent need what is your view on this is there a different way to approach this transition or do you have experience with similar transitions and would you say it was worth it in the long run
