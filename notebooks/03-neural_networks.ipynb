{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "Copy of Copy of 03-neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVW1_6Y2Qi5K"
      },
      "source": [
        "# Neural Network Training in Google Colab\n",
        "\n",
        "This notebook is used for training the neural networks in Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXXiFnVUQi5K"
      },
      "source": [
        "Choose `Runtime` > `Change Runtime type` > `GPU` from the menu. Run the following cell to confirm that your notebook has been connected to a GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwHtvLSAQi5L",
        "outputId": "50f00672-afaa-4b67-cf07-633198ad7d27"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jun 30 00:50:47 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUqaeR2CQi5L"
      },
      "source": [
        "Clone the repository and `cd` into it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-U524_KQi5M",
        "outputId": "87c01896-7238-4eba-a508-9c69f6a18f19"
      },
      "source": [
        "!git clone https://github.com/kingyiusuen/reddit-post-classification.git\n",
        "%cd reddit-post-classification\n",
        "%env PYTHONPATH=.:$PYTHONPATH"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'reddit-post-classification'...\n",
            "remote: Enumerating objects: 265, done.\u001b[K\n",
            "remote: Counting objects: 100% (265/265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 265 (delta 106), reused 248 (delta 89), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (265/265), 8.23 MiB | 18.12 MiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n",
            "/content/reddit-post-classification\n",
            "env: PYTHONPATH=.:$PYTHONPATH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8doIb-uQi5M"
      },
      "source": [
        "Install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHVC9-p1Qi5M",
        "outputId": "d69d062c-7fa5-4a87-8b32-dac4cc313f38"
      },
      "source": [
        "!make install"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pip install -e . --no-cache-dir\n",
            "Obtaining file:///content/reddit-post-classification\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-lightning==1.3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/6a/20d0bf3b967ab62333efea36fe922aaa252d1762555b4a7afb2be5bbdcbf/pytorch_lightning-1.3.5-py3-none-any.whl (808kB)\n",
            "\u001b[K     |████████████████████████████████| 808kB 7.5MB/s \n",
            "\u001b[?25hCollecting uvicorn==0.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/fe/a41994c92897b162c0c83e8ef10bec54ebdefbce3f3725b530d2091492ac/uvicorn-0.14.0-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 51.3MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.3MB/s \n",
            "\u001b[?25hCollecting hydra-core==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/cd/85aa2e3a8babc36feac99df785e54abf99afbc4acc20488630f3ef46980a/hydra_core-1.1.0-py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 57.7MB/s \n",
            "\u001b[?25hCollecting pandas==1.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/0a/90da8840e044c329a0271fb0244ff40a68a2615bc360c296a3dc5e326ab6/pandas-1.2.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9MB 47.8MB/s \n",
            "\u001b[?25hCollecting praw==7.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/3f/14c92b8a33564aaab1725162240ca081dc28d84e384bb1c08ada83485ccf/praw-7.3.0-py3-none-any.whl (165kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 60.5MB/s \n",
            "\u001b[?25hCollecting wandb==0.10.32\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/b4/9d92953d8cddc8450c859be12e3dbdd4c7754fb8def94c28b3b351c6ee4e/wandb-0.10.32-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 10.0MB/s \n",
            "\u001b[?25hCollecting hydra-optuna-sweeper==1.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/6e/a5afcd542744b3a4fc44f7925aa03065b22d578d7626386538dc97ff3284/hydra_optuna_sweeper-1.1.0-py3-none-any.whl\n",
            "Collecting fastapi==0.65.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/a8/a6be420593c4061c086e6d2ba47db46401d9af2b98b6cd33d35284f706d3/fastapi-0.65.2-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 52.1MB/s \n",
            "\u001b[?25hCollecting tensorboard!=2.5.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/21/eebd23060763fedeefb78bc2b286e00fa1d8abda6f70efa2ee08c28af0d4/tensorboard-2.4.1-py3-none-any.whl (10.6MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6MB 47.4MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/44/e21e1bc2f0a2955abd3ef0683dcd3bc74c29348c3af3b0b1028fd8a25bbd/torchmetrics-0.4.0-py3-none-any.whl (232kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 39.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (4.41.1)\n",
            "Collecting pyDeprecate==0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/14/52/aa227a0884df71ed1957649085adf2b8bc2a1816d037c2f18b3078854516/pyDeprecate-0.3.0-py3-none-any.whl\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 39.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.19.5)\n",
            "Collecting PyYAML<=5.4.1,>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 41.2MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3a/666e63625a19883ae8e1674099e631f9737bd5478c4790e5ad49c5ac5261/fsspec-2021.6.1-py3-none-any.whl (115kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 64.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (20.9)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.14.0->reddit-post-classification==0.1.0) (3.7.4.3)\n",
            "Collecting asgiref>=3.3.4\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/77/68b78d54f9865e1f4b8f8a9e0de15d328cddaa8a9cd5abeb69cb4077d9ab/asgiref-3.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: click>=7.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.14.0->reddit-post-classification==0.1.0) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/0f/7a0eeea938eaf61074f29fed9717f2010e8d0e0905d36b38d3275a1e4622/h11-0.12.0-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 61.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->reddit-post-classification==0.1.0) (1.4.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->reddit-post-classification==0.1.0) (1.0.1)\n",
            "Collecting omegaconf==2.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/96/1966b48bfe6ca64bfadfa7bcc9a8d73c5d83b4be769321fcc5d617abeb0c/omegaconf-2.1.0-py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 61.9MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 63.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core==1.1.0->reddit-post-classification==0.1.0) (5.1.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.5->reddit-post-classification==0.1.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.5->reddit-post-classification==0.1.0) (2.8.1)\n",
            "Collecting update-checker>=0.18\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/5f/3c211d168b2e9f9342cfb53bcfc26aab0eac63b998015e7af7bcae66119d/websocket_client-1.1.0-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 56.3MB/s \n",
            "\u001b[?25hCollecting prawcore<3,>=2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/27/e5ca770e299ed4e94646eb630a8c2dfbb202eefc4d17980550f4ec817e5a/prawcore-2.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.10.32->reddit-post-classification==0.1.0) (2.23.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 63.1MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.10.32->reddit-post-classification==0.1.0) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 63.1MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.10.32->reddit-post-classification==0.1.0) (1.15.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 62.1MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.10.32->reddit-post-classification==0.1.0) (3.12.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.10.32->reddit-post-classification==0.1.0) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting optuna<2.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/b4/a1a80252cef3d8f5a0acdf6e678d6dc07e2e6964ee46d0453a2ae1af1ecb/optuna-2.4.0-py3-none-any.whl (282kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 56.4MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f2/2d5425efe57f6c4e06cbe5e587c1fd16929dcf0eb90bd4d3d1e1c97d1151/pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 37.7MB/s \n",
            "\u001b[?25hCollecting starlette==0.14.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/34/db1890f442a1cd3a2c761f4109a0eb4e63503218d70a8c8e97faa09a5500/starlette-0.14.2-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.31.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (0.4.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (57.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.34.1)\n",
            "Collecting aiohttp; extra == \"http\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core==1.1.0->reddit-post-classification==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb==0.10.32->reddit-post-classification==0.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb==0.10.32->reddit-post-classification==0.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb==0.10.32->reddit-post-classification==0.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb==0.10.32->reddit-post-classification==0.1.0) (2021.5.30)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 62.0MB/s \n",
            "\u001b[?25hCollecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/11/aea1cacbd4cf8262809c4d6f95dcb3f2802594de1f51c5bd454d69bf15c5/cliff-3.8.0-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 62.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna<2.5.0->hydra-optuna-sweeper==1.1.0->reddit-post-classification==0.1.0) (1.4.18)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/80/ef186e599a57d0e4cb78fc76e0bfc2e6953fa9716b2a5cf2de0117ed8eb5/alembic-1.6.5-py2.py3-none-any.whl (164kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 63.6MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/32/e6/e9ddc6fa1104fda718338b341e4b3dc31cd8039ab29e52fc73b508515361/colorlog-5.0.1-py2.py3-none-any.whl\n",
            "Collecting cmaes>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/01/1f/43b01223a0366171f474320c6e966c39a11587287f098a5f09809b45e05f/cmaes-0.8.2-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (1.3.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 60.3MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 56.5MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (21.2.0)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna<2.5.0->hydra-optuna-sweeper==1.1.0->reddit-post-classification==0.1.0) (2.1.0)\n",
            "Collecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 50.5MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/6a/e929ec70ca05c5962f6541ef29fb9c207dd41f0f2333680fa39f44fa4357/cmd2-2.1.1-py3-none-any.whl (140kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 49.7MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/e0/1d4702dd81121d04a477c272d47ee5b6bc970d1a0990b11befa275c55cf2/pbr-5.6.0-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 65.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna<2.5.0->hydra-optuna-sweeper==1.1.0->reddit-post-classification==0.1.0) (1.1.0)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/54/dbc07fbb20865d3b78fdb7cf7fa713e2cba4f87f71100074ef2dc9f9d1f7/Mako-1.1.4-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 63.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.5->reddit-post-classification==0.1.0) (3.1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from PrettyTable>=0.7.2->cliff->optuna<2.5.0->hydra-optuna-sweeper==1.1.0->reddit-post-classification==0.1.0) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/2c/4c64579f847bd5d539803c8b909e54ba087a79d01bb3aba433a95879a6c5/pyperclip-1.8.2.tar.gz\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna<2.5.0->hydra-optuna-sweeper==1.1.0->reddit-post-classification==0.1.0) (2.0.1)\n",
            "Building wheels for collected packages: future, antlr4-python3-runtime, subprocess32, pathtools, pyperclip\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491070 sha256=1e699cca0e03f7e9ff6a164b0ccd43c078f6bf0dabb6bed2bb0c6bd261d0dba5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-15e33b8c/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=be2dbd00a20c240bea40d6c9ec2a68da44417fb974eb3ca449e34f6123aa4e36\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-15e33b8c/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=b40ea16618793fe7ef1075c81fb953a67db29a40b8687d0709ece976f03ef708\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-15e33b8c/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=a5d600194fa9b0d7d7a32bcce996422154f4a84b9bbec73aca4e5a73f661df69\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-15e33b8c/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-cp37-none-any.whl size=11136 sha256=2007e88cc49c90cd7e7c8af40dc58cf359a0f6c151727ac6ebaa1f342357c923\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-15e33b8c/wheels/25/af/b8/3407109267803f4015e1ee2ff23be0c8c19ce4008665931ee1\n",
            "Successfully built future antlr4-python3-runtime subprocess32 pathtools pyperclip\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, torchmetrics, pyDeprecate, future, PyYAML, multidict, yarl, async-timeout, aiohttp, fsspec, pytorch-lightning, asgiref, h11, uvicorn, threadpoolctl, scikit-learn, antlr4-python3-runtime, omegaconf, hydra-core, pandas, update-checker, websocket-client, prawcore, praw, subprocess32, docker-pycreds, smmap, gitdb, GitPython, shortuuid, sentry-sdk, configparser, pathtools, wandb, pbr, stevedore, pyperclip, colorama, cmd2, cliff, python-editor, Mako, alembic, colorlog, cmaes, optuna, hydra-optuna-sweeper, pydantic, starlette, fastapi, reddit-post-classification\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Running setup.py develop for reddit-post-classification\n",
            "Successfully installed GitPython-3.1.18 Mako-1.1.4 PyYAML-5.4.1 aiohttp-3.7.4.post0 alembic-1.6.5 antlr4-python3-runtime-4.8 asgiref-3.4.0 async-timeout-3.0.1 cliff-3.8.0 cmaes-0.8.2 cmd2-2.1.1 colorama-0.4.4 colorlog-5.0.1 configparser-5.0.2 docker-pycreds-0.4.0 fastapi-0.65.2 fsspec-2021.6.1 future-0.18.2 gitdb-4.0.7 h11-0.12.0 hydra-core-1.1.0 hydra-optuna-sweeper-1.1.0 multidict-5.1.0 omegaconf-2.1.0 optuna-2.4.0 pandas-1.2.5 pathtools-0.1.2 pbr-5.6.0 praw-7.3.0 prawcore-2.2.0 pyDeprecate-0.3.0 pydantic-1.8.2 pyperclip-1.8.2 python-editor-1.0.4 pytorch-lightning-1.3.5 reddit-post-classification scikit-learn-0.24.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 starlette-0.14.2 stevedore-3.3.0 subprocess32-3.5.4 tensorboard-2.4.1 threadpoolctl-2.1.0 torchmetrics-0.4.0 update-checker-0.18.0 uvicorn-0.14.0 wandb-0.10.32 websocket-client-1.1.0 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL4NissxQi5M"
      },
      "source": [
        "Run the following cells to start training and run hyperparameter optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_kfD639Qi5N",
        "outputId": "37b12169-3c98-4266-95c8-bb237f53b641"
      },
      "source": [
        "!python scripts/train.py -m model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna hydra.sweeper.n_trials=10"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-06-30 02:09:58,568]\u001b[0m A new study created in memory with name: no-name-66fe9300-7304-47f9-8874-d7de8fa27b90\u001b[0m\n",
            "[2021-06-30 02:09:58,568][HYDRA] Study name: no-name-66fe9300-7304-47f9-8874-d7de8fa27b90\n",
            "[2021-06-30 02:09:58,568][HYDRA] Storage: None\n",
            "[2021-06-30 02:09:58,568][HYDRA] Sampler: TPESampler\n",
            "[2021-06-30 02:09:58,568][HYDRA] Directions: ['maximize']\n",
            "[2021-06-30 02:09:58,572][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:09:58,572][HYDRA] \t#0 : optimizer.lr=0.009303199318889763 model.num_kernels=256 model.embedding_dim=256 model.dropout=0.1 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n",
            "[2021-06-30 02:09:58,805][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:09:58,817][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkingyiusuen\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:10:00.077227: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mjumping-feather-51\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/ng7g0u6e\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_020959-ng7g0u6e\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | CNN              | 2.8 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "2.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.8 M     Total params\n",
            "11.041    Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:03<00:00,  4.85it/s, loss=17.2, v_num=0u6e]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:03<00:00,  5.24it/s, loss=17.2, v_num=0u6e]\n",
            "Epoch 0: 100% 20/20 [00:03<00:00,  5.34it/s, loss=17.2, v_num=0u6e, val/acc=0.366, val/loss=1.680]\n",
            "Epoch 1:  90% 18/20 [00:03<00:00,  5.12it/s, loss=1.44, v_num=0u6e, val/acc=0.366, val/loss=1.680]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.96it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:03<00:00,  5.35it/s, loss=1.44, v_num=0u6e, val/acc=0.653, val/loss=0.919]\n",
            "Epoch 2:  90% 18/20 [00:03<00:00,  5.14it/s, loss=0.7, v_num=0u6e, val/acc=0.653, val/loss=0.919]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:03<00:00,  5.36it/s, loss=0.7, v_num=0u6e, val/acc=0.724, val/loss=0.731]\n",
            "Epoch 3:  90% 18/20 [00:03<00:00,  5.12it/s, loss=0.542, v_num=0u6e, val/acc=0.724, val/loss=0.731]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:03<00:00,  5.36it/s, loss=0.542, v_num=0u6e, val/acc=0.754, val/loss=0.706]\n",
            "Epoch 4:  90% 18/20 [00:03<00:00,  5.11it/s, loss=0.397, v_num=0u6e, val/acc=0.754, val/loss=0.706]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.23it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:03<00:00,  5.34it/s, loss=0.397, v_num=0u6e, val/acc=0.776, val/loss=0.657]\n",
            "Epoch 5:  90% 18/20 [00:03<00:00,  5.10it/s, loss=0.354, v_num=0u6e, val/acc=0.776, val/loss=0.657]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:03<00:00,  5.36it/s, loss=0.354, v_num=0u6e, val/acc=0.776, val/loss=0.657]\n",
            "Epoch 5: 100% 20/20 [00:03<00:00,  5.33it/s, loss=0.354, v_num=0u6e, val/acc=0.784, val/loss=0.643]\n",
            "Epoch 6:  90% 18/20 [00:03<00:00,  5.09it/s, loss=0.333, v_num=0u6e, val/acc=0.784, val/loss=0.643]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.72it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:03<00:00,  5.30it/s, loss=0.333, v_num=0u6e, val/acc=0.780, val/loss=0.649]\n",
            "Epoch 7:  90% 18/20 [00:03<00:00,  5.12it/s, loss=0.316, v_num=0u6e, val/acc=0.780, val/loss=0.649]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.61it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:03<00:00,  5.34it/s, loss=0.316, v_num=0u6e, val/acc=0.784, val/loss=0.653]\n",
            "Epoch 8:  90% 18/20 [00:03<00:00,  5.13it/s, loss=0.307, v_num=0u6e, val/acc=0.784, val/loss=0.653]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.95it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:03<00:00,  5.35it/s, loss=0.307, v_num=0u6e, val/acc=0.791, val/loss=0.650]\n",
            "Epoch 9:  90% 18/20 [00:03<00:00,  5.13it/s, loss=0.317, v_num=0u6e, val/acc=0.791, val/loss=0.650]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.79it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:03<00:00,  5.36it/s, loss=0.317, v_num=0u6e, val/acc=0.791, val/loss=0.651]\n",
            "Epoch 10:  90% 18/20 [00:03<00:00,  5.09it/s, loss=0.307, v_num=0u6e, val/acc=0.791, val/loss=0.651]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:03<00:00,  5.33it/s, loss=0.307, v_num=0u6e, val/acc=0.784, val/loss=0.651]\n",
            "Epoch 11:  90% 18/20 [00:03<00:00,  5.14it/s, loss=0.297, v_num=0u6e, val/acc=0.784, val/loss=0.651]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.93it/s]\u001b[A\n",
            "Epoch 11: 100% 20/20 [00:03<00:00,  5.37it/s, loss=0.297, v_num=0u6e, val/acc=0.787, val/loss=0.651]\n",
            "Epoch 12:  90% 18/20 [00:03<00:00,  5.10it/s, loss=0.299, v_num=0u6e, val/acc=0.787, val/loss=0.651]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.65it/s]\u001b[A\n",
            "Epoch 12: 100% 20/20 [00:03<00:00,  5.32it/s, loss=0.299, v_num=0u6e, val/acc=0.787, val/loss=0.651]\n",
            "Epoch 13:  90% 18/20 [00:03<00:00,  5.10it/s, loss=0.304, v_num=0u6e, val/acc=0.787, val/loss=0.651]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13: 100% 20/20 [00:03<00:00,  5.34it/s, loss=0.304, v_num=0u6e, val/acc=0.787, val/loss=0.651]\n",
            "Epoch 13: 100% 20/20 [00:03<00:00,  5.34it/s, loss=0.304, v_num=0u6e, val/acc=0.787, val/loss=0.651]\n",
            "[2021-06-30 02:10:56,473][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00,  9.93it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.7933884263038635, 'test/loss': 0.5318678021430969}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2697\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_020959-ng7g0u6e/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_020959-ng7g0u6e/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.29965\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 238\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 57\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019056\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.78731\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.65128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.79339\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.53187\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▆▇▇██████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▃▂▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mjumping-feather-51\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/ng7g0u6e\u001b[0m\n",
            "[2021-06-30 02:11:01,785][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:11:01,785][HYDRA] \t#1 : optimizer.lr=0.0021251467576750933 model.num_kernels=256 model.embedding_dim=128 model.dropout=0.30000000000000004 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n",
            "[2021-06-30 02:11:02,024][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:11:02,030][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:11:03.156074: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwild-water-52\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/lu3h4mrn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_021102-lu3h4mrn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | CNN              | 1.4 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.531     Total estimated model params size (MB)\n",
            "Epoch 0:  90% 18/20 [00:02<00:00,  7.06it/s, loss=4.55, v_num=4mrn]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 20/20 [00:02<00:00,  7.27it/s, loss=4.55, v_num=4mrn, val/acc=0.422, val/loss=2.630]\n",
            "Epoch 1:  90% 18/20 [00:02<00:00,  7.21it/s, loss=1.46, v_num=4mrn, val/acc=0.422, val/loss=2.630]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:02<00:00,  7.47it/s, loss=1.46, v_num=4mrn, val/acc=0.754, val/loss=0.681]\n",
            "Epoch 2:  90% 18/20 [00:02<00:00,  7.13it/s, loss=0.65, v_num=4mrn, val/acc=0.754, val/loss=0.681]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:02<00:00,  7.37it/s, loss=0.65, v_num=4mrn, val/acc=0.791, val/loss=0.583]\n",
            "Epoch 3:  90% 18/20 [00:02<00:00,  7.11it/s, loss=0.491, v_num=4mrn, val/acc=0.791, val/loss=0.583]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:02<00:00,  7.28it/s, loss=0.491, v_num=4mrn, val/acc=0.813, val/loss=0.543]\n",
            "Epoch 4:  90% 18/20 [00:02<00:00,  7.16it/s, loss=0.423, v_num=4mrn, val/acc=0.813, val/loss=0.543]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:02<00:00,  7.41it/s, loss=0.423, v_num=4mrn, val/acc=0.810, val/loss=0.551]\n",
            "Epoch 5:  90% 18/20 [00:02<00:00,  7.18it/s, loss=0.406, v_num=4mrn, val/acc=0.810, val/loss=0.551]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:02<00:00,  7.39it/s, loss=0.406, v_num=4mrn, val/acc=0.806, val/loss=0.543]\n",
            "Epoch 6:  90% 18/20 [00:02<00:00,  7.08it/s, loss=0.392, v_num=4mrn, val/acc=0.806, val/loss=0.543]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:02<00:00,  7.28it/s, loss=0.392, v_num=4mrn, val/acc=0.821, val/loss=0.536]\n",
            "Epoch 7:  90% 18/20 [00:02<00:00,  7.18it/s, loss=0.382, v_num=4mrn, val/acc=0.821, val/loss=0.536]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:02<00:00,  7.43it/s, loss=0.382, v_num=4mrn, val/acc=0.821, val/loss=0.539]\n",
            "Epoch 8:  90% 18/20 [00:02<00:00,  7.17it/s, loss=0.383, v_num=4mrn, val/acc=0.821, val/loss=0.539]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:02<00:00,  7.34it/s, loss=0.383, v_num=4mrn, val/acc=0.821, val/loss=0.539]\n",
            "Epoch 9:  90% 18/20 [00:02<00:00,  7.06it/s, loss=0.375, v_num=4mrn, val/acc=0.821, val/loss=0.539]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:02<00:00,  7.30it/s, loss=0.375, v_num=4mrn, val/acc=0.817, val/loss=0.540]\n",
            "Epoch 10:  90% 18/20 [00:02<00:00,  7.15it/s, loss=0.374, v_num=4mrn, val/acc=0.817, val/loss=0.540]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:02<00:00,  7.39it/s, loss=0.374, v_num=4mrn, val/acc=0.817, val/loss=0.540]\n",
            "Epoch 11:  90% 18/20 [00:02<00:00,  7.12it/s, loss=0.38, v_num=4mrn, val/acc=0.817, val/loss=0.540]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: 100% 20/20 [00:02<00:00,  7.28it/s, loss=0.38, v_num=4mrn, val/acc=0.817, val/loss=0.540]\n",
            "Epoch 11: 100% 20/20 [00:02<00:00,  7.28it/s, loss=0.38, v_num=4mrn, val/acc=0.817, val/loss=0.540]\n",
            "[2021-06-30 02:11:37,114][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00,  6.59it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.8595041036605835, 'test/loss': 0.4454779624938965}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2744\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_021102-lu3h4mrn/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_021102-lu3h4mrn/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.37981\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019097\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.81716\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.53992\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.8595\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.44548\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▃▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▇▇█████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mwild-water-52\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/lu3h4mrn\u001b[0m\n",
            "[2021-06-30 02:11:42,979][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:11:42,979][HYDRA] \t#2 : optimizer.lr=0.009648693745382655 model.num_kernels=256 model.embedding_dim=128 model.dropout=0.4 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n",
            "[2021-06-30 02:11:43,212][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:11:43,218][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:11:44.346428: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mautumn-wildflower-53\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2z9e865e\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_021143-2z9e865e\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | CNN              | 1.4 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.531     Total estimated model params size (MB)\n",
            "Epoch 0:  90% 18/20 [00:02<00:00,  6.99it/s, loss=11.5, v_num=865e]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 20/20 [00:02<00:00,  7.19it/s, loss=11.5, v_num=865e, val/acc=0.403, val/loss=1.310]\n",
            "Epoch 1:  90% 18/20 [00:02<00:00,  7.08it/s, loss=1.14, v_num=865e, val/acc=0.403, val/loss=1.310]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:02<00:00,  7.34it/s, loss=1.14, v_num=865e, val/acc=0.683, val/loss=0.898]\n",
            "Epoch 2:  90% 18/20 [00:02<00:00,  7.09it/s, loss=0.821, v_num=865e, val/acc=0.683, val/loss=0.898]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:02<00:00,  7.33it/s, loss=0.821, v_num=865e, val/acc=0.761, val/loss=0.725]\n",
            "Epoch 3:  90% 18/20 [00:02<00:00,  7.05it/s, loss=0.674, v_num=865e, val/acc=0.761, val/loss=0.725]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:02<00:00,  7.23it/s, loss=0.674, v_num=865e, val/acc=0.728, val/loss=0.727]\n",
            "Epoch 4:  90% 18/20 [00:02<00:00,  7.08it/s, loss=0.541, v_num=865e, val/acc=0.728, val/loss=0.727]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:02<00:00,  7.32it/s, loss=0.541, v_num=865e, val/acc=0.757, val/loss=0.704]\n",
            "Epoch 5:  90% 18/20 [00:02<00:00,  7.10it/s, loss=0.478, v_num=865e, val/acc=0.757, val/loss=0.704]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:02<00:00,  7.29it/s, loss=0.478, v_num=865e, val/acc=0.765, val/loss=0.688]\n",
            "Epoch 6:  90% 18/20 [00:02<00:00,  7.09it/s, loss=0.443, v_num=865e, val/acc=0.765, val/loss=0.688]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:02<00:00,  7.32it/s, loss=0.443, v_num=865e, val/acc=0.765, val/loss=0.696]\n",
            "Epoch 7:  90% 18/20 [00:02<00:00,  7.05it/s, loss=0.437, v_num=865e, val/acc=0.765, val/loss=0.696]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:02<00:00,  7.31it/s, loss=0.437, v_num=865e, val/acc=0.761, val/loss=0.693]\n",
            "Epoch 8:  90% 18/20 [00:02<00:00,  7.05it/s, loss=0.433, v_num=865e, val/acc=0.761, val/loss=0.693]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:02<00:00,  7.23it/s, loss=0.433, v_num=865e, val/acc=0.765, val/loss=0.691]\n",
            "Epoch 9:  90% 18/20 [00:02<00:00,  7.15it/s, loss=0.428, v_num=865e, val/acc=0.765, val/loss=0.691]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:02<00:00,  7.40it/s, loss=0.428, v_num=865e, val/acc=0.772, val/loss=0.690]\n",
            "Epoch 10:  90% 18/20 [00:02<00:00,  7.08it/s, loss=0.423, v_num=865e, val/acc=0.772, val/loss=0.690]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:02<00:00,  7.30it/s, loss=0.423, v_num=865e, val/acc=0.772, val/loss=0.690]\n",
            "Epoch 11:  90% 18/20 [00:02<00:00,  7.03it/s, loss=0.415, v_num=865e, val/acc=0.772, val/loss=0.690]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: 100% 20/20 [00:02<00:00,  7.23it/s, loss=0.415, v_num=865e, val/acc=0.772, val/loss=0.690]\n",
            "Epoch 12:  90% 18/20 [00:02<00:00,  7.09it/s, loss=0.414, v_num=865e, val/acc=0.772, val/loss=0.690]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12: 100% 20/20 [00:02<00:00,  7.26it/s, loss=0.414, v_num=865e, val/acc=0.769, val/loss=0.691]\n",
            "Epoch 13:  90% 18/20 [00:02<00:00,  7.11it/s, loss=0.422, v_num=865e, val/acc=0.769, val/loss=0.691]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13: 100% 20/20 [00:02<00:00,  7.33it/s, loss=0.422, v_num=865e, val/acc=0.769, val/loss=0.691]\n",
            "Epoch 13: 100% 20/20 [00:02<00:00,  7.30it/s, loss=0.422, v_num=865e, val/acc=0.772, val/loss=0.691]\n",
            "Epoch 14:  90% 18/20 [00:02<00:00,  7.13it/s, loss=0.426, v_num=865e, val/acc=0.772, val/loss=0.691]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14: 100% 20/20 [00:02<00:00,  7.29it/s, loss=0.426, v_num=865e, val/acc=0.772, val/loss=0.691]\n",
            "Epoch 14: 100% 20/20 [00:02<00:00,  7.29it/s, loss=0.426, v_num=865e, val/acc=0.772, val/loss=0.691]\n",
            "[2021-06-30 02:12:26,835][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00, 11.59it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.7933884263038635, 'test/loss': 0.5365856289863586}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2795\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_021143-2z9e865e/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_021143-2z9e865e/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.43037\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 255\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019147\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.77239\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.69075\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.79339\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.53659\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▆█▇███████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▃▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mautumn-wildflower-53\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2z9e865e\u001b[0m\n",
            "[2021-06-30 02:12:32,053][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:12:32,053][HYDRA] \t#3 : optimizer.lr=0.006570341721432181 model.num_kernels=128 model.embedding_dim=128 model.dropout=0.5 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n",
            "[2021-06-30 02:12:32,290][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:12:32,296][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:12:33.442912: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglamorous-dragon-54\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/jo0t2jvn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_021232-jo0t2jvn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | CNN              | 888 K \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "888 K     Trainable params\n",
            "0         Non-trainable params\n",
            "888 K     Total params\n",
            "3.554     Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:01<00:00,  8.97it/s, loss=5.37, v_num=2jvn]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:01<00:00,  9.65it/s, loss=5.37, v_num=2jvn]\n",
            "Epoch 0: 100% 20/20 [00:02<00:00,  9.72it/s, loss=5.37, v_num=2jvn, val/acc=0.485, val/loss=1.590]\n",
            "Epoch 1:  90% 18/20 [00:01<00:00,  9.68it/s, loss=1.1, v_num=2jvn, val/acc=0.485, val/loss=1.590]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:02<00:00,  9.91it/s, loss=1.1, v_num=2jvn, val/acc=0.698, val/loss=0.738]\n",
            "Epoch 2:  90% 18/20 [00:01<00:00,  9.51it/s, loss=0.709, v_num=2jvn, val/acc=0.698, val/loss=0.738]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:02<00:00,  9.79it/s, loss=0.709, v_num=2jvn, val/acc=0.698, val/loss=0.701]\n",
            "Epoch 3:  90% 18/20 [00:01<00:00,  9.72it/s, loss=0.608, v_num=2jvn, val/acc=0.698, val/loss=0.701]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:02<00:00, 10.00it/s, loss=0.608, v_num=2jvn, val/acc=0.754, val/loss=0.645]\n",
            "Epoch 4:  90% 18/20 [00:01<00:00,  9.73it/s, loss=0.49, v_num=2jvn, val/acc=0.754, val/loss=0.645]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:02<00:00, 10.00it/s, loss=0.49, v_num=2jvn, val/acc=0.757, val/loss=0.631]\n",
            "Epoch 5:  90% 18/20 [00:01<00:00,  9.73it/s, loss=0.461, v_num=2jvn, val/acc=0.757, val/loss=0.631]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:01<00:00, 10.01it/s, loss=0.461, v_num=2jvn, val/acc=0.765, val/loss=0.645]\n",
            "Epoch 6:  90% 18/20 [00:01<00:00,  9.70it/s, loss=0.428, v_num=2jvn, val/acc=0.765, val/loss=0.645]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:02<00:00,  9.98it/s, loss=0.428, v_num=2jvn, val/acc=0.772, val/loss=0.632]\n",
            "Epoch 7:  90% 18/20 [00:01<00:00,  9.64it/s, loss=0.405, v_num=2jvn, val/acc=0.772, val/loss=0.632]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:02<00:00,  9.92it/s, loss=0.405, v_num=2jvn, val/acc=0.769, val/loss=0.625]\n",
            "Epoch 8:  90% 18/20 [00:01<00:00,  9.58it/s, loss=0.407, v_num=2jvn, val/acc=0.769, val/loss=0.625]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:02<00:00,  9.84it/s, loss=0.407, v_num=2jvn, val/acc=0.761, val/loss=0.629]\n",
            "Epoch 9:  90% 18/20 [00:01<00:00,  9.70it/s, loss=0.407, v_num=2jvn, val/acc=0.761, val/loss=0.629]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:02<00:00,  9.96it/s, loss=0.407, v_num=2jvn, val/acc=0.765, val/loss=0.628]\n",
            "Epoch 10:  90% 18/20 [00:01<00:00,  9.55it/s, loss=0.403, v_num=2jvn, val/acc=0.765, val/loss=0.628]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:02<00:00,  9.82it/s, loss=0.403, v_num=2jvn, val/acc=0.765, val/loss=0.628]\n",
            "Epoch 11:  90% 18/20 [00:01<00:00,  9.65it/s, loss=0.399, v_num=2jvn, val/acc=0.765, val/loss=0.628]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: 100% 20/20 [00:02<00:00,  9.95it/s, loss=0.399, v_num=2jvn, val/acc=0.765, val/loss=0.628]\n",
            "Epoch 11: 100% 20/20 [00:02<00:00,  9.94it/s, loss=0.399, v_num=2jvn, val/acc=0.765, val/loss=0.628]\n",
            "[2021-06-30 02:12:58,943][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00, 15.31it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.8388429880142212, 'test/loss': 0.4653345048427582}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2840\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_021232-jo0t2jvn/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_021232-jo0t2jvn/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.40366\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019179\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.76493\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.62783\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.83884\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.46533\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▆▆█████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▂▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mglamorous-dragon-54\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/jo0t2jvn\u001b[0m\n",
            "[2021-06-30 02:13:04,151][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:13:04,151][HYDRA] \t#4 : optimizer.lr=0.00018304414962137957 model.num_kernels=256 model.embedding_dim=128 model.dropout=0.2 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n",
            "[2021-06-30 02:13:04,391][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:13:04,397][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:13:05.557462: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msweet-valley-55\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3vao94pu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_021304-3vao94pu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | CNN              | 1.4 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.531     Total estimated model params size (MB)\n",
            "Epoch 0:  90% 18/20 [00:02<00:00,  7.05it/s, loss=1.14, v_num=94pu]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 20/20 [00:02<00:00,  7.21it/s, loss=1.14, v_num=94pu, val/acc=0.675, val/loss=0.934]\n",
            "Epoch 1:  90% 18/20 [00:02<00:00,  7.11it/s, loss=0.882, v_num=94pu, val/acc=0.675, val/loss=0.934]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:02<00:00,  7.33it/s, loss=0.882, v_num=94pu, val/acc=0.690, val/loss=0.790]\n",
            "Epoch 2:  90% 18/20 [00:02<00:00,  7.14it/s, loss=0.73, v_num=94pu, val/acc=0.690, val/loss=0.790]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:02<00:00,  7.38it/s, loss=0.73, v_num=94pu, val/acc=0.795, val/loss=0.697]\n",
            "Epoch 3:  90% 18/20 [00:02<00:00,  7.06it/s, loss=0.627, v_num=94pu, val/acc=0.795, val/loss=0.697]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:02<00:00,  7.25it/s, loss=0.627, v_num=94pu, val/acc=0.802, val/loss=0.650]\n",
            "Epoch 4:  90% 18/20 [00:02<00:00,  7.11it/s, loss=0.565, v_num=94pu, val/acc=0.802, val/loss=0.650]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:02<00:00,  7.36it/s, loss=0.565, v_num=94pu, val/acc=0.795, val/loss=0.642]\n",
            "Epoch 5:  90% 18/20 [00:02<00:00,  7.15it/s, loss=0.569, v_num=94pu, val/acc=0.795, val/loss=0.642]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:02<00:00,  7.32it/s, loss=0.569, v_num=94pu, val/acc=0.795, val/loss=0.638]\n",
            "Epoch 6:  90% 18/20 [00:02<00:00,  7.08it/s, loss=0.549, v_num=94pu, val/acc=0.795, val/loss=0.638]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:02<00:00,  7.33it/s, loss=0.549, v_num=94pu, val/acc=0.795, val/loss=0.635]\n",
            "Epoch 7:  90% 18/20 [00:02<00:00,  7.15it/s, loss=0.553, v_num=94pu, val/acc=0.795, val/loss=0.635]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:02<00:00,  7.38it/s, loss=0.553, v_num=94pu, val/acc=0.799, val/loss=0.635]\n",
            "Epoch 8:  90% 18/20 [00:02<00:00,  7.13it/s, loss=0.544, v_num=94pu, val/acc=0.799, val/loss=0.635]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:02<00:00,  7.30it/s, loss=0.544, v_num=94pu, val/acc=0.795, val/loss=0.635]\n",
            "Epoch 8: 100% 20/20 [00:02<00:00,  7.30it/s, loss=0.544, v_num=94pu, val/acc=0.795, val/loss=0.635]\n",
            "[2021-06-30 02:13:31,469][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00, 12.26it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.85537189245224, 'test/loss': 0.5481699705123901}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2885\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_021304-3vao94pu/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_021304-3vao94pu/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.54262\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 153\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019211\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.79478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.63534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.85537\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.54817\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▅▃▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▄▄▄▄▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▂███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▅▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msweet-valley-55\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3vao94pu\u001b[0m\n",
            "[2021-06-30 02:13:37,459][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:13:37,459][HYDRA] \t#5 : optimizer.lr=0.006598470712514473 model.num_kernels=256 model.embedding_dim=256 model.dropout=0.5 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n",
            "[2021-06-30 02:13:37,699][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:13:37,704][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:13:38.839937: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwobbly-moon-56\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2h7snyoc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_021337-2h7snyoc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | CNN              | 2.8 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "2.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.8 M     Total params\n",
            "11.041    Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:03<00:00,  4.78it/s, loss=14.9, v_num=nyoc]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:03<00:00,  5.18it/s, loss=14.9, v_num=nyoc]\n",
            "Epoch 0: 100% 20/20 [00:03<00:00,  5.29it/s, loss=14.9, v_num=nyoc, val/acc=0.317, val/loss=2.140]\n",
            "Epoch 1:  90% 18/20 [00:03<00:00,  5.13it/s, loss=2.11, v_num=nyoc, val/acc=0.317, val/loss=2.140]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:03<00:00,  5.36it/s, loss=2.11, v_num=nyoc, val/acc=0.612, val/loss=0.971]\n",
            "Epoch 2:  90% 18/20 [00:03<00:00,  5.14it/s, loss=0.784, v_num=nyoc, val/acc=0.612, val/loss=0.971]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.34it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:03<00:00,  5.36it/s, loss=0.784, v_num=nyoc, val/acc=0.735, val/loss=0.703]\n",
            "Epoch 3:  90% 18/20 [00:03<00:00,  5.10it/s, loss=0.626, v_num=nyoc, val/acc=0.735, val/loss=0.703]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:03<00:00,  5.29it/s, loss=0.626, v_num=nyoc, val/acc=0.757, val/loss=0.681]\n",
            "Epoch 4:  90% 18/20 [00:03<00:00,  5.09it/s, loss=0.538, v_num=nyoc, val/acc=0.757, val/loss=0.681]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.53it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:03<00:00,  5.31it/s, loss=0.538, v_num=nyoc, val/acc=0.761, val/loss=0.682]\n",
            "Epoch 5:  90% 18/20 [00:03<00:00,  5.07it/s, loss=0.458, v_num=nyoc, val/acc=0.761, val/loss=0.682]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:03<00:00,  5.32it/s, loss=0.458, v_num=nyoc, val/acc=0.761, val/loss=0.682]\n",
            "Epoch 5: 100% 20/20 [00:03<00:00,  5.29it/s, loss=0.458, v_num=nyoc, val/acc=0.776, val/loss=0.659]\n",
            "Epoch 6:  90% 18/20 [00:03<00:00,  5.03it/s, loss=0.415, v_num=nyoc, val/acc=0.776, val/loss=0.659]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:03<00:00,  5.28it/s, loss=0.415, v_num=nyoc, val/acc=0.757, val/loss=0.666]\n",
            "Epoch 7:  90% 18/20 [00:03<00:00,  5.11it/s, loss=0.397, v_num=nyoc, val/acc=0.757, val/loss=0.666]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.52it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:03<00:00,  5.34it/s, loss=0.397, v_num=nyoc, val/acc=0.769, val/loss=0.666]\n",
            "Epoch 8:  90% 18/20 [00:03<00:00,  5.10it/s, loss=0.394, v_num=nyoc, val/acc=0.769, val/loss=0.666]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:03<00:00,  5.34it/s, loss=0.394, v_num=nyoc, val/acc=0.769, val/loss=0.666]\n",
            "Epoch 9:  90% 18/20 [00:03<00:00,  5.08it/s, loss=0.403, v_num=nyoc, val/acc=0.769, val/loss=0.666]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.58it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:03<00:00,  5.31it/s, loss=0.403, v_num=nyoc, val/acc=0.769, val/loss=0.666]\n",
            "Epoch 10:  90% 18/20 [00:03<00:00,  5.12it/s, loss=0.39, v_num=nyoc, val/acc=0.769, val/loss=0.666]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:03<00:00,  5.36it/s, loss=0.39, v_num=nyoc, val/acc=0.769, val/loss=0.666]\n",
            "Epoch 10: 100% 20/20 [00:03<00:00,  5.36it/s, loss=0.39, v_num=nyoc, val/acc=0.769, val/loss=0.666]\n",
            "[2021-06-30 02:14:21,630][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00, 10.09it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.78925621509552, 'test/loss': 0.5333786606788635}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2930\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_021337-2h7snyoc/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_021337-2h7snyoc/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.389\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 187\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019261\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.76866\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.66584\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.78926\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.53338\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▅▇████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mwobbly-moon-56\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/2h7snyoc\u001b[0m\n",
            "[2021-06-30 02:14:27,530][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:14:27,530][HYDRA] \t#6 : optimizer.lr=0.009650011213982108 model.num_kernels=256 model.embedding_dim=256 model.dropout=0.4 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n",
            "[2021-06-30 02:14:27,760][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:14:27,770][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:14:28.941008: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgenial-field-57\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1vkr42xg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_021427-1vkr42xg\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | CNN              | 2.8 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "2.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.8 M     Total params\n",
            "11.041    Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:03<00:00,  4.77it/s, loss=15.6, v_num=42xg]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:03<00:00,  5.18it/s, loss=15.6, v_num=42xg]\n",
            "Epoch 0: 100% 20/20 [00:03<00:00,  5.29it/s, loss=15.6, v_num=42xg, val/acc=0.313, val/loss=3.330]\n",
            "Epoch 1:  90% 18/20 [00:03<00:00,  5.10it/s, loss=1.64, v_num=42xg, val/acc=0.313, val/loss=3.330]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:03<00:00,  5.35it/s, loss=1.64, v_num=42xg, val/acc=0.601, val/loss=0.813]\n",
            "Epoch 2:  90% 18/20 [00:03<00:00,  5.12it/s, loss=0.714, v_num=42xg, val/acc=0.601, val/loss=0.813]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:03<00:00,  5.38it/s, loss=0.714, v_num=42xg, val/acc=0.601, val/loss=0.813]\n",
            "Epoch 2: 100% 20/20 [00:03<00:00,  5.35it/s, loss=0.714, v_num=42xg, val/acc=0.724, val/loss=0.675]\n",
            "Epoch 3:  90% 18/20 [00:03<00:00,  5.06it/s, loss=0.575, v_num=42xg, val/acc=0.724, val/loss=0.675]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.25it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:03<00:00,  5.25it/s, loss=0.575, v_num=42xg, val/acc=0.772, val/loss=0.621]\n",
            "Epoch 4:  90% 18/20 [00:03<00:00,  5.11it/s, loss=0.46, v_num=42xg, val/acc=0.772, val/loss=0.621]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:03<00:00,  5.30it/s, loss=0.46, v_num=42xg, val/acc=0.784, val/loss=0.607]\n",
            "Epoch 5:  90% 18/20 [00:03<00:00,  5.08it/s, loss=0.412, v_num=42xg, val/acc=0.784, val/loss=0.607]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.02it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:03<00:00,  5.29it/s, loss=0.412, v_num=42xg, val/acc=0.787, val/loss=0.612]\n",
            "Epoch 6:  90% 18/20 [00:03<00:00,  5.10it/s, loss=0.389, v_num=42xg, val/acc=0.787, val/loss=0.612]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:03<00:00,  5.33it/s, loss=0.389, v_num=42xg, val/acc=0.780, val/loss=0.614]\n",
            "Epoch 7:  90% 18/20 [00:03<00:00,  5.09it/s, loss=0.355, v_num=42xg, val/acc=0.780, val/loss=0.614]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:03<00:00,  5.35it/s, loss=0.355, v_num=42xg, val/acc=0.780, val/loss=0.614]\n",
            "Epoch 7: 100% 20/20 [00:03<00:00,  5.33it/s, loss=0.355, v_num=42xg, val/acc=0.772, val/loss=0.614]\n",
            "Epoch 8:  90% 18/20 [00:03<00:00,  5.13it/s, loss=0.34, v_num=42xg, val/acc=0.772, val/loss=0.614]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:03<00:00,  5.36it/s, loss=0.34, v_num=42xg, val/acc=0.769, val/loss=0.613]\n",
            "Epoch 9:  90% 18/20 [00:03<00:00,  5.07it/s, loss=0.345, v_num=42xg, val/acc=0.769, val/loss=0.613]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:03<00:00,  5.33it/s, loss=0.345, v_num=42xg, val/acc=0.769, val/loss=0.613]\n",
            "Epoch 9: 100% 20/20 [00:03<00:00,  5.30it/s, loss=0.345, v_num=42xg, val/acc=0.769, val/loss=0.611]\n",
            "Epoch 10:  90% 18/20 [00:03<00:00,  5.15it/s, loss=0.33, v_num=42xg, val/acc=0.769, val/loss=0.611]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:03<00:00,  5.38it/s, loss=0.33, v_num=42xg, val/acc=0.769, val/loss=0.609]\n",
            "Epoch 10: 100% 20/20 [00:03<00:00,  5.38it/s, loss=0.33, v_num=42xg, val/acc=0.769, val/loss=0.609]\n",
            "[2021-06-30 02:15:11,728][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00, 10.21it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.8181818127632141, 'test/loss': 0.5058497786521912}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2977\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_021427-1vkr42xg/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_021427-1vkr42xg/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.32989\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 187\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019311\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.76866\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.60922\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.81818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.50585\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▅▇████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mgenial-field-57\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1vkr42xg\u001b[0m\n",
            "[2021-06-30 02:15:17,674][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:15:17,674][HYDRA] \t#7 : optimizer.lr=0.007202790846042897 model.num_kernels=256 model.embedding_dim=128 model.dropout=0.2 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n",
            "[2021-06-30 02:15:17,912][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:15:17,918][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:15:19.066435: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mserene-frog-58\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1vsz3uy9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_021517-1vsz3uy9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | CNN              | 1.4 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.531     Total estimated model params size (MB)\n",
            "Epoch 0:  90% 18/20 [00:02<00:00,  7.03it/s, loss=9.89, v_num=3uy9]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 20/20 [00:02<00:00,  7.24it/s, loss=9.89, v_num=3uy9]\n",
            "Epoch 0: 100% 20/20 [00:02<00:00,  7.20it/s, loss=9.89, v_num=3uy9, val/acc=0.321, val/loss=2.380]\n",
            "Epoch 1:  90% 18/20 [00:02<00:00,  7.13it/s, loss=1.37, v_num=3uy9, val/acc=0.321, val/loss=2.380]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:02<00:00,  7.39it/s, loss=1.37, v_num=3uy9, val/acc=0.705, val/loss=0.903]\n",
            "Epoch 2:  90% 18/20 [00:02<00:00,  7.13it/s, loss=0.634, v_num=3uy9, val/acc=0.705, val/loss=0.903]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:02<00:00,  7.38it/s, loss=0.634, v_num=3uy9, val/acc=0.754, val/loss=0.748]\n",
            "Epoch 3:  90% 18/20 [00:02<00:00,  7.07it/s, loss=0.473, v_num=3uy9, val/acc=0.754, val/loss=0.748]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:02<00:00,  7.22it/s, loss=0.473, v_num=3uy9, val/acc=0.761, val/loss=0.673]\n",
            "Epoch 4:  90% 18/20 [00:02<00:00,  7.04it/s, loss=0.355, v_num=3uy9, val/acc=0.761, val/loss=0.673]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:02<00:00,  7.28it/s, loss=0.355, v_num=3uy9, val/acc=0.791, val/loss=0.651]\n",
            "Epoch 5:  90% 18/20 [00:02<00:00,  7.02it/s, loss=0.312, v_num=3uy9, val/acc=0.791, val/loss=0.651]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.86it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:02<00:00,  7.19it/s, loss=0.312, v_num=3uy9, val/acc=0.795, val/loss=0.648]\n",
            "Epoch 6:  90% 18/20 [00:02<00:00,  7.16it/s, loss=0.291, v_num=3uy9, val/acc=0.795, val/loss=0.648]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:02<00:00,  7.41it/s, loss=0.291, v_num=3uy9, val/acc=0.776, val/loss=0.658]\n",
            "Epoch 7:  90% 18/20 [00:02<00:00,  7.09it/s, loss=0.267, v_num=3uy9, val/acc=0.776, val/loss=0.658]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:02<00:00,  7.33it/s, loss=0.267, v_num=3uy9, val/acc=0.772, val/loss=0.662]\n",
            "Epoch 8:  90% 18/20 [00:02<00:00,  7.11it/s, loss=0.263, v_num=3uy9, val/acc=0.772, val/loss=0.662]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:02<00:00,  7.32it/s, loss=0.263, v_num=3uy9, val/acc=0.772, val/loss=0.662]\n",
            "Epoch 8: 100% 20/20 [00:02<00:00,  7.28it/s, loss=0.263, v_num=3uy9, val/acc=0.784, val/loss=0.656]\n",
            "Epoch 9:  90% 18/20 [00:02<00:00,  7.15it/s, loss=0.252, v_num=3uy9, val/acc=0.784, val/loss=0.656]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:02<00:00,  7.40it/s, loss=0.252, v_num=3uy9, val/acc=0.776, val/loss=0.657]\n",
            "Epoch 10:  90% 18/20 [00:02<00:00,  7.03it/s, loss=0.258, v_num=3uy9, val/acc=0.776, val/loss=0.657]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:02<00:00,  7.22it/s, loss=0.258, v_num=3uy9, val/acc=0.776, val/loss=0.657]\n",
            "Epoch 10: 100% 20/20 [00:02<00:00,  7.21it/s, loss=0.258, v_num=3uy9, val/acc=0.776, val/loss=0.657]\n",
            "[2021-06-30 02:15:50,536][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00, 10.85it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.8099173307418823, 'test/loss': 0.5419489145278931}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3022\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_021517-1vsz3uy9/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_021517-1vsz3uy9/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.25746\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 187\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019350\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.77612\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.65684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.80992\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.54195\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▇▇████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mserene-frog-58\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1vsz3uy9\u001b[0m\n",
            "[2021-06-30 02:15:55,879][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:15:55,879][HYDRA] \t#8 : optimizer.lr=0.004452481598259614 model.num_kernels=256 model.embedding_dim=128 model.dropout=0.5 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n",
            "[2021-06-30 02:15:56,121][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:15:56,127][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:15:57.286110: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpolished-water-59\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1gysqw2d\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_021556-1gysqw2d\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | CNN              | 1.4 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.531     Total estimated model params size (MB)\n",
            "Epoch 0:  90% 18/20 [00:02<00:00,  7.05it/s, loss=5.82, v_num=qw2d]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 20/20 [00:02<00:00,  7.23it/s, loss=5.82, v_num=qw2d, val/acc=0.537, val/loss=2.460]\n",
            "Epoch 1:  90% 18/20 [00:02<00:00,  7.08it/s, loss=1.28, v_num=qw2d, val/acc=0.537, val/loss=2.460]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:02<00:00,  7.35it/s, loss=1.28, v_num=qw2d, val/acc=0.694, val/loss=0.809]\n",
            "Epoch 2:  90% 18/20 [00:02<00:00,  7.15it/s, loss=0.735, v_num=qw2d, val/acc=0.694, val/loss=0.809]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:02<00:00,  7.38it/s, loss=0.735, v_num=qw2d, val/acc=0.765, val/loss=0.611]\n",
            "Epoch 3:  90% 18/20 [00:02<00:00,  7.10it/s, loss=0.593, v_num=qw2d, val/acc=0.765, val/loss=0.611]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:02<00:00,  7.27it/s, loss=0.593, v_num=qw2d, val/acc=0.776, val/loss=0.619]\n",
            "Epoch 4:  90% 18/20 [00:02<00:00,  7.12it/s, loss=0.47, v_num=qw2d, val/acc=0.776, val/loss=0.619]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:02<00:00,  7.37it/s, loss=0.47, v_num=qw2d, val/acc=0.791, val/loss=0.569]\n",
            "Epoch 5:  90% 18/20 [00:02<00:00,  7.11it/s, loss=0.414, v_num=qw2d, val/acc=0.791, val/loss=0.569]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:02<00:00,  7.31it/s, loss=0.414, v_num=qw2d, val/acc=0.787, val/loss=0.568]\n",
            "Epoch 6:  90% 18/20 [00:02<00:00,  7.09it/s, loss=0.387, v_num=qw2d, val/acc=0.787, val/loss=0.568]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:02<00:00,  7.32it/s, loss=0.387, v_num=qw2d, val/acc=0.787, val/loss=0.596]\n",
            "Epoch 7:  90% 18/20 [00:02<00:00,  7.12it/s, loss=0.367, v_num=qw2d, val/acc=0.787, val/loss=0.596]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:02<00:00,  7.38it/s, loss=0.367, v_num=qw2d, val/acc=0.802, val/loss=0.570]\n",
            "Epoch 8:  90% 18/20 [00:02<00:00,  7.03it/s, loss=0.369, v_num=qw2d, val/acc=0.802, val/loss=0.570]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:02<00:00,  7.25it/s, loss=0.369, v_num=qw2d, val/acc=0.802, val/loss=0.570]\n",
            "Epoch 8: 100% 20/20 [00:02<00:00,  7.21it/s, loss=0.369, v_num=qw2d, val/acc=0.799, val/loss=0.564]\n",
            "Epoch 9:  90% 18/20 [00:02<00:00,  7.08it/s, loss=0.354, v_num=qw2d, val/acc=0.799, val/loss=0.564]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:02<00:00,  7.32it/s, loss=0.354, v_num=qw2d, val/acc=0.799, val/loss=0.567]\n",
            "Epoch 10:  90% 18/20 [00:02<00:00,  7.11it/s, loss=0.352, v_num=qw2d, val/acc=0.799, val/loss=0.567]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:02<00:00,  7.37it/s, loss=0.352, v_num=qw2d, val/acc=0.799, val/loss=0.566]\n",
            "Epoch 11:  90% 18/20 [00:02<00:00,  7.09it/s, loss=0.347, v_num=qw2d, val/acc=0.799, val/loss=0.566]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: 100% 20/20 [00:02<00:00,  7.28it/s, loss=0.347, v_num=qw2d, val/acc=0.799, val/loss=0.566]\n",
            "Epoch 12:  90% 18/20 [00:02<00:00,  7.07it/s, loss=0.357, v_num=qw2d, val/acc=0.799, val/loss=0.566]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12: 100% 20/20 [00:02<00:00,  7.33it/s, loss=0.357, v_num=qw2d, val/acc=0.799, val/loss=0.566]\n",
            "Epoch 12: 100% 20/20 [00:02<00:00,  7.33it/s, loss=0.357, v_num=qw2d, val/acc=0.799, val/loss=0.566]\n",
            "[2021-06-30 02:16:34,162][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00, 12.04it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.8223140239715576, 'test/loss': 0.4864349365234375}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3067\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_021556-1gysqw2d/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_021556-1gysqw2d/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.359\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 221\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 38\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019394\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.79851\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.56591\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.82231\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.48643\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▅▇▇█████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▂▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mpolished-water-59\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1gysqw2d\u001b[0m\n",
            "[2021-06-30 02:16:40,027][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:16:40,027][HYDRA] \t#9 : optimizer.lr=0.006801049746560324 model.num_kernels=128 model.embedding_dim=256 model.dropout=0.1 model=cnn logger=wandb trainer.gpus=1 hparams_search=cnn_optuna\n",
            "[2021-06-30 02:16:40,257][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:16:40,262][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:16:41.417632: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwarm-microwave-60\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1o81zyyi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_021640-1o81zyyi\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | CNN              | 1.8 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "1.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.8 M     Total params\n",
            "7.098     Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:02<00:00,  6.48it/s, loss=8.4, v_num=zyyi] \n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:02<00:00,  6.97it/s, loss=8.4, v_num=zyyi]\n",
            "Epoch 0: 100% 20/20 [00:02<00:00,  7.05it/s, loss=8.4, v_num=zyyi, val/acc=0.571, val/loss=1.500]\n",
            "Epoch 1:  90% 18/20 [00:02<00:00,  6.94it/s, loss=0.944, v_num=zyyi, val/acc=0.571, val/loss=1.500]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:02<00:00,  7.19it/s, loss=0.944, v_num=zyyi, val/acc=0.716, val/loss=0.892]\n",
            "Epoch 2:  90% 18/20 [00:02<00:00,  6.91it/s, loss=0.593, v_num=zyyi, val/acc=0.716, val/loss=0.892]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:02<00:00,  7.12it/s, loss=0.593, v_num=zyyi, val/acc=0.772, val/loss=0.676]\n",
            "Epoch 3:  90% 18/20 [00:02<00:00,  6.93it/s, loss=0.471, v_num=zyyi, val/acc=0.772, val/loss=0.676]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:02<00:00,  7.20it/s, loss=0.471, v_num=zyyi, val/acc=0.791, val/loss=0.602]\n",
            "Epoch 4:  90% 18/20 [00:02<00:00,  6.84it/s, loss=0.277, v_num=zyyi, val/acc=0.791, val/loss=0.602]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:02<00:00,  7.12it/s, loss=0.277, v_num=zyyi, val/acc=0.802, val/loss=0.617]\n",
            "Epoch 5:  90% 18/20 [00:02<00:00,  6.91it/s, loss=0.229, v_num=zyyi, val/acc=0.802, val/loss=0.617]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:02<00:00,  7.19it/s, loss=0.229, v_num=zyyi, val/acc=0.780, val/loss=0.613]\n",
            "Epoch 6:  90% 18/20 [00:02<00:00,  6.93it/s, loss=0.212, v_num=zyyi, val/acc=0.780, val/loss=0.613]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:02<00:00,  7.22it/s, loss=0.212, v_num=zyyi, val/acc=0.802, val/loss=0.618]\n",
            "Epoch 7:  90% 18/20 [00:02<00:00,  6.90it/s, loss=0.209, v_num=zyyi, val/acc=0.802, val/loss=0.618]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:02<00:00,  7.10it/s, loss=0.209, v_num=zyyi, val/acc=0.802, val/loss=0.606]\n",
            "Epoch 8:  90% 18/20 [00:02<00:00,  6.86it/s, loss=0.194, v_num=zyyi, val/acc=0.802, val/loss=0.606]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:02<00:00,  7.16it/s, loss=0.194, v_num=zyyi, val/acc=0.795, val/loss=0.605]\n",
            "Epoch 9:  90% 18/20 [00:02<00:00,  6.92it/s, loss=0.193, v_num=zyyi, val/acc=0.795, val/loss=0.605]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:02<00:00,  7.20it/s, loss=0.193, v_num=zyyi, val/acc=0.802, val/loss=0.608]\n",
            "Epoch 9: 100% 20/20 [00:02<00:00,  7.19it/s, loss=0.193, v_num=zyyi, val/acc=0.802, val/loss=0.608]\n",
            "[2021-06-30 02:17:10,702][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00, 10.32it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.8140496015548706, 'test/loss': 0.46970516443252563}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3112\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_021640-1o81zyyi/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_021640-1o81zyyi/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.18922\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 170\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019430\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.80224\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.60788\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.81405\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.46971\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▃▃▃▄▄▅▅▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▃▃▃▄▄▅▅▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▅▇██▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▃▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mwarm-microwave-60\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1o81zyyi\u001b[0m\n",
            "[2021-06-30 02:17:16,222][HYDRA] Best parameters: {'optimizer.lr': 0.0021251467576750933, 'model.num_kernels': 256, 'model.embedding_dim': 128, 'model.dropout': 0.30000000000000004}\n",
            "[2021-06-30 02:17:16,222][HYDRA] Best value: 0.8171641826629639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRHDx7xtT9Lw",
        "outputId": "37dae26a-3077-4447-f4f6-9383d2c48501"
      },
      "source": [
        "!python scripts/train.py -m model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna hydra.sweeper.n_trials=10"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-06-30 02:18:15,087]\u001b[0m A new study created in memory with name: no-name-0cbc1e15-7dca-49f7-aa3d-cfcad57f8a75\u001b[0m\n",
            "[2021-06-30 02:18:15,087][HYDRA] Study name: no-name-0cbc1e15-7dca-49f7-aa3d-cfcad57f8a75\n",
            "[2021-06-30 02:18:15,087][HYDRA] Storage: None\n",
            "[2021-06-30 02:18:15,087][HYDRA] Sampler: TPESampler\n",
            "[2021-06-30 02:18:15,087][HYDRA] Directions: ['maximize']\n",
            "[2021-06-30 02:18:15,091][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:18:15,091][HYDRA] \t#0 : optimizer.lr=0.009303199318889763 model.embedding_dim=128 model.rnn_type=LSTM model.rnn_hidden_dim=64 model.rnn_dropout=0.1 model.rnn_num_layers=4 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n",
            "[2021-06-30 02:18:15,349][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:18:15,359][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkingyiusuen\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:18:16.591280: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdark-planet-61\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/g8qgdb2t\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_021815-g8qgdb2t\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | RNN              | 792 K \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "792 K     Trainable params\n",
            "0         Non-trainable params\n",
            "792 K     Total params\n",
            "3.169     Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:09<00:01,  1.89it/s, loss=1.09, v_num=db2t]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:09<00:00,  2.08it/s, loss=1.09, v_num=db2t]\n",
            "Epoch 0: 100% 20/20 [00:09<00:00,  2.14it/s, loss=1.09, v_num=db2t, val/acc=0.463, val/loss=1.030]\n",
            "Epoch 1:  90% 18/20 [00:08<00:00,  2.00it/s, loss=1.03, v_num=db2t, val/acc=0.463, val/loss=1.030]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.59it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:09<00:00,  2.15it/s, loss=1.03, v_num=db2t, val/acc=0.474, val/loss=1.010]\n",
            "Epoch 2:  90% 18/20 [00:09<00:01,  2.00it/s, loss=0.989, v_num=db2t, val/acc=0.474, val/loss=1.010]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.29it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:09<00:00,  2.15it/s, loss=0.989, v_num=db2t, val/acc=0.541, val/loss=0.983]\n",
            "Epoch 3:  90% 18/20 [00:08<00:00,  2.00it/s, loss=0.929, v_num=db2t, val/acc=0.541, val/loss=0.983]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.56it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:09<00:00,  2.15it/s, loss=0.929, v_num=db2t, val/acc=0.586, val/loss=0.971]\n",
            "Epoch 4:  90% 18/20 [00:08<00:00,  2.01it/s, loss=0.858, v_num=db2t, val/acc=0.586, val/loss=0.971]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.63it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:09<00:00,  2.15it/s, loss=0.858, v_num=db2t, val/acc=0.571, val/loss=0.959]\n",
            "Epoch 5:  90% 18/20 [00:09<00:01,  1.99it/s, loss=0.788, v_num=db2t, val/acc=0.571, val/loss=0.959]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.52it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:09<00:00,  2.14it/s, loss=0.788, v_num=db2t, val/acc=0.597, val/loss=0.951]\n",
            "Epoch 6:  90% 18/20 [00:09<00:01,  1.99it/s, loss=0.761, v_num=db2t, val/acc=0.597, val/loss=0.951]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.53it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:09<00:00,  2.14it/s, loss=0.761, v_num=db2t, val/acc=0.593, val/loss=0.960]\n",
            "Epoch 7:  90% 18/20 [00:08<00:00,  2.00it/s, loss=0.724, v_num=db2t, val/acc=0.593, val/loss=0.960]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.67it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:09<00:00,  2.15it/s, loss=0.724, v_num=db2t, val/acc=0.593, val/loss=0.965]\n",
            "Epoch 8:  90% 18/20 [00:09<00:01,  1.99it/s, loss=0.713, v_num=db2t, val/acc=0.593, val/loss=0.965]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.02it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:09<00:00,  2.14it/s, loss=0.713, v_num=db2t, val/acc=0.593, val/loss=0.958]\n",
            "Epoch 9:  90% 18/20 [00:09<00:01,  2.00it/s, loss=0.696, v_num=db2t, val/acc=0.593, val/loss=0.958]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.32it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:09<00:00,  2.15it/s, loss=0.696, v_num=db2t, val/acc=0.597, val/loss=0.956]\n",
            "Epoch 10:  90% 18/20 [00:09<00:01,  2.00it/s, loss=0.709, v_num=db2t, val/acc=0.597, val/loss=0.956]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.53it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:09<00:00,  2.15it/s, loss=0.709, v_num=db2t, val/acc=0.597, val/loss=0.956]\n",
            "Epoch 10: 100% 20/20 [00:09<00:00,  2.15it/s, loss=0.709, v_num=db2t, val/acc=0.597, val/loss=0.956]\n",
            "[2021-06-30 02:20:02,851][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00,  8.42it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.557851254940033, 'test/loss': 0.996212899684906}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3175\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_021815-g8qgdb2t/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_021815-g8qgdb2t/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.70248\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 187\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 108\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019603\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.59701\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.9564\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.55785\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.99621\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▆▅▄▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▂▅▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▇▄▃▂▁▂▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdark-planet-61\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/g8qgdb2t\u001b[0m\n",
            "[2021-06-30 02:20:08,464][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:20:08,465][HYDRA] \t#1 : optimizer.lr=0.005720477787908698 model.embedding_dim=128 model.rnn_type=LSTM model.rnn_hidden_dim=256 model.rnn_dropout=0.30000000000000004 model.rnn_num_layers=4 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n",
            "[2021-06-30 02:20:08,711][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:20:08,716][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:20:09.873232: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfine-star-62\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/dq9wck39\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_022008-dq9wck39\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | RNN              | 5.9 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "5.9 M     Trainable params\n",
            "0         Non-trainable params\n",
            "5.9 M     Total params\n",
            "23.670    Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:19<00:03,  1.15s/it, loss=1.1, v_num=ck39]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:19<00:01,  1.04s/it, loss=1.1, v_num=ck39]\n",
            "Validating:  67% 2/3 [00:00<00:00,  3.24it/s]\u001b[A\n",
            "Epoch 0: 100% 20/20 [00:20<00:00,  1.01s/it, loss=1.1, v_num=ck39, val/acc=0.425, val/loss=1.050]\n",
            "Epoch 1:  90% 18/20 [00:19<00:02,  1.09s/it, loss=1.04, v_num=ck39, val/acc=0.425, val/loss=1.050]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.31it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:20<00:00,  1.01s/it, loss=1.04, v_num=ck39, val/acc=0.425, val/loss=1.050]\n",
            "Epoch 1: 100% 20/20 [00:20<00:00,  1.01s/it, loss=1.04, v_num=ck39, val/acc=0.459, val/loss=1.030]\n",
            "Epoch 2:  90% 18/20 [00:19<00:02,  1.09s/it, loss=1.02, v_num=ck39, val/acc=0.459, val/loss=1.030]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.30it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:20<00:00,  1.01s/it, loss=1.02, v_num=ck39, val/acc=0.459, val/loss=1.030]\n",
            "Epoch 2: 100% 20/20 [00:20<00:00,  1.01s/it, loss=1.02, v_num=ck39, val/acc=0.507, val/loss=1.010]\n",
            "Epoch 3:  90% 18/20 [00:19<00:02,  1.08s/it, loss=0.985, v_num=ck39, val/acc=0.507, val/loss=1.010]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.33it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.985, v_num=ck39, val/acc=0.507, val/loss=1.010]\n",
            "Epoch 3: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.985, v_num=ck39, val/acc=0.526, val/loss=0.994]\n",
            "Epoch 4:  90% 18/20 [00:19<00:02,  1.09s/it, loss=0.927, v_num=ck39, val/acc=0.526, val/loss=0.994]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.24it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.927, v_num=ck39, val/acc=0.526, val/loss=0.994]\n",
            "Epoch 4: 100% 20/20 [00:20<00:00,  1.02s/it, loss=0.927, v_num=ck39, val/acc=0.575, val/loss=0.917]\n",
            "Epoch 5:  90% 18/20 [00:19<00:02,  1.09s/it, loss=0.834, v_num=ck39, val/acc=0.575, val/loss=0.917]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.35it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.834, v_num=ck39, val/acc=0.575, val/loss=0.917]\n",
            "Epoch 5: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.834, v_num=ck39, val/acc=0.642, val/loss=0.872]\n",
            "Epoch 6:  90% 18/20 [00:19<00:02,  1.09s/it, loss=0.754, v_num=ck39, val/acc=0.642, val/loss=0.872]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.36it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.754, v_num=ck39, val/acc=0.642, val/loss=0.872]\n",
            "Epoch 6: 100% 20/20 [00:20<00:00,  1.02s/it, loss=0.754, v_num=ck39, val/acc=0.642, val/loss=0.894]\n",
            "Epoch 7:  90% 18/20 [00:19<00:02,  1.09s/it, loss=0.696, v_num=ck39, val/acc=0.642, val/loss=0.894]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.32it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.696, v_num=ck39, val/acc=0.642, val/loss=0.894]\n",
            "Epoch 7: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.696, v_num=ck39, val/acc=0.657, val/loss=0.816]\n",
            "Epoch 8:  90% 18/20 [00:19<00:02,  1.09s/it, loss=0.681, v_num=ck39, val/acc=0.657, val/loss=0.816]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.27it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.681, v_num=ck39, val/acc=0.657, val/loss=0.816]\n",
            "Epoch 8: 100% 20/20 [00:20<00:00,  1.02s/it, loss=0.681, v_num=ck39, val/acc=0.657, val/loss=0.817]\n",
            "Epoch 9:  90% 18/20 [00:19<00:02,  1.09s/it, loss=0.664, v_num=ck39, val/acc=0.657, val/loss=0.817]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.24it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.664, v_num=ck39, val/acc=0.657, val/loss=0.817]\n",
            "Epoch 9: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.664, v_num=ck39, val/acc=0.642, val/loss=0.832]\n",
            "Epoch 10:  90% 18/20 [00:19<00:02,  1.09s/it, loss=0.653, v_num=ck39, val/acc=0.642, val/loss=0.832]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.30it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.653, v_num=ck39, val/acc=0.642, val/loss=0.832]\n",
            "Epoch 10: 100% 20/20 [00:20<00:00,  1.02s/it, loss=0.653, v_num=ck39, val/acc=0.642, val/loss=0.833]\n",
            "Epoch 11:  90% 18/20 [00:19<00:02,  1.09s/it, loss=0.647, v_num=ck39, val/acc=0.642, val/loss=0.833]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.29it/s]\u001b[A\n",
            "Epoch 11: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.647, v_num=ck39, val/acc=0.642, val/loss=0.833]\n",
            "Epoch 11: 100% 20/20 [00:20<00:00,  1.02s/it, loss=0.647, v_num=ck39, val/acc=0.642, val/loss=0.832]\n",
            "Epoch 12:  90% 18/20 [00:19<00:02,  1.08s/it, loss=0.649, v_num=ck39, val/acc=0.642, val/loss=0.832]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.27it/s]\u001b[A\n",
            "Epoch 12: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.649, v_num=ck39, val/acc=0.642, val/loss=0.832]\n",
            "Epoch 12: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.649, v_num=ck39, val/acc=0.638, val/loss=0.831]\n",
            "Epoch 12: 100% 20/20 [00:20<00:00,  1.01s/it, loss=0.649, v_num=ck39, val/acc=0.638, val/loss=0.831]\n",
            "[2021-06-30 02:24:35,451][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00,  3.26it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.6487603187561035, 'test/loss': 0.8203667402267456}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3224\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_022008-dq9wck39/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_022008-dq9wck39/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.65034\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 221\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 268\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019876\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.63806\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.83107\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.64876\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.82037\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▇▆▅▄▃▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▂▃▄▆███████▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▇▇▆▄▃▃▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfine-star-62\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/dq9wck39\u001b[0m\n",
            "[2021-06-30 02:24:41,786][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:24:41,787][HYDRA] \t#2 : optimizer.lr=0.007514175711585728 model.embedding_dim=256 model.rnn_type=GRU model.rnn_hidden_dim=256 model.rnn_dropout=0.5 model.rnn_num_layers=3 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n",
            "[2021-06-30 02:24:42,030][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:24:42,038][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:24:43.199066: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mapricot-pond-63\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/37ax6rxi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_022442-37ax6rxi\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | RNN              | 3.9 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "3.9 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.9 M     Total params\n",
            "15.782    Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:13<00:02,  1.31it/s, loss=1.62, v_num=6rxi]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:13<00:00,  1.44it/s, loss=1.62, v_num=6rxi]\n",
            "Epoch 0: 100% 20/20 [00:13<00:00,  1.48it/s, loss=1.62, v_num=6rxi, val/acc=0.377, val/loss=1.100]\n",
            "Epoch 1:  90% 18/20 [00:13<00:01,  1.38it/s, loss=1.12, v_num=6rxi, val/acc=0.377, val/loss=1.100]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.55it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:13<00:00,  1.48it/s, loss=1.12, v_num=6rxi, val/acc=0.392, val/loss=1.090]\n",
            "Epoch 2:  90% 18/20 [00:13<00:01,  1.38it/s, loss=nan, v_num=6rxi, val/acc=0.392, val/loss=1.090]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.67it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:13<00:00,  1.47it/s, loss=nan, v_num=6rxi, val/acc=0.317, val/loss=nan.0]\n",
            "Epoch 3:  90% 18/20 [00:12<00:01,  1.39it/s, loss=nan, v_num=6rxi, val/acc=0.317, val/loss=nan.0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.62it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:13<00:00,  1.48it/s, loss=nan, v_num=6rxi, val/acc=0.317, val/loss=nan.0]\n",
            "Epoch 4:  90% 18/20 [00:12<00:01,  1.39it/s, loss=nan, v_num=6rxi, val/acc=0.317, val/loss=nan.0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.60it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:13<00:00,  1.48it/s, loss=nan, v_num=6rxi, val/acc=0.317, val/loss=nan.0]\n",
            "Epoch 5:  90% 18/20 [00:12<00:01,  1.39it/s, loss=nan, v_num=6rxi, val/acc=0.317, val/loss=nan.0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.56it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:13<00:00,  1.48it/s, loss=nan, v_num=6rxi, val/acc=0.317, val/loss=nan.0]\n",
            "Epoch 6:  90% 18/20 [00:13<00:01,  1.38it/s, loss=nan, v_num=6rxi, val/acc=0.317, val/loss=nan.0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.58it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:13<00:00,  1.48it/s, loss=nan, v_num=6rxi, val/acc=0.317, val/loss=nan.0]\n",
            "Epoch 6: 100% 20/20 [00:13<00:00,  1.48it/s, loss=nan, v_num=6rxi, val/acc=0.317, val/loss=nan.0]\n",
            "[2021-06-30 02:26:19,136][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00,  4.64it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.3140496015548706, 'test/loss': nan}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3283\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_022442-37ax6rxi/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_022442-37ax6rxi/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss nan\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 119\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 97\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625019979\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.31716\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss nan\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.31405\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss nan\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▁     \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▄▄▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▇█▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▁     \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mapricot-pond-63\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/37ax6rxi\u001b[0m\n",
            "[2021-06-30 02:26:24,959][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:26:24,959][HYDRA] \t#3 : optimizer.lr=0.002985873632533843 model.embedding_dim=256 model.rnn_type=GRU model.rnn_hidden_dim=128 model.rnn_dropout=0.5 model.rnn_num_layers=5 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n",
            "[2021-06-30 02:26:25,200][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:26:25,208][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:26:26.341518: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdrawn-sun-64\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/hpcwuvjz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_022625-hpcwuvjz\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | RNN              | 2.3 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "2.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.3 M     Total params\n",
            "9.088     Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:14<00:02,  1.16it/s, loss=1.1, v_num=uvjz] \n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:14<00:00,  1.27it/s, loss=1.1, v_num=uvjz]\n",
            "Validating:  67% 2/3 [00:00<00:00,  3.94it/s]\u001b[A\n",
            "Epoch 0: 100% 20/20 [00:15<00:00,  1.31it/s, loss=1.1, v_num=uvjz, val/acc=0.418, val/loss=1.080]\n",
            "Epoch 1:  90% 18/20 [00:14<00:01,  1.23it/s, loss=1.03, v_num=uvjz, val/acc=0.418, val/loss=1.080]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.01it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:15<00:00,  1.31it/s, loss=1.03, v_num=uvjz, val/acc=0.493, val/loss=1.090]\n",
            "Epoch 2:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.99, v_num=uvjz, val/acc=0.493, val/loss=1.090]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.00it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.99, v_num=uvjz, val/acc=0.485, val/loss=1.060]\n",
            "Epoch 3:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.924, v_num=uvjz, val/acc=0.485, val/loss=1.060]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.97it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.924, v_num=uvjz, val/acc=0.507, val/loss=1.090]\n",
            "Epoch 4:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.895, v_num=uvjz, val/acc=0.507, val/loss=1.090]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.04it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.895, v_num=uvjz, val/acc=0.507, val/loss=1.030]\n",
            "Epoch 5:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.836, v_num=uvjz, val/acc=0.507, val/loss=1.030]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.95it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.836, v_num=uvjz, val/acc=0.549, val/loss=1.050]\n",
            "Epoch 6:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.741, v_num=uvjz, val/acc=0.549, val/loss=1.050]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.98it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.741, v_num=uvjz, val/acc=0.608, val/loss=0.920]\n",
            "Epoch 7:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.634, v_num=uvjz, val/acc=0.608, val/loss=0.920]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.04it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.634, v_num=uvjz, val/acc=0.604, val/loss=0.963]\n",
            "Epoch 8:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.537, v_num=uvjz, val/acc=0.604, val/loss=0.963]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.96it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.537, v_num=uvjz, val/acc=0.627, val/loss=0.919]\n",
            "Epoch 9:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.495, v_num=uvjz, val/acc=0.627, val/loss=0.919]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.96it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.495, v_num=uvjz, val/acc=0.619, val/loss=0.929]\n",
            "Epoch 10:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.46, v_num=uvjz, val/acc=0.619, val/loss=0.929]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.00it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.46, v_num=uvjz, val/acc=0.631, val/loss=0.932]\n",
            "Epoch 11:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.46, v_num=uvjz, val/acc=0.631, val/loss=0.932]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.94it/s]\u001b[A\n",
            "Epoch 11: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.46, v_num=uvjz, val/acc=0.623, val/loss=0.932]\n",
            "Epoch 12:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.446, v_num=uvjz, val/acc=0.623, val/loss=0.932]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.96it/s]\u001b[A\n",
            "Epoch 12: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.446, v_num=uvjz, val/acc=0.627, val/loss=0.932]\n",
            "Epoch 13:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.447, v_num=uvjz, val/acc=0.627, val/loss=0.932]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.89it/s]\u001b[A\n",
            "Epoch 13: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.447, v_num=uvjz, val/acc=0.627, val/loss=0.932]\n",
            "Epoch 14:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.461, v_num=uvjz, val/acc=0.627, val/loss=0.932]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.92it/s]\u001b[A\n",
            "Epoch 14: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.461, v_num=uvjz, val/acc=0.627, val/loss=0.932]\n",
            "Epoch 15:  90% 18/20 [00:14<00:01,  1.23it/s, loss=0.455, v_num=uvjz, val/acc=0.627, val/loss=0.932]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.96it/s]\u001b[A\n",
            "Epoch 15: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.455, v_num=uvjz, val/acc=0.627, val/loss=0.932]\n",
            "Epoch 15: 100% 20/20 [00:15<00:00,  1.31it/s, loss=0.455, v_num=uvjz, val/acc=0.627, val/loss=0.932]\n",
            "[2021-06-30 02:30:31,888][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00,  3.96it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.6198347210884094, 'test/loss': 0.9449632167816162}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3330\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_022625-hpcwuvjz/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_022625-hpcwuvjz/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.45202\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 247\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625020232\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.62687\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.93246\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.61983\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.94496\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▇▆▆▅▄▃▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▃▃▄▄▅▇▇████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss ▇█▇█▆▆▁▃▁▁▂▂▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdrawn-sun-64\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/hpcwuvjz\u001b[0m\n",
            "[2021-06-30 02:30:38,151][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:30:38,151][HYDRA] \t#4 : optimizer.lr=0.00841520059603814 model.embedding_dim=64 model.rnn_type=GRU model.rnn_hidden_dim=64 model.rnn_dropout=0.2 model.rnn_num_layers=5 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n",
            "[2021-06-30 02:30:38,392][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:30:38,398][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:30:39.538650: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mradiant-disco-65\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1yqtsucd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_023038-1yqtsucd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | RNN              | 545 K \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "545 K     Trainable params\n",
            "0         Non-trainable params\n",
            "545 K     Total params\n",
            "2.182     Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:10<00:01,  1.56it/s, loss=1.1, v_num=sucd]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:11<00:00,  1.72it/s, loss=1.1, v_num=sucd]\n",
            "Epoch 0: 100% 20/20 [00:11<00:00,  1.77it/s, loss=1.1, v_num=sucd, val/acc=0.381, val/loss=1.110]\n",
            "Epoch 1:  90% 18/20 [00:10<00:01,  1.65it/s, loss=1.06, v_num=sucd, val/acc=0.381, val/loss=1.110]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  6.29it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:11<00:00,  1.77it/s, loss=1.06, v_num=sucd, val/acc=0.455, val/loss=1.060]\n",
            "Epoch 2:  90% 18/20 [00:10<00:01,  1.65it/s, loss=1, v_num=sucd, val/acc=0.455, val/loss=1.060]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  7.13it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:11<00:00,  1.78it/s, loss=1, v_num=sucd, val/acc=0.549, val/loss=0.954]\n",
            "Epoch 3:  90% 18/20 [00:10<00:01,  1.66it/s, loss=0.888, v_num=sucd, val/acc=0.549, val/loss=0.954]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  7.35it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:11<00:00,  1.78it/s, loss=0.888, v_num=sucd, val/acc=0.649, val/loss=0.885]\n",
            "Epoch 4:  90% 18/20 [00:10<00:01,  1.65it/s, loss=0.761, v_num=sucd, val/acc=0.649, val/loss=0.885]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  6.23it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:11<00:00,  1.77it/s, loss=0.761, v_num=sucd, val/acc=0.649, val/loss=0.844]\n",
            "Epoch 5:  90% 18/20 [00:10<00:01,  1.66it/s, loss=0.669, v_num=sucd, val/acc=0.649, val/loss=0.844]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  7.26it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:11<00:00,  1.78it/s, loss=0.669, v_num=sucd, val/acc=0.657, val/loss=0.794]\n",
            "Epoch 6:  90% 18/20 [00:10<00:01,  1.65it/s, loss=0.621, v_num=sucd, val/acc=0.657, val/loss=0.794]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  7.19it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:11<00:00,  1.77it/s, loss=0.621, v_num=sucd, val/acc=0.664, val/loss=0.793]\n",
            "Epoch 7:  90% 18/20 [00:10<00:01,  1.66it/s, loss=0.58, v_num=sucd, val/acc=0.664, val/loss=0.793]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  7.24it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:11<00:00,  1.78it/s, loss=0.58, v_num=sucd, val/acc=0.679, val/loss=0.788]\n",
            "Epoch 8:  90% 18/20 [00:10<00:01,  1.66it/s, loss=0.559, v_num=sucd, val/acc=0.679, val/loss=0.788]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  7.28it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:11<00:00,  1.78it/s, loss=0.559, v_num=sucd, val/acc=0.679, val/loss=0.786]\n",
            "Epoch 9:  90% 18/20 [00:10<00:01,  1.65it/s, loss=0.573, v_num=sucd, val/acc=0.679, val/loss=0.786]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  7.30it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:11<00:00,  1.78it/s, loss=0.573, v_num=sucd, val/acc=0.675, val/loss=0.787]\n",
            "Epoch 10:  90% 18/20 [00:10<00:01,  1.65it/s, loss=0.562, v_num=sucd, val/acc=0.675, val/loss=0.787]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  7.43it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:11<00:00,  1.78it/s, loss=0.562, v_num=sucd, val/acc=0.675, val/loss=0.787]\n",
            "Epoch 11:  90% 18/20 [00:10<00:01,  1.66it/s, loss=0.558, v_num=sucd, val/acc=0.675, val/loss=0.787]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  7.31it/s]\u001b[A\n",
            "Epoch 11: 100% 20/20 [00:11<00:00,  1.78it/s, loss=0.558, v_num=sucd, val/acc=0.675, val/loss=0.787]\n",
            "Epoch 12:  90% 18/20 [00:10<00:01,  1.66it/s, loss=0.561, v_num=sucd, val/acc=0.675, val/loss=0.787]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  7.40it/s]\u001b[A\n",
            "Epoch 12: 100% 20/20 [00:11<00:00,  1.77it/s, loss=0.561, v_num=sucd, val/acc=0.672, val/loss=0.787]\n",
            "Epoch 12: 100% 20/20 [00:11<00:00,  1.77it/s, loss=0.561, v_num=sucd, val/acc=0.672, val/loss=0.787]\n",
            "[2021-06-30 02:33:07,171][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00,  7.22it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.6900826692581177, 'test/loss': 0.7094351053237915}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3377\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_023038-1yqtsucd/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_023038-1yqtsucd/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.56565\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 221\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 149\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625020387\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.67164\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.7866\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.69008\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.70944\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▇▅▃▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▃▅▇▇▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▇▅▃▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mradiant-disco-65\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/1yqtsucd\u001b[0m\n",
            "[2021-06-30 02:33:12,317][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:33:12,317][HYDRA] \t#5 : optimizer.lr=0.0013802142390900991 model.embedding_dim=256 model.rnn_type=LSTM model.rnn_hidden_dim=128 model.rnn_dropout=0.4 model.rnn_num_layers=4 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n",
            "[2021-06-30 02:33:12,555][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:33:12,560][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:33:13.704569: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdainty-water-66\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3t7p2n8z\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_023312-3t7p2n8z\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | RNN              | 2.4 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "2.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.4 M     Total params\n",
            "9.483     Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:12<00:02,  1.36it/s, loss=1.07, v_num=2n8z]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:12<00:00,  1.49it/s, loss=1.07, v_num=2n8z]\n",
            "Epoch 0: 100% 20/20 [00:13<00:00,  1.54it/s, loss=1.07, v_num=2n8z, val/acc=0.459, val/loss=1.040]\n",
            "Epoch 1:  90% 18/20 [00:12<00:01,  1.45it/s, loss=1.02, v_num=2n8z, val/acc=0.459, val/loss=1.040]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.50it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:12<00:00,  1.55it/s, loss=1.02, v_num=2n8z, val/acc=0.485, val/loss=1.000]\n",
            "Epoch 2:  90% 18/20 [00:12<00:01,  1.45it/s, loss=0.959, v_num=2n8z, val/acc=0.485, val/loss=1.000]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.44it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:12<00:00,  1.54it/s, loss=0.959, v_num=2n8z, val/acc=0.556, val/loss=0.976]\n",
            "Epoch 3:  90% 18/20 [00:12<00:01,  1.45it/s, loss=1.02, v_num=2n8z, val/acc=0.556, val/loss=0.976]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.45it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:12<00:00,  1.55it/s, loss=1.02, v_num=2n8z, val/acc=0.466, val/loss=1.020]\n",
            "Epoch 4:  90% 18/20 [00:12<00:01,  1.45it/s, loss=0.971, v_num=2n8z, val/acc=0.466, val/loss=1.020]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.41it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:12<00:00,  1.54it/s, loss=0.971, v_num=2n8z, val/acc=0.526, val/loss=1.000]\n",
            "Epoch 5:  90% 18/20 [00:12<00:01,  1.45it/s, loss=0.939, v_num=2n8z, val/acc=0.526, val/loss=1.000]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.37it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:12<00:00,  1.54it/s, loss=0.939, v_num=2n8z, val/acc=0.522, val/loss=0.982]\n",
            "Epoch 6:  90% 18/20 [00:12<00:01,  1.45it/s, loss=0.917, v_num=2n8z, val/acc=0.522, val/loss=0.982]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.32it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:13<00:00,  1.54it/s, loss=0.917, v_num=2n8z, val/acc=0.549, val/loss=0.960]\n",
            "Epoch 7:  90% 18/20 [00:12<00:01,  1.45it/s, loss=0.892, v_num=2n8z, val/acc=0.549, val/loss=0.960]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  4.36it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:12<00:00,  1.55it/s, loss=0.892, v_num=2n8z, val/acc=0.549, val/loss=0.956]\n",
            "Epoch 7: 100% 20/20 [00:12<00:00,  1.55it/s, loss=0.892, v_num=2n8z, val/acc=0.549, val/loss=0.956]\n",
            "[2021-06-30 02:34:58,659][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00,  4.38it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.4834710657596588, 'test/loss': 1.004919409751892}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3422\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_023312-3t7p2n8z/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_023312-3t7p2n8z/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.89775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 136\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 107\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625020499\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.54851\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.95576\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.48347\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 1.00492\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▆▃▇▄▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▄▄▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▄▄▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▄▄▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▃█▂▆▆▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▅▃▇▅▃▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdainty-water-66\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3t7p2n8z\u001b[0m\n",
            "[2021-06-30 02:35:04,437][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:35:04,437][HYDRA] \t#6 : optimizer.lr=0.0060080194323662755 model.embedding_dim=64 model.rnn_type=LSTM model.rnn_hidden_dim=256 model.rnn_dropout=0.5 model.rnn_num_layers=5 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n",
            "[2021-06-30 02:35:04,719][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:35:04,726][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:35:05.927863: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhopeful-plasma-67\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3oslbwp0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_023504-3oslbwp0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | RNN              | 7.2 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "7.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "7.2 M     Total params\n",
            "28.664    Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:25<00:04,  1.49s/it, loss=1.11, v_num=bwp0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:25<00:01,  1.35s/it, loss=1.11, v_num=bwp0]\n",
            "Validating:  67% 2/3 [00:00<00:00,  2.73it/s]\u001b[A\n",
            "Epoch 0: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.11, v_num=bwp0, val/acc=0.362, val/loss=1.090]\n",
            "Epoch 1:  90% 18/20 [00:25<00:02,  1.40s/it, loss=1.09, v_num=bwp0, val/acc=0.362, val/loss=1.090]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  2.76it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:25<00:00,  1.30s/it, loss=1.09, v_num=bwp0, val/acc=0.362, val/loss=1.090]\n",
            "Epoch 1: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.09, v_num=bwp0, val/acc=0.381, val/loss=1.060]\n",
            "Epoch 2:  90% 18/20 [00:25<00:02,  1.41s/it, loss=1.06, v_num=bwp0, val/acc=0.381, val/loss=1.060]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  2.73it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:26<00:00,  1.30s/it, loss=1.06, v_num=bwp0, val/acc=0.381, val/loss=1.060]\n",
            "Epoch 2: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.06, v_num=bwp0, val/acc=0.489, val/loss=1.030]\n",
            "Epoch 3:  90% 18/20 [00:25<00:02,  1.40s/it, loss=1.05, v_num=bwp0, val/acc=0.489, val/loss=1.030]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  2.75it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:26<00:00,  1.30s/it, loss=1.05, v_num=bwp0, val/acc=0.489, val/loss=1.030]\n",
            "Epoch 3: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.05, v_num=bwp0, val/acc=0.474, val/loss=1.050]\n",
            "Epoch 4:  90% 18/20 [00:25<00:02,  1.40s/it, loss=1.04, v_num=bwp0, val/acc=0.474, val/loss=1.050]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  2.73it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:25<00:00,  1.30s/it, loss=1.04, v_num=bwp0, val/acc=0.474, val/loss=1.050]\n",
            "Epoch 4: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1.04, v_num=bwp0, val/acc=0.474, val/loss=1.020]\n",
            "Epoch 5:  90% 18/20 [00:25<00:02,  1.40s/it, loss=1, v_num=bwp0, val/acc=0.474, val/loss=1.020]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  2.67it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:26<00:00,  1.30s/it, loss=1, v_num=bwp0, val/acc=0.474, val/loss=1.020]\n",
            "Epoch 5: 100% 20/20 [00:26<00:00,  1.31s/it, loss=1, v_num=bwp0, val/acc=0.500, val/loss=0.997]\n",
            "Epoch 6:  90% 18/20 [00:25<00:02,  1.41s/it, loss=0.981, v_num=bwp0, val/acc=0.500, val/loss=0.997]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  2.67it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.981, v_num=bwp0, val/acc=0.500, val/loss=0.997]\n",
            "Epoch 6: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.981, v_num=bwp0, val/acc=0.500, val/loss=1.010]\n",
            "Epoch 7:  90% 18/20 [00:25<00:02,  1.41s/it, loss=0.96, v_num=bwp0, val/acc=0.500, val/loss=1.010]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  2.75it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:26<00:00,  1.30s/it, loss=0.96, v_num=bwp0, val/acc=0.500, val/loss=1.010]\n",
            "Epoch 7: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.96, v_num=bwp0, val/acc=0.489, val/loss=0.994]\n",
            "Epoch 8:  90% 18/20 [00:25<00:02,  1.41s/it, loss=0.95, v_num=bwp0, val/acc=0.489, val/loss=0.994]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  2.73it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:26<00:00,  1.30s/it, loss=0.95, v_num=bwp0, val/acc=0.489, val/loss=0.994]\n",
            "Epoch 8: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.95, v_num=bwp0, val/acc=0.496, val/loss=0.986]\n",
            "Epoch 9:  90% 18/20 [00:25<00:02,  1.41s/it, loss=0.946, v_num=bwp0, val/acc=0.496, val/loss=0.986]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  2.69it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:26<00:00,  1.30s/it, loss=0.946, v_num=bwp0, val/acc=0.496, val/loss=0.986]\n",
            "Epoch 9: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.946, v_num=bwp0, val/acc=0.500, val/loss=0.981]\n",
            "Epoch 10:  90% 18/20 [00:25<00:02,  1.40s/it, loss=0.941, v_num=bwp0, val/acc=0.500, val/loss=0.981]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  2.72it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:25<00:00,  1.30s/it, loss=0.941, v_num=bwp0, val/acc=0.500, val/loss=0.981]\n",
            "Epoch 10: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.941, v_num=bwp0, val/acc=0.496, val/loss=0.980]\n",
            "Epoch 10: 100% 20/20 [00:26<00:00,  1.31s/it, loss=0.941, v_num=bwp0, val/acc=0.496, val/loss=0.980]\n",
            "[2021-06-30 02:39:55,622][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00,  2.74it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.3801652789115906, 'test/loss': 1.1709805727005005}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3469\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_023504-3oslbwp0/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_023504-3oslbwp0/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.94031\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 187\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 292\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625020796\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.49627\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.98028\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.38017\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 1.17098\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▆▅▅▃▂▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▂▇▇▇██▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▆▄▅▃▂▃▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mhopeful-plasma-67\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3oslbwp0\u001b[0m\n",
            "[2021-06-30 02:40:01,863][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:40:01,863][HYDRA] \t#7 : optimizer.lr=0.008236601358982134 model.embedding_dim=256 model.rnn_type=LSTM model.rnn_hidden_dim=64 model.rnn_dropout=0.30000000000000004 model.rnn_num_layers=2 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n",
            "[2021-06-30 02:40:02,130][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:40:02,135][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:40:03.282528: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfearless-bush-68\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3qelccnf\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_024002-3qelccnf\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | RNN              | 1.1 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "1.1 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.1 M     Total params\n",
            "4.214     Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:04<00:00,  3.67it/s, loss=1.07, v_num=ccnf]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:04<00:00,  3.99it/s, loss=1.07, v_num=ccnf]\n",
            "Epoch 0: 100% 20/20 [00:04<00:00,  4.06it/s, loss=1.07, v_num=ccnf, val/acc=0.448, val/loss=1.020]\n",
            "Epoch 1:  90% 18/20 [00:04<00:00,  3.92it/s, loss=1, v_num=ccnf, val/acc=0.448, val/loss=1.020]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.44it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:04<00:00,  4.10it/s, loss=1, v_num=ccnf, val/acc=0.549, val/loss=0.975]\n",
            "Epoch 2:  90% 18/20 [00:04<00:00,  3.89it/s, loss=0.984, v_num=ccnf, val/acc=0.549, val/loss=0.975]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.13it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:04<00:00,  4.07it/s, loss=0.984, v_num=ccnf, val/acc=0.515, val/loss=1.000]\n",
            "Epoch 3:  90% 18/20 [00:04<00:00,  3.92it/s, loss=0.925, v_num=ccnf, val/acc=0.515, val/loss=1.000]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  9.05it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:04<00:00,  4.11it/s, loss=0.925, v_num=ccnf, val/acc=0.519, val/loss=0.993]\n",
            "Epoch 4:  90% 18/20 [00:04<00:00,  3.91it/s, loss=0.856, v_num=ccnf, val/acc=0.519, val/loss=0.993]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.71it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:04<00:00,  4.10it/s, loss=0.856, v_num=ccnf, val/acc=0.549, val/loss=0.992]\n",
            "Epoch 5:  90% 18/20 [00:04<00:00,  3.88it/s, loss=0.808, v_num=ccnf, val/acc=0.549, val/loss=0.992]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  7.64it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:04<00:00,  4.06it/s, loss=0.808, v_num=ccnf, val/acc=0.541, val/loss=0.980]\n",
            "Epoch 6:  90% 18/20 [00:04<00:00,  3.89it/s, loss=0.785, v_num=ccnf, val/acc=0.541, val/loss=0.980]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  8.16it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:04<00:00,  4.07it/s, loss=0.785, v_num=ccnf, val/acc=0.541, val/loss=0.974]\n",
            "Epoch 6: 100% 20/20 [00:04<00:00,  4.07it/s, loss=0.785, v_num=ccnf, val/acc=0.541, val/loss=0.974]\n",
            "[2021-06-30 02:40:38,799][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00,  9.00it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.5, 'test/loss': 1.1016334295272827}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3516\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_024002-3qelccnf/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_024002-3qelccnf/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.77859\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 119\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 37\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625020839\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.54104\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.97407\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 1.10163\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▆▆▄▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▃▃▅▅▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▃▃▄▄▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▄▅▅▅▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▄▅▅▅▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁█▆▆█▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▁▅▄▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfearless-bush-68\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3qelccnf\u001b[0m\n",
            "[2021-06-30 02:40:43,969][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:40:43,969][HYDRA] \t#8 : optimizer.lr=0.009751644541430321 model.embedding_dim=128 model.rnn_type=GRU model.rnn_hidden_dim=64 model.rnn_dropout=0.4 model.rnn_num_layers=2 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n",
            "[2021-06-30 02:40:44,224][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:40:44,231][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:40:45.389287: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrim-universe-69\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/37fsuw0r\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_024044-37fsuw0r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | RNN              | 543 K \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "543 K     Trainable params\n",
            "0         Non-trainable params\n",
            "543 K     Total params\n",
            "2.175     Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:03<00:00,  4.28it/s, loss=1.08, v_num=uw0r]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:04<00:00,  4.65it/s, loss=1.08, v_num=uw0r]\n",
            "Epoch 0: 100% 20/20 [00:04<00:00,  4.72it/s, loss=1.08, v_num=uw0r, val/acc=0.493, val/loss=1.030]\n",
            "Epoch 1:  90% 18/20 [00:03<00:00,  4.54it/s, loss=1.02, v_num=uw0r, val/acc=0.493, val/loss=1.030]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:04<00:00,  4.80it/s, loss=1.02, v_num=uw0r, val/acc=0.493, val/loss=1.030]\n",
            "Epoch 1: 100% 20/20 [00:04<00:00,  4.76it/s, loss=1.02, v_num=uw0r, val/acc=0.496, val/loss=1.020]\n",
            "Epoch 2:  90% 18/20 [00:03<00:00,  4.54it/s, loss=0.973, v_num=uw0r, val/acc=0.496, val/loss=1.020]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:04<00:00,  4.80it/s, loss=0.973, v_num=uw0r, val/acc=0.496, val/loss=1.020]\n",
            "Epoch 2: 100% 20/20 [00:04<00:00,  4.76it/s, loss=0.973, v_num=uw0r, val/acc=0.541, val/loss=0.992]\n",
            "Epoch 3:  90% 18/20 [00:03<00:00,  4.57it/s, loss=0.888, v_num=uw0r, val/acc=0.541, val/loss=0.992]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:04<00:00,  4.81it/s, loss=0.888, v_num=uw0r, val/acc=0.560, val/loss=0.964]\n",
            "Epoch 4:  90% 18/20 [00:03<00:00,  4.60it/s, loss=0.751, v_num=uw0r, val/acc=0.560, val/loss=0.964]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:04<00:00,  4.85it/s, loss=0.751, v_num=uw0r, val/acc=0.593, val/loss=0.923]\n",
            "Epoch 5:  90% 18/20 [00:03<00:00,  4.54it/s, loss=0.673, v_num=uw0r, val/acc=0.593, val/loss=0.923]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:04<00:00,  4.74it/s, loss=0.673, v_num=uw0r, val/acc=0.578, val/loss=0.914]\n",
            "Epoch 6:  90% 18/20 [00:03<00:00,  4.56it/s, loss=0.631, v_num=uw0r, val/acc=0.578, val/loss=0.914]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:04<00:00,  4.81it/s, loss=0.631, v_num=uw0r, val/acc=0.612, val/loss=0.912]\n",
            "Epoch 7:  90% 18/20 [00:03<00:00,  4.56it/s, loss=0.596, v_num=uw0r, val/acc=0.612, val/loss=0.912]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:04<00:00,  4.81it/s, loss=0.596, v_num=uw0r, val/acc=0.623, val/loss=0.890]\n",
            "Epoch 8:  90% 18/20 [00:03<00:00,  4.60it/s, loss=0.579, v_num=uw0r, val/acc=0.623, val/loss=0.890]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:04<00:00,  4.82it/s, loss=0.579, v_num=uw0r, val/acc=0.619, val/loss=0.891]\n",
            "Epoch 9:  90% 18/20 [00:03<00:00,  4.59it/s, loss=0.58, v_num=uw0r, val/acc=0.619, val/loss=0.891]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:04<00:00,  4.82it/s, loss=0.58, v_num=uw0r, val/acc=0.623, val/loss=0.886]\n",
            "Epoch 10:  90% 18/20 [00:03<00:00,  4.58it/s, loss=0.568, v_num=uw0r, val/acc=0.623, val/loss=0.886]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:04<00:00,  4.83it/s, loss=0.568, v_num=uw0r, val/acc=0.619, val/loss=0.886]\n",
            "Epoch 11:  90% 18/20 [00:03<00:00,  4.56it/s, loss=0.566, v_num=uw0r, val/acc=0.619, val/loss=0.886]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: 100% 20/20 [00:04<00:00,  4.79it/s, loss=0.566, v_num=uw0r, val/acc=0.623, val/loss=0.886]\n",
            "Epoch 12:  90% 18/20 [00:03<00:00,  4.55it/s, loss=0.575, v_num=uw0r, val/acc=0.623, val/loss=0.886]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12: 100% 20/20 [00:04<00:00,  4.78it/s, loss=0.575, v_num=uw0r, val/acc=0.623, val/loss=0.885]\n",
            "Epoch 12: 100% 20/20 [00:04<00:00,  4.77it/s, loss=0.575, v_num=uw0r, val/acc=0.623, val/loss=0.885]\n",
            "[2021-06-30 02:41:40,887][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00, 10.43it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.5495867729187012, 'test/loss': 0.9460524320602417}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3561\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_024044-37fsuw0r/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_024044-37fsuw0r/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.56661\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 221\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 57\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625020901\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.62313\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.88542\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.54959\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.94605\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▆▅▃▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▆▆▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▁▄▅▆▆▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss ██▆▅▃▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrim-universe-69\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/37fsuw0r\u001b[0m\n",
            "[2021-06-30 02:41:46,089][HYDRA] Launching 1 jobs locally\n",
            "[2021-06-30 02:41:46,089][HYDRA] \t#9 : optimizer.lr=0.002451531699688771 model.embedding_dim=256 model.rnn_type=GRU model.rnn_hidden_dim=256 model.rnn_dropout=0.1 model.rnn_num_layers=4 model=rnn logger=wandb trainer.gpus=1 hparams_search=rnn_optuna\n",
            "[2021-06-30 02:41:46,335][reddit_post_classification.utils][INFO] - Disabling python warnings! <cfg.ignore_warnings=True>\n",
            "[2021-06-30 02:41:46,340][reddit_post_classification.data][INFO] - Loading train and val data...\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2021-06-30 02:41:47.547241: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlyric-terrain-70\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3swcy2yo\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in ./wandb/run-20210630_024146-3swcy2yo\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name    | Type             | Params\n",
            "---------------------------------------------\n",
            "0 | model   | RNN              | 5.1 M \n",
            "1 | loss_fn | CrossEntropyLoss | 0     \n",
            "2 | metrics | ModuleDict       | 0     \n",
            "---------------------------------------------\n",
            "5.1 M     Trainable params\n",
            "0         Non-trainable params\n",
            "5.1 M     Total params\n",
            "20.513    Total estimated model params size (MB)\n",
            "Epoch 0:  85% 17/20 [00:18<00:03,  1.07s/it, loss=1.16, v_num=y2yo]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  95% 19/20 [00:18<00:00,  1.03it/s, loss=1.16, v_num=y2yo]\n",
            "Validating:  67% 2/3 [00:00<00:00,  3.57it/s]\u001b[A\n",
            "Epoch 0: 100% 20/20 [00:18<00:00,  1.06it/s, loss=1.16, v_num=y2yo, val/acc=0.481, val/loss=1.060]\n",
            "Epoch 1:  90% 18/20 [00:18<00:02,  1.01s/it, loss=1.04, v_num=y2yo, val/acc=0.481, val/loss=1.060]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.55it/s]\u001b[A\n",
            "Epoch 1: 100% 20/20 [00:18<00:00,  1.07it/s, loss=1.04, v_num=y2yo, val/acc=0.481, val/loss=1.060]\n",
            "Epoch 1: 100% 20/20 [00:18<00:00,  1.06it/s, loss=1.04, v_num=y2yo, val/acc=0.507, val/loss=1.010]\n",
            "Epoch 2:  90% 18/20 [00:18<00:02,  1.01s/it, loss=0.962, v_num=y2yo, val/acc=0.507, val/loss=1.010]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.47it/s]\u001b[A\n",
            "Epoch 2: 100% 20/20 [00:18<00:00,  1.07it/s, loss=0.962, v_num=y2yo, val/acc=0.507, val/loss=1.010]\n",
            "Epoch 2: 100% 20/20 [00:18<00:00,  1.06it/s, loss=0.962, v_num=y2yo, val/acc=0.515, val/loss=0.994]\n",
            "Epoch 3:  90% 18/20 [00:18<00:02,  1.01s/it, loss=0.858, v_num=y2yo, val/acc=0.515, val/loss=0.994]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.59it/s]\u001b[A\n",
            "Epoch 3: 100% 20/20 [00:18<00:00,  1.07it/s, loss=0.858, v_num=y2yo, val/acc=0.515, val/loss=0.994]\n",
            "Epoch 3: 100% 20/20 [00:18<00:00,  1.06it/s, loss=0.858, v_num=y2yo, val/acc=0.586, val/loss=0.916]\n",
            "Epoch 4:  90% 18/20 [00:18<00:02,  1.01s/it, loss=0.657, v_num=y2yo, val/acc=0.586, val/loss=0.916]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.61it/s]\u001b[A\n",
            "Epoch 4: 100% 20/20 [00:18<00:00,  1.07it/s, loss=0.657, v_num=y2yo, val/acc=0.586, val/loss=0.916]\n",
            "Epoch 4: 100% 20/20 [00:18<00:00,  1.06it/s, loss=0.657, v_num=y2yo, val/acc=0.649, val/loss=0.846]\n",
            "Epoch 5:  90% 18/20 [00:18<00:02,  1.01s/it, loss=0.529, v_num=y2yo, val/acc=0.649, val/loss=0.846]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.51it/s]\u001b[A\n",
            "Epoch 5: 100% 20/20 [00:18<00:00,  1.07it/s, loss=0.529, v_num=y2yo, val/acc=0.649, val/loss=0.846]\n",
            "Epoch 5: 100% 20/20 [00:18<00:00,  1.06it/s, loss=0.529, v_num=y2yo, val/acc=0.675, val/loss=0.840]\n",
            "Epoch 6:  90% 18/20 [00:18<00:02,  1.01s/it, loss=0.459, v_num=y2yo, val/acc=0.675, val/loss=0.840]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.63it/s]\u001b[A\n",
            "Epoch 6: 100% 20/20 [00:18<00:00,  1.06it/s, loss=0.459, v_num=y2yo, val/acc=0.687, val/loss=0.821]\n",
            "Epoch 7:  90% 18/20 [00:18<00:02,  1.01s/it, loss=0.4, v_num=y2yo, val/acc=0.687, val/loss=0.821]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.53it/s]\u001b[A\n",
            "Epoch 7: 100% 20/20 [00:18<00:00,  1.07it/s, loss=0.4, v_num=y2yo, val/acc=0.687, val/loss=0.821]\n",
            "Epoch 7: 100% 20/20 [00:18<00:00,  1.06it/s, loss=0.4, v_num=y2yo, val/acc=0.668, val/loss=0.805]\n",
            "Epoch 8:  90% 18/20 [00:18<00:02,  1.01s/it, loss=0.372, v_num=y2yo, val/acc=0.668, val/loss=0.805]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.56it/s]\u001b[A\n",
            "Epoch 8: 100% 20/20 [00:18<00:00,  1.07it/s, loss=0.372, v_num=y2yo, val/acc=0.668, val/loss=0.805]\n",
            "Epoch 8: 100% 20/20 [00:18<00:00,  1.06it/s, loss=0.372, v_num=y2yo, val/acc=0.675, val/loss=0.802]\n",
            "Epoch 9:  90% 18/20 [00:18<00:02,  1.01s/it, loss=0.359, v_num=y2yo, val/acc=0.675, val/loss=0.802]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.51it/s]\u001b[A\n",
            "Epoch 9: 100% 20/20 [00:18<00:00,  1.07it/s, loss=0.359, v_num=y2yo, val/acc=0.675, val/loss=0.802]\n",
            "Epoch 9: 100% 20/20 [00:18<00:00,  1.06it/s, loss=0.359, v_num=y2yo, val/acc=0.679, val/loss=0.804]\n",
            "Epoch 10:  90% 18/20 [00:18<00:02,  1.01s/it, loss=0.361, v_num=y2yo, val/acc=0.679, val/loss=0.804]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.39it/s]\u001b[A\n",
            "Epoch 10: 100% 20/20 [00:18<00:00,  1.07it/s, loss=0.361, v_num=y2yo, val/acc=0.679, val/loss=0.804]\n",
            "Epoch 10: 100% 20/20 [00:18<00:00,  1.06it/s, loss=0.361, v_num=y2yo, val/acc=0.679, val/loss=0.804]\n",
            "Epoch 11:  90% 18/20 [00:18<00:02,  1.01s/it, loss=0.361, v_num=y2yo, val/acc=0.679, val/loss=0.804]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating:  33% 1/3 [00:00<00:00,  3.41it/s]\u001b[A\n",
            "Epoch 11: 100% 20/20 [00:18<00:00,  1.07it/s, loss=0.361, v_num=y2yo, val/acc=0.679, val/loss=0.804]\n",
            "Epoch 11: 100% 20/20 [00:18<00:00,  1.06it/s, loss=0.361, v_num=y2yo, val/acc=0.683, val/loss=0.804]\n",
            "Epoch 11: 100% 20/20 [00:18<00:00,  1.06it/s, loss=0.361, v_num=y2yo, val/acc=0.683, val/loss=0.804]\n",
            "[2021-06-30 02:45:35,493][reddit_post_classification.data][INFO] - Loading test data...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 2/2 [00:00<00:00,  3.49it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test/acc': 0.7272727489471436, 'test/loss': 0.7291138768196106}\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3606\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: ./wandb/run-20210630_024146-3swcy2yo/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: ./wandb/run-20210630_024146-3swcy2yo/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.35714\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime 230\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp 1625021136\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc 0.68284\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss 0.80434\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc 0.72727\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss 0.72911\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▆▅▃▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val/acc ▁▂▂▅▇██▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/loss █▇▆▄▂▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test/acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mlyric-terrain-70\u001b[0m: \u001b[34mhttps://wandb.ai/kingyiusuen/reddit-post-classification/runs/3swcy2yo\u001b[0m\n",
            "[2021-06-30 02:45:41,368][HYDRA] Best parameters: {'optimizer.lr': 0.002451531699688771, 'model.embedding_dim': 256, 'model.rnn_type': 'GRU', 'model.rnn_hidden_dim': 256, 'model.rnn_dropout': 0.1, 'model.rnn_num_layers': 4}\n",
            "[2021-06-30 02:45:41,368][HYDRA] Best value: 0.6828358173370361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6QQ5aLTZVeo"
      },
      "source": [
        "The performance of RNN seems to depend a lot on the hyperparameters, but is consistently worse than that of CNN."
      ]
    }
  ]
}