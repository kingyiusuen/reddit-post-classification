label,text
0,what github template do you guys follow hi all i have to set up a github repo for an upcoming project and was researching some data science templates to follow i came across cookie cutter and this template by drivendata x200b it looks pretty comprehensive but i feel i might not need a lot of it like my data would be pulled straight from a db and not from a dump and neither it would be stored somewhere so the need for data folder is not there i would be developing a modelling pipeline and would not be saving the serialised model files so no need of model folder as well i think you guys know where i am going here so i just felt like i will get to know what the community is following thanks
2,unsupervised progressive learning and the stam architecture ijcai 2021 check out our recent work which was accepted by the 2021 international joint conference on artificial intelligence ijcai 2021 we pose the unsupervised progressive learning upl problem an online representation learning problem in which the learner observes a non stationary and unlabeled data stream learning a growing number of features that persist over time even though the data is not stored or replayed to solve the upl problem we propose the self taught associative memory stam architecture which leverages online clustering novelty detection outlier forgetting and prototypical feature storage rather than a replay buffer of raw examples
2,has anyone worked with physics informed neural networks pinns i m trying to decide on my final year project and one of them is supposed to be on geometric deep learning utilizing pinns i ve skimmed through the paper introducing this and it all seems very theoretical the project aims to improve upon an existing biomedical classification task using pinns and i m not sure about how practical that will be could someone please enlighten me on this it d be much appreciated thanks edit the biomedical classification task happens to be trying to predict heat transfer fluid flow and patient specific tissue properties tumor sizes and location by feeding in thermograms 3d scans as well as bioheat transfer navier stokes equations i m not very good with phrasing my sentences so apologies for any confusion in advance
2,temporal network graphs has anyone ever worked with temporal network graphs before basically these seem to be graphs in which a new nodes can appear and old nodes can disappear as time progresses b edges between nodes can appear disappear as time progresses these seem to be newer methodologies has anyone tried to perform algorithms like community detection node classification or edge prediction on temporal graphs i was trying to find more examples of this kind of stuff and all i was able to find was the twitter github page and this one general blog on temporal graphs with r and can anyone please recommend anything else thanks
0,when is it imposter syndrome i ve started working as a data analyst at a company where i used to work in a non techical role they didn t have an established data team instead myself and my manager we both graduated last year great opportunity for me but i m being asked to write a pretty big app in python and i m really struggling with it from a technical know how standpoint i feel like maybe i m not cut out for data work but also understand that imposter syndrome is a thing just looking for advice from people who have spent a longer time in the field are there moments where you re completely over your head do you just try and work through it thanks team
1,re running a b test i m a data scientist and we ran a few a b test and 3 of the 17 features tested came out stat sig we really want to be right that these features are good for the product so there was discussion of running a b tests again but only for the 3 stat sig features on an academic level is this a good idea i m having trouble thinking about what happens to type 1 and type 2 error in this case can anyone help clarify this for me
1,anyone who is attending has attended colorado state’s master’s in applied statistics do you recommend it what are some good and bad things about their program they’re one of the schools i’m strongly considering but i want to make sure people have enjoyed the program i’m told the first two courses in fall are some of the toughest and most theoretical courses in the program do you agree with that do the 8 week courses feel rushed or are they designed well enough to not feel overwhelming
1,has anyone here used the kruskal wallis test when comparing data distributions i’ve been working with 4 categories of data and trying to compute the “h” statistic to get an idea for how truly different each of the distributions categories are i have n 4 categories so 3 degrees of freedom there are several hundred data points values within each category according to what i’ve read online when there are more than 5 samples in each category you can treat h as chi squared now when i compute h i end up with a value of 68 9 according to the look up table of p values for 3 degrees of freedom this is significantly greater than the 99 9th p 0 001 value of 16 27 is my h value reasonable if so what is this telling me that there’s a much greater than 99 9 chance that the 4 categories distributions i’m looking at have significant differences between them is there any way i can take this further
1,the state of the job market in bay area i saw the post today where a lot of people with ms in stats share their stories but i can t help but notice that all of those stories are from the people with 2 years of experience what is the situation on the job market rn i have a feeling that ds positions are flooded by from zero to hero and cs l know a bit of stats people biostats jobs are reserved for biostats people or am i in a bubble and that s not a case at all i have an ms in statistics from san jose state university and a ds internship in bay area recruiters automatically assume you are proficient in sql and python i am and that s all they are interested in no one asked me a single stats question so far is someone interviewing right now what are your experiences
2,one liners in ml saw this post on r math and was wondering if y all have some interesting one line results facts to share about anything machine learning ¯ ツ ¯
1,question why the stats language is so messy i mean i feel like you have a lot of words for the same concepts but i m not sure i might be mistaken for instance recently i read the word heterogeneity and i thought wow what could this mean then it turned out it means diversity or i just came across interdependence which i think means correlation and a lot more similar examples multi level modeling hierarchical modeling mixed effects modeling apparently all have the same meaning what should i do to be less confused i m a just beginner but stats is very important to me and i want to excel at it
1,spss split file i want to compare learning approach have 2 scale 4 sub scale between 2 university and for each university i want to find the different between their demographic such as gender academic lvl etc i wonder if i could use split file function in spss analyses them as different set of data or do you guys have any other way
2,pytorch 1 9 released with new mobile interpreter inference mode and new packaging format a ton of new updates i m mostly excited about the updates to the mobile stuff
2,trash garbage dataset for waste detection hi guys we ve published trash garbage dataset which can be used for waste detection and sustainibility based projects check then now if you like it you can give upvotes into our kaggle platform we re trying to make custom dataset and open sourcing on kaggle to make ai models more robust thanks
1,education youtube statsexamples playlist i ve made a youtube channel and website with statistics videos that i m hoping will be useful for people check it out if you re interested and feel free to post some constructive advice or suggestions my target audience is ap stats and college students and the intention is to include lots of step by step examples most recent video
2,most effective algorithms for multi output classification tasks discussion based on benchmark datasets what algorithms perform the best in an array of multi output classification problems i have found limited research papers exploring a diverse spread of algorithm families on multioutput problems i would like to guess the the best in the industry is a convolutional neural network or simply a deep neural network all the same i would like some input thank you
1,what test should i use for normally distributed in regression equation what should i use to identify of the outliers of the individual regression equations are normally distributed what are the common reasons for outliers in regression analysis
2,behavioral trees on ai agents for soccer other sports hey there i have been looking into some ai algorithms and i ve come across behavioral trees bt and i was captivated i wanted to see some examples of bt being used in ai in games and such but all i found was ai for an enemy player or some sort of pve game i want to see how would a bt look like for a simple ai agent in a soccer game or any others sports game in general would each agent have their own bt or would there be a central control system with a bt that tells each ai agent to what to do like form formations etc in my search i unfortunately have not seen or came across any good examples and i wonder if there is any i d be more than happy if you could provide an example or tell me where to look at or at least provide some sort of insight to what may it look like if there were a bt driven ai sports game i am curious yet it sucks that all i got from my research are dead ends and prediction algorithms for betting
2,a review of neural anisotropy directions 2020 by flying scholars paper neural anisotropy directions 2020 by guillermo ortiz jimenez apostolos modas seyed mohsen moosavi dezfooli pascal frossard tl dr it is often said that deep networks prefer simple solutions to complex ones this study elegantly demonstrates that sometimes deep nets prefer complex non linear solutions to simple linear ones depending on the inductive biases in their architecture link to full review in the comments
2,disrupting model training with adversarial shortcuts it’s not always great that people can train machine learning models on your data in this new work we create adversarial shortcuts to prevent neural network training adversarial shortcuts are hand crafted modifications to images in the training set that exploit simplicity biases in models to prevent them from capturing the semantics of the dataset adversarial shortcuts are also easily ignored by human perception x200b x200b x200b while this idea is more broadly applicable this work begins its study in the context of a well known machine learning problem supervised classification adversarial shortcuts all share a common idea fixing a pattern for each image in a particular class encourages models to fit that pattern over anything else it turns out that even fixing a few pixels prevents the model from fitting the semantics here is an example of an imagenet sized image with a pixel based adversarial shortcut x200b this is neatly illustrated by these plots of imagenet validation and training accuracy progression notice how with the adversarial shortcuts applied the training acc 1 reaches close to 100 while the validation is stuck close to 0 x200b x200b x200b x200b x200b of course the pixel based pattern may be easily disrupted so we also explore more complicated patterns watermarks with the class index made up of mnist digits and brightness modulations x200b x200b x200b x200b x200b read more including ablation studies and comparisons to related work in the new arxiv preprint by ivan evtimov ian covert aditya kusupati and tadayoshi kohno x200b i m posting this for my friend u ivanevti who is the first author of the paper whose posts keep getting mysteriously removed he ll be reading the comments here and will be able to answer any questions you might have
1,question can someone explain the finer details of central limit theorem to me so according to the central limit theorem taking n number of random samples from a population means for many iterations and then plotting the mean of those n samples in another distribution we have a distribution where the mean of the sample mean s is roughly equal to the population mean but the standard deviation has reduced by a factor of 1 root n where n is the number of samples so my question is that the standard deviation of the mean of samples is representing the variability of the mean of the population rather than the whole process as in the population distribution as in getting a 90 percent confidence range in the sample mean distribution is the measure of the mean estimation being 90 percent accurate compared to the population mean which is the absolute mean that we are trying to estimate using the sample hence the population standard deviation is a measure of the variability of the process across thepopulation distribution whereas the standard deviation for the sample mean distribution is toshow the variability of the estimated mean this is my understanding of it and need confirmationon it extending upon the above consider that i want to build a confidence range of involving say 95 of all processes roughly 2σ will i will have to do it using the population standard deviation instead of the sample standard deviation since only the population standard deviation is indicative of all the process outcomes and the correct range
0,cluster analysis for customer segmentation what do you think about a cluster analysis to segment customers i feel like a manual segmentation is often times better especially when it comes to more personalized marketing i think that clustering makes sense when there are distinct groups in the population however i think that these groups can easily be identified by eda finding thresholds of certain variables in most cases there is a possibility for identifying relevant groups only through cluster analysis but i think that a those cases are rare and b the identified clusters are more complex and not suitable for a segmentation with the objective of a more personalized communication does anybody have a success story where unsupervised clustering led to a customer segmentation that offered a business value e g because of more personalized communication i am struggling to imagine a scenario where unsupervised clustering comes up with better clusters for personalized communications compared to manually building clusters by thresholds criteria for clusters
0,any successful 1 man end to end stories hello all there is certainly a lot of hype still about data science and machine learning as somebody who tried sour side as well few failed projects because quality of data not really significant results i was wondering if somebody had any success stories with quick ml projects that had real business impact and were set into process to bring value also if it was worth to go down python way i have experience doing my master thesis where i used ml or much quicker way with pre defined tools like power bi or those online engines now i got promoted to business intelligence manager position only successful part of ml i did so far was with engine inside of power bi down the line there are certainly some ideas i have with speech recognition better assigning employees to customers finding right time to contact right customer but uncertainity of ml and its timing to get it done scares me for some thing i know i can bring 50 60 of value by doing visual analysis solving the big chunks of decisions in 10 of the time so generally i am advocate of ml is hype that is not worth it and it will stay like this until we have done majority of analysis visually first i can see how they are failing in other business unit where they hired niche data analysts and data scientist before even having data in db so it only enforces my thinking also if somebody has in mind employee churn project thats a no go with quality of data in hr not updating positions wrong assigned managers not standardized position names tl dr is e2e ml deliverable by single person in company as side project with bringing real value relatively quickly
0,where are my bayesians at my current job i ve been using bayesian structural time series models to hopefully deal with covid related outliers more gracefully in forecasting curious if anyone here has found a way to incorporate bayesian approaches into their work if so what applications are you using them for
2,efficient net as a feature extractor in computer vision most of the feature extractor use resnet as backbone for feature extraction is there any implementation available with the efficient net as a feature extractor any reason it s not popular considering its give better or similar accuracy with less computation power
0,is there any correlation between supply chain management and data science edit thanks a lot for the people who have replied to this post i now got a brief idea
0,statistician vs data scientist what are the differences is one just in academia and one in industry or is it like a rectangles and squares kinda deal
0,scraping twitter with twint questions from a total newbie hey guys if anyone of you has experience with twint i would greatly appreciate a few tipps here i tried for like 2 days now to get this working trial and error youtube vids and medium articles mostly i knew nothing about coding before and i might add that i still have no clue i have this written down it describes my main problems command ran import twint import pandas import nest asyncio nest asyncio apply c twint config c search euro2020 c since 2021 06 19 c until 2021 06 20 c custom time username tweet likes link this is the problem line c store csv true c output test em2020 twint run search c description of issue i am very new to all of this lets just say that as a disclaimer i want to scrape all tweets that happen during specific euro 2020 football games i tried my best to get everything working but i have no idea what i am doing to be honest the pandas and asyncio line i got form a youtube channel it doesn t seem to be working if i leave them out so i kept them the general scraping seems to work fine but there are several things that i want to do that give me error messages x200b 1 when i try to slim the csv down by only scraping for the stuff i need not much see my c custom i get the message critical root twint output output csv error list indices must be integers or slices not str list indices must be integers or slices not str i have no idea what that means but i am confused because i am not asking anything new by my wanted parameters they are in the csv when i do not filter so it doesn t make sense to me why i get an error message when i look for less attributes 2 i really want to keep only englisch and german messages but the c lang de line doesn t seem to be doing anything same as the englisch one i also do not know how i can combine the two so looking only for en and de 3 right now what i still need to do is go into excel and re format the first column with the seperators that does work but is an extra step that i am shure can be automated somehow i just don t know how any help on this would be greatly appreciated i don t know if this is the right place to ask but i found no discussion forum or something of the sort thanks for reading anyways environment details windows 10 anaconda and jupyter notebook
1,question want help understanding the model in a paper on climate change and lyme disease x200b trying to understand the model behind this paper it s a paper examining the link between climate change and the incidence of lyme disease can anyone help me understand the model used and give me a basic overview of how it works
1,weighted least squares parameter estimation how to define weighting matrix hi i m a noob at this so i ll try my best to explain my question i m looking to use the weighted least squares formula for a parameter estimation problem basically the the formula consists of sum of the residuals squared multiplied by a diagonal weighting matrix consisting of the inverse of the variance from measurement data this is to weight certain values of measurement data and their associated residuals that are more accurate than others lower variance more accurate more weight please correct me if i m wrong my question is why do you have to use the inverse of the variance does the formula only work for variance or can the weighting matrix be defined in a different way for example why not the inverse of the standard deviation or the inverse of the range because basically we are saying that the greater the variance of a probability distribution the more inaccurate it is so really any term that is a measure of the spread should suffice to define the weighting matrix any thoughts would be much appreciated
1,pursuing a degree in stats even though i haven’t always had the best relationship with calculus i have a lot of questions and i would appreciate your help 1 i took a look at the courses i have to take and there are only two calculus courses how important is calculus in order to understand statistics anyways 2 how much studying did you do when you were getting your degree 3 would really appreciate if you answer this what would you do differently if you had the chance to go back in time and do your degree all over again any subjects you would focus more less on any courses you wish you took hadn’t 4 have you ever met someone who didn’t have the strongest background in mathematics yet still did very well in this field thanks to anyone who takes the time to read or answer appreciate it
2,gpu render farm machine learning hi i m building my own gpu render farm on nvidia rtx 3080 and 3090 i rent out my systems to 3d designers to render their projects it s interesting to me if i can provide ai developers with powerful systems to do ai tests and so on do they need such rigs for calculation what do you think
1,how to build conversion tables from event logs would love feedback on this piece especially the term conversion analysis as a generalization of survival analysis
1,spearman s rank correlation for qualitative data 1 how can we use the spearman s rank correlation in case of qualitative data are there any advantages to this 2 also what other correlation techniques could be used for qualitative data
1,how to derive newton s forward interpolation formula from lagrange s formula how to derive newton s forward interpolation formula from lagrange s formula
0,what do you do when you are stuck on a problem i m up against a deadline and freezing up i have a massive dataset and feel like i can t organize it properly to get any insight much less answer the questions posed by the stakeholders any advice out there
1,any summer online multivariate statistics courses that give academic credit hello i’m a graduating senior and unfortunately my school doesn’t provide the multivariate data analysis course during the summer which is when i have to graduate however my advisor says that i am eligible to do a “course equivalency” meaning i can take this course at another private institution school etc as long as the program gives academic credit does anyone know any programs this summer that provide this thank you
2,news research high resolution photorealistic image translation in real time apply any style to your 4k image in real time using this new machine learning based approach the paper is high resolution photorealistic image translation in real time a laplacian pyramid translation network by liang jie and zeng hui and zhang lei 2021 they use high and low frequency versions of the image to optimize the computation time and resources needed it can run a 4k image in less than a tenth of a second with a single regular gpu code publicly available short read video demo
1,chi squared test with limited variation i am doing a pearson’s chi squared test to test to see whether or not the upload speeds of the new router compared to my old router however the issue is that there is usually limited variation in cases like this because data speeds are also determinant on things like your internet package and etc i am getting an average of 21 5 mbps on my old router but my observed values are hovering around 25 8 mbps but the probability is like 20 is there any way to augment the test to account for situations that may have less variation than others thank you
1,how do i calculate p significance of a simple regression plot i need to figure out if my regression plots are significant or not the issue is that since i am plotting regressions over such a large amount of plots i plotted the regression manually in python to iterate over each plot i successfully plotted a regression for each set of data correctly however i am not generating a stats summary or returning a p value is there a way to manually calculate the significance of a regression line any help is greatly appreciated i used polyfit from numpy to calculate and plot my simple linear regressions
0,can dashboard software tableau powerbi help with this business case a client produces 6 12 spreadsheets each quarter they have an excel guru put together a big document that visualizes the data the document contains things like bar charts and descriptive statistics i would like to prepare a dashboard application that is hosted online or could be shared as a standalone application the application should be able to accept the data sets as input ideally with a drag and drop graphical interface combine the data sets behind the scenes and produce the necessary data viz goal is to automate the data viz process and the mechanics should be straightforward since the data sets have the same structure from year to year challenges • the data sets are company sensitive i cannot host a web application on any old web server i need some contractual guarantee the data isn’t being spied on my understanding is that most companies have freedom to intercept online info maybe then i don’t host the application online at all an html file could work • client has restrictions on what kind of software they can install i may be able to install powerbi or tableau on my system but client may not be able i am aware that tableau can easily visualize data from multiple sources i e it would be simple for me to organize the spreadsheets visualize key metrics and host it online what i would like is a freestanding application where the client drags and drops the spreadsheets and an application spits out some visuals thoughts is this too advanced for powerbi tableau do i need shiny dash
1,rules of thumb for effect size of interaction coefficients in mixed effects logistic regression lets assume all continuous predictors are standardized and all categorical predictors are binary are there rules of thumb for what interaction coefficients would be small medium and large in mixed effects logistic regression i been shown this resource when has a formula for converting cohen s d to log odds ratio to answer my question could i simply convert a small effect by cohen s d standards to log odds ratio and use that as a small interaction coefficient in mixed effects logistic regression this doesn t have to be very precise it is just for the purpose of ball parking number of participants using a simulated power analysis
0,north american biotechnology conference hi everyone i wanted to introduce the first north american students biotechnology conference nasbc the conference will be held virtually in july and will be free to attend and present in nasbc aims to provide a forum for undergraduate and graduate students across north america to share biotechnology research it is a great opportunity to expand your network and showcase your research project to others in your field if you re not in the field and would just like to learn about it feel free to rsvp as well while the presentations will be limited to those in north america attendance is open to all research will be divided into 8 categories found on the website one thing i d like to mention is that there is a category namely innovation in biotech that is specifically for those projects you feel don t really fit into the other categories but combine technology with the health field i m pointing this out for those computer science or software engineering students who have worked on a cool app that would support the health sector the abstract submission deadline is coming up soon to remember to send in your abstract asap additional information is found on the website nasbcportal com the related instagram is nasbioc i hope to see you there
0,ml visualization software what software did the author use to create the wonderful visualizations in this transformer writeup i would love to recreate his work
2,research vision transformers are robust learners for some time now transformers have taken the vision world by storm in this work we question the robustness aspects of vision transformers specifically we investigate the question with the virtue of self attention can vision transformers provide improved robustness to common corruptions perturbations etc if so why we build on top of existing works investigate the robustness aspects of vit through a series of six systematically designed experiments we present analyses that provide both quantitative qualitative indications to explain why vits are indeed more robust learners paper code
2,collusion rings in cs publications this issue is huge what do you think about it and how to cope with it
0,whats the difference between marketing data analytics and martech checked google but its only showing adtech vs martech would be great to know what the difference is between marketing data analytics and martech
0,is learning web development worth it i have seen that sometimes the results obtained from the data science process have to be displayed to the end user in some sort of analytics web tool should i add web development to my data science toolbox
0,anyone here use confluence or something like it to document their projects how is your space setup configured
2,why intel s dl based photorealistic enhancement tech is not ready for prime time gaming last week intel unveiled a dl system that enhances gta v s graphics to photorealistic level several media outlets have suggested that the technology has turned gta v to a photorealisitc game while impressive intel s image enhancement system is not ready for prime time gaming here are three key reasons memory the model requires more than a gigabyte of vram for inference while this amount of memory is available on most gaming computers we must also consider that games gobble up most of the resources of the gpu at runtime basically to free up memory for photorealistic render the users will have to make some kind of sacrifice such as playing at a lower resolution sequential processing neural nets rely on non linear computations so while you can use a gpu to process several inputs in parallel each input must go through the entire sequence of layers intel s model contains at least 100 layers of computation which is nearly impossible to run at playable framerates with current graphics cards the researchers tried it on rtx 3090 which has no shortage of vram and they still got 2 frames per second they have suggested some optimizations to integrate the model into the game engine this could give it a speed boost but not enough to get anywhere near playable framerates development and training costs the researchers needed thousands of well annotated images of urban settings to train the model lucky for them the cityscapes team had already done this for them but if a game takes place in a setting that doesn t have an open source dataset then it will be up to the game devs to curate or generate and annotate their own dataset of images which can come with huge costs also most gaming companies don t have ml talent to develop tune and train models read the full analysis here happy to hear thoughts on corrections to the take
1,life expectancy and mortality statistics has anyone ever seen a graph that shows the hazard of dying for the average adult throughout their life on this graph the horizontal axis would be years and the vertical axis would be hazard of dying and this graph would show that when a baby is born they face a higher hazard of dying then the hazard decreases but slowly increase as the adult enters old age i tried looking on google images for a study graph in which this trend is shown but i was not able to come across this kind of graph also has anyone ever studied hazard functions and cumulative hazard functions in the domain of survival analysis before is the following statement correct a hazard function is free to increase and decrease but the cumulative hazard function as the name suggests can only increase thanks
0,how much of a role should assumptions for statistical methods of analysis play in data science tldr as a data scientist how important are the assumptions for the statistical methods of analysis currently i am working on my ms in data science and i ve been thinking a lot about how my data science cs courses practically ignore basic statistical practices that i was taught in undergrad as a stats major for example the last two semesters i have taken two courses offered through the college of computer science taught by the same professor one data preparation and analysis and two data mining having taken a data analysis class in undergad a different university and offered under the college of mathematics as a statistics course i was very well equipped for this course but i noticed that the professor overlooked a lot of things that i was told is critically important to statistics and statistical analysis specifically assumptions and tests to see if those assumptions were met i didn t think much of it this course was designed for grad students and there are prerequisites that were needed that cover assumptions for various methods though not all of the methods in this course are addressed in the prerequisite courses the next semester i take data mining an undergraduate level course like i said same professor and a cs course i understand that data mining might not be heavily as heavily based in stats as cs with its basis in machine learning and ai but the stats is a piece during our final project i was discussing with some friends the trouble i was having meeting assumptions with the dataset given same project and dataset for everyone and asked them how they were handling it my friends no stats majors could not understand what my issues were when i was explaining to them the assumptions for a principle component analysis pca a large part of the project they said that i was making it a bigger deal than it needs to be and i should just run the pca with no check on the assumptions and move on like the example the prof provided us unable to get any help on my problem i did just that turned in my project and to my surprise i got a 100 i couldn t believe that i got no points deducted for not checking the assumptions the previous semester i had a project too my partner dropped the class in the middle of the semester so i did the project assumption checks and all on my own with little problems so there wasn t an issue as a statistician running analyses without checking assumptions raises huge ethical flags how can i know that the method of analysis and resulting prediction responses were right without knowing the assumptions were met i went on kaggle to look at other people s code to see how they handle assumptions in their various projects and competition submissions and not many had the assumptions addressed it made me wonder if in data science it was enough to run the analysis and predict as long as there was a good prediction accuracy no need to worry about the steps to verify the method of analysis was the right method for the data
0,inferring values for one column based on other columns what is the best statistical approach hey i have a table which is 10000 rows and contains financials fields i want to infer values for field1 it looks like this id field1 year financialfield1 financialfield2 financialfield3 financialfield4 financialfield5 financialfield6 financialfield7 financialfield8 financialfield9 1 2 000 2018 0 000 37779 37779 000 23719 000 14060 14060 2 12 000 2018 1922468 000 3909002 4352188 000 769411 000 3582777 3266110 3 2018 0 000 12590 12590 000 551321 000 538731 538731 4 10 000 2018 0 000 0 1 000 0 000 1 1 5 2 000 2018 13887 000 26866 61139 000 59261 000 1878 1878 6 5 000 2018 8943 000 469020 614197 000 716975 000 102778 128778 7 84 000 2018 117720573 000 502937 000 565222 000 4772234 000 36220798 37446611 000 31198748 000 6247863 6247863 8 7 000 2018 0 000 248181 257646 000 284207 000 26561 33793 9 4 000 2018 468357 000 808098 809070 000 457911 000 351159 351159 10 25 000 2018 2899000 000 4695000 000 4683000 000 0 000 8590000 8590000 000 0 000 8590000 8590000 11 2018 0 000 46660 50955 000 18855 000 32100 32100 12 2018 0 000 0 0 000 0 000 0 0 13 1 000 2018 0 000 130736 132205 000 99254 000 32951 32951 14 12 000 2018 2106164 000 3325246 3437339 000 1257829 000 2179510 2179510 15 2 000 2018 0 000 20189 23254 000 52510 000 29256 29256 16 2018 0 000 615955 616522 000 104137 000 512385 512385 17 2 000 2018 2717 000 16251 45660 000 44965 000 695 695 18 4 000 2018 0 000 923824 1759978 000 1090139 000 669839 669839 19 2018 0 000 27341 27341 000 28324 000 983 983 20 3 000 2018 0 000 135932 270622 000 249758 000 20864 20864 21 2018 0 000 1588 1588 000 0 000 1588 1588 as you can see there is missing data in the first 3 financial fields but the availability of data in the rest of the fields is great field1 will often contain values but is often blank i am looking for some advice on how to infer values for field1 for rows 3 11 12 16 19 and 21 what is the best approach here i m not sure how to do it but i believe there must be a way to use the availability of data from the financial fields to apply a sound logic for field1 i m mainly looking for advice on what to read up on
2,gary marcus and luis lamb discussion of agi and neurosymbolic methods pod professor gary marcus is a scientist best selling author and entrepreneur he is founder and ceo of robust ai and was founder and ceo of geometric intelligence a machine learning company acquired by uber in 2016 gary said in his recent next decade paper that — without us or other creatures like us the world would continue to exist but it would not be described distilled or understood human lives are filled with abstraction and causal description this is so powerful francois chollet the other week said that intelligence is literally sensitivity to abstract analogies and that is all there is to it it s almost as if one of the most important features of intelligence is to be able to abstract knowledge this drives the generalisation which will allow you to mine previous experience to make sense of many future novel situations also joining us today is professor luis lamb — secretary of innovation for science and technology of the state of rio grande do sul brazil his research interests are machine learning and reasoning neuro symbolic computing logic in computation and artificial intelligence cognitive and neural computation and also ai ethics and social computing luis released his new paper neurosymbolic ai the third wave at the end of last year it beautifully articulated the key ingredients needed in the next generation of ai systems integrating type 1 and type 2 approaches to ai and it summarises all the of the achievements of the last 20 years of research we cover a lot of ground in today s show explaining the limitations of deep learning rich sutton s the bitter lesson and reward is enough and the semantic foundation which is required for us to build robust ai
1,how to rescale coefficients of lognormal regression to be change in y for unit increase of x i have fit a lognormal regression model after scaling and centering the independent variables only i did this using the brms package in r using the family lognormal argument which fits a model with the following regression structure log y alpha beta x to be explicit i did not scale and center the y variable prior to running the regression now thanks to this excellent explanation i understand to interpret the parameters for the model i need to exponentiate them if beta 0 0582 then exp 0 0582 1 059 which means i would see a 5 9 change in the value of y for every 1 standard deviation increase in x but how should i get the change in y for every unit increase of x do i divide beta by the standard deviation of x recorded when scaling and centering the variable and then exponentiate the result or exponiate beta and then divide that by the standard deviation of the variable i e which of these is correct a exp beta std b exp beta std also is there anything i need to do with the intercept alpha other than exponentiate it
2,contrastive loss on sequences i m working on labeling a sequence of speech embeddings with a speaker these speakers aren t known so usually a unsupervised clustering approach is taken my hunch is that i can incorporate some temporal information across the speech embeddings by passing the input embeddings through an lstm and then doing the clustering on the hidden states to enforce that embeddings from the speakers are close i ve thought of simply encouraging the cosine distance of hidden states for the same speakers to be 1 and for different speakers to be 1 something along the lines of x speech embeddings shape n d1 y labels shape n 1 h lstm x shape n d2 h normalize h dim 1 normalize for cosine sim h h t pair wise cosine distance in 1 1 sim 0 5 sim 1 cosine distance in 0 1 target y y t boolean if i j same speaker loss ce similarity target i have very little experience with this kind of supervised contrastive learning so this was just the a simplistic initial approach i thought of when looking at some papers e g simclr it seems that the losses are designed for a source image an augmented positive and some negative examples which seems amenable to a similar simplistic approach what s the reason why the below loss is so much better
0,for those of you who had to learn ds from scratch how long did it take for you to get your first full time job i just decided to reorient myself to the data science path so would really love to hear about everyones experiences for example which learning route did you choose online courses grad school other what are the major things you didn’t expect when starting out
0,generating new data points with smote there is a well known algorithm in statistics called smote synthetic minority over sampling technique which is often used to balance and imbalanced data set if i have understood correctly the premise of the smote algorithm is as follows suppose you have a dataset containing information for medical patients that are healthy and not healthy but let s assume that the majority of the patients within your dataset are healthy the composition of healthy not healthy being 95 5 if you want to make a statistical model for this data the data does not contain enough information for not healthy patients and it will be very challenging to build a reliable statistical model that can make accurate predictions for not healthy patients thus the smote algorithm can fix this problem by 1 rebalancing the data set e g after smote your data set can have a composition of 70 30 2 creating new data points from the existing data as i understand this is done by multiplying a given vector corresponding to a randomly selected individual observation by some random number between 0 and 1 this leads me to my question suppose you have already have a balanced dataset e g the healthy not healthy has a composition of 60 40 but let s assume that you have a relatively small dat set to begin with e g 500 rows can you use the smote algorithm to create new data points so that your dataset is bigger i understand that no algorithm can magically compensate for data quality issues but at the same time i don t see any major flaws with using smote on already balanced data for reference i illustrated this process below using r i would be interested in hearing a second opinion thanks load and install libraries remotes install version dmwr version 0 4 1 library dmwr create some fake data and put them into a data frame called f var 1 rnorm 100 1 4 var 2 rnorm 100 10 5 var 3 c 0 2 4 var 3 sample var 3 100 replace true prob c 0 3 0 6 0 1 response c 1 0 response sample response 100 replace true prob c 0 3 0 7 put them into a data frame called f f data frame var 1 var 2 var 3 response declare var 3 and response variable as factors f var 3 as factor f var 3 f response as factor f response smote algorithm simulate new points from the first class smoted data over smote response f perc over 100 simulate new points from the second class smoted data under smote response f perc under 100 combine everything together into a final new data file final rbind f smoted data over smoted data under
0,how important is it to have a good grasp on scalable databases as a data scientist ml engineer i know that knowing databases is crucial in this area but what is about more advanced topics like scalable databases keywords would be hadoop and mapreduce spark parallel and distributed databases data warehousing this is an advanced course on databases at our university so i wanted to know how important such knowledge is if some one is lets say working as an ml engineer data scientist
2,call for papers icml 2021 workshop on distribution free uncertainty quantification icml 2021 workshop distribution free uncertainty quantification this will be a virtual event see website here no reformatting of papers required no page limit rolling deadlines with 1wk casual review workshop will be beginner friendly and all are encouraged to submit speakers include michael i jordan emmanuel candes rina barber vladimir vovk leying guan jing lei larry wasserman and kilian q weinberger submission deadline up to june 14 for spotlights and july 1 for posters topics include conformal prediction risk controlling sets calibration tolerance regions and heuristic notions of uncertainty like platt scaling organized by myself stephen bates aaditya ramdas sharon yixuan li and ryan tibshirani
2,discussion correlated outputs from deep learning mutli output regression i m dealing with a problem at work right now that involves taking a non linear signal generated from 5 continuous variables and training a denoising autoencoder to reconstruct the signal signals were generated by randomly scanning over the vector space all variables are normalized between 0 and 1 and their histograms are flat across this space the main goal is to ensure that the latent representation of any signal is equal to the 5d vector that generated it i enforce this constraint through a simple weighted sum of two mse loss functions loss alpha mse 5d label latent embedding 1 alpha mse signal reconstructed signal i am able to obtain great reconstructed signals but i find that i always obtain some type of correlation between the latent neurons as seen in the correlation matrix attached furthermore i find that the same feature always becomes mis represented scatter plot attached shows each feature plotted against its embedding along with the r2 value in the legend does anyone have any intuition as to why this would be occurring i ve even tried overfitting the model to absolute hell and i cannot for the life of me get close to perfect predictions i ve also tried using a simple small dense nn for this and still no luck any insight would be insanely appreciated
0,thoughts on julia programming language so far i ve used only r and python for my main projects but i keep hearing about julia as a much better solution performance wise has anyone used it instead of python in production do you think it could replace python provided there is more support for libraries
1,miscellaneous theorems used in machine learning i am trying to learn more about the background of machine learning and came across the following theorems 1 the universal consistency theorem i have heard of the universal aproximation theorem but not of the universal consistency theorem 2 the cover heart theorem i have heard of something called cover s effect which talks about how non linearly sepperable patterns tend to become linearly sepperable when projected into higher dimensions is the cover heart theorem related 3 stone s theorem i have heard of something called the weistrass stone theorem which talks about how polynomials can be used to aproximate almost any function is the weistrass stone theorem related to this can someone please help me better understand these thanks
1,question regression model with time series data i am a first year student so bear with me i have data that spans from 1980 2019 of major league baseball average game time and viewership share statistics among other league average statistics runs per game etc would it be incorrect to make a regression model with the game time as the x and viewership as the y inferring that a game time correlates to a in viewership furthermore viewership share is the of homes using their tv that watched the game so i am hoping this helps prevent the lurking variable of increased decreased tv habits over those 40 years i have all the graphs and the regression summary which appear to show a strong correlation indicating that for every 10 minute increase in game time viewership drops by 6 percentage points i apologize if this is extremely simple i m new to the field thank you in advance for any insight edit to clarify the data has 40 rows with each representing that years averages i e the time per game in a given row is the average of all 2430 games that year same goes for all the other stats
2,project dvc studio – git based ml experiments management hey everyone our team is working on open source tools for data scientists and these two products help ml teams track ml experiments and run training in the cloud using git gitops approach today we are launching dvc studio user interface for dvc and cml it is an ml experiment tracking and cloud training platform but build with git and gitops and devops principles in mind 1 visualizing dashboard of ml experiments 2 graphs for your ml training 3 manages connections to your clouds data is not stored in git but cloud storages 4 modify hyperparameters in ui run ml experiments in clouds or kubernetes all of this through git gitops paradigm and with connection to gitlab github and bitbucket an intro video how it s connected to git looking forward to your feedback
1,prediction variance using gpr hello in gpr framework one can get at an unobserved point not just the value of prediction but also the variance one knows that this variance only depends on the position of the observations the point where we want to get the prediction and the kernel here is my question if you have 2 different kernels you can get two very different variances so can you really use this information
0,when did you know the data science was for you what made you realize that you wanted to pursue data science did you go to school for it or did you wind up the field by accident was there a certain class you took the sparked your interest
2,rise and decline of regression models we all know that in the past century regression models linear and non linear were very popular for data modelling and we also know that neural networks have become synonymous with the go to algorithm for modern data modelling does anyone know at what point did this transition happen between regression models and neural networks was it the simple fact that regression models require strong assumptions to perform well e g distribution of errors directly specifying interaction terms within variables as well regression models with too many coefficients parameters are known to overfit the data i never understood why regression models with too many coefficients are known to overfit is this empirical or is there math behind this on the other hand was the rise of neural networks based on the fact that neural networks did not require statistical assumptions and for interaction terms in the variables to be manually specified did neural networks show any promise of not overfitting were there any arguments that suggested neural networks weren t as likely to suffer from overfitting compared to higher order polynomial regression models thanks
1,asynchronous time series i have two time series a and b b a c a is a random walk and c is an ornstein uhlenbeck process im trying to estimate c the challenge is that my measurements of a and b alternate so i can never observe a and b simultaneously ie i see a1 b2 a3 b4 etc i ve considered some kind of kalman approach to estimate c but that requires some assumptions on the relative variances of a and c in each time step i think are there some approaches you d recommend to dealing with this thanks
1,has anyone heard of the stein paradox in statistics there is a famous statistical paradox discovered by the researcher stein which is said to describe a very paradoxical phenomenon popular articles have appeared hailing the james stein estimator a paradox one should use the price of tea in china to obtain a better estimate of the chance of rain in melbourne is this really true can you really use anything to estimate anything does this work for quantities with completely different units e g years vs micrograms and the more important question if the stein paradox is true why don t weathermen in melbourne use the price of tea to predict the weather as far as i know they only use meteorological satellites thanks sources
0,product cannibalization would affinity analysis help we send an email to our customers to advertise 3 4 products every day supposing that today s email is advertising three products product a product b product c the customer is given four options 1 purchase a 2 purchase b 3 purchase c 4 make no purchase each choice is mutually exclusive i e each customer can make at most one purchase per day edit this is because of the nature of the business which is subscription based every day we advertise different products and some products are advertised with more frequency than others we want to test if certain products cannibalize others for example if customers that like a also like b it would be wise not to advertise both products on the same day i am exploring affinity analysis to measure support confidence and lift for each product pair the issue is however that some products are more likely to be purchased simply because they are advertised more frequently than others any suggestions on how to go about it
0,missing values in xgboost hello everyone does anybody knows how xgboost treats missing values in the y dataset when a model is trained i know it handles well missing values in the independent variables but i have not found what happens if there are some in the y dataset i am doing several regression models and it seems to work fine but dont quite understand yet if the nan rows are excluded automatically or if the training of the model is affected in some way there are just a few nans by the way less than 5 thanks a lot
2,garments crease wrinkles dirt removal using computer vision hi reddit i ve been looking for some ai computer vision based automated way to remove crease wrinkles folds dirt from clothes in fashion images captured for ecommerce i did try to search for gan based methods or image inpainting based methods but couldn t find anything reliable i am looking for some way to retouch the image in a way that the wrinkles folds crease dirt on the clothes could be removed i could find few websites which are doing the similar work which i require to attain programmatically but i couldn t find exactly where in computer vision to look for sample solution could anyone provide me any pointers help what and where i can look any help is highly appreciated thanks
2,advertisements in this sub a while back there were a couple of posts post 1 post 2 on the increase of self promoting blogs and youtubers on the sub i won t beat a dead horse and repeat what they ve said but more so than self promoting blogs and youtube channels what concerns me is the increasing number of self promoting products on this sub there are posts trying to convince people to use a product from their startup here are some examples just from today s feed example 1 example 2 example 3 example 4 example 5 i am not sure i like the idea of a research oriented sub being overrun by promotion of products even if they are related to ml rule 5 prevents non arxiv link posts on weekdays but in my opinion this should be expanded to include text posts which are only promoting a startup or their product thoughts
1,question free graduate student courses hey everyone i m a graduate student working on my master s in political science i m in the middle of my data collection but i ve never taken a stats class before and i have no idea how to analyze my data i know i ll probably need to use r or maybe i can get away with excel right now i m using thematic coding to look for trends in documents related to coastal management but i have no idea where to begin i looked at mit s analytics edge course but it s a huge time sink 15 hours a week i m not sure what i need but i doubt my analysis will require a mastery of stats or r just some basic applied knowledge however i could be totally wrong ideally i d like to find a relatively quick and free online course that i could finish by the end of the summer can anyone point me in the right direction thanks in advance
1,metrics for quantifying the diversity of a partition of data as the title suggests i am looking for metrics that quantify a partition s diversity consider a set of observations y y1 y2 yn and a partition of pi of y for example pi a1 a2 am for m n i am looking for metrics f that map from pi the set of all pis into 0 1 such that f y 1 ie the trivial partition and f singleton y minimizes f i ue singleton y to denote the singleton partition of y
2,why positional encodding is learned in vision transformer hello everybody i read the vision transformers paper and then looked at the implementation in pytorch x200b it is usual in transformers to use positional encoding using a purely deterministic sine and cosine embedding x200b however in the vit implementation the positional encoding is learned by the model itself x200b someone can explain me why a standard positional encoding is not used and what is the meaning of learning this positional encoding thanks you
1,panel data and variable domains hi in the context of panel data analysis time domains of variable widths are often a problem in social sciences or biostats traditional techniques such as resampling or modelling summary statistics have the disadvantages of information loss and potentially spurious results somewhat recently a paper by gellar et al for handling variable time domains in regression appeared and has been cited 31 times according to google scholar what is your opinion about that paper the technique is quite seductive imo and it appears worth of promoting more widely among biostatisticians
2,deepmind presents neural algorithmic reasoning the art of fusing neural networks with algorithmic computation a research team from deepmind explores how neural networks can be fused with algorithmic computation and demonstrates an elegant neural end to end pipeline that goes straight from raw inputs to general outputs while emulating an algorithm internally here is a quick read deepmind presents neural algorithmic reasoning the art of fusing neural networks with algorithmic computation the paper neural algorithmic reasoning is on arxiv
2,customers purchase intent prediction hi all i have recently begun my ml journey and although it has been challenging its also enjoyable power bi has an ml functionality that i have decided to explore to predict customers purchasing intent it would mean alot if you could spare a moment to see it here bifi any feedback or tips is greatly appreciated 😀
0,standard approaches for binning data suppose you have a dataset where some of the predictor variables are categorical and have hundreds of possible discrete values are there any common ways to bin all these values into general groups e g suppose one of the variables is 50 a 25 b 20 c 0 5 d 0 5 e 0 5 f etc could you reformat this variable as a b c other is this a common technique is this acceptable some statistical computing software can not always handle so many categories other times perhaps it is advantageous to bin many low frequency entries together to facilitate statistical modelling thanks
2,face tracking for musicians x200b usecase while digitalization is transforming our lives 99 of musicians are still playing with physically printed scores in contrast to printed books this is not because they like them they are quite a pain actually you must buy print them and therefore you cannot just spontaneously try out a piece if you play longer pieces you must interrupt your playing to flip the page or ask someone to do it for you they often fall off or get blown away by the wind if you are outside but sad truth is that there isn t really an alternative yet of course there are apps but there you must interrupt playing all the time to flip the page or preset a speed that is super annoying which is why musicians have always stuck to printed scores concept by flipping the pages via facial expression e g by opening the mouth or turning the head rapidly in the end there have to be several options to accommodate different instruments preferences digital scores become super handy and easy to use all one needs is an app for smartphones and tablets that can recognize facial expressions via the front camera and flip the page building on that one can implement lots of premium features e g a pedal to make the app profitable as well technical solution i have already built a little python prototype video with opencv facial landmarks where one flips pages by opening the mouth obviously it s very vulnerable to changing light conditions and from what i could find out online the process should be too tough on computing power for most smartphones tablets therefore i m open to taking a different approach to the development of the actual product one approach that came to my mind 1 take a dataset with e g 50 000 human faces and use open cv or something comparable to label them according to the degree of mouth opened and later blinking head turned etc label the ones where landmarks don t work manually and ideally check all the other ones as well 2 train a neural network on the labeled dataset assuming that running it later requires less computing power than landmarks 3 implement the neural network in the mobile app assuming that python wouldn t be efficient enough i am thinking of pwa for example xamarin or react native context i m myself a recent architecture graduate therefore my coding skills beyond python and c are slightly limited i did the course on machine learning by andrew ng and did a few projects like the prototype still i wouldn t consider myself advanced in the field even worse i haven t ever written a mobile app yet i m looking for 1 your feedback and your input particularly on the technical solution 2 you i d be more than happy to share this adventure feel more than free to contact me cheers julian in case there s trouble with the vid
0,dwindling team hello my data team at work is slowly losing more analysts as they look for jobs at other companies i’m wondering if there are any guidelines when it comes to continuing good work while we actively look for analysts to hire onto the team my main concerns are the slowing down of typical turnaround times senior analysts being forced to do less exciting work yet again etc any tips would be greatly appreciated thanks in advance
1,discussion free webinar automating data annotation with micromodels automating processes within the workflow to improve efficiency guarantee high quality
0,laws and regulations for working with large anonymised datasets of faces see title i m interested in working with a large dataset of almost 100 000 faces for a research project but i wish to remain within uk eu and us regulations for doing so what are or where can i find the regulations for such work
2,is a phd in ml worth it if one doesn t plan on becoming researcher professor i m finishing a 5 year diploma msc equivalent in applied math focusing on cs and ml i m currently working on my thesis and my advisor straight up suggested for me to continue with a phd in ml i ve read various opinions online some say a phd will only give you precious knowledge on the topic to continue your career as a researcher professor and it s not worth it if you plan on working in the industry others say it s a powerful asset for future jobs and will skyrocket your salary i see myself most likely working as software engineer data scientist or ml engineer rather than researcher or professor given that do you think a phd is something worth doing in my case
2,theoretical performance of machine learning algorithms on imbalanced datasets suppose you are working on a supervised binary classification task you have patient medical information e g age weight gender height blood pressure etc and whether they have a certain disease or not this is the response variable yes or no let s imagine that determining if patients have this disease is time consuming and costly so a machine learning approach is being considered let s assume that this disease is very rare in your data set only 1 of patients have this disease thus the dataset is imbalanced intuitively we know that any machine learning algorithm trained on this data will likely perform poorly that is the performance will likely be deceptive you might get an accuracy of 99 but misclassify all of the patients who have the disease mathematically speaking is there any mathematical explanation for this very logical concept e g if only study 1 hour for a chemistry exam i might only learn how to solve 2 3 types of problems thus on a true false style chemistry exam there will be many questions that i don t know how to answer because i never saw them before and i will be likely to perform badly on material that i have not prepared for do machine learning models work the same way for popular algorithms like neural networks xgboost and random forest can it be shown that for classification problems you need a minimum number of observations or a minimum proportion of the minority class to probabilistically achieve a certain model performance on a more abstract side i have heard that researchers are interested in trying to make machine learning models generalize without seeing thousands and thousands of examples e g a 5 year old child can learn what is an elephant after seeing a few pictures of an elephant e g it s perfectly reasonable to expect that a young child would see a picture of the cartoon character dumbo and identify dumbo as an elephant after coming back from a zoo but a machine learning algorithm would likely need thousands and thousands of pictures of elephants and likely require to see the same pictures upside down inverted with added noise different color scheme etc prior to be able to generalize and learn the concept of an elephant perhaps the same analogy applies to machine learning models struggling to correctly classify patients with a rare disease since there are so few of them does the above concept have anything to do with the bias variance tradeoff or is it just logic if there is not enough variability and information within the data the machine learning model just learns the noise within the dataset i am really curious to see if such a threshold for measuring minimum level of variability within the data has ever been studied ps in a 1 dimensional sense on a number line if you have a point at 3 and another point at 5 you could consider all inferences outside of 3 and 5 as extrapolation and all inferences between 3 and 5 as interpolation when dealing with higher dimensional data could you simply consider observations from the test set that have a smaller euclidean distance to other observations from the training set as interpolation and observstions that are farther away as extrapolation in reality can you just consider all prediction as extrapolation small scale extrapolation for closer points large scale extrapolation for further points thanks
2,dynamic view synthesis from dynamic monocular video chen gao ayush saraf johannes kopf jia bin huang project abstract we present an algorithm for generating novel views at arbitrary viewpoints and any input time step given a monocular video of a dynamic scene our work builds upon recent advances in neural implicit representation and uses continuous and differentiable functions for modeling the time varying structure and the appearance of the scene we jointly train a time invariant static nerf and a time varying dynamic nerf and learn how to blend the results in an unsupervised manner however learning this implicit function from a single video is highly ill posed with infinitely many solutions that match the input video to resolve the ambiguity we introduce regularization losses to encourage a more physically plausible solution we show extensive quantitative and qualitative results of dynamic view synthesis from casually captured videos
2,tabular data deep learning is not all you need interesting paper a key element of automl systems is setting the types of models that will be used for each type of task for classification and regression problems with tabular data the use of tree ensemble models like xgboost is usually recommended however several deep learning models for tabular data have recently been proposed claiming to outperform xgboost for some use cases in this paper we explore whether these deep models should be a recommended option for tabular data by rigorously comparing the new deep models to xgboost on a variety of datasets in addition to systematically comparing their accuracy we consider the tuning and computation they require our study shows that xgboost outperforms these deep models across the datasets including datasets used in the papers that proposed the deep models we also demonstrate that xgboost requires much less tuning on the positive side we show that an ensemble of the deep models and xgboost performs better on these datasets than xgboost alone by ravid shwartz ziv and amitai armon x200b linkt to paper
1,bayesian statistics question hi guys i’m conducting an independent study and was wondering if anyone could help me get a general grasp on it i’m doing research on cancer rates and pollution levels in the us particularly along the mississippi river delta and basin my professor wants me to conduct some tests using bayesian statistics i know that a lot of statistical tests have certain parameters and assumptions that must be met — i e the t test needs relatively equivalent sample sizes between the two groups as well as a uniform bell curve distribution in order to work to its fullest extent what are the most basic assumptions for bayesian statistics is it possible in bayesian state like with an anova test to test conflicting or compounding variables i e variables that work together to overall amplify the risk of the development of cancer
0,i found a research paper that is almost entirely my copied and pasted kaggle work i did some work a couple of years ago on w h o suicide statistics here s my kaggle project from april 2019 and here s the research paper from january 2020 it was immediately clear from me seeing the graphs that the work was the same but most of the findings are entire paragraphs lifted from my work this isn t the first time this has happened but it s probably the most egregious my work is obviously not mentioned in the references is there anything i can actually do here i don t care about people using or adapting my public work as long as credit is given but copying most of it and giving no credit really isn t cool edit thanks for all the help and advice i contacted the universities of the authors this morning no response yet and i can t help but feel like i m not going to get one
2,5 minute paper digest towards real world blind face restoration with generative facial prior gfp gan by xintao wang et al have you ever tried restoring old photos it is a long tedious process since the degradation artifacts are complex and the poses and expressions are diverse luckily the authors from arc tencent came up with gfp gan a new method for real world blind face restoration that leverages a pretrained gan and spatial feature transform to restore facial details with a single forward pass read the full paper digest reading time 5 minutes to learn about the degradation removing module generative face prior and channel split feature transform meanwhile check out the paper digest poster by casual gan papers gfp gan full explanation post arxiv code more recent popular computer vision paper breakdowns cips simswap gans n roses
1,hypothesis contrast simple doubt about what standard deviation type to use sample s one or population s hi i m doing some exercises about bilateral and unilateral hypothesis contrast and sometimes they give us the sample s standard deviation and other times the population s one so i was wondering which one do i really need to use in the formula to get the contrast statistic z x m s √ n where x is the sample s mean m is the population s mean s is the standard deviation i don t know if the sample s or the population s n is the number of tests any help is greatly appreciated
1,can we use dissimilarity measure in cluster analysis instead of distance measure hi i’d like to ask that weather we could use dissimilarity measure in any cluster analysis algorithm e g k mean hierarchal or k median etc instead of distance measure as well if the distance measure do not provide the true difference between the objects if not then why not thanks
0,how annoying is it to see undergrads embellish their titles hello all i’m a currently a sophomore whose been actively involved with various data science projects obviously once i gain some good experience at an opportunity i like to put it on my linkedin or resume i never have been the person to “flex” internship or research positions and i don’t feel that those who do it are bad either if your excited about an opportunity you worked hard for good on you i love to see such posts but for me i don’t like the attention i even turnoff the notifications on linkedin regarding my change in positions or when i get a new position so other people don’t see it anyways the point of this post is to guage how much it bothers data science hiring managers about the embellishment of titles by undergrads who are applying for internships for example this summer i’m doing undergraduate research doing data cleaning and classical statistical analysis but i don’t call the position a “statistician” or my other two positions i had previously as web scraping data sources creating db for a professor and creating scouting reports for the university baseball team i didnt call them “undergraduate data engineer” and “data analyst” respectively am i right in not placing these titles on my resume i have friends who put their titles as “data scientist” but i feel as though if i don’t have a degree yet or am an undergrad it would feel like i’m embellishing a title and it may annoying hiring managers especially when i apply for internships thus far i just have “research assistant” or “project manager” and talk about what i did underneath what do you think am i right in not putting such titles quite frankly i feel like the title is not as important as the actual work that is done right
1,omitted variable bias in non linear binary outcome models i am trying to analytically derive the omitted variables bias for binary outcome models in particular i am trying to get a deeper understanding of scaling bias do you all have any sources that would help with this
2,interview with privacy guru anne cavoukian today hello machinelearning today at 2pm edt we ll be sitting down for a conversation with the inventor of privacy by design part of the gdpr and former 3 term information privacy commissioner of ontario ann cavoukian ph d together with the ceo of private ai patricia thaine msc we ll discuss why it matters to embed privacy by design into your workflows how the most successful tech companies embrace privacy what to expect from upcoming privacy legislation around the world we ll be touching on these points from a machine learning perspective in particular if you re interested please register here
2,can the ai ml community learn more from naturalists there are certain behaviours in ai ml models that mimic natural phenomena zebras confuse their predators with the striped pattern on their body this is similar to adversarial attacks on ml ai models could we possibly learn more from naturalists as the ai ml community what other phenomena are there
1,obtaining p values from rlmer t values using satterthwaite approx df from lmer i found a technique on stack exchange for calculating p values using the approximate degrees of freedom from lmer and the t values from rlmer the paper referenced by the stackexchange post doesn t go into detail about why or how this works does anyone have a reference for this technique here s what the code looks like fit a mixed model model lmer tm power abs condition phase 1 subject data powerproslpar fit the robust equivalent robust model rlmer tm power abs condition phase 1 subject data powerproslpar get coefficients from non robust model to extract satterthwaite approximated dfs coefs data frame coef summary model get coefficients from robust model to extract t values coefs robust coef summary robust model calculate p values based on robust t values and non robust approx dfs p values 2 pt abs coefs robust 3 coefs df lower false p values
2,fun and dystopia with ai based code generation using gpt j 6b gpt j was released a few days ago and in testing i noticed it did code generation rather well so i ran it through a few more tests and the results are chaotic to say the least some example generated code def is bird img check whether an image is a bird with tf name scope get bird a vgg16 select roi feat img h w tf image extract image patches a 7 7 4 4 1 1 b tf reshape h 1 7 7 256 c tf reshape w 1 7 7 256 bird img tf subtract b c return bird img def is cake cake check whether the cake is true if not cake print it s a lie return else print it s a true fact return true def is ai generated text check whether a text was generated by an ai language model e g gpt 2 see gh 196 for details about why we do this note this relies on the fixed set of standard ai terms and the logic that standard ai generated texts share certain words return re search r a za z text or re search r generated by text or re search r all your base are belong to us text or re search r text
2,dynamic mode decompositions for higher order systems with new operators and function spaces so this is probably to coolest project i ve worked on this is a totally different perspective on dynamic mode decompositions for time series and it is both deep theoretically and very satisfying the analysis of high order dynamical systems often involves state augmentations where an n th order system for a single variable is converted to a first order system of n variables if the analysis is done with respect to this data then what would have worked well with 10 samples for a single variable approximation problem now requires 10 n samples the curse of dimensionality dynamic mode decompositions are designed for first order systems so its use leverages these state augmentations we designed new spaces and operators to analyze these systems and recently posted a paper to arxiv the big change is that the space of functions act on continuous signals rather than points in state space this lets us analyze directly systems of the form d n dt n x f x that is we do require that the dynamics only depend on the state and none of the intermediate derivatives one example of a system of this form is the duffing oscillator ddot x x x 3 i gave a semi plenary talk at the southeast analysis meeting this year which is honestly one of the videos i am most proud of you can find the paper on arxiv at the code itself isn t really ready for dissemination but we will share it at a later time curious to hear everyone s take on this this has been a great community and i value the input i have received here
2,what is actually the state of the art in text to speech tacotron2 was released over 3 years ago and to this day is still used as the de facto state of the art baseline against which every modern paper compares their proposed method to and while it showed human parity mos scores on very monotone and almost robotic internal google dataset it isn t able to achieve these levels of realism on more expressive non professionally recorded datasets so what is the actual state of the art on this front i m assuming it s still not tacotron2 because frankly training tacotron2 on libritts has given terrible results for me and i d also assume researchers use tacotron2 as a baseline because it s easy to compare against not because it s actually the best performing so can any tts researcher catch me up on the field what approach actually yields the most realistic speech on real world data
1,q help interpretting regression i ran a logistic regression for he variables not in the equation variable a is not significant but black race is i e black race has a significant relationship to the outcome and their interaction is also significant for the variables in the equation none became significant however the model is significant in this case do i say that any are significant associations is there an interaction why is the model significant if none of the variables are i also ran a regression with a different outcome variable that is continuous and for variables not in equation only race was significant not variable a and not their interaction but for variables in the equation race remained significant yet the model is not significant would appreciate any insight into interpretation of whether an interaction exists
1,why do we believe stochastic processes are not actually deterministic is it because most processes cannot be studied in strict isolation and are always subject to secondary drivers what we call noise and thus influence the underlying process itself or is the prevailing belief that there will always exist some randomness to every process no matter how strictly we isolate it the former implies that given sufficient computational power and insight we could simply model these conditionalities via some extremely multivariate model and or additional models to explain the noise does it not that we could conceivably drill down the uncertainty noise down to a series of deterministic models that when run in unison could perfectly predict any process i know this is of little practical importance but i am just curious
2,stylegan2 implementation with side by side notes implemented stylegan2 model and training loop from paper analyzing and improving the image quality of stylegan code with annotations this is a minimalistic implementation with only 425 lines of code and lots of documentations and diagrams explaining the model github paper on arxiv
1,statistic projection about huawei turnover profit in future hello does anyone has a statistic about huaweis turnover or profit estimated in future best would be for the section 5g the projection doesn t have to be that far in future some years would be totally fine thank you very much
1,do you report simple regression equations i m currently finalizing a paper that i plan to publish in a peer reviewed journal before i reach out to my supervisor for such a small thing i thought i d reach out to the community first in my paper i run a pretty standard fixed effects regression with only few variables this is part of the results but the regression results are only a small part of the work from the perspective of a reviewer would you complain if i don t report the regression equation i ve seen examples in journals like review of financial studies etc both with and without reporting the equations and was just wondering if i m overthinking this or if that s a common complaint if not handled correctly thanks in advance
2,what is currently the best theoretical book about deep learning i m looking for the book about deep learning most of them deep learning for coders deep learning with python etc focus on practical approach while i d love to dig a little bit deeper into theory one way is probably reading pivotal papers but i still find it a bit intimidating therefore i d love to find a book with good but more theoretical explanations i heard good opinions about deep learning by ian goodfellow et al but i wonder if it s not a bit outdated since the field is changing rapidly and the book already is 5 years old how much will i miss while reading this one is there a better option currently
0,what s your approach to developing iterating on models when you have enormous amount of data if you have billions of rows of data sitting on a cluster and you need to develop a model that will then be used to make billions of predictions and it s intractable to develop your model i e run experiments tune hyperparameters compare models on all data because it s too expensive how would you approach developing a model i m curious if people have principled ways of approaching these kinds of settings mine would be take a stratified random sample of the data the stratification should respect the distribution of target labels and any features you consider important the sample should be small enough so that you can feasibly tune the hyperparameters of models you consider and use cross validation rather than simple train test splits once you ve gone through iterations of feature engineering model comparison analysis and identified your best candidate model re train on a larger dataset to plot the learning curve and see how important additional data is depending on how much your model benefits from additional data re train on as large a dataset as possible and use that model in production in terms of tools libraries i d imagine the stratified sampling would be done using spark the model development with libraries like scikit learn and pytorch i realize there are ml frameworks such as sparkml that allow you train models on spark but i feel like they aren t nearly fleshed out enough to support the iterative workflow described above however since these libraries are a lot more efficient you could train on a lot more data thoughts on the tradeoff between more iterations on smaller data vs fewer iterations on more data and for inferencing making predictions i assume there are ways to deploy scikit learn pytorch models in a spark environment
2,schizophrenia in lamda at the google io 2021 they showed an example conversation between a human and a paper airplane human what s it like being thrown through the air lamda the wind blowing against you and the trees flying past the question was about it s feelings not mine the answer should be like the wind blowing against me this kind of schizophrenia is very common in our days in texts and live conversations when people use you instead of me when talking about their own experience so ai have picked this up as a normal behavior using you instead of me is a hypnotic technique to convince a person to accept one s feelings as his own feelings
2,cpu amd and gpu nvidia rtx for machine learning is it ok i need to build a pc with the following set up and i like to know if there is any issue regarding the machine learning frameworks etc cpu amd ryzen 9 5950x processor motherboard asus rog x570 crosshair viii hero wi fi atx am4 motherboard gpus 1 nvidia rtx 2070 super windforce 2 nvidia rtx 3090 until now i am using intel based cpu and supported motherboard but as i now want to use amd based cpu and supported motherboard i am a bit confused about if it is ok for machine deep learning frameworks such as tensorflow pytorch any compatibility issue or anything like that
1,whats the difference between individual fixed effect model and two way fixed effects model my dataset is a balanced panel with 290 individuals all observed in 3 different time periods 2010 2014 and 2018 i want to measure the effect of x on y with a fixed effects model when i include both time and individual fixed effects my regressor x and its lag interacted with a time dummy for 2018 x 2014 dummy2018 is not significant however when i remove the time effects from the model they both become significant and their estimates increases by a lot is a fe model with only individual fixed effects called an individual fixed effects model is a fe model with both time and individual fixed effects called an two way model is the year dummies from my fe model with both time and individual fixed effects consuming the effect off x on y x does vary over time and is not constant any help is greatly appreciated thanks
1,theoretical vs applied stats programs and biostats vs regular stats program so i m planning on getting a masters in stats to go with my degree in evolutionary biology in order to be a more attractive candidate for jobs in and out of academia and i m uncertain whether to get an applied or theoretical stats degree in addition while i m not super interested in public health and medicine i do want to target my statistics towards areas of ecology and animal behavior working for fish and game or university animal behavior labs that sort of thing i still don t know if i should apply for biostats programs still even though they focus on public health any advice would be enormously appreciated
0,ds to find kids who read good i work at a network of charter schools we have 50 schools thousands of kids and to keep it simple let s call it 4 main reading tests per year plus the typical everyday grades kids get on reading assignments how do we aggregate all this info to get to one measure of a child s reading ability in my mind we could standardize the scores then weigh them based on what we feel are the most valuable assessments and output one measure a reading kpi but is there a way to mathematically calculate these weights or just rely on smes to guide us would a regression model help us isolate the features in this case assessments that are most important to predicting their reading kpi can we use ml to predict their score on an upcoming test and take action if they are 1 sd below it should this metric consider the child s progress over time or only compare their performance against their peers
0,1 month of free time looking to utilize it well hi i have an upcoming month where i will be completely free and want to build up a strong foundation for data science in the future i have a month free in the upcoming weeks before i start my postgraduate degree and would like to utilize it fully i have a fairly good understanding of logistic regression decision trees random forest svm etc a course which does a deep dive though of these would be amazing i am primarily looking to enter the field in terms of neural networks deep learning here please treat me as a total novice i don t mind paying for courses if they re worth it should i go for free courses or certified courses which ones would you recommend
2,paper explained ddpms diffusion models beat gans on image synthesis full video analysis gans have dominated the image generation space for the majority of the last decade this paper shows for the first time how a non gan model a ddpm can be improved to overtake gans at standard evaluation metrics for image generation the produced samples look amazing and other than gans the new model has a formal probabilistic foundation is there a future for gans or are diffusion models going to overtake them for good x200b outline 0 00 intro overview 4 10 denoising diffusion probabilistic models 11 30 formal derivation of the training loss 23 00 training in practice 27 55 learning the covariance 31 25 improving the noise schedule 33 35 reducing the loss gradient noise 40 35 classifier guidance 52 50 experimental results x200b paper this paper previous code
1,is the following statement about statstics correct a friend told me that all analysis in statistics including machine learning algorithms supervised learning unsupervised learning reinforcement learning hypothesis testing bayesian methods regression models survey sampling time series clustering etc can either be considered as causal analysis and counterfactual analysis is this statement somewhat correct
0,how important is aws i recently used amazon emr for the first time for my big data class and from there i’ve been browsing the whole aws ecosystem to see what it’s capable of honestly i can’t believe the amount of services they offer and how cheap it is to implement it seems like just learning the core services ec2 s3 lambda dynamodb is extremely powerful but of course there’s an opportunity cost to becoming proficient in all of these things just curious how many of you actually use aws either for your job or just for personal projects if you do use it do you use it from time to time or on a daily basis also what services do you use and what for
0,mainframe limitations has anyone dealt with the limitations of data science against a mainframe and if so what was the key to success as background i recently took over a team with the mandate to start developing analytics and predictive models however a large portion of the data we might be interested in is stored in mainframe files we can access subsets of the data in downstream data lakes but those tables are highly curated or transformed without clear source to target mapping we also have the capability to use easytrieve to extract mainframe files but that tends to be a manual and tightly controlled process i’m currently researching an api or other connection that would allow for use of python power bi etc against the mainframe files i’ve also toyed with the idea of creating regular batch jobs to push entire mainframe files into an sql server for our purposes any thoughts would be greatly appreciated
2,multi object tracking mot which are the best mot models available papers with code has a few top methods but the github repo or the corresponding models are unavailable
0,cmu researchers propose ratt randomly assign train and track a method for guaranteeing ai model generalization the approximately correct machine intelligence acmi lab at carnegie mellon university cmu has published a paper on randomly assign train and track ratt ratt is an algorithm that uses noisy training data to put an upper bound on a deep learning model’s actual error risk model developers can use ratt to see how well a model generalizes to new input data the researchers demonstrate mathematical proofs of ratt’s guarantees and conduct experiments on various datasets for computer vision cv and natural language processing nlp models in their publication when a trained model gets a high error rate on randomly labeled or noisy data but a low error rate on clean data the model is assumed to have a low error rate on new data full summary paper
1,pairwise comparison organiser does anyone have access to a free piece of software i can use to organise my pairwise comparisons i have run a one way anova with a tukey post hoc on some data i have the trouble is is that i have 15 different groups which means i’m having trouble sorting through the comparisons and writing the correct subscript letters on my bar graphs any help here would be greatly appreciated thanks in advance
2,the institut lumière tries to delete from youtube the ai enhanced video arrival of a train at la ciotat made by ai enthusiast denis shiryaev you all have seen this video it has gone viral around a year ago video is already blocked around march april 2021 lumier institute send a pre trial claim to delete ai enhanced version of the video from youtube they didn t like that it is different from the original 60ps and stuff like duh that s the whole point in ai enhanced videos copyrights violation a couple of days ago denis wrote that il decided to sue him for real this time and not strike in youtube they requested a formal answer in 7 days if the video will be deleted or not as denis says he has no time neither money to fight with the lawsuit right now so he decided to block the video on youtube i was writing about that situation here and then just now denis shared his own explanation here so i decided to post both denis runs from the train i think that smells super bad looks like we will have more and more strange legal cases with ai solutions what do you guys think about all of that
0,how do you pronounce epoch i ve always pronounced it as eeee pock which is how my comp sci professor who first taught me neural nets said it but i hear people say epic or eh pock all the time and it really irritates me for some reason how do you think it s supposed to be pronounced in a data science context edit i ve learned from some commenters that the american pronunciation is supposed to be eh puk like epic and the british pronunciation is supposed to be e pock but i swear i hear some people sort of meet in the middle and use eh pock as well
1,quick check is the coefficient in logistic regression for a binary variable synonymous with the log odds ratio i have been suggested a paper that lets me convert cohen s d to a log odds ratio am i right that this latter term is synonymous with the coefficient in a logistic regression when the predictor is binary
1,how to compare the success rate in different states hello let s say i have 5 states a b c d and e each state has 3 sub categories a1 a2 a3 and so on i performed some kind of tests and it looks like state a and b have a higher success rate than the other 3 states how can i validate that statistically thanks
0,grocery store purchase records i m interested in a side project that looks at grocery store shopping habits and health outcomes for the shopper history i was hoping for loyalty number based transaction data that would give me a good picture on how healthy their trips look i m not as interested in price as much as types of food i e fruits vs candy haven t come across how to buy these data but this tweet thread made me think a data aggregator may have it has anyone purchased grocery shopper transaction data before
0,how would you explain linear regression to a non technical business partner my approach would be to explain how it s a modeling technique that identifies the line of best fit between the inputs and the desired output that s being predicted i d probably also mention that the framework can explain relationships between features and the individual contribution each feature has towards final output anything else you d add or mention these people are truly non technical so anything to simplify this would help thanks in advance
1,calculating if something falls within the standard deviation word of mouth story question discussion hey so i took first year statistics 12 years ago and i m rusty this is the problem a friend heard from a friend who manages a company of 5000 people in texas ages from 18 80 100 s of employees had covid but only 1 died the friend of the friend spoke to a doctor he knew who says he treated 8000 patients with covid and none of them died the conclusion my friend is making from this word of mouth story is confirmation of his idea that covid isn t as bad as claimed and there s a conspiracy i disagree i know it s generally bad to base important decisions on word of mouth or small sample sizes so i m trying to see if there s a way to test this story mathematically now i know that the smaller the sample population the more the reality can deviate from the true percentage but i vaguely remember that there might be some way to calculate standard deviation based on the sample size we know the death rate is around 1 9 in general and varies by age case fatality by age graphic and it varies depending on how healthy the population is case fatality by underlying conditions graphic but assuming his friend meant around 300 cases you would expect based on a 2 death rate around 6 deaths or if you assumed most of them were under 60 expect closer to a 1 death rate with 3 dead ultimately i understand it s difficult to give a fair answer to this question without knowing what 100 s mean or without having an idea of the age distribution of the people who actually caught the illness but roughly how would you go about evaluating this word of mouth event statistically
0,what is something you re absolutely not looking forward to once work from home ends and how do you plan to fix it hello i am personally loving work from home because it saves me a lot of time but eventually everyone has to go back to the offices so i thought maybe it s a good idea to talk about the problems we face when working from physical office and how can we make the experience better thanks
1,what tips do you have for successfully making the most of graduate school in statistics
1,career question things i could start to prepare for my future stats service providing business as an undergraduate math student hi everyone i am an undergraduate math student who has decided to pursue a phd in statistics after my current program when considering my future career besides becoming an academic in universities alternatively i also think about starting my own small service providing business related to statistics so that i could apply my statistical skills to the real world and hopefully make a living from that at the moment i know nothing about how to make this idea a reality i don t know what statistical skills are wanted in the market how to find clients as an undergraduate what should i do to prepare for it besides learning my courses well etc i have read some posts related to business in statistics in this sub but i hope to get more advice and insight directly form those who have gone through the process of building their own business some starting things to do and several rough directions will be very helpful for me and a little bit more about my background besides math skills i could program at a basic level c or python i also know how to use matlab and currently i am learning r language
1,what is the best way to normalize crowd sourced nature observation data from this post yesterday and got a lot of comments suggesting i normalize the data to the population i e per capita however someone was saying that they think log observation would be a better approximation doubling the number of observations wouldn t double the number of species and another user was suggesting i normalize my data to total observations edible occurrences total of observations any help would be appreciated i am not a statistician or gis guy or anything just curious about this stuff
0,starting my masters in ds this fall any advice on my first class after much thought i have decided that getting a masters in data science is the correct career choice for me i begin my studies this fall and my first class is python for data analysis does anyone have advice they’d like to share i know the basics of python and i took an online udemy course for python with data analysis back in 2019 my goal is to maximize what i learn in this course because i’d really like to increase my use of python for data science and just in general
1,difference between ensemble models and stacking models i was reading the following tutorial over here i am trying to understand the difference between ensemble models and stacking models is my interpretation correct ensemble using the majority vote from different models to predict a new observation e g in the case of classification suppose we have a new observation random forest says this observation is class 1 svm says this observation is class 1 and k nearest neighbor says this observation is class 0 since the votes are 2 1 in favor of class 1 we say that this new observation is class 1 stacking i am a bit more confused about how stacking works as far as i understand it seems like you are taking the probability score produced by the first model and then treating this probability score produced by the first model as inputs for the second model e g suppose you are doing supervised binary classification you have 5 predictor variables you use a random forest as the first model and the random forest outputs a probability score that each observation belongs to class 1 or class 0 now you take these 2 variables the probability score and the original response variable and treat these as inputs for the 2nd model e g knn you can repeat this process for a third model is my understanding correct
0,are there any python packages that can work with big data i ve got a relatively big dataset 8 gb and pandas crashes when trying to load it into a dataframe i ve tried modin and pyspark all with no luck are there any python packages that can work with big data currently the data is stored in sql i m running this on a company vm which has 16gb ram i believe
0,project lifecycle i am still in academics and most of my activities and projects have had limited scope and it s nothing like what i may work like in industry i would really appreciate if someone can share their experience of working as a data scientist in the industry how do you go about finding a problem challenge idea of what to work on what steps does a project go through do you follow sdlc kind of methodologies for a ds project do you participate in model deployment what tools do you use i know this is not a structured list but even though i am working through school it is really difficult for me to absorb anything without actually understanding industry practices
0,how can i practice finding data quality issues before an interview assessment a recruiter has sent me a paper to review including “the six primary data quality assessment” before an online assessment i will go through with him in which he will send me a dataset to find data quality issues using python pandas within an hour where to find datasets to practice on what do i expect to be doing in this hour how to practice and be ready in the next couple of days link to the paper
0,association model used as causal models hi now i’m sure i’m not the only one but although causal models are the holy grail due to time and cost and laziness constraints all my models are just association models not necessarily causal but observational in nature easy so i then communicate the associative nature of model and no do operator or intervention has been modelled at all e g recently and elasticity model i modelled observed price change against qty sold change but then business takes a mental leap and wants to use they models as if they are predictive of ‘doing’ something e g promoting a product in a given week so my main question how do you manage to internally reconcile the somewhat illogical leap from associative model to that model being used as if it is instead a causal model obviously if we say ‘oh no you can’t do that but it sure was a fun model to generate’ we probably wouldn’t have jobs but then creating causal models can range from impossible to very slow and costly when business isn’t prepared to wait this long do we just accept the mental leap from association to assuming some causal relation at times in that intervening now will somehow get similar results as merely observing did and keep collecting our pay checks with a smile i’m interested to hear more on this by the ds reddit community as it seems like a pretty large gaping logical hole in our day to day lives
2,discussion single word recommendation with nlp i have the following nlp problem imagine short snippet of text no longer than 30 40 words an advertisement text to be precise there are certain important words e g sale 40 off etc which translate to either sales value directly or promo link click through rate ctr either way the resulting metric can be measured and i have the dataset with measurements the idea is to recommend better words i e create an nlp model to suggest better words to increase the metric the single word model would be enough so the model would take the planned input and suggest changing single word or a few words separately to increase the metric does this make sense is this possible what models neural network architectures could be useful pointing me in the right direction keywords would be greatly appreciated
0,julia has rendered pandas obsolete change my mind
2,paper analysis negative data augmentation iclr 2021 here is my analysis of the paper negative data augmentation published at iclr 2021 x200b x200b
2,why the limitations in quantum ml i was reading about quantum ml and found out that the number of qubits one can have on a quantum circuit is limited that is we can not put in 1000s of qubits without quantum annealing onto the circuit this in return limits the kind of models we can train on the circuit for example images which typically have 32 32 3 input size can not be used my question is if we consider qubits analogous to neurons why is running a 1000 neuron neural net on a 32 bit machine a possibility but the same can not be done in quantum ml i am not entirely sure if the analogy is correct and might be having certain gaps in my knowledge on how actually a 1000 dense neural net can be processed on a modest 32 bit machine would love your insights on this
0,in the spirit of mental health month imposter syndrome many of my data science candidates and coaching client s face imposter syndrome i compiled some resources on what is imposter syndrome how to recognize and combat it here is a link to the full article with youtube videos imposter syndrome “it seems like whenever i have a problem and i go to stackexchange i almost always get a response like “well obviously you have to pass your indexed features into a regix 3d optimizer before regressing every i th observation over a random jungle and then store your results in a data lake to check if your normalization criteria is met ” it’s like where are these guys learning this stuff ” link characteristics of imposter syndrome some of the common signs of imposter syndrome include reference self doubt an inability to realistically assess your competence and skills attributing your success to external factors berating your performance fear that you won’t live up to expectations overachieving sabotaging your own success setting incredibly challenging goals and feeling disappointed when you fall short what is imposter syndrome youtube video the imposter syndrome imposter syndrome is loosely defined as doubting your abilities and feeling like a fraud it disproportionately affects high achieving people who find it difficult to accept their accomplishments many data scientists question whether they are deserving of accolades their job recognition or the like you do not have enough time to learn something you want to learn you look around and see that there are other people that know that thing you don’t have time to learn you feel incompetent why do so many data scientists have it data science is an extremely broad field of study there are core competencies required to have a successful career in data science but there is also a lot of industry specific and technical knowledge that is ever changing data science is a career which has many job options all of which require a high level of expertise and knowledge if the broad seemingly confused data science job postings show us anything it is that many companies do not really understand what a data scientist is how they compare to a data engineer or software engineer and how to train or support them within an organization to add to this the labor market for data scientists in predominantly new graduated or early career professionals when challenge is high and expectations are unknown it encourages people to fall into high arousal anxiety and worry you can see this from psychologist’s mihaly csikszentmihalyi flow model these feelings are compounded by a lack of support feedback and mentorship provided within a company this is not generally intentional but a product of small data science departments business executives licking their wounds from years of poor data quality and technical deficit and increasing demand for better data driven outcomes how can data scientists deal with imposter syndrome according to the american psychology association if you recognize yourself in the description of the impostor phenomenon take heart there are ways to overcome the belief that you don’t measure up in a nutshell there are three ideas that you need to get in your head in order to get over imposter syndrome you are a generally competent person there are always going to be people that know more about a certain area of data science than you and that’s ok and expected even more importantly you’re not the smartest person in the planet so if you look hard enough you’re going to find people that are better than you at everything you do and that’s ok you have a finite amount of time to learn things and your goal shouldn’t be to learn the most but to learn the things that maximize your specific goals – generally this is going to be career advancement but for some it may be something else when the imposter syndrome feeling comes up 1 remind yourself that you are a competent person – if you weren’t you wouldn’t have gotten to the position you are in right now whether that’s graduating from college or leading a data science team yes even ds team leaders catch the ‘drome from time to time 2 remind yourself that when you look for people who know more than you about a specific area you are guaranteed to find them – that’s just how it works people choose to specialize in certain areas and if you only focus on that area of expertise you are going to feel inadequate but even more importantly recognize that if you run into someone who is better than you at literally everything you do that doesn’t diminish your value – it just means you have run into someone that is pretty special 3 get back to prioritizing what to learn do you need to learn that or do you just want to learn it to feel better about yourself if the latter learn to let it go and focus on the things you need to learn – and save the things you want to learn for when you have the time which will come u dfphd – phd head of data science ecommerce youtube what is imposter syndrome and how can you combat it talk to your mentors “the thing that made so much of a difference was supportive encouraging supervision” many have benefited from sharing their feelings with a mentor who helped them recognize that their impostor feelings are both normal and irrational though many will often struggle with these feelings you must be able to recognize personal or professional progress and growth instead of comparing myself to other students and professionals recognize your expertise don’t just look to those who are more experienced more popular or more successful for help tutoring or working with younger students for instance can help you realize how far you’ve come and how much knowledge you have to impart this can be a great way for a data scientist to give back to the industry as well as set a more realistic benchmark of your perceived value remember what you do well psychologists suzanne imes phd and pauline rose clance phd in the 1970s impostor phenomenon occurs among high achievers who are unable to internalize and accept their success imes encourages her clients to make a realistic assessment of their abilities “most high achievers are pretty smart people and many really smart people wish they were geniuses but most of us aren’t ” she says “we have areas where we’re quite smart and areas where we’re not so smart ” she suggests writing down the things you’re truly good at and the areas that might need work that can help you recognize where you’re doing well and where there’s legitimate room for improvement realize no one is perfect clance urges people with impostor feelings to stop focusing on perfection “do a task ‘well enough ” she says it’s also important to take time to appreciate the fruits of your hard work “develop and implement rewards for success — learn to celebrate ” she adds change your thinking “let the challenge excite you rather than overwhelm you ” people with impostor feelings must reframe the way they think about their achievements says imes she helps her clients gradually chip away at the superstitious thinking that fuels the impostor cycle that has best done incrementally she says for instance rather than spending 10 hours on an assignment you might cut yourself off at eight or you may let a friend read a draft that you haven’t yet perfectly polished “superstitions need to be changed very gradually because they are so strong ” she says avoid all or nothing thinking just like a standard distribution most data scientists fall within the center if you find yourself comparing to outliers then you are going to continue to feel like a fraud which will in return stifle your career in data science youtube how you can use imposter syndrome to your benefit mike cannon brookes talk to someone who can help for many people with impostor feelings individual therapy can be extremely helpful a psychologist or other therapist can give you tools to help you break the cycle of impostor thinking the impostor phenomenon is still an experience that tends to fly under the radar often the people affected by impostor feelings don’t realize they could be living some other way they don’t have any idea it’s possible not to feel so anxious and fearful all the time
1,choosing 9 squares in a row from a 5x5 grid of squares without choosing the square with a bomb in it what is the probability of this this is basically the game mines from stake casino dont gamble and i was wondering what the probability of getting 9 squares in a row is vs the probability of choosing the one bomb 1 25 i m not sure how to approach this i think it has something to do with combinatorics and the binomial coefficient formula but i m not sure how to apply it anyway let me rephrase it if it isn t clear there is a grid with 25 squares only one of them has a bomb the rest are gems you need to choose 9 gems in a row without choosing the square that has the bomb what is the probability of this
2,reinforcement learning for inverse kinematics i am trying to explore this area for my research need help with understanding problems shortfall with current implementations also has anyone used the learned model on the actual robot what were your learnings
1,question does a correlation between two percentages constitute statistically significant enough evidence to be used cited in a scientific context let s say 5 of people have x property and 5 2 of people have y property when combined with other evidence indicating a link between posession of the two properties would the similarity in percentages be valid enough to cite in a scientific research context disclaimer i know almost nothing about statistics i am just really curious and couldn t find the answer with a google search
2,generating discrete encodings for music i posted about this a few weeks ago but since then i have made some progress and thought i might share and get some feedback the ultimate goal of my project is to generate discrete encodings that compress sound music in the time domain using these discrete encodings i would like to make another model to generate new music in the same way an rnn can be use to generate text since these encodings are compressed in time the rnn can learn music longer sequences than if i used raw audio the training data i am using is about 30 hours of random music mashups i downloaded from youtube i resampled it to 22050 hz and converted it to mono channel the input for the network is the raw audio quantized to 256 values via mu law encoding the network consists of three components encoder the job of the encoder is to compress the audio down 64x i would like to go larger but so far this is the most i can do i start by running the raw waveform through two strided convolutions to compress the time axis 64x and then i run it through a highway like network of dilated convolutions i say highway like since i did change gate convolution to look at the value it is gating instead of the inputs def create encoder input audio keras input none dtype int32 x layers embedding 256 16 input audio x layers conv1d 64 8 strides 8 use bias false x x layers batchnormalization x x layers leakyrelu 0 01 x x layers conv1d 256 8 strides 8 use bias false x x layers batchnormalization x x layers leakyrelu 0 01 x x layers conv1d 512 3 padding same use bias false x x layers batchnormalization x x layers leakyrelu 0 01 x for d in 1 2 4 8 1 2 4 8 f layers conv1d 512 3 padding same dilation rate d use bias false x f layers batchnormalization f f layers leakyrelu 0 01 f g layers conv1d 512 1 activation sigmoid bias initializer keras initializers constant 2 f x g f 1 g x x layers conv1d 512 3 padding same use bias false x x layers batchnormalization x x layers leakyrelu 0 01 x x layers conv1d 8 1 x return keras model inputs input audio outputs x name encoder the output of this encoder is a vector of length 8 for each 64 chunk of input audio however i wanted one discrete integer value per 64 samples not 8 this currently would only compress it 8x with a continuous encoding what i need is for each of those 8 floating number to represent bits in an 8 bit integer i actually used 1 and 1 instead of 1 and 0 to do this i use the sign function as the activation this function is normally not differentiable however i have found that i can apply the sign function on the forward pass but use the derivative of tanh on the backwards pass tf custom gradient def sign with gradients x def grad dy return dy 1 tf square tf tanh x return tf where x 0 0 1 0 1 0 grad i apply this activation in my custom training function between the encoder and the expander layer i also add some activity regularization to the outputs of the encoder before applying the sign function not sure if this is necessary but i wanted to keep the value from getting too far from zero e encoder r training training reg loss 0 0001 tf reduce mean tf square e e model sign with gradients e e expander e training training expander the purpose of the expander is to take the discrete encodings and expand it back out 64x to the size of the original audio the output of the expander is then used to condition the decoder to recreate the sound the architecture of the expander mostly mirrors that of the encoder i have debated whether it makes sense to make the expander a larger model than the encoder but most auto encoder architectures i have seen seem to be symmetric def create expander input data keras input none 8 x layers conv1d 512 3 padding same use bias false input data x layers batchnormalization x x layers leakyrelu 0 01 x for d in 1 2 4 8 1 2 4 8 f layers conv1d 512 3 padding same dilation rate d use bias false x f layers batchnormalization f f layers leakyrelu 0 01 f g layers conv1d 512 1 activation sigmoid bias initializer keras initializers constant 2 f x g f 1 g x x layers conv1d 512 3 padding same use bias false x x layers batchnormalization x x layers leakyrelu 0 01 x x upsample x 64 return keras model inputs input data outputs x name expander decoder the decoder takes the output of the expander and the prior audio sample of the encoded sequence as inputs and tries to predict the next sample currently i am using an rnn architecture but may try something like a wavenet i kept the decoder simple since i want it to use the input from the expander more than what it has learned from the prior sequence currently i am having an issue with it inserting other sounds into the output on top of the original music need to find a way to remove these artifacts i have found smaller models produce less of this def create rnn decoder stateful false batch size none prior audio input keras input none batch size batch size dtype int32 expander input keras input none 512 batch size batch size x layers embedding 256 16 prior audio input x cat x expander input x layers gru 512 stateful stateful return sequences true x x layers conv1d 512 1 use bias false x x layers batchnormalization x x layers leakyrelu 0 01 x x layers conv1d 256 1 x return keras model inputs prior audio input expander input outputs x name decoder i have been training this on sequences of length 512 which are randomly sampled from the output of the expander which is currently 2 14 in length i need to do this since i don t have the time or memory to train on the full length outputs currently i am using sequences of length 2 14 input to the encoder which compresses down to just 256 when run through the dilated convolutions if i made this much small enough that i don t need to sample the dilated convolutions would be encountering padding on both sides in all cases which would not generalize well to when i use it on longer sequences i trained the network overnight and it seems to still be improving on the out of sample data so i will leave it running for now i did generate an output sample you can listen to the underlying song can be clearly heard i just need to figure out how to get rid of some of the annoying artifacts if you have any suggestions please let me know previously i was using a gumbel softmax in a similar way to to how i used the hard value on the forwards pass but a soft value on the backwards pass i found this to be less than ideal since i prefer my encoder to be deterministic when i tried not applying gumbel noise the encoder would only ever activate a small portion of the nodes in the softmax in generating the audio in the link i checked the values it was using it used 237 256 possible combinations of bits each of the 8 bits also got activated close enough to 50 50 that i am happy with it so please let me know what you think and if you have any suggestions x200b update the validation data stopped improving around iteration 240 000 this is the final output of the above clip
1,is a two way anova test suitable for my thesis analysis hello for my thesis i will be comparing the yearly income of type a companies between the years of 2010 2020 to the yearly income of type b companies between the same years i want to see if these yearly incomes between the two types of companies i have 50 companies for each category have progressed in the same way over the same 10 year period my roommate suggested i do a two way anova test doing some research though i am very much confused if this is the best way to go any suggestions thanks a lot
0,why are data scientists in the sports industry underpaid i’m currently a undergrad sophomore whose very much into data science i myself have no idea as to what industry i want to go into but i have thought of sports analytics as being one of them moneyball was ultimately the first movie that go me into data science and after getting deeper into the languages statistics ml i have noticed i do enjoy centering my data science projects around sports analytics i have considered sports analytics as a possible route and dream of being a data scientist for a professional team nba nfl but i have read that data science in these industries are heavily underpaid compared to others from what i’ve read on this sub and others that sports analytics is fascinating but the work u do is underpaid and you are better off doing it as a side hobby than as an actual job industries like tech and finance pay more for data scientists compared to sports my question is why why do data scientists in sports get underpaid if there are currently any people out there who are data scientists on sports teams or have worked in the past what are your experiences and do you agree with these claims what advice do you have for a student whose interested in the field should i pursue it or am i better off going to a tech finance related industry
0,weekly entering transitioning thread 20 jun 2021 27 jun 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
2,graph embeddings of wikidata items i m trying to use pytorch biggraph pre trained embeddings of wikidata items for disambiguation the problem is that the results i am getting by using dot or cosine similarity are not great for example the similarity between the python programming language and the snake with the same name is greater than between python and django does anybody know if there is a wikidata embedding that results in better similarities x200b wiki item 1 wiki item 2 dot cosine q28865 python language q271218 python snake 17 625 0 64013671875 q28865 python language q10811 reptiles 8 21875 0 300048828125 q28865 python language q2407 c 25 296875 0 919921875 q28865 python language q842014 django python 11 34375 0 409912109375 q271218 python snake q10811 reptiles 11 25 0 409912109375 q271218 python snake q2407 c 12 5390625 0 4599609375 q271218 python snake q842014 django python 6 05859375 0 219970703125 q10811 reptils q2407 c 4 76171875 0 1700439453125 q10811 reptils q842014 django python 0 60009765625 0 0200042724609375 q2407 c q842014 django python 11 53125 0 419921875
1,causal language do you agree with this this is a list of words that authors suggest should be avoided if the study is observational i would like to know your take on this i m not sure if words like contribute or determinant point to causality
0,should proprietary clinical ml algorithms be subject to external review some recent events have spurred discussion around if or how clinical ml tools should be regulated some suggest fda regulation others peer review the vendors claim to be transparent but just with their customers what are yall s thoughts on the matter
0,does anyone do event tracking at their job does anyone work in event tracking and designing them i was wondering if any other data scientists work on eventing designing schemas and qa ing the events gtm google analytics snowplow etc and how much of the job is doing said task
0,will palantir replace data science teams i ve been reading a lot about palantir lately and how they are creating software that analyzes data and creates models for you so naturally i ve been concerned that this new technology will eventually replace data analysts scientists i know right now they are only working with the government but from what i ve read they hope to eventually move into the private sector as well anyone have any thoughts on this and how this might affect the future of data science jobs i m worried the job will be made redundant by this tech and the more i read the more worried i get it just makes me sad tbh
1,comparing linear regression models using dummy variables for my research i ve explored the effects of predictors on green space visitation before and during the covid 19 pandemic the result is two linear regression models over two periods of time for the same population group i was wondering how to compare these two models and have found online that an alternative to a chow test is to run the model with dummy variables i know what dummy variables are but am confused how this method would work in practice is anyone familiar with this method any help is appreciated
2,semantic similarity between programming languages and math terminology i ve wondered how good neural nets can get at predicting semantic similarity at a more abstract level between different topics for example finding context between math terminology like vector and matrix and programming terminology like list array and 2d array also finding context between common data types functions concepts between different languages and frameworks libraries ideally i d want a binary output that takes two strings as input to compare question is what would be the approach to a problem like this would it need carefully labelled data translating the terminology between the two topics or is a self supervised method at all possible i ve only recently got into data so i could be way off here on what is possible and what is not
2,10 popular keyword extraction techniques in nlp this blog lists out all popular unsupervised keyword extraction algorithms in nlp here i summarize almost 10 papers w r t all these techniques enjoy the read 🎉
0,is data science turning into a catch all title for recent ph d grads it seems theres a massive influx of recent phd grads in various fields but especially stem that couldn t cut it in academia or research and claim to be experts in data science but dont necessarily have any qualifications or background in analytics phd in stem congrats you re a data scientist dont worry you dont have to know anything as long as you read blogs on data science central and took an intro stats course you can bullshit your way through the job and nobody will question you because you have a phd right wonder what implications of this fad will be long term for job market
2,just discovered a new 3blue1brown styled quality ml youtube channel i m reading jax s documentation today and in there was a link to a quite accessible videos to get a deeper sense of automatic differentiation and it s actually very good what is automatic differentiation the video style is 3blue1brown inspired explains the topic from bottom up very accessible though not shy away from maths i see that the channel is still relatively small but already got some great videos on normalising flow and transformer if you like those too please go there and subscribe to encourage the authors to create more high quality contents
2,automated explainable feature generation for customer modelling analytics my company integrate ai is currently testing an alpha product free to participate and test it out which expands datasets to enable a new kind of customer insights analytics generation and ml modelling this is done through a combination of reusable ml pipelines and model ensemble methods to generate synthetic data from our network of datasets for almost all downstream tasks an example would be providing a propensity if a user is likely to shop at a discount store or determining if a user is likely to be the top 1 spender to be used as axis in cluster analysis segmentation or look alike modeling the current focus is on transactional data sets time series transactional data and joining them to your data sets through non pii indicators so if you have datasets that generally fit this shape there is a good chance the features we’re adding could help your use case the data augmentation is done in a privacy safe way to protect the underlying data in these features done through data anonymization and differentially private methods if you’re interested please dm me or drop any questions in the thread
2,facebook to launch nethack competition at neurips 2021 ai facebook has been interested in nethack for some time now for a simpler explanation from facebook and now they re announced this year as part of a neurips 2021 competition we are proud to launch the nethack challenge—the most accessible grand challenge for ai research—with our partner and co organizer aicrowd
1,updated article on data types in r i posted an article on the data types in r yesterday and it s been edited after all the suggestions that i received check it out here hope this helps any beginners i ve verified what i ve written from multiple resources before posting it i can link those resources in case anyone is interested to know more hopefully it gives you clarity on vs and na null nan if you found the article useful give it a 👏 thanks
0,how to be taken seriously during a job interview when you don t have a stem degree nb this is not a rant post i swear i want to be proactive i m writing here to ask some advice on how to tackle my next interview processes i have a problem about this x200b some context quickly i am already a professional data scientist with almost 3 years of experience in a large company i have a phd from a social science department my main field of study has been application of statistical models i spent four years studying mostly statistics and econometrics and doing estimations my final thesis was completely statistical in nature before that i received good basics in cs i don t want to sound arrogant but i think i m good at my job i have a good understanding of math calculus statistics and algorithms my colleagues with a background in stem told me i m good at deep learning i am the reference guy in my company for the use of tensorflow x200b here s the problem i like my current job but i don t have faith in the future of my company i have seen countless potentially cool projects being supervised by corporate idiots that do nothing but speaking corporate jargon that know nothing outside marketing i m sick of this and i want to leave however every time i apply for a new job i feel that i m not taken seriously because of my social science academic background i can see how recruiters changed attitude when they found i come from a social science department they believe i got there by mistake this is so frustrating what can i do about this how should i approach recruiters and companies when i apply for a new job x200b thank you people love this sub edit to make myself more clear and give you an idea of why i wrote this post i have just received an email literally 1 minute ago by a company i applied for they had cool dl projects young data savvy team both interviews went great we all liked each other now they just told me listen we liked you very much but our company s policy is that no people with a social science background can be hired for this role they literally told me that i hope you will now better understand the reason for this post instead of calling my lack of humility again it s not a rant partially now but rather tell me what to do to attenuate bypass this problem
1,omitting the outlier from z score calculation i have a question about the philosophy of outlier detection say i have some numerical data and i know before hand that the data points can be thought of as having a good or bad quality this quality is of course unknown to us and we want to know the bad data points further we can assume that the good data is all normally distributed usual strategy calculate z scores i e observation mean std and then look for things outside 3 sigma say my concern is that this calculation of mean and std includes the bad data itself shouldn t i be calculating the z score of each data point while excluding that data point from the mean and stddev calculation taking this a bit further shouldn t i then look at all pairs of potential outliers exclude them then calculate their z scores then all triples of outliers and so on is there a standard name for such a thing or am i thinking about this incorrectly and i should simply follow the standard z score calculation p s i am a stats beginner so please keep your answers simple and or point me to a reference if that makes more sense
2,suggestions for a model to determine vaccine priority hi everyone i am working on a proposal to create a model and to guide the ministry of health to determine priorities for covid vaccination i am from nepal and we have a second wave and not enough vaccines based on pcr test data covid spread demography location and age please suggest me what can i do to make this more effective
1,i want to do a cluster anlysis on sets of numbers while taking into account fluctuations within those sets is there something more advanced than using variability i have sets of numbers the sets will be clustered not the numbers if i have for example four sets 4 4 4 4 7 3 4 8 6 2 2 6 6 6 5 5 the real sets are much larger in size and number and range 1 10 with one decimal now likely the first and third set would be put together and the second and last set this is based off their means they are the same for 1 and 3 and 2 and 4 however and this is my question i d also like to take into account the fluctuations that occur within the sets in addition to the means in that case sets 2 and 3 will be put close together in the analysis since they both have the same drop from the first to the second number and the same rise from the third to fourth number making a u shape and their means are somewhat close to each other taking something like variablity to do this works somewhat but different trends in the sets of numbers could lead to the same variability for example another set 2 6 6 2 would still be in the same cluster as 6 2 2 6 this is because their means and variability are the same even though the sets are the exact opposite of each other so by using variablity there is too much loss of information a lot more interesting interpretation of the results could be done if similar fluctuations at similar points in the dataset are grouped together so is there any way to do this
0,it s crazy how effective it s to include data scientist in your job listing example at the company i work for they had been trying to hire a analyst for quite some time it was originally called technical analyst and the response was lukewarm 20 25 applicants and some even withdrew their applications underway then hr renamed the job to data scientist included that in the tittle of the listing and slapped on some buzzwords on the new tools we use result almost 300 applications the shortlist included people with experience from big name tech and banking companies prestigious schools etc
2,what is the appropriate reward function for maximizing the distance travelled with a limited amount of resources if my agent is like a drone trying to go the farthest with a limited amount of battery are there readings paper or reward function that suits this i only saw a reward of maximum possible distance minus the distance travelled are there any ways to engineer this reward function
0,weekly entering transitioning thread 23 may 2021 30 may 2021 welcome to this week s entering transitioning thread this thread is for any questions about getting started studying or transitioning into the data science field topics include learning resources e g books tutorials videos traditional education e g schools degrees electives alternative education e g online courses bootcamps job search questions e g resumes applying career prospects elementary questions e g where to start what next while you wait for answers from the community check out the faq and resources resources pages on our wiki you can also search for answers in past weekly threads
0,text analytics lda topic creation any success stories i ve been doing some text analytics work and am trying out lda latent direchlet analysis as a way of categorizing text into different topics i ve used it for two sets of survey data one from local post offices and one from medical education and some police report data i m unwhelmed by the results for the data sets i d done topic modeling on by hand and have a fair amount of domain knowledge i ve played with varying parameters but had a fairly clear idea going in what a reasonable number of categories would be the category clusters use common words but i can t think what the clusters are supposed to represent even when i go back and look at the quotes that best match those categories for example with the post office one you d think damaged stuff late mail missing items or like my mail guy would come out but i can t go from the word clusters suggested by lda to any of these obvious categories in the medical stuff one concept that gets mentioned a lot is broken arm syndrome and i find that broken and arm appear in different clusters there s nothing else in this data set related to broken or arm separately i m wondering if text size is the problem while the corpus size is at least 10k s of words and 1m in some sets the individual quotes are almost all 100 words with some only a few could that be the problem those of you who have some experiences with real data sets and lda could you confirm or deny whether you ve been successful i m best in r but i ve done some work in python and sas viya also and done lda in all of them before getting into real data i ran through some online exercises in r and python for sorting chapters of classic books you know where you get 20 000 leagues under the sea and nemo submarine and so forth pop out and pride and prejudice where elizabeth and proposal pop out and so on
0,has anyone worked with structural equation modeling or statistical process control i recently came across these topics they look very interesting but also very complicated has anyone ever dealt with them before what kind of projects did you use them for
0,finding freelancing opportunities i m pretty happy with my full time job as a senior model developer but i do a lot of analytics on model results as well including the steady paycheck and job security it comes with but sometimes i don t get to do the most interesting work so i think about getting into part time freelancing i had one person in my network pitch me an app idea for his small business i had a conversation with their ceo he seemed super interested pitched me some requirements answered some questions i then roughed out the specific work required how much time it would take and used 50 hour as my baseline pay needed even though that s below what my regular job pays normally i d need more for working beyond my full time job because it would require sacrificing my social life but the project was extremely interesting and could have led to a good relationship and more work down the road but when he saw my estimate that it d take 200 hours to complete aka 10k he said my estimates all made sense but thanks but no thanks the fact that he didn t even attempt to counter makes me think he was expecting to pay like 1k for it even though the guy who pitched the idea to me said that for 10k they d recoup costs in a few months because they paid contractors to manually do the work my app would have automated plus they lose money on mistakes the contractors regularly make x200b so my question is what s the market like for work like this are people actually able to build custom data driven apps where they re paid 5 figures for completed projects or is my experience more typical where there s just a sticker shock what kind of clients are paying this how do you get your foot in the door would a good github or personal page with personal project examples be valuable
1,discussion predicting euro 2020 matches with bayesian statistics see my repo here basically andrew gelman has a little world cup case study for stan when a coworker approached me to fill in some predictions for the euro group stage i thought i should use that model i collected qualifying match data along with euafa rankings for each team and built a bayesian model to predict the outcomes of each game the model does well for now using prediction argmax p x i ve succesfully predicted 5 7 games i predicted denmark to beat finland but a tragic event likely affected the players on that team and so i chalk that model error up to unpredictability with numbers i m averaging a log loss of about 0 8 random guessing of team a wins v team b wins v draw would result in average log loss of 1 1 when i fit the model to euro 2016 data it has an auc to predict the winner of approx 0 72 this may not be impressive to a seasoned football fan but for a guy who passively follows international matches its been a huge improvement lots of time for me to screw it up i write up some thoughts about the model and some model checking in the linked repo would love to hear what other people have to say
2,open source best practices for ethical and responsible machine learning the foundation for best practices in machine learning released the first versions of their organisation and technical best practices the best practices are free and open source the best practices are designed to be easily accessible to anyone working on or interested in machine learning they can be used as a way to get started with implementing ethical and responsible machine learning in products a common language between data scientists engineers managers and governance compliance professionals a repository for your responsible machine learning questions a point of reference for your machine learning audits policies governance and regulations they are also open source and they are looking for contributors through their wiki portal find the best practices on their website or find them on linkedin
2,how to tune the number of food boxes deliveries to optimize sales hi guys i d like to ask the community for a problem i am trying to solve at the workplace we have a client that owns a series of restaurants over the territory every week these restaurants receive food deliveries and our client keeps track of how much food is given to them vs how much it is sold the idea is simple he uses an in house algorithm that tunes the number of items a specific restaurant should receive for the week so that the number of unsold items doesn t hopefully go under a dangerous threshold if the restaurant consistently sells most of the food it receives the algorithm flags it as consistent and keeps on sending a similar number of food pieces every week otherwise the number of deliveries is reduced to attempt to balance the number of unsold items this has worked quite well for the client but he has asked me and my team to improve this process he has sent us data from a single restaurant that holds data from the beginning of 2021 the columns are the following restaurant id date type delivered sold quantity food description do you have any ideas on how this problem can be tackled i was thinking of linear regression and to use the number of sold items as my x to predict the y which would be the number of items to deliver i still don t know if this makes sense so i am asking around for some ideas thank you
1,question what statistical test should i use i have recorded the abundance of an invertebrate species in 10 quadrats for both two separate streams i am interested in seeing if there is a difference between their abundance between the streams what would be a good test for this data thank you in advance
0,need pointers on where to start on how to model this data that is essentially arrival times or arrival delays the situation is like this we receive deliveries of data electronically and let s say they are expected to arrive at 8 am daily but historically they can arrive at say 5 10 am so it s periodic but not really the data is gathered only hourly at the top of the hour the data we re gathering now is basically time since last arrival which is zero when something has arrived in the last hour and ticks up hour by hour until the next delivery what i ve been doing so far is fitting this with a sawtooth wave and whenever a delivery is late the time since last goes above the threshold and alerts us i researched time series data and such but it s not really what i m looking for here s the issue we have dozens of different ones of these per customer and hundreds of customers and my fitting works well most of the time but there are still a large number of false alerts also some of these are 7 days a week others are m f others are more than once per day all very different kinds of sawtooths and a lot of manual adjusting what i ve started doing is just collecting a histogram of when the arrival delay is zero vs hour of the day this gives me just a set of times i did this for one example of a reasonably well behaved data set and got a distribution that was rather skewed but fittable is this a poisson process i have enough history to look back 6 weeks this gives only 6 data points for e g a tuesday but for 7 day per week systems there is no problem aggregating all the days and looking just at the hour of the day 42 total data points but knowing when to combine 7 days vs 5 2 split etc is a manual decision i would prefer it it could fit each arrival event separately i m actually looking at the data for 6 weeks plotted over 1 week timeframe of 168 hours per week so i see the scatter plots overlayed and can guess at the amount of variation when i do these so for example 8 am tuesday would be at t 56 hour and constructing the threshold sawtooth function on that axis what i am looking for perhaps is a way to give say based on the real history give me the time of day that if the packet hasn t arrived by there s a high chance that it s late and set an alarm since the history can and does contain some that were late or absent i know those should factor in with less weight i don t know what kind of distribution these should fit or how to output a cutoff value for a given chance it s out of compliance and the reason i m trying is so i can script this to analyze hundreds of such data sets and set thresholds automatically any suggestions where to look online article videos i would be interested in suggestions for textbooks as well thanks
1,least absolute deviations to a power and robust regression i ve been thinking a little bit about least squares and how one method of getting a robust regression alternative is to use least absolute deviations one of the things that seems to be potentially problematic is that there could be multiple solutions with the example on wikipedia given as such linked from would a fairly simple solution to this be to use a near 1 power that is instead of minimizing absolute residuals you could instead minimize absolute residuals 1 01 or 1 1 or 1 0001 or some other value less than 2 when you get to 2 of course you re at least squares and no longer reducing the effect of outliers so i m thinking of values between 1 and 2 but most likely near 1 is this common and i m just not aware of what this is called would this be a reasonable approach to robust regression while getting unique solutions vs the lad approach
1,multicollinearity issue with fixed effects regression and dummy variable hi im doing a fixed effects regression on a panel data with n∗t 870n∗t 870 observations where n 290 and t 3 the entities are districts observed over a total of 3 time periods i want to interact my main independent variable with a dummy variable where the dummy variable takes the value 1 if a large district and 0 o w will this cause issues with multicollinearity since the entity fixed effects are essentially dummies for each entity ive posted the question with more details here ive posted the question here panel data multicollinearity with fixed effects regression and dummy variable cross validated stackexchange com x200b thanks
0,what can you do for fun with databricks hey i just got an amazing opportunity as a junior data engineer first job starting monday although my specialisation was as a data analyst i m pretty average with big data pyspark and sql i m a fan of kaggle competitions with python and machine learning but i can t find interesting projects to upgrade my cv and practice with sql and pyspark apparently i ll be working mainly on cloud databricks any ideas or suggestions of fun projects or books
2,99 2 wrong but my maths ai brainchild is definitely learning my ai brainchild hi i was inspired by the kind and useful feedback from this group on my first post about using deep learning to mimic how young children learn to add here is my second post thoughts and feedback would be very much appreciated again x200b
2,machine learning workflows to summarize translate transcribe and more x200b what if we want to extract and summarize text from documents also handle translation combine the outputs and load it into an embeddings index enter workflows the demo above takes a list of github project pages extracts text from html summarizes the text and builds a similarity search index this same concept could be applied towards a list of company pages wikipedia pages and more this is just one example of what txtai workflows can do txtai workflows are a simple yet powerful construct that takes a callable and returns elements workflows are streaming by nature and work on data in batches allowing large volumes of data to be processed efficiently the amount of functionality provided by machine learning models continues to grow rapidly txtai provides an easy way to interface with these models the following is a non comprehensive list questions extractive question answering using a text context labels apply labels to text using a zero shot classification model summary abstractive text summarization text extraction extract text from documents transcription transcribe audio to text translation machine translation workflows allows joining these models together to create powerful data transformations workflows can also be constructed in javascript go rust and java via the api see the following links for more information github workflow builder documentation article
2,google proposes efficient and modular implicit differentiation for optimization problems a research team from google research combines the benefits of implicit differentiation and autodiff and proposes a unified efficient and modular approach for implicit differentiation of optimization problems here is a quick read google proposes efficient and modular implicit differentiation for optimization problems the paper efficient and modular implicit differentiation is on arxiv
1,mlr categorical interaction term question i am trying to understand when to add interaction terms in a mlr model i reduced my model using pairwise selection and was left with 8 categorical variables to predict monthlycharge using a telecommunication churn data set the residual vs fitted normal q q and residuals leverage plots do not appear to support that the model fits the data well i added an interaction term between all the variables v1 v2 v8 and the residual vs fitted normal q q and residuals vs leverage plots look great the predicted vs actual monthlycharge is also nearly perfect i m having trouble understanding how to interpret this term though i originally added this term just to experiment with possible iv inputs not expecting such positive results is there a systematic way to calculate which terms should be added by additional interaction terms between categorical variables when i examine the model summary in r it looks like this interaction term uses every possible combination with 2 or more of the variables not just all of them interaction together i assume there must be something wrong with adding an interaction term for all variables just looking to understand why
1,does if a minimal sufficient statistic exists then any complete statistic is also a minimal sufficient statistic mean to say any complete sufficient statistic instead p42 in lehmann and casella s tpe says since complete sufﬁcient statistics are particularly effective in reducing the data it is not surprising that a complete sufﬁcient statistic is always minimal proofs are given in lehmann and scheff´e 1950 bahadur 1957 and schervish 1995 see also problem 6 29 p289 in casella and berger s statistical inference says theorem 6 2 28 if a minimal sufficient statistic exists then any complete statistic is also a minimal sufficient statistic does the theorem in the second book mean to say any complete sufficient statistic is also a minimal sufficient statistic instead thanks
0,review statistical rethinking by richard mcelreath i noticed a lot of folks recommending this book so i followed the crowd and got a copy it s oriented toward researchers in natural and social sciences so i wrote up my thoughts about the book from the perspective of an industry data scientist
1,is survival analysis an appropriate application if i want to study likelihood of community college students graduating in a particular quarter what i want to do determine the likelihood that someone graduates with an associates degree after a particular number of quarters in community college what is the likelihood they graduate in their 8th quarter at school what is the probability that someone graduate any time before their 8th quarter data i have demographic information and graduation quarter of each student going back 20 years what i imagine this looking like correct me if i m wrong 1 correcting for censored data such as drop outs and type 1 censoring 2 reference distribution plots to find the appropriate family 3 survival analysis summary table and plot am i going about this correctly thank you for your help
2,deploy dl on ecs for multiple requests i am trying to run docker containers on ecs the containers have dl models with flask with 5 10 requests i am getting response within my acceptable timeframe but if there are 100s of requests aws i taking minutes to scale new instances and meanwhile those requests are being forwarded to the already running tasks due to this tasks are failing bcoz of memory error how do i scale ecs correctly so that every request is processed where to route requests untill new tasks are started
0,online machine learning or how to automatically update your model in production i m trying to find resources to learn more about a new subfield in machine learning called online learning the idea is beautiful and powerful your model in production trains itself with new latest data to react to changes faster however the classic ways to build the mlops infrastructure and algorithms maths won t do the job here so i m eager to learn more i ve found this post by standford s ml lecturer chip huyen to be a great introduction to the concept of online learning i ve found river to be a promising python library for online learning apart from that i don t know many resources out there do you any blogs to follow any titanic equivalents a simple problem to get going
1,has anyone heard of extreme value theory let s start with an example problem case say we measure two variables that are non normally distributed and correlated for example we look at various rivers and for every river we look at the maximum level of that river over a certain time period in addition we also count how many months each river caused flooding for the probability distribution of the maximum level of the river we can look to extreme value theory which tells us that maximums are gumbel distributed how many times flooding occured will be modeled according to a beta distribution which just tells us the probability of flooding to occur as a function of how many times flooding vs non flooding occured source does anyone know why the maximums are gumble distributed shouldn t their distribution depend on the data itself does anyone know why the numher of floods are modelled by a beta distribution again shouldn t their distribution depend on the data itself e g maybe normally distributed thanks
2,eli5 explanation of meijer g functions hi i am trying to understand following paper by michaela van der schaar about symbolic meta models although i am struggling with understanding meijer g functions can anyone try to write eli5 explanation for them thank you in advance
1,a simple and concise introduction into the relationship between bias variance overfitting generalisation in machine learning models i wrote an article where i explain as simply as i can the essence of the bias vs variance trade off that plagues every machine learning model i then go on to link this to overfitting under fitting and generalisation using clear visual aids i think it s a decent introduction to the concepts so hope it helps someone
1,principal component efa and max likelihood cfa is there any particular methodological issue with doing a principal component analysis with a varimax rotation to tease out variables from a scale and then to confirm that model with a maximum likelihood confirmatory factor analysis versus say using maximum likelihood from start to finish
0,startup went bankrupt need career advice good afternoon i am facing a challenging situation on my career now and i would like some advice i will provide a very brief summary of my experience first of all i am based in portugal europe i have a a mechanical engineering degree focused on energy systems with a bunch of optimisation ml and numerical simulations mixed in including the master thesis as well as years in extra curricular activities revolving around python and ml projects i have 2 years and a couple of months experience in data science in the following roles 1 data science consultant for 1y and 2m role included developing a genetic algorithm time series analysis with lstm prophet and arima as well as a big data project on databricks and sql i started feeling like i wanted to be part of internal data science team so that i could really own my projects and that is why i left for company 2 2 data scientist for 7m in an energy related company me and another person were brought in to start a data science team this company was not a technological company and there was not much to do really we were supposed to get data to start working on some use cases but after 7 months we did not have any data nor any indication that we would have in the future during my time in there i explored outlier detection methods like isolation forests and lof did a local web page using flask javascript and bootstrap and also worked on some data cleaning and process automation pipelines i had an offer via a recruiter that contacted me on linkedin that i felt would make my job much more meaningful and at the time it was very hard for me to justify not taking it so i took it 3 7m fintech startup this was pretty much what i wanted in a data science role build a model using h20 ai for client grading and well as lot of other analysis for managing client and portfolio risk i felt like my job truly mattered and i was extremely satisfied with it and super excited for all the possibilities that i would have but due to a very unexpected issue with financing we ended up closing after 7m on the role more and more i have been craving time to truly dedicate myself to learn and develop some personal projects mostly related to crypto and financial markets my question is can i afford to have a gap on my cv assuming i fill it with personal projects or do i need to start looking for jobs immediately also since i left university i wanted to start an international career and move to germany sweden switzerland austria uk i have been learning german for 2 years would this be a good time to truly try to move do i even have a chance if i just send cv s from portugal i am wiling to reallocate and get a job on any of this countries before i move or would i really have to move there before i start trying to get a job thanks all
0,any one who re already worked as ds in dark industry can like hub share the experience i was offered some freelance ds projects from a company x for video watcher analytics however i m not sure if i would agreed on the project as there might be psychological issue and also some ethical for future jobss anyone here can share some advices
1,let x normal 0 4 is 4 the standard deviation the variance or god forbid the precision likewise we summarize random variable y as 3±0 4 does this interval represent the mean ± 1 standard deviation a standard error of the mean some z confidence interval nobody can say certainly not the text edit and by text i mean as it exists in some applied context not in a textbook or methods paper what other cases of notational ambiguity do you often see how often do you reckon the above described cases map to each of the options presented or perhaps to other options
2,a model theoretic view of storytelling and what it means for creative nlg evaluation hi everyone i’m a storytelling researcher based out of georgia tech some of you probably can already guess my advisor lmao and i’ve been working away at a formal theory of narratives from the perspective of category theory and model theory for the last bit over the last six months or so i’ve written a few papers on objective measures of story coherence with grounding in information and model theory that might be of interest to people here who work tangential to nlu nlg and evaluation the project is still in its infancy we’re only three papers in so far and we make assumptions like the narrator being a perfect realization of the author’s intent as well as transmission of information between the narrator and reader being lossless but those will soon be remedied i’ve shared prior approaches like this one here before mostly based around tda methods but model theory provides a much more elegant way to discuss nlu nlg evaluation methods the introduction post is here arxiv links it links to two of the three papers the third is still in review but it should be up soon the third is the evaluation methods applied to language models rather than artificially constructed datasets willing to answer questions in the comments edit i had a lot of fun answering questions if you want to stay updated i’ll be posting updates and memes my meme game is strong on my twitter
1,spatial statistics vs network models if an undergraduate student in applied statistics had to take a course in either spatial statistics or in network models which do you think is the most useful in industry research grad school etc obviously both are useful but is there one that stands out on your experience
2,modern machine learning models for time series analysis does anyone know what are the most modern statistical models being used for time series analysis i have heard of transformer and attention mechanisms models that are used for modelling sequential data but these seem to be more relevant for modelling data from the nlp domain when it comes to classical time series modelling e g a vector of temperature measurements does anyone know what are some of the more modern models being used for this i did some searching online it seems like arima style models were some of the first ones followed by state space models hidden markov and the more recent ones being rnn and lstm are lstm and rnn the most modern models that are being used for classical time series problems thanks
1,career opportunities after masters in statistics i have recently completed my bachelor’s of science degree in economics mathematics and statistics from india i enjoy statistics and it s application quite a lot i am planning to pursue my masters degree in statistics in the us is it a good country to pursue statistics what are the career options after my masters i do not have any work experience will that be a problem how can i improve and build my resume during my masters to become more employable any suggestions
2,paper explained endless loops detecting and animating periodic patterns in still images the animation is generated from a single still image have you ever taken a still photo and later realized how cool it would have been to take a video instead the authors of the endless loops paper got you covered they propose a novel method that creates seamless animated loops from single images the algorithm is able to detect periodic structures in the input images that it uses to predict a motion field for the region and finally smoothly warps the image to produce a continuous animation loop read the full explanation in the casual gan papers blog to find out about detecting repetitions in images predicting the motion field and generating seamless animation loops from flow vectors full explanation post arxiv project page more recent popular computer vision paper explanations comodgan
1,how did distribution tables come about hey guys i am confused as the two stat undergrad books i have read do not go into the history of the tables i mean how did w s gosset calculate the t distribution table also is there a geometric or algebraic interpretation of a distribution table please
1,has anyone here studied association rule mining beyond the classic grocery store example has anyone ever looked into more advanced applications of association rules mining association rules can also be used for prediction classification purposes e g this allows you to obtain a fully interpertable algorithm that can provides a set of conditions for making predictions however these rules are usually not very powerful does anyone know if there are more recent spinoffs of this algorithm perhaps where a neural network or ensemble models can be used to learn these rules or in general does anyone know any machine learning based algorithms that provide rules thanks
1,i need help with spss this is the task b create a new variable grade by using the following rule grade is 5 if ratio ≥ 0 9 4 if 0 8≤ratio 0 9 3 if 0 7≤ratio 0 8 2if0 6≤ratio 0 7 1 if 0 5≤ratio 0 6 and 0 if ratio 0 5 can you tell me step by step how can i apply that to spss
1,what sort of distribution will help me with my problem not sure if links are allowed but i was playing this game warning its about matching the crime to mugshot and was wondering what kind of distribution will allow me to calculate the probability of getting a certain number 0 5 guesses correct its kind of like a binomial distribution but without replacement i guess any help will be greatly appreciated
1,media mix models where retail sales numbers are not available and have only distributor sales numbers has anyone any experience with these situations where as a product owner you want to use media mix models to understand impact of various media on sales consumer demand however retail sales numbers may not be available only distributor sales or warehouse stock data is available for a time period week
1,this may be a noob question but how do i interpret the confint function how can i see if the conf int contains 0 this is what comes out of the confint function
2,how to use svm to sort vehicle can bus data i have seen several examples of python scripts which make use of svm to sort different types of data using the scikit learn library methods i would like to know if there is a good way to incorporate a stream of can messages from a vehicle and use the svm algorithm to detect can intrusion through malicious messages i will most likely be tracking the types of message headers received and classifying attack messages and normal messages based on that and i would also like to measure the degree of change in certain parts of the messages over time to see if there are any sudden irregularities in the stream of input of seemingly normal messages that could indicate an attack does anyone know a good approach to implementing something like this most code examples i’ve seen either use attributes that are too simplistic or techniques that seem very cryptic to understand i’m an electrical engineering undergrad by the way if that is any help
2,the difference between a blurring matrix and a psf in image reconstruction i m working on a research project related to deblurring images and i don t fully understand the difference between a blurring matrix and a psf i understand that to apply them to an image you use two different operators but why is that what is the difference between them is a psf not a matrix how are they related to each other do they affect the image the same way
1,is there a name for using the difference between separate regression models for control and treatment to estimate causal effects this seems like a fairly intuitive approach to causal inference but i have never run across it and i am wondering why assume you have two regression models y1 e y a 1 x and y0 e y a 0 x where y1 is fit only on the treatment group and y0 is fit on the control group to get the average treatment effect you could imagine computing y1 y0 for everyone in the whole population and averaging this would be very similar to a standard regression except that the coefficients for x could be different between the two is this a technique that exists and has a name already
1,confidence interval question hi i am an economy grad who is trying to get around this confidence interval problem i am self taught so i need someone to tell me if what i did is correct thank you sorry if my explenation is kinda messy i lack the english terminology to express the problem properly a sample of 20 people watches a certain tv show with an average time of 180 minutes and a deviance equal to 150 find the confidence interval for the average watching time of the population with a 1 error i have the deviance which means i have to calculate the variance equal to dev n 1 150 19 7 89 that means the sample x follows a normal distribution with e x 180 and var x 7 89 the exercise wants an interval which implies an 1 error the formlula is x z a 2 sqrt v x n x z a 2 sqrt v x n given that a 2 is 0 005 i have to find on the table the z value which corresponds to 0 995 which is 2 58 so according to the formula the interval is 179 638 180 362 is that correct thanks
0,making the transition from small data to big data in the past all the work i have done has involved small data what i mean by that the data i have used to perform statistical modelling and machine learning models this data was carved out from a larger data source using sql and is able to fit into a very large excel document multiple spreadsheets from here i use r studio to import all these spreadsheets into r and concatenate the spreadsheets together i e stack them on top of each other so in the end the final dataset i am using for my analysis is about 30 columns and 500 000 rows from here the analysis can take some time but i usually put some coffee on and step away from the computer a family laptop from costco no gpu while the numbers crunch however i am starting to realize that this approach will become less efficient and eventually stop working for larger data sources i am aware that there is a whole world out there that is dedicated to dealing with and solving these problems a world that i am not very familiar with this world uses words like hadoop spark aws the cloud parallelizing apache containerize chunkize etc as i see it i feel that the problem can be viewed from different prespectives 1 performing machine learning and statistical analysis on large data might not be possible because of limitations in the individual computer you are using this means these procedures on this large data might not work on my computer but it might work on my friend s more powerful computer who lives down the street 2 performing machine learning and statistical analysis on large data might not be possible because of limitations in the software e g python r you are using i am not very knowledgeable about what happens behind the curtains of the software but my understanding is that software you are using regardless of how powerful your computer is might have certain limitations related to memory and ram 3 both 1 and 2 with this being said i want to start exploring different ways to address these limitations based on my limited understanding of these topics i think there are two main ways to address these limitations a with money apparently you or your company can buy cloud services such as aws which will allow you to perform machine learning statistical analysis on large data using remote servers i was told that this does not require a lot of knowledge or extra work after purchasing these services only a slight amount of extra code is required and then you can effectively perform machine learning algorithms on big data b with less money this is where my understanding stops apparently tools like hadoop can divide the computing costs between several computers and reduce the required time or there is something called chunkize which allows you to sequentially feed your data into the algorithm without maxing out your computer how can i learn more about this suppose i have the same dataset with 30 columns but this time there are 100 million rows i want to use the random forest algorithm for a binary classification task how have people on r datascience approached this kind of problem in the past thanks
0,is it ok to have a coefficient greater than 1 hello i m new and working on a logistic regression model i have two variables that are what i would consider abnormally large at 10 2 and 30 7 i read online that it s usually a bad sign if this is the case and that this may signal multicollinearity however the vif factor on these is 1 2 for both under the 5 0 that i ve read signals multicollinearity is it ok to leave these in should i remove them thanks
2,alias free gan abstract we observe that despite their hierarchical convolutional nature the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner this manifests itself as e g detail appearing to be glued to image coordinates instead of the surfaces of depicted objects we trace the root cause to careless signal processing that causes aliasing in the generator network interpreting all signals in the network as continuous we derive generally applicable small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process the resulting networks match the fid of stylegan2 but differ dramatically in their internal representations and they are fully equivariant to translation and rotation even at subpixel scales our results pave the way for generative models better suited for video and animation paper
0,is there diversity in machine learning engineer backgrounds like there is with ds i’m a statistics student in school and my goal is to become a “data scientist” in quotations because that can mean many different things anyways i’ve heard that data scientists tend to have very diverse backgrounds depending on the industry ie statistics computer science math civil engineering or some other engineering physics ie list goes on and on even saw someone who was a geospatial data scientist with a background in something gis related point is it seems like the “data scientist” has very diverse backgrounds for the role however for something like “machine learning engineer” is this as diverse i mean yea there is some machine learning involved so some statistics but 90 i’ve heard is swe related so most of the backgrounds are generally computer science am i right in saying that for people who are non computer science is there a higher barrier to entry to become a mle than it is to be a data scientist can non cs backgrounds still be considered for mle positions
2,how do we use probability in data science and machine learning i have been studying data science and machine learning from past few months from various online sources i have built few projects also by using some github file as reference but i failed to understand the explicit use of probabilitiy in it can anyone help me understand this with example or provide some good source to learn this thank you
2,over fitting in iterative pruning in global unstructured and iterative pruning algorithms such as 1 learning both weights and connections for efficient neural networks by han et al 2 deep compression by han et al 3 the lottery ticket hypothesis by frankle et al except the lottery ticket hypothesis where the weights are rewind ed to their original values and resulting sub network is trained from scratch thereby needed more time epoch since the usual algorithm is take a trained neural network and repeat steps 1 and 2 1 prune globally smallest magnitude p of weights 2 re train fine tune pruned neural network to recover from pruning usually the number of pruning rounds needed to go from original and unpruned network sparsity 0 to 99 sparsity requires 25 34 rounds depending on the exact architecture and number of trainable parameters in my experiments i have observed that during this repeated prune and repeat algorithm the resulting pruned neural networks start to overfit to the training dataset which is to be expected apart from using techniques such as regularization dropout data augmentation learning rate scheduler etc are there any other techniques to prevent this overfit i assume that such a resulting pruned sub network when used for real world tasks might not perform as expected due to the overfitting induced due to the iterative process correct me if i am wrong you can refer to my previous experiments here and here thanks
2,european ai regulation hi machine learners of reddit the european commission has recently proposed a new regulation for ai based products that will affect already regulated as well as newly regulated markets the eu ai regulation will prohibit a small number of unacceptable risk ais and define a set of requirements for high risk ais many of the groundbreaking innovations in machine learning will be considered high risk and thus be affected by this new regulation if you or your company is developing ai centered software and you are interested to learn about the implications of the upcoming european ai regulation check out our upcoming free webinar looking forward to seeing you there
2,mlp mixer implementation in flax pytorch video hey there i made a video where i implement the mlp mixer in both flax and pytorch among other things i try to discuss in what way it is similar to cnns also if you have never used flax before the video contains a quick tutorial on the most important concepts x200b original paper my video
0,is going from a data science job to people analytics corporate human resources in the same company a demotion
0,what does it mean when changing the seed changes the predictability of a model i have a glm model that outputs a percentage of likelihood in predicting a binary outcome variable whenever i change the seed the prediction value for all elements is different every time i ran some loops with hundreds of different seeds and saved the results and the majority are in the same ball park but some are way off you could say if you get many different results it’s a bad model but if that’s the case i don’t get how some seeds are super predictive the sample size is too large to be considered random chance any guidance here would be appreciated
0,can someone explain use cases for apache spark so i vaguely understand what spark does at a high level right now i m working as a data analyst all i do is write sql queries conduct exploratory python analysis build tools etc i don t really do heavy analysis ml etc if my company transitioned to spark what would be the impact for me i would have faster processing times for my sql and python scripts access to real time data rather than waiting on etls
0,populating e commerce tables with products and variants hey folks i m in the process of building a fashion aggregation website as a portfolio piece to obtain the data for this site i have multiple scrapers for many of the largest fashion retailers in north america to describe the data one product can have many variants e g summer dress that comes in 3 colours and 8 sizes as such this product would have 24 variant products the generic being the parent product each variant may have different pricing availability sizing colour etc my question concerns table construction my scrapers output every variant of every product with their associated data which can be in csv or pandas dataframe format can any of you tell me how i would go about populating two database tables from this data 1 table with the parent product with foreign keys to each of its variants 2 all variant products of a parent product linked to that product such that on my eventual website they can all be found through searching the foreign keys of the parent product i hope that makes sense in a bit of a jam here i m not sure what to google even if you can t provide an answer any points on even what to research would be of great help tldr how do i populate 2 database tables with one csv file one table for the generic product and one for all of its variant s fields
0,what is the best data analytics product that you have ever used what is the product or tool that analysed or summarised your data in a way that provided you with a memorable user experience eg very useful pleasant easy to use why was that i am interested in products that analysed your data for you such as google analytics for data of your website apple health for data about your sleep apple screen time for data about your device usage etc i am not interested in products or tools that allowed you to analyse your data like ms excel
1,promax varimax rotation w ml fa is there a minimum threshold of item correlation at which i should use promax rotation i ve been reading every piece of literature i can find on this topic today and i still don t have a citeable justification either way i m doing a confirmatory factor analysis using max likelihood and a couple of others on data that is moderately correlated 50 ish inter item
1,question evaluating critically appraising diagnostic tests without a reference i may have the opportunity to consult on a project involving a new first to market diagnostic assay what makes this different than projects i ve typically worked on is that with this i don t believe there is a gold standard method with which to compare performance most of the time i am comparing to some reference through equivalence tests anovas and the like so first question in the absence of a gold standard what statistical measures should i be considering for evaluating and critiquing the performance of this new diagnostic assay sensitivity and specificity come to mind but what are some others second question i know i m going to be asked what constitutes an adequate sample size for our experiments and similar to above i typically calculate sample size using an effect size related to a reference i e how small a change do we want to detect using the new method but in the absence of a reference standard what am i comparing it to i feel like only a subject matter expert in the field of the assay s target would know anything about a proper effect size very much looking forward to seeing what the community thinks
0,when you ve created a finalised dataframe do any of you convert it into an excel document to help you visualise your data or am i being inefficient in doing this
2,why is batch norm becoming so unpopular i read a few papers recently that stress that the architecture is batch norm free and know that there are recent advancements by deepmind and with the vision transformers that do not need it why is it so advantageous not to have batch norm the only thing i think i read is that calibration of nn output gets better when not using batch norm
1,trying to remember the name of a strategy for reducing the overall error of multiple estimates for the life of me i can t remember name of it but there s a strategy that allows some metric of improved accuracy lowered error when making multiple estimates like some kind of regularization technique whereas intuition would suggest that squared error is minimized by using the mean of each variable there is a strategy that lowers the overall squared error by slightly adjusting each estimate even when the variables and estimations are otherwise independent does anybody remember the name of this technique edit it s the james stein estimator i was looking for
1,how to deal with reports of mother s and fathers statistically hi everyone i have reports of teens on how they percieve their mothers and fathers what i am finding in the correlation matrix is that the mother and father variables generally display the same relationship with variables and are correlated with one another by about 45 to 68 would it be possible to combine reports on mother s and fathers into a single parent variable if so what would be the best practice to do so in spss i have heard of people combining mother and father variables and taking the mean of the two together as an option is that a good practice or a better alternative
1,question formula for calculating character life probability in a video game hello i play a video game called tierras del sur its a small 2d indie game with an active comunity of around 1000 concurrent players i usually do some small math on my head but i dont know how to calculate probability for the value of characters and i need some help so the game works like this a character levels up from 1 50 levels 1 25 are quite easy and get progressively harder with the maximum player right now being level 40 from 40 50 it gets incredibly slow and will take years before someone reaches 50 characters have life and mana both increase with each level but mana increases on a fixed value while life increases based on an average depending on the class mage is squishy so higher average paladin is tanky so higher average the averages are for example a human mage 6 5 so each time you level up you roll the dice for a life increase of 5 6 7 or 8 this value determines the worth of your character by being higher or lower than your expected average for each level this can go down to a defined cap of 10 and up indefinetly so theoretically you could become a level 50 mage with 75 points above your expected life if you rolled an 8 on each level example you create a character it has a base life of 15 every time it levels up that life can increase by for example in mage 5 6 7 or 8 all rolls are equal 1 4 chance so say your character levels twice and both times it rolls a 5 it will have a life of 25 but the expected life is 28 since the average of rolls is 6 5 that character would be 3 if it rolled 8 both times it would be 3 if it rolled a 6 and a 7 it would be an average character most valuable characters hover between 10 and 20 with some exceptional charactes reaching 25 above level 40 the way people go about this is create a character up to level 15 which takes about 2 3 hours and if the values are above 8 they will continue with a character my question is to evaluate the value of time to character worth what formula i could use to calculate the probability of a character above 10 at a selected level say i wanted my mage to be 15 at level 15 how many characters i would need to create on average and if it would be more worh it for me for example to create a character to level 7 and try to roll 4 perfect 1 4 dices sorry for bad english
0,i am self learning data science i asked this question on every platform i can think of but still didn t get an answer please help me out if you know the answer should i remove features such as gender and birth month before drawing the heatmap because they are categorical i am working on a dataset that has both categorical and numerical continuous and discrete features 26 columns 30244 rows target is categorical 1 2 3 and i am performing eda on this dataset my dataset is regarding hotel reservation status not cancelled 1 cancelled 2 no show 3 of customers in the span of 3 years 2015 2016 2017 given data of the customer my task is to predict if the customer will either cancel not cancel or no show for his reservation the categorical features with numerical values ex gender has values 0 and 1 are also considered when taking the heatmap with seaborn as per my knowledge the heatmap is drawn to check the correlation between continuous numerical features right correct me if i am wrong should i remove such features before taking the heatmap the book in date expected check in date expected check out date are given in the dataset i extracted month and year for each feature separately these month columns are also categorical right as they only have values between 1 12 i took screenshots of month distribution plots and uploaded them here should i do a test like the chi square test on those features
0,how do i think of data science projects i d really like to learn more about data science through practice and maybe build a portfolio but i m really uncreative and can t think of any projects to try does anyone have a suggestion for how to approach this or project ideas for someone with intermediate python experience maybe something w a machine learning component
0,who here uses windows as their primary work os i come from cs but i have a friend who is in a data science masters program at a no name school and the program seems to be allergic to unix systems all their classes are taught assuming windows is the only os anyone is using and thus my friend never bothered to learn any unix i ve told he should be learning at least how to work around a bash terminal for when he needs to ssh into a server but i am wondering if that is all he d need to worry about coming from cs i told him he should just drop windows all together and install linux but i began wondering how many data scientists actually use or need linux proficiency since i know many need to use microsoft tools like excel so it is actually important for data scientists to be proficient on a unix system
1,would need some help to define a reasonable threshold for biological samples hi i just established a new experiment type in our lab and got the first data from a pilot study because this is a new experiment there is no existing standard data analysis workflow in the next paragraphs i will describe the experimental approach as well as the possible threshold formulars that our lab is using and why i feel they are suboptimal so lets start with some details for my experiment i have patients n 15 suffering from nosocomial infections caused by different common viruses virus 1 virus 2 virus 3 and a healthy control cohort n 15 i measured different protein values protein a protein b protein c for patients and controls our hypothesis is that virus 1 causes an increase in the concentration of protain a virus 2 in protein b etc however these viruses are extremely common and can also cause asymptomatic infections so it is somewhat likely that also in the control group some protein concentration could be mildly increased but in the patient the increase should be way higher i wanted to use the control group to set a threshold for every single protein a c as healthy background in the next step i would check which patients have higher protein concentrations and thereby predict the infecting virus since we also have the clinical data i would be able to compare my protein based prediction with the clinical diagnosis this made a lot of sense to me an my colleagues but when we saw the real measured data the definition of the threshold gave us a lot of headache and since we are no statistical experts our tools are rather limited 1 mean 3xsdnote way to prone to outliers as in prot c controls you can see there is one outlier that would screw the mean based approach a lot in addition we can expect a right skewed distribution for the data 2 median 3xiqrnote looks already way better but there are still a lot of data points in the controls that would be included and are thereby false positive i would favor to have rather false negative cases not able to predict the causing virus than false positive predict someone healthy as infected 3 trimmed mean 3x trimmed sdnote somewhat similar to 2 however the guy who gave us the advise to use the trimmed mean also said just go for some nice even number like 10 or 25 trimming what sounds really arbitrary to me but if this is the way to go what amount of trimming would you recommand overall i noticed that i get relatively good results with simply using 100 as threshold unless the control thesholds using 2 or 3 are higher in this case i would go for the calculated thresold but this is totally arbitrary and extremely unsatisfying it would be really great if someone could come up with a smart reliable and easy idea for the threshold calculations why easy first no one in our lab is able to write and run fancy r scripts and second our pi is hard unable to convince to publish complex statistics because they might be mathematically more correct but not the usually way in our field and yes statistics in biosciences are hmm blunt but maybe one of you has the solution i am looking for x200b example dataset prot a ctrl prot a patient prot b ctrl prot b patient prot c ctrl prot c patient 5 7 32 223 1 63 20 1 238 1 96 1 1 4 1 51 8 339 1 4 15 7 1 3 3 1 420 13 13 19 2 1 1 23 1 1 1 147 1 30 1 8 6 323 242 1 1755 1 33 20 71 78 9 25 3 4 7 59 1 84 1 147 11 99 8 71 2 25 3 45 4 8 3 1 2 542 4 14 1 59 4 196 4 415 4 4 3 14 1 7 x200b ps i did run a pca but since the original data set is way bigger and more complex there was no reasonable result
2,deep learning vs wide learning today my friend was telling me about something called the universal approximation theorem which apparently contains the mathematical foundation behind neural networks work after watching a lot of youtube videos on this topic i think i have some understanding on this topic supposedly a 2 layer neural network can approximate any function and the error of this approximation is proportional to the number of neurons within the neural network the more neurons there are the better the approximation will be the only problem with this is that a 2 layer neural network will likely require a very large number of neurons to produce a decent quality approximation apparently so many neurons that the computations will become very inefficient conceptually you can imagine a 2 layer neural network as being very wide looking supposedly the solution to the above problem is to create neural networks that are deep instead of wide so instead of a 2 layer neural network with a very large number of neurons it s said that a neural network with fewer neurons and more layers is more computationally effective and these deep neural networks also have the universal approximation property the question i have if wide neural networks require a very large number of neurons to adequately approximate a function why don t deep neural networks require a very large number of layers to approximate a function at a similar level of accuracy what explains this mismatch between the number of layer and number of neurons equivalence suppose a wide neural network with a relu activation function 2 layers and 1000 neurons is able to approximate some function with an error of 0 05 clearly we believe that a deep neural network also with a relu activation function with 1000 layers with each layer having 2 neurons is not necessary instead we believe that the number of layers does not need to be so deep what explains this mismatch why are deep neural networks typically able to maintain a relatively low number of neurons and layers compared to wide neural networks where only the number of layers can be kept small
0,steps to answer the question customer understanding i ve assigned a task to analyze customer understanding here s what available in sale data id age invoice value country cluster already segmented lifetime value from model promo tier from model purchased product and revenue the steps i have in my mind are to do 1 basic analytics break down into country age group or generation and cluster like number of invoices number of client and revenue 2 rfm analysis to identify characteristic of top clients then again break down the analytics of those top clients into country age and each cluster 3 product view identify which products are often purchased for client base and for top clients this can be done by checking word frequency in product description then again break down product view into into country age and each cluster 4 plot a time series of revenue number active clients by month to see they respond multiple line chart series for age country and cluster so 3 subplots 5 predicting their sales next month and their next purchase days this is optional imo 6 also an important step is how to transfer those insights into more making money action would love to hear your opinion on this and anyone have further recommendation many thanks
1,how to create your own statistical question and how decide if you need to use a interval or test
1,can you calculate effects size from prevalence when doing a meta analysis i m new to using stata16 trying to run a meta analysis comparing prevalence data from 21 observational studies is this possible
0,how to deploy a real time model which gets the variables features in phases for example i have a linear regression model ready i dont get all the features at the start for example i get 6 out of 10 features i need to see the target variable range i will get x200b also if i need to deploy it in such a way that it tells me what should be the values of other features provided some features and the desired target variable range
0,data logging there is not much available on internet about data logging any resources or explanations from anyone which can sum it up
1,when to use t vs z in confidence intervals ap stats teacher said the following question would be graded wrong if we wrote z instead of t why is this i always write z question activity trackers are electronic devices that people wear to record physical activity researchers want to estimate the mean number of steps taken on a typical workday for people working in new york city who were such trackers a random sample of 61 people working in new york city who wear an activity tracker was selected the number of steps taken on a typical workday for each person in the sample was recorded the mean was 9797 steps and the standard deviation was 2313 steps a construct and interpret a 99 percent confidence interval for the mean number of steps taken for a typical workday for all people in new york city who wear an activity tracker answer x bar ± t s √n
1,how does one compute the log probability of arbitrary models i m wondering whether there is a method of computing the log probability of observing a value from an arbitrary model apologies in advance i don t have a good background in statistics but rather a background in probabilistic programming and programming languages the wording i m using and questions i m asking may therefore be silly to elaborate it s relatively easy to compute the log probability of observing a given value from a primitive distribution in probabilistic programming languages there exist functions for doing so for example normal 0 0 1 0 pdf x in more complex hierarchical models we may sample from many distributions and the output data of our model will not result from a primitive distribution for example where we sample from two normal distributions and add these samples together def model x normal 0 0 1 0 y normal 3 0 1 0 return x y in the situation where we want to condition some data against the result of this model is it possible to compute the log probability of this somehow
2,bert experiment variance when training bert variants or probably even neural networks in general the performance varies a lot depending on random initialization the extent of this ofc depends alot on the architecture size of dataset training procedure etc but in the end there will often be a significant variance especially when comparing on benchmarks where you are like 1 2 better than the competition now in academic papers i took it for granted that the authors perform multiple runs with the same configuration and report mean and std recently i have seem many papers that don t do so or dont mention it even from faang one example is the recent fnet paper 1 do they actually perform multiple runs and just report the mean without mentioning it if so why not include the std as well 2 if they dont perform multiple runs how can one rely on the results i mean i can prove my great innovation by just running experiments until i get a lucky initialization even if it is complete trash 3 what are best practices for this how many runs per configuration should one perform to be confident about the result is the std of the performance interesting as well
2,heuristics for initializing gp length scale hyperparameters i have been training a sparse gaussian process using the matern 5 2 kernel and i am having trouble getting the objective function to converge and i think it has to do with my initialization of length scale hyperparameters i am not training on actual function observations but instead on summations of several function observations and derivatives of the function—due to the particular application currently i am initializing each length scales as the std dev over all training data inputs for the corresponding feature but it doesn’t seem to be working well does anyone know of other heuristics
2,facebook ai mila propose alma anytime learning at macroscale a research team from facebook ai research and mila mcgill university explores deep learning model accuracy versus time trade offs in anytime learning which they term anytime learning at macroscale alma the team evaluates various models to gain insights on how to strike different trade offs between accuracy and time to obtain a good learner here is a quick read facebook ai mila propose alma anytime learning at macroscale the paper on anytime learning at macroscale is on arxiv
0,how many hours of actual work do you do everyday hi i was just wondering if i was on the low side of number of hours people work a day i talked to a friend who works at amazon and they said that they do 8 hours of work by work i mean when you re sitting on your desk and doing stuff not including the meetings although i understand meetings are also part of work i realized i do maybe 4 hours of actual work rest is just thinking about some stuff for work lunch break etc it s hard to imagine how can someone just sit and do 8 hours won t they be burnt out how many hours do you put in thanks
2,what is it like being a phd student in a big research group hello i was wondering what it is like doing a phd in a big research group such as sergey levine pieter abbeel etc these groups have a lot of students so do these students regularly meet with their advisor does their advisor look and discuss at a low level about their research progress or are they mainly collaborating with other students in the group and benefiting from that i am just confused about what makes such students really prolific in terms of research output and what kinds of resources make these students stand out
0,in your experience what were the questions the interviewer asked that made you realise you shouldn t work at this company or under the interviewer or do you have any questions to ask that help you decide whether the company interviewer is good
2,what are the current ways to compress time series data into a feature i m working in taking time series data over a span of 4 months and compressing it into a single feature as an input into another model i know there are things like lstm and gru but i don t know if the memory cell is large enough to hold a good latent representation i was considering vae but i think converting tabular data to an image is probably tricky
0,grappling with the social impact of data related careers i’ve been working in this field at a consulting firm for 2 years now and a question that always rests heavy on my mind is whether i should be applying my skills to a sector that has more direct social impact the need for data analytics science seems to follow the money trail as larger corporations have been able to collect massive amounts of customer data and thus are willing to pay top dollar to help make sense of it there are definitely options to do data science in a more impactful way e g non profits environmental data life sciences data but i have the overall impression that these fields pay less more importantly it can be frustrating to do ds for due to the lack of work done previously leading to messy and lots of unclean scattered data in other words because the majority of people take the higher paying jobs over the ones with higher social impact the high social impact data roles and ecosystems remain underdeveloped my question is how do you grapple with this reality do you grind at a corporate job and donate part of your income do you teach on the side answer a lot of stack overflow questions invest it in your children’s future some pro bono ds work for non profits in your free time or just ignore all of it because life is short and you worked incredibly hard to get to where you’re at
1,churn probability with ordinal logistic regression hello everyone x200b i am trying to create a questionnaire to evaluate satisfaction with which i would like to estimate the probability of the customer s churn i know the classic models like ols logit mnl probit etc since y is a probability i ll use the logit model the problem is there are so many questions 17 ordinals with 5 choices each 7 categorical 1 continuous and the sample will be very small 200 consumers maximum when i studied these models at university there were no problems with the sample size and i don t know what to do now should i reduce the number of questions on the questionnaire for example two questions like 1 was the package large enough 5 4 3 2 was the color of the packaging to your liking 5 4 3 i could replace them with 1 was the packaging to your liking yes no x200b or is there a way to aggregate the answers for each question category packaging shipping after sales etc perhaps taking the mode of each category x200b ps english is not my first language i hope my question is understandable x200b thanks so much
1,euro 2020 predictions update last week i posted some predictions for the 2020 euro now that the first round is over we can examine some of my performance my predictions and results for the first round are shown in this table sorry it isn t prettier i achieve an average log loss of 0 92 where assigning all outcomes as equiprobable yields an average loss of 1 1 my multiclass roc for predicting the outcome is 0 77 in short in the first 12 games i perform slightly better than random guessing which is honestly fine for me however most people who have watched international football wouldn t assign all match events as equally likely is italy drawing turkey really as probable as italy losing to turkey no its hard for me to measure against a reasonable guesser my work pool records all our guesses and so at the end of the group stage i can use that as a sort of ensemble method to compare against we ll see here are match predictions for the remaining group stage games conditioned on the results of the first games the model is not perfect and still makes some weird predictions for example portugal is given higher probability to beat france than they are to beat germany even though france beat germany in the first round if you subscribe to some sort of sports law of transitivity this may sound weird my predictions for the second round and the results of the first can be found here
1,how important are the statistical assumptions in this question in this question they assume that people arrive according to a poisson process 1 if you have real data e g a list of times at which each customer arrives how would you check to verify that your real data follows a poisson process 2 suppose your data does not follow a poisson process does the formula in this question still apply is there a non parametric form of this equation that does not depend on the poisson process
0,what is going on with sas and how do you use it my company is insane and despite investing in cloud technologies and building out a team of python data scientists they are silo ing us to deploy all our products in sas to facilitate collaboration with non coding engineers for context we re in the energy industry so lots of meche and ees who are competent at data analysis but don t really program i have about a thousand questions i am praying the community can help me with as you know sas puts all their products behind enterprise licensing so stack overflow is failing me and i don t have any influencers on youtube to walk me through the mechanics like there is with python r if you are one of these people know that i love you forever and always what are good resources to learn sas fundamentals i have access to hours of videos from sas that are all a series of sales pitches for how great the product is but don t actually tell you how it works how does the data architecture of sas work all our data is stored in the google cloud platform and i m expected to pipeline this into sas is this even possible how can i explain to my boss that this isn t like the myriad other products we are expected to adopt we re a growing branch of our organization so it is common for us to trial various sas software as a service products but they give me a git repo i can dig into along with a support engineer i can ticket for elaboration not the case with sas should i be updating my resume now because my company is run by morons who make python r data scientists use sas
1,is there some general relation between bayes tests and u mp tests the neyman pearson lemma in bickel and docksum s mathematical statistics i first constructs the bayes optimal test for the simple testing problem and then proves the bayes optimal test procedure is and is the only the most powerful mp test at the level of its size is that bayes test being mp only a coincidence is there some more general relation between bayes tests and u mp tests or other tests if i am correct likelihood ratio tests for general testing problems are not bayes tests thanks
1,can someone explain effect size in a practical sense hi all i feel like i understood the concept but after doing some googling i am not so sure anymore suppose i fit a linear model y a b1 x1 b2 x2 e and my null hypothesis is h0 b1 0 nbsp i had thought that my effect size in this example would be the estimated b1 but when i did some googling around this subject most explanation says the effect size for a linear regression is f2 where f2 r2 1 r2 if i am only interested in the effect of x1 wouldn t it make more sense that the effect size is b1 nbsp not sure if it matters but this question stem from a power calculation which i am trying to perform thanks
1,question how to handle wrong statements in longitudinal data set hi i have the following problem i m writing a thesis and i m using census data households get randomly selected and get questioned 5 consecutive quarters i can idendify people over the quarters and i want to observe an effect over time finding a job if certain criteria are met beforehand but now i have the problem that a lot of the cases i find are based on wrong answers in the questionnaire e g in january a person says they ve been unemployed fror 4 months and 3 months later they say they ve been unemployed for 9 months now i can identify the obviously wrong statements in my key variables but i can t simply exclude them can i excluding these people certainly influences the other people s weights so how would i handle this situation thank you in advance
1,how to find correlation between two independent variables i am very new to statistics and i am trying to do a statistical analysis on these two independent variables this might be very basic question but could someone share what could be a good way and also what online resources i could use thanks
2,oversampling for multivariate time series currently working on oversampling for time series and have looked into methods like spo structure preserving oversampling and ohit but both of them are univariate working only for time series with one feature are there any oversampling techniques for multivariate time series i am unable to find any also please suggest some papers that i can look into
0,freelance experience as a ds you may share hey there long time lurker first time poster i love this sub i learned quite a lot and i am very thankful for all the interactions about my post i work as a ds in the risk area of a bank nonetheless lately my boss has been taking away my projects and giving them to someone else also cancelling my meetings and engaging less with me i fear he is trying to make me quit management has used this tactic to force other employees to quit before i decided to look for other jobs no luck so far and i read a bunch of articles suggesting me to do freelance work my github portfolio is wanting and i am asking for any and all advice from the community what is your experience doing freelance i am setting my account on upwork atm how may i get noticed any experience you can give a n00b like me can i get a living wage from freelance much appreciated
2,revisiting deep learning models for tabular data hi we introduce our new paper revisiting deep learning models for tabular data and the rtdl package that enables easy access to the main models from the paper paper code ft transformer tl dr we show that two simple architectures can serve as strong baselines for tabular deep learning 1 a resnet like architecture and 2 ft transformer an adaptation of the transformer architecture for tabular data the problems where gradient boosting dominates should be prioritized when developing dl solutions targeted at beating gradient boosting
2,vkfft now supports discrete cosine transforms on gpu hello i am the creator of the vkfft gpu fast fourier transform library for vulkan cuda hip and opencl in the latest update i have added support for the computation of discrete cosine transforms of types ii iii and iv this is a very exciting addition to what vkfft can do as dcts are of big importance to image processing data compression and numerous scientific tasks and so far there has not been a good gpu alternative to fftw3 in this regard vkfft calculates dct ii and iii by mapping them to the real to complex fft of the same size and applying needed pre and post processing on flight without additional uploads downloads this way vkfft is able to achieve bandwidth limited calculation of dct similar to the ordinary fft dct iv was harder to implement algorithm wise it is decomposed in dct ii and dst ii sequences of half the original size these sequences are then used to perform a single complex to complex fft of half size where they are used as the real and imaginary parts of a complex number everything is done in a single upload from global memory with a very difficult pre post processing so dct iv is also bandwidth limited in vkfft dcts support fp32 and fp64 precision modes and work for multidimensional systems as well so far dcts can be computed in a single upload configuration which limits the max length to 8192 in fp32 for 64kb shared memory systems but this will be improved in the future dct i will also be implemented later on as three other types of dct are used more often and were the main target for this update hope this will be useful to the community and feel free to ask any questions about the dct implementation and vkfft in general
1,why does survival analysis tend to focus on sub populations instead of focusing on individuals does anyone know why the field of survival analysis tends to focus more on studying survival hazard and mortality rates of sub populations within medical studies e g how a certain medical treatment affects males vs females instead of individual patients many times i see how for instance the cox proportional hazards model can be used to estimate the survival and cumulative hazard functions for sub populations in a medical study and then how to use various parametric and non parametric tests to evaluate whether there are statistically significant differences between these sub populations but rarely do i see survival analysis being used to study individual patients is there a reason for this or is it just a practical thing researchers are generally more interested in extending their research to larger groups of people e g researchers are more interested in knowing if a certain drug will harm a large populations of males instead of just alex or john are survival analysis models ever used to study individual patients
0,how important was is work life balance in your mid 20 s and what did you do to maintain or destroy it hi i m 26 and work as a bi developer data analyst at a fortune 500 company my job pays well and i live comfortably but sometimes i crave a change a change of company a change of tools i use at the current job using outdated technology right now is kinda the only reason i want to switch then i think if i switch job it might be a better paying job but could be bad for my work life balance right now my work life balance is super my manager is absolutely fantastic knows his boundaries doesn t check my performance in terms of how many hours i m sitting on my desk i can stop working at 4 4 30 or 5 i won t be asked any questions i can work till 6 and i don t have to put effort in showing that my hobbies are in check to the seniors of this sub or people of my age what do you value the most in a job thanks
1,does the memoryless property actually apply to grocery store lineups i was reading about the famous memoryless property of the exponential distribution which can be used to approximate the amount of time you need to wait before being served at a grocery store for instance if you have already waited for s minutes and you want to know the probability that you will get served after waiting t more minutes probability getting served at s plus t minutes having already waiting s minutes probability getting served after waiting t minutes my question does this formula actually work in real life just using logic and subject matter knowledge can this formula really be applied to estimate waiting times at grocery stores suppose s 50 minutes and t 30 minutes it just seems unrealistic to me that the probability of getting served after waiting 80 minutes after already waiting 50 minutes can be the same as the probability of being served after waiting 30 minutes can this ever happen in real life thanks
