id,label,text
ri11hg,1,autoencoder high dimensional data data 100k features wanted try reduce autoencoder making framework inputs take way much memory way make autoencoder many features
rrjqt5,1,find intrinsic dimensionality pretrained network hello image dataset classification task using transfer learning pretrained networks pytorch want find intrinsic dimensionalities deep features however know start even know whether defined problem properly suggestion would nice
r4lty2,0,automl commercial solution joint image text classification set images title captions like joint classification task using inputs seen commercial automl solution train anyone know solution
r12jwv,1,want say non ml generated titles conceived bath ml based nonverbal commentaries nature time pyroclastic renubilation boundaries apothecary era reimagined today humans apple eye orange first tries replacing human genitals extra hair blood thorns globular ellipsoids phat tremor beef placement intriguity purple day glo revival spackle captions particle diving boards earlier mentioned pursued caught retracted marketing department lark keepsakes froze intact woke injected microdotted housefly larval brain incongruities sober rush crazy titles started let remember gcc thanksgiving nod history without statues
r7brz3,0,time crystalline study published nature journal observes new phase matter quantum processor team google research stanford university university massachusetts university california columbia university princeton university max planck institute physics complex systems university oxford uses quantum processor observe time crystal new phase matter could one significant physical discoveries decades quick read time crystalline study published nature journal observes new phase matter quantum processor paper time crystalline eigenstate order quantum processor nature website
rpjnzd,1,generalized sequence set similar sequences hello problem sequences like aaabbbccc aaabbbccd aacbbbcdcc xaabbbxccc sequences similar want create general sequence like aaabbbccdc learned sequential pattern mining prefixspan algorithm sure right track algorithm feed sequences get generalized sequence something close
r7ch51,1,true learn game free epic games played know much learn true learn machine learning game free game week epicgames com
rm4fgz,0,static analyzer detecting tensor shape errors deep neural network training code hi reddit colleagues shared static analyzer pytea detects shape errors pytorch project github arxiv pytea successfully analyze entire training evaluation path training imagenet classifiers also supports major ml libraries including torchvision numpy pil implementing basic integration vscode interactively find shape mismatch code editor please let us know need support library use
rv37yq,0,hopes machine learning 2022 hi r machinelearning wondering hoping ml accomplish overcome new year interested hearing thoughts
rjg6uz,0,paper explained resolution robust large mask inpainting fourier convolutions w author interview end video interview paper authors lama system amazing removing foreground objects images especially objects cover large part image lama specifically trained reconstruct large masked areas includes global information throughout forward propagation using fourier convolutions layers makes incredibly effective reconstructing periodic structures long range consistency compared regular convolutions x200b outline 0 00 intro 0 45 sponsor clearml 3 30 inpainting examples 5 05 live demo 6 40 locality weakness convolutions 10 30 using fourier transforms global information 12 55 model architecture overview 14 35 fourier convolution layer 21 15 loss function 24 25 mask generation algorithm 25 40 experimental results 28 25 interview authors x200b paper code online demo
r4yj1u,1,1 best value laplace smoothing hello everyone applied laplace smoothing k values lower higher 1 naive bayes classifier comparing accuracy f1 scores obvious k 1 best value smoothing wondering would grateful feedback
rc5jb0,0,optimize model hyperparameters build best synthetic model possible explore popular open source package optuna demonstrate optimize model hyperparameters build best synthetic model possible
rfum8k,0,batch normalisation batch size 1 batch normalisation shown poor performance small mini batch size large scale computer vision tasks large input data e g medical image segmenation batch size 1 sometimes required simple question still adviseable use batch normalisation batch size 1 different instance normalisation see wu 2018 group normalisation e tensorflow pytorch implementation batch norm instance norm calculate exactly metrics batch size 1
qk2bj6,0,current state art novel research support vector machines title pretty much says already interested current state art svms svrs topics researchers currently work area guess optimization still big one would also appreciate paper links posted much known svm extensions etc go nuts
rrhhkh,1,multi class classifier trained 100 binary classifiers classify 100 classes separately want add trained model one model classify 100 classes possible yes thank advance
ra4er4,1,capture words order sentence hi guys data science student trying capture words order sentence checking n triples subject predicate object order respected example given phrase rougue comedy movie given 3 triples 1 rougue movie 2 rougue movie 3 movie rogue example first triple correct task guess order achieve goal vectorize reference sentence understand capture correct order first triple turns right thanks edit dataset 90 composed simple sentences order v respected even triples extracted second check follow order first check thought using bag words tf idf check presence words contained extracted triple within reference sentence however still clear check word order within triple respected know coarse job however serves basic control skimming
rkwwja,0,blog post birth evolution image augmentation library albumentations always curious open source projects born evolve chance participate creation development open source library albumentations library probably known people train neural networks computer vision tasks sake logging wrote blog post library born evolved promoted long probably full unnecessary details wanted cover much detail possible make useful others work open source library downloaded 2 6 million times widely used machine learning competitions even adoption academia largest user library china
rgo7s6,1,recommendation affordable ml courses certificate hello guys hope great would like recommendations ml beginner courses affordable give certificates affordable mean less us 100 really looking forward andrew ng stanford ml course coursera life use platform accept credit cards costumer support whatsoever thanks much help
reva1d,0,researchers propose ‘proxyfl’ novel decentralized federated learning scheme multi institutional collaborations without sacrificing data privacy tight rules generally govern data sharing highly regulated industries like finance healthcare federated learning distributed learning system allows multi institutional collaborations decentralized data also protecting data privacy collaborator institutions disciplines unable aggregate communicate data limiting research model development progress robust accurate models would result sharing information institutions maintaining individual data privacy example healthcare industry histopathology undergone increasing digitization providing unique opportunity improve objectivity accuracy diagnostic interpretations machine learning preparation fixation staining techniques utilized preparation site among things cause significant variation digital photographs tissue specimens diversity medical data must integrated across numerous organizations hand medical data centralization involves regulatory constraints well workflow technical challenges managing distributing data histopathology image often gigapixel file often one gigabytes size latter important digital pathology paper github short summary nitish
ql2qjz,0,scientific literature review generation hello everyone developed algorithm automatically generate literature review hopefully could useful phds non phds curious understand works thankful remarks cheers
qvl3d5,0,pytorch lit infer large models even fit main memory deep learning models rapidly growing terms size complexity inference end devices becoming impossible gpt j 6b parameters example requires 24 gb ram full precision mode ready execution may impossible systems even powerful gpu like rtx 2060 6 gb memory even contain gpt j half precision mode making direct inference impossible pytorch lit solves problem running large models end devices loading parameters secondary memory needed time using disk secondary memory intend implement faster alternatives future github
rwl4xl,0,pretraining discriminator least squares gan trying train gan generate human poses 3d space using humans 3 6m dataset output gan thus 3d coordinates human joints experimenting vanilla gans output quite noisy looking least squares gan wondering good idea pretrain discriminator least squares gan since lsgans address problem vanishing gradients loss saturation
ru9xro,1,resources learning ml without python bunch frameworks problem interested learning machine learning basic understanding tutoruals blogs books found using python using bunch frameworks hate looking going scratch eaxmple gone several tutorials according way fun learning experience build neural network frm scratch trained evealuate b rather using bunch frameworks make chatbot image caption generator n lines python code good resource like thatnwhere learn ml scratch note said totally opinion think wrong totally respect
rur95m,0,anyone switched vision robotics i’m finish phd whole field robotics looks exciting right especially applications like farming recycling anyone switched pure deep learning vision nlp robotics happen get robotics related job focusing vision side things key experience robotics side getting job also i’m curious what’s best location robotics like go hong kong new york finance sf software shenzhen hardware
qwygwu,0,anyone tried deepmind haiku started learn jax sure library use making neural networks thought haiku looks quite interesting taken look sure think one hand looks quite capable fast however almost nothing documentation regards examples tutorials well issues github year old one person advised worth learning stick flax experience tf 1 2 keras scikit learn etc pytorch
rfks1b,1,use autoencoders change existing image instead create one scratch trying think use auto encoders edit existing image instead say creating new one scratch give example say train data mnist dataset give model 9 ask convert 8 would able alter pixels create 8 yes point resources thanks
rq6hvs,0,open source desktop app read display export car sensor data made app electron js framework python back end using python obd library communicate car export data neat csv file machine learning tasks made apps cost like 20 bucks customizable wanted create kind pipeline within app tested devices lot yet supported windows cars support obd ii protocol cars made 1996 read online though car video kia forte also gave practice application development paves road machine learning car sensor data hope works people also allows tinker ml demo video code instructions p give star github u like
rl12cu,0,made blockly machine learning hello everyone developed visual programming language google blockly generate run python code supports basic ml algorithms ui allows download upload xml layout github hosted webpage please star repo like think addition new feature bonus forget raise issue
rdhncs,1,deal categorical features feature selection use starting project target categorical 1 0 half features categorical categoricals nominals ordinals nominal columns given integers 1 2 3 4 using classifications trees predict target use ordinal encoder heard works tree models however dont think numbers would make much sense feature selection stick one hot encoder problem originally 17 features would end 70 features one hot encoding select features features broken pieces especially month feature would break 12 different columns type feature selection use also use logistic regression rank also numerical features scale first feature selection first using python way
qksq6q,0,generate images text clip vqgan easy follow 5 minute tutorial casual gan papers cartoon village mushroom valley trending artstation hey everyone playing gans want create something want try text image google colabs generative art seen twitter sure get started tutorial name kirill writing weekly ml paper summaries almost 9 months casualganpapers com helpful lot people already working generative modeling field realized digests interesting starting generative ai journey starting new series posts focused quickly getting started world generative art 3d ai based image editing check first post use popular clip vqgan colabs create beautiful generative art 5 10 minutes excluding training time enjoyed tutorial make sure follow casual gan papers telegram twitter get notified next post released take care kirill
r2qcer,0,kwai kuaishou eth zürich propose persia distributed training system supports deep learning based recommenders 100 trillion parameters research team kwai inc kuaishou technology eth zürich builds persia efficient distributed training system leverages novel hybrid training algorithm ensure training efficiency accuracy extremely large deep learning recommender systems 100 trillion parameters quick read kwai kuaishou eth zürich propose persia distributed training system supports deep learning based recommenders 100 trillion parameters code available project’s github paper persia open hybrid system scaling deep learning based recommenders 100 trillion parameters arxiv
r55v90,1,statistical learning theory setting hypothesis class chosen looking data strict statistical learning theory setting data scientist supervised classification consult domain experts choose features believe predictive label also choose hypothesis class aka model believe contains hypothesis low true error aka population error things mentioned done making use prior knowledge classification task question data scientist draws big enough sample collects data empirical risk minimization training would different choose hypothesis class data exploration google search regarding model use confused could thought prior knowledge cannot
qplwld,0,commercial distribution openai jukebox songs hello copyright process music created artificial intelligence made song using openai jukebox wondering commercially distribute streaming platforms spotify
rni9dt,1,download unsolved coding assignments nlp course provided deeplearning coursera youness mourri lukasz kaiser currently completed 1st course free courses coding assignments locked afford courses also checked github repos solved solutions available
qntbjh,0,arch net family neural networks built operators bridge gap computer architecture asic chips neural network model architectures key takeaways turns arch net actually building bridge translates computer architectures asic chips neural network model architectures changing existing floating point dnns hardware friendly quantized arch net structure archnet made five operators 3×3 convolutions batch normalization concatenation 2×2 max pooling fully connected layers conversion arch net much simpler without labeled data researchers employ blockwise model distillation feature maps researchers extensive experiments image classification machine translation tasks confirm arch net effective efficient fast paper github
rmy3ct,1,introduce site introduce site channel conferences artificial intelligence technologies algorithms used described full detail information latest developments
r8r8qr,1,anywhere offline running text speech works linux german male voice every single text speech find either googles shitty gtts requires internet 1 female voice german pyttsx3 works fine windows german male voice linux german voice horrible robotic sounding 1960 voice unless way get windows voices linux use pyttsx3 anything else
ric7vd,0,vit beat cnn field deep generative model recently vit beats cnn many field proving vit superior backbone network cnn papers tried use vit discriminator architecture gan results simply look good vit beat cnn field deep generative model
qq5c51,0,iclr2022 review stats crawled iclr2022 preliminary reviews help another repo uploaded crawled raw data crawled today around 2pm utc 1 also find quick stats like distribution mean scores etc best paper mean median score controversial paper std scores following notebook feel free play around ✌ excerpt data best 10 paper median score paper id title link keywords mean max min std median num ldlwbbp2mlq minibatch vs local sgd shuffling tight convergence bounds beyond local sgd minibatch sgd shuffling without replacement convex optimization stochastic optimization federated learning large scale learning distributed learning 8 8 8 0 8 3 imsjopcon0p mt3 multi task multitrack music transcription music transcription transformer multi task learning low resource learning music understanding music information retrieval 8 8 8 0 8 4 brpdx1bdzkq demodice offline imitation learning supplementary imperfect demonstrations imitation learning offline imitation learning imperfect demonstration non expert demonstration 7 33333 8 6 0 942809 8 3 sok zs6whb responsible disclosure generative models using scalable fingerprinting generative models fingerprinting responsible disclosure deep fake detection attribution 6 4 8 3 2 05913 8 5 bvvmotlmiw diva dataset derivative learning task leave one cross validation automl dataset optimization 7 8 5 1 41421 8 3 lrocyb 0st2 approximation learning deep convolutional models kernel perspective kernel methods deep learning theory convolution approximation generalization 7 5 8 6 0 866025 8 4 sict4xzn5ve happens sgd reaches zero loss mathematical framework sgd implicit bias generalization deep learning implicit regularization manifold 8 10 6 1 41421 8 4 0dlwqqlmqv nas bench suite nas evaluation surprisingly easy neural architecture search automl 7 5 8 6 0 866025 8 4 k0e f0gfdga multiberts bert reproductions robustness analysis pre trained models bert bootstrapping hypothesis testing robustness 7 33333 8 6 0 942809 8 3 fkv aszk47 learning similarity metrics volumetric simulations multiscale cnns metric learning pdes numerical simulation physical modeling 6 33333 8 3 2 35702 8 3 10 controversial papers std scores paper id title link keywords mean max min std median num p0rcmden visual hyperacuity moving sensor recurrent neural computations visual system convolutional neural networks recurrent neural networks active vision active sensing ocular drift 4 75 10 3 3 03109 3 4 fpgs276lueq palette image image diffusion models machine learning artificial intelligence computer vision 4 75 10 3 3 03109 3 4 sc6jbeviud0 white paper assistance step forward beyond shortcut learning shortcut learning bias classification imbalanced classification robustness 3 75 8 1 2 94746 3 4 7iwgzq6gz1d constructing good behavior basis transfer using generalized policy updates reinforcement learning lifelong learning transfer learning successor features 6 10 3 2 94392 5 3 jgo8cvg5s9 universal approximation constraints possible transformers constrained universal approximation probabilistic attention transformer networks geometric deep learning measurable maximum theorem non affine random projections optimal transport 7 10 3 2 94392 8 3 3ilxkq7yelm learning continuous environment fields via implicit functions continuous scene representation implicit neural networks 5 8 1 2 94392 6 3 v1mbgnbx5e mask understand evaluating importance parameters influence function interpretability model pruning feature importance ranking 4 8 1 2 94392 3 3 tq75md fqqp efficient modular implicit differentiation implicit differentiation bilevel optimization autodiff jax 6 33333 10 3 2 86744 6 3 memmmuwrxsy robust robotic control pixels using contrastive recurrent state space models contrastive learning model based rl distractions predictive coding 4 66667 8 1 2 86744 5 3 kxarp2zoqak information aware time series meta contrastive learning information aware time series meta contrastive learning 6 33333 10 3 2 86744 6 3
r0edbv,0,deviation analysis two 3d meshes hi folks looking guidance places start researching playing around metal additive manufacturing using metal polymer filament basically 3d print model using filament debind part e slowly burn plastic component raise furnace sintering temperatures remaining metal particles sinter bind together giving metal component components suffer fairly severe shrinkage debinding sintering process anywhere 10 20 globally pre scaling part get close desired dimensions individual features component shrink less expected e g long thin features shrink warp thick features holes infill affect surrounding areas presence absence supports etc etc like explore ml techniques scale model intelligently sure best way turn tractable problem though 1 print debind sinter part 2 3d scan resulting part 3 convert original model 3d scan something fed network 2d depth maps multiple angles perhaps deviation analysis two meshes first convert kind 2d representation slice scan compare slice model 4 get result iterate next version 1 reality probably print sinter multiple models time batch format help speed things sound even remotely feasible main stumbling block getting 3d data something usable sure typically handled assumed would better translate something 2d instead dealing meshes point clouds perhaps incorrect also assuming kind deep learning right approach happy investigate something else including non nn techniques happy tips ideas papers techniques etc start researching thanks
rh71mv,1,best books would recommend learn math machine learning math physics background college i’ve taken linear algebra probability calculus series already forgot looking refresh skills trying find appropriate books subjects would something like james stewart’s calculus work well know linear algebra it’s recommended gilbert strang go probability
r3a0dx,1,ai ml hard learn applied artificial intelligence machine learning course college nd im stressed coz everyone says machine learning tough difficult want know basics course motivate learn hope something u guys
qu0fvq,0,change resolution model gan hi guys want change resolution gan currently working generator discriminator consist convolutional layers conv2dtranspose conv2d know change layer get example twice big resolution tried google whole day find feasible solution really appreciate help provide code gan tensorflow version
ro9ti6,1,amd hardware deep learning related question ryzen 5 3500u radeon vega 8 heard deep learning requires gpu wanted know alternative cuda nvidia
rf2xv1,1,watered resources svm large margin classification taken linear algebra calculus feeling bit lazy looking something easier andrew ng coursera ml course learn svm anyone know resources example book really waters linear logistic regression
qyl5fc,0,asking go methodologies 2 different tasks correlated labels hi everyone assume presented typical small size dataset let call dataset2 also assume model needs learned using dataset2 generalize unknown datapoints however also larger dataset1 labels actually informative labels dataset2 mean assuming train model larger dataset1 use model acquire predictions dataset2 prediction obtained actual labels dataset2 correlate extend however tasks meaning nature labels 2 datasets different mean thing know many options boost performance task regarding dataset2 like use model gained dataset1 acquire predictions dataset2 use predictions features training smaller model dataset2 labels b fine tune dataset1 model weights dataset2 labels using standard transfer learning regime already tried mixed results c train separate model dataset2 combine predictions better generalization multi task learning combining datasets could better options 4 given scenario thinking knowledge distillation could used extend tasks even though inherently closely connected thanks advance apologies bad english
r2kiya,0,cross validation leave one early stopping doubt hi using 10 f cross validation evaluate performance model whole dataset due size limit thing leave one group use validation fold images coming source evaluate images source train test introduces bias mind using tensorflow early stopping restore best weights set true looking results getting huge improvement average performances using loo reasoning understand early stopping focuses reducing overfitting fold folds loo composed mainly one four images early stopping restore weights model able correctly classify images one image validation fold stops whenever able correctly classify image question use cross validation leave one along early stopping falsificates results find argument literature anywere
r8bbsb,1,requesting help situation hello undergraduate researcher senior year computer science engineering degree research focused mainly computer vision currently working pose estimation image segmentation object tracking working research 3 months enough machine learning background complete tasks far last 3 months made realize gaps knowledge 2 years ago completed deep learning course udacity using pytorch went basics machine learning including lot essential mathematics however apply learned awhile course bit rusty details additionally took autonomous vehicles course university included basic introductions computer vision also learned lot past 3 months basically comfortable things like utilizing existing models solve specific problem tuning models hyperparameters structure boost performance however mathematics theory behind fuzzy memory would really like rebuild understanding ground next semester required advanced work using computer vision techniques either fine tune existing animal behavioral analysis network finish building one already started likely benefit additional learning part would like use winter break little 3 weeks question best utilize time know 3 weeks ton time want efficient possible time know coursera stanford machine learning course highly recommended however concerned broad introductory case covers decent amount topics already comfortable hand good chunk information course great fill gaps knowledge would better advanced course focuses computer vision specifically even though fundamentals missing rusty thanks
r8kxdc,1,creating dataset popular ml problem good choice want build project mgr music genre recognition studying papers famous gtzan dataset mentions flaws data listened music samples dataset found repetition samples artists inferences 1 samples song 2 genres data disproportionately sampled taking samples single artist 3 100 songs given genre total 10 genres 4 super genres included planning follow musicmap major genre classification 5 one song labelled single genre micheal jackson songs labelled r b hip hop another dataset used alternative gtzan fma free music archive studied yet data size certainly larger 100000 gtzan 1000 fma also include super genres given musical piece come many different genres thinking training nn give percentage super genres present song think use dataset fma add sample genres absent data good approach
ro8oym,1,want solve andrew ng machine learning course assignments using python confused library use number libraries scikit learn pytorch pandas numpy tensorflow etc know please guide one easy beginners
rw2uac,0,style transfer noise vector hi everyone looking model perform style transfer also takes auxiliary noise vector similar stylegan generate many stylized images single input image anyone aware model meeting requirements best idea far first embed image stylegan latent space paper add noise vector
qv9rfy,0,best mobile architectures real time semantic segmentation hey guys recently working project mobile involving semantic segmentation recently i’m running quantized unet mobilenetv2 backbone deployed using tflite however fps performance good enough real time segmentation best performance wise best fps rates segmentation models run mobile currently researching architectures often evaluated gpus
r43jhv,1,deep learning python good absolute beginners ml hello everyone keep post short basically want get deep learning nlp absolutely background higher mathematics ml ai dl whatsoever intermediate python programmer basic skills programming languages would like deep learning python would goood start would better go standard machine learning andrew ng thanks much support sorry newb question
r5k4yh,0,deep learning production book hello everyone proud share first edition new book mlops machine learning infrastructure deep learning production effort aggregate best practices build train deploy scale deep learning models premise start simple jupyter notebook work way towards building fully function web application serve million users book based old articles series wrote blog big portion content already available free organized restructured articles added new material use variety examples libraries tensorflow flask uwsgi nginx docker kubernetes tensorflow extended google cloud vertex ai full code articles found github much appreciate feedback suggestions work upon second edition thank time
rnnynt,1,combining two different neural models hey guys two different neural models trained two different datasets different tasks would like adapt one using dataset one idea done read ensembling methods think applied case since datasets different help would appreciated
qxive5,0,bias criminal sentencing parole student high school presentation need make machine learning bias criminal sentencing parole information matter specific examples used possible ideas eliminate bias glad hear
ri1qvp,1,trouble scipy optimize minimize building logistic regression scratch hi trying build logistic regression model predict heart diseases kaggle dataset unfortunately struggling issue cannot find answer google code df pd read csv heart classification csv delimiter shuffling target ordered df df sample frac 1 reset index drop true data df numpy creating training validation set n data shape train nb round 0 8 x train train data train nb 1 data train nb 1 x train np concatenate np ones x train shape 0 1 x train axis 1 x val val data train nb 1 data train nb 1 features df columns initializing theta theta np random randn n def sigmoid z return 1 1 np exp z def cost function theta x x shape 0 pred sigmoid np dot x theta j 1 np sum np dot np log pred np dot 1 np log 1 pred grad 1 np dot x pred return j grad options maxiter 500 res optimize minimize cost function theta x train train jac true method tnc options options j res fun theta res x executing gives typeerror nonetype object subscriptable tried removing method options arguments using optimize minimize learnt completing andrew ng coursera course could code gradient descent best way try yet idea happening could change code unrelated comment structure welcomed thanks
rcdhkt,0,learning multiple gaits quadruped robot using hierarchical reinforcement learning hello share results learning multiple gaits quadruped robot using hierarchical reinforcement learning simply parameterized policy output considering periodic features different gaits although currently limitations hope proposed simple method could give insights researchers related fields curious methods results detail check paper slides code linked enjoy title learning multiple gaits quadruped robot using hierarchical reinforcement learning abstract growing interest learning velocity command tracking controller quadruped robot using reinforcement learning due robustness scalability however single policy trained end end usually shows single gait regardless command velocity could suboptimal solution considering existence optimal gait according velocity quadruped animals work propose hierarchical controller quadruped robot could generate multiple gaits e pace trot bound tracking velocity command controller composed two policies working central pattern generator local feedback controller trained hierarchical reinforcement learning experiment results show 1 existence optimal gait specific velocity range 2 efficiency hierarchical controller compared controller composed single policy usually shows single gait codes publicly available paper slides code contact awesomericky snu ac kr mailto awesomericky snu ac kr
rcfb33,1,requirements entry level job hi curious requirements every level job regarding machine learning originally computer sciences food sciences engineering long experience creativity problem solving 3 years r recently learned python learning machine learning data mining scraping since last month today starting apply jobs think requirements entry level jobs machine learning
r3k9b3,0,discussion wrote article inferencing large transformers gpus instances databricks practical ml practitioners use want run millions rows large transformer models need need feedback write helpful content community article wrote high performance inferencing large transformer models spark
rovzdh,1,correctly learning dnn model hi new machine learning field wrote simple neural network using tensorflow discovering heart diseases using data correct way professional data scientists solve problems thanks lot feedback please tell something wrong solution github
ra5420,1,mixture model different distributions hi one trains mixture model using em two observed variables depend hidden variable mean multiply probabilities together running em algorithm im thinking possible however sure looking graphical representation model seems independent path tail tail respect hidden variable
rwxmw4,0,search engine time series hi last year developed passenger flow forecasting model passenger flows heavily influenced lockdown measures weather wanted incorporate features relating model encountered various frustrations x200b data providers generally focus specific domain e g covid weather ecommerce forecasts however influenced data many domains finding providers signing reading documentation time consuming takes lot code get data want example get covid data province first call list endpoint retrieve province identifier loop data endpoint maximum range 2 months think core problem api’s acronym indicates meant interfaced programmatically data scientists end calling api’s manually model runs production proposed solution build platform partners many data providers indexes time series make searchable semantic search result would single time series search would usable via web ui endpoint screenshot web ui prototype library currently working prototype pretty decent search really validated assumptions would great could provide feedback make sense focus time series would general semantic data search make sense currently get external data model major pain points semantic search could speed process acquiring external datasets thoughts thanks feedback
rbrx6y,1,need insights possible project people reddit save soul hey folks reddit dire situation need complete college project artificial intelligence lost know build solid experience ever built apps websites project given detecting seat occupancy using example take photo 9 seats 3 occupied friends able tell 3 9 seats occupied go grasp terms deep learning since played around tiny bit help would greatly appreciated cheers
quvctp,0,mlops new era devops powered machine learning interesting mix terms “ai ml” “development operations ” mlops assortment methods utilized ai life cycle computerization calculations execution enormous scope clears smooth way coordinated effort information researcher proficient consequently goes scaffold abilities methods apparatuses utilized information designing ai devops read
qzoad5,0,dataprofiler scaleable sensitive data detection analysis structured unstructured files hello created library one stop shop data exploration monitoring project two objectives 1 quickly accurate cheaply identify sensitive data pii npi datasets 2 generate data profiles utilized downstream ml applications regarding sensitive data detection published workshop paper model within library sensitive data detection high throughput neural network models financial institutions addition sensitive data detection library also calculates statistical features general characteristics dataset helped team quickly evaluate datasets also enabled profiles use downstream applications nifty features community may interested load files one command data dp data filename profile data single command profile dp load data save load profiles profile save dp profiler load filename merge profiles profile1 profile2 compare profiles profile1 diff profile2 extending current entity detection model transfer learning easy takes lines code retrain scratch possible though tad rough add new custom model entity detection generally looking feedback curious community thinks project
rf1kph,1,need help making transformer model alterations make using tensorflow googlecolab assigned come creative solutions neural network first get started make transformer model code made build model compile fit next less trying bunch different experiments approaches seeing affect data cool interesting things could add neural network see would affect outcome recommend exploring ideas analysis options conducting series experiments data analyses experiment make change analyzing data might use different optimizer add another layer network changes make guided questions problems encounter example model accuracy might low might try improve accuracy adding new layer network another example observe overfitting model might try adding regularizer reduce overfitting observe effect change make data analysis evaluate change example adding another layer network improve accuracy also increase overfitting
rdfmll,1,best way learn ai break background 19m college student majoring cs learned basic web dev data manipulation java python software engineering java blockchain smart contract development love linux cli anyways finished hardest semester ever probably feel like need learning something want spend winter break learning ai maybe technologies would love suggestions technologies best place start wonderful experiences udemy courses plan find course know area ai study foundational knowledge 4 5 hours research definitely say gans interesting would love get hands experience anything help secure high paying job future would also nice guess question moreso area ai get traction right get involved want experience able predict technologies heading
rrlgee,1,fun simple cat mouse challenge would solve numberphile link let assume clever enough come solution presented video could derive solution ml algorithm would choose
rqycwh,0,pretrained models useful getting low level image data quite popular open source models used getting image embedding represents high level semantic meaning would like know models help encode low level image details like style image eg pixel art sepia vaporwave patterns prominent shapes color distributions etc aware properties easily extracted using algorithms like fourier transform laplacian filters etc would interesting see extracted learned model
rc0coi,0,gopher 280 billion parameters language model deepmind blog post direct paper link seems like compilation findings scaling lm bit gpt3 retro retrieval style model
quutqc,0,online free hong kong machine learning meetup 17 october 7pm gmt 8 join us hkml s4e4 monthly free online machine learning meetup aware technical content talks 1 application natural language processing unstructured financial text create alternative data finance 2 ai automation game changer way apply ai business 3 tbd
r1f130,0,deep knowledge data science trust mle think i’ve learned applications major ml techniques linear regression logistic regression gda naive bayes svms ffnn lstms gmms knn kmeans little tiny bit transformers however can’t say understand know apply would really study algorithm make sure i’m violating assumptions much mles know deep understanding go far pulling hidden states lstms feed future states try completely derive new models stick basics tune well known models i’m first person apply ml dl company work i’ve learned everything still feel know little really don’t anyone compare plan moving another company soon i’m scared getting fired i’m par like mles said company
qlugqf,0,state art shot text classification say many text snippets one four classes also cannot get large scale labeled dataset 30 50 labeled examples per class methods currently state art settings
rfhwpn,0,happens output two lstm bidirectional lstm implementing blstm confused happens output 2 lstm forward backward suppose 3 words 4 length embedding e input form 4x3 one group people say output two lstm serially concatenated e 4x3 goes lstm independently suppose output 4 4x3 matrix output lstm result concatenated fully meaning result 12 length vector another group says output two lstms goes activation function independently meaning output two lstm two 4x3 matrices go activation function word time first word example 4 4 inputs activation function another group say output concatenated word word basis meaning final output 8x3 matrix 4 forward 4 backward currently using papers use mix original paper using something closer third option thanks
re99pq,1,huggingface great idea poorly executed project trying use huggingface transformers library build particular classifier keras jeez nightmares every time try understand use api tutorials find extremely hard understand api incredibly ambiguous idiot sentiment shared
qnchpo,0,google uc berkeley’s data driven offline optimization approach significantly boosts hardware accelerator performance reduces simulation time 90 research team google research uc berkeley proposes prime offline data driven approach architect hardware accelerators without form simulations compared state art simulation driven methods prime achieves impressive performance improvements 1 54× reducing total required simulation time 99 percent quick read google uc berkeley’s data driven offline optimization approach significantly boosts hardware accelerator performance reduces simulation time 90 paper data driven offline optimization architecting hardware accelerators arxiv
rfmmxx,0,much would willing pay good scientific article recommendation app monthly basis free trial beforehand personalised view poll
r6pedp,1,resource learn time series analysis prepare quantitative research take home assessment hi received offer 24 hour coding assessment role quantitative researcher trading company main component seems statistically analyze large scale tick tick financial data extract alpha patterns guess supposed analyze large time series data detect alpha patterns python numpy pandas background dealt professional time series analysis presentations resources eg tutorials sample presentations articles books youtube resources prepare clarification expect would great well thanks
rjw51t,0,dataset omicron daily cases country covid 19 variant x200b hi guys unfortunately omicron getting worse every day infecting people around world made auto updating daily omicron case country dataset x200b available x200b first posted wanted dataset curious speed omicron infects world time progresses situation getting worse might useful everyone x200b data 1 location country variants information provided 2 date date data entry 3 variant variant corresponding data entry 4 num sequences number sequences processed country variant date 5 perc sequences percentage sequences total number sequences country variant date 6 numsequencestotal total number sequences country variant date x200b x200b enjoy hopefully dataset become useless soon stay safe
qwwf82,0,happen neural turing machine differentiable computer line work recently saw renewed interest algorithmic reasoning petar velickovic essentially augment traditional discrete algorithms continuous pattern recognition dl reminds neural turing machine differentiable computer mostly spearheaded alex graves believe share motivation algorithmic reasoning approach heard much major new work since neural differential computer wondering still fascinated idea research since working topic sure challenges pitfall aware instability implementation thought open source code would help anybody insights direction explored recent years one schmidhubered idea ahead time people squeezed internal memory capacity transformer idea external dynamic memory would bounce back
rb7c6j,1,using bayesian belief networks numerical data way tutorials seen categorical variables makes sense given probabilities associated node way use bbns numerical data without transforming categorical dataset
qnhtxj,0,place create online data science portfolio browse community data science projects hey data scientist shifted career biomedical field working tech company hard learn data science skills showcase first employers stand created datascienceportfol io create online portfolio showcasing projects skills effective way also get inspired browsing projects created community still early days working section browse projects people get inspired please let know think feedback improvement ideas welcome thanks much pasquale
rc5zzf,0,ctrlgen workshop neurips 2021 controllable generative modeling language vision excited generation control disentanglement come ctrlgen controllable generation workshop neurips next monday december 13th feature mix 7 talks latest controllable generation live qa panel discussion poster presentations several interesting works creative demos controllable generation systems networking opportunities effort organized researchers stanford cmu microsoft dataminr university minnesota invited speakers panelists include researchers facebook google deepmind university washington new york university stanford tel aviv university
ru0nzv,1,sentiment analysis large datsets hello project perform sentiment analysis really large datasets one 6 million strings 90 using huggingface looks like inference super slow like passed one datasets crunching away 24 hours running gpu optimized batches takes way long considering pruning dataset much like get whole dataset 72mil ish still solve massive time issue faster way apart multiple computers solving subsets data thanks advance
rwig2c,1,implement minibatch smote 25000 images 4 classes one classes small number samples thought using smote need use mini batch approach since images cant loaded memory pls help
r4zaut,0,eliminate pytorch cuda error memory 1 line code working fast pytorch wrapper solves oom error automatically project link project aims flexible possible works existing pytorch code would love hear thoughts suggestions welcome
rbsbgf,1,hi anyone able help college project linear models gradient descrnt neural networks deep learning python please dm thanks 😊
r5tfm4,1,unbalanced predictions mnist implemented simple lenet model learn predict mnist dataset purpose experiment new loss function evidential deep learning makes model aware epistemic uncertainty e cat dog classifier say know picture whale training set balanced labels number samples label weights randomly seed initialised dataset shuffled yet predictions weighted lower digits see image rather substantially 0 digit implementation pytorch anyone know methods mitigate x200b
r6v7ic,0,paper explained sparse enough scaling transformers aka terraformer video walkthrough transformers keep pushing state art language domains mainly due ability scale ever parameters however scaling made prohibitively expensive run lot inference requests transformer terms compute memory requirements scaling transformers new kind architecture leverage sparsity transformer blocks massively speed inference including additional ideas architectures create terraformer fast accurate consumes little memory x200b outline 0 00 intro overview 4 10 recap transformer stack 6 55 sparse feedforward layer 19 20 sparse qkv layer 43 55 terraformer architecture 55 05 experimental results conclusion x200b paper code
qszlto,0,discussion fine tuning language models generating sql queries hey hope well things wanted discuss language modeling bunch critical data stored database format company working something used train language model convert natural language sql query currently rule based system tons else conditions works issue see new database gets added currently existing rule based system break thinking way automate nl2sql using machine translation model gpt 2 later followed something bigger like gpt j looked papers namely picard corresponding challenge called spider wanted discuss queries genral thoughts people problem batshit cray think finetuning large language models new data set would work well alternative would suggest go plain use trainer api hugging face genral thoughts opinions etc thank taking time read hope meaningful conversation
r5a7i5,1,separate overlapping clusters kmeans hello trying understand might best approach clustering data science problem encountered clusters overlap explain conceptual example conducting study trying detect much time spent using different water consuming devices lack better word households bath tub sink etc water consumption readers water using device reporting water usage levels case gallons minute chronological minute intervals row indicates minute worth water use indicated gallon minute rate across 10 households id dataframe looks like gpm gallons per minute id gpm 0 1 43 1 1 23 2 1 54 3 1 33 134 2 44 135 2 52 136 2 63 137 2 33 245 3 23 246 3 12 247 3 15 248 3 51 356 4 22 357 4 61 358 4 54 359 4 84 shown household id reading attributed water consumption value given different water using devices different typical water consumption levels water consumption values data likely cluster cluster represents different water using device households 5 studied water using devices using 5 clusters goal create 5 unique ranges similar bins could simply drop values new dataset find number minute observations fall within water using device range therefore say oh new household spent x minutes week using sink x minutes week using shower etc could simply run k means entire dataset ignoring id would account consideration households could different models sizes sinks bath tubs showers etc meaning slightly different typical water consumption values thus distorting clusters want run clustering sub dataset grouped id clustering true household would run kmeans gpm values attributed id 1 run next kmeans value values attributed id 2 run next kmeans value values attributed id 3 example lets say 25 unique ids corresponding gpm values want create encompassing summary cluster ranges capture individual kmeans runs specifically mean id kmeans run want produce min max value 5 clusters want produce single range cluster captures values whole dataset means want find minimum min value maximum max value cluster across ids would make simple summary dataframe min max 0 1 2 3 4 would create cluster 1 range would capture cluster 1 values across ids create cluster 2 range would capture cluster 2 values across ids create cluster 3 range would capture cluster 3 values across ids etc big problem ranges summary dataframe overlap theory seemed like idea would work final summary ranges would capture values dataset correctly appears much shift values cluster id hard time trying find properly label problem would someone perhaps know ways could address issue produce accurate summary cluster ranges thanks please accept apologies problem clear would happy go back explain better
r8zwc5,1,deep learning model would work kind string translation task dataset looks something like nlvpmvatv casspvtggiygytf glctlvaml csardgtgngytf nlvpmvatv casrpdgretqyf nlvpmvatv cassetgfgnqpqhf nlvpmvatv casslapgatneklff thing comma input thing output would good dl model learn kind structure data always capital letters doesnt special characters think character level model would made sense really idea start anyone experience similar task string translation anything would helpful thanks
rwy8za,1,need study data structures algorithms machine learning many people online emphasize much relevance knowledge data structures algorithm programmer left wondering applies one going machine learning spent last 3 months learning python dunno take detour learn going study machine learning maths thanks advance time
rhd64w,0,jupyterlab alternative enhanced academic writing capabilities hi folks looking self hosted online options combine jupyter notebooks capabilities academic writing bibtex export latex etc found curvenote wondering options thanks rh
riy24d,0,bert self attention actually look self attention mechanism central success transformer large language models gpt2 gpt3 bert t5 spawned towards data science article summarizes work variety researchers digging self attention heads actually looking surprising answer large fraction staring meaningless separator tokens small handful performing anything analogous traditional nlp tasks like coreference resolution
rqsyn9,0,paper explained glide towards photorealistic image generation editing text guided diffusion models video walkthrough diffusion models learn iteratively reverse noising process applied repeatedly training result used conditional generation well various tasks inpainting openai glide builds recent advances diffusion models combines text conditional diffusion classifier free guidance upsampling achieve unprecedented quality text image samples x200b try x200b outline 0 00 intro overview 6 10 diffusion model 18 20 conditional generation guided diffusion 31 30 architecture recap 34 05 training result metrics 36 55 failure cases results 39 45 safety considerations x200b paper code model x200b diffusion papers
qxepxd,0,transferability prompt tuning natural language understanding empirically investigate transferability soft prompts across different tasks models demonstrate promising directions worth exploring prompt arxiv nlp prompt
r7gb1g,0,aaai2022 final say paper gets accepted rejected paper submitted aaai received 1 weak accept 1 reject 2 accepts meta reviewer recommended weak accept however paper rejected usual final say paper gets accepted rejected
qxidh8,0,machine learning bootcamp train future ml contests invite everyone online bootcamp machine learning participation free decided hold machine learning bootcamp train participants future ml competitions ml bootcamp two day intensive training help increase machine learning knowledge acquire practical skills study basic deep learning tools welcome undergraduate postgraduate students making first steps world ml contests need register strict prerequisites potential participants least familiar classic ml algorithms like linear models decision trees pandas numpy matplotlib sklearn libraries 4 5 december 2021 online join
rl2913,1,general idea machine learning hardware data collected get point training models feels far away even though seems like pieces already know bring together
rc1dor,0,paper explained nüwa visual synthesis pre training neural visual world creation video walkthrough nüwa unifying architecture ingest text images videos brings quantized latent representation support multitude visual generation tasks text image text guided video manipulation sketch video paper details encoders different modalities constructed latent representation transformed using novel 3d nearby self attention layers experiments shown 8 different visual generation tasks model supports x200b outline 0 00 intro outline 1 20 sponsor clearml 3 35 tasks naming 5 10 problem recurrent image generation 7 35 creating shared latent space w vector quantization 23 20 transforming latent representation 26 25 recap self cross attention 28 50 3d nearby self attention 41 20 pre training objective 46 05 experimental results 50 40 conclusion comments x200b paper github
raxyt7,1,guys know course tutorial mnist need learn tensorflow 1 x project basically 6 features want 1 binary ouput tried adapting mnist code perhaps work need working seem training stays constant matter epochs basic mlp graph x tf placeholder tf float32 shape none 6 w tf variable tf zeros 6 1 b tf variable tf zeros 1 1 tf nn softmax tf add tf matmul x w b highly likely dumb reshaped stuff train reshaped tf reshape train 800 1 test reshaped tf reshape test 200 1 placeholder correct result real tf placeholder tf float32 none 1 loss function cross entropy tf reduce mean tf reduce sum real tf log axis 1 optimization optimizer tf train gradientdescentoptimizer 0 5 train step optimizer minimize cross entropy maybe new cost optimizer cost tf reduce mean tf nn softmax cross entropy logits logits labels real train step tf train adamoptimizer minimize cost initialization init tf global variables initializer tf session session epochs 1 session run init range epochs session run train step feed dict x x train real train reshaped eval correct prediction tf equal real accuracy tf reduce mean tf cast correct prediction tf float32 network accuracy session run accuracy feed dict x x test real test reshaped eval print accuracy data 2f format network accuracy 100
rs3qef,0,neural pseudo random number generator let say neural net generates numbers called g neural net predictor called p g goal let p guess generates p goal approximate g ie g maximizes difference g p outputs p minimizes papers explores kind setup
r9cz9r,1,reproducing webnlg challenge 2017 opennmt py hi guys data science student learning use opennmt py master degree thesis reproduced challenge old deprecated repository would like replicate updated repository need similar task within thesis approaching nlp field clear things since translation task necessary build vocabulary like machine translation opennmt py tutorial epochs command noticed deprecated works train steps however clear conversion speak old repository number epochs train model 13 tried looking old problems repositories default train steps 100000 deault batch size 64 13 epochs number old repository 20313 reasoning correct thanks everyone attention
rlil7y,0,cfp first international symposium tsetlin machine grimstad norway june 20 21 2022 hybrid x200b call papers first international symposium tsetlin machine welcome first international symposium tsetlin machine grimstad norway june 20 21 2022 hybrid invite submissions papers topics related tsetlin machines well learning automata novel ai algorithms explainable interpretable ai energy efficient ai systems design new ai applications intelligent data preprocessing paper submission deadline march 4 2022
r5bqyg,1,nodes lose activation activated understand nodes take sum nodes connected checks sum activation function activates depending value bias activates retain activated value added activation every loop retain activated value added activation post activation normalized set activation back 0 hope question makes sense thanks
rbvz3z,0,edge weight prediction undirected graphs hello everyone pretty new graph learning research audio generation specific task working think data may better represented undirected weighted graph abstract sense problem would given set vertices rich features weighted adjacency matrix may complete may incomplete — want look formulations predict 1 edge weight unseen test vertex vertex input graph 2 edge weight two unseen test vertices like reading possible trying hack something together wondering anyone seen work anything like struggling find literature similar problem many thanks
ruf6eq,1,eigenvector 2 minute visual guide x200b 🔵 eigenvectors 🔵 🚀 eigenvector special vector associated linear transform special sense applying said transform change direction gets scaled multiplied scalar value eigenvalue 🔨 eigenvector comes corresponding scalar called eigenvalue breaking matrix eigenvalues corresponding eigenvectors called eigendecomposition 🔭 word eigenvector comes eigen german means originally used study rigid body motion discovery principal axes however nowadays found way wide array applications favorite principal component analysis differential equations problems physics chemistry relating wave transport molecular orbitals 🎭 another one classical applications eigenfaces project facial recognition eigenfaces decompose face composition face templates basis called eigenfaces imagine n eigenfaces e 1 e n given new face f written composition n eigenfaces example f 10 e 1 55 e 2 35 e 3 eigenface would represent meaningful feature high dimensional space faces like content would like steer topics cover suggest topics would like know comments
rllfmq,1,anyone tried using linear regression logistic binary classifier
rv63gk,1,starlite python async asgi api framework hi people wrote article introducing starlite new asgi api framework discuss came relation fastapi core functionalities article found happy answer questions might
r2t541,1,would train bot analyze movie scripts make i’ve researching bots analyze series movie scripts text e college humor infomercials syllabus etc data collected make curious would take make one pretty good understanding programming thought python would best language write something like thought results would make pretty funny project however don’t know start sources tutorials begin thanks
r8kfpf,0,dynamic batching gpt j api hi created small fastapi back end gpt j huggingface dynamic batching dynamic batching potential increase throughput expense latency pads sequences batch length longest one project determined maximum amount tokens batch size x sequence length could fit memory tuning batch size keeping sequence length fixed would get memory errors anyone knows reliable way determine optimal batch size would appreciate pointers purpose project show implement dynamic batching real production environments probably use c go instead python fastapi github link blog post
riqxrq,0,large language models understand us blog post blaise aguera arcas summary large language models llms represent major advance artificial intelligence ai particular toward goal human like artificial general intelligence agi it’s sometimes claimed though machine learning “just statistics” hence progress ai illusory regard grander ambition take contrary view llms great deal teach us nature language understanding intelligence sociality personhood specifically statistics amount understanding falsifiable sense furthermore much consider intelligence inherently dialogic hence social requires theory mind since interior state another understood interaction objective answer possible question “it” becomes “who” — many people neural nets running computers likely cross threshold near future
rp7qdv,1,project advice recommender system generates useful insights random datasets hi currently designing recommender system generate useful insights random datasets course study two specific problems would like hear people opinion giving example sketch context let say supply dataset data tests done students class dataset appears certain student higher average score rest class recommender system could recommend end user eg teacher plot insight bar chart make clear end user student scores higher average insight could described follows combination 1 dimension student id 1 metric score aggregated average take average student average metric datatype double significantly higher average entire column outlier insight type five properties insight described 1 dimension 1 metric metric datatype aggregate average outlier could seen features insight like movie might genre year production simple example go quite far like allowing combination 2 dimensions 2 metrics idea system moment 1 intake given dataset 2 generate recipe possible insights limit amount dimension metric combinations 3 check insights insight type one eg outlier trend reversal could also quantify types outlier x higher rest column let take quantification consideration right 4 remove spurious insights like 100 correlation low outliers 5 rank insights remain recommender system finite amount insights possible insights predefined possible match insights across datasets questions lot classic recommender system examples movies recommended users like compare movies insights users datasets trying find analogy problem hand classic movie user example 1 know user dataset analogy good one dataset columns set features amount columns amount dimensions kurtosis certain metric variance certain metric would like recommend insights datasets similar values feature done collaborative filtering right normally classic example user defined ratings movies inherent features age country origin 2 new dataset system ratings insights looking solution would use dataset distance datasets dataset space recommend insights useful datasets similar features new dataset anyone ideas links tutorials readings solve issue think little complicated cold start problem would like always take account dataset features start nature system recommendations would also done first time user uploads dataset really way recommendations certain dataset would become better time user selected rated insights
qqvsu0,0,modernizing gov finance data creating infrastructure fed ai adoption justin marsico bfs cdo thursday november 18 2021 11 30 et hi r machinelearning wanted share upcoming webinar details website featured guest speaker justin marsico chief data officer deputy assistant commissioner bureau fiscal service presentation learn us treasury bureau fiscal service building better public understanding federal finance continue modernize management data data sharing justin marsico chief data officer fiscal service shares unique opportunities around data federal level entails create infrastructure ai adoption recruiting data scientists well challenges opportunities data governance security ownership related data areas offering clear accessible information citizens see taxpayer dollars spent learn federal budget justin give live demonstration latest online resources attendees learn fiscal service enhancing data education building enhancing public trust greater transparency come join us great presentation around areas related enhancing ai skills government workforce recruiting data scientists finding appropriate use cases creating infrastructure ai adoption stick around q justin end agenda 11 30 12 30pm featured presentation 12 30 13 00pm q interaction link website
ruxg67,1,estimating orientation fractional occupancy two crossing bundles hi goal currently model estimates orientation fractional occupancy two crossing bundles cubic volume based noisy signal x200b fig 1 2d representation two crossing bundles current approach since two bundles basically indistinguishable decided always put bundle higher fractional occupancy first target bundle lowest fractional occupancy second target network architecture target ordering depicted fig 2 fig 2 network architecture issue two bundles fractional occupancy close example frac 0 0 51 frac 1 0 49 network sometimes swap bundles frac 1 x z associated bundle 1 output bundle 0 backpropagation network thus heavily penalized swapping since error x z huge network wrongly estimates frac 1 frac 0 swap bundle heavily penalized x200b fig 3 mae fractional occupancy function ground truth fractional occupancy mae increase volume fraction close 0 5 due bundle swapping learning x200b way better arrange target prevent kind issue thanks regards
r4aaj9,1,suggestions applying machine learning structural health monitoring mechanical fault prediction mechanical engineer working reputed organisation recent years asked take projects related machine learning apply concepts fault prediction similar things rudimentary idea ml algorithms used sklearn 4 5 years back anyone suggest apply ml fault prediction field books tutorials suggestions really appreciated
rhwofs,0,right use phd student expert annotator dataset creation paper give authorship hi guys sure seen recent discussions role datawork ml research example definitely complicated lots room disagreement today came across example well known researcher choosing give phd student authorship made wonder guys think post question names particular expert annotators ones wrote dataset acknowledged name principle seems good think surprising though expert annotators seem actually phd students last author department linguistics data collection relied domain expertise presumably qualifies task strikes questionable include student department author especially contributed meaningfully dataset going split system domain expertise active graduate level researcher sufficient authorship project writing dataset much less important running model tl dr fair fucked make phd student author project worked worked data side dataset paper
rd359m,1,best resources found enjoyed deep learning francois choillet book still going used kaggle yet
ruijl9,1,sieve processed 24 hours security footage 10 mins search per frame hey everyone i’m one creators sieve i’m excited sharing sieve api helps store process automatically search video data–instantly efficiently think 10 cameras recording footage 30 fps 24 7 would 27 million frames generated single day videos might searchable timestamp finding moments interest like searching needle haystack built visual demo link little back we’d love get feedback it’s 24 hours security footage api processed 10 mins simple querying export functionality enabled see applications better understanding data figuring data send labeling sampling datasets training building multiple test sets models scenario try videos visual dashboard walkthrough
r0gnej,0,one sentence highlight every neurips 2021 paper plus code 200 list 2 300 neurips 2021 neural information processing systems papers one sentence highlight neurips 2021 held online dec 06 highlights code
r3hb3j,1,overview specific layers groups layers strange situation working program help people learn neural networks showing pretty much everything goes inbetween like outputs layers visualized look like images data goes layers experienced creating nns know strange situation writing program help people understand nns without understanding tensorflow js people browser without write code think program gui tfjs drag drop layers around automatically generates graphs like corresponding python code tf model browser example structure network gui part works fine people know nns already use program train network browser want add something like real albeit modified screenshot working program prototype question overview groups layers like conv2d followed maxpooling something add bracket notations like one make easier people understand going since data diverse aware single way describing thouse groups correct time fine correct default use cases really appreciate links said please assume idea nns really except high level description
r3bs4u,1,video taking shooting game extract making system makes montage valorant montage videos good playing games words taking players row first attempted use cnn cutting video playing game training data cutting frame think way something better cutting data 100gb much hassle beginner machine learning maybe way know
rbq0oq,1,organize machine learning infrastructure let say want start using machine learning company would go organising machine learning infrastructure example thinking 1 one big service machine learning code models nlp churn prediction user clustering etc maybe django service multiple apps app endpoint call predictions 2 multiple rest apis model call predictions please motivate answer pros cons approach possible also share resources links books etc
qsrdyk,0,text image rudall e kandinsky xxl 12 billion parameter model checkpoint apparently available download download page rudall e kandinsky xxl 12 billion parameter model checkpoint english translation sign try download file 12b parameter model available per interpretation press release edit according comment public availability december 1 prior mention rudall e subreddit text image models rudall e kandinsky xxl 12 billion parameters rudall e malevich xl 1 3 billion parameters demo latter available rudall e model open source p
qk7tuv,0,thoughts pathways google research recently found proposal called pathways jeff dean article seemed obscure ideas single hint would solved whenever question posed word pathways thrown another huge transformer google wanted know everyone thinks
r1pf4c,1,image tinted blue even opened afresh x200b original image displayed image hello code pre processes images however image tinted blue end process reason understand og image imread test image1 png cv2 imshow og image even use imread imshow display original image towards end file still looks blue reason understand turns blue somewhere along way even read display original image afresh anybody knows common mistakes lead please tell libraries used skimage numpy cv2 find anything useful stackoverflow anywhere else sorry post entire code obvious reasons try best share much help understand problem kinda new help greatly appreciated
regt37,1,using amd radeon tf anaconda spyder hello understand tensorflow geared towards proprietary nvidia cuda workaround amd radeon gpu macbook pro amd radeon 580 external gpu card
rb7cs3,1,someone explain code vqgan clip art generation link read articles explained vqgan worked like gan clip discriminator looked code understand much want know code block fifth block example defines functions like sinc lanczos actually know used thank
rdtaiy,0,discussion quick reminder servers go 24tb ram seen sub lamenting bunch times large companies models large companies run use latest bunch comments deepmind thread maybe missing something think chance people outdated view computers want state facts understand given limitations would still problem normal research lab run large models currently outside super computers latest 2021 hardware ram motherboard fit 24 5tb running 8x intel xeons available e g aws good intel xeons fast e probably faster old gen cuda 11 compatible gpu even gpu optimized libraries ala torch much faster gpus cpu optimized optimization algorithms hash map based used server motherboards also fit 8gpus think number actually 32 never seen offer 8 eh cloud providers 16gb gpus 8gpus per server largest gpus run 80gb ram though widely available largest gpu optimized servers boast 1 3t gpu ram x200b price wise buying said gpu servers hard pin seem 200 600k range assume soon available rent price translate 200 600 hr expensive providers maybe low 50 100 cheap ones older models uneducated estimate based cpu server pricing compared buy price prices 6 2tb server 50 hr presumably 24 5tb version 200 hr seems like cpu gpu routes allow training certainly running big model biggest models either google openai price point high operating 200 hr putting half million hardware every 1 3 years seems reasonable small time well funded lab given researcher salaries 1 4th 1x amount given often enough research work models smaller extract valid heuristics models 10 100x size necessary obviously prohibitive people run laptop seems unfair say large text image models limited giant corporations server farms seems bloke knowledge clustering computers half million worth funding could easily work models size gpt 3 inference also testing seems increase availability models something like dozen thousand companies hundred even thousands well funded academic labs
qnh7w4,0,jupyter notebook doesnt store requirements require packages ipynb file ipynb file json file list required packages easily added separate file
rrxikg,0,mel frequency cepstral coefficients transformation hi need guidance project work simple keyword spotting problem spiking neural networks snn use akida brainchip catch snn layers work unsigned integers mfcc functions librosa audio ops etc returning float since expert audio data wanted ask simple common way usually transform mfcc integers currently shift data minimum new zero point simply cast integer afterwards really happy approach
r74c38,0,extract label data wikipedia dataqa hi recently added new feature dataqa able extract entities wikipedia need upload file wikipedia urls example file wikipedia urls unicorn companies url column needed tool extracts parses wikipedia content including tables divides content smaller paragraphs sections easier label simple upload labelling wiki tables output file csv file text extracted wikipedia manual automated rules based labels file used train custom ner model wikipedia data simply dataset use cases example output csv feel free come talk comment section would love engage people
r110aq,1,machine learning profit classification hi guys learning machine learning question seems machine learning used classification purposes example looking horse racing predictions models simply interested classifying winner example horse racing interested classifying winner producing model maximizes profit learning data set would course much find model predict steady load high odds winners rather lots smaller priced winners models built purpose
rij20m,0,internship ml phd hello everyone recently submitted phd thesis focused optimization rl university europe since advisor internships funding allow one graduated without internship experience difficult land full time job applied many full time roles got rejections almost time case make sense apply internships big companies see faang companies hiring lot interns nowadays suggestions thanks lot help
r8v65d,1,career advice best prepare get purely r data science position currently last semester master hoping get taste industry couple years pursing phd perhaps pursue part time either way goal secure data science position focused performing research potential publish papers deploy maintain models big data offer work associate data scientist local startup made clear would spending half time development half r data science team two people starting somewhat concerned much opportunity expand ml skill set however also good chance securing full time research intern position government organization research advisor involved position six months even another opportunity could arise extra six months hunt jobs wondering couple things 1 research intern position government org considered industry experience regards hiring managers like restricted applying entry level positions 2 would better obtain industry experience start even chance mostly r get experience full time research even temporary 3 plausible secure position 100 r without phd options would better stepping stone towards
r8ob3o,0,visualize millions datapoints using plotly 1 line code created python plotly figure wrapper adds adaptive resampling plotly figure github repo project aims enable visualizing large sequential data example visualize 110m data points would love hear feedback
qnxu7h,0,cvpr really computer vision know cvpr literally means computer vision pattern recognition really means submission ml field direct link computer vision discarded thanks
rhv6u9,1,synthetic time series data generation want generate time series tabular data generative deep learning models consists vae gan part relating images videos etc please point relevant tutorial souces includes code along theory better pertaining synthethic time series data generation using deep learning models techniques
rrbth9,1,pytorch vs tensorflow 2022 pytorch tensorflow ecosystems developing quickly thought time take another look stack one another made write comparing two frameworks thought might helpful sub getting started ml frameworks much overlap technical perspective previous years comparison focuses practical considerations namely model availability deployment ecosystems end give recommendations framework use based use case like one industry recommendation flowchart industry flowcharts hobbyists total beginners looking career change might especially relevant sub jump portion relevant links read whole analysis love hear thoughts regarding framework best use case maybe ditched frameworks started jax let know comments
qzic0a,0,zeroshot topics label text data infer topics text data automatically zeroshot topics github link hand labelled training sets expensive time consuming create usually datasets call domain expertise eg medical finance datasets etc given factors around costs inflexibility hand labelling would nice tools help us get started quickly minimal labelled dataset enter weak supervision labelled data way still label data automatically way zeroshot topics might useful help running quickly zeroshot topics lets exactly leverages power zero shot classifiers transformers knowledge graphs automatically suggest labels topics text data need point towards data x200b please check share feedback
qt4y6g,0,paper overview mae masked autoencoders scalable vision learners video paper abstract paper shows masked autoencoders mae scalable self supervised learners computer vision mae approach simple mask random patches input image reconstruct missing pixels based two core designs first develop asymmetric encoder decoder architecture encoder operates visible subset patches without mask tokens along lightweight decoder reconstructs original image latent representation mask tokens second find masking high proportion input image e g 75 yields nontrivial meaningful self supervisory task coupling two designs enables us train large models efficiently effectively accelerate training 3x improve accuracy scalable approach allows learning high capacity models generalize well e g vanilla vit huge model achieves best accuracy 87 8 among methods use imagenet 1k data transfer performance downstream tasks outperforms supervised pre training shows promising scaling behavior
rwj813,0,ideal deep learning library researcher perspective miss libraries like pytorch tensorflow could improved possible examples way autodiff works debugging features working axes einops something feel awkward inconvenient incomplete would much appreciate could share thoughts
r2trln,0,pretrained tokenizers work hi first post subreddit x200b using pretrained tokenizers available huggingface transformers library working well use case x200b however able clearly understand 1 need pretrain tokenizers 2 pretrained like code logic behind 3 work tried searching relevant literature google well aclanthology nothing insightful turned x200b someone elucidate concepts perhaps links papers x200b help deeply appreciated
rgu4u8,1,running collaborative ml experiments dvc tutorial sharing ml experiments compare models important working team engineers might need get another opinion experiments results share modified dataset even share exact reproduction specific experiment following tutorial explains bundle data code changes ml experiment push remote somebody else using google drive folder check using dvc data version control tool running collaborative experiments tutorial shows setting dvc remotes addition git remotes lets share data code hyperparameters associated experiment anyone pick left training process
rphiin,0,2022 research topics areas hi good people would like suggest latest interesting research areas work 2022 specially deep learning deep reinforcement learning federated learning meta learning areas thank
rwl2hh,1,style transfer multiple style sources planning simple web service get image input send back style series particular paintings got loop ml quick view seen style transfer seems work one source one target style tinkered ml past used larger datasets since 40 ish image style dataset wondering technique employ one something hard already implemented python possibly
r7ejz8,1,machine learning maths good evening new machine learning question reason non trivial know likely operate data consists small amount observarions great amount variables however cases recommendation systems computer vision e c cannot avoid ml models face case described models better others terms thank
rh9t9x,0,training machine learning models efficiently dataset distillation today google ai blogpost focuses dataset distillation arising infinite width limit theory neural networks small learned datasets used train kernels neural networks high test accuracy one highlight result using 10 learned images labels 64 7 test accuracy achieved cifar 10 blogpost papers discussed dataset distillation infinitely wide convolutional networks neurips 2021 dataset meta learning kernel ridge regression iclr 2021
rvadz4,0,🐸yourtts towards zero shot multi speaker tts zero shot voice conversion everyone yourtts brings power multilingual approach task zero shot multi speaker tts possible fine tune model less 1 minute speech achieve state art results voice similarity reasonable quality 🤖 demo 👩‍💻 code 🚀 blogpost 📎 paper
r9dri0,0,sota stylegan inversion explained hyperstyle stylegan inversion hypernetworks real image editing 5 minute digest casual gan papers proved surprisingly difficult task balance reconstruction quality images inverted latent space stylegan2 generator ability edit images afterward yuval alaluf omer tov team originally reported infamous reconstruction editability tradeoff “designing encoders editing” paper back new encoder design inspired recent pti paper sidesteps tradeoff finetuning generator’s weights way places inverted image well behaved region latent space leaves editing capability unchanged hyperstyle hyper network speeds things training single encoder predict weight offsets input image replacing compute intensive per image optimization single forward pass model takes second instead minute authors able predict weight offsets entire stylegan2 generator efficient manner let’s find full summary h blog post hyperstyle arxiv code demo subscribe casual gan papers follow twitter weekly ai paper summaries
rgudcg,1,best source learn pytorch improved tf hands ml tensorflow keras scikit learn anything equivalent book pytorch equivalent need book
qqwa9u,0,motivation 3d bounding box adas one common test sets car recognition called kitti 2017 2d bounding box test set around year moved 3d bounding box test sets anyone explain motivation behind 3d bounding box adas features
rd2kq6,1,need guidance crop production forecasting found agro tech company data crop production 90 countries forecasting model currently use barely 50 accurate data date variety quantity fields might relevant boss told could even get model around 70 accuracy would good enough want accuracy variety continent level new ml worked simple kaggle models someone tell methods study helpful similar datasets kaggle practice see work others thanks
rkvq1x,0,compute costs machine learning team researchers mit yonsei university university brasilia launched new website computer progress analyzes computational burden 1 000 deep learning research papers data site show computational burden growing faster expected rate suggesting algorithms still room improvement work needed algorithmic efficiency brute force compute data strategies x200b validates many concerns stated post
rxh1kl,1,one go guys want get machine learning time get system came option base model 8 core cpu 7 core gpu macbook air m1 8 256 option b lenovo ideacentre g5 gaming desktop amd ryzen 7 3700x 8gb 1tb hdd 256gb ssd windows 10 nvidia rtx 2060 cd 6gb gddr6 graphics raven black 90q1004gin comparing base model macbook costing also adding anything extra escalate price macbook lot country india least yes yes build would cost lot gpu expensive rare find moment know x200b thank reading
