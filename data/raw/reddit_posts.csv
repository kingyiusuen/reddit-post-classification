id,created_utc,title,selftext,subreddit_name
rxf7wq,1641477562.0,[N] MDLI Ops – A free conference to help you make sense of the MLOps landscape,"*Disclaimer*: Not my conference, but a good friend's work. He's doing an awesome job community building and I thought this might interest the community here as well (I am not a sponsor).

Hey r/ML, some of you might have heard about MDLI – short for Machine & Deep Learning Israel. It's an independent Israeli community (over 25K members) for professionals in data science and ML, and they are having a conference for everyone (in English), in just 14 days.

You can register here [https://machinelearning.co.il/mdli-ops-2022/](https://machinelearning.co.il/mdli-ops-2022/)

I know that sometimes these free events tend to feel like commercials, but you should really check out the agenda. Some of the talks are going to be by ML teams at companies like AppsFlyer and BigPanda, explaining how they built their internal stacks and ML systems.

There's also a super interesting talk by the NVIDIA team, where they will talk about building supercomputers to train ML models on them. This one seems crazy awesome to me.

It's also a good opportunity to listen to offerings by some cool ML startups, which might give you a better understanding of how they compare, and what you actually care about when choosing MLOps tools.

The organizers will probably be here in the comments if you have any questions, but in my opinion, every event this community organizes is really awesome, and I get to learn a lot, so I really recommend this.",MachineLearning
rxe28v,1641474020.0,[P] Machine Learning Engineering Conferences,"I was wondering if anyone could recommend and conferences suitable for MLE’s, maybe with a focus on deploying models. I’ve been to traditional data science focused one, such as the Anaconda one. Though I wanted to see if any existed that were more closely connected to deployment.",MachineLearning
rxczzl,1641470572.0,[P] Deepchecks: an open-source tool for high standards validations for ML models and data.,"Hey everyone!

I wanted to share with you an open-source tool we've been building for a while. Deepchecks is an open-source tool for validating & testing models and data efficiently:

[https://github.com/deepchecks/deepchecks](https://github.com/deepchecks/deepchecks)

Deepchecks is a python package, implementing validations and tests needed in order to trust an ML pipeline. It contains many built-in checks, such as verifying the data integrity, inspecting its distributions, validating data splits, evaluating your model, and comparing between different models.

In addition, it contains test suites, similar to the test suites in software programs, that can accompany you through all building blocks of the ML pipeline development. Each test suite contains checks necessary for the specific part in the pipeline.

The suite result looks something like this:

&#x200B;

[Suite result](https://preview.redd.it/pbh8bbbu42a81.png?width=752&format=png&auto=webp&s=5b11ba59e0eacbd0982e538258c5066e0a8c2b60)

The suites and checks have a simple syntax and are highly customizable.

If you want to jump right in, you can try it out in the quick start notebook:

[https://docs.deepchecks.com/en/stable/examples/guides/quickstart\_in\_5\_minutes.html](https://docs.deepchecks.com/en/stable/examples/guides/quickstart_in_5_minutes.html)

What do you think? I’ll be happy to hear your thoughts and feedback.",MachineLearning
rx2vov,1641433568.0,[D] Retrieval transformers for other domains?,I am just going through the research behind this: [http://jalammar.github.io/illustrated-retrieval-transformer/](http://jalammar.github.io/illustrated-retrieval-transformer/) and was wondering if there has been work like this for other domains say computer vision for example?,MachineLearning
rwxt8r,1641419702.0,[D] Legal use of functions in Pytorch or Tensorflow,"Does anybody know, is it legal to use Dropout or BatchNorm from Pytorch and Tensorflow due to Google's patents of these two functions? Did some library avoided patent infringement in its implementation of those functions?",MachineLearning
rwxmw4,1641419171.0,[D] Search engine for time series,"Hi,

Last year I developed a passenger flow forecasting model. Passenger flows are heavily influenced by lockdown measures and the weather, so I wanted to incorporate features relating to that into my model. Doing so, I encountered various frustrations:

&#x200B;

* Data providers generally focus on a specific domain (e.g. covid, weather, ecommerce). Forecasts however can be influenced by data from many domains. Finding these providers, signing up, and reading documentation is very time consuming.
* It takes a lot of code just to get the data you want. For example, to get covid data for my province, I first had to call a /list endpoint, then retrieve the province identifier, and then loop over the data endpoint because the maximum range was 2 months. I think the core problem is that API’s (as the acronym indicates) are meant to be interfaced with programmatically, but that data scientists end up calling API’s manually before the model runs in production.

My proposed solution is to build a platform that partners with many data providers, indexes their time series, and make them searchable with a semantic search. Each result would be a single time series. This search would be usable via a web ui, and endpoint.

Screenshot of web ui prototype:

https://preview.redd.it/1bt4xup8wx981.png?width=1938&format=png&auto=webp&s=536393b0c57109a33107f12c75516c13cb708f02

library:

https://preview.redd.it/6agy2hq7wx981.png?width=1614&format=png&auto=webp&s=2af4871607e566a468e76abac4c4d5495aaa0d1a

I currently have a working prototype, with pretty decent search, but haven't really validated my assumptions. It would be great if you could provide some feedback:

\- Does it make sense to focus on time series, or would a general semantic data search make more sense?

\- How do you currently get external data into your model? What are your major pain points?

\- Do you semantic search could speed up the process of acquiring external datasets?

\- Any other thoughts

Thanks for your feedback!",MachineLearning
rwwjal,1641415777.0,[D] (A paper suggests) Most Time Series Anomaly Detection Papers are Wrong," I just stumbled on this very nice paper \[a\], which will appear in AAAI-22.

The title seems much too modest, they show that a random algorithm can achieve apparent SOTA results in this domain. This seems to be a stunning result, that casts doubt on the contribution of dozens of papers.

For some reason, the area of Time Series Anomaly Detection seems to be the wild west of dubious papers and sloppy thinking.

As an aside, there is a benchmark set of 250 datasets here \[b\] that can be evaluated in a way that is free of the flaw.

(my post title reflects my understanding of the paper, the authors may have a different preferred claim).

\[a\]  Towards a Rigorous Evaluation of Time-series Anomaly Detection  [https://arxiv.org/pdf/2109.05257.pdf](https://arxiv.org/pdf/2109.05257.pdf)

\[b\] www.cs.ucr.edu/\~eamonn/time\_series\_data\_2018/UCR\_TimeSeriesAnomalyDatasets2021.zip",MachineLearning
rwwdh0,1641415220.0,[D] Multi-agent Deep Reinforcement learning,"Hello!

I hope you´re doing well.

I am working on a multi-agent system with MADDPG.At time t when an agent asks for a task, the other agents are busy (i.e., the busy agents are those that are still processing a task, they didn´t finish it yet).So with this configuration, in the learning phase, I don´t know how to mask the state of the busy agents when injecting the state and action pair to the critic network.

Thank you.",MachineLearning
rwumjj,1641410126.0,[D] VQ-VAE: Are there heuristics for the number of embeddings and the embedding dimension?,"Hi r/MachineLearning, does anyone with experience training VQ-VAEs know if there are good rules of thumb for the embedding size?

E.g. given data of dimension N, use M embeddings of size P

Thanks for any help!",MachineLearning
rwtpxv,1641407836.0,[D] Preparing for a Comprehensive Exam in ML,"I am a PhD student based in Canada and have a comprehensive exam coming up in 4-6 months. This is an exam I have been nervous about since I began my PhD. I am fairly confident about the actual proposal and answering questions related to my field. What concerns me more is fundamental/background question as ML and statistics is so broad. Plus, I am a little on the older side and my memory is a little poor.

Have any students here taken a comprehensive exam? If so, what was your experience and how did you prepare? Is reading/making notes from a textbook a good idea? Or is preparing a list of topics and reading extensively about them a better option?",MachineLearning
rwseha,1641404454.0,[D] What is the format of full paper presentations in general ML conferences like IJCAI and AAAI?,"Hi all,

This is my first conference season, so I am curious about how do author of full papers (not extended abstracts or student-track, not invited speeches) present in conferences like ICML,  AAAI and IJCAI?

I mean, for instance: are presentations performed in a panel with 3-5 presenters using slides, or are they all presented as posters where authors stay available for a duration of time for the interested readers to show up and discuss? Or something else?",MachineLearning
rwrnk2,1641402449.0,"[D] Can you recommend funny papper like ""Single Headed Attention RNN: Stop Thinking With Your Head""","I really enjoyed reading this for a change to the textbook papers.

>Abstract
The leading approaches in language modeling are
all obsessed with TV shows of my youth - namely
Transformers and Sesame Street. Transformers
this, Transformers that, and over here a bonfire
worth of GPU-TPU-neuromorphic wafer scale sil-
icon. We opt for the lazy path of old and proven
techniques with a fancy crypto1
inspired acronym:
the Single Headed Attention RNN (SHA-RNN).
The author’s lone goal is to show that the entire
field might have evolved a different direction if we
had instead been obsessed with a slightly differ-
ent acronym and slightly different result. We take
a previously strong language model based only
on boring LSTMs and get it to within a stone’s
throw of a stone’s throw of state-of-the-art byte
level language model results on enwik8. This
work has undergone no intensive hyperparameter
optimization and lived entirely on a commodity
desktop machine that made the author’s small stu-
dio apartment far too warm in the midst of a San
Franciscan summer2
. The final results are achiev-
able in plus or minus 24 hours on a single GPU as
the author is impatient. The attention mechanism
is also readily extended to large contexts with
minimal computation. Take that Sesame Street.

https://arxiv.org/abs/1911.11423",MachineLearning
rwq3tb,1641398373.0,[D] Normalizing flows for distributions with finit support,"I need to learn a map from Gaussain distribution to Gamma distribution with some custom parameters. So for both distributions, I can sample and evaluate probability density. The first thing, that came to my mind is using normalizing flow.

Most approaches include log target probability density evaluation in the loss function. Obviously, normalizing flow sometimes returns negative values, and this term equals to infinity. ""Positivation"" functions on top of the NF break bijection properties for some regions of space (if not theoretically, but numerically - defenetely).

Does NF approach is inapplicable from the box for such a simple problem or I'm missing something?",MachineLearning
rwots2,1641394787.0,[D] Methods to create monolingual language model from pretrained multilingual model,"Apart from just fine-tuning the pretrained multilingual language model on the target language, is there anything more sophisticated that people are doing?",MachineLearning
rwmvn7,1641389191.0,[P] Blogs on fundamentals of Score-based and Diffusion Probabilistic Models,"This two-part blog describes the theoretical fundamentals of Score based models, Diffusion Probabilstic models and their relationship. It is written to be a coherent documentation of the theoretical developements in this new class of generative model. Rigorous mathematical proofs are excluded in order to make it more readable. Sharing it in case anyone finds it useful.

* Part 1: [Score-base models](https://ayandas.me/blog-tut/2021/07/14/generative-model-score-function.html)
* Part 2: [Diffusion Probabilistic Models](https://ayandas.me/blog-tut/2021/12/04/diffusion-prob-models.html)",MachineLearning
rwl4xl,1641383395.0,[D] Pretraining the discriminator of a Least Squares GAN,"I am trying to train a GAN to generate human poses in 3D space using the Humans 3.6M dataset. The output of the GAN is thus the 3D coordinates of the human joints. I have been experimenting with vanilla GANs but the output is quite noisy.

I am now looking into Least Squares GAN but was wondering if it is a good idea to pretrain the discriminator of a Least Squares GAN since LSGANs address the problem of vanishing gradients and loss saturation?",MachineLearning
rwju9t,1641378553.0,[D] Minimum Corpus Size for Word Embedding Extraction,"Dear all,

I have a smallish \~100MB corpus (of historical text in a non-mainstream language), on which I want to apply word embedding.

\- Is that enough? Shall I only consider ""frequent"" words? How frequent? does it help if I do some preprocessing such as stemming etc...?

\- How do I choose the parameters, especially the embedding dimensionality?

\- Any libraries recommended?

\- Are there some language-agnostic, unsupervised ways to evaluate the embeddings?

&#x200B;

Thanks",MachineLearning
rwj813,1641376124.0,[D] Ideal deep learning library,"From researcher perspective:
  - What do you miss in libraries like PyTorch or TensorFlow?
  - What could be improved?

Some possible examples:
  - The way how autodiff works
  - Debugging features
  - Working with axes, einops
  - Something that just feel awkward, inconvenient or incomplete

I would very much appreciate it if you could share your thoughts on this.",MachineLearning
rwehvc,1641358260.0,Is a scalar reward enoguh to create AGI? [R]," Check out this paper which discusses the idea that a scalar reward is not enough to create agi.

[https://arxiv.org/abs/2112.15422](https://arxiv.org/abs/2112.15422)

What are your thoughts on this?",MachineLearning
rwcikx,1641352186.0,"Predicting future labels where the future values of only some features are known (RNNs, time series)[D]","I've done a fair bit of work with RNNs and Time series data, but I now realize there may be a fundamental gap in my knowledge. I was reading through [this](https://www.tensorflow.org/tutorials/structured_data/time_series) and it raised some questions about the different kinds of time forecasting problems.

So, this is my summary of how it all works. Please correct me if I’m wrong.

Scenario 1: You want to predict the value of some stocks into the future. Let’s say you have k stocks and n days of data. You don’t have features/labels stocks, rather the input is each stocks’ current and previous values, and the output is each stocks’ future values.

Your two main options are the ‘auto-regressive’ approach where you predict the values one step at a time and feed them back in, or ‘single shot’ approach where you predict all values a fixed amount of time steps into the future at the same time. This all seems pretty standard and is a well understood problem.
Scenario 2: You have 3 time series features: A, B, C and one label Y. You know that Y is dependent on A, B, and C. You goal is to predict the future values of Y. My understanding is you can’t use the auto-regressive approach because you aren’t predicting A, B, or C, only Y. But for the single-shot approach your input would have shape (4, w), and your output would have shape (1, p) where w is the number of warmup timesteps, and p is the number of timesteps you want to predict Y’s value.

Now, what if you know the future values of A, B, and C p timesteps into the future? This extra information would obviously help the prediction. Essentially you could allow your model to peak into the future, but only for A, B, C. However, complications arise because you can’t pass an input with shape (4, w + p), because you don’t know the future values of Y (that’s why we are trying to predict them).

I’ve not been able to find any papers or information on this type of problem, most likely because I don’t know the name of this type of problem, but I’ve had a couple of thoughts:

 - Have the future values of features with known future values as
   separate features. So you would have an input shape of (7, w), rather
   than (4, w) since you know the future values of all 3 features.
 - Some sort of recursive approach where you first come up with some
   arbitrary prediction of Y p time steps into the future. Then when
   training you would pass an input of shape (4, w + p) (the past and
   future values of A,B,C, and the past and predicted values of Y) and
   an output of shape of (1, p). Your loss would be the
   distance/difference between your predicted future Y values, and the
   actual future Y values. You then replace the arbitrarily generated
   future Y values with the ones you just generated and repeat the
   process. Not 100% sure if this would work though, I’d have to suss
   out the details.

Scenario 3: Similar to the previous scenario, but instead of knowing the future values for A, B, and C, you only know the future values for A. For example, let Y be the temperature, and A, B, C be rainfall, humidity, and cloud cover respectively. Let’s say you use meteorological rainfall predictions as your know values into the future for A. You would ‘know’ some future feature information, but not all. Obviously, there are some issues about using another prediction as a future value of a feature, but let’s leave that aside for now.
I’m curious as to what work has been done in this area. My intuition says that treating A, B, C, and Y as all inputs when you know Y is dependent on A, B, C adds an extra layer of context that would be good to incorporate into this problem.",MachineLearning
rwc7zy,1641351325.0,[P] Classification with imbalanced datasets question,"I've been working on a medical classification project with an imbalanced tabular dataset. I have 3 classes, and each class has 44, 16, and 14 rows of data respectively. When I train a random forest classifier, I see that my model is only predicting the dominant class for all test instances most of the time. How can I get around to this? Also, are there any recommendations you can give me for dealing with imbalanced datasets? Thank you",MachineLearning
rwc10x,1641350759.0,[P] I implemented Conformer: Convolution-augmented Transformer,"I implemented Google AI's ""Conformer: Convolution-augmented Transformer for Speech Recognition"" paper, it achieves the best of both worlds by combining CNNs and transformers to model both local and global dependencies and improves the local inductive bias in Transformers.

[https://github.com/Rishit-dagli/Conformer](https://github.com/Rishit-dagli/Conformer)",MachineLearning
rw8jxd,1641340765.0,[P] ray-skorch - distributed PyTorch on Ray with sklearn API,"**tl;dr**: train PyTorch models on large tabular datasets with a scikit-learn (skorch) API

Hi r/MachineLearning,

I'm the principal author of [ray-skorch](https://github.com/Yard1/ray-skorch), a library that lets you run distributed PyTorch training on large-scale datasets while providing a familiar, scikit-learn compatible [skorch](https://github.com/skorch-dev/skorch) API, integrating well with the rest of the scikit-learn ecosystem.

Under the hood, ray-skorch uses [Ray Train](https://docs.ray.io/en/latest/train/train.html) for distributed PyTorch training and [Ray Data](https://docs.ray.io/en/latest/data/dataset.html) for handling and shuffling large datasets.

ray-skorch works only with tabular data. Currently, it can use numpy arrays, pandas dataframes and Ray Data Datasets.

`pip install ray-skorch`

You can switch your skorch code to ray-skorch just by changing a few lines:

    import numpy as np
    from sklearn.datasets import make_classification
    from torch import nn
    # pip install pytorch_tabnet
    from pytorch_tabnet.tab_network import TabNet

    from ray_skorch import RayTrainNeuralNet

    X, y = make_classification(1000, 20, n_informative=10, random_state=0)
    X = X.astype(np.float32)
    y = y.astype(np.int64)

    net = RayTrainNeuralNet(
        TabNet,
        num_workers=2,  # the only new mandatory argument
        criterion=nn.CrossEntropyLoss,
        max_epochs=10,
        lr=0.1,
        # TabNet specific arguments
        module__input_dim=20,
        module__output_dim=2,
        # required for classification loss funcs
        iterator_train__unsqueeze_label_tensor=False,
        iterator_valid__unsqueeze_label_tensor=False,
    )

    net.fit(X, y)

    # predict_proba returns a ray.data.Dataset
    y_proba = net.predict_proba(X).to_pandas()

More examples, including ones on bigger datasets, can be found here - [https://github.com/Yard1/ray-skorch/tree/main/examples](https://github.com/Yard1/ray-skorch/tree/main/examples)

The package is experimental, and I’d love to hear your feedback - both on the package itself and on the concept of distributed training on tabular data with simple, familiar APIs. Any comments, suggestions or bug reports are hugely appreciated!",MachineLearning
rw50hg,1641331110.0,[D] Deep Learning is the future of gaming.,"Hey everybody --- I know this isn't hard core AI research but I have been thinking a lot about deep learning and gaming recently and put together a little presentation on how I see things unfolding. Lots of cool research featured in the video.

[https://www.youtube.com/watch?v=JDL8rZzYVwQ](https://www.youtube.com/watch?v=JDL8rZzYVwQ)

I go over:

1. Photorealistic neural rendering
2. Deepfakes for gaming ([https://www.youtube.com/watch?v=RR7u11ANDWE](https://www.youtube.com/watch?v=RR7u11ANDWE) is a better example than the obama one I used)
3. GAN theft auto and dreaming up game engines with neural networks
4. Large language models for building realistic NPCs and storytelling
5. Using OpenAI Codex to automatically program games.

It's really clear that deep learning is the most important technology to impact gaming since the advent of 3D graphics. Would love to talk with anybody who is working on stuff in this space.",MachineLearning
rw2uac,1641325389.0,[D] Style Transfer with Noise Vector,"Hi everyone, I'm looking for a model which can perform style transfer, but also takes an auxiliary noise vector similar to that for StyleGAN to generate many stylized images for a single input image. Is anyone aware of any model meeting these requirements? My best idea so far is to first embed the image into the StyleGAN latent space with [this paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Abdal_Image2StyleGAN_How_to_Embed_Images_Into_the_StyleGAN_Latent_Space_ICCV_2019_paper.pdf), and then add noise to that vector.",MachineLearning
rvzhnh,1641316666.0,"[D] Interpolation, Extrapolation and Linearisation (Prof. Yann LeCun, Dr. Randall Balestriero)","Special machine learning street talk episode! Yann LeCun thinks that it's specious to say neural network models are interpolating because in high dimensions, everything is extrapolation. Recently Dr. Randall Balestriero, Dr. Jerome Pesente and prof. Yann LeCun  released their paper learning in high dimensions always amounts to extrapolation. This discussion has completely changed how we think about neural networks and their behaviour.

In the intro we talk about the spline theory of NNs, interpolation in NNs and the curse of dimensionality.

YT: [https://youtu.be/86ib0sfdFtw](https://youtu.be/86ib0sfdFtw)

Pod: https://anchor.fm/machinelearningstreettalk/episodes/061-Interpolation--Extrapolation-and-Linearisation-Prof--Yann-LeCun--Dr--Randall-Balestriero-e1cgdr0


References:

Learning in High Dimension Always Amounts to Extrapolation \[Randall Balestriero, Jerome Pesenti, Yann LeCun\]
https://arxiv.org/abs/2110.09485

A Spline Theory of Deep Learning \[Dr. Balestriero, baraniuk\] https://proceedings.mlr.press/v80/balestriero18b.html

Neural Decision Trees \[Dr. Balestriero\]
https://arxiv.org/pdf/1702.07360.pdf

Interpolation of Sparse High-Dimensional Data \[Dr. Thomas Lux\] https://tchlux.github.io/papers/tchlux-2020-NUMA.pdf",MachineLearning
rvz50d,1641315758.0,[D] Neural Networks using a generic GPU framework,"I have a (personal) ML project that uses CNNs but I have two little problems: 1. not everyone has a NVidia GPU at home (myself included, sadly); 2. The CNN needs to be trained every time it is used (it's photo to photo style transfer).

So, what would be a good framework to implement the CNN for training (targeting desktop only)? I thought about using OpenGL, but I don't know if using GLSL shaders would be a good fit for it.",MachineLearning
rvyuhb,1641315010.0,[D] What are interviews usually like for ML positions?,"For context, I'm applying for PhD level positions. Should I expect technical interviews including coding challenges similar to SWE?

Any advice on prepping?",MachineLearning
rvw0rv,1641307619.0,[N] Launching DagsHub 2.0 – Git-integrated data labeling and smart ML discussions,"**TL;DR** – DagsHub is integrated with Label Studio, and you can now open datasets from Git and DVC remotes, label them and commit labels back, without doing any DevOps. You can also comment on labels, bounding boxes, or any file. Check out [the example project](https://dagshub.com/DAGsHub-Official/annotations-showcase), or [try out the tutorial](https://dagshub.com/docs/tutorial/label-studio-tutorial/).

[Comparing annotations](https://preview.redd.it/wqzsxhxcuo981.png?width=1688&format=png&auto=webp&s=250cc1940f7d867796d0a871b5f05b4d1151a710)

Hi r/ML! I'm one of the creators of DagsHub ([https://www.dagshub.com](https://www.dagshub.com/)). We help ML practitioners create a central repository for their projects, where they can leverage open-source tools to version datasets and models, track experiments, and starting today – label data, and comment on anything. Like GitHub for machine learning (you probably heard that before, but we mean it).

Our vision is that anyone could jump into an open-source data science project and contribute code, data, labeling, models, experiments via pull request, just like you would with an open-source software project.

We take awesome, popular, open-source tools, and connect them to the place you build data science projects. Lowering the barrier by doing the DevOps work for you and creating a coherent workflow that makes sense for production-oriented teams and our open-source community.

How does it work?

1. Version your code with Git (notebooks also work)
2. Version your data, models, and pipeline with DVC
3. Git push to your DagsHub repo
4. DVC push to your free DVC compatible DagsHub storage, or to your preferred cloud storage which we support
5. You get a zero-config, access-controlled MLFlow API endpoint to log experiments.

We saw that labeling was a big challenge for many community projects, as well as teams in the industry we spoke with. We wanted to make our community’s life a bit simpler, and since many of the challenges were around integrating labeling into the rest of the ML lifecycle, that’s exactly what we built with DagsHub Annotations and the Label Studio integration.

The main additions are:

1. **Zero setup labeling** – we did all the work, so after you push data to DagsHub, just click a button to fire up a labeling instance
2. **Sync data from your Git/DVC remote** – after it’s up you can select what data you want to label, and it will be waiting in your workspace for you. This is something we know [many of you have wanted for a while](https://www.reddit.com/r/MachineLearning/comments/m6eoru/p_label_studio_v10_an_open_source_data_labeling/gr8dmoq/?utm_source=share&utm_medium=web2x&context=3).
3. **Commit back to Git** – when you’re done with a version of labels, we created a “Commit” button that just adds the annotations to Git, making it easy to pull them for model training, or any other need you have while preserving the project history.
4. **Diff and discuss annotations** – you can see annotations on the data itself (check out the example project below), and have discussions on them in a way that preserves context (even comment on bounding boxes on images.

[Committing annotations back to Git](https://i.redd.it/tp56m26olo981.gif)

We created an example project that you can play with to see how it actually looks ([https://www.dagshub.com/DAGsHub-Official/annotations-showcase](https://www.dagshub.com/DAGsHub-Official/annotations-showcase)), and of course, I'd love to hear your thoughts and feedback, and answer any questions you might have. Thank you :)

If you want to read more about the launch, check out the launch blog: [https://dagshub.com/blog/launching-dagshub-2-0/](https://dagshub.com/blog/launching-dagshub-2-0/)",MachineLearning
rvvfoa,1641305969.0,[D] Why is VAE used instead of AutoEncoder in the World Models paper (https://arxiv.org/pdf/1803.10122.pdf)?,"Hi All,

I was just reading this paper and was wondering if we just want to achieve a compact version of the original representation we could just use a traditional AutoEncoder. Is there any specific reason the VAE is used?

Thanks!",MachineLearning
rvqr95,1641289917.0,[R] Play against an AI to detect fake audio,"Hi everybody,

i'm a PhD student interested in audio spoofs (voice recordings faked with the help of AI), and have developed an online game: You play against an artificial intelligence and try to distinguish spoofed from real audio recordings.

It's fun, and very much supports my research. All partificpation (i.e. playing the game), comments or suggestions are welcome!

[https://deepfake-demo.aisec.fraunhofer.de/](https://deepfake-demo.aisec.fraunhofer.de/)",MachineLearning
rvol2l,1641280883.0,"[D] What are the reviewers' score of the submissions nominated for best paper award in top ML conferences such as NeurIPS, ICML, AISTATS, etc.?",I submitted a paper to AISTATS 2022 that can be a breakthrough with outstanding contributions. The paper received 876 scores from reviewers that could only improve to 877 after the rebuttals. What are the chances that our submission enters the short list for best paper recognition? What are the average reviewers' score of the ones getting nominated in these top conferences?,MachineLearning
rvn3vp,1641275602.0,[D]who knows the paper address of the code?,"[https://github.com/yuzisheng/trajectory-compress](https://github.com/yuzisheng/trajectory-compress)

especially the  Spatio-Temporal Curvature Streaming",MachineLearning
rvn3dh,1641275553.0,[P] Sieve: We processed ~24 hours of security footage in <10 mins (now semantically searchable per-frame!),"Hey everyone! I’m one of the creators of [Sieve](https://sievedata.com/), and I’m excited to be sharing it!

Sieve is an API that helps you store, process, and automatically search your video data–instantly and efficiently. Just think 10 cameras recording footage at 30 FPS, 24/7. That would be 27 million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack.

We built this visual demo ([link here](https://sievedata.com/app/query?api_key=AIzaSyAfKwf0tuuNOHbYi_JX-ew_dXH6SzdxZWY)) a little while back which we’d love to get feedback on. It’s \~24 hours of security footage that our API processed in <10 mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario.

To try it on your videos: [https://github.com/Sieve-Data/automatic-video-processing](https://github.com/Sieve-Data/automatic-video-processing)

Visual dashboard walkthrough: [https://youtu.be/\_uyjp\_HGZl4](https://youtu.be/_uyjp_HGZl4)

https://preview.redd.it/bn8hoqoa1m981.png?width=2540&format=png&auto=webp&s=25fb08037438593291fecf7e50ca58ec1f9bea72

https://preview.redd.it/jwkd7uoa1m981.png?width=2540&format=png&auto=webp&s=e25382b4b09855e5934608754a8b74bdbaf93204

https://preview.redd.it/0dd74toa1m981.png?width=2540&format=png&auto=webp&s=05b7625195947b8f15891a9019070efa3730b336

https://preview.redd.it/alg4ruoa1m981.png?width=2540&format=png&auto=webp&s=f5caad143b0d23f3add08f431d0ada322ae4e84d

https://preview.redd.it/8c2pw0pa1m981.png?width=2540&format=png&auto=webp&s=e6438f03e3fc7a00ccdf01c9b7075b9e8752affd",MachineLearning
rvmiyr,1641273656.0,[D] Paper Summary [Rethinking Segmentation from a Sequence-to-Sequence Perspective with Transfromers]," Hi, I have just published my latest medium article. It is a summary of a scientific paper that aims to eliminate the effect of locality which is one of the limitations of CNNs. In this attempt, researchers tried to reform the image semantic segmentation problem then operate a proposed transformer, and finally, introduce three different decoder architectures.
Please read it and give me your feedback. If you find it interesting, you can share it with others who are interested in ML as well. Also, if you find it helpful, you can follow me on medium to be updated on my forthcoming articles.🙂

[https://rezayazdanfar.medium.com/26868efacc52](https://rezayazdanfar.medium.com/26868efacc52)",MachineLearning
rvmeo6,1641273255.0,[D] Which tools can be helpful for annotation of videos for action recognition?,There is a team in my university who work on ergonomics. They want to do action recognition on some videos. They approached me for help. I work on images. I don't have idea about videos. I have dataset. I want to annotate key points in each frame. Please tell me which tools can be helpful for annotation of videos?,MachineLearning
rvadz4,1641239687.0,[R] 🐸YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone,"YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS... it is possible to fine-tune the model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality.

🤖 Demo: [https://coqui.ai](https://coqui.ai/)
👩‍💻 Code: [https://github.com/coqui-ai/tts](https://github.com/coqui-ai/tts)
🚀 Blogpost: [https://coqui.ai/blog/tts/yourtts-zero-shot-text-synthesis-low-resource-languages](https://coqui.ai/blog/tts/yourtts-zero-shot-text-synthesis-low-resource-languages)
📎 Paper: [https://arxiv.org/abs/2112.02418](https://arxiv.org/abs/2112.02418)",MachineLearning
rva1dk,1641238769.0,"[D] ""why academia tends to under-invest in engineering infrastructure?""","Tweet from @jackclarkSF asks an interesting question:

> Is there a good paper that explains how/why academia tends to under-invest in engineering infrastructure?

https://twitter.com/jackclarkSF/status/1478077579110207489",MachineLearning
rv7yos,1641233428.0,[D] How to measure accuracy of kNN Imputation?,"I have a dataset in which the the best way to impute missing values is to use kNN but before I go ahead and do that I'd like to check what kind of accuracy I have with that form of imputation in this specific dataset and which k should be used. My original solution was as follows:

1. From my original dataset, remove all rows with missing values
2. From this dataset, impute NaNs randomly throughout the dataset with the same frequency that they were missing originally and store the values that were replaced with NaN in a new dataset as the ground truth
3. Impute using kNN
4. Check the accuracy of the imputed values against the ground truth values stored in step 2 using MAE for different k values

Is there an easier way to do this? If not, should I be using MAE or another accuracy score?",MachineLearning
rv4u2w,1641225400.0,[D] Paper that mathematically proves that gradient descent can achieve zero training error.,"I think this is a well-known paper, but I have not been able to find it. I am interested in the paper that mathematically proves neural nets can fit any set of datapoints. So far what I have found mostly is papers that show empirically that or something related to that, like [this](https://arxiv.org/pdf/2002.08709.pdf) one. I'd appreciate any help.

Edit: u/the_new_scientist shared this paper which is what I was looking for: [https://arxiv.org/pdf/1810.02054.pdf](https://arxiv.org/pdf/1810.02054.pdf) Also, I apologize for my vague description. Now that the paper is shown, I hope it is more clear to future readers what kind of results I meant, but in case that is not case, I was wondering about this question: under what conditions can a neural network achieve zero training error? and in particular, I am interested in papers with mathematical (even without empirical) results.",MachineLearning
rv4nah,1641224900.0,[D] NLP: Hybridization of statistical approach and expert system ?,"Hi everyone!

I have a question for you. For context, we aggregate on a platform the various AI APIs on the market (GCP, Azure, etc.) and including NLP APIs (keyword extraction, sentiment analysis, NER, etc.). The idea is that a developer doesn't have to create accounts with different providers and can have them all on one API to test, compare and change whenever he wants.

However, many customers ask us how to mix the ""statistical"" approach behind these APIs with expert systems and how to achieve hybridization.

Do you have any idea how to do this?

Thanks,",MachineLearning
rv3yty,1641223088.0,"[P] Bringing serverless to ML - stateful, arbitrary dependency, serverless for ML","Serverless infrastructure is yet practical to use for ML but we think it could bring lots of benefits. So, with a friend, we decided to make serverless easy for ML and we are building a platform to solve the main issues we find in serverless for ML:


\- **Stateful:** We don´t want to reload a whole model every time a user calls model.predict
\- **Arbritary dependecies:** Normal python code with any package dependencies you use and love, just many many times in parallel
\- **Scale-up and scale-down:** scale up with ease and auto shutdown to keep resources consumptionYou can visit our webpage, try the demo, and request early access to use our platform!

Webpage: [https://telekinesis.cloud](https://telekinesis.cloud/)


Happy to receive questions and comments on what we are building!",MachineLearning
rv37yq,1641221013.0,[D] What are your hopes for Machine Learning in 2022?,"Hi r/MachineLearning!

I was just wondering what some of you are hoping ML can accomplish or overcome in this new year - interested in hearing your thoughts!",MachineLearning
rv2j9k,1641219010.0,[R] The Illustrated Retrieval Transformer (GPT3 performance at 4% the size),"Hi r/MachineLearning,

I spent some time wrapping my head around DeepMind's Retro Transformer and visualizing how it works. Hope you find it useful. All feedback is welcome!

[http://jalammar.github.io/illustrated-retrieval-transformer/](http://jalammar.github.io/illustrated-retrieval-transformer/)",MachineLearning
ruz0nc,1641207011.0,[D] What causes feature collapse?,"For those of you unfamiliar feature collapse is when you train a model for classification and the model ends up mapping out-of-distribution data or data of different classes in very close proximity in multi-dimensional space. So for example once your model learns a cluster so to speak for cat, during test it projects a dog into the center of that cluster and classifies it as cat. Some ways to sort of deal with this in CV is double gradient penalty and spectral norm of resnet blocks, but **what causes feature collapse?**",MachineLearning
ruyk5i,1641205194.0,"[R] New paper: ""A relational Tsetlin machine with applications to natural language understanding""","&#x200B;

[Relational Tsetlin Machine](https://preview.redd.it/rdy7550y7g981.png?width=1446&format=png&auto=webp&s=ad52834243e2e1f41551d44f6a018edadecf028b)

The paper introduces the first Relational #TsetlinMachine, which reasons with relations, variables, and constants. The approach is based on first-order logic and Herbrand semantics, taking the first steps toward the computing power of a universal Turing machine. The approach can take advantage of logical structures appearing in natural language, to learn rules that represent how actions and consequences are related in the real world. The outcome is a logic program of Horn clauses, bringing in a structured view of unstructured data. In closed-domain question-answering, the first-order representation produces 10× more compact knowledge bases, along with an increase in answering accuracy from 94.83% to 99.48%. The approach is further robust towards erroneous, missing, and superfluous information, distilling the aspects of a text that are important for real-world understanding. [https://link.springer.com/article/10.1007/s10844-021-00682-5](https://link.springer.com/article/10.1007/s10844-021-00682-5) [\#ML](https://twitter.com/hashtag/ML?src=hashtag_click) [\#AI](https://twitter.com/hashtag/AI?src=hashtag_click) [\#NLP](https://twitter.com/hashtag/NLP?src=hashtag_click) [\#MachineLearning](https://twitter.com/hashtag/MachineLearning?src=hashtag_click) [\#Logic](https://twitter.com/hashtag/Logic?src=hashtag_click) [\#Relational](https://twitter.com/hashtag/Relational?src=hashtag_click)",MachineLearning
ruwidq,1641196825.0,[D] How to deal with huge Categorical data,"I have a dataset that already contains about 55 columns and out of this, around 10 Columns or so have categorical data in it. If I were to OneHotEncode them, I will end up having a column count of more than 300. Is this something advisable? How do you people deal with such huge number of columns? I mean 300 columns is not a big deal, but I would like to know your opinion and thoughts on this.",MachineLearning
ruwchh,1641196174.0,[D] Spotted this post in LessWrong. Can anyone verify the rather fantastic claims being made here?,"https://www.lesswrong.com/posts/rCP5iTYLtfcoC8NXd/self-organised-neural-networks-a-simple-natural-and#Roadmap

The writing has some red flags, but it looks interesting enough. Having some trouble with my gpu drivers so I can't run it right now.",MachineLearning
rut9hs,1641185228.0,[D] Is there flow-based method which treats input data as different lengths each?,"Hello. I am searching the researches that different size of data are generated through the flow-based network, not super-resolution task such as continuous mapping.

I want to generate output as time-aligned scalar data, for example

&#x200B;

Input: noise sampling (B x T x C)

Output: scalar data (B x T, C=1)

&#x200B;

with introducing the variational data augmentation technique (in vFlow, which can output high-dimensionality as concatenate noise vector for input and output both) for output.

But there's a problem time dimension T is different for each of all data input. How can I treat this problem?

&#x200B;

p.s. I am very appreciate if I can read the flow-based research in NLP task.",MachineLearning
rur95m,1641178986.0,[D] Anyone switched from vision to robotics?,"I’m about to finish my PhD and the whole field of robotics looks so exciting right now, especially applications like farming and recycling. Has anyone switched from more pure deep learning (vision / NLP) to robotics and how did it happen?

Did you just get a robotics related job focusing on the vision side of things or is it key to have more experience on the robotics side before getting a job?

Also I’m curious what’s the best location for robotics? Like how you go to Hong Kong / New York for finance, SF for software or Shenzhen for hardware.",MachineLearning
rur2j3,1641178454.0,[P] I like YOLOv5 but the code complexity is...,"I like YOLOv5 but the code complexity is...

I can't deny that YOLOv5 is a practical open-source object detection pipeline.
However, the pain begins when adding new features or new experimental methods. Code dependencies are hard to follow which makes the code difficult to maintain.
We wanted to try various experimental methods but hate to write one-time code that is never re-used.

So we worked on making an object detection pipeline to have a better code structure so that we could continuously improve and add new features while easy to maintain.

https://github.com/j-marple-dev/AYolov2

And we applied CI(Formating, Linting, Unittest) to ensure code quality with Docker support for development and inference. Our Docker supports the development environment with VIM.

Our code design from the beginning was to try various experimental methods with fewer efforts. The features so far developed are as follows.

1. You can easily use the trained model for another project without code copy and paste.
PyTorch requires model code to use the model. We build the model by the library that builds the PyTorch model from the YAML file (https://github.com/JeiKeiLim/kindle). So the trained model is portable with pip install kindle.
2. Model compression support by tensor decomposition and pruning.
3. Export model to TorchScript, ONNX, and TensorRT
4. Inference with TorchScript and TensorRT
5. (WIP) C++ Inference with TorchScript and TensorRT
6. Auto search for NMS parameter
7. (WIP) Knowledge distillation support
8. (WIP) Representation learning support

AYolov2 also supports W&B with model upload and load function to make trained models easy to manage.

`python3 val.py --weights j-marple/AYolov2/179awdd1 `

For instance, the above single command line will download the trained model from W&B and run the inference.

By the time you read here, you might wonder why the name is AYolov2. AYolov2 comes from Auto-yolo v2. Our initial goal was to implement an auto model architecture search.  And v2 represents that there was v1. Where did v1 go? We have built an auto model architecture search based on the original yolov5 and it worked pretty nice but it became unmanageable. Please stay tuned NAS feature will be coming soon.

If you have any suggestions or feedback, any kind will be appreciated.

Thank you and happy new year!",MachineLearning
ruofql,1641171033.0,[D] GUI-based Machine Learning applications?,"I was previously using Azure Machine Learning Studio(classic), and of course, it was discontinued last month. Any other free ML applications?

The new Azure Machine Learning Studio isn't free, and this is a school project so I'm aiming for free and simple.

Any suggestions? Or maybe someone else is using Studio(classic) and knows a way around this?",MachineLearning
ruja9s,1641157205.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 128,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|121-130|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)|[Week 121](https://reddit.com/pmzx3g)|||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)|[Week 122](https://reddit.com/pw14z5)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)|[Week 123](https://reddit.com/q5fi12)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)|[Week 124](https://reddit.com/qjxfu9)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)|[Week 125](https://reddit.com/qtzbu1)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)|[Week 126](https://reddit.com/r4e8he)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)|[Week 117](https://reddit.com/omy345)|[Week 127](https://reddit.com/rez90o)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)|[Week 118](https://reddit.com/ovz52j)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)|[Week 119](https://reddit.com/p50knh)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)|[Week 120](https://reddit.com/pe2idh)||

Most upvoted papers two weeks ago:

/u/CatalyzeX_code_bot: [Paper link](https://arxiv.org/abs/2012.09841)

/u/rakshith291: https://www.reddit.com/user/rakshith291/draft/d569ed32-6025-11ec-b62a-2ae7cf80f48b

/u/rakshith291: https://rakshithv-deeplearning.blogspot.com/2021/12/neurips-2021-curated-papers-part2.html

Besides that, there are no rules, have fun.",MachineLearning
ruj3ja,1641156692.0,[D] ICLR 2022 Open Discussion Quality,"First time submitting to ICLR, I'm wondering how is the open discussion on openreview.net really different from the review-rebuttal procedure used in other conferences.

For the papers I'm reviewing, about half the reviewers reacted to the authors' responses (clarifications, modifications, additional experiments, etc.). As to my submission (5568), I run extra experiments and answered each of the concerns directly but received 0 feedback from the reviewers.

As a reviewer, I think it doesn't matter if the rebuttal changes your mind about the quality of the submission but it's very basic manner to reply to the authors' responses. A simple ""Thank the authors for the responses but I don't think these addressed my concerns"" would work. Saying nothing only means you are uncertain if the responses make sense and you just doesn't care to figure it out. The authors spent a whole week running experiments to answer some of your questions and if you don't give \*\*\*\* about their responses, just keep the questions with you and don't submit your review.

I was hoping for a different experience submitting to ICLR and then I realized the ""discussion"" is basically broken.",MachineLearning
rud2m5,1641140624.0,[P] Tensorflow / Keras implementation of Vision Transformer https://arxiv.org/abs/2010.11929v2,"An Image is Worth 16x16 Words: ViT Excellent results compared to SOTA CNNs while requiring fewer computational resources to train.

Paper : https://arxiv.org/abs/2010.11929v2
Code : https://github.com/avinash31d/paper-implementations",MachineLearning
rucjmx,1641139214.0,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning
ru91o8,1641127978.0,"[D] Paper Explained & Author Interview - Player of Games: All the games, one algorithm! (Video Walkthrough)","[https://youtu.be/U0mxx7AoNz0](https://youtu.be/U0mxx7AoNz0)

Special Guest: First author Martin Schmid ([https://twitter.com/Lifrordi](https://twitter.com/Lifrordi))

Games have been used throughout research as testbeds for AI algorithms, such as reinforcement learning agents. However, different types of games usually require different solution approaches, such as AlphaZero for Go or Chess, and Counterfactual Regret Minimization (CFR) for Poker. Player of Games bridges this gap between perfect and imperfect information games and delivers a single algorithm that uses tree search over public information states, and is trained via self-play. The resulting algorithm can play Go, Chess, Poker, Scotland Yard, and many more games, as well as non-game environments.

&#x200B;

OUTLINE:

0:00 - Introduction

2:50 - What games can Player of Games be trained on?

4:00 - Tree search algorithms (AlphaZero)

8:00 - What is different in imperfect information games?

15:40 - Counterfactual Value- and Policy-Networks

18:50 - The Player of Games search procedure

28:30 - How to train the network?

34:40 - Experimental Results

47:20 - Discussion & Outlook

&#x200B;

Paper: [https://arxiv.org/abs/2112.03178](https://arxiv.org/abs/2112.03178)",MachineLearning
ru7k5y,1641121715.0,[D] Machine Learning Research,"Hi everyone, I've compiled the trusted sources of ideation based on top-tier conferences on Machine Learning and Deep Learning worldwide. This repository includes datasets, tasks, state-of-the-art and more.

[Repository GitHub](https://github.com/tuanlda78202/MLR)

https://preview.redd.it/9p1jkei8c9981.png?width=1000&format=png&auto=webp&s=906c20c58b5ee569b25848a7fbf0b41ca4caf354",MachineLearning
ru7drf,1641120935.0,"[P] Quick-Deploy - Optimize, convert and deploy machine learning models","Hello Reddit, releasing one of my OSS projects: Quick-Deploy ..

github: [https://github.com/rodrigobaron/quick-deploy](https://github.com/rodrigobaron/quick-deploy)

blog post: [https://rodrigobaron.com/posts/quick-deploy](https://rodrigobaron.com/posts/quick-deploy)

&#x200B;

It's in the very early stage, feel free to contribute or give a star 🙂",MachineLearning
ru70fv,1641119273.0,[D] Raising errors while using accelerators,"Why is it so hard to raise exceptions when ML pipeline is using a GPU?

When for example you make a classic *""Index out of bounds""* error, in libraries like PyTorch you get some generic ""CUDA"" error and you can't see the exact error until you transfer the tensors explicitly to CPU and rerun the code.

Do you think there is a possibility for this to improve in the future?

Sorry if this is more CS-related question",MachineLearning
ru5xw8,1641114571.0,[D] Are NN actually overparametrized?,"I often read that NN or CNN are overparametrized. But, for example, resnet18 has 11M parameters while cifar10 has 50k*32*32*3=153M data points. How is that be an overparametrized network on cifar10? Or even on mnist which has 60k*28*28=47M data points",MachineLearning
ru06dy,1641093453.0,[D] Coding Practices,"My job is to work with ML engineers and provide them with whatever they need to experiment with/train/test/deploy ML models -- GPU infrastructure, distributed training support, etc. When I interface with their code, I almost always find it so poorly written, with little to no thought given to long-term stability or use -- for code that they 100% know is going to production.

They're brilliant people, far smarter than me, and really good at what they do, so it's not a matter of them not being good enough. I feel (from my very limited experience, so I'm happy to be wrong) like ML engineers are incentivized to write poor code. The only metric for evaluation seems to be accuracy, loss, and all the plots that come up. In research, I understand completely, that's where the focus lies, but in industry? I've seen many models perform poorly because the code is so hard to read and refactor that big issues remained unspotted for months together. And this is especially befuddling because for a field that is completely fine with spending months to get an ROI of single digit increases in model performance metrics during the experimentation phase, they don't seem to care about anything that might go wrong in production. That just feels like a fundamental disconnect, since without the core ML stuff working perfectly, none of the other stuff (like what I do) has any value -- and even so, I'm taught to hold my code to a much higher standard than the critical stuff -- which I'm happy about since I can now write production code by default -- but it's just... weird. Like the vending machines at a nuclear power plant being better engineered than the reactor.

Is this a common problem or is this a localized issue that I'm facing?",MachineLearning
rtukp2,1641076823.0,[D] Plug or Integrate a GNN Pytorch code base into Spark Cluster,Does anyone have a better explanation or resources to share for plug or Integrate a Pytorch based GNN models into Pyspark or similar cluster services?,MachineLearning
rtrbso,1641067758.0,"""[R]"" Neuron outputs as weights",[https://stats.stackexchange.com/questions/558864/what-if-weights-of-model-is-output-of-neurons](https://stats.stackexchange.com/questions/558864/what-if-weights-of-model-is-output-of-neurons),MachineLearning
rtndgm,1641056502.0,[D] Best Practices in Machine Learning,"This is a non-profit that promotes best practices in machine learning, specifically for responsible ML. The practices are open source too, which is cool.

Link here: https://www.fbpml.org/the-best-practices

I think their technical best practices seems a little stronger than the organisational ones. Thoughts?

** this is their LinkedIn URL: https://www.linkedin.com/company/the-foundation-for-best-practices-in-machine-learning",MachineLearning
rt1vfy,1640980539.0,[P] Play around with StyleGAN2 in your browser,"I built a little page to run and manipulate StyleGAN2 in the browser.

https://ziyadedher.com/faces

It was pretty fun learning about ONNX and how to port GANs to web. You can play around with the random seeds and also distort the intermediate latents to produce some really wacky results. You can check out a [GIF on Twitter](https://twitter.com/ziyadedher/status/1476728367827144704).

Let me know if you come up with anything cool!",MachineLearning
rsv2o4,1640960968.0,[D] Machine learning alternative to MCMC or Nested Sampling?,"For work I'm regularly in the position of having to fit complex models to datasets. For the most part it involves defining some kind of likelihood function (merit function) that uses a certain set of parameters to find the distance between a model and data. Currently, the standard methods for this in my field are usually MCMC (and their variants) and Nested Sampling.

MCMCs and NS are a robust and safe way to find the global minimum/maximun, but in high-dimensional parameter spaces they become very slow. I'm right now running a (possibly) month long NS chain, just for an exploratory test (in a 32 cores PC). While a month-long run could be OK-ish for a definitive result, it is not feasible for exploratory exercises (I really can't do one each time I tweak a thing).

Is there any fast/efficient alternative to MCMC or Nested sampling out there? Something that, while might not be as safe as Nested Sampling, could give me a good estimate of the best set of parameters in a significantly shorter run?",MachineLearning
rsstqr,1640953445.0,[P] Top arXiv Machine Learning papers in 2021 according to metacurate.io,"With 2021 almost in the books (there are still a couple of hours to go at the time of this writing), here are the top machine learning papers per month from the arXiv pre-print archive as picked up by [metacurate.io](https://metacurate.io/) in 2021.

# January

1. [Can a Fruit Fly Learn Word Embeddings?](https://arxiv.org/abs/2101.06887)
2. [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)
3. [Muppet: Massive Multi-task Representations with Pre-Finetuning](https://arxiv.org/abs/2101.11038)

# February

1. [How to represent part-whole hierarchies in a neural network](https://arxiv.org/abs/2102.12627)
2. [Patterns, predictions, and actions: A story about machine learning](https://arxiv.org/abs/2102.05242)
3. [Fast Graph Learning with Unique Optimal Solutions](https://arxiv.org/abs/2102.08530)

# March

1. [Fast and flexible: Human program induction in abstract reasoning tasks](https://arxiv.org/abs/2103.05823)
2. [Learning to Resize Images for Computer Vision Tasks](https://arxiv.org/abs/2103.09950)
3. [The Prevalence of Code Smells in Machine Learning projects](https://arxiv.org/abs/2103.04146)

# April

1. [Retrieval Augmentation Reduces Hallucination in Conversation](https://arxiv.org/abs/2104.07567)
2. [Getting to the Point. Index Sets and Parallelism-Preserving Autodiff for Pointful Array Programming](https://arxiv.org/abs/2104.05372)
3. [NICE: An Algorithm for Nearest Instance Counterfactual Explanations](https://arxiv.org/abs/2104.07411)

# May

1. [Are Pre-trained Convolutions Better than Pre-trained Transformers?](https://arxiv.org/abs/2105.03322)
2. [Content Disentanglement for Semantically Consistent Synthetic-to-Real Domain Adaptation](https://arxiv.org/abs/2105.08704)
3. [KLUE: Korean Language Understanding Evaluation](https://arxiv.org/abs/2105.09680)

# June

1. [Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers](https://arxiv.org/abs/2106.15195)
2. [Time-Aware Language Models as Temporal Knowledge Bases](https://arxiv.org/abs/2106.15110)
3. [Multiplying Matrices Without Multiplying](https://arxiv.org/abs/2106.10860)

# July

1. [DeepTitle — Leveraging BERT to generate Search Engine Optimized Headlines](https://arxiv.org/abs/2107.10935)
2. [Demystifying Neural Language Models’ Insensitivity to Word-Order](https://arxiv.org/abs/2107.13955)
3. [Reading Race: AI Recognises Patient’s Racial Identity In Medical Images](https://arxiv.org/abs/2107.10356)

# August

1. [Mitigating dataset harms requires stewardship: Lessons from 1000 papers](https://arxiv.org/abs/2108.02922)
2. [Program Synthesis with Large Language Models](https://arxiv.org/abs/2108.07732)
3. [How to avoid machine learning pitfalls: a guide for academic researchers](https://arxiv.org/abs/2108.02497)

# September

1. [Physics-based Deep Learning](https://arxiv.org/abs/2109.05237)
2. [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)
3. [Machine-Learning media bias](https://arxiv.org/abs/2109.00024)

# October

1. [Learning in High Dimension Always Amounts to Extrapolation](https://arxiv.org/abs/2110.09485)
2. [Non-deep Networks](https://arxiv.org/abs/2110.07641)
3. [lambeq: An Efficient High-Level Python Library for Quantum NLP](https://arxiv.org/abs/2110.04236)

# November

1. [GFlowNet Foundations](https://arxiv.org/abs/2111.09266)
2. [Rebooting ACGAN: Auxiliary Classifier GANs with Stable Training](https://arxiv.org/abs/2111.01118)
3. [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)

# December

1. [Player of Games](https://arxiv.org/abs/2112.03178)
2. [Linear algebra with transformers](https://arxiv.org/abs/2112.01898)
3. [ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/abs/2112.12731)

# About metacurate.io

[metacurate.io](http://metacurate.io/) continuously reads a [number of sources](https://metacurate.io/sources/newsletters/) on AI, machine learning, NLP and data science. It then aggregates the links to stories therein, and scores them according to their social score, that is the number of shares, likes, and interactions in social media for the 5 days after they’ve entered the system. metacurate.io retrieved 240,000+ links in 2021, 1,124 of which were links to arXiv papers published last year.",MachineLearning
rsqktc,1640944527.0,[D] What are the recent breakthroughs for the generative models for art?,"Hey everyone,

I am interested in the recent developments or models in the area of Neural Style Transfer (I am also interested in other applications in art, not only Neural Style Transfer, so feel free to point those as well). What are the mostly used models in this area nowadays? Can you point me to the papers?

Thanks a lot!",MachineLearning
rsn8mc,1640931308.0,"[D] MSE Difference Magnitude in ""Tabular Data: Deep Learning is Not All You Need""","I'm reading the paper ""*Tabular Data: Deep Learning is Not All You Need*"" and looking at the differences in MSE between the various DL architectures and XGBoost and the ensembles tested.

Table 2 in particular, where the YearPrediction and Rossman data sets comparisons are shown as MSE.  Frankly, these values all seem fairly close, and some of them are very close.  For example the \*Deep Ensemble with XGBoost\* does best on YearPrediction with an MSE of 76.19, while \*NODE\* is really close at 76.39.  Several others are within a couple of points of these.

Are the differences really that great?.  Given that the error is squared, I would guess not (unless the error < 1?).  I don't know exactly if the 76 is bad or good itself, but 76 vs 78 seems close enough that I'm not sure the DL + XGBoost ensemble is worth the trouble?

I suppose that the correct answer would be ""It depends, on your need for accuracy vs cost of training, etc."", but I'm trying to get an intuition here.  If I were to look at these, some of these DL models seem pretty close to XGBoost, and even the furthest don't seem to be that bad, at 83 vs 78.

Am I missing something?

The same question applies to the cross-entropy comparisons, but I am less familiar with that metric.",MachineLearning
rsn858,1640931259.0,[D] Drop your best open source Deep learning related Project,"Would anyone like to share their open source projects in deep learning, federated learning, blockchain+deep learning, Distributed job or parallel scheduling based on PyTorch or Tensoflow to look at!",MachineLearning
rsm8st,1640927829.0,[D] Bayesian evidence calculation with normalizing flows,Is there any way to calculate evidence (marginal likelihood or model evidence) using normalizing flows? I usually calculate it using nested sampling algorithms and was wondering if there was an ML alternative.,MachineLearning
rsjil0,1640919024.0,[D] A pretty extensive 6 part blog series on AI accelerators,"https://twitter.com/IAmAdiFuchs/status/1472905719213182979?t=YbQW8BW4TMg1hB99F_I66Q&s=19

Article: https://medium.com/@adi.fu7/ai-accelerators-part-i-intro-822c2cdb4ca4

I found this to be a pretty extensive and solid resource on the dizzying array of specialized hardware we see nowadays in ML.",MachineLearning
rs7juu,1640886463.0,[D] Is Rust stable/mature enough to be used for production ML? Is making Rust-based python wrappers a good choice for performance heavy uses and internal ML dependencies in 2021?,"Hi.

I'm an ML engineer, and while our team is using Python extensively, and most of the code we're using are libraries with C/C++ backened, there are a couple of specific situations that are happening right now that we need to really write some ""non-python"" dependencies/libraries, to get far better performance.

In most cases, people will blindly jump into writing C/C++ but I wanted to see, would it make sense to go for a rust backend? Is rust mature enough? I've seen huggingface writing their tokenizers in rust, and those are some badass tokenizers, and that was my main inspiration of asking this question. If you've gone this route, what were your hurdles? What should I look out, and what should I look forward too? Was packaging rust and python hard? Was dockerizing/packaging the environment hard, and it made model deployment challenging?

Thank you!",MachineLearning
rs6k65,1640883921.0,[D] Most important metrics for labeling data,"There are a lot of great articles about measuring the performance of data annotators’ agreement on labels, like this one [https://towardsdatascience.com/the-definite-guide-for-creating-an-academic-level-dataset-with-industry-requirements-and-6db446a26cb2](https://towardsdatascience.com/the-definite-guide-for-creating-an-academic-level-dataset-with-industry-requirements-and-6db446a26cb2).

I see mentions in a lot of places of Cohen’s Kappa/Krippendorf’s alpha, Fleischer’s Kappa, Comparing to predefined ground truth, etc.

If you’re managing an annotation process in your organization, how do you evaluate your annotators, and what challenges have you faced in the process?

As a side note, is anyone using programmatic labeling in a real dataset? Thoughts?",MachineLearning
rs6bu8,1640883353.0,"[R] Does anyone know of any databases of political text which are labelled with the classes 'conservative', 'liberal' and 'neutral' for supervised learning?",I'd like to fine tune Google's BERT natural language processing machine learning model to a political domain. Many thanks.,MachineLearning
rs65ei,1640882872.0,[P] Adapting Class Activation Maps for Object Detection and Semantic Segmentation,"Hi r/MachineLearning,

[https://github.com/jacobgil/pytorch-grad-cam](https://github.com/jacobgil/pytorch-grad-cam) is a project that has a comprehensive collection of Pixel Attribution Methods for PyTorch (like the package name grad-cam that was the original algorithm implemented).

Class Activation Maps can help diagnose properties about the model predictions, like ""where does the model see a cat in the image"".

After many requests I added support for Object Detection and Semantic Segmentation, and wanted to share this with you.

Here you can find detailed notebook tutorials about this:

* [A tutorial on using Class Activation Maps for Object Detection](https://github.com/jacobgil/pytorch-grad-cam/blob/master/tutorials/Class%20Activation%20Maps%20for%20Object%20Detection%20With%20Faster%20RCNN.ipynb)
* [A tutorial on using Class Activation Maps for Semantic Segmentation](https://github.com/jacobgil/pytorch-grad-cam/blob/master/tutorials/Class%20Activation%20Maps%20for%20Semantic%20Segmentation.ipynb)

&#x200B;

[Computing the CAM for object detection](https://preview.redd.it/kiiqmfgchp881.png?width=224&format=png&auto=webp&s=2ea8dda8cb4c524fdd49cbcaa29c5ffdf1f2e661)

[Computing the CAM for semantic segmentation](https://preview.redd.it/rl61zbgchp881.png?width=500&format=png&auto=webp&s=5db103e2b5e9ec0a147fb50a240998fbfb8f03c0)

## The problem

Class Activation Maps are usually researched and applied for classification models.

A repeating request in this repository, and also in some object detection projects, was to add support for grad-cam for object detection.

One challenge with this, is that object detection frameworks typically don't output tensors you can back-propagate through to compute gradients.

They typically output dictionaries with bounding boxes, labels, etc, after a lot of processing, and don't expose any way to compute gradients with respect to those detections.

If you want to compute CAMs for them, you typically have to dive into the code of these object detection packages and create solutions that work only with them.

There was no ""generic"" tool that just works and can be adapted to new object detection models.

## The solution - gradient free methods

Some Class Activation Map methods don't depend on computing the gradients. Examples of these:

* EigenCAM, computes PCA on the activations and returns the first principle component.
It's very fast since it requires a single forward pass, but it doesn't have good enough ""class discrimination"" in case you might have several different objects in the same bounding box.
* AblationCAM, ablates individual activations and measures how the output score drops.
This is a SOTA method with class discrimination, but it's much slower since it requires many forward passes for doing the ablations. Object detection networks are already heavy, and ablating these activations makes it slower.
In practice many of the activations don't contain useful information at all, and if we can identify those we can just skip ablating them. I tried coming up a heuristic that computes a binary mask for where the objects might be based on comparing EigenCAM with a low threshold , and then scoring the activations according to how much of their pixels values fall inside the mask. Then we can control the ratio of activations we actually want to ablate, where a lower ratio makes it faster.
This seems to give good results with dramatic run time reductions.

We can use these methods applied on the activations from the Feature Pyramid Network in object detection networks, to adapt them for Object Detection.

## Custom CAM target functions for object detection / segmentation

A CAM is computed to target some property about the image, like ""What parts of the image is important for the dog category"".

We can adapt this for object detection by targeting properties like ""What parts of the image are important to get a high IOU with the original bounding box detections and score high on the same categories"".

You can similarly adapt this for segmentation, by asking questions like ""what pixels in the image are important for predicting the car pixels"".

&#x200B;

I hope you find this useful, and that this will be a good starting point towards applying CAM methods more in practice and in production, for monitoring and diagnostics of vision models.",MachineLearning
rs3qef,1640876447.0,[R] Neural pseudo random number generator,"Let's say we have a neural net that generates numbers called G and a neural net predictor called P. G's goal is to not let P guess what it generates and P's goal is to approximate G. ie. G maximizes the difference between G and P's outputs while P minimizes it.

Are there any papers that explores this kind of setup?",MachineLearning
rs21jj,1640871616.0,"[D] Are there good attempts at recognizing ASL/other sign languages, beyond simple static signs like numbers and letters?","I've seen a lot of projects aimed at recognizing the ASL alphabet, but a lot of sign language depends on movement. What's the current state of actual sign language recognition/translation? I haven't been able to find anything online.",MachineLearning
rrydgm,1640858960.0,[P] Ecco - Language model analysis and visualization toolkit,"Hi r/MachineLearning,

Over the last couple of years we've been building this toolkit to explore and visualize Transformer language models. It brings together a wide variety of tools and methods to visualize and analyze model inner workings and activation spaces.

**Features:**

* Support for a wide variety of language models (GPT2, BERT, RoBERTA, T5, T0, and others).
* Ability to add your own **local models** (if they're based on Hugging Face pytorch models).
* **Feature attribution** (IntegratedGradients, Saliency, InputXGradient, DeepLift, DeepLiftShap, GuidedBackprop, GuidedGradCam, Deconvolution, and LRP via Captum)
* **Capture neuron activations** in the FFNN layer in the Transformer block
* Identify and **visualize neuron activation patterns**  (via Non-negative Matrix Factorization)
* Examine neuron activations via comparisons of activations spaces using SVCCA, PWCCA, and CKA
* Visualizations for:
   * Evolution of processing a token through the layers of the model ([Logit lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens))
   * Candidate output tokens and their probabilities (at each layer in the model)

&#x200B;

We just released v0.1.0 which brings encoder-decoder model support (T5, T0), feature attribution via Integrated Gradients (and many other methods provided by Captum).

GitHub: [https://github.com/jalammar/ecco](https://github.com/jalammar/ecco)

Paper: [https://aclanthology.org/2021.acl-demo.30/](https://aclanthology.org/2021.acl-demo.30/)",MachineLearning
rrxzyy,1640857572.0,[D] Machine learning use cases in telecom industry,"Apart from churn prediction, customer segmentation and anomaly detection, how is data science used in the telecommunications sector? are you aware of industry/research use cases in that field?",MachineLearning
rrxikg,1640855835.0,[P] Mel Frequency Cepstral Coefficients Transformation,"Hi,

I need some guidance on my project. I work on a simple keyword spotting problem with spiking neural networks (SNN) and for that I use the Akida BrainChip. Now the catch is that the  SNN layers can only work with unsigned integers and the MFCC functions from librosa, audio\_ops etc. are returning float. Since I'm not an expert on audio data I wanted to ask if there is simple or common way in which you usually transform MFCC's to integers.

Currently, I shift the data such that the minimum is the new zero point  and simply cast it to integer afterwards. But I'm not really happy with that approach.",MachineLearning
rrrjrz,1640835802.0,[D] Is spectral clustering still useful?,"My impression is that before the Deep Learning revolution, spectral clustering was an incredibly powerful tool.


With the explosion in unsupervised deep learning techniques, I was wondering if anyone still uses spectral clustering. Do they? If so, in what settings would I expect spectral clustering to be state of the art?",MachineLearning
rrcav2,1640795917.0,[D] I made a modern data catalog tool for anyone using a word document or excel sheet as a data catalog. I’m curious if anyone would like to try it out.,"At my old job, it seemed like I'd have a new project with a new dataset every few weeks. The hardest part of my job was understanding the data not completing the project.

Last year, I built a data catalog using the no-code platform bubble and shared it here. We ended up with quite a few people testing it out and using it on personal projects. In the last 12-months, I took the original platform I built and leveraged some open-source platforms like Amundsen to rebuild a modern data catalog focused on making data documentation transparent, collaborative, and straightforward for anyone or company.

We have a sandbox environment with dummy data that we're looking for user feedback on. If anyone is interested in giving it a spin, please let me know! We're planning to release a public version for anyone to use early next year.

Happy New Year, and I appreciate anyone willing to give it a try.",MachineLearning
rr95gq,1640787423.0,[P] Semantic search with finetune in realtime using CLIP,"Here is a personal project I made just after the release of CLIP almost an year ago:

[https://github.com/brunodoamaral/clip-search](https://github.com/brunodoamaral/clip-search)

I provides a web front-end to search images within a local folder. It supports search by text or images (just drag and drop).

I think that the most interesting feature is the ""more like this"" button. You can finetune your input query in real time!

It still lacks some functions, like save the results or export the finetuned embedding... But I'm planning to add these features in the next weeks.

Any other ideas? Comments are welcome!",MachineLearning
rr8key,1640785646.0,[P] I wrote a program with OpenAI's Codex that fixes errors,"&#x200B;

https://i.redd.it/t5ojoemdkh881.gif

You can find the program on GitHub: [https://github.com/tom-doerr/fix](https://github.com/tom-doerr/fix).
The AI generates mostly wrong solutions, but enough of the generated solutions are actually working for it to be useful. It already helped me installing the right dependencies during a docker build when even many desperate Google searches didn't help me.
What do you think?",MachineLearning
rr17f9,1640758819.0,[P] 4.5 times faster Hugging Face transformer inference by modifying some Python AST,"Recently, 🤗 Hugging Face people have released a commercial product called Infinity to perform inference with very high performance (aka very fast compared to Pytorch + FastAPI deployment). Unfortunately it’s a paid product costing 20K for one model deployed on a single machine (no info on price scaling publicly available) according to their product director.

[Transformer-deploy](https://github.com/ELS-RD/transformer-deploy) is an open source alternative build over enterprise-grade softwares:

* Inference server: Nvidia Triton (it takes queries and passes them to an engine, plus adds features useful for inference like dynamic batching, or multi inference engine dispatching)
* Inference engines: Microsoft ONNX Runtime (for CPU and GPU inference) and Nvidia TensorRT (GPU only)

It appears that without much effort,[ it was easy to match the very few HF Infinity public benchmarks](https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c?source=friends_link&sk=cd880e05c501c7880f2b9454830b8915).

But there was still an opportunity to push inference performances further that, AFAIK, is not yet leveraged by any other OSS project: GPU quantization for all Transformer models!

Please find below our measures on Roberta-base, seq len 256, batch 32, MNLI dataset (classification):

https://preview.redd.it/hv87lknscf881.png?width=585&format=png&auto=webp&s=db52143c1d108aa026ca847d02c64625dec2375e

Source code: [https://github.com/ELS-RD/transformer-deploy/blob/main/demo/quantization/quantization\_end\_to\_end.ipynb](https://github.com/ELS-RD/transformer-deploy/blob/main/demo/quantization/quantization_end_to_end.ipynb)

repo: [https://github.com/ELS-RD/transformer-deploy](https://github.com/ELS-RD/transformer-deploy)

Performing GPU quantization requires modifying model source code (to add some specific nodes called QDQ on costly operations like matrix multiplication), which is both error prone, boring and a good generator of technical debts (you maintain yourself the source code of your modified model). We have done that work manually for several models, and it appeared to us that it can be made automatically by just patching the model module abstract syntax tree (aka the source code).

On the user end, performing basic quantization of a model on the GPU looks like:

https://preview.redd.it/j6ip416qcf881.png?width=843&format=png&auto=webp&s=bee70158f20785f76b9f2c1000878f41be5aa280

As shown in the benchmark, to get a model 4.5 times faster than vanilla Pytorch, it costs 0.4 accuracy point on the MNLI dataset, which is in many cases a reasonable tradeoff. It’s also possible to not lose any accuracy, the speedup will be around 3.2 faster. Of course the exact trade-off depends on the model, the dataset, etc. but it gives a basic idea. It’s a big improvement compared to a previous version of this project where speedup was costing over 1 accuracy point.

Behind the scene, transformer source code is parsed to AST, and operators like matmul or LayerNorm are wrapped by a quantizer, Linear layers are replaced by quantized versions of them, some TensorRT unsupported operators are replaced, etc. Then new source code replaces, in RAM, the original one.

Right now we have successfully tested the process with Albert, Bert (including miniLM), Distilbert, Roberta (including Camembert, XLM-R, DistilRoberta, etc.), Electra. It should work out of the box or with very little effort for any transformer model which can be exported to ONNX format.

Regarding CPU inference, quantization is very easy, and supported by [Transformer-deploy](https://github.com/ELS-RD/transformer-deploy) , however performance on transformer are very low outside corner cases (like no batch, very short sequence and distilled model), and last Intel generation CPU based instance like C6 or M6 on AWS are quite expensive compared to a cheap GPU like Nvidia T4, to say it otherwise, on transformer, until you are ok with slow inference and takes a small instance (for a PoC for instance), CPU inference is probably not a good idea.",MachineLearning
rr0q7h,1640757212.0,[D] Best resources or tools to draw nicer table for comparing different models/frameworks performance,"Hi all, would you like to suggest or share some resources to draw the table to compare different deep learning model like MInst, cifar so on with it's performance. I was thinking of Latex but the way I was trying to do it, its not the exact like what I was looking exactly, hence looking for some suggestions to make it nicer actually.",MachineLearning
rqycwh,1640749758.0,[D] Which pretrained models are useful for getting low level image data?,"There are quite a few popular and open source models used for getting image embedding which represents high level semantic meaning. I would like to know if there are any models which can help encode low level image details like style of an image (eg pixel art, sepia, vaporwave), patterns, prominent shapes, color distributions etc.

I am aware some of these properties can be easily extracted using algorithms like Fourier transform, Laplacian filters etc. But if would be interesting to see if they can be extracted from a learned model.",MachineLearning
rqycm4,1640749732.0,[D] Elon Musk talking to Lex Fridman about AI at Tesla,"This is the third interview and there are a few interesting points about how they manage the training and inference of neural nets at Tesla. Some of the points are repeated from other talks like how its all dot products all the way down and repeatedly saying 'vector space'.

https://www.youtube.com/watch?v=DxREm3s1scA

Some high level topics:

* The term 'hydranet' (Multi-headed prediction) network is not used any more and they have some new type of architecture.
* Large focus on the extensive code written in C/C++ to reduce the inference time especially on device. This point is mentioned many, many times.
* The risk of 'jitter' - volatility of inference time and how that can affect the robustness of the overall system.
* Retraining the networks based on raw image data instead of 'processed image data'. Also a small comment that some of the networks they use were built on old hardware and the process of updating.
* New versions of the networks will rely on multiple layers of neural networks instead of some 'heuristics'. This implied as they move closer to version 11 that there has been a large redesign.
* Distilling 'giant bags of points' - or outputs of models that create inferences on multiple outcomes back into something digestible.",MachineLearning
rqvrwx,1640742198.0,[P] X-MLPs - Highly configurable all-MLP architecture built on Jax and Haiku,"I wanted to share a new project I developed: X-MLPs. This library provides a flexible all-MLP architectural foundation so you can quickly implement, mix-and-match, and test various SOTA MLP blocks and architectures. A key pattern used throughout X-MLPs is the factory function, enabling arbitrary blocks to be created and stacked in a network. It's built on Jax and Haiku.

The initial release implements several MLP blocks you can use out of the box, including: ResMLP, MLP-Mixer, gMLP\*, and S\^2-MLP. I plan to implement several more as well, along with some QoL improvements. However, my primary goal with X-MLPs is for anyone to be able to create their own blocks and rapidly experiment with different approaches. Code is pretty well documented, so there should be enough info in the repo to learn how to extend it.

Let me know if you have any questions!

Repo: [https://github.com/PyriteAI/x-mlps](https://github.com/PyriteAI/x-mlps)

\* Tiny attention is not yet implemented for gMLP",MachineLearning
rqtl0z,1640736007.0,[D] How to create a question answering system with a (potentially very large) corpus of text?,"Hello guys,

so, I was wondering how you would go about creating a query answering system based on a potentially large corpus of text. I had some decent exposure to NLP, and I realize how we could use a transformer for, say, answering questions in SQuAD format, where the reference text is small and you can pass it together with the question to the transformer to get an answer.

However, how would you answer the questions based on, say, a very large corpus of text, when it will not be possible to pass the whole text to the neural network each time you ask a question? One option that I see is going through the text looking for similar words / sentences - however, that might be very ""costly"" for each question. Ideally, it would make sense to create some kind of ""knowledge base"" based on the provided corpus and use it to get the answers, but I am not entirely sure how it should be done. So, do you guys have any references / ""best practices""? Thanks so much in advance!",MachineLearning
rqsyn9,1640734326.0,[D] Paper Explained - GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models (Video Walkthrough),"[https://youtu.be/gwI6g1pBD84](https://youtu.be/gwI6g1pBD84)

Diffusion models learn to iteratively reverse a noising process that is applied repeatedly during training. The result can be used for conditional generation as well as various other tasks such as inpainting. OpenAI's GLIDE builds on recent advances in diffusion models and combines text-conditional diffusion with classifier-free guidance and upsampling to achieve unprecedented quality in text-to-image samples.

&#x200B;

Try it yourself: [https://huggingface.co/spaces/valhalla/glide-text2im](https://huggingface.co/spaces/valhalla/glide-text2im)

&#x200B;

OUTLINE:

0:00 - Intro & Overview

6:10 - What is a Diffusion Model?

18:20 - Conditional Generation and Guided Diffusion

31:30 - Architecture Recap

34:05 - Training & Result metrics

36:55 - Failure cases & my own results

39:45 - Safety considerations

&#x200B;

Paper: [https://arxiv.org/abs/2112.10741](https://arxiv.org/abs/2112.10741)

Code & Model: [https://github.com/openai/glide-text2im](https://github.com/openai/glide-text2im)

&#x200B;

More diffusion papers:

[https://arxiv.org/pdf/2006.11239.pdf](https://arxiv.org/pdf/2006.11239.pdf)

[https://arxiv.org/pdf/2102.09672.pdf](https://arxiv.org/pdf/2102.09672.pdf)",MachineLearning
rqnjst,1640719722.0,"[D] How do you guys tune hyperparameters, when a single training run takes a long time (days to weeks)?","Training a large model, for example pretraining a large BERT model, can take weeks. How do you guys do hyperparameter tuning in such scenario?",MachineLearning
rqm5y6,1640715889.0,[P] How do you measure fairness without access to demographic data?,"Hi all! I'm working on a paper about measuring algorithmic fairness in cases where you don't have direct access to demographic data (for example, if you want to see whether a lender is discriminating against a particular race but the lender is not collecting/releasing race data of loan applicants).

If you have \~10 minutes and work in the ethical AI space, it would be a great help to hear from this community on whether/how often you have faced this issue in practice and what you think should be done to mitigate.

Survey link is here: [https://cambridge.eu.qualtrics.com/jfe/form/SV\_e9czBBKDitlglaC](https://cambridge.eu.qualtrics.com/jfe/form/SV_e9czBBKDitlglaC)",MachineLearning
rqdpqs,1640690610.0,[D] ML in Agriculture,"Hey. I'm working on a software product for fruit growers, helping them organize tasks through the season and have better quality fruits. I'm wondering what is the potential for ML here, has anyone had an experience in this field?

**Ideas**:
\-predicting the yield for each field, using historical weather data + actual yield for each year (+ actions taken in the field?)
\- predicting the price of each fruit/variety, based on total users' yield predictions + amount of searches for that variety on the ""Marketplace"" subpage)

What I was hoping to focus on, though, was the quality/quantity of produced fruits. There are so many **factors**, like:
\- temperature/humidity of air/soil (from weather stations, or even our own sensors to make it more accurate)
\- macro/micro-nutrients (unfortunately available \~once a year - requires lab)
\- actions like tree thinning (hard to digitalize? perhaps using image recognition)

...that have impact on: yield, quality (firmness, color, diameter), fungi/pest infection risk.

Appreciate your ideas/opinion on that!",MachineLearning
rq9shn,1640675673.0,[Project] Color detection for wound detection,"I have a project that detects the color of the wounds, bruise or and cuts. I already implemented HSV for the detection but I'm not satisfied for the result or maybe my range is not good. I research other colorspace to use on my program and the best colorspace I read is the use of LAB colorspace. Is it good that HSV?

Here some result of detection using HSV:
[https://imgur.com/a/sf3aqi9](https://imgur.com/a/sf3aqi9)

If LAB is good, there are any resources about this topic and how can I implement it on a efficient way?",MachineLearning
rq6uih,1640666161.0,[D] Other AI methods/algorithms except deep neural network that are promising?,"I have heard some other AI methods except Deep Learning, like these:

Free energy principle: [https://www.kaggle.com/charel/learn-by-example-active-inference-in-the-brain-1](https://www.kaggle.com/charel/learn-by-example-active-inference-in-the-brain-1)

Thousand brains theory: [https://numenta.com/blog/2019/01/16/the-thousand-brains-theory-of-intelligence/](https://numenta.com/blog/2019/01/16/the-thousand-brains-theory-of-intelligence/)

Hierarchical temporal memory: [https://github.com/numenta/htmpapers](https://github.com/numenta/htmpapers)

Tsetlin Machines: [http://tsetlinmachine.org](http://tsetlinmachine.org)

Spiking neural networks: [https://simons.berkeley.edu/news/spiking-neural-networks](https://simons.berkeley.edu/news/spiking-neural-networks)

Predictive Coding / Associative memory: [https://arxiv.org/abs/2109.08063](https://arxiv.org/abs/2109.08063)

Fractal AI: [https://github.com/Guillemdb/FractalAI](https://github.com/Guillemdb/FractalAI)

**(Updated)**

Hyperdimensional Computing: [https://github.com/HyperdimensionalComputing/collection](https://github.com/HyperdimensionalComputing/collection)

Hyperbolic machine learning: [https://github.com/nalexai/hyperlib](https://github.com/nalexai/hyperlib)

Complex valued neural networks: [https://www.reddit.com/r/MachineLearning/comments/o2q1h8/r\_complexvalued\_neural\_networks/](https://www.reddit.com/r/MachineLearning/comments/o2q1h8/r_complexvalued_neural_networks/), [https://arxiv.org/abs/1705.09792](https://arxiv.org/abs/1705.09792), [https://github.com/wavefrontshaping/complexPyTorch](https://github.com/wavefrontshaping/complexPyTorch)

&#x200B;

Do you think which methods/algorithms are interesting and will become more popular,or are there any other methods/algorithms do you think that is promising?

&#x200B;",MachineLearning
rq6hvs,1640665073.0,"[P] open-source desktop app to read, display and export car sensor data","i made this app with electron.js framework and python in the back-end using the python-obd library to communicate with the car. then i can export my data in a neat csv file for machine learning tasks! i only made this because all the other apps cost like 20 bucks and aren't customizable if i wanted to create some kind of a pipeline within the app.

i haven't tested it on other devices a lot yet, but it is only supported on windows and with cars that support OBD-II protocol which is most cars made after 1996. you can read more on that online though. my car in the video is a KIA Forte.

this also gave me practice with application development and paves the road for me to do machine learning with car sensor data. i hope it works for other people also and allows them to tinker with ML.

demo video is here: \[[https://www.youtube.com/watch?v=pkEYJG\\\_9p7k](https://www.youtube.com/watch?v=pkEYJG_9p7k)\]([https://www.youtube.com/watch?v=pkEYJG\_9p7k](https://www.youtube.com/watch?v=pkEYJG_9p7k))

code and some instructions are here: \[[https://github.com/8ahmedanwer8/ElectronOBD](https://github.com/8ahmedanwer8/ElectronOBD)\]([https://github.com/8ahmedanwer8/ElectronOBD](https://github.com/8ahmedanwer8/ElectronOBD))

p.s. give it a star on github if u like it ;)",MachineLearning
rq68jj,1640664315.0,[R] Modern Artificial Intelligence 1980s—2021 and Beyond (Schmidhuber’s Talk),"[Schmidhuber](https://twitter.com/schmidhuberai/status/1475508292831133705) uploaded a talk to his relatively new YouTube channel: https://youtu.be/pGftUCTqaGg

**From YouTube description:**

*This keynote talk had its premiere on 3 Dec 2020 at the AIJ conference in Moscow (where it was translated into Russian). It was also presented at NVIDIA's GTC-21 conference (US, 2021), the 2021 Machine Learning Summit in Beijing (where it was translated into Mandarin), Big Data and AI (Toronto, 2021), IFIC (China, 2021), AI Boost (Lithuania, 2021), and ICONIP 2021 (Jakarta, 2021). According to some of the organizers, it had millions of viewers outside of YouTube.*

**Abstract.** Significant historic events appear to be occurring more frequently as time goes on. Interestingly, it seems like subsequent intervals between these events are shrinking exponentially by a factor of four. This process looks like it should converge around the year 2040. The last of these major events can be said to have occurred around 1990 when the cold war ended, the WWW was born, mobile phones became mainstream, the first self-driving cars appeared, and modern AI with very deep neural networks came into being. In this talk, I'll focus on the latter, with emphasis on Metalearning since 1987 and what I call ""the miraculous year of deep learning"" which saw the birth of—among other things—(1) very deep learning through unsupervised pre-training, (2) the vanishing gradient analysis that led to the LSTMs running on your smartphones and to the really deep Highway Nets/ResNets, (3) neural fast weight programmers that are formally equivalent to what’s now called linear Transformers, (4) artificial curiosity for agents that invent their own problems (familiar to many nowadays in the form of GANs), (5) the learning of sequential neural attention, (6) the distilling of teacher nets into student nets, and (7) reinforcement learning and planning with recurrent world models. I’ll discuss how in the 2000s much of this has begun to impact billions of human lives, how the timeline predicts the next big event to be around 2030, what the final decade until convergence might hold, and what will happen in the subsequent 40 billion years. Take all of this with a grain of salt though.

https://youtu.be/pGftUCTqaGg",MachineLearning
rq4wp4,1640660431.0,[D] Has anyone got AutoKeras working on any sort of scale?,"I have played around with AutoKeras over the last while and while it does exactly what it says on the tin in regards to small scale single-GPU configurations, when I scale up to multi-GPU setups, the performance benefit just isn’t there. The documentation says there’s ostensibly support for multi-GPU setups via the mirrored training strategy, but that doesn’t seem to increase performance, but rather decrease it.",MachineLearning
rq1cnm,1640650150.0,[D] Diffusion Models Beat GANs on Image Synthesis Explained: 5-minute paper summary (by Casual GAN Papers),"I have been dodging this one long enough, it is finally time to make a paper summary for Guided Diffusion!

GANs have dominated the conversation around image generation for the past couple of years. Now though, a new king might have arrived - diffusion models. Using several tactical upgrades the team at OpenAI managed to create a guided diffusion model that outperforms state-of-the-art GANs on unstructured datasets such as ImageNet at up to 512x512 resolution. Among these improvements is the ability to explicitly control the tradeoff between diversity and fidelity of generated samples with gradients from a pretrained classifier. This ability to guide the diffusion process with an auxiliary model is also why diffusion models have skyrocketed in popularity in the generative art community, particularly for CLIP-guided diffusion.

Does this sound too good to be true? You are not wrong, there are some caveats to this approach, which is why it is vital to grasp the intuition for how it works!

Full summary: [https://t.me/casual\_gan/228](https://t.me/casual_gan/228)

Blog post: [https://www.casualganpapers.com/guided\_diffusion\_langevin\_dynamics\_classifier\_guidance/Guided-Diffusion-explained.html](https://www.casualganpapers.com/guided_diffusion_langevin_dynamics_classifier_guidance/Guided-Diffusion-explained.html)

[Guided Diffusion - SOTA generative art model for CLIP](https://preview.redd.it/c6kbce5id6881.png?width=1408&format=png&auto=webp&s=316b08a4766d356dec3dacd9ad6b9d12ed078e49)

[arxiv](https://arxiv.org/pdf/2105.05233.pdf) / [code](https://github.com/openai/guided-diffusion)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",MachineLearning
rq0b9k,1640647219.0,[D] Is using model checkpoints on validation sets while doing 5-fold cross validation an issue?,"Suppose I am training a neural network on a dataset and performing 5-fold CV. I am saving the 'best' model weights by checkpointing on improvements to a particular metric on the validation set and evaluating later using them on the same validation set. I just had the thought about whether I should be doing this model checkpointing like this?

I mean in theory, for cross validation to give an idea of generalized performance on an unseen test set, shouldn't the validation sets be treated like I would treat a test set? As in not accessing it during the training ?

Would appreciate any feedback.",MachineLearning
rpv5va,1640633283.0,[R] Looking for papers that prove that Deep Learning cannot solve a given problem.,"Hey Guys, I am looking for works where it has been proven that Machine Learning cannot solve a given problem because of the lack of information present in the input data,
PS:- This is very relevant in fields involving high-stakes decisions like healthcare, crime, social good, etc.",MachineLearning
rpum3a,1640631838.0,[R] Article Suggestions for Where Graphs Meet Transformers,"Hey Everyone,

I am trying to gather some articles about Graph Network and use of Transformer-like architecture that uses attention mechanisms which are used for biological or medicinal purposes, but I am open to read some articles from other fields too.

Also, I am trying to come up with a review article in a Disease-Drug-Therapy triad (about the use of ml/dl, of course). So if you feel like some of the studies from this field are undervalued, comment below and I will take a look. Thanks in advance.

Cheers.",MachineLearning
rptzd6,1640630140.0,[Project] Idris and XLA: linear algebra and probabilistic modelling w. dependent types,"In June, [I announced](https://www.reddit.com/r/MachineLearning/comments/o9lqb8/probabilistic_modelling_project_w_dependent_types/?utm_source=share&utm_medium=web2x&context=3) I'd started work on a probabilistic modelling [library](https://github.com/joelberkeley/spidr) in Idris. This post is to announce the first major milestone: **basic linear algebra in Idris backed by XLA**. Right now, this is only addition, but other ops are easy to add from here.

What's the project mission? Well it's evolving, but roughly: we've seen a number of impressive numerical computing projects. Some lead the pack in performance, while others leverage advanced language features and theory for expressive APIs. With this project, we hope to explore both.

Some highlights:

- design, including user-friendliness, is paramount. Features come second
- dependently typed: tensor shapes are verified at compile time
- expect to support GPU in future, and possibly other accelerators
- XLA for competitive performance

See the comments for more detail.",MachineLearning
rprmq3,1640623823.0,"[D] SentencePiece, WordPiece, BPE... Which tokenizer is the best one?","There are several popular tokenization algorithms that I frequently encounter: Byte Pair Encoding, SentencePiece, WordPiece and less often Unigram.

The title is formulated somewhat provocatively and I assume there is no \*\*single best\*\* algorithm between the candidates. But what are the key differences and situations where one might be preferred over the others?",MachineLearning
rpqudp,1640621635.0,[D] Paper Explained - Federated Learning for Mobile Keyboard Prediction,"Ever wondered how your mobile keyboard gives you the next word suggestions? How do they give personalised suggestions, while at the same time ensuring the privacy of individuals?

Check out my blog post ""Federated Learning for Mobile Keyboard Prediction"", which talks about how this happens, in a privacy-preserving manner.

Blog Post -  [PPML Series #3 - Federated Learning for Mobile Keyboard Prediction](https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/)

Annotated Paper -  [Annotated-ML-Papers/Federated Learning for Mobile Keyboard Prediction](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/PPML/Federated%20Learning/Federated%20Learning%20for%20Mobile%20Keyboard%20Prediction.pdf)",MachineLearning
rppsrl,1640618624.0,[R] The new CPPE-5 dataset,"This paper introduces the ""CPPE-5: Medical Personal Protective Equipment"" dataset a new challenging image dataset with the goal to allow the study of subordinate categorization of medical PPE (gloves, masks, coveralls, face shields, goggles) unlike any other existing dataset. Furthermore, you can easily get started to use this dataset with the tutorials and data loaders in the code repository or use one among the models from the model zoo for this dataset.

***Supporting Links:***

Code repository: [https://git.io/cppe5-dataset](https://git.io/cppe5-dataset)

Paper: [https://arxiv.org/abs/2112.09569](https://arxiv.org/abs/2112.09569)",MachineLearning
rpl389,1640602799.0,[D] Categorical features in image classification,"I'm training a CNN classifier with tensorflow on a large image dataset. There are some obvious distinct  groups of images in the training set which - in a tabular data setting - I would have included in training in the form of a categorical variable/feature. Is there a way to do this with image data?

An obvious workaround would be to train two separate models - one per group (assuming 2 groups). But would that be the only way? I may be  having more categorical features soon so having a separate model for all their combinations won't be practical. I'm looking for an algo/library which takes both pixels and categorical data as input in training.

Any suggestions?",MachineLearning
rphiin,1640588922.0,"[D] 2022 Research topics, or areas","Hi good people, would you like to suggest some latest (interesting) Research areas to work on 2022, specially on Deep Learning, Deep Reinforcement Learning, Federated learning, Meta learning areas. Thank you.",MachineLearning
rpctc4,1640573578.0,[D] Is crowdsourced evaluation of model a good idea?,Is it a good idea to evaluate a model by letting users grade the model on some scale (which might vary from problem to problem) on the basis of its correctness (which I believe may be subjective in some cases)?,MachineLearning
rpa2pm,1640565213.0,[D] Did Tesla Create it's own ML models from scratch or did they start by using another company's services?,"I couldn't find an answer when googling so thought I'd ask the brains trust.

Musk  doesn't seem like the kind of person to want to outsource this kind of  task. His ownership of OpenAI would have also helped to build the  initial models. That being said, many large firms have bought ML  companies and integrated them rather than build them from scratch, for  example, Apple bought Siri from Nuance Communications.

Furthermore,  are Mercedes and other automobile companies who hope to join the  self-driving future all building their own models or outsourcing this  massive task to outside firms? From a democratic position it makes sense to ensure no one company monopolizes navigation but from an optimisation standpoint it absolutely does. I would have thought Google would use  Waymo to create the 'operating system' of the new age car. It would have  made perfect sense with their maps, Android and search services.",MachineLearning
rp849m,1640559537.0,[D] Research paper figure drawing,"Hi all, as a novice researcher, I have one question about drawing great quality of figure for scientific research paper. Would you like to suggest me how mostly Researcher draw very interesting figure with many customization in deep learning based papers. Could you suggest some great tools to make drawing much easier to express or add, also any latex source code or website to customize the drawing or any video series where they talked about or demonstrate how to draw such figure for Research paper. Thank you for your time and suggestions regarding this.",MachineLearning
rp7wfs,1640558914.0,[D] Anyone know any projects involving generating objects in the COCO segmentation dataset using GANs?," I know this is not the intended purpose of this dataset, but it seems pretty good given the size of the dataset and object bounds/masks.",MachineLearning
roy4nw,1640529464.0,[P] Comparison Between Player of Games and AlphaZero,"In this blog post I describe the differences between DeepMinds new algorithm Player Of Games and AlphaZero. I describe what GT-CFR is with some code examples of how to implement it in python.

[Blog Post](https://link.medium.com/KZKAlDEVimb)",MachineLearning
rovtz1,1640520862.0,"[Research] Looking for interesting ML papers to read for the break or the new year? Here is a curated list I made. (with video explanation, short read, paper, and code for each of them)","The best AI papers of 2021 with a clear video demo, short read, paper, and code for each of them.

In-depth **blog article**: [https://www.louisbouchard.ai/2021-ai-papers-review/](https://www.louisbouchard.ai/2021-ai-papers-review/)

The full list on **GitHub**: [https://github.com/louisfb01/best\_AI\_papers\_2021](https://github.com/louisfb01/best_AI_papers_2021)

Short Recap Video: [https://youtu.be/z5slE\_akZmc](https://youtu.be/z5slE_akZmc)",MachineLearning
roulqh,1640515370.0,Apple AI Residency 2022 [R],"Hi all, I am starting this thread for everyone who has applied to the Apple AI Residency in 2022. The applications have closed on the 15th of December. Has anyone heard about the online coding test?

Thank you for this and wishing everyone the best of luck with their applications. :)",MachineLearning
rotyst,1640512388.0,[D] Is there a repo on which many light-weight self-attention mechanism are introduced?,"Hi, guys. I am searching many applications of self-attention reaching efficiently computation or fast convergence such as reformer, longformer, or bigbird.

Could I ask you there is organized paper or repo of them? ;-;!!",MachineLearning
rorax0,1640500693.0,[D] Can I use my own images for VQGAN CLIP generation?,"I am a newbie to machine learning. I am now using google colab to play with the VQGAN notebook.

I can't code, but can understand a bit.

I am wondering if I can modify the notebook, and use my own set of images to generate images.

Let's say I would like to generate a fashion portrait with my wardrobe styling and model. Can I take thousands of the wardrobe details and the model, and then put them into the GAN and generate an abstract image? If yes, how?

&#x200B;

Bunch of thanks!",MachineLearning
ropcvb,1640493298.0,[R] Researchers from the University of Chicago and Tel Aviv University Introduce ‘Text2Mesh’: A Novel Framework to Alter both Color and Geometry of 3D Meshes According to a Textual Target,"In recent years, neural-based generative models have been at the center of attention for their exceptional capability of creating aesthetically attractive graphical content seemingly out of nowhere. Recent solutions of this kind, like VQGAN and in general all derivations of Generative Adversarial Networks and their combination with other Deep Learning techniques, like CLIP from OpenAI (a joint image-text embedding model), led to amazing results, using very complex and powerful generative techniques. With the advent of NFTs and the application of transformer-based techniques to computer graphics in videogames, the hype built during recent years for generative models might finally lead AI-generated art to meet the growing market demand in the field of entertainment. 

The main perk of generative models, that is, their versatility in learning latent representations of given datasets, comes at the cost of higher complexity and lower success rate of training experiments. Researchers from the University of Chicago and Tel Aviv University Introduce the’ [Text2Mesh’ model](https://threedle.github.io/text2mesh/). The Text2Mesh model tries to avoid the above problem by proposing a non-generative method to alter the appearance of 3D meshes by using a “*Neural Style Field*” (NSF), learned with neural techniques, that maps vertices of an input mesh to RGB color and a local displacement along their normal direction, based on a text prompt that determines the style and appearance of the result. The model is powered by the CLIP joint text-image embedding space and appropriate regularization to avoid degenerate solutions.

Paper Summary: [https://www.marktechpost.com/2021/12/25/researchers-from-the-university-of-chicago-and-tel-aviv-university-introduce-text2mesh-a-novel-framework-to-alter-both-color-and-geometry-of-3d-meshes-according-to-a-textual-target/](https://www.marktechpost.com/2021/12/25/researchers-from-the-university-of-chicago-and-tel-aviv-university-introduce-text2mesh-a-novel-framework-to-alter-both-color-and-geometry-of-3d-meshes-according-to-a-textual-target/)

Paper: https://arxiv.org/pdf/2112.03221.pdf

GitHub: https://github.com/threedle/text2mesh

Project Page: [https://threedle.github.io/text2mesh/](https://threedle.github.io/text2mesh/)

&#x200B;

https://i.redd.it/zbz7m520ft781.gif",MachineLearning
rolzia,1640481564.0,[P] Segmentation based on shape identification,"I need help making a deep learning classifier do what I want.

So right now I have an algorithm that splits an image into equal squares, then classifies each segment and counts how many of each.

I want to use a mask that finds shapes of the objects first, then the classifier will work within the individual shapes it finds. So instead of classifying equal segments, it will classify pixels located within the strange shapes it found. See drawing https://imgur.com/a/9K0rWOM

Sometimes the mask can't find shapes and contour lines on certain parts of an image. On those areas I'd like it to segment into a grid and classify that way. See pic 3 of drawing. https://imgur.com/a/9K0rWOM

Hope that makes sense and someone can point me in the right direction. Its in python",MachineLearning
rogiq2,1640463102.0,"[D] Weak supervision in practice, when to collect ""strongly"" labelled data?","I've spent some time trying to find good resources for practical weak supervision. Most tutorials, guides, research papers (this is somehow reasonable), etc. assume you have a strongly labelled validation set before you start defining rules/labeling functions. In most cases, this validation set is also used to evaluate labeling functions (e.g., in Snorkel via the LF summary).

In this context, I'd love to know what are people doing in practice. First label/collect validation set and then do weak supervision, or other workflows such as starting with rules/lfs and then labelling a subset of weakly labelled data.

Would love to hear your experiences! (and pointers)",MachineLearning
rocen2,1640448717.0,[D] GANs and probability distributions on images,"When training GANs (either with the classic loss or Wasserstein loss), we try to minimize the distance between the probability distribution of the real data and the probability distribution of the generated data.

In the case of GANs for images, e.g. trained on CelebA: How does a probability distribution over images look like? What is an intuitive way to understand this concept?",MachineLearning
ro8p1o,1640434143.0,[R] Dataset containing similar looking objects in the wild.,"Have you've seen a dataset like this? 10 images of car A and 10 images of car B. Both A and B looks similar. This is almost what I am looking for but not exactly:

https://link.springer.com/article/10.3758/s13428-019-01211-7",MachineLearning
ro7xei,1640430610.0,[D] Paper Explained – Linear Algebra with Transformers,"Why would one build a transformer to solve linear algebra problems when there is numpy.linalg?

Paper: Charton, François. ""Linear algebra with transformers."" [https://arxiv.org/abs/2112.01898](https://arxiv.org/abs/2112.01898)

Video: [https://youtu.be/dqKM1Mbt0pU](https://youtu.be/dqKM1Mbt0pU)

Abstract:
Most applications of transformers to mathematics, from integration to theorem proving, focus on symbolic computation. In this paper, we show that transformers can be trained to perform numerical calculations with high accuracy. We consider problems of linear algebra: matrix transposition, addition, multiplication, eigenvalues and vectors, singular value decomposition, and inversion. Training small transformers (up to six layers) over datasets of random matrices, we achieve high accuracies (over 90%) on all problems. We also show that trained models can generalize out of their training distribution, and that out-of-domain accuracy can be greatly improved by working from more diverse datasets (in particular, by training from matrices with non-independent and identically distributed coefficients). Finally, we show that few-shot learning can be leveraged to re-train models to solve larger problems.

Check out the video to find out why F. Charton trained transformers to solve Linear Algebra problems and how these transformers work: [https://youtu.be/dqKM1Mbt0pU](https://youtu.be/dqKM1Mbt0pU)

Video outline:

00:00 Linear algebra with transformers

00:41 Weights & Biases (Sponsor)

02:21 Why throwing transformers at linear algebra is cool.

08:08 How do transformers solve linear algebra?

09:50 Encoding matrices for transformers

11:28 Training data and results

12:43 Generalization!?

16:05 Few-shot learning!?",MachineLearning
ro224d,1640405356.0,[D] How AD is implemented in JAX/Tensorflow/Pytorch?,"Hi everyone.
I heard that there are two ways to implement automatic differentiation: Source code transformation (SCT) and Operator overloading (OO).
Which type does Jax use? And same question for Tensorflow and Pytorch.",MachineLearning
ro1ebu,1640402807.0,"[D] redditors who work at big tech, does 0.1% improvement matter to your company?","I have seen from time to time that when a paper is on the borderline of acceptance because of marginal improvement (0.1-0.2% accuracy of some sort), the authors can always argue that ""a marginal but consistent improvement can be a large sum of revenue for companies that make billions of dollars"".

ML in industry is not so simple: although it is just a new method, redeploying a whole pipeline can be very time consuming. I want to ask people who are working at big tech: does that improvement really matter? Are your companies willing to redeploy this improvement?",MachineLearning
ro12hr,1640401586.0,[D] Drawbacks to the Louvain/Leiden algorithm for Community Detection?,"I've been looking for the drawbacks to the Louvain algorithm, and the more recent Leiden algorithm for community detection. An internet search turns up almost nothing, except that Louvain can lead to disconnected communities (which is fixed in the Leiden algorithm).


However, surely the Leiden algorithm is not the end all be all of community detection, is it? What are some of the drawbacks of the Louvain and Leiden algorithms? What are some instances where it performs poorly? What are instances where I'd prefer other community detection methods e.g. spectral?


(And how come I don't see it very often in classic data clustering, where one could build a kNN graph and run Louvain/Leiden on top of it?)",MachineLearning
rntvid,1640376678.0,How to make lower and upper bound constrained predictions using sci-kit learn? [D],"I'm trying to run a regression model on sports player performance data, such that the sum of the individual predictions falls between an upper bound and lower bound of a team's expected prediction distribution.

Given the current COVID situation, many star players are missing games, leading to many ""no name"" players getting unprecedented opportunity. The problem is that the regression model underpredicts almost all of them.

Say I know that the Dallas Cowboys have the best offensive line in football, and they're going against the worst rushing defense in football. Their star running back Ezekiel Elliot is out for the game, and hypothetically, the two running backs filling in haven't played much and are averaging 5 yards a game, and 10 yards a game, respectively.

A regression model will predict somewhere in the ballpark of 5 and 10 for those players, which implies the Dallas Cowboys will rush for 15 yards the entire game. The whole world knows that's not going to happen, and historical data says there's a 95% chance the Dallas Cowboys rush for between 75 and 125 rushing yards altogether. Is there any way to group predictions together in scikit-learn and set lower bounds and upper bounds on the team's sum?

It's almost like a multilevel model type thing where the two have to agree with each other to make logical sense. If we were predicting team rushing yards using team statistics it would predict somewhere between 75 and 125 yards. So how can we get the individual player predictions to sum up to something that makes sense.

I'm essentially trying to combine regression with constrained optimization.

Thanks so much.",MachineLearning
rnnube,1640357568.0,[D] Could we give a Transformer long term memory by reserving part of it's attention window for world vector embeddings?,"The inputs for a Transformer are merely vectors, like for every other ML architecture. These vectors usually represent tokens.

But what if we allow a Transformer to generate and store it's own world vector embeddings and to select a batch of them via attention for each new input? It would create a functional loop for it to access it's own memory, right?",MachineLearning
rnhyeo,1640334741.0,[D] How to visualize ground truth boxes on images?,Hello guys. As you might have guessed from the topic I want to visualize the ground truth boxes on my images from the PASCAL VOC dataset. Can anyone of you help me out with this dilemma? I have been trying to find out ways on how to display the ground truth boxes on the images. Any help would be good.,MachineLearning
rnhena,1640332553.0,[D] Dimensionality reduction for geometrical data,"Hi!

I have a dataset where each data is a set of geometrical 3D data points.

For example, one data could be:

x1,y1 x2,y2, x3,y3, x4,y4.... xN,yN

&#x200B;

I am looking for dimensionality reduction techniques that could fit such a data.

Do you know what techniques I could apply?

Thanks in advance!",MachineLearning
rnfmrr,1640325736.0,Learning both Weights and Connections for Efficient Neural Networks (Research Paper Walkthrough) [D],"Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. This research proposes a 3-step method for training efficient neural networks that are lightweight and can be deployed on-device yet retaining the SOTA accuracy numbers.

Paper summary: https://youtu.be/2fy17SwDHUw
Paper link: https://arxiv.org/pdf/1506.02626.pdf",MachineLearning
rn9f19,1640305325.0,[D] What are some cool random forest ML applications?,"I have seen some jaw-dropping examples of neural networks and deep learning (e.g., deep fakes). I am looking for similarly awesome examples of what random forests can do. Please share.",MachineLearning
rn6m8o,1640297143.0,[D] Reinforcement Learning with Initial Policy,"Hi guys happy holidays!  So I understand the difference between passive RL and active RL is that in passive RL we’re merely learning the values of the states under a policy we feed the agent, but in active RL, the agent can learn the optimal policy itself.

What’s the best course of action if we have an initial policy we think is good, but we also want to find if there exists a better policy? I thought (approximate) Q-learning might be suitable, where we initialize the Q-states in our policy with values  > 0 and all other Q-states with values = 0, and then set a low epsilon to encourage testing that policy first before increasing it over time to see if there are better policies. However, this seems kind of backwards - because usually we’d want to explore more first, starting from a “blank slate,” and then exploit. Is there another way of going about this? perhaps combining with some form of passive RL?",MachineLearning
rn63vl,1640295698.0,[P] ClipRCNN: Tiny text-guided zero-shot object detector,"We propose a new tiny architecture of zero-shot object detector inspired by the classical R-CNN object detector, named ClipRCNN. How it works:
1) First, we should generate region proposals. So, as we need a class agnostic region proposals generator, we chose a classical algorithm named Selective Search instead of any modern pre-trained object detection networks. This step is similar to proposal generation in R-CNN network.
2) After, we compute CLIP loss between each proposal and all user's prompts (texts and images).
3) Last, we return top k best proposals (with minimum CLIP loss) as a prediction of the ClipRCNN model.


This approach isn't perfect at all, but it is really simple and works after writing just a few lines of code.
You can find our implementation of the ClipRCNN here: [https://github.com/bes-dev/pytorch\_clip\_guided\_loss/tree/master/examples/object\_detection](https://github.com/bes-dev/pytorch_clip_guided_loss/tree/master/examples/object_detection)",MachineLearning
rn5rlo,1640294714.0,[D] Can any article architecture be used commercially?,"I am very curious to know how companies in the ML area work in this sense. Say for example some neural network architecture, can any article architecture be used commercially? If I find an implementation on GitHub of that article, would it be legal to use it (depending on repository license)? Or maybe if I implement it myself.",MachineLearning
rmxri9,1640272464.0,[D] Why and how do residual/skip connections work? Looking for literature,"You can share any comments, insights, discussion, or particular works you have in mind.

Aside: I'll just mention an extremely subtle hint about a recent work I remember coming across, but cannot recall its title or other info.

- That work was analyzing residual connections in one of its sections, and commented that a function block with a skip connection was easier to train as it ""only"" has to ***learn the ""domain shift"" from the original (input) data space***. Not sure if they specifically used the words ""domain shift"", but it was close.

- I'd not met this _particular_ framing before that paper. Besides _everything else_ you may want to comment/share here, I'd appreciate if you can comment on or share papers dealing with particular framing as well. **It does not have to come from ML literature. Stats, Math, etc. are also welcome** (with some help to clarify the more jargon-heavy parts).

Existing, relevant threads: https://www.reddit.com/r/MachineLearning/comments/px3hzd/d_has_the_resnet_hypothesis_been_debunked/",MachineLearning
rmws0h,1640269541.0,[R] A Mathematical Framework for Transformer Circuits,"[Link to paper](https://transformer-circuits.pub/2021/framework/index.html).

&#x200B;

>In this paper, we attempt to take initial, very preliminary steps towards reverse-engineering transformers.  Given the incredible complexity and size of modern language models, we have found it most fruitful to start with the simplest possible models and work our way up from there.  Our aim is to discover simple algorithmic patterns, motifs, or frameworks that can subsequently be applied to larger and more complex models.  Specifically, in this paper we will study transformers with two layers or less which have only attention blocks – this is in contrast to a large, modern transformer like GPT-3, which has 96 layers and alternates attention blocks with MLP blocks.",MachineLearning
rmtcvh,1640257555.0,[D] Shape Changing And Conformal Mapping,"I am looking for shape changing algorithm meanly from linear to nonlinear shapes, I think conformal mapping is related here, does anyone has an experience with shape changing, how it works?",MachineLearning
rmrifl,1640249940.0,[D] Multi output generation with sparsely fixed output,"I  am trying to use generative deep learning to synthetically generate  certain biological parameters given some experimental constraints. I am  using GANs to do this and I have been successful so far. For the next  step, I want to fix certain parameters in the output vector (to some  experimentally verified values) and then generate the rest of the  parameters in the output vector based on this. The features in the  output vector of the GANs are obviously coupled by some complex  non-linearities so changing or fixing one of them will propagate to the  other outputs as well. Is there some way to do this ? What I have  already tried/ researched -

1. Transfer  learning on a trained generator with a curated training dataset where  the output features I want to fix are already constrained. It works (the  GAN learns to fix the values of certain output features but this is  because they are already fixed in the training data).
2. Context  encoding in GANs -I am aware that context encoding and similar tasks  have been already showcased where GANs are used to fill in missing  regions in an image based on the rest of the image. But extrapolating  this to my case is hard. To make a direct comparison, my case is  analogous to having an image that has only a few pixels that are shown  and the rest of the image is masked and need to be inferred.

I  could not find any literature on such sparsely fixed output. If anyone  is aware of some literature or blog that tackles a similar problem  please let me know.",MachineLearning
rmcp62,1640201174.0,[D] Why does convolution lead to translation equivariance?,"I asked a similiar question before. It is my understanding that translational equivariance is one of the main properties that allows convolution to be so powerful at finding patterns in images. However, I am having trouble finding concrete proofs for how this happens. Does anyone have any papers that explains why convolution leads to translation equivariance, preferably from a linear algebraic perspective. Secondly, does anyone have any papers that explains why translational equivariance allows Convolutional Neural Networks to be so successful at image recognition or explains how this idea of translation equivariance relates to what is happening at each layer?

Thanks",MachineLearning
rm4i9v,1640177011.0,[P][SP] Lottery Ticket Hypothesis - Paper implementation [screencast],"Hi all!


I created a video where I tried to implement the paper [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635) ""from scratch"". I made a lot of simplifications in order to fit everything in a single video (e.g. only working with the multilayer perceptron and using the MNIST dataset). I also ran a couple of experiments to see whether I could get similar results to what the paper presented. The video also contains a quick tutorial on pruning in PyTorch. Lastly, the video includes chapters so that it is easier to navigate.


Hope some of you find it interesting. Don't hesitate to share any constructive feedback:)


Video link:
[https://youtu.be/bQt0CLXXAqg](https://youtu.be/bQt0CLXXAqg)",MachineLearning
rm4fgz,1640176739.0,[P] A Static Analyzer for Detecting Tensor Shape Errors in Deep Neural Network Training Code,"Hi reddit!

My colleagues and I just shared a static analyzer PyTea which detects shape errors in the PyTorch project.

GitHub: [https://github.com/ropas/pytea](https://github.com/ropas/pytea)

arXiv: [https://arxiv.org/abs/2112.09037](https://arxiv.org/abs/2112.09037)

PyTea can successfully analyze the entire training/evaluation path such as Training Imagenet classifiers in [https://github.com/pytorch/examples](https://github.com/pytorch/examples). It also supports major ML libraries including torchvision, numpy, PIL, and so on.

I've been implementing a basic integration with VSCode so that you can interactively find any shape mismatch from your code editor.

Please let us know if you need support for the library you use!",MachineLearning
rlxggy,1640149103.0,[D] Since gradient continues to decrease as training loss decreases why do we need to decay the learning rate too?,"This has always bothered me. I will write my question point by point to make myself more clear.

1. The weight update for SGD is *gradient times learning rate*.
2. Initially when the training loss is high the gradients are naturally going to be very high. Usually we use high learning rate at this step.
3. As the training progresses, training loss falls and *gradients also become smaller*. We usually lower the learning rate as the training progresses.
4. So we are reducing the *weight update* through both *learning rate* and *gradient.*
5. Interestingly, for adaptive optimization methods such as ADAM we normalize the gradient by it's second order moments which kind of counteracts the effect of gradients becoming smaller. (I'm not too sure about this though!)
6. So, my question is why do we need to decay the learning rate?",MachineLearning
rlvyvl,1640144076.0,What architectures exist for time series map prediction? [D],"I've been given a problem of generating a map of predicted dust clouds. Imagine you have an n by m grid, and you know the amount of dust in each cell. You also know how much new dust is being produced in each cell. Intuitively, the amount of dust in a cell at time t+1 will depend on the amount of dust in the nearby cells at time t, the amount of dust being produced in nearby cells at time t, and other factor such as wind. For simplicity, lets ignore the win.

I'm curious what architectures exist for this sort of problem. My initial idea was to have an n by m by 2 input shape, and an n by m by 1 output. The input would be the current dust map in the first layer, and the map of the newly produced dust in the second layer. The output would be the dust map after one time step. In this case, a convolutional autoencoder seems like the simplest solution. This technique is used in [this paper](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020MS002109).

Very few models for dust exist, so I've been researching weather prediction models, as they are very similar to my problem. I've seen [techniques that use GANs](https://www.nature.com/articles/s41586-021-03854-z.pdf) although the discussions on this forum about it seemed to have some doubts about its effectiveness.

I've briefly looked at R-CNN methods. I don't think they would be applicable, as they are more for object detection, but feel free to tell me otherwise.

I've also considered that if my grid is small enough, I could probably get away with not using convolutions at all and could just run it as a straight RNN problem, but that might require a lot more data to converge as its missing the adjacency context the convolutions provide.

Any ideas are much appreciated.",MachineLearning
rlsa2y,1640132629.0,[Discussion] Hot-Reloading Headless Deep Learning Rig,"Hey folks. Not sure if this has been asked before. I am plenty experienced with ML scripting, but am looking for opinions on setting up a hot-reloading, external, headless rig to offload some RL training.

My employer uses a kubernetes-based ML platform which is really neat for distributed big data jobs via Dask… hot-reloading a heavy duty Argo WF via Tilt across up to 1,600 nodes, each with a GPU and 32GB memory…

I want to dig into some hardcore deep RL and want to set up a personal development environment that is compatible with remotely hot-reloading from my MacBook. So like, upon saving any changes to my local scripts they are updated and (try) to run on a headless Linux rig whilst logs are returned and printed.

Any folks got recommendations on some (open source) ML development toolkits that are ideal for this?",MachineLearning
rlmr8s,1640116675.0,[D] A hand-picked selection of the best Python ML Libraries of 2021,"Hi all,

We have been compiling a list of the **top Python libraries** released or popularized in 2021, hoping that you'll find some good ones here that you might have missed during the year.

Most of these are related to data science / machine learning workflows.

We decided to expand the post by listing many more than 10 libraries, although we do have several main picks which are related to ML:

* [Awkward Array](https://github.com/scikit-hep/awkward-1.0).
* [jupytext](https://github.com/mwouts/jupytext).
* [Gradio](https://github.com/gradio-app/gradio).
* [AugLy](https://github.com/facebookresearch/AugLy).
* [skweak](https://github.com/NorskRegnesentral/skweak).
* [Evidently](https://github.com/evidentlyai/evidently).
* [Jina](https://github.com/jina-ai/jina) and [Finetuner](https://github.com/jina-ai/finetuner).
* [Hub](https://github.com/activeloopai/Hub).

**The full list with expanded descriptions is available here, in case you are curious:** [https://tryolabs.com/blog/2021/12/21/top-python-libraries-2021](https://tryolabs.com/blog/2021/12/21/top-python-libraries-2021)

There are many more listed under ""Extra picks"", all relevant to 2021 or late 2020.

So:

* What do you think about our picks?
* Did we miss any good ones?",MachineLearning
rlli7q,1640113133.0,[P] Axon: Deep Learning in Elixir,"Repo: [https://github.com/elixir-nx/axon](https://github.com/elixir-nx/axon)

Hi everyone, I just wanted to share a passion project of mine that I've been hacking on and off for a little bit now.  Before you ask ""OP why would you bother creating a DL framework in another programming language? Just use TF/PyTorch/JAX/insert framework here"" - I didn't set out with the (unrealistic) goal of overtaking any existing tooling. I really just enjoy learning more about the internals of modern DL frameworks/algorithms/etc. And I really like Elixir :)

With that said, I am also not here to convince anybody that Elixir or Axon are better than Python and any other existing stack out there. I'm just here to share something that (I think) is cool.

Disclaimers aside, here's a little bit about Axon:

\- Built on top of the Elixir Nx project which supports automatic differentiation and JIT compilation using XLA (HEAVILY modeled after JAX)

\- Keras-like model creation API with support for pretty much anything you can find in a modern framework as well as custom layers for implementing anything you don't already have

\- Optax-inspired optimization API for creating and composing gradient-based optimizers

\- PyTorch Ignite-inspired training/validation/testing API for creating and instrumenting training loops, validation loops, testing, etc.

I'm also actively developing a library on top of Axon which converts to/from ONNX.

The library is still very WIP and there are a lot of features I'm still hacking away at such as multi-device support. But I've found it to be pretty useable for every example I've implemented so far.

Feedback and questions are always welcome!",MachineLearning
rlj83w,1640106674.0,[D] How important is Numerical Analysis for machine learning?,"I’m already signed up for a numerical methods class from the CS department, which is very application focused, and was originally planning on doing a sequence of graduate numerical analysis from the math department next year, which is very theory focused, but now I’m wondering if I’m better off taking some other class instead of the sequence (ie: a graduate algorithms class and a graduate real analysis class). Is that a good idea? Is an upper division undergrad numerical methods class sufficient?

P.s. I’m aiming for a PhD in statistics after undergrad.",MachineLearning
rlixkx,1640105909.0,[P] minDALL-E: PyTorch implementation of a 1.3B text-to-image generation model trained on 14 million image-text pairs,"Hello. I introduce an open source project, which released the checkpoint of the text-to-image generation model, DALL-E.Link: [https://github.com/kakaobrain/minDALL-E](https://github.com/kakaobrain/minDALL-E)

[Example 1: text-to-image generation of minDALL-E](https://i.redd.it/e0r0oi79fx681.gif)

[Example 2: text-to-image generation of minDALL-E](https://i.redd.it/e6reik79fx681.gif)

The naming of this project is ""minDALL-E"", which is originated from the ""minGPT"".minDALL-E is the transformer with 1.3B params and trained on CC-15M.In addition, larger model (4B?) will be released soon.

To the best of my knowledge, it's the first model that supports english.Check the project and enjoy to generate various images conditioned on your own texts.",MachineLearning
rlil7y,1640104949.0,"[R] CfP: First International Symposium on the Tsetlin Machine, Grimstad, Norway, June 20-21, 2022 (Hybrid)","&#x200B;

[Call for Papers: First International Symposium on the Tsetlin Machine](https://preview.redd.it/kn616918cx681.png?width=2338&format=png&auto=webp&s=54542932be9f957677e486aaac8efc005e29863c)

Welcome to the First International Symposium on the Tsetlin Machine (https://istm.no) in Grimstad, Norway, June 20-21, 2022 (Hybrid). We invite submissions of papers on all topics related to Tsetlin Machines, as well as Learning Automata, Novel AI Algorithms, Explainable and Interpretable AI, Energy-efficient AI Systems Design, New AI Applications, and Intelligent Data Preprocessing. Paper Submission Deadline: March 4, 2022.",MachineLearning
rlgygp,1640100372.0,"[P] Solo-learn 1.0.3: new methods, support for transformer architectures, better evaluation, improved docs, and additional results.","Hi Reddit, the solo-learn team is back again with interesting news about its [SSL library](https://github.com/vturrisi/solo-learn).

During the last few months, we worked hard to improve the library by adding the following features:

1. **New methods** (NNBYOL, NNSIAM, and VIbCReg);
2. **Support for transformer architectures** (ViT, Swin, and PoolFormer) to keep up with new research;
3. **Better evaluation** with k-NN and Object Detection (Detectron2 on Pascal VOC and COCO);
4. **Improved docs** by adding tutorials, code of conduct, and contribution guidelines;
5. **Additional results** for MoCo V2+ and Barlow Twins on ImageNet.

Feel free to ask questions here or contribute on GitHub.

Enrico and Victor",MachineLearning
rlaltq,1640077307.0,[D] Do you know any tools (libraries/frameworks) that are intuitive enough for teenagers for a practical introduction to AI?,"Hello, I am a computer science student and I am trying to set up a workshop for high-school students to give them a first hands-on introduction to AI, more specifically to computer vision. So, I was wondering if you know any frameworks or tools or libraries that would raise awareness about AI and give a first incentive about it to teenagers.Any help would be very much appreciated!Cheers!",MachineLearning
rl8jrb,1640069241.0,[P] Milvus vector database 2.0 is now cloud-scalable,"## Objectives of Milvus vector database

When the idea of the [Milvus vector database](https://github.com/milvus-io/milvus) first came to our minds, we wanted to build a data infrastructure that could help people accelerate AI adoptions in their organizations.

[Milvus in AI\/ML applications Development Cycle](https://preview.redd.it/ycdqapfjcu681.png?width=2048&format=png&auto=webp&s=ceaf1c4002cbb43ca81b0bc9c7e638f4baa6fe31)

We have set two crucial objectives for the Milvus project to fulfill this mission.

### Ease of use

AI/ML is an emerging area where new technologies keep coming out. Most developers are not entirely familiar with the fast-growing technologies and tools of AI. Developers have already consumed most of their energies finding, training, and tuning the models. It's hard for them to spend additional efforts handling the large amounts of embedding vectors generated by the models. Not to mention the manipulation of a large volume of data is always a very challenging task.

Thus we give ""ease of use"" a very high priority since it could significantly reduce the development cost.

### Low running costs

One of the primary hurdles of AI in production is to justify the return of investment. We would have more opportunities to put our AI applications into production with lower running costs. And it would be conducive to lifting the margin of potential benefits.

### Design principles of Milvus 2.0

We made a start towards these goals in Milvus 1.0. But it's far from enough, especially in scalability and availability. Then we started the development of Milvus 2.0 to improve these points. The principles we have laid down for this new version include:

* Aiming for high scalability and availability
* Building upon mature cloud infrastructure and practice
* Minimum performance compromise in the cloud

In other words, we want to make the Milvus database cluster cloud-native.

## The evolution of database clusters

The vector database is a new species of database, as it handles new types of data (vectors). But it still shares the same challenges as other databases, with some of its own requirements. In the rest of this article, I will focus on what we have learned from the existing database cluster implementations and the thinking process of how we designed the new Milvus group architecture.

If you are interested in the implementation details of Milvus group components, please stay on top of the Milvus documentation. We will continuously publish technical articles in the Milvus GitHub repo, Milvus website, and Milvus Blog.

### The ideal database cluster

Let's first list the critical capabilities an **ideal** database cluster should have.

1. Concurrency and no single point of failure: users connected to different group members can simultaneously have read/write access to the same piece of data.
2. Consistency: different group members should see the same data.
3. Scalability: we can add or remove group members on the go.

Honestly, all of these capabilities are hard to acquire together. In the modern implementations of database clusters, people have to compromise some of these capabilities. People don't expect a perfect database cluster as long as it can fit into the user scenarios. However, the shared-everything cluster was once very close to an ideal database cluster. If we want to learn something, we should start from here.

### The key considerations of a database cluster

The shared-everything cluster has a more extended history compared to other modern implementations. Db2 data sharing group and Oracle RAC are typical of shared-everything clusters. Many people think shared-everything means sharing disks. It's far more than that.

A shared-everything cluster only has one kind of database member in the group. Users could connect to any one of these symmetric members to access any data. What is ""everything"" that needs to be shared for making this work?

#### The sequence of events in the group

First, the group event sequence is crucial to resolve the potential conflicts caused by the concurrent access from different groups members. We usually use the database log record sequence number to represent the event sequence. At the same time, the log record sequence number is generally generated from the timestamp.

Thus the requirement of group event sequence is equal to the need of a global timer. If we could have an atomic clock for the group, that would be fabulous. Yet, Milvus is an open-source software project, which means we should rely on commonly available resources. To date, an atomic clock is still a premium option for large companies.

We have implemented the time synchronization component in Milvus 2.0 database cluster. You can find the link in the appendix.

#### Global locking

The database has a locking mechanism to resolve concurrent access conflicts, whether optimistic or pessimistic locks. Similarly, we need global locking to resolve simultaneous access conflicts across different group members.

Global locking means different group members have to talk with each other to negotiate the lock requests. Several vital factors would impact the efficiency of this global lock negotiation process:

* The speed of inter-system connections
* The number of group members who need to participate in the negotiation process
* The frequency of group conflicts

The typical group size is no more than 100. For example, Db2 DSG is 32; Oracle RAC is 100. Those group members will be placed in one server room connected with optical fiber to minimize transfer latency. That's why it is sometimes called a centralized cluster. Due to the group size limitation, people will choose high-end servers (mainframes or minicomputers, which have much more capacity in CPU, memory, I/O channels, etc.) to consist of the shared-everything clusters.

This hardware presumption has dramatically changed in the modern cloud environment. Nowadays, cloud data centers comprise high-dense server rooms full of (thousands of) commodity X86 servers with TCP/IP connections. If we rely on these X86 servers to build the database cluster, the group size should increase to hundreds of (even thousands of) machines. And in some business scenarios, we will want these hundreds of X86 machines to spread in different regions. Thus implementing global locking might not be worth it anymore, as the global locking performance will not be good enough.

In Milvus 2.0, we are not going to implement the global locking facility. On the one hand, there is no update for vector data. (People should rather delete-then-insert instead of update.) So we don't need to worry about the multi-writer conflicts on the same piece of data in the Milvus group with sharding arrangement. Meantime, we could use MVCC (multi-version concurrency control, a lock-avoidance concurrency control method) to resolve the reader-writer conflicts.

On the other hand, vector data processing consumes a much higher memory footprint than structured data processing. People are looking for much higher scalability in vector databases.

#### Shared in-memory data cache

We can briefly divide a database engine into two parts: the storage engine and the computing engine. The storage engine is responsible for two critical tasks:

* Write data to permanent storage for durability purposes.
* Load data from the permanent storage to the in-memory data cache (AKA buffer pool); this is the only place where the computing engine accesses data.

In the database cluster scenario, what if member A has updated the data cached in member B? How could member B know its in-memory data is expired? The classic shared-everything cluster has a buffer cross invalidation mechanism to resolve this issue. The buffer cross invalidation mechanism will work similarly to global locking if we maintain a strong consistency across the group members. As stated before, it is not practical in the modern cloud environment. **So we decided to lower the consistency level in the Milvus cloud-scalable group to an eventual consistency manner.** In this way, the buffer cross invalidation mechanism in Milvus 2.0 can be an asynchronous process.

#### Shared storage

Shared storage is probably the first thing people would think about when discussing a database cluster.

Storage options have also significantly changed in recent years of cloud storage evolution. The storage attached network (SAN) was (and still is) the storage foundation of the shared-everything group. But in the cloud environment, there is no SAN. The database has to use the local disk attached to the cloud virtual machines. Using local disk introduces the challenge of data consistency across the group members. And we also have to worry about the high availability of the group members.

Then Snowflake made a great role model for cloud databases using cloud shared storage (S3 storage). It inspires Milvus 2.0 too. As stated before, we intend to rely on mature cloud infrastructure. But before we could utilize cloud shared storage, we have to think about a couple of things.

First, S3 storage is cheap and reliable, but it is not designed for instant r/W access like database scenarios. We need to create the data components (which we call data nodes in Milvus 2.0) to bridge the local memory/disk and S3 storage. There are some examples (like Alluxio, JuiceFS, etc.) we could learn. The reason we can not integrate these projects directly is we focus on different data granularity. Alluxio and JuiceFS are designed for datasets or POSIX files, while we focus on the data record (vector) level.

When the vector data is settled on S3 storage, the answer for metadata is easy: store them in ETCD. How about the log data, then? In the classic implementations, the log store is also based on SAN. The log files of one database group member are shared within the database cluster for failure recovery purposes. So this was not a problem until we got into the cloud environment.

In the Spanner paper, Google illustrated how they implemented the globally-distributed database (group) with Paxos consensus algorithm. You need to program the database cluster as a state machine replication group. The redo log is usually the ""state"" that will be replicated across the group.

Redo-log replication by consensus algorithms is a powerful tool, and it has substantial advantages in some business scenarios. But for the Milvus vector database, we don't find enough incentives for creating a state machine replication group as a whole. We decided to use the cloud messaging queue/platform (Apache Pulsar, Apache Kafka, etc.) as an alternative cloud shared storage for the log store. By delegating the log store to the messaging platform, we acquire the benefits below.

* The group is more event-driven, which means many processes can be asynchronous. It improves scalability.
* The components are more loosely coupled, making it much easier to perform online rolling upgrades. It improves availability and operability.

We will revisit this topic in the later section.

So far, we have wrapped up the crucial considerations of the database cluster. Before we can jump to the discussion on the Milvus 2.0 architecture, let me first explain how we manage vectors in Milvus.

### Data management and performance predictability

Milvus stores vectors in collections. The ""collection"" is a logical concept, equivalent to a ""table"" in SQL databases. A ""collection"" could have multiple physical files to keep vectors. A physical file is a ""segment"". The ""segment"" is a physical concept like a tablespace file in SQL databases. When the data volume is small, we can save everything in a single segment/physical file. But nowadays, we are constantly facing big data. When there are multiple segments/physical files, how should we spread the data in different data partitions?

Although data comes first rather than indexes, we have to store data in the way that the index algorithm prefers to make the data access efficiently in most cases. A frequently used strategy in SQL databases is partition by the range of partitioning key values. People usually create a clustered index to enforce the partitioning key. Overall, this is a decent approach for SQL databases. Data is stored in good shape, optimized for I/O (prefetch). But there are still defects.

* Data skew. Some of the partitions might have much more data than others. The distribution of real-world data is not as simple as the numeric range.
* Access hotspots. More workload might go to some of the data partitions.

Imagine more workload goes to partitions with more data. We need to rebalance the data across the partitions when these situations occur. (This is a DBA's tedious daily life.)

[The Clustered index for vectors](https://preview.redd.it/uhgtqzducu681.png?width=1280&format=png&auto=webp&s=6b35c68716670106daf662251cd326e0538b03f4)

We can also create a clustered index for vectors (an inverted list index). But that is not the same case as SQL databases. Once the index is built in SQL databases, it's very efficient to access the data through the index, with less computation and less I/O operations. But for vector data, there will be far more computation and I/O operations even with an index. So the defects mentioned before will have a more severe impact on vector database clusters. Moreover, the cost of rebalancing vectors across different segments is very high due to the data volume and computing complexity.

In Milvus, we use the strategy of partition by growth. When we inject data into a vector collection, Milvus will append the new vectors to the latest segment in the collection. Milvus will close the segment once its size is large enough (the threshold is configurable) and build the index for the closed segment. In the meantime, a new segment will be created to store the upcoming data. This simple strategy is more balanced for vector processing.

The vector query is a process to search for the most similar candidates in the vector collection. It is a typical MapReduce procedure. For example, we want to search the top 20 similar results from a vector collection with ten segments. We can search the top 20 on each one of the segments and then merge the 20 \* 10 results into the final 20 results. Since each segment has the same amount of vectors and a similar index, the processing time on each segment is almost identical. It gives us the advantage of performance predictability, which is essential when planning the scale of the database clusters.

### New paradigms in Milvus 2.0

In Milvus 1.0, we implemented a read/write splitting sharding group like most SQL databases. It was a good attempt at scaling the Milvus database cluster. But the problems are quite obvious too.

[Milvus 1.0: sharding cluster](https://preview.redd.it/yc51oizzcu681.png?width=1280&format=png&auto=webp&s=ccd186df0d97eba91c2d8568fbf4ae19ea1902f9)

In Milvus 1.0, the r/W node has to take total care of the latest segment, including vector appending, searching in this unindexed segment, building index, etc. Since each collection only has one writer, the writer is very busy if the data is continuously streamed into the system. The performance of data sharing between the r/W node and the reader nodes is also a problem. Besides, we must either rely on NFS (not stable) or premium cloud storage (too expensive) for shared data storage.

These existing problems are hard to tackle in the Milvus 1.0 architecture. Thus, we have introduced new paradigms into the Milvus 2.0 design to resolve these issues.

[Milvus 2.0: cloud-scalable vector database](https://preview.redd.it/wveh4vj4du681.jpg?width=2000&format=pjpg&auto=webp&s=cb203a8b1498836f01207f44a963b5d2d76bddcb)

#### Actor model

There are two models to program concurrent computation systems.

* Shared memory that means concurrency control (locking) and synchronous processing
* The actor model (AKA message passing) means message-driven and asynchronous processing

We can also apply these two models in distributed database clusters.

As stated before, most high-profile distributed databases use the same method: redo-log replication by consensus algorithms. This is synchronous processing using consensus algorithms to build a distributed shared memory for redo-log records. Different companies and venture capitals have invested billions of bucks in this technology. I didn't want to comment on this until we started to work on Milvus 2.0. Many people regard this technology as the only way to realize distributed database systems. This is annoying. If I don't say something, people might misunderstand that we were reckless in distributed database design.

In recent years, Redo-log replication by consensus algorithms has been the most overestimated database technology. There are two key issues.

* The presumption that redo-log replication is better is fragile.
* Vendors mislead people's expectations on the capability of consensus algorithms.

Let's say we have two database nodes, the source node, and the target node. In the ever beginning, they have the exact copy of the data. We have some change operations (I/U/D SQL statements) on the source node, and we want to keep the target node updated. What should we do? The simplest way is to replay the operations on the target node. But this is not the most efficient way.

Thinking about the running cost of an I/U/D statement, we can divide it into the execution preparation and the physical work parts. The execution preparation part includes the work of SQL parser, SQL optimizer, etc. No matter how many data records will be affected, it is a fixed cost. The cost of the physical work part depends on how many data records will be affected; it is a floating cost. The idea behind redo-log replication is to save the fixed cost on the target node; we only replay the redo-log (the physical work) on the target node.

The cost-saving percentage is the reciprocal of the number of redo-log records. If one operation only affects one record, I should see significant savings from redo-log replication. What if it's 10,000 records? Then we should worry about the network reliability. Which one is more reliable, send the one operation or the 10,000 redo-log records? How about one million records? Redo-log replication is super in scenarios like payment systems, metadata systems, etc. In these scenarios, each database I/U/D operation only affects a small number of records (1 or 2). But it's hard to work with I/O intensive workloads like batch jobs.

Vendors always claim consensus algorithms could provide strong consistency to the database clusters. But people only use consensus algorithms to replicate the redo-log records. The redo-log records are consistent on different nodes, but that doesn't mean the data views on other nodes are consistent either. We have to merge the redo-log records into the actual table records. So even with this synchronous processing, we can still only get eventual consistency on the data views.

We should use redo-log replication by consensus algorithms in the appropriate places. The metadata system (ETCD) and messaging platform (e.g., Apache Pulsar) used in Milvus 2.0 have implemented consensus algorithms. But as I said before, ""for the Milvus vector database, we don't find enough incentives for being a state machine replication group as a whole.""

In Milvus 2.0, we use the actor model to organize the worker nodes. The worker nodes are lonely. They only talk to the messaging platform, getting commands and sending results. It sounds boring.

The actor model is asynchronous. It is suitable for scalability and availability. Since the worker nodes don't know each other, there is no impact on other worker nodes if some of the worker nodes join or are removed.

#### Separation of availability and durability

In Milvus 2.0, we do operation replay rather than log replay, because in the vector database, there is not much difference between operation replay and log replay. We don't have the Update function nor the Insert with Select function. And it's also much easier to do operation replay with the actor model.

So multiple worker nodes might execute the same operation from the messaging platform according to their responsibility. I mentioned before we decided to use the S3 cloud storage as the shared storage layer of the Milvus database cluster. The S3 storage is very reliable. Then is it necessary for different worker nodes to write out the same data to the shared storage?

Thus we designed three roles for the worker nodes.

* The query node maintains an in-memory data view according to the assignment. The work of the query node includes doing vector search and keeping the in-memory data updated. But it doesn't need to write anything to the S3 storage. It is the most memory-sensitive node in the group.
* The data node is responsible for writing the new data to the S3 storage. The data node doesn't need to maintain the in-memory data view, so the hardware configuration of the data node is quite different from the query node.
* The index node builds indexes for the segments closed by the data node when the size of the segments reaches the threshold. This is the most CPU-intensive work in the group.

These three types of nodes represent different kinds of workload. They can scale independently. We call it separation of availability and durability learned from the Microsoft Socrates cloud database.",MachineLearning
rl7jfz,1640065650.0,[P] OSLO: Open Source framework for Large-scale transformer Optimization,"&#x200B;

https://preview.redd.it/e1mnp1zk3u681.png?width=825&format=png&auto=webp&s=8559cab81f8655aa8eb2c260b708a5d68688aad9


OSLO is a framework that provides various GPU based optimization features for large-scale modeling. As of 2021, the \[Hugging Face Transformers\]([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)) is being considered de facto standard. However, it does not best fit the purposes of large-scale modeling yet.


This is where OSLO comes in. OSLO is designed to make it easier to train large models with the Transformers. For example, you can fine-tune \[GPTJ\]([https://huggingface.co/EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B)) on the \[Hugging Face Model Hub\]([https://huggingface.co/models](https://huggingface.co/models)) without many extra efforts using OSLO. Currently, GPT2, GPTNeo, and GPTJ are supported, but we plan to support more soon.
For more information, see [https://github.com/tunib-ai/oslo](https://github.com/tunib-ai/oslo) !",MachineLearning
rl4nqz,1640056138.0,[D] Why is deep learning regarded as black-box? Why do people continue to insist that we don't know how it works or why it does what it does?,"

Genuinely curious; multiple papers I've read over the past year make the claim that a clear understanding of how DNNs work and why they work is yet to be achieved. Perhaps it is ignorance, but I view these claims incredulously. A DNN is a giant logistic regressor made up of smaller logistic regressors, and it is fitting a curve. There is absolutely no magic, it is actually very mechanical and borderline mundane.

So why the proliferation of magical claims professing our ignorance? What exactly are all these authors referring to?

For reference:

[https://arxiv.org/pdf/2108.07258.pdf](https://arxiv.org/pdf/2108.07258.pdf)

*""Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities, and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models,* ***we currently lack a clear understanding of how they work***\*, when they fail, and what they are even capable of due to their emergent properties.""\*

A foundational model here is literally a model such as those in the imagenet zoo, used for transfer. It's got a vast diversity of feature representations? The layers responsible for highly fundamental, and therefore transferable, features have converged closer to some effervescent global optima? It is a very big regressor? Like what is the mystery here? Am I just stupid?",MachineLearning
rl12cu,1640045355.0,[P] I made Blockly for Machine Learning!!,"Hello Everyone!!

I  developed a Visual Programming Language from Google Blockly to generate and run python code which supports basic ML algorithms. This UI allows you to download and upload XML layout too.

GitHub: [https://github.com/chekoduadarsh/BlocklyML](https://github.com/chekoduadarsh/BlocklyML)

hosted webpage: [https://blocklyml.herokuapp.com/](https://blocklyml.herokuapp.com/)

please star the repo if you like, and if you think addition of any new feature will be a bonus don't forget to raise an issue!!!",MachineLearning
rkxmt2,1640035649.0,[D] Question about collaboration with data scientists and engineers?,"Hello! (newby to the group here), I was wondering if you would mind elaborating on potential demand for ML operations collaboration with data analytics departments and data scientists?  When I first started in data science the last thing I  wanted to do was integrate with APIs and perform ETL, clean and format data into feature sets, and manage models.  I had the most fun calculating interesting statistical reports and building and running simulations.  I'm wondering if with today's data science landscape and tools available folks are still feeling that problem?  If the problem exists how are folks handling it, particularly when building production capabilities from the research and models?

A  little background if you are interested... I started developing over 20 years ago because of an interest in data science (particularly in finance and economics), multi agent systems, and machine learning (in Lisp at the time), but my career (application and hosting platform engineering and architecture) took me away from that objective, and I gradually became unhappy with what I was building and managing.  I made a choice to get back to that passion and I started working on a platform to do the underlying plumbing of managing cloud infrastructure and data integration and processing, so I  personally could create multi agent simulators and self optimizing recommendation engines.  But admittedly I am behind on my knowledge about what is really going on in the data science market.  I created this open source project after my brother died and I would have built it even if nobody else in the world wanted it because it connected my experience with my passion, and I really needed something fun to do.

The reason I am thinking the demand might be there is I have a friend who has a small AI consulting shop and I have been able to work with his team on a US federal government project, which allowed me to work with others on model strategy and implementation.  They did the strategy and initial model implementation in notebooks and rough libraries, I translated that into more pluggable and scalable application code, and my open source software is handling the production ops and recommendation engines.  We all got to do what we loved and it seems to be working out well for the client.   This project took me back to a place I loved in software engineering, and I am wondering if this would be a good market to serve, because starting in the new year I HAVE to start generating some more revenue and I would prefer to do that in a area I am passionate about.",MachineLearning
rkwwja,1640033641.0,[P] Blog post about the birth and the evolution of the image augmentation library Albumentations,"I was always curious how open-source projects are born and evolve.


I  had a chance to participate in the creation and development of the open-source library Albumentations. The library is, probably, known to people who train neural networks for Computer Vision tasks.


For the sake of logging, I wrote a blog post about the library. How it was born, how it evolved, and how we promoted it.


It is long, and, probably, full of unnecessary details, but I wanted to cover as much detail as possible to make it useful to myself and to others who work in open-source.


The library was downloaded 2.6 million times, is widely used in machine learning competitions, and even has adoption in academia. The largest user of the library is China.


[https://medium.com/@iglovikov/the-birth-of-albumentations-fe38c1411cb3](https://medium.com/@iglovikov/the-birth-of-albumentations-fe38c1411cb3)",MachineLearning
rkwowu,1640033054.0,[R] Improving the expressive power of GNNs using subgraphs,"The expressive power of Message-Passing Graph Neural Networks is inherently limited due to their equivalence to the Weisfeiler-Lehman graph isomorphism test. Several concurrent recent works show that this limitation can be overcome by applying a GNN on a collection of subgraphs obtained by removing nodes or edges from the input graph.

In a new post co-authored with Leonardo Cotta, Fabrizio Frasca, Haggai Maron, Christopher Morris and Lingxiao Zhao, we review the common themes and nuances of these different approaches:

&#x200B;

[https://towardsdatascience.com/using-subgraphs-for-more-expressive-gnns-8d06418d5ab?sk=8806ffcd9ecf74c440d40df53528c1c7](https://towardsdatascience.com/using-subgraphs-for-more-expressive-gnns-8d06418d5ab?sk=8806ffcd9ecf74c440d40df53528c1c7)",MachineLearning
rkw26f,1640031315.0,"AWS: A Visual, Interactive Introduction (and Explanation) to Double Descent [P]","I hope this is allowed here, given it's not an arxiv-link (though the derivation in the second article can easily be thrown into one).Two articles about Double Descent.The first introduces the concept in a visual manner: [https://mlu-explain.github.io/double-descent/](https://mlu-explain.github.io/double-descent/)The second provides an explanation *for linear models* as a relation to energy in natural cubic splines: [https://mlu-explain.github.io/double-descent2/](https://mlu-explain.github.io/double-descent2/)

Both designed to be quick for consumption and understanding!",MachineLearning
rkvq1x,1640030401.0,[N] Compute Costs with Machine Learning,"A team of researchers from [MIT](https://web.mit.edu/), [Yonsei University](https://www.yonsei.ac.kr/en_sc/index.jsp), and [University of Brasilia](https://international.unb.br/) has launched a new website, [Computer Progress](https://www.computerprogress.com/),  which analyzes the computational burden from over 1,000 deep learning research papers. Data from the site show that computational burden is growing faster than the expected rate, suggesting that algorithms still have room for improvement.

More work is needed on algorithmic efficiency over brute force compute + data strategies

&#x200B;

This validates many of the concerns stated in this post: [https://blog.modernmt.com/the-carbon-footprint-of-machine-learning/](https://blog.modernmt.com/the-carbon-footprint-of-machine-learning/)",MachineLearning
rkrcyh,1640018390.0,[P] Inverting PhotoDNA with Machine Learning,"Microsoft [PhotoDNA](https://www.microsoft.com/en-us/photodna) creates a “unique digital signature” of an image which can be matched against a database containing signatures of previously identified illegal images like CSAM. The technology is [used](https://www.makeuseof.com/what-is-photodna-how-does-it-work/) by companies including Google, Facebook, and Twitter. Microsoft says:

>A PhotoDNA hash is not reversible, and therefore cannot be used to recreate an image.

This project shows that PhotoDNA does not perfectly hide information about the source image used to compute the signature, and that in fact, a PhotoDNA hash can be used to produce thumbnail-quality reproductions of the original image:

&#x200B;

[Rough body shapes and faces can be recovered from the PhotoDNA hash. \(The face is from thispersondoesnotexist.com\)](https://preview.redd.it/yvm6pupt6q681.png?width=800&format=png&auto=webp&s=8804055aff330b74f9059652d508a8370248abab)

More details about PhotoDNA and the approach used to invert PhotoDNA hashes are in this [blog post](https://www.anishathalye.com/2021/12/20/inverting-photodna/). Code is available on [GitHub](https://github.com/anishathalye/ribosome).",MachineLearning
rkqfjc,1640015894.0,"[D] How Shopify Applies ML for Anomaly Detection and Forecasting At Scale, with Dr. Ella Hilal, Head of Data Science, Engineering, Revenue and Growth at Shopify at Enterprise Data & AI - Jan 6 at 11:30 AM","Hi r/MachineLearning!

I wanted to share a webinar coming up in January 2022 at Enterprise Data & AI. I put the info from the website below along with the link if you're interested. I'm interested in hearing how they built their anomaly detection engine and how that has been performing.

\-------------

**Featured Guest Speaker: Dr. Ella Hilal, Head of Data Science, Engineering, Revenue and Growth at Shopify!**

At Shopify, we have over 1.7 million merchants across over 175  countries, with hundreds of millions of consumers shopping at their  stores. We’re focused on leveraging the scale of our data to not only  empower Shopify, but to create new experiences for our merchants that  are impossible without data. In our daily operations at Shopify, we are  highly data informed. Some of the ways we’re leveraging advanced  analytics is by building an anomaly detection engine that allows us to  process over hundreds of thousands of metric/segment combinations in a  very accessible way. In her talk, Dr. Hilal will share key tips on how  you can apply machine learning for anomaly detection and forecasting at  scale.

**Agenda:**

* 11:30-12:30pm: Featured Presentation
* 12:30-13:00pm: Your Q&A and interaction

\------------

Link to website: [https://events.cognilytica.com/CLNTg1NHwyNA](https://events.cognilytica.com/CLNTg1NHwyNA)",MachineLearning
rkoj9z,1640010614.0,[D] What open-source dataset lacks annotations?,"Hi r/MachineLearning,

Data Scientist at DagsHub here.

I'm personally excited about open-source software and open-source data science. As part of the recent Hacktoberfest, we decided to host a community challenge to create and curate open-source audio datasets – We got over **40 contributions from you**, which is awesome! These are all easily accessible here: [https://dagshub.com/DagsHub/audio-datasets](https://dagshub.com/DagsHub/audio-datasets).

We want to keep the momentum and continue **contributing to the** **open-source dataset** ecosystem – this time, by focusing on a **labeling challenge.**

We want to make something useful for everyone, so I'd love to hear from you **what open-source dataset lacks annotations, and what you think will help the community the most?**

The only limitation is the data must be open source so it can be made available to everyone!

I'd love to hear your thoughts and feedback.",MachineLearning
rkla87,1639999886.0,[D] Hyperparameter Optimization for Noise Robustness,"Imagine you propose a novel method that claims to be more robust towards label noise, e.g., in a classification setting, in the sense that its generalization performance does not suffer as much as other methods do when facing increasing degrees of noise. A common scheme to demonstrate such behavior in an empirical evaluation is to vary the noise degree of the training data and report the resulting test performance of models trained on this data.

However, many studies miss a proper hyperparameter optimization to conduct somewhat fair comparisons (or they do not report the way they optimized the parameters in depth). Often, it does not become clear to me whether the validation data (think of e.g., single train-/val-splits - employing a cross-validation makes it even less obvious) is assumed to have the same noise as the training data or being a ""cleaner"" representative of the test data distribution. What kind of evaluation scheme (noisy train and val data + clean test vs. noisy train + clean val and test data) would be preferred when?",MachineLearning
rkl5ev,1639999385.0,"[P] Weather station project should get trend symbols, powered by sklearn rbf trend prediction","Hello fellows,

&#x200B;

for my weather station project (raspberry pi + bosch bme280 temp/hum/pressure sensor) i want to show trend prediction symbols. Because the raw input from the sensor variies and because i want to become in touch with ml i want to use sklearn rbf for interpolation of the temp/hum/pres data (already working) and for trend prediction (not working). I just want to say: ""from now its getting warmer etc"" according to my model. (if i just comprae the last raw values it doesnt work good, because the raw values fluctuating)

&#x200B;

What i think i have to do: i think i have to calculate the second confidence (german: ""2. Konfidenz, lokale Extremwerte""), then i know if i have a local minimum/maximum. So from now its likely that the temperature is rising/sinking.

&#x200B;

My weather station uses rbf kernel do draw a nice and smooth line for temp/hum/pres (i feed numpy array to sklearn rbf and get result numpy array as result), based on \[1\]. But i dont know how to calculate/predict if im on al local extrema and therefore have to change my trend symbols.

&#x200B;

\[1\] [https://scikit-learn.org/stable/auto\_examples/svm/plot\_svm\_regression.html](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html)

&#x200B;

What do you think ?

&#x200B;

EDIT: fixed typos",MachineLearning
rkkpcg,1639997739.0,[D] How much can a single 3D model replace a dataset?,"Example, for the sake of argument:
I love my epoxy-encased hotdog more than life itself. He likes to travel around the world and I want to make sure he's safe. I have every camera in existence to try to track his location at all times of the day or night, but I can't collect data from them for training. Before he left, I took a perfect 3d scan of him so I can simulate angles, lighting conditions, resolutions, occlusions, and optical parameters so no matter what camera I'm using I can find him. I have no tolerance for false negatives in case my baby ever needs my help.

&#x200B;

To put it more seriously - as 3d ML techniques evolve and datasets like Objectron and CO3D help us isolate objects....

* Do simulated datasets begin to play a bigger part as well?
* Are there any good controlled experiments of detection between training with *really good* renders vs. real-world data?
* Are there companies focusing on ""custom data augmentation"" right now?",MachineLearning
rkewa3,1639975493.0,[D] What are your machine learning superstitions?,"Or things you believe that you have ZERO explanations for.

Here's mine:

* Random seed = 0 gives bad results
* Random seed = 42 GOOD RESULTS
* Even-valued k in k-Means = insightful segmentation",MachineLearning
rkdxfs,1639972237.0,[D] 100x faster NeRF explained - Plenoxels: Radiance Fields without Neural Networks 5-minute summary (by Casual GAN Papers),"Every now and then comes along an idea so pertinent that it makes all alternatives look too drab and uninteresting to even consider. NeRF, the 3D neural rendering phenomenon from last year, is one such idea… Yet, despite the hype around it Alex Yu, Sara Fridovich-Keil, and the team at UC Berkley chose another approach to focus on. Perhaps surprisingly, without any neural networks at all (yes, you are still reading a blog about AI papers), and even more surprisingly, their approach, coined Plenoxels, works really well! The authors replace the core component of NeRF, the color, and density predicting MLP, with a sparse 3D grid of spherical harmonics. As a result, learning Plenoxels for scenes is two orders of magnitude (100x) faster than optimizing a NeRF, and there is no noticeable drop in quality whatsoever.

Crazy? Yeah, let’s learn how they did it!

Full summary: [https://t.me/casual\_gan/222](https://t.me/casual_gan/222)

Blog post: [https://www.casualganpapers.com/nerf-3d-voxels-without-neural-networks/Plenoxels-explained.html](https://www.casualganpapers.com/nerf-3d-voxels-without-neural-networks/Plenoxels-explained.html)

[Plenoxels - 100x faster NeRF](https://i.redd.it/g3tq7xyidm681.gif)

[arxiv](https://arxiv.org/abs/2112.05131) / [code](https://github.com/sxyu/svox2)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",MachineLearning
rkasgc,1639962383.0,[P] An easy framework for pretraining lanuguage models,"* Project link: [https://github.com/lassl/lassl](https://github.com/lassl/lassl)

Hi, ML redditors! I and my colleagues( u/seopbo, and more ) made **LA**nguage framework for **S**elf-**S**upervised **L**earning (LASSL). LASSL aims to provide an easy-to-use framework for pretraining language model by only using Huggingface's Transformers and Datasets. Currently, BERT, RoBERTa, GPT2, and ALBERT are provided, and the model will be continuously updated in the future. Also, in order to see the effectiveness of the code, we will continue to add models trained using it to the model hub along with downstream evaluation. We hope it helps whoever want to make their own language models to make it easy and fast.

I hope it will be helpful for someone. :)

Cheers.",MachineLearning
rk7ffw,1639952648.0,[Discussion] Was GPT-3 trained to achieve double descent phenomena?,"Question in title. Also,

Are there any other Language models that have attempted to re-create this phenomena to aid in breaking SOTA benchmarks?",MachineLearning
rk77ho,1639952048.0,[D] How to build a binary classification model on an imbalanced dataset that performs better than a naive model which always predict the majority class?,"I'm working on building a binary classification model on an imbalanced (95% majority class: 0) dataset. The model will ultimately only be evaluated on a similarly imbalanced test set for which the only goal is highest overall accuracy (even though I understand one typically shouldn't use accuracy for evaluation on an imbalanced dataset).

My best models so far never predict the minority class: 1, giving me 95% accuracy. But, I'm lost as to how I can make this higher given that these models are only performing as well as a naive model which just predicts 0 all of the time.

Should my training set include more of the minority class (e.g. do SMOTE, up/down sampling) even if the model will only ultimately be evaluated on a similarly skewed test set? Should I be using a different model (currently Logistic Regression and Random Forest work best)? Should my model instead output probabilities for which I can set a threshold myself to make predictions (any resources on this)? Is there a way in Python to change the loss function to punish the model for getting the minority class wrong?",MachineLearning
rk1rql,1639936134.0,[D] Simplefeature - A tiny python package with almost no dependencies that lets you extract deep local features!,"Check it out! A tiny python package that requires almost no dependencies, and lets you extract generalized ""deep"" features from any image! Built off of some older, but still solid, work from FG2011. Anyone here have a cool use for something like this?

[https://pypi.org/project/simplefeature](https://pypi.org/project/simplefeature/)",MachineLearning
rk117l,1639933928.0,[D] What will come after Machine Learning?,"Hi, I would like to know according to your experience what will be the next hot topic. Some people might say Machine learning / Data Science will never die but I would like to know what will be the trend in the next couple of years. Would it be Quatum computing?
If it will be machine learning, what will be the topic in ML / DL?

Thank you in advance.",MachineLearning
rjzlf7,1639929617.0,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning
rjye12,1639925950.0,[D] Why does a sudden increase in accuracy at a specific epoch in these model,"I am learning Convolution Neural Network now. Recently, I have been reading papers related to optimizers, such as Radam.  When looking at the visual results of papers, I found that their images showed a sudden increase in accuracy at the 80th epoch( the figure6 in paper ""ON THE VARIANCE OF THE ADAPTIVE LEARNING RATE AND BEYOND"" ) [https://arxiv.org/pdf/1908.03265.pdf](https://arxiv.org/pdf/1908.03265.pdf) or 150th epoch （the figure3 in paper ""ADAPTIVE GRADIENT METHODS WITH DYNAMICBOUND OF LEARNING RATE""）[https://arxiv.org/pdf/1902.09843.pdf](https://arxiv.org/pdf/1902.09843.pdf) no matter what kind of algorithm.

 Can anyone tell me why this happened? Thank you.

&#x200B;

[Radam](https://preview.redd.it/voks38ooji681.png?width=968&format=png&auto=webp&s=7e8c74eaa3c3f26578269336069160fe5165cadd)

&#x200B;

[Adabound](https://preview.redd.it/av7o3j2qji681.png?width=590&format=png&auto=webp&s=ab816c06bc8eefd6b933e550e55ff3c4b2feb9a8)",MachineLearning
rjy067,1639924702.0,"[D] Anyone got a hypothesis on why ""Icing on the Cake"" method improves accuracy?","[https://arxiv.org/pdf/1807.06540.pdf](https://arxiv.org/pdf/1807.06540.pdf)

From my understanding, the icing on the cake paper claims higher accuracy by saving the feature maps generated by convolution layers before the classifier layer during training , and using them to retrain just the classifier (fully connected layers) again.

The authors comment they have no idea why. I hypothesize that (assuming the classifier training feature maps are given the same order as original training) because the feature maps from early in training are very poor (CNN not trained and bad at extracting features)  they actually act as a regularization method for the classifier. Then  the much better feature maps from the end of original training re converges the classifier to a slightly different point in the loss landscape. This seems to be similar to layer wise regularization through noise injection perhaps?

What are your guys thoughts?",MachineLearning
rjwwfi,1639920981.0,[R] Advice for training on full ImageNet?,"For my project I need to train on the full ImageNet from scratch. It’s taking me ages and I’m struggling to get good accuracy. Any advice on general approach / directions to follow / parameter choices would be greatly appreciated.

Setup:

PyTorch, VGG16, ILSVRC2012 (ImageNet).

Batch size = 300, optimizer = SGD, lr = 0.001, momentum = 0.9, weight decay = 0.006, num workers in DataLoader = 12, pin memory in DataLoader = true

Using nn.DataParallel for the five blocks of VGG16.

Hardware: a cluster with RTX6000, I'm using 4.

&#x200B;

I have been getting many cuda OOM errors, hence the choice of small batch and particular num workers. These are the highest that seemed to work. Generally, the cluster I’m on seems to affect training when many other jobs are running.

In terms of parameters, I’ve heard that Adam should do a much better job as the optimizer compared to SGD, so I’m going to try that next. Also, from what I found online PyTorch’s DistributedDataParallel seemed like a useful, but difficult to implement parallelization option. I’m not sure it will be manageable in the time frame of my project.

This is what training looks like at the moment:

https://preview.redd.it/2hqjsyg95i681.png?width=947&format=png&auto=webp&s=491a67852a775734ae7ddf1df74236c5dea7d69b

PS: I am a newbie in ML if that's not obvious already lol",MachineLearning
rjw51t,1639918250.0,[P] - Dataset: Omicron daily cases by country (COVID-19 variant),"&#x200B;

Hi Guys,

Unfortunately, Omicron is getting worse every day infecting more and more people around the world.

I've made an auto-updating daily omicron case by country dataset.

&#x200B;

It is available \[here\]([https://www.kaggle.com/yamqwe/omicron-covid19-variant-daily-cases](https://www.kaggle.com/yamqwe/omicron-covid19-variant-daily-cases)).

&#x200B;

When I first posted this I just wanted to have a dataset for myself because I was curious about the speed at which omicron infects the world. But as time progresses and the situation is getting worse this might be useful for everyone.

&#x200B;

\*\*The data\*\*

1. location- this is the country for which the variants information is provided;

2. date - date for the data entry;

3. variant - this is the variant corresponding to this data entry;

4. num\_sequences - the number of sequences processed (for the country, variant, and date);

5. perc\_sequences - the percentage of sequences from the total number of sequences (for the country, variant, and date);

6. numsequencestotal - total number of sequences (for the country, variant, and date);

&#x200B;

&#x200B;

Enjoy yourself. Hopefully, the dataset will become useless soon!

Stay safe!",MachineLearning
rjvm5e,1639916149.0,Over/under parametrizing in random feature models [D],"Let's say I have a dataset X (nxd)  from which I construct a ""feature dataset"" F (nxp) doing relu(XW) where W is a random matrix.
How will the relation between d and p influence the train and test loss? Is there a sort of bias-variance tradeoff?",MachineLearning
rjsh06,1639902460.0,"[D] For those of you who don't own a GPU, how do you run your experiments or train your models?","I'm aware that many people use cloud computing services like AWS or GCP, but I'm curious if there are any other methods out there that I'm not aware of.

### Edit

I should have clarified this... When I say ""don't own a GPU"" I meant ""don't have access to a privately owned GPU."" I used to use my lab's servers and GPUs but now that I'm graduating I can't use those anymore.",MachineLearning
rjqgrc,1639894153.0,[P] One to many mapping using probabilistic UNet,"Hi!

I have been working on this project for over a month now. However, I couldn’t make it work yet. So, I need help.

It is basically applying the [probabilistic UNet](https://arxiv.org/pdf/1806.05034.pdf) paper on the leaf segmentation dataset.

Given an image, I want to output one instance at a time by sampling from a distribution.

The probabilistic UNet architecture is shown below.

&#x200B;

https://preview.redd.it/nfd5nd2hxf681.png?width=1761&format=png&auto=webp&s=4b7cb75cd80553c656e7078cc5b4bbc576ba2c58

&#x200B;



The architecture has two loss components; the KL divergence and reconstruction loss. When I train it, the KL loss goes to zero, however the reconstruction loss works differently that what I want.

What I want:

1. Sample a noise
2. Get one instance

&#x200B;

https://preview.redd.it/0hv9u7ejxf681.png?width=1235&format=png&auto=webp&s=f80255893ef97c49fdf3ec05a63b86cd60dec2e5

&#x200B;

 What actually happens is that the noise is ignored it acts like a semantic segmentation. Basically, the RGB image and the different instances are mapped to the same latent space. I have tried [beta scheduling](https://arxiv.org/pdf/1903.10145.pdf) KL, and even removed the KL term, but the noise is still ignored.

&#x200B;

&#x200B;

https://preview.redd.it/r6yfd5qlxf681.png?width=1257&format=png&auto=webp&s=41b4986b0cca92b9f9eda5a09442be1f1092f375

&#x200B;

Can someone be kind enough to review my code [here](https://github.com/Assefa123/Probabilistic-UNet) or explain why this approach might not work?

Thank you.",MachineLearning
rjnwo2,1639884997.0,Sinusoidal kernel for nearest neighbors? [R],"I have a partially ordered set and want to find n subsets where the set consists of numbers that have the least error when, in the original order, the values are evaluated for harmonic motion. Some weird stuff: the motions will all have the same frequency. The phase may be different. There is not a geometric lattice, I.e some elements of the subset may go away in a fixed window, but come back later, and the order can therefore change. (This will change the frequency, but erroneously.) I can evaluate the data for the gist of the ground truth, in a more manual way, but I’m hoping to find a vectorized solution that uses a target function, a number of subsets to find, and uses something efficient to solve for set membership like a KNN approach. My theory is I can do this by FFT with a target window size and then make use of the imaginary part to deconvolve the set...?",MachineLearning
rjk88d,1639872676.0,Cheap Conferences [D],"NeurIPS last week was my first conference. Only cost 25 dollars for me as a student. Are there any other good machine learning conferences that are affordable? AAAI 22, e.g., I find a little expensive at 145 dollars for students.",MachineLearning
rjhlot,1639864551.0,Can you help me to find a book in the arxiv? I am going insane![D],"**EDIT: FOUND IT**

Thanks so much to u/fni19WYm who has definitely a better brain than me

The book is the nicely titled: ""Algebra, Topology, Differential Calculus, and
Optimization Theory for Computer Science and Machine Learning"" by Jean Gallier and Jocelyn Quaintance ,adding up to 1958 pages!!

https://www.cis.upenn.edu/~jean/gbooks/geomath.html



*ORIGINAL POST*

I  lost my hard drive some months ago and with it a book whose table of contents I recall liking.I was looking forward to read it. But now, I can't find it!

This is what I remember:

The book is written by a mathematician, it was a hefty volume (at least 800-900 pages) and it was about the mathematics of machine learning or deep learning (tbh I dont remember which one) but it is possible it was not titled that way because I havent been able to find it using those terms and you know how mathematicians are (""Elements of analysis and algebra in the study of universal approximators""???)

As noted in the title it was posted to the arxiv, it had not been published as far as I recall. It is the kind of book senior professors write summarizing lots of stuff.

The book was rigorous, I think it starts all the way back to analysis or even set theory and spends like 500 pages before ""touching ground""

The author is a French mathematician I think, although based in the US

I know the link for the book has been posted here and also in Hacker News.

Obvious notes:

No, the professor is not Mr LeCun

No, the book is not MML or the Bengio et al one

I have been racking my brain all day and nothing. Maybe one of you guys can provide relief.

I reserve the right to be wrong in some of the details, you know the brain is a tricky bastard.

Thanks",MachineLearning
rjgbxc,1639860745.0,[P] Memory Efficient Attention - Self-attention Does Not Need O(n^2) Memory,"This project is **unofficial** implementation of [Self-attention Does Not Need O(n\^2) Memory](https://arxiv.org/abs/2112.05682v2) for Jax and PyTorch.

Implementation is almost the same as the one proposed in the paper, with additional **masking and adding bias compatibility**, **batch dimensions support,** and **PyTorch implementation**. For computing attention, the proposed method requires only **O(sqrt(n))** memory, and the provided functions can be used as a drop-in replacement for attention calculation.

Github: [https://github.com/AminRezaei0x443/memory-efficient-attention](https://github.com/AminRezaei0x443/memory-efficient-attention)",MachineLearning
rjg6uz,1639860321.0,[D] Paper Explained - Resolution-robust Large Mask Inpainting with Fourier Convolutions (w/ Author Interview),"[https://youtu.be/Lg97gWXsiQ4](https://youtu.be/Lg97gWXsiQ4)

At the end of the video is an interview with the paper authors!

LaMa is a system that is amazing at removing foreground objects from images, especially when those objects cover a large part of the image itself. LaMa is specifically trained to reconstruct large masked areas and includes global information throughout its forward propagation by using Fourier Convolutions in its layers. This makes it incredibly effective at reconstructing periodic structures with long-range consistency, compared to regular convolutions.

&#x200B;

OUTLINE:

0:00 - Intro

0:45 - Sponsor: ClearML

3:30 - Inpainting Examples

5:05 - Live Demo

6:40 - Locality as a weakness of convolutions

10:30 - Using Fourier Transforms for global information

12:55 - Model architecture overview

14:35 - Fourier convolution layer

21:15 - Loss function

24:25 - Mask generation algorithm

25:40 - Experimental results

28:25 - Interview with the authors

&#x200B;

Paper: [https://arxiv.org/abs/2109.07161](https://arxiv.org/abs/2109.07161)

Code: [https://github.com/saic-mdal/lama](https://github.com/saic-mdal/lama)

Online Demo: [https://cleanup.pictures/](https://cleanup.pictures/)",MachineLearning
rjdy4s,1639853732.0,[Discussion] Should there be a new IDE built from the ground up for ML Engineers/Practioners?,"There are a lot of plugins for things like VSCode and folks who use PyCharm and the like. I've even found some interesting early projects like this, focused on making ML engineering easier: [https://torchstudio.ai/](https://torchstudio.ai/).


Curious to hear from the community, what might an IDE built purely for ML Engineers from the beginning look like? Should something like this exist? I'm someone who has been working in this space for a while, and there are so many tools, that I keep coming back to this idea of creating an IDE just for this space.",MachineLearning
rjckcw,1639849655.0,[R] optimizing model input based on some desired output,"Hey all, would like to hear some of your opinions regarding my research:In my lab, we are working on a bio-inspired small aerial vehicle,  (resembles a fly), and there has been a suggestion to use ML principles to enhance its performance.

one option was related to this framework: we provide some input I to our motor (via controller), say a sine wave. such input should maximize some measurable output y (for example - the robot's altitude). the question is - what would be the best I (that maximizes y)?

one could think of such a problem in a more ""mathematical"" form -  suppose our robot can be represented by some function R (unknown to us), such that R(I)=y. our goal would be to find arg-max R(I).

a somewhat ""trivial"" suggestion is to go over all options for I and find the maximum value of R(I), but such space of inputs can be very big.

Anyway, I was trying to classify this task to some known frame of ML related tasks, but couldn't find the answer

few last thoughts, to maybe spark a discussion - I thought of this problem as sort of cross-validation (of the input I), but again, checking for all I's sound like a bad idea. Another thing is that this problem doesn't seem to be supervised or even unsupervised - the input changes. perhaps RL?. also, the input can be thought of as sequential, but still, I think that RNNs aren't really suitable (not supervised per se)

and if you've read thus far - thanks! would appreciate your comment",MachineLearning
rjbsxn,1639847390.0,"[R] Need recommendation for demosaicing halftone on CPU, as well as versatile film grain detection.","Looking for existing ML programs (preferably python-based) that are very good at demosaicing halftone printing patterns and retaining/bringing back lost details. As per my last post, I've only been finding ones that require CUDA.

Would also like recommendations for libraries that may be good at tackling this problem if I need to attempt to build something.

I do have some basic scripts that are straight image filters, but would like to find (or build) one that is more robust, for working with large batches and large sizes, can work in float32 RGB and with RAW formats like CR2 and DNG so the metadata can be retained, or float TIFF and EXR for when metadata is not needed.

As for **film grain**, I've noticed that most ML programs seem to handle denoising with a blanket number/percentage. Does anything exist for setting a target *grain size* or channel independent grain attributes? I find that with very old images you tend to get larger grain size (think 1800s), and the software needs to be pushed too far to remove it, causing the image to become too blurry in the process.

Thanks",MachineLearning
rja4ma,1639842499.0,"[Project] Introducing fastshap, for quick (kernel) model explanations.","The purpose of fastshap is to make the calculation of shap values as lightweight and fast as possible. This is accomplished  by two batching  routines, which keeps the process inside vectorized operations as often as it can. There is also some intelligent numpy slicing involved. Info on how this works, and how you can determine optimal batch sizes, is on the github:

[https://github.com/AnotherSamWilson/fastshap](https://github.com/AnotherSamWilson/fastshap)

On   the Boston dataset, using all 506 rows as a background set, this runs   in about 26 seconds. The original shap package (KernelExplainer) takes   about 11 minutes. This difference grows more pronounced with larger   datasets.

A few notes:

1. This  package was really designed to be used on tabular data, as of now  features can't be grouped, i.e. no superpixels. This feature will be  added in the near future, though.
2. Only a kernel explainer is implemented, which calculates shap values for any arbitrary model. Model specific methods (i.e. TreeExplainer) are still much faster.
3. Can automatically handle pandas DataFrames or numpy arrays
4. Background dataset stratifying methods are available.
5. Can only calculate shap values for 1 dimensional outputs as of now (n dimensions coming soon)
6. The linear model, from which we get the shap values from the coefficients, can be any of the models from the sklearn.linear\_model module.

This is available now on pypi, coming soon to conda-forge.",MachineLearning
rj7ivc,1639834169.0,[D] Different results for the model every time I train the model with same parameters.,"I have the following model. I am facing an issue with the results as every time I run the model I get different results. The train and test data remains the same in every run.

I can give you more details if you have any questions for me. Let me know.

Note: Data 1 and Data 2 do have a correlation between them and that's the reason for using them both.

&#x200B;

https://preview.redd.it/kkqk01l3za681.png?width=2032&format=png&auto=webp&s=b1ecebcaafd8043fd687881e1849e4309ec09be3",MachineLearning
rj628g,1639828742.0,[D]Land Pattern Annotation,"Hi,

I would like to train a semantic segmentation model to detect chunks of soil and shadows. The annotation itself, without any helper tools, is time-consuming. Below is the sample image:

https://preview.redd.it/mui5nfssia681.jpg?width=2758&format=pjpg&auto=webp&s=6b222a837f7163c9b8df6075fd42903a793f818c

So far, for annotation, I have used mostly CVAT and LabelMe. For such patterns, however, it would take too much time to annotate a single image.

I would be thankful if anyone recommended any helpful tool or approach to annotate the images like the one above.",MachineLearning
riy24d,1639797341.0,[R] What Does BERT's Self-Attention Actually Look At?,"The self-attention mechanism is central to the success of the Transformer, and with it all of the large language models (GPT2, GPT3, BERT, T5) that have spawned from it. [This Towards Data Science article](https://towardsdatascience.com/what-does-transformer-self-attention-actually-look-at-5318df114ac0) summarizes the work of a variety of researchers digging into what these self-attention heads are actually looking at. The surprising answer is that a large fraction of them are just staring at meaningless separator tokens, and only a small handful are performing anything analogous to traditional NLP tasks like coreference resolution.",MachineLearning
rivass,1639788500.0,"[R] Looking for a deep learning python program for low-light image recovery, w/ CPU support","I'm on macOS and looking for scripts that will run on CPU, since no CUDA available to me. All the ones I'm finding are CUDA only. I'd prefer to use something existing rather than having to try to build it from scratch atm, as I don't have the hardware for training, have never created ML algorithms before and have only modified existing programs.

I personally need it for RAW files (CR2, DNG, etc), as part of a larger project. Specifically this is to recover/rebuild detail and remove digital noise artifacts and streaking when images are underexposed in DSLRs.

If nothing like this exists that supports CPU, perhaps you may know of some libraries that could help in building something like this—aside from the usual opencv, numpy, etc—or perhaps some resources on modifying CUDA scripts to function using CPU (such as for pytorch).

I'd appreciate a point in the right direction.

Thanks",MachineLearning
riqxrq,1639775288.0,[D] Do large language models understand us?,"Blog post by [Blaise Aguera y Arcas](https://medium.com/@blaisea/do-large-language-models-understand-us-6f881d6d8e75).

**Summary**

Large language models (LLMs) represent a major advance in artificial intelligence (AI), and in particular toward the goal of human-like artificial general intelligence (AGI). It’s sometimes claimed, though, that machine learning is “just statistics”, hence that progress in AI is illusory with regard to this grander ambition. Here I take the contrary view that LLMs have a great deal to teach us about the nature of language, understanding, intelligence, sociality, and personhood. Specifically: statistics do amount to understanding, in any falsifiable sense. Furthermore, much of what we consider intelligence is inherently dialogic, hence social; it requires a theory of mind. Since the interior state of another being can only be understood through interaction, no objective answer is possible to the question of when an “it” becomes a “who” — but for many people, neural nets running on computers are likely to cross this threshold in the very near future.

https://medium.com/@blaisea/do-large-language-models-understand-us-6f881d6d8e75",MachineLearning
ripn6f,1639771531.0,[D] The Human-In-The-Loop to Drive ML Model Improvements," While more data and compute is one way to improve ML models - direct user feedback may be the best way to rapidly improve an ML model. Why is Human-in-the-loop necessary for MT? The truth is that there is no existing training data set that is so perfect, complete, and comprehensive as to produce an algorithm that consistently produces perfect translations 

Link: [https://kv-emptypages.blogspot.com/2021/11/the-human-in-loop-driving-mt-progress.html](https://kv-emptypages.blogspot.com/2021/11/the-human-in-loop-driving-mt-progress.html)

An Inconvenient Truth About AI: ""Just about every successful deployment of AI has either one of two expedients: It has a person somewhere in the loop, or the cost of failure, should the system blunder, is very low."" **#deeplearning** **#linguistics** **#data** **#neuralnetworks**",MachineLearning
rin63t,1639764332.0,[D] Best tool for drawing U-Net style diagrams?,"I'm struggling to find a good tool for drawing U-Net style architectures: https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png

The likes of PlotNeuralNet and NN-SVG seem like excellent tools, but they don't seem to lend themselves to drawing U-Nets (Correct me if I'm wrong though since I'm otherwise familiar with both!).

I don't need anything particularly fancy, just want to do something similar to the standard U-Net diagram I linked above.",MachineLearning
rimqij,1639763120.0,[D] How To Embed PowerBI Report In JupyterNotebook,"PowerBI reports could add new dimension in the visualization capabilities in jupyter notebook,  I have tried to demonstrate in below post about how one can access their custom powerBI report inside jupyter notebook

https://mymlspace.com/integrating-power-bi-with-jupyter-notebook/",MachineLearning
riji40,1639753907.0,[D] GPU access without limit increases,"Hi folks,

Trying to get access to GPUs for some urgent training jobs. But looks like most cloud providers require 3+ business days for turnarounds. Are there any alternatives someone can suggest so I could get started with a training job right away?",MachineLearning
rij20m,1639752648.0,[D] Internship after ML phd?,"Hello everyone,

I recently submitted my phd thesis focused on optimization and RL at a university in Europe. Since my advisor was against internships and my funding didn't allow for one, I graduated without any internship experience and it is difficult to land a full time job. I applied for many full time roles but I got rejections almost all the time.

In my case, does it make sense to apply for internships at big companies? I see that FAANG companies are hiring a lot of interns nowadays. Do you have any suggestions?

Thanks a lot for your help!",MachineLearning
rie4k9,1639735037.0,[P] Machine Learning Research in Quantitative Finance (Automation with Notion API),"I wanted an easy way to get a pulse on hundreds of Quant research insights produced every day to share with my students at NYU. The dashboard helps them follow ArXiv, SRRN, RePec, and published financial journal papers, including important Twitter and LinkedIn posts, GitHub repos, and hundreds of blogs. This can surely be done for other sub-disciplines in machine learning?

[https://www.ml-quant.com/](https://www.ml-quant.com/)

After attempting multiple solutions over the past few months, I came up with a tech stack that uses the Notion API, GCP Functions, Cloud Scheduler, Python scrapers, and RSS feeds.

[ML-Quant Website](https://preview.redd.it/eno9dqr1t2681.png?width=1890&format=png&auto=webp&s=ec1f7fb439babb2fb5fdf2afe00f7bd0b9a2de5b)

Let me know if you have any questions or feedback, I would be happy to share more and answer targeted questions.",MachineLearning
rie49f,1639735002.0,[P] Scientific Literature Review Generation v0.2," Hello everyone,

I've developed recently an algorithm to automatically generate a literature review : [https://www.naimai.fr](https://www.naimai.fr/)

Hopefully that could be useful for the PhDs (and the non PhDs) !

More details on the algorithm [here](https://yaassinekaddi.medium.com/scientific-literature-review-generation-386f36b05eae).

I'll be thankful if you have any remarks about it :)

Cheers,",MachineLearning
rie3i9,1639734906.0,[D] How to collect speech datasets for voice-enabled technology?,"Some of the good examples of speech data collection technology, Alexa and Siri",MachineLearning
ric7vd,1639726810.0,[D] Why ViT does not beat CNN in the field of deep generative model?, Recently ViT beats the CNN in many field proving that ViT is superior backbone network than CNN. There are some papers who tried to use ViT as a discriminator architecture in GAN but their results simply do not look so good. Why ViT does not beat CNN in the field of deep generative model?,MachineLearning
rib9m5,1639723279.0,"[D] Is there any work on embeddings that ""understand"" different measurements ?","I would like to know if it is possible to create embeddings that understand different measures. For example, with such embeddings, I would have text A more similar to B than C in terms of cosine similarity.- Text A: ""13 inch screen""- Text B: ""30.41 x 21.24 cm ""- Text C: "" 3456×2234 pixels ""Create an embedding that understand the order of magnitude and measurements.",MachineLearning
ri8f7a,1639713454.0,[D] Has there been any work on detecting & predicting smells?,"Seeing that there are electronics which can detect aspects of air, like humidity, temperature, VoCs, other gases, is there any ML research into detecting smells? And research into what sensors would be needed to make decent predictions? I'm not thinking about detecting specific chemicals, but more along the lines of how the nose works. A lot of ML work is about image detection and language modeling, how about other senses?",MachineLearning
ri7m95,1639710864.0,[Discussion] How important is graduate degree?,"I always suspect that graduate degree is needed in Machine Learning in industry. So I basically scraped 500+ job posts with MLE, and basically filter for Education level key words. And here are the results, I just want to share with everyone with these statistics.

So of all 500 job posts that're MLE, out of the ones that **did mention education level** (not all job posts mention education level),

71% mention BS,

58% MS,

44% PHD.

I guess my takeaway is that compared to other specialties ML does tend favor graduate degree a lot more (compare to SWE, DevOps, etc.), but it's not a necessity.

Do u guys agree?

&#x200B;

[source](https://pathfinder.fyi/career/MachineLearningEngineer/overview)

https://preview.redd.it/xg9734lms0681.png?width=932&format=png&auto=webp&s=694991fe371ace1c456b7abeb20ff87147e23084",MachineLearning
ri76tj,1639709471.0,[D] Geo DeepFakes are not far away,Here is a webapp demonstrating the capabilities of Geo Deepfakes: [https://mayachitra-thislocationdoesnotexist.azurewebsites.net/](https://mayachitra-thislocationdoesnotexist.azurewebsites.net/),MachineLearning
ri4pqb,1639701840.0,[R] Understanding AlphaZero Neural Network’s SuperHuman Chess Ability (Summary of the Paper 'Acquisition of Chess Knowledge in AlphaZero'),"As a common and (sometimes) proven belief, deep learning systems seem to learn uninterpretable representations and are far from human understanding. Recently, some studies have highlighted the fact that this may not always be applicable, and some networks may be able to learn human-readable representations. Unfortunately, this ability could merely come from the fact that these networks are exposed to human-generated data. So, to demonstrate their ability to learn like humans (and not that they are simply memorizing human-created labels), it is necessary to test them without any label. 

Following this idea, [the DeepMind and Google Brain teams, together with the 14th world chess champion Vladimir Kramnik, studied](https://arxiv.org/pdf/2111.09259.pdf) [their creature AlphaZero from this point of view](https://arxiv.org/pdf/2111.09259.pdf). AlphaZero is the descendant of AlphaGo, the super neural network that beat the world champion Lee Sedol in a best-of-five GO match, a turning point in the history of deep learning, as can also be seen in the wonderful Netflix documentary *AlphaGo*. 

Unlike AlphaGo, AlphaZero is trained through self-play (i.e., it learns to play competing against itself) and masters not only GO but also chess and shogi. This trait makes AlphaZero the perfect case study to explore this idea. Moreover, given the fact that it performs at a superhuman level, understanding its functionality is also particularly useful for highlighting unknown patterns which have never been discovered by chess theorists.

Full Paper Summary by Leonardo Tanzi: [https://www.marktechpost.com/2021/12/16/understanding-alphazero-neural-networks-superhuman-chess-ability/](https://www.marktechpost.com/2021/12/16/understanding-alphazero-neural-networks-superhuman-chess-ability/)

Paper: [https://arxiv.org/pdf/2111.09259.pdf](https://arxiv.org/pdf/2111.09259.pdf)

https://preview.redd.it/096omb8m10681.png?width=808&format=png&auto=webp&s=c375a2bfffc4949399e17c7ebbe2e2c334a2a44d",MachineLearning
ri2cpj,1639694770.0,[D] From thousands to billions: An overview of methods for scaling Graph Neural Networks.,"Graph Neural Networks (GNNs) have become very popular in recent years. Early approaches easily handled small graphs with a few thousand nodes but scaled poorly to large graphs with millions of nodes. Several approaches have been proposed for scaling GNNs to large graphs. We recently published an overview of a few core approaches for GNN scalability.

Read on: [Scalable graph representation learning with Graph Neural Networks](https://www.thejournal.club/blog/3/scalable-graph-convolutional-neural-networks)

We welcome constructive feedback. Cheers!",MachineLearning
ri20hq,1639693794.0,"[D] Why Do ""Good"" Research Ideas Fail?","Hi ML peeps, I did a write-up on the major lessons I learned doing ML research at Google and PathAI, including my time working with Samy Bengio and Ian Goodfellow.

The main questions I'm curious to answer are (1) why do so many good-seeming ideas fail? and (2) what should we do about it? So I'm curious to know y'alls answers on those questions too.

[https://nathanieltravis.com/2021/12/16/why-good-research-ideas-fail/](https://nathanieltravis.com/2021/12/16/why-good-research-ideas-fail/)",MachineLearning
rhwofs,1639678853.0,[D] Is it right to use your PhD student as an expert annotator on a dataset creation paper and not give them authorship?,"Hi guys, I'm sure you've seen the recent discussions about the role of datawork in ML research \[for example, [here](https://arxiv.org/abs/2007.07399),  [here](https://arxiv.org/abs/1803.09010), [or here](https://aclanthology.org/Q18-1041/)\]. Some of that is definitely complicated, and there's lots of room for disagreement. But today, I came across an example of a well known researcher choosing not to give a PhD student authorship and it made me wonder... what do you guys think?

Here's the post in question: [https://twitter.com/sleepinyourhat/status/1471225421794529281](https://twitter.com/sleepinyourhat/status/1471225421794529281)

The names of particular expert annotators (the ones who wrote the dataset) are acknowledged by name. In principle that seems good (I think?). What is surprising though, is that some of these expert annotators seem to actually be PhD students in the last author's department (linguistics)? The data collection itself relied on their domain expertise. Presumably, this is what qualifies them for the task. It strikes me as questionable not to include a student in your department as an author, especially when they contributed meaningfully to the dataset... are we going to have a split system now, where domain expertise \*as an active graduate-level researcher\* isn't sufficient for authorship on the project? Is writing the dataset that much less important than running a model on it?

tl;dr: Is it fair or fucked not to make a PhD student an author on a project they worked on just because they worked on the data side of a dataset paper?",MachineLearning
rhum5j,1639673032.0,[Research] Bayesian Optimisation Live Series Lecture,"In a small survey I conducted today, I realized a lot of people are not very engaged with the Bayesian optimization literature but would like to learn about it.

&#x200B;

Would you attend a Lecture series on this?

[View Poll](https://www.reddit.com/poll/rhum5j)",MachineLearning
rhui36,1639672697.0,[P] Made Some Pytorch Modules For Agent Systems,"I am starting a little Evolutionary Algorithms project (I know it's a bit frowned upon) and noticed that if you are working with Deep Neural Networks, you need to instantiate them separately and iterate over them to do each network's forward pass, which is very slow even in GPU.

For that reason I made this little package of pytorch modules. The main class, WideLinear, behaves as a family of linear layers, each different, but each running fully in parallel so you do a single forward pass through all of them at the same time. Even works in GPU.

This has some application outside of evolutionary algorithms, but mostly still in agent based systems. Gradients work as expected.

I have a brief documentation in my github, [https://github.com/joaoperfig/WideLinears](https://github.com/joaoperfig/WideLinears), and it is available through pip.",MachineLearning
rhu6fw,1639671767.0,"[D] As a researcher, should I regularly review the basic math, ML, etc?","I'm using Anki which is spaced repetition app, and there are some flashcards on ML, linear algebra, prob&stats I should review. One advantage the anki brings to me is that now I can choose what knowledge I remember. Since reviewing basic cards does not bring immediate advantage, I wonder if it is better to discard those cards and re-learn the knowledge when I actually need it, or to invest times expecting it will save me a lot of time someday. Do you review your basic subjects regularly, or simply re-learn when you need it?",MachineLearning
rhtdrw,1639669456.0,"[P] Don't know why, but you can make ruDalle generate similar images to the input one by optimizing its text embedding","Input image: https://static8.depositphotos.com/1370441/848/i/600/depositphotos_8486144-stock-photo-beach-and-tropical-sea.jpg

Input text: 'elon musk'

Result: [image](https://scontent-frt3-2.xx.fbcdn.net/v/t39.30808-6/266850659_10215735351777199_4719489214825321057_n.jpg?_nc_cat=101&_nc_rgb565=1&ccb=1-5&_nc_sid=730e14&_nc_ohc=F-dcUV66HngAX-_WQwX&_nc_ht=scontent-frt3-2.xx&oh=00_AT-_oZpQ7XN81wUUtEnn6moUUwvneOtsQ9XHKySKuz3WhA&oe=61C00A55)

Colab that runs out of memory: https://colab.research.google.com/drive/1ancv6fQMrzaz67Ikvfv3wnjlwpWsoebO?usp=sharing

Disclaimer: I'm not a scientist, nor a developer. In fact, I'm an industrial designer. And I wanted to see if I can make a tool that would generate variations on the input pictures (in my case, the design of objects, like headphones)

I first tried [CLIP-guided image generation](https://colab.research.google.com/drive/1ZAus_gn2RhTZWzOWUpPERNC0Q8OhZRTZ#scrollTo=gmK0k5zQeT5u) to make changes to the images, but it is lacking on creating particular objects, but seems to retain object features quite well. [This](https://scontent-frt3-1.xx.fbcdn.net/v/t39.30808-6/261698599_10215690241209463_3407671733492942937_n.jpg?_nc_cat=102&_nc_rgb565=1&ccb=1-5&_nc_sid=730e14&_nc_ohc=_DutliT-VWoAX-0ICqZ&_nc_ht=scontent-frt3-1.xx&oh=00_AT9eoNrIQ__c9mEYACVZwemtmRTATQJwtRunakEVFd1q6Q&oe=61C002E8) is 'forest' from the input image above

My method is to optimize the text embedding of the transformer, in order to make the output closer to the input image. Same thing as fine-tuning, but optimizing text embeddings, instead of model weights. I had to modify model's forward pass to make it retain the gradient. Sorry for messy code

My aim, however, is to take it one step further and achieve text-based image modification, same as [in the original example by OpenAI](https://openai.com/blog/dall-e/#im2im-animal), but without using image prompts

Also, does anyone know if it's possible to reverse text embedding in these models? (I guess not) Would be interesting to see what the final image accounts to",MachineLearning
rhsefd,1639666614.0,"[D] Anyone else suspicious/concerned about the spread of ""Data Science"" degrees?","Along with MS programs in ""Artificial Intelligence,"" rather than an MS in CS/Stats with a focus on AI. I could understand if, say, CMU wanted to have an out-and-out MS in AI, which would probably be pretty good prep for a PhD in the subject. But, for example, Yeshiva University has an MS in AI, despite as far as I can tell having literally only two full members of the faculty working in the area.

So I'm somewhat concerned about the rise of all these degree programs in ML/AI/DS specifically, because they seem really specialized in a way that undergraduate/professional degrees probably shouldn't be, and aren't always offered by departments that I trust to deliver appropriate instruction. To me, it would be way more appropriate for students to do a degree in Math/Statistics/CS, and then pick coursework and do research in a narrower specialty, rather than potentially be left holding the bag once the hype dies down.",MachineLearning
rhppgq,1639657547.0,[Research] New library for Bayesian Optimisation and Hyper-parameter tuning,"\[Research\] I am glad that we open-sourced a new library ([https://github.com/huawei-noah/HEBO](https://github.com/huawei-noah/HEBO)) for Bayesian Optimisation both in low and high-d domains. The library includes:

* HEBO--The algorithm that won the NeurIPS BBO challenge can be used for hyper-parameter tuning ([https://arxiv.org/abs/2012.03826](https://arxiv.org/abs/2012.03826))
* T-LBO--An algorithm that combines deep metric learning with latent space Bayesian optimization to enable high-dimensional opt \[can arrive at optimal molecules reducing 97% data demands\] ([https://arxiv.org/abs/2106.03609](https://arxiv.org/abs/2106.03609))
* CompBO--An algorithm based on our JMLR paper ([https://arxiv.org/abs/2012.08240](https://arxiv.org/abs/2012.08240)) to efficiently optimize acquisition functions.

We will continue to grow this library and your contributions and comments are more than welcome. Please have a look, share, and star the repo if you find it useful.

[View Poll](https://www.reddit.com/poll/rhppgq)",MachineLearning
rhpjg6,1639656910.0,[D] Would anyone be interested in an automated object detection data labeling tool?,"Hi all, I'm a recently graduated (jobless) computer engineer

I'm currently working on a  tool which allows for automated photo capture, bounding box labeling, data augmentation and training of object detection models such as yolov5.

You can basically create a fully labeled and diverse dataset (for 1 object category)  in less than 10 seconds of manual work, as opposed to the tens or even hundreds of hours usually required to collect, label and augment the dataset using conventional methods . The main advantage of my tool is the fully automated labeling of bounding boxes, which otherwise can take a very long time to manually annotate, as well as extremely potent data augmentation. The training of the model is also automated (not really anything new) and will take the usual 8-30 hours depending on the GPU hardware.

The main limitation is that the object must be smaller than approximately 50x50x50cm (to fit into the automated photo capture/labeling device ).  You cannot for example label a sofa(too big) or a single red blood cell (too small). There is also a physical hardware cost (around $1000USD)  for the automated photo capture/labeling device.

I would like to know if the CV/ML community would find this tool useful or worth the $1000USD the capture device would likely cost.  I currently have a working prototype, but I want to know if anything similar exists and whether it is worth it to keep developing and maybe developing it into a product in the future?

Thanks for any of your ideas, suggestions or criticisms.",MachineLearning
rhohk2,1639652608.0,[D] What is the best way to perfectly overfit your dataset with minimal variables,"Hello guys,

Are there any approaches to reach 100% training accuracy using minimal variables?

What I can think of is just binary searching on # of variables of a NN. Although the training of an NN doesn't guarantee a perfectly efficient usage of its parameters..

Similar thing can be done with GBTs I guess.",MachineLearning
rhod30,1639652124.0,[Project] Deepmind's Perceiver IO available through Hugging Face,"Deepmind's Perceiver IO (the first Transformer-based neural network that works on all kinds of modalities (text, images, audio, video, point clouds,...) was added to HF Transformers.


[Blog post](https://huggingface.co/blog/perceiver)

[Example Notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Perceiver)

&#x200B;

I wonder if someone has real life experience working with this one, and what are the  consideration of choosing it over other , ""dedicated"" models (unless the use cases involves a mix of modalities).",MachineLearning
rhnfm7,1639647996.0,[D] Can Neural Networks tell whether they have already seen a given example?,"We want our neural network to perform the following task: given a training dataset `D = [x_1, x_2, x_3 .. x_n]`, the network should output YES when provided with one example in D or a very similar one, and NO when provided with one example not similar to any in D. Examples not in D cannot be provided at training time, therefore the network would need to learn to memorize what it has seen and answer NO to anything not similar to it.

To me, this looks like an out-of-distribution detection task but I am not very familiar with the literature, so any reference or idea would be really appreciated",MachineLearning
rhkcl9,1639635187.0,[D] Does it harm your PhD application if you've worked before applying?,"I hear this fairly frequently around me but am not really succeeding at finding evidence. The rationale that I usually get is that if you work then you're ""away from research"" or ""lose the student mindset.""

I'm in a position where I was supposed to apply for PhD programs this year but due to personal reasons am looking for work instead. I'm wondering if anyone here also thinks about the statement in the title.",MachineLearning
rhiqoc,1639629774.0,[Project] Ru-DALLE diffusion - better images from Ru-DALLE with diffusion decoder,"link: https://github.com/Jack000/ru-dalle

I previously posted this project: https://www.reddit.com/r/MachineLearning/comments/r8v4fq/project_dall3_generate_better_images_with_fewer/ which uses a DDPM instead of VQGAN for the decoder layer.

This new model applies the same idea to Ru-DALLE.

The DDPM model can produce much more detailed images compared to the default VQGAN + RealESRGAN setup, decoding from the exact same image embeddings. Because the DDPM model is trained on 256px images with 16x16 token dimensions, it will output 512px images when given 32x32 tokens from Ru-DALLE.

The model architecture is the same as before - I took the pretrained DDPM models released by OpenAI and modified the middle block to take image embeddings as input. Fine-tuning the pre-existing OpenAI model allows me to train it in a reasonable amount of time on a prosumer ML rig, but it constrains certain architectural decisions. Specifically the middle block must take embeddings with 8x8 dims (we do one downscale pass on the 16x16 embeddings) It should be possible to get better performance with fewer parameters with a custom DDPM that's matched to the image embedding dimensions.",MachineLearning
rhh5zr,1639624852.0,[Project] Determining what a classifier thinks a rabbit looks like,"[How it works](https://youtu.be/wgl6ofk66_E).

Trying to see what classifiers thought different classes looked like, I ended up generating pictures for every class in ImageNet. Those pictures did not look like what I was expecting, though I should probably know better at this point, lol. I used an ImageNet pretrained VGG-16 model for the classifier.

Fun results of not rabbits (evolving the generated picture from a gray image to something the classifier is a 100% sure is thing):

&#x200B;

https://i.redd.it/4hnyowkgnt581.gif

&#x200B;

https://i.redd.it/bc71lqc8ot581.gif

&#x200B;

[I'm surprised by the lack of the color orange in the generated image.](https://i.redd.it/e9wircbtot581.gif)",MachineLearning
rhdr2b,1639614351.0,"[P] ruDALL-E text-to-image 12 billion parameter ""commercial version"" (XXL 12B) is available","[Press release](https://www.sberbank.com/news-and-media/press-releases/article?newsID=a26a208d-6c72-4f8a-a3b7-aefe1112cbae&blockID=7&regionID=77&lang=en&type=NEWS).

[Project page (Russian)](https://sbercloud.ru/ru/datahub/rugpt3family/ru-dalle-12b). [English translation](https://sbercloud-ru.translate.goog/ru/datahub/rugpt3family/ru-dalle-12b?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=nui).

[Post about the 1.3 billion parameter free version of RuDALL-E](https://www.reddit.com/r/bigsleep/comments/ql9n81/new_texttoimage_ai_models_rudalle_example_from/).

&#x200B;

Examples that I created using the 1.3 billion parameter free version (upscaled from 256x256 to 1024x1024):

""woman with rainbow hair"":

https://preview.redd.it/6ww2jpkfts581.jpg?width=1024&format=pjpg&auto=webp&s=ab5c459ee763c7b6359a232d76fc96215e5ae428

""sketch of a chipmunk"":

https://preview.redd.it/zxteeyxqus581.jpg?width=1024&format=pjpg&auto=webp&s=6c8f86372c53b0399cc46fcbdb9de46e2c90c9f5

""semi-abstract art"":

https://preview.redd.it/om44sss5vs581.jpg?width=1024&format=pjpg&auto=webp&s=d27f05994b07c5098399357add52ee1a1baebe38",MachineLearning
rhdofe,1639614137.0,[D] Audio/speech “harmonization” task?,"Is there an equivalent of image harmonization/compositing task for audio/speech (not music) data?

Say we want to combine clean speech data and natural scene background so that the speaker naturally sounds like they are in that background scene.

Generally, what I’m asking is how do we make a clean speech data more “natural” given a natural sound data as a reference?

I’m not familiar with sound design or SFX either, so please let me know if this particular task is achievable with non-neural approach.",MachineLearning
rhdisq,1639613671.0,"[D] Any recommendation of books for ""Information theory and Statistics""?","I want to know more about f-divergence, Renyi, Shannon, KL, Wasserstein divergences. I am not a Math student, is there any book (or any kind of resources) that you recommend for the non-Math students to get a more general and high-level understanding of these metrics/divergences?",MachineLearning
rhd64w,1639612663.0,[D] jupyterlab alternative with enhanced academic writing capabilities?,"Hi folks, I am looking for self-hosted or online options which combine jupyter notebooks and capabilities for academic writing (bibtex, export to latex etc.) I found curvenote but wondering if there are other options out there.

Thanks,

RH",MachineLearning
rh9t9x,1639603577.0,[R] Training Machine Learning Models More Efficiently with Dataset Distillation,"Today's Google.ai blogpost focuses on dataset distillation arising from the infinite-width limit theory of neural networks, in which small learned datasets can be used to train kernels and neural networks to high test accuracy. One highlight result is that by using only 10 learned images and labels, 64.7% test accuracy can be achieved on CIFAR-10.

Blogpost:

[https://ai.googleblog.com/2021/12/training-machine-learning-models-more.html](https://ai.googleblog.com/2021/12/training-machine-learning-models-more.html)

Papers discussed:

Dataset Distillation with Infinitely Wide Convolutional Networks (NeurIPS 2021)

[https://arxiv.org/abs/2107.13034](https://arxiv.org/abs/2107.13034)

Dataset Meta-Learning from Kernel Ridge-Regression (ICLR 2021)

[https://arxiv.org/abs/2011.00050](https://arxiv.org/abs/2011.00050)",MachineLearning
rh4l9j,1639589887.0,[D] Area under statistical power curve?," In machine learning, there is the field of binary classification. A common metric for measuring the performance of such models is the AUROC (area under receiver operating characteristics curve). In statistical hypothesis testing, we have the power curve which turns out to be the same as the ROC curve (both plot true and false positive rates). While the area under the ROC curve has a very nice interpretation, I haven't heard anyone talk about the area under the power curve. It also has an interpretation: the probability a test statistic from the null will be higher than one from the alternate. See here for a proof: [Interpreting AUROC in Hypothesis Testing | by Rohit Pandey | Dec, 2021 | Medium](https://medium.com/@rohitpandey576/interpreting-auroc-in-hypothesis-testing-a45f6f757a62)",MachineLearning
rh1viq,1639582802.0,[P] Layer-wise Relevance Propagation in PyTorch,"Hi there,

I have set up a basic implementation for Layer-wise Relevance Propagation (LRP) in PyTorch that comes with an additional relevance filter method for much crisper heatmaps.

I was not so happy with the LRP implementations that were available for PyTorch. So, here you have it. The implementation is easy to understand and should be extensible without much effort.

Here is a small preview:

https://preview.redd.it/8he3g0tz6q581.png?width=1316&format=png&auto=webp&s=f9d5af1b876e6a1b8f6db5973459b068f1632cb9

The repository can be found here: [https://github.com/KaiFabi/PyTorchRelevancePropagation](https://github.com/KaiFabi/PyTorchRelevancePropagation)

Thanks.",MachineLearning
rh1jwm,1639581956.0,[P] AI-Generated Pokemon made using a finetuned ruDALL-E model,"https://www.reddit.com/r/pokemon/comments/rgmyxp/i_trained_an_ai_on_all_the_official_pokemon/

This was a test on my end to see what would happen if I tried to finetune ruDALL-E on a specific domain with a relatively low sample size (~900 images). The results are much better than expected!

The ideal goal was to see if the finetuned model could preserve ruDALL-E's ability to write prompts; unfortunately, it didn't. It's a tough balance.",MachineLearning
rh0rb2,1639579729.0,[D] Sota of adversarial examples,I m having a hard time finding whats currently the state of the art of adversarial attacks and defenses. From what i ve gathered is that its still hard to defend against cleverly engineered ones. How does it impact ur research or ur decision to use cnns or so? Do other methods such as svm display the same weaknesses ?,MachineLearning
rgzppr,1639576759.0,[D] Is Arxiv worth it for the academic career?,"I don't know if this topic has already been discussed here. If that is the case, feel free to delete my thread and point me to the right one.


I've just finished my PhD and now I am a PostDoc researcher. I want to pursue the academic career, and my department uses Scopus metrics in its public competitions.

I used to upload all my work on Arxiv as soon as it was ready, and i always received useful feedback from the community. My papers were always automatically scraped by PapersWithCode and other websites, and that gave me nice exposure. I even had a paper with 12 citations in the first two months (which is a lot for me) and received a lot of emails from other researchers asking for clarifications or collaborations. In general, posting preprints to Arxiv has always been a good experience.

The problem is that by doing that, I ""lost"" a lot of citations because everybody cited the Arxiv version of the articles and not the journal one. All these citations do not appear on Scopus and do not count from a public competition point of view.

Some colleges suggested me to use ResearchGate instead of Arxiv because with that platform you have more control over the citations. What do you think about that? Have you ever faced the same problem? Do you have any platform to suggest that gives exposure similar to the one that Arxiv gives? Should I just stop publishing preprints and just wait for the journal/conference to publish the articles?",MachineLearning
rgykys,1639573241.0,[D] I just found out that my 1 years' worth of research has already been published.,"I'm a PhD student in the middle of my studies. A year ago I had an idea  about designing a neural network for medical image segmentation using  shape priors. I have done a quick literature review at that time  (although I admit, it might not have been thorough enough) and I found  that no one really tried to use those shape priors before, especially  for the task that i wanted to use them on (these descriptors would fit  the specific task especially well). I worked hard on the implementation,  designing the network architecture, writing the article and  understanding all the necessary mathematical proofs/theorems related to  this task. I just submitted the article a few weeks ago (no word from it yet), and today, I  found an article on arxiv (no citations) that has been published this  spring and basically uses the same idea for the same task as I did. The  network architecture is different than mine and the performance  evaluation is different, but the main selling point of my article, the  usage of these shape priors has already been published. I am a bit  devastated at this point because this would have been my first 1st  author paper and I really put a lot of effort and thought into this,  only to discover that my idea has already been discovered before.  Obviously I need to do a much more thorough literature review next time  so that this doesn't happen again, but besides that, I don't know what  else I could do to mitigate the damage that has been done to my  motivation. I am even considering quitting PhD at this moment because I  feel like I wasted a lot of time because of my stupidity. Has anything  similar happened to you before? Do you have any advice? How could you  cope with similar issues in your career?",MachineLearning
rgxiav,1639569634.0,[D] State-of-the-art online Deep Reinforcement Learning algorithm for Continuous Action spaces,"Hi,

What's the current state-of-the-art online Deep Reinforcement Learning algorithm for Continuous action spaces?  I'm looking particularly for one that has an open source implementation that I can download from GitHub.

Also, does anyone know if the Muesli algorithm (Muesli: Combining Improvements in Policy Optimization by Hessel et al. 2021) [http://arxiv.org/abs/2104.06159](http://arxiv.org/abs/2104.06159) has an open source implementation yet?

Thank you.",MachineLearning
rgpqix,1639540179.0,"[D] Is this exaggerate?? ""to what extent is gpt-3 capable of reasoning"".","[To what extent is GPT-3 capable of reasoning?](https://www.lesswrong.com/posts/L5JSMZQvkBAx9MD5A/to-what-extent-is-gpt-3-capable-of-reasoning)

The reasoning ability of gpt3 shown in this article shocked me. But my own experiment using gpt-3 API is far less effective than the results in this article. this confuse me. is there any trick making gpt-3 answering like that?

My result is the same as the last comment: ""GPT-3 just produced one of the *dumbest* things I've ever read.""",MachineLearning
rgosh5,1639537348.0,[R] The Future of Artificial Intelligence is Self-Organizing and Self-Assembling,"Blog post by Sebastian Risi:

http://sebastianrisi.com/self_assembling_ai/

*Excerpt:*

This is the first post in a series I plan to write on the work from our group and others that combines ideas from deep learning with ideas from self-organization and collective systems. In this first post, we’ll look at some of the developed approaches and the domains they have been applied to, ranging from growing soft robots and Minecraft machines to self-assembling modular robots, and creating more resilient and adaptive reinforcement learning agents. The merger of these ideas could ultimately allow our AI systems to escape their current limitations such as being brittle, rigid, and not being able to deal with novel situations. However, the combination of these methods also poses new challenges and requires novel ways of training to work as efficiently as possible.

One of the most fascinating aspects of nature is that groups with millions or even trillions of elements can self-assemble into complex forms based only on local interactions and display, what is called, a collective type of intelligence. For example, ants can join to create bridges or rafts to navigate difficult terrain, termites can build nests several meters high without an externally imposed plan, and thousands of bees work together as an integrated whole to make accurate decisions on when to search for food or a new nest. Surprisingly, achieving these incredible abilities is a result of following relatively simple behavioral rules and through a process of self-organization, which Camazine et al. (2001) define as:

>    “As a process in which pattern at the global level of a system emerges solely from numerous interactions among the lower level components of the system. Moreover the rules specifying interactions among the system’s components are executed using only local information, without reference to the global pattern. In short pattern is an emergent property of the system rather than being imposed on the system by an external ordering influence.“

Self-organizing systems are made out of many components that are highly interconnected. The absence of any centralized control allows them to quickly adjust to new stimuli and changing environmental conditions. Additionally, because these collective intelligence systems are made of many simpler individuals, they have in-built redundancy with a high degree of resilience and robustness. Individuals in this collective system can fail, without the overall system breaking down.

Multicellular organisms learned to exploit self-organizational principles to self-assemble, starting from a single egg cell and only through the process of local cell interaction during embryonic development. Similar to the robustness of swarms of organisms, the self-organization of cell populations is remarkably robust to perturbations. In some animals, this goes as far as being able to regenerate complete body parts, such as a salamander’s tail. This type of self-repair is a common feature of self-organizing systems and interestingly does not involve any additional processes:

>    “The same self-organization process that built the initial pattern can operate to repair the pattern.” — Camazine et al. (2001)

(...)

Rest of the blog: http://sebastianrisi.com/self_assembling_ai/",MachineLearning
rgmgcn,1639530406.0,[P] <10LOC PyTorch wrapper of TokenLearner,"Hey everyone,

I recently built a PyTorch wrapper over **TokenLearner** and **TokenFuser** from the paper ""[TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?](https://arxiv.org/abs/2106.11297)"" by Google AI for better image and video understanding.

Check it out:
[https://github.com/rish-16/tokenlearner-pytorch](https://github.com/rish-16/tokenlearner-pytorch)

https://preview.redd.it/5vdmiup2wl581.png?width=640&format=png&auto=webp&s=1cb651dc1f635cc82e522834e04f75d7f47dc258

It's fully compatible with existing PyTorch modules and layers. You can plug and play with Vision Transformers as mentioned in the paper.

Do star and share if you find it useful!",MachineLearning
rglz0f,1639528991.0,[D] Reaction to a Beautiful Moment,"Do y'all ever have moments of pure glee when developing your networks?

I have an encoder-decoder type network that had a custom-designed intermediate function acting upon the output of the encoder. Thus far training per batch took north of 180 seconds (batch of 8, with number of samples, took a holistic training time north of 13 hours), and just now I got it reduced to < 1 second per batch for training.

Never felt happier :)",MachineLearning
rgk8db,1639524031.0,[D] What are challenges that Quantum Machine Learning can solve?,"I am looking to study quantum machine learning in a Ph.D. but I want to understand more about how it can solve problems. I understand the BPP and BQP problems and it’s ability to speed up computation, but how can it solve problems in common life? Usually when I need to understand something I go to job postings (in this case Quantum Machine Learning Engineer) and I see companies are using QML for Chemistry and Agriculture. Any help would be appreciated!",MachineLearning
rggtdu,1639514602.0,[N] Exciting New Effort to Develop Synthetic Data for Genomic Research!,[https://www.businesswire.com/news/home/20211214006240/en/Gretel-and-Illumina-Partner-to-Develop-Synthetic-Data-for-Genomic-Research](https://www.businesswire.com/news/home/20211214006240/en/Gretel-and-Illumina-Partner-to-Develop-Synthetic-Data-for-Genomic-Research),MachineLearning
rgg36d,1639512614.0,[R] Results of the NetHack Challenge at NeurIPS 2021,"Report: https://nethackchallenge.com/report.html

*Excerpt from the results section:*

The results of the showdown showed that — for the time being — symbolic bots (red) quite clearly have the upper hand in this difficult environment.

The top three spots in the Overall Best Agent all went to agents from the Symbolic Agent Track. Then the following three went to top of the Neural Agents track, with the winner of this track being the only highly hybrised model in the competition, alternating between symbolic and neural play depending on the proximity of monsters.

The margin of victory was significant, with the top symbolic agent beating the top neural agent by a factor of almost 3 in the median score. This was, in fact, increased when looking at the very best agents from each team, where frequently we might see almost an order of magnitude improvement in the median score between the best symbolic and neural agents.

While our best symbolic teams had moderate-to-expert NetHack domain understanding, we were surprised to find they often had extensive ML experience as well. In fact, both winning symbolic teams said they had intended to enter the neural track, but found their symbolic methods scaled much better.

In over half a million evaluation games, no agent managed to ascend.

**What does it all mean for AI?**

>    “Despite the game of NetHack being far from solved by these agents, seeing them descend over 20 levels deep into the dungeons of NetHack to achieve scores of over 100,000 points is very encouraging! Past versions of NetHack have a rich history of symbolic-agent-type bots, so the methods of machine learning may have some catching up to do in this specific realm of playing NetHack, but I am optimistic for the future of both methods after seeing the results of the challenge. It has been amazing to see a game that I cherish so dearly be used to make new advancements in machine learning and artificial intelligence, and I look forward to seeing how teams improve in next year’s challenge!”

>    – ToneHack

**NetHack is far from solved.**

First of all the results show that NetHack is still a tremendously hard challenge for AI agents, whether they are symbolic bots or deep reinforcement learning agents. The top median score of ~5,000 is several orders of magnitude short of typical human ascensions, and while some bots managed to achieve much higher scores in a few limited runs, most runs did not descend very deep into the dungeons of NetHack and instead stayed within early stages of the game.
Median score is good — but ascensions would be better.

The challenge highlighted the complex relationship between score and ascension. We found many entrants elected to “camp” in the early stages of the dungeon, grinding out a high score by killing monsters, instead of progressing into the dungeon. While this undoubtedly helped the weakest ‘roles’ in the game, like Tourist or Healer, it will not lead to winning the game. We learnt that score and ascensions are not always well-aligned, and our objective may be due a rethink in future challenges. That said, the focus on the median agent performance is still important, incentivising the creation of robust, general agents; but the focus on in-game score may be less so.

**Symbolic bots can strategize like a human; Can neural ones?**

NetHack benefits ‘strategic’ play — good play often involves executing a series of actions with a well-defined, expressible sub-objective, eg: “Find Sokoban” or “Apply a Unicorn Horn to Cure Poison”. Symbolic bots found it easy to define ‘strategy’-like subroutines and to decide when to deploy them based on rich, human-legible representations of the game state. This made it easy for participants developing symbolic bots to incorporate their domain knowledge. Neural agents struggle in this area, since hierarchical RL is an open problem in the research field, and it is hard for agents to discover ‘strategy’-type patterns of behaviour in environments featuring such a large action space and sparse reward.

**Symbolic bots can be know-it-alls; Neural agents find it harder.**

The game of NetHack is only partially observable. Only a single dungeon-level is ever visible, and the many objects and player states are often hidden unless inspected. Remembering a discovery, or incorporating extra knowledge is often key to making a good decision.

Symbolic bots excelled in keeping the full game state in memory, and incorporating external knowledge into their strategies. They found it easy to transfer domain knowledge to the decision-making process. In contrast, neural agents find it harder to maintain information in memory, especially if there is no reward directly associated with it.",MachineLearning
rge3b1,1639507188.0,[R] A new dataset for text classification and domain adaptation in social media,"A dataset of \~22,500 labeled documents across four different domains. You can find it here:

[https://github.com/p-karisani/illness-dataset](https://github.com/p-karisani/illness-dataset)",MachineLearning
rgdwjy,1639506677.0,[D] New Datasets to Democratize Speech Recognition Technology,"Hey, we at The Gradient just published [New Datasets to Democratize Speech Recognition Technology](https://thegradient.pub/new-datasets-to-democratize-speech-recognition-technology-2/), written by the folks at [MLCommons.org](https://MLCommons.org) . Since this has to do with datasets, seems like it would be of interest to folks in this sub.


Here's a TLDR / preview:

Over the last year, we at [MLCommons.org](https://mlcommons.org/en/)  set out to create public datasets to ease two pressing bottlenecks for  open source speech recognition resources. The first is prohibitive  licensing: Several free datasets do exist, but most of sufficient size  and quality to make models truly shine are barred from commercial use.  As a response, we created [The People’s Speech](https://mlcommons.org/speech), a massive English-language dataset of audio transcriptions of full sentences (see [Sample 1](https://thegradient.pub/new-datasets-to-democratize-speech-recognition-technology-2/#sample1)).  With over 30,000 hours of speech, this dataset is the largest and most  diverse freely available English speech recognition corpus today. The  second is that these datasets are heavily English-centric. We also  present the [Multilingual Spoken Words Corpus](https://mlcommons.org/words) (MSWC): a 50-language, 6000-hour dataset of individual words ([Sample 2](https://thegradient.pub/new-datasets-to-democratize-speech-recognition-technology-2/#sample2)  contains random examples of “hello” in multiple languages). Single-word  transcriptions are useful for training keyword spotting (KWS) models,  such as the ones used to activate Google Voice Assistant, Alexa, or  Siri. This dataset provides a significant leap in diversity of available  keyword spotting datasets. Together, these datasets greatly improve  upon the depth (TPS) and breadth (MSWC) of speech recognition resources  licensed for researchers and entrepreneurs to share and adapt.",MachineLearning
rgcrfj,1639503632.0,"[P] ZenML: An extensible, open-source framework to create reproducible machine learning pipelines","Hey everyone! I wanted to share with you an open-source tool we've been building over here in Munich called ZenML. ZenML is an extensible, open-source MLOps framework to create production-ready machine learning pipelines. It has a simple, flexible syntax, is cloud and tooling agnostic, and has interfaces/abstractions that are catered towards ML workflows. I think the Metaflow project has similar goals, but there are differences in approach, not just in constructing the pipelines but also the abstraction layers provided for the underlying infrastructure. Could you feedback us on the vision?

It goes something like this:


[Create pipelines and deploy them on any stack with the above code](https://preview.redd.it/2mb4dceknj581.png?width=735&format=png&auto=webp&s=21619406b2a4e0e07530823c0e81c07932bb1e2d)

There are of course other tools out there that look similar to the above, but because ZenML is focused on ML workflows, here are some key advantages:

* Pipelines are **data dependent**, rather than task dependent. This means that artifacts flowing through pipelines can be modeled in a specific way to enable features like caching and lineage.
* Artifacts flowing through pipeline steps can be **standardized** (adding a standard validation and deployment step for standard data and model artifacts).
* Steps can be standardized to enable the same effect. You can then enable **special features** for certain steps (e.g. distributed training for the trainer step).
* ZenML can **materialize** (read/write) common objects like Pandas dataframes and PyTorch modules automatically, regardless of the environment in which this pipeline is running (local or in the cloud). The data scientist can then use these objects natively as they always do.


This sets ZenML apart from tools like Airflow/Luigi/Prefect that are focused on data engineering use-cases and [hard to implement for ML-specific tasks](https://huyenchip.com/2021/09/13/data-science-infrastructure.html) by both developers and data scientists.

All this is from the point of view of an application, but what about infrastructure? Even with all the advantages above, these pipelines and integrations need to work across varied environments and infrastructure requirements for any use-case. This is where the notion of a MLOps stack comes in.


Happy for feedback and looking actively for contributors!


[Create complex MLOps stacks within your application](https://preview.redd.it/0p6qwz9snj581.png?width=979&format=png&auto=webp&s=837ad9761429d1e9fb9dba7a3fe07da811ff8c25)

We can then simply do:

    zenml stack up

This spins up the infrastructure for you on a target of your choosing. In addition, ZenML takes care of deploying your pipelines to the relevant stack automatically. e.g. Try spinning up a Kubeflow-based stack ([https://github.com/zenml-io/zenml/tree/main/examples/kubeflow](https://github.com/zenml-io/zenml/tree/main/examples/kubeflow)) on your local machine with this simple command. ZenML will build the container for you, create the Kubeflow pipeline, and run it automatically, with a simple command. In the future, we hope to expand this to include more complex deployments.


So what do you think? Links below:


* GitHub: [https://github.com/zenml-io/zenml](https://github.com/zenml-io/zenml) (A star would be appreciated!)
* Why: [https://blog.zenml.io/why-zenml/](https://blog.zenml.io/why-zenml/)
* Docs: [https://docs.zenml.io/](https://docs.zenml.io/)",MachineLearning
rgbbc5,1639499654.0,[N] Spring '21 Reproducibility Challenge Results and Support for Fall '21 Edition,"**TL;DR:** We announced that we'd support the Reproducibility Challenge by awarding $500 per paper reproduced, and we're announcing the award winners for the Spring '21 edition, as well as our support for the Fall '21 edition of the challenge. Check out the awesome papers below 👇

\---

Hey r/ML! Dean from [DagsHub](https://dagshub.com/) here. A while back, I [announced](https://www.reddit.com/r/MachineLearning/comments/np6eph/d_supporting_the_ml_reproducibility_challenge/) our support for the Papers with Code ML Reproducibility Challenge, and that we'd award participants ***$500*** per paper reproduced ([according to the guidelines](https://dagshub.com/pages/reproducibility-challenge)), to align incentives and put our money where our mouth is!

Today, I'm really happy to share the teams that were given the award, and the projects they worked on – read the full blog here: [https://dagshub.com/blog/ml-reproducibility-challenge-spring-2021/](https://dagshub.com/blog/ml-reproducibility-challenge-spring-2021/)

I honestly think the full read is interesting and worth your time, but here are the highlights from the papers:

1. **Contextual Decomposition Explanation Penalization (CDEP)** – The original paper proposes a method to reduce the chance of models learning spurious correlations instead of the actually important features. The team that reproduced it re-implemented the original project in Tensorflow, rewriting some functions completely from scratch! Along the way, they made a contribution to the Tensorflow addons repo
2. **Self-supervision for Few-shot Learning** – As its name suggests, this paper tests the importance of self-supervised learning in few-shot learning contexts. The team that reproduced it explored different input configurations than the one proposed in the article, and found out that it significantly affects the performance.
3. **GANSpace: Discovering Interpretable GAN Controls** – A proposed method to use ""simple"" PCA to create controls for GANs that are more humanly interpretable while being more computationally efficient. The team re-implemented the original implementation in Tensorflow and trained the model with a few benchmark datasets, they have a lot of very cool examples of the method in [their report](https://dagshub.com/midsterx/Re-GANSpace/wiki/Reproduction+of+GANSpace).

Thank you to everyone who took part in this challenge! None of this could be possible without you and we learned a lot in this process!

So what's next – well we've decided to continue the support the Fall 2021 edition of the Reproducibility Challenge! We want to host more reproduced papers since this makes the ML field better for everyone.

If you want to take part and move the field forward on the reproducibility front, check out the guidelines for more information on how to take part:

[https://dagshub.com/DAGsHub-Official/reproducibility-challenge/wiki/ML+Reproducibility+Challenge+Fall+2021](https://dagshub.com/DAGsHub-Official/reproducibility-challenge/wiki/ML+Reproducibility+Challenge+Fall+2021)",MachineLearning
rgahri,1639497401.0,[D] Virtual MLOps Round Table,"Given the turnout at our last two events and the great feedback we've received, we've decided to hold another Virtual MLOps Round Table on December 16th at 5 pm PST!

We'll follow the same format of forming breakout groups of 5-7 people and letting the peer-learning discussion flow from there.

There is absolutely no selling or pitching. The focus is pure peer learning.

You can sign up here if you're interested: [https://www.eventbrite.com/e/214267729547](https://www.eventbrite.com/e/214267729547)

Let me know if you have any ideas, thoughts, or feedback.",MachineLearning
rga91a,1639496697.0,[D] Are you using PyTorch or TensorFlow going into 2022?,"PyTorch, TensorFlow, and both of their ecosystems have been developing so quickly that I thought it was time to take another look at how they stack up against one another. I've been doing some analysis of how the frameworks compare and found some pretty interesting results.

For now, PyTorch is still the ""research"" framework and TensorFlow is still the ""industry"" framework.

The majority of *all* papers on Papers with Code use PyTorch

https://preview.redd.it/p62rqqidzi581.png?width=747&format=png&auto=webp&s=9c3b19ecc9c1386f6706f5b03e905280610ee81e

While more job listings seek users of TensorFlow

https://preview.redd.it/lcvzxrwmik581.png?width=747&format=png&auto=webp&s=e669f33897491225e0e793ae452b7ff64da17dee

**I did a more thorough analysis of the relevant differences between the two frameworks,** [**which you can read here**](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/) **if you're interested.**

Which framework are you using going into 2022? How do you think JAX/Haiku will compete with PyTorch and TensorFlow in the coming years? I'd love to hear your thoughts!",MachineLearning
rg8rw2,1639492548.0,[N][R] PADL: A unifying open-source development framework with Functional API for PyTorch to better build deep learning models,"We are happy to announce our new opensource project that brings functional building of models in PyTorch. PADL allows you to easily build pipelines using Pytorch layers along with normal Python functions/classes together.

PADL:

    * is a pipeline builder for PyTorch.

    * may be used with all of the great PyTorch functionality you're used to for writing layers.

    * allows users to build pre-processing, forward passes, loss functions and post-processing into the pipeline.

    * models may have arbitrary topologies and make use of arbitrary packages from the python ecosystem.

    * allows for converting standard functions to PADL components using a single keyword transform.

Github: https://github.com/lf1-io/padl

Notebooks to try it out: https://github.com/lf1-io/padl/tree/main/notebooks

Website: https://padl.lf1.io/

Pip: `pip install padl`",MachineLearning
rg69um,1639484263.0,[D][R] State of the art methods for identifying DAG parameters,"Say I have written down a directed acyclic graph (a causal model) with a few dozen variables and specified the functional form of the corresponding posterior PDFs. Moreover, I have a dataset with  observations for many (though not all) of the variables. For simplicity, let us assume that all variables are categorical. Which methods are state of the art for identifying the model parameters / the posteriors in the graph?",MachineLearning
rg650d,1639483736.0,[R] Time-to-event survival analysis for churn prediction (telecom),"Hello, so I've been studying different models to do churn prediction in a time-to-event manner. As such, i've came across simpler models like random survival trees, support vector regression and simple NN and RNNs.

I was wondering for anyone doing research or working in survival analysis, what are the current state of the art models, or more advanced models for time-to-event churn prediction. Thank you!",MachineLearning
rg4r3l,1639478054.0,[P] How can i decrease the response time of Meanshift algorithem in sklearn?,Currently i am using a m1 macbook pro and i want to use the Meanshift clustering algorithem to segement an image. But it takes like 2-3 seconds to give the output. Is there anyway i can reduce the time?,MachineLearning
rg43yb,1639475313.0,[N] International Data Analysis Olympiad (IDAO 2022),"&#x200B;

https://preview.redd.it/xr6g0ae6ch581.jpg?width=5760&format=pjpg&auto=webp&s=5b11736271d3e3d6b9720268d7e3d8e6f2b03529



We invite ML students and specialists from all over the world to take part in the **International Data Analysis Olympiad**. HSE University and Yandex are organizing it for the 5th time, and the Otkritie bank will be our platinum partner this year.

Since it’s our first anniversary, we decided to change the format: students and ML specialists are divided into two separate divisions. Only students are able to join the main competition — the Student Division. All others can join the Open Division to participate hors concours (for their own interest).

Traditionally, the first stage’s task will be given by the Laboratory of Methods for Big Data Analysis (LAMBDA, HSE University). It will be about predicting the properties of two-dimensional crystals of various configurations. The task for the Finals will be provided by the Otkritie bank. The Olympiad includes two stages:

**Online Stage (1-28 February 2022):**

• Track 1: Traditional machine learning competition on Yandex.Contest platform. You will need to make new predictions and upload them to the automatic verification system.

• Track 2: Come up with a solution for the same problem, keeping within a rigid framework of time and memory used.

**Final (16-17 April 2022, Moscow):**

• top 30 teams according to the Online Stage results will be invited to the Online final.

• In the final 36 hours of the competition, participants will try not just to train the model, but to create a full-fledged prototype, which will be tested both in terms of accuracy and performance.

**Registration is open till February 13:** [https://idao.world/](https://idao.world/)",MachineLearning
rg2g5v,1639468236.0,[D] Which cloud provider are you using for GPU inference?,"We've evaluated several options, but they all come up lacking.

- Hostkey: Unreliable and shoddy service
- GCP: Doesn't perform as well as our expectations
- AWS: Expensive",MachineLearning
rfzbr7,1639456665.0,[D] Training large language models: All you need is waiting and hope.,"Training large language models need so many patient, especially when you have few GPUs, finetune model and at the later period of finetune.

I am finetuning a 2.6B GPT-2 model, after finetuned 10% of datasets, the loss curve of continued finetuning has nearly no drop within 12 days.

It makes me feel doubt, fear and uneasiness, uncertain how the model will go. Divergence or no change anymore both will be bad, and you will waste much compute time.

Fortunately, as long as your hyperparams and datasets were good, codes and model has no bug, then the slow  improvement of loss at the later period of finetune should be ok, it will always improve until the end of training. (UPDATE: You can see loss curve in below image is keeping drop at the end of 12 days)

It reminds me of the last sentence of the count of Monte Cristo: all you need is waiting and hope.

&#x200B;

(UPDATED: in below image, the loss before black line is finetuned in 12 days, after black line is where the loss start to drop.)

&#x200B;

[after finetuned 10&#37; of datasets，loss curve of continued finetuning in 12 days](https://preview.redd.it/pa82z7e07i581.png?width=1299&format=png&auto=webp&s=6648b668de1f8df718be8036fde08327660bc503)

&#x200B;",MachineLearning
rfyujd,1639455131.0,[R]Is meta-learning use the same test samples as transfer learning use in few shot learning?,"1. As far as I know, **meta-learning** and **transfer learning** are two common ways to adress image classification task in **Few Shot** senario. But I found that, we offen use the images in train set of the specified dataset for test procedure other than test set in meta-learning; while in traditional transfer learning, we use test dataset to deal with test task. I wonder if I was getting the wrong understandding, and **is the accurracy reported from meta learning method is comparable with the report from transfer learning ?**
2. I found a word called ""**meta dataset**"", such as omniglot, Fungi, etc., and there is no testset in such dataset, so can I use transfer learning strategy deal with the mata-dataset ? If so, how can I split the dataset into trainset and testset, and weather the accuracy reported from transfer learning is comparable with metalearning strategy ?

Thanks !",MachineLearning
rfw5vf,1639446850.0,New open source project: Model Validation Toolkit [P],"My colleagues and I have open sourced a project for validating and monitoring machine learning models: the Model Validation Toolkit. Included in the Model Validation Toolkit are modules for:

* Measuring concept drift
* Specialized performance measures for biased data
* Assessing credibility of performance measurements taken from small samples
* Building interpretable neural networks
* Adaptively setting thresholds to maximize performance while intelligently checking for what might be missed
* Sensitivity analysis

User guides, documentation, and notebook tutorials, can be found on the project website: [https://finraos.github.io/model-validation-toolkit/](https://finraos.github.io/model-validation-toolkit/)

Please feel free to reach out to me here, on [Gitter](https://gitter.im/FINRAOS/model-validation-toolkit?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge), or via [Github issues](https://github.com/FINRAOS/model-validation-toolkit/issues) for questions and comments!",MachineLearning
rfvb02,1639444190.0,[P] Advice on stabilizing unstable DQN card-playing agent,"I'm attempting to train a reinforcement learning agent to play the game Sushi Go, using deep Q-learning with a memory recall buffer and separate model and target networks. In its current form, the Q-network takes as input a 25-element state vector and returns a 12-element vector of action values, with one hidden layer in between; all layers are fully connected with ReLU activation functions. I've been doing a search to tune the size of the hidden layer, trying values between 15 and 30  neurons. My methodology is to run five trials per network configuration as follows: starting with a randomly initialized network, run 2,500 training games, then evaluate the resultant performance over 10 trials of 100 games each.

While doing this search, though, I noticed that the network's behavior is wildly unstable. Starting from a randomly initialized model (no seed), the same network configuration will sometimes reach a 90% win rate after playing 2,500 games, while other times it will quickly plateau at a <10% win rate and show no improvement. I've tried longer trials (25,000 or  50,000 games, for example), and seen similarly unstable results. Anybody have any advice for tuning this performance? I've tried adding an  additional hidden layer to similar results; my next best guess is to start playing around with the state vector. Any thoughts are appreciated.",MachineLearning
rfum8k,1639442052.0,[D] Batch normalisation with batch size 1?," Batch  normalisation has shown to have poor performance on small mini-batch  size for large scale computer vision tasks. But for large input data  (e.g. medical image segmenation), batch size of 1 is sometimes required.  I have a very simple question: is it still adviseable to use batch  normalisation with batch size 1, and is this different from instance  normalisation ? (See Wu 2018, group normalisation).

I.e.  do the tensorflow/pytorch implementation of batch norm and instance  norm calculate exactly the same metrics for batch size 1?",MachineLearning
rfty6i,1639439823.0,[D] Favor volume or quality for BERT-based text classifier?,"Ill train a binary classifier. Yes samples make up about 5 percent of all samples. There are multiple persons doing the labelling. They have a pairwise alpha of 0.65

Scenario A:

Label each sentence once, and have every 10 sentence for all workers to check reliability. Resulting in 52000 single vote samples, plus 6000 multiple vote samples by all. Together about 3000 positive labels

Scenario 2

Tripple label everything, resulting in 20000 samples, where i can majority vote, but only have 1000 positive labels.

In your experience, is the better quality of samples worth the volume?",MachineLearning
rfssk6,1639436574.0,"[D] In transformers, why are positional embeddings added to the tokens rather than multiplied (element-wise) with them?","I think of a token as a vector describing the input (word/patch/etc.). Element-wise multiplication of the positional embedding and token would change the token in a relative fashion, whereas addition does so absolutely. Intuitively, I would have thought that a relative modulation makes more sense.

How should I be thinking about addition in this context?",MachineLearning
rfmxi5,1639421341.0,[D] What to do when you find a closely related paper has mistakes,"Hello guys, I am a new master student in ML and currently writing a paper. I found that there is a very closely related paper submitted to Arxiv several months ago, but the main proof in this paper is wrong for certain. I have contacted the author but heard no response.

So how should I treat this in my own paper? Of course I can't just ignore it and not cite it. But if I point out this mistake in my paper, will it make my paper ""dangerous"" during the review process in a conference? (Sorry if it is a silly question I am just worried about my first paper)",MachineLearning
rfmmxx,1639420571.0,[D] How much would you be willing to pay for a good scientific article recommendation app?,"On monthly basis. Free trial beforehand. Personalised.

[View Poll](https://www.reddit.com/poll/rfmmxx)",MachineLearning
rfkcdy,1639414818.0,[D] My deep learning 2021 year review and predictions for 2022,"Hey everyone wondering what people's thoughts are for deep learning in 2022. Here is my little recap and predictions. A little background is I work as a data scientist with python. Have been working with supervised deep learning models for NLP tasks for my own personal endeavors. Just recently started looking into audio models for fun. I haven't played with many computer vision image models like CLIP and DALL-E but I have been meaning to because of all the cool art I see. Luckily I have next year.

Python Library's Pytorch, Tensorflow, JAX

Pytorch definitely made big moves this year in research. With tools like speech brain being released for audio. models like EleutherAI GPT-NeoX and other large transformer models using PyTorch. They are also beating TensorFlow in google trends. Tensorflow still reigns supreme on GitHub with 3x the stars, and in industry. The tensor board and TFX for pipelining are really powerful stuff. Google used it for most of their papers like BERT which was also huge. I haven't built anything in JAX yet but it looks like it's got great potential. I can't wait to try it out in 2022. My understanding is you're able to run the same code on either GPUs or TPUs.

This brings me to my next prediction Cloud Platforms GCP, AWS, Azure, IBM Cloud

(My background I mainly use GCP so biased I have used AWS sage-maker but not a lot). AWS sage-maker in the industry has basically been unmatched till this year. GCP released their Vertex AI service and well it works. It's just way too new for many people to adapt to it yet. Additionally, they also released their fully managed Kubernetes clusters. It's my first time working with Kubernetes so I can't speak on this but talking to other people say it's AMAZING. I have no experience with azure or IBM cloud so please fill me in.

My prediction is that JAX will lead to a huge rise in models that can use TPU's. Google has plans to release its TPUv4 while Amazon Trainium is still in early access. 2022 (deep learning) might go to GCP they got the TPU's and the Kubernetes. Amazon of course still has the ML industry in a chokehold. Then again with everything being containerized switching services over well, it's still a hassle but if you're working with large deep learning models I think you might consider moving some models over to GCP.

Shout out some other cool organizations in the DL/ML space. Hugging Face for growing like crazy with their transformers library. Weights and Biases for always tracking my models. Nvidia has just been flexing hard with styleGAN3 and Megatron, Open AI GPT-3 (I bought into the hype for a bit that it solved language), EleutherAI GPT-J, and Neo. Speech Brain is not an org but a cool toolkit that was released this year. What are some other groups that do awesome stuff you love?

There is way too much stuff to keep track of I would love to do a broad overview year review of 2021 so please comment below your thoughts of the year. I think the attention transformer models are the biggest thing this year. I want to hear more about these crazy image models and what they can do. I really wanna hear from people who do unsupervised learning!

Which libraries have you used in 2021? Which is your favorite/ most used? Does anyone not use python?

What cloud provider do you use for ML? did you switch? do you want to? Do you use Azure? IBM cloud? hows Watson doing?

What are your predictions? Will people not care as much for TPU's? Are the slaughter bots coming?

One thing is for sure there's only going to be more with no hope of keeping track of it all.
\- Cup",MachineLearning
rfhwpn,1639408422.0,[D]What happens to the output of the two LSTM's in a bidirectional LSTM?,"I'm implementing a BLSTM but I'm confused as to what happens to the output of the 2 LSTM's (forward and backward).

Suppose we have 3 words each with a 4-length embedding. i.e: input is of the form 4x3

+ One group of people say that the output of the two LSTM's is *serially concatenated*. i.e: the 4x3 goes through each LSTM independently (suppose their output is 4) and a 4x3 matrix is output for each LSTM. the result is concatenated *fully*, meaning the result is a 12-length vector.

+ Another group says that the output of the two LSTMs goes through an activation function independently. Meaning the output of the two LSTM's (two 4x3 matrices) go through an activation function each word at a time. For the first word for example you have 4+4 inputs to the activation function.

+ Another group say the output is just concatenated on a word-by-word basis. Meaning the final output is an 8x3 matrix, 4 from forward and 4 from backward. (What I'm currently using).

The papers use a mix of these, with the original paper using something closer to the third option.

Thanks.",MachineLearning
rfcrr3,1639391345.0,[D] How difficult/easy is to learn NLP once you have experience in a CV?,"Hi, I have \~3 years of industry experience in different CV/Deep Learning tasks as an ML Engineer and recently started to scratch the surface of the NLP for educational purposes and, to be honest, it's quite interesting. As far as I could notice, NLP and CV share some concepts.",MachineLearning
rfb7c3,1639384827.0,"[D] Are there ""long running"" forex trading machine learning models or companies?","Basically those that already stood the test of time by having robust models for trading.

Also any thoughts on forex trading and using machine learning for long term usage?",MachineLearning
rfaz1q,1639383864.0,[D] how do you choose correct lambda values for your loss function ?,"how do you choose correct loss lambda value?  are All losses should be same scale? When your lambda selection process, do you just try different lambdas using any search algorithm (random search, grid search eth.) ? I wonder that, Because I work with deep learning models, I use lambdas but I don't know exact intuition.",MachineLearning
rf95gl,1639376705.0,[D] Useful data summary statistics with image classification," Hello!

&#x200B;

I am doing image classification with TensorFlow for learning purposes. I am splitting the data into 5 folds. I would like to get useful summary statistics on these validation sets. What could be useful other than the shape of the validation sets?",MachineLearning
rf59gu,1639363923.0,[P] Jina - An open-source framework to build scalable deep learning applications in mins,"[Jina](https://github.com/jina-ai/jina) is a neural search framework that empowers anyone to build SOTA and scalable deep learning search applications in minutes. Think building image search, video search, semantic search and more quickly.

⏱️ Save time - The design pattern of neural search systems. Native support for PyTorch/Keras/ONNX/Paddle. Build solutions in just minutes.

🌌 All data types - Process, index, query, and understand videos, images, long/short text, audio, source code, PDFs, etc.

🌩️ Local & cloud friendly - Distributed architecture, scalable & cloud-native from day one. Same developer experience on both local and cloud.

🍱 Own your stack - Keep end-to-end stack ownership of your solution. Avoid integration pitfalls you get with fragmented, multi-vendor, generic legacy tools.

-------


How to get started - `pip install jina`

[Checkout Jina on Github](https://github.com/jina-ai/jina)

This is a design pattern, that will get you to speed quickly and make good choices for your next deep learning app.

I'm seeking your feedback - **Do you find it too much opinionated or too much abstract?**",MachineLearning
rez90o,1639345900.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 127,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|121-130|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)|[Week 121](https://reddit.com/pmzx3g)|||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)|[Week 122](https://reddit.com/pw14z5)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)|[Week 123](https://reddit.com/q5fi12)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)|[Week 124](https://reddit.com/qjxfu9)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)|[Week 125](https://reddit.com/qtzbu1)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)|[Week 126](https://reddit.com/r4e8he)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)|[Week 117](https://reddit.com/omy345)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)|[Week 118](https://reddit.com/ovz52j)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)|[Week 119](https://reddit.com/p50knh)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)|[Week 120](https://reddit.com/pe2idh)||

Most upvoted papers two weeks ago:

/u/CatalyzeX_code_bot: [Paper link](https://arxiv.org/abs/2012.09841)

/u/PM_ME_YOUR_PROFANITY: https://arxiv.org/abs/2112.03178

Besides that, there are no rules, have fun.",MachineLearning
rew9uj,1639337350.0,[D] Has the ML community grown too big?,"EDIT: Probably should have titled this ""Have ML conferences grown too big?""..

There are a lot of [conferences](https://aideadlin.es) relevant to the machine learning community. General conferences like NeurIPS, ICLR, or ICML are [possibly approaching](https://www.reddit.com/r/MachineLearning/comments/hdn8ye/d_why_has_iclr_grown_so_much_faster_than_other/) ""carrying capacity"".

It also seems that 2022 will bring even more, with new conferences such as [CoLLA](https://lifelong-ml.cc) (Conference on Lifelong Learning Agents), [CLeaR](https://www.cclear.cc) (Causal Learning and Reasoning), and [AutoML](https://automl.cc). Today, a new journal TMLR (Transactions on Machine Learning Research) [was also announced](https://twitter.com/hugo_larochelle/status/1470094581953970176).

While [Transformers seem to be consolidating the field](https://www.reddit.com/r/MachineLearning/comments/rec0nk/d_karpathy_on_the_consolidation_of_the_field/), at least in terms of model architecture, we can also clearly observe more **branching** of the community around specific subfields, to address the ""next wave"" of problems to solve. Conferences for NLP, computer vision, robotics, and ML theory already exist as well.

What are your thoughts on these divisions of the community? How do you think future progress will be supported by these specialized conferences? Do you think NeurIPS, ICLR, and ICML will hold their place as top venues, or will research move away to be published elsewhere?",MachineLearning
reva1d,1639334553.0,[R] Researchers Propose ‘ProxyFL’: A Novel Decentralized Federated Learning Scheme For Multi-Institutional Collaborations Without Sacrificing Data Privacy,"Tight rules generally govern data sharing in highly regulated industries like finance and healthcare. Federated learning is a distributed learning system that allows multi-institutional collaborations on decentralized data while also protecting the data privacy of each collaborator. Institutions in these disciplines are unable to aggregate and communicate their data, limiting research and model development progress. More robust and accurate models would result from sharing information between institutions while maintaining individual data privacy.

For example, in the healthcare industry, histopathology has undergone increasing digitization, providing a unique opportunity to improve the objectivity and accuracy of diagnostic interpretations through machine learning. The preparation, fixation, and staining techniques utilized at the preparation site, among other things, cause significant variation in digital photographs of tissue specimens. 

Because of this diversity, medical data must be integrated across numerous organizations. On the other hand, medical data centralization involves regulatory constraints as well as workflow and technical challenges, such as managing and distributing the data. Because each histopathology image is often a gigapixel file, often one or more gigabytes in size, the latter is very important in digital pathology.

Paper: https://arxiv.org/pdf/2111.11343v1.pdf

Github: [https://github.com/layer6ai-labs/ProxyFL](https://github.com/layer6ai-labs/ProxyFL)

Short Summary by Nitish: [https://www.marktechpost.com/2021/12/12/researchers-propose-proxyfl-a-novel-decentralized-federated-learning-scheme-for-multi-institutional-collaborations-without-sacrificing-data-privacy/](https://www.marktechpost.com/2021/12/12/researchers-propose-proxyfl-a-novel-decentralized-federated-learning-scheme-for-multi-institutional-collaborations-without-sacrificing-data-privacy/)

https://preview.redd.it/d67spocrp5581.png?width=1234&format=png&auto=webp&s=5b7224f9fe4beb5b4e8d1f1d55231d1c9f6fea24",MachineLearning
rev18h,1639333852.0,[N] Announcing the Transactions on Machine Learning Research,"Announcement of a new ML Research Journal:

https://medium.com/@hugo_larochelle_65309/announcing-the-transactions-on-machine-learning-research-3ea6101c936f

*With this post, we’re happy to announce that we are founding a new journal, the [Transactions on Machine Learning Research](https://medium.com/@hugo_larochelle_65309/announcing-the-transactions-on-machine-learning-research-3ea6101c936f) (TMLR). This journal is a sister journal of the existing, well-known Journal of Machine Learning Research (JMLR), along with the Proceedings of Machine Learning Research (PMLR) and JMLR Machine Learning Open Source Software (MLOSS). However it departs from JMLR in a few key ways, which we hope will complement our community’s publication needs. Notably, TMLR’s review process will be hosted by OpenReview, and therefore will be open and transparent to the community. Another differentiation from JMLR will be the use of double blind reviewing, the consequence being that the submission of previously published research, even with extension, will not be allowed. Finally, we intend to work hard on establishing a fast-turnaround review process, focusing in particular on shorter-form submissions that are common at machine learning conferences.*",MachineLearning
rerlx0,1639323985.0,[D] Question regarding Downsampling in Convolutional Networks.,"In every convolutional network which has some kind of encoder, there are downsample steps, which transforms a tensor with shape [batch_size, h, w, filters] into a tensor with shape [batch_size, h/2, w/2, filters*2].

This is always done in one of two ways:

-Using a Convolutional-Layer with Strides=(2,2) and filters=filters*2

-Using a MaxPool2D-Layer followed by a Convolutional-Layer with filters=filters*2

There is a option that is never used:

-Concatenating a MaxPool2D-Layer and a Convolutional-Layer with Strides=(2,2) and filters=filters.

Why is this, at least for me, apparent third option never used? What are the drawbacks?",MachineLearning
relhfv,1639301075.0,"[D] What's the difference between a posterior distribution, predictive distribution and a posterior predictive distribution?","I'm trying to understand the difference between these three terms and everytime I read an answer online, I'm back to square one.",MachineLearning
regev9,1639281303.0,[P] Automating the process of finding and fixing bugs in ML code,"Hi everyone,

I noticed that I regularly spend 30-50% of my time debugging my machine learning models. Most of the time ML models will work, even with bugs, so you have to be really careful and not introduce them in the first place.

In an attempt to speed up the debugging process, I tried using static linters (Pyflakes/Pylint), and it looks like they work for syntax errors only and are very basic.

As a result, I decided to take the matter into my own hands and build an ML model that fixes bugs in ML models:

[https://gettensorbox.com/](https://gettensorbox.com/)

Please, let me know what you think :) Would really appreciate any feedback.",MachineLearning
refydd,1639279689.0,[D] Has the ML community outdone itself?,"It seems after GPT and associated models such as DALI and CLIP came out roughly a year ago, the machine learning community has gotten a lot quieter in terms of new stuff, because now to get the state-of-the-art results, you need to outperform these giant and opaque models.

I don't mean that ML is solved, but I can't really think of anything to look forward to because it just seems that these models are too successful at what they are doing.",MachineLearning
rec0nk,1639266676.0,[D] Karpathy on the consolidation of the field,"https://twitter.com/karpathy/status/1468370613886599170

I’ve definitely noticed a trend in the papers I’m seeing this year, with people combining building blocks in creative ways.

I wonder how far this trend will go, and what it will unlock",MachineLearning
reaxwn,1639263235.0,[D] How to Train your Decision-Making AIs (The Gradient),"Hi, just want to share our latest article on The Gradient, [How to Train your Decision-Making AIs](https://thegradient.pub/how-to-train-your-decision-making-ais/). It is a revision/summary of the paper [Recent advances in leveraging human guidance for sequential decision-making tasks](https://link-springer-com.stanford.idm.oclc.org/article/10.1007/s10458-021-09514-w) written by the lead author of that paper. Personally I think it's quite interesting, hope you enjoy!",MachineLearning
re1si9,1639235684.0,[D] PhD Internships,"Forgive me if there is a better venue for this--- r/cscareers does not appear to be appropriate as it's mostly software engineering discussions.

**Question**: What is the best way to land internships during a PhD?

Basically, I started getting conference publications precisely when COVID hit, so I have not been able to attend any conferences to meet people, and have zero personal connections and cannot get referrals that way. In any case, the few times I've been lucky enough to actually land an interview, the feedback was implicitly---but overwhelmingly---that I had no previous internship experience, and was therefore not a top candidate. This is especially apparent for the research engineering positions I found, but was also true for the research scientist positions as well. It appears to be a chicken-and-egg problem... any kind advice?

**Context**: I have several first-author publications (and several more second-author, etc.) at all the big conferences (NeurIPS, ICML, and ICLR). I come from a top European university, and my PhD lab/supervisor is average/somewhat well-known, but definitely far from being the top 10 labs in the world. My h-index is somewhere in the 5-10 range, and my citation count is somewhere in the 200-500 range. My research has been in and around the periphery of reinforcement learning, but may be a bit niche, so there's nothing ""obvious"" that an industry lab will be able to immediately ""plug me into"". I've done zero internships so far, because I have not been able to land any. Oh, and it is clear that my supervisor is not going to be of any help (they are more helpful should I choose to continue within academia, which I really do not wish to).

**Problem**: I've been job-/internship-hunting for a whole year now. My expected graduation date is coming up Summer, which is stressing me out as I have no continuation/exit plan whatsoever! The debt is piling high (my stipend has been, well, poor), and at this point I've noticed that I'm increasingly yearning more for ***any*** job at all, just to be able to pay my bills, than actually thinking about what I wish to work on / do research on. I fill out applications every day they simply seem to go nowhere.

Again, apologies if this is not the correct place for asking this. Appreciate any and all similar experience, knowledge, or advice!",MachineLearning
rdv2xw,1639210150.0,"[R] I'm Releasing Three of my Pokemon Reinforcement Learning AI tools, including a Computer Vision Program that can play Pokemon Sword Autonomously on Nintendo Switch | Video Proof & Source Code Available","Hullo All,

I am Tempest Storm.

# Background

I have been building Pokemon AI tools for years. I couldn't get researchers or news media to cover my research so I am dumping a bunch here now and most likely more in the future.

I have bots that can play Pokemon Shining Pearl autonomously using Computer Vision. For some reason, some people think I am lying. After this dump, that should put all doubts to rest.

Get the code while you can!

# Current Work:

[https://www.youtube.com/watch?v=9-FGntNrg80](https://www.youtube.com/watch?v=9-FGntNrg80)

Title: Pokémon AI - Shining Pearl Random AI Wild Battle | HonorableSaladAI talk | \[Source Code Available\]

Topics Covered:

1. Pokemon Shining Pearl Random AI Demo
2. Why I have a patent and Autonomous Navigation Explanation
3. Honorable Salad AI Model Overview
4. Pokemon Sword Random AI Demo

# Old Videos From 2019-2020

Let's start with the video proof. Below are videos that are marked as being two years old showing the progression of my work with Computer Vision and building Pokemon bots:

[https://vimeo.com/389171777](https://vimeo.com/389171777)

[https://vimeo.com/379207494](https://vimeo.com/379207494)

[https://vimeo.com/381522506](https://vimeo.com/manage/videos/381522506)

[https://vimeo.com/378229181](https://vimeo.com/378229181)

The videos above were formerly private, but I made them public recently.

# Repos

Keep in mind, this isn't the most up date version of the sword capture tool. The version in the repo is from Mar 2020. I've made many changes since then. I did update a few files for the sake of making it runnable for other people.

Tool #1: Mock Environment of Pokemon that I used to practice making machine learning models

[https://github.com/supremepokebotking/ghetto-pokemon-rl-environment](https://github.com/supremepokebotking/ghetto-pokemon-rl-environment)

Tool #2: I transformed the Pokemon Showdown simulator into an environment that could train Pokemon AI bots with reinforcement learning.

[https://github.com/supremepokebotking/pokemon-showdown-rl-environment](https://github.com/supremepokebotking/pokemon-showdown-rl-environment)

Tool #3 Pokemon Sword Replay Capture tool.

[https://github.com/supremepokebotking/pokemon-sword-replay-capture](https://github.com/supremepokebotking/pokemon-sword-replay-capture)

Video Guide for repo: [https://vimeo.com/654820810](https://vimeo.com/654820810)

# Presentation

I am working on a Presentation for a video I will record at the end of the week. I sent my slides to a Powerpoint pro to make them look nice. You can see the draft version here:

[https://docs.google.com/presentation/d/1Asl56GFUimqrwEUTR0vwhsHswLzgblrQmnlbjPuPdDQ/edit?usp=sharing](https://docs.google.com/presentation/d/1Asl56GFUimqrwEUTR0vwhsHswLzgblrQmnlbjPuPdDQ/edit?usp=sharing)

# Interested In Computer Vision?

I have a Create a YoloV3 Custom Object Detector Course:

[https://www.youtube.com/watch?v=Pe0utdaTvKM&list=PLbIHdkT9248aNCC0\_6egaLFUQaImERjF-](https://www.youtube.com/watch?v=Pe0utdaTvKM&list=PLbIHdkT9248aNCC0_6egaLFUQaImERjF-)

# Conclusion

I will do a presentation of my journey of bring AI bots to Nintendo Switch hopefully sometime next weekend. You can learn more about me and the repos then.",MachineLearning
rdtaiy,1639203040.0,[Discussion] Quick reminder: servers go up to 24TB of RAM,"I've seen this sub lamenting a bunch of times about large-companies having models only large companies can run and use. The latest such is a bunch of comments in the deepmind thread.

Maybe I am missing something, but I think there is a chance that people here have an outdated view of computers. So I want to just state a few facts, and understand if given these limitations there would still be any problem for a ""normal"" research lab to run large models:

\- Currently, outside of super computers and the latest 2021 hardware, the most RAM a motherboard can fit is \~24.5TB running with 8x intel Xeons (and available on e.g. aws: [https://aws.amazon.com/ec2/instance-types/high-memory/](https://aws.amazon.com/ec2/instance-types/high-memory/))

\- Good intel Xeons are very fast, i.e. probably faster than an old gen cuda 11 compatible GPU even for GPU optimized libraries ala torch, much faster than all but a few GPUs with CPU-optimized optimization algorithms (hash map based) are used

\- Server motherboards can also fit up to 8GPUS (I think this number should actually be 32 but I've never seen an offer with more than 8, so eh?)

\- Most cloud providers have 16GB GPUs (with up to 8GPUs per server)

\- The largest GPUs run 80GB of RAM ([https://www.nvidia.com/en-us/data-center/a100/](https://www.nvidia.com/en-us/data-center/a100/)) though they are not widely available

\- The largest GPU-optimized servers boast 1.3T of GPU RAM ([https://www.nvidia.com/en-us/data-center/hgx/](https://www.nvidia.com/en-us/data-center/hgx/))

&#x200B;

Price-wise buying said GPU servers are hard to pin down but seem to be in the 200-600k range. I assume they are or will soon be available to rent and that price will translate to 200-600$/hr for ""expensive"" providers and maybe as low as 50-100$ for cheap ones with older models. But this is an uneducated estimate based on CPU server pricing compared to buy price.

Prices for the 6.2TB server are 50$/hr ([https://aws.amazon.com/ec2/pricing/on-demand/](https://aws.amazon.com/ec2/pricing/on-demand/)) and presumably the 24.5TB version is \~200$/hr.

It seems to me like both the CPU and GPU routes here should allow for training and certainly running \*very\* big model, more so than the biggest models from either Google or OpenAI.

The price point is high, but operating at 200$/hr or putting down half a million for hardware every 1-3 years seems more than reasonable for a small-time well-funded lab, given that most researcher salaries are 1/4th to 1x that amount.

Given that often enough you can do research work on models that are smaller and extract valid heuristics for models 10-100x the size this is not necessary.

Obviously, this is prohibitive for most people, I can't run it on my laptop. But it seems unfair to say that large text (or image) models are limited to ""a few giant corporations with their server farms"". It seems that a bloke with no knowledge of clustering computers and half a million worth of funding could easily work with models the size of GPT-3, not only for inference but also for testing.

This seems to increase the availability of such models to something like a few dozen thousand companies and a few hundred or even thousands of well-funded academic labs.",MachineLearning
rdfdcv,1639161637.0,[P] Collection of 33 Psychology Related Datasets Finally Released!,"I am happy to announce that I finally finished cleaning, organizing creating baselines, and uploading the first version of the **Massive 33 datasets collection of** [**Openpsychometrics**](https://openpsychometrics.org/)

The whole project took me around two weeks to clean and organize (way more than I planned) so please if you find this of value:  Your feedback & support is highly appreciated!

**Why would you want to work on them?**

* **Small** \- Size is similar to titanic.
* **Clean** \- Someone on this cleaned them a lot!
* **Intuitive** \- You see what was the question. You see what the individual answered. Nothing more. Nothing less.
* **Funny & Fun to work with** \- Questions like Would you rather own a gun or do the dishes?
* **Has the prettiest starter notebooks you have ever seen in your life**Seriously. I invested an embarrassingly amount of time on the basline notebooks. check them out!

## The Collection:

The collection includes 32 psychology & pseudo-psychology datasets, links are all here:

[https://www.kaggle.com/general/284428](https://www.kaggle.com/general/284428)",MachineLearning
rde0ti,1639158003.0,[P] CodeParrot 🦜: Train your own CoPilot from scratch!,"We are releasing CodeParrot 🦜 - my first project at Hugging Face!


**What is it?**
It is a code base to build large language models for code generation from scratch. It includes the code to clean up GitHub scale datasets, train GPT-2 style models from scratch on distributed infrastructure, and an evaluation benchmark of OpenAI's HumanEval dataset. We are also releasing two trained models and some demos to play with the models: [https://huggingface.co/spaces/lvwerra/codeparrot-generation](https://huggingface.co/spaces/lvwerra/codeparrot-generation)

This is my first project at Hugging Face and I thought this is a good opportunity to talk about some of the difficulties behind the scenes of such a project, which are usually not so much communicated after its release.


**The topic of the story: distributed debugging is hard!**
The first few small experiments training GPT-2 models on a code dataset went well, so we decided to scale and train the first smallish model for longer. For mysterious reasons, the training just stopped after a few hours. Not with an error, but the training loop just didn't continue. Even more interesting, when repeating the experiment it happened again but always at another step. And we never observed it in experiments with fewer workers.


**What the hell was going on?**
After literally weeks of debugging and experimenting, finally some insight. I started to log everything with the maximum verbosity possible. Interestingly, the training stop always coincided with a little debug message concerning some sort of retry. It did not always stop when such a message appeared, but whenever it stopped such a message was there.


**Small details matter!**
We used a feature called streaming in the the datasets library to read the data on the fly. It has a retry mechanism when reading the next chunk from a file fails. It turns out that when many workers are reading from the same file and one worker fails and tries to read again, there is a tiny chance for a deadlock. The more workers there are, the higher the chance of a deadlock, which also explains why we never observed this in smaller experiments.


The retries could easily be avoided by changing some streaming settings, and later we switched to having a single data processing worker for iterable datasets later, which resolved the issue altogether and also improved training efficiency by up to 25%!


**My main takeaways**
I totally underappreciated how hard and stressful the expensive debugging sessions are at scale. Every experiment is expensive and might take a long time to fail which limits the iteration cycle considerably. Distributed systems also behave very differently to simple single process programs, and it takes some time to adapt the mental model. All this make scaling training and models not as simple as it sounds.


**More resources:**
If you are curious about the what it takes to train such a model, checkout the blog post for a brief tutorial on training CodeParrot 🦜: [https://huggingface.co/blog/codeparrot](https://huggingface.co/blog/codeparrot)

Also, all code is open source, free to use, and available here: [https://github.com/huggingface/transformers/tree/master/examples/research\_projects/codeparrot](https://github.com/huggingface/transformers/tree/master/examples/research_projects/codeparrot)

If you are interested in more details about the design considerations when setting up a large dataset, building efficient tokenizers, and architecture choices, make sure you have a look at the CodeParrot 🦜 chapter in the upcoming book on Transformers and NLP: [https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/](https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/)",MachineLearning
rdd5f9,1639155717.0,[N] AutoML-Conf'22,"We are excited to announce that after 8 years of AutoML workshops at ICML, in 2022 we'll hold the first international conference on AutoML!
Website: [http://automl.cc](http://automl.cc/)
Amazing speakers: Anima Anandkumar, Jeff Clune, Chelsea Finn, Timnit Gebru, Julie Josse, Alex Smola
Co-located with ICML, 3 days, currently planned in person.
Submission deadline: Feb 24, 2022.
Great co-organizers: Frank Hutter, Mihaela van der Schaar, Marius Lindauer, Isabelle Guyon
Please spread the word, submit your best work, and see you all at the conference :-)",MachineLearning
rdb1uw,1639149974.0,[P] uttt.ai: AlphaZero-like solution for playing Ultimate Tic-Tac-Toe in the browser,"**tl;dr** I developed AI solution (MCTS + NN) inspired by AlphaZero for playing Ultimate Tic-Tac-Toe game in the browser. You can try it yourself here: https://uttt.ai.

Why?

Ever since I started working in Machine Learning 5 years ago, I have always wanted to do some cool project for my portfolio. Reading scientific papers gave me plenty of ideas, but only after I read [AlphaZero preprint](https://arxiv.org/abs/1712.01815) I knew: this is it!

AlphaZero is a third paper in AlphaGo, AlphaGo Zero, AlphaZero, MuZero sequence. In AlphaZero paper Deepmind generalizes previous work so that AI can learn through self-play not only how to master Go, but also Chess and Shogi. I had read previous papers, but it was AlphaZero specifically that sparked my imagination. Probably because I love simple and elegant engineering solutions and AlphaZero is mostly about that.

I discovered Ultimate Tic-Tac-Toe and implemented AlphaZero in early 2018. After a few weeks of work I realized it's not going to be an easy ride. There were two major problems that essentially made me forget about this project for a long time.

Firstly, although Ultimate Tic-Tac-Toe (UTTT) looks easier than Chess or Go, it is still quite a challenging game. The average length for UTTT game is somewhere between 40 and 50 [plies](https://en.wikipedia.org/wiki/Ply_\(game_theory\)). The average number of legal actions per position is somewhere around 7 (my estimate from self-play data). It is difficult setup for a side-project. One of the key factors enabling AlphaZero success is massive computing power (5000 TPU v1 for self-play and 64 TPU v2 for training). I had to figure out much cheaper way to develop interestingly good AI under my personal budget.

Secondly, when I envisioned deploying AlphaZero in the browser I had zero knowledge of web development and frontend in general. Which meant I had to find some time to learn it. Not easy if you already have a full-time job and other stuff going on in your life. I decided to put the whole project on hold and said to myself: ""maybe one day there will be better time for this...""

Fast-forward to 2021. I left my job and decided to spend a year on a career break, pursuing my interests. I realized that I finally had enough time and resources to conquer this project. I learned the basics of web browsers, HTML, CSS, JavaScript and React. I bought a desktop PC. I've managed to incrementally redesign AlphaZero self-play training into something more executable on my computer. I evaluated AI and confirmed it's superior in comparison to already existing implementations that you can access online (other websites and mobile apps). I built a React app, tested it and finally deployed this week: https://uttt.ai.

Differences from the original AlphaZero:

- Much smaller Policy-Value Network architecture designed specifically for playing Ultimate Tic-Tac-Toe in the browser with only 5 million parameters (20 MB): [source code](https://github.com/ar-nowaczynski/uttt/blob/main/utttpy/selfplay/policy_value_network.py)
- Total separation of self-play data generation process from the Policy-Value Network training (offline RL). This was crucial change. There is no way I could succeed with a single script that implements online RL and runs for 10 weeks on my desktop. This had to be broken down into more manageable stages.
- More MCTS simulations per position for training (self-play data quality over quantity).
- The initial self-play dataset was generated from pure MCTS simulations (random playouts are faster and better than random Policy-Value Network predictions).
- Search simulations are synchronous, single-threaded and sequential.
- Enabled data augmentation by flipping the board during Policy-Value Network training.
- Value target for MSE loss function is defined as the root's mean state value rather than the game outcome.
- Masked KL divergence loss for policy head instead of Cross Entropy loss.
- Auxiliary policy head loss for predicting action values next to action logits.

There is no external benchmark to compare my solution with, so I came up with my own evaluation setup. Details are on https://github.com/ar-nowaczynski/uttt#evaluation. The main selling point is that the final Policy-Value Network checkpoint with 1k simulations is much better and faster than MCTS with random playouts and 10M simulations (4-order of magnitude difference). In other words, Policy-Value Network learned useful information about Ultimate Tic-Tac-Toe enabling better and faster evaluations.

I haven't found any other publicly available AI for Ultimate Tic-Tac-Toe that can beat https://uttt.ai. The best version online I found is: https://www.theofekfoundation.org/games/UltimateTicTacToe/ which implements MCTS with random rollouts and some custom modifications (source code: https://github.com/The-Ofek-Foundation/UltimateTicTacToe/blob/master/script.js). It keeps up for the first 10-15 moves with uttt.ai, but eventually does some mistakes and losses the game. Sometimes there is a draw.

Various technical details and takeways from the project:

- https://uttt.ai is build using React + onnxruntime and deployed as a Azure Static Web App (shout-out to Microsoft for providing great service).

- Policy-Value Network is running in the browser on your device using WebAssembly backend. It utilizes only CPU. I wanted to use WebGL backend, which enables GPU access, but they don't support ConvTranspose2D layer yet. This or I have to rewrite and retrain Policy-Value Network without ConvTranspose2D.

- To learn web dev & frontend I read the entire https://javascript.info/ course twice, watched plenty of DevEd videos (https://www.youtube.com/c/DevEd) and implemented many small throwaway projects.

- My computing hardware for developing this project was: Intel i7-10700K with 8 cores x 3.80GHz, 2 x RTX 2080 Ti, 64 GB RAM.

- The ONNX format is great. You can load PyTorch model in JavaScript (via https://onnxruntime.ai/) very easily!

- torch.jit nad libtorch are brilliant tools for using PyTorch model in C++!

- https://uttt.ai works best on desktop and gaming laptops. On my desktop: 75 simulations / sec, on my laptop: 70 sims/sec, on my phone 2.5 sim/sec.

- I created a video showing AI self-play with 100,000 simulations: https://www.youtube.com/watch?v=oqbHx3NSzaY. I think about recording another video with all games from NMCTS2 10k vs MCTS 10M evaluation, to show how MCTS is dominated by NMCTS2.

- If you don't know the Ultimate Tic-Tac-Toe game rules: https://uttt.ai/rules

My strategy for playing Ultimate Tic-Tac-Toe (learned from AI) is as follows:

- start in the center square of the center subgame (undoubtedly the best move, unless you want to surprise the opponent with something weird)
- the 'O' response is to push to the corner subgame, so then let the next 8 moves be played in the corner subgames
- when the 'O' breaks out of the corner subgames, jump between the side subgames (these are the least useful to take, but still, one has to be careful not to mess up here)
- maintain the overall balance on the board and wait for the opponent's mistake (the game is a marathon, not a sprint)
- think twice before sending your opponent to the finished subgame (being able to choose any move from the unfinished subgames is very powerful)

source code: https://github.com/ar-nowaczynski/uttt

twitter thread: https://twitter.com/ArNowaczynski/status/1469318918837870593",MachineLearning
rd8wum,1639143695.0,[D] How to make the best out of NeurIPS?,"I will be starting to work on my PhD project in a few months - I am already enrolled, but the first months are devoted to looking for a supervisor and PhD project. I am attending NeurIPS and I would like to make the best out of it. The amount of papers, events, etc.. is daunting for someone who's just starting. What recommendations do you have for me and for people in my situation?",MachineLearning
rd7pd2,1639139795.0,[D] What are techniques for few shot domain adaptation?,"I am trying to do domain adaptation from synthetic to real images. The task is anomaly detection,

Usually the problem for domain adaptation is that there are a lot of target images without labels. In my case I only have a few target images with labels. Therefore, the common technique of creating pseudo labels for the target domain is not useful.

My current idea is to do some kind of style transfer from the synthetic images to the real images. ([CyCADA](https://arxiv.org/abs/1711.03213), [Contrastive unpaired image to image](https://arxiv.org/abs/2007.15651))
Do you have other ideas of domain adaptation where I have a lot of source images but only a few target images (but with ground truth)?


I would be glad to be pointed in the right direction.",MachineLearning
rd609b,1639133018.0,[R] 3D Printing Process Parameters Optimization,"

Hi,

I am doing my dissertation and I have to make a machine learning model to optimize 3D printing process parameters (any 3D printing process). The probelm is i can't seem to find any data online. All I have is 2000 records of data of an L-PBF process (4 predictor variables), which is largely synthetic. I have been trying to research if I can actually make an ML model based on so less data. Any help would be appreciated.

P.S. I know this is a Machine Learning subreddit, but any leads regarding 3D printing process parameter dataset would also be appreciated. Thanks.",MachineLearning
rd3oby,1639123082.0,[P] Yuno: An AI search engine that recommends anime given a specific description.,"**Yuno In Action**

&#x200B;

[Yuno](https://reddit.com/link/rd3oby/video/usbwwme58o481/player)

This is the search engine that I have  been working on past 6 months. Working on it for quite some time now, I  am confident that the search engine is now usable.

source code: [**Yuno**](https://github.com/IAmPara0x/yuno)

Try Yuno on (both notebooks has UI):

1. [**kaggle notebook**](https://www.kaggle.com/iamparadox/yunoo/)  (recommended notebook)
2. [**colab notebook**](https://colab.research.google.com/drive/1WAewYgHDmDEWhPBBOvGgyLTiOaasVyOz?usp=sharing)

My Research on [**Yuno**](https://medium.com/@confusedstudent13/yuno-context-based-search-engine-for-anime-39f5cb86f845)**.**

# What does it do?

Basically  you can type what kind of anime you are looking for and then Yuno will analyze and compare more **0.5 Million** reviews and other anime information  that are in it's index and then it will return those animes that might  contain qualities that you are looking. [r/Animesuggest](https://www.reddit.com/r/Animesuggest/) is the inspiration for this search engine, where people essentially does the same thing.

# How does it do?

This is my favourite part, the idea is pretty simple it goes like this.

Let says that, I am looking for *an romance anime with tsundere female MC.*

**If  I read every review of an anime that exists on the Internet, then I  will be able to determine if this anime has the qualities that I am  looking for or n**ot.

or framing differently,

**The  more reviews I read about an anime, the more likely I am to decide  whether this particular anime has some of the qualities that I am  looking for.**

&#x200B;

Consider a section of a review from anime ***Oregairu:***

>Yahari Ore isn’t the first anime to tackle the anti-social protagonist,  but it certainly captures it perfectly with its characters and deadpan  writing . It’s charming, funny and yet bluntly realistic . You may go  into this expecting a typical rom-com but will instead come out of it  lashed by the harsh views of our characters .

Just By reading this much of review, we can conclude that this anime has:

1. anti-social protagonist
2. realistic romance and comedy

If we will read more reviews about this anime we can find more qualities about it.

If this is the case, then reviews must contain enough information about that particular anime to satisfy to query like mentioned above. Therefore all  I have to do is create a method that reads and analyzes different anime  reviews.

# But, How can I train a model to understand anime reviews without any kind of labelled dataset?

This  question took me some time so solve, after banging my head against the wall for quite sometime I managed to do it and it goes like this.

**Let** ***x*** **and** ***y*** **be two different anime such that they don’t share any genres among them, then the sufficiently large reviews of anime** ***x*** **and** ***y*** **will have totally different content.**

This idea is inverse to the idea of web link analysis which says,

**Hyperlinks in web documents indicate content relativity,relatedness and connectivity among the linked article.**

**That's pretty much it idea, how well does it works?**

&#x200B;

[Fig1: 10K reviews plotted from 1280D to 2D using TSNE](https://preview.redd.it/d3hzr8gf8o481.png?width=1008&format=png&auto=webp&s=1b8596f591326857de8ceee8165ab9eebae64d83)

&#x200B;

[Fig2: Reviews of re:zero and re:zero sequel](https://preview.redd.it/d24hte0j8o481.png?width=635&format=png&auto=webp&s=0216141c99b459c72ac332f0b097c996b112bfc8)

As, you will able to see in **Fig1** that there are several clusters of different reviews, and **Fig2** is a zoomed-in version of **Fig1,** here the reviews of re:zero and it's sequel are very close to each other.But, *In our definition we never mentioned that an anime and it's sequel should close to each other.*  And this is not the only case, every anime and it's sequel are very  close each other (if you want to play and check whether this is the case  or not you can do so in this interactive [kaggle notebook](https://www.kaggle.com/iamparadox/anime-search-visualization) which contains more than 100k reviews).

&#x200B;

Since,  this method doesn't use any kind of handcrafted labelled training data  this method easily be extended to different many domains like: [r/booksuggestions](https://www.reddit.com/r/booksuggestions/), [r/MovieSuggestions](https://www.reddit.com/r/MovieSuggestions/) . which i think is pretty cool.

&#x200B;

# Context Indexer

This is my favourite indexer coz it will solve a very crucial problem that is mentioned bellow.

Consider a query like: *romance anime with medieval setting and with revenge plot.*

Finding such a review about such anime is difficult because not all review talks about same thing of about that particular anime .

For eg:  consider a anime like [Yona of the Dawn](https://anilist.co/anime/20770/Akatsuki-no-Yona)

This anime has:

1. great character development
2. medieval theme
3. romance theme
4. revenge plot

Not all reviews of this anime will mention about all of the four things mention, some review will talk about romance theme or revenge plot. This means that we need to somehow ""remember"" all the reviews before deciding whether this anime contains what we are looking for or not.

I have talked about it in the great detail in the mention article above if you are interested.

&#x200B;

**Note:**
  please avoid doing these two things otherwise search results will be very bad.

1. Don't make spelling mistakes in the query (coz there is no auto word correction)
2. Don't type nouns in the query like anime names or character names, just properties you are looking for.
**eg**: don't type: anime like attack on titans

type: action anime with great plot and character development.


This is because Yuno hadn't ""watched"" any anime. It just reads reviews that's why it doesn't know what attack on titans is.


&#x200B;

If  you have any questions regarding Yuno, please let me know I will be  more than happy to help you. Here's my discord ID (I Am ParadØx#8587).

Thank You.

&#x200B;

Edit 1:  Added a bit about context indexer.

Edit 2:  Added Things to avoid while doing the search on yuno.",MachineLearning
rd3hwy,1639122291.0,[P] WaveNets - a comprehensive presentation with code,"I have been studying WaveNets for some time now and have published my experiences at

[https://github.com/cheind/autoregressive](https://github.com/cheind/autoregressive)

as a presentation with code.

WaveNets are exceptionally efficient in capturing distant relationships with few parameters and I think the concepts are not immediately clear to everyone. The very property that WaveNets are generative models allows us to use the same model to both generate new data, continue existing data, or (progressively) classify observations (as shown below).

For illustration, in my approach WaveNets are not applied to sound as usual, but to random Fourier series and MNIST images. The image below shows progressive classification of MNIST images by a WaveNet trained on p(X=image|Y=class). Classification is performed via Bayes rule.

https://i.redd.it/tyzdlc0b6o481.gif

I would be happy if this presentation helps some of you to understand WaveNets better and maybe use them in your next project.",MachineLearning
rd2l00,1639118726.0,[P] 1st ever method to perform *GPU* quantization on most 🤗 HF transformer models: > 2X faster inference!,"Quantization is a technique to significantly accelerate inference by replacing high precision tensors by lower precision representation in a way where accuracy is kept intact (or close to).

It’s quite common in CPU inference, a lot less on GPU, even if the performance boost is significant.

End-to-end method (we also added many other benchmarks): [https://github.com/ELS-RD/transformer-deploy/blob/main/demo/quantization\_end\_to\_end.ipynb](https://github.com/ELS-RD/transformer-deploy/blob/main/demo/quantization_end_to_end.ipynb)

The library repo (Apache 2 licence): [https://github.com/ELS-RD/transformer-deploy](https://github.com/ELS-RD/transformer-deploy)

To give you an idea of the latency speed up:

&#x200B;

[roberta-base \(classification, MNLI\) latency benchmark - batch 32, seq len 256](https://preview.redd.it/yemwyjlwvn481.png?width=589&format=png&auto=webp&s=a238ea8365a09e192cd65e43429792ba0ca67d29)

AFAIK 3 methods of GPU quantization exist for the 2018 vanilla Bert architecture (2 from Nvidia and 1 from Microsoft) but none exist for any other architecture (Roberta, Electra, DistillBert, Deberta, etc.) limiting the benefit of GPU quantization to old models. We hope that this project will help generalisation of quantization in NLP and ease NLP big models deployment.

It’s a big deal as quantization is rarely used on GPU (unlike CPU) because it requires some Nvidia tools not well known from most ML practitioners (like TensorRT). In the lib we have wrapped those tools so they are transparent to the final user.

The result is a model that is ***always several times*** ***faster*** than vanilla Pytorch on GPU on any batch size / seq length for any transformer flavour (small, base, large, XX-large, etc.).

This work is based on a very recent model (QDQBert -> 2018 vanilla Bert which supports quantization) added by Nvidia to the Hugging Face transformer library a few weeks ago.

Next to the generic method we have developed, we have also implemented in the lib a new model as a proof of concept, QDQRoberta (and no, Roberta is not identical to Bert!). Of course the idea is to extend the process to all transformer architectures (there are not that many).

**Both approaches offer different trade-offs in generalisation and accuracy** (details in the notebook)**.**

There are many things which are still to be experimented, for instance for QDQRoberta (and other future QDQ models), is Roberta source code (almost trivial) modification a good idea? Should we just patch ONNX files like Microsoft does? Or build from scratch the graph directly in TensorRT like Nvidia does? Or leverage the new Pytorch FX interface? All approaches should, in theory, lead to the same result (accuracy/speed), but offer different ease of use (final user) VS ease of maintainability (library maintainer) trade-offs.

**GPU quantization is not very discussed/known in NLP, so please don’t hesitate to comment/ask questions below, so we can improve the tuto and democratize GPU quantization.**",MachineLearning
rd26uw,1639117269.0,[P] Increasing the Accuracy of Textual Data Analysis on a Corpus of 2 Billion Words,"[https://engineering.soroco.com/increasing-the-accuracy-of-textual-data-analysis-on-a-corpus-of-2000000000-words-part-1/](https://engineering.soroco.com/increasing-the-accuracy-of-textual-data-analysis-on-a-corpus-of-2000000000-words-part-1/)

At Soroco, we ingest between 200 million and 2 billion words over the course of model training and analysis for a single team of workers using our Scout product. In this blog post, I talk about some tips and tricks that we might use to increase the accuracy of our models, including appropriate processing of text for the purpose of leveraging standard techniques from machine learning. I then demonstrate this by showing how to represent text in a high-dimensional vector space with applications to a toy regression problem.",MachineLearning
rd0hm6,1639111456.0,[P] Utility scripts for COCO JSON format for object detection/segmentation,"Sharing a simple collection of functions/scripts for manipulating object detection/segmentation datasets in COCO format (being the dominant dataset format). Some dataset conversion utilities are provided as well. Feedback (& contributions) are welcomed :)

[https://github.com/levan92/cocojson](https://github.com/levan92/cocojson)",MachineLearning
rcuyax,1639094458.0,[R] is there a paper/application where a direct interpretation is given to the hidden state in a RNN?,"As in after an RNN is optimized/estimated the resulting hidden state sequence is used further, or a direct interpretation is given to it based on the subject where the RNN is being applied to.",MachineLearning
rcttt3,1639091177.0,[D] The Carbon Footprint of Machine Learning,"**The energy costs of AI have risen 300,000-fold between 2012 and 2018 and the focus on large language models like GPT-3 will make this worse** Reducing the carbon footprint has become a critical need for the AI community - are huge models the best way forward?

Blog Link: [https://kv-emptypages.blogspot.com/2021/11/the-carbon-footprint-of-machine-learning.html](https://kv-emptypages.blogspot.com/2021/11/the-carbon-footprint-of-machine-learning.html)

&#x200B;

[The outlook for ML training costs - Source: Ark Investments LLC](https://preview.redd.it/v00jb73yll481.png?width=1660&format=png&auto=webp&s=c6880a49aa3453c0ae8f8a0d9b76b826299493aa)",MachineLearning
rcszg7,1639088772.0,[R] Research Survey: Silent Bugs in Keras/Tensorflow framework,"Hello,

Our research group at Polytechnique Montréal (under the supervision of Prof. Foutse Khomh) is conducting a survey on “Silent Bugs in Keras/Tensorflow frameworks”. We have prepared an online survey that takes around 10 minutes of your valuable time to complete asking about the relevance and severity of observed silent bugs in Keras/Tensorflow. Moreover, you could kindly share this survey with colleagues/students who you consider eligible to participate.We will only send a reminder in a week to remind you about completing the study. Please, if you do not want to take part in the study simply ignore those two emails. We will not disturb you further than that.The results of this survey will be publicly accessible through arXiv.org in anonymized form. At no point in the survey will we ask you for your name, and we will not be logging your IP address to allow anonymity. If you would like to know more about this study, feel free to contact us with your questions.

Link: [https://forms.gle/yyPZZCJQP2eYWVzq8](https://forms.gle/yyPZZCJQP2eYWVzq8)

We really appreciate your time supporting our research!

Best regards,

Amin Nikanjam ([amin.nikanjam@polymtl.ca](mailto:amin.nikanjam@polymtl.ca)),

SWAT Lab., Polytechnique Montréal, Montréal, Canada, [http://swat.polymtl.ca/](http://swat.polymtl.ca/)",MachineLearning
rcn2ha,1639072415.0,"[P] Pretrained models (GPT-2, CLIP, etc.) as API endpoints for building web apps","Hi all, I recently built [pretrained.convect.ml](https://pretrained.convect.ml/).

I’ve been interested in the potential for building web apps on top of pretrained models that have been gaining popularity in the machine learning community. One foundational piece for these apps would be access to predictions by these models with fast response times. So, I’ve made models available via an API for a few AI tasks:

1. Text generation (GTP-2, GPT-Neo 125M, and PEGASUS for paraphrasing): Provide text and generate more text with a similar style and content. Use these models to build an AI writing assistant or even synthesize entire articles.
2. Computer vision (CLIP): Measure the association between any sequence of images to a list of arbitrary texts. Use CLIP in an app to detect vehicles, animals, trees, household appliances, or other physical objects that you can describe with words.
3. Conversation (Blenderbot 400M Distill): Build an AI-powered chatbot that responds to user inputs. TBH Blenderbot’s responses don’t always make sense, so I’d be careful with this one.
4. Article summarization (Bart Large CNN): Generate a summary of the salient points in an article. Use this model to build tools to help people consume information faster.
5. Text classification (Bart Large MultiNLI): Measure the association between any sequence of words to a list of arbitrary text labels. A classification model can be used to detect topics, e.g. send customer call center transcripts to this API to detect if customers are reaching out about specific topics of interest, such as product defects or payment discrepancies.
6. Sentiment analysis (DistilBERT base uncased finetuned SST-2): Detect the sentiment of a piece of text. This model, for example, could be used to measure overall customer approval levels for a product from social media posts.

Many of the endpoint response times are sub-second and could be used in applications to provide a near-real-time experience. I have also included CodePens (in React) for each of the models to make it easy to get started with building on top of them. All the endpoints handle preflight requests from any origin, so applications can be purely browser-based if you want to do that.

I’m not the first to make these models available for free over an API but am hoping to make the experience of getting started as easy as possible. I’d also love to chat with anyone who has been curious about using these models in their projects or if there’s a particular model that you wish was readily available as an API endpoint.",MachineLearning
rcl0l0,1639066661.0,[D] Quick tips about building a chatbot with GPT-3 or GPT-J,"Hello!

I  realize I have more and more questions from people trying to leverage  GPT-3 or GPT-J for their next chatbot. And usually questions are always  about 2 things:

* How to format my requests so the model understands that I am in conversational mode?
* How can the model keep an history of my conversation?

I'm answering these 2 points in this quick article: [https://nlpcloud.io/how-to-build-chatbot-gpt-3-gpt-j.html](https://nlpcloud.io/how-to-build-chatbot-gpt-3-gpt-j.html?utm_source=reddit&utm_campaign=k431103c-ed8e-11eb-ba80-4242ac130007)

I hope it will help!

I any question please don't hesitate to ask.",MachineLearning
rchzpy,1639057573.0,[D] What do you guys thinks about this AI DREAM app? How do u think was it created?,Tools and resources used for this [app](https://app.wombo.art)?  What will it take to create a text to video generation app?,MachineLearning
rchib3,1639055943.0,[R] Neural Flows: Efficient Alternative to Neural ODEs (NeurIPS 2021),"Want to use neural ODEs but the solver takes too long? Try modeling the ODE solutions directly instead, using neural flows.

Our model takes the initial condition and the time(s) at which we want to obtain the solution. It outputs the solution that corresponds to solving some underlying ODE. This is done in a single call without numerical solvers. To enable this, the model has to satisfy two simple properties: 1) it has to return the initial condition at t=0, 2) it has to return unique solutions. This is easily implemented in code. The speed-up is measured in orders of magnitude and we get better results across time series and density estimation tasks.

Check out the [paper](https://arxiv.org/abs/2110.13040) & [code](https://github.com/mbilos/neural-flows-experiments) for more info.

https://preview.redd.it/tzlwqshpmi481.png?width=950&format=png&auto=webp&s=00d079304528a1afb19ac49c4ecfb618567d0293",MachineLearning
rcgkhi,1639052539.0,[D] Who owns the rights to images produced by an AI?,"Thought about this when using an AI art creator. If you used a text-to-image AI like StarryAI or NeuralBlender, who owns the rights to the image it creates? Do you own it, since you made the prompt, or does the AI's developer own it, since they made the AI? Or, since the AI was trained on pre-existing images, do the owners of the intial images have the rights?",MachineLearning
rcdhkt,1639039184.0,[R] Learning multiple gaits of quadruped robot using hierarchical reinforcement learning,"Hello.

We share our results of learning multiple gaits of quadruped robot using hierarchical reinforcement learning.

We simply parameterized the policy output considering the periodic features of different gaits.

Although currently there are some limitations, we hope the proposed simple method could give insights to other researchers in related fields.

If you are curious of the methods and results in detail, check the paper, slides, and code linked below.

Enjoy!

https://preview.redd.it/qxsk7icgbh481.png?width=960&format=png&auto=webp&s=67a495a828c48358ca28658fe69845e7c372de82

========================================================

Title: Learning multiple gaits of quadruped robot using hierarchical reinforcement learning

Abstract:

There is a growing interest in learning a velocity command tracking controller of quadruped robot using reinforcement learning due to its robustness and scalability. However, a single policy, trained end-to-end, usually shows a single gait regardless of the command velocity. This could be a suboptimal solution considering the existence of optimal gait according to the velocity for quadruped animals. In this work, we propose a hierarchical controller for quadruped robot that could generate multiple gaits (i.e. pace, trot, bound) while tracking velocity command. Our controller is composed of two policies, each working as a central pattern generator and local feedback controller, and trained with hierarchical reinforcement learning. Experiment results show 1) the existence of optimal gait for specific velocity range 2) the efficiency of our hierarchical controller compared to a controller composed of a single policy, which usually shows a single gait. Codes are publicly available.

Paper: [http://arxiv.org/abs/2112.04741](http://arxiv.org/abs/2112.04741)

Slides: [https://docs.google.com/presentation/d/17ZrTDFcFmWwuCZntB9HBuxNkyBjPDHFFjrzHRfF0Kuk/edit?usp=sharing](https://docs.google.com/presentation/d/17ZrTDFcFmWwuCZntB9HBuxNkyBjPDHFFjrzHRfF0Kuk/edit?usp=sharing)

Code: [https://github.com/awesomericky/Multiple-gait-controller-for-quadruped-robot](https://github.com/awesomericky/Multiple-gait-controller-for-quadruped-robot)

Contact: [awesomericky@snu.ac.kr](mailto:awesomericky@snu.ac.kr)",MachineLearning
rc5zzf,1639012452.0,[R] CtrlGen Workshop at NeurIPS 2021 (Controllable Generative Modeling in Language and Vision),"Excited by generation, control, and disentanglement? Come to our CtrlGen controllable generation workshop ([https://ctrlgenworkshop.github.io](https://ctrlgenworkshop.github.io/?fbclid=IwAR2lx-sDgf_snUoI16g79geBeAHJ__6i9Wd6duQQbJRlrg4xI76jDutg9iA)) at NeurIPS next Monday, December 13th! We feature a mix of 7 talks on the latest in controllable generation, a live QA + panel discussion, poster presentations of several interesting works, creative demos of controllable generation systems, and networking opportunities.

This is an effort organized with researchers from Stanford, CMU, Microsoft, Dataminr, and the University of Minnesota. Our invited speakers and panelists include researchers from Facebook, Google, DeepMind, University of Washington, New York University, Stanford, and Tel-Aviv University.",MachineLearning
rc5jb0,1639011036.0,[R] How to optimize your model hyperparameters and build the best synthetic model possible.,We explore the popular open-source package Optuna to demonstrate how you can optimize your model hyperparameters and build the best synthetic model possible. [https://gretel.ai/blog/optuna-your-model-hyperparameters](https://gretel.ai/blog/optuna-your-model-hyperparameters),MachineLearning
rc4yiw,1639009282.0,[P] Imputation methods for missing data,"Maybe it is simple question but I want to see your opinions on this, I have small dataset of about 2000-3000 samples that were third party collected, since Physicians were collecting most of that data they most of time think like “ oh I forgot to ask this question to patient or to do this analysis who cares” problem is that 5-10% of data is missing but in different categories which makes it impossible to remove, so I wanted to check your opinion about imputation methods.",MachineLearning
rc44cl,1639006723.0,[D] Best in style transfer for distributions?,"Hi everyone. I am working on a style transfer project inspired by the recent results of toonification projects based on StyleGAN e.g. [\[1\]](https://github.com/bryandlee/naver-webtoon-faces)[\[2\]](https://github.com/mchong6/SOAT).

Now all of the StyleGAN based papers that I've seen use a reference image for the style transfer, which on the one hand is great in that it's flexible, but on the other hand it's not great because the reference image has too much of an influence on the output image (for example in the toonification case, it could change the color of the person's hair in the toon version to match the reference image even though the toon dataset may have had a more similar hair color within its distrubtion).

I know that a more straightforward approach to solve the style transfer problem between 2 distributions is something like CycleGAN or the many architectures it influenced, however from what I've seen none of those seem to have as good results as the StyleGAN based ones.

Thanks for the replies!",MachineLearning
rc32jz,1639003560.0,[D] Looking for Website Like ArXiv But In Foreign Language,"I would like to do some document classification/topic modeling of non-English documents.    For other NLP work I have downloaded many articles from ArXiv.   Their FAQ says that they accept submissions in multiple languages but I have not found a way to search for them.

Ideally I'm looking for a few hundred documents on 4 or 5 different topics, and I'd like them all to be in Chinese or all in Russian.    Does anyone know of an open/free source for journal articles in other languages?",MachineLearning
rc1dor,1638998688.0,[D] Paper Explained - NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion (Video Walkthrough),"[https://youtu.be/InhMx1h0N40](https://youtu.be/InhMx1h0N40)

NÜWA is a unifying architecture that can ingest text, images, and videos and brings all of them into a quantized latent representation to support a multitude of visual generation tasks, such as text-to-image, text-guided video manipulation, or sketch-to-video. This paper details how the encoders for the different modalities are constructed, and how the latent representation is transformed using their novel 3D nearby self-attention layers. Experiments are shown on 8 different visual generation tasks that the model supports.

&#x200B;

OUTLINE:

0:00 - Intro & Outline

1:20 - Sponsor: ClearML

3:35 - Tasks & Naming

5:10 - The problem with recurrent image generation

7:35 - Creating a shared latent space w/ Vector Quantization

23:20 - Transforming the latent representation

26:25 - Recap: Self- and Cross-Attention

28:50 - 3D Nearby Self-Attention

41:20 - Pre-Training Objective

46:05 - Experimental Results

50:40 - Conclusion & Comments

&#x200B;

Paper: [https://arxiv.org/abs/2111.12417](https://arxiv.org/abs/2111.12417)

Github: [https://github.com/microsoft/NUWA](https://github.com/microsoft/NUWA)",MachineLearning
rc0coi,1638995737.0,[R] Gopher: 280 Billion Parameters Language model by DeepMind,"Blog Post: [https://deepmind.com/blog/article/language-modelling-at-scale](https://deepmind.com/blog/article/language-modelling-at-scale)

Direct Paper link: [https://storage.googleapis.com/deepmind-media/research/language-research/Training%20Gopher.pdf](https://storage.googleapis.com/deepmind-media/research/language-research/Training%20Gopher.pdf)

Seems like a compilation of their findings on scaling LM's a *bit* more than GPT3 + RETRO a retrieval style model",MachineLearning
rc08a4,1638995387.0,[D] What is the latest large dataset used to benchmark feature selection models?,"I submitted a paper to AAAI22 but was rejected, and one of the comments left by the reviewers are that I used traditional small datasets (I used all UCI datasets). I have read many papers in this domain, but none seem to be consistent on what datasets to use and many are still using UCI data. My paper is in the field of dimensionality reduction/feature selection for classification algorithms. What is a good large dataset to use for this field of research?",MachineLearning
rbz69k,1638992399.0,[D] Lazy regularization for WGAN-GP training.,I was reading the paper about StyleGAN 2 ([https://arxiv.org/pdf/1912.04958.pdf](https://arxiv.org/pdf/1912.04958.pdf)).   In there they talk about using lazy R1 regularization which is applied every 16 steps.  Training a WGAN with gradient penalty significantly slows the training.   Has anyone ever tried lazy regularization for the gradient penalty?  Any thoughts?,MachineLearning
rbvz3z,1638983544.0,[D] Edge weight prediction for undirected graphs?,"Hello everyone. I'm pretty new to graph learning as my research is in audio generation, but there's a specific task I'm working on where I think my data may be better represented as an undirected weighted graph.

In the abstract sense, the problem would be: given a set of vertices, each with rich features, and a weighted adjacency matrix (this may be complete, or it may be incomplete — I want to look at both formulations), predict:

1. the edge weight between an unseen test vertex and any vertex in the input graph
2. the edge weight between two unseen test vertices

I'd like to do some reading on this if possible, before trying to hack something together, so I'm wondering if anyone has seen work on anything like this? I'm struggling to find any literature on a similar problem.

Many thanks!",MachineLearning
rbue4h,1638979142.0,"[N] US Gov Launches ML Competition To Predict Snow Water From Remote Sensing Data . $500,000 Prize Pool.","[https://www.drivendata.org/competitions/86/competition-reclamation-snow-water-dev/](https://www.drivendata.org/competitions/86/competition-reclamation-snow-water-dev/)

&#x200B;

>Seasonal mountain snowpack is a critical water resource  throughout the Western U.S. Snowpack acts as a natural reservoir by  storing precipitation throughout the winter months and releasing it as  snowmelt when temperatures rise during the spring and summer. This  meltwater becomes runoff and serves as a primary freshwater source for  major streams, rivers and reservoirs. As a result, snowpack accumulation  on high-elevation mountains significantly influences streamflow as well  as water storage and allocation for millions of people.
>
>Snow water equivalent (SWE)  is the most commonly used measurement in water forecasts because it  combines information on snow depth and density. SWE refers to the amount  of liquid water contained in a snowpack, or the depth of water that  would result if a column of snow was completely melted. Water resource  managers use measurements and estimates of SWE to support a variety of  water management decisions, including managing reservoir storage levels,  setting water allocations, and planning for extreme weather events.
>
>Over the past several decades, ground-based instruments including [snow course and SNOwpack TELemetry (SNOTEL) stations](https://www.wcc.nrcs.usda.gov/snow/)  have been used to monitor snowpacks. While ground measures can provide  accurate SWE estimates, ground stations tend to be spatially limited and  are not easily installed at high elevations. Recently, high resolution  satellite imagery has strengthened snow monitoring systems by providing  data in otherwise inaccessible areas at frequent time intervals.
>
>Given the diverse landscape in the Western U.S. and shifting climate,  new and improved methods are needed to accurately measure SWE at a high  spatiotemporal resolution to inform water management decisions.
>
>**The goal of this challenge is to estimate snow water  equivalent (SWE) at a high spatiotemporal resolution over the Western  U.S. using near real-time data sources.**  Prizes will be awarded based on the accuracy of model predictions and write-ups explaining the solutions as described below.
>
>Getting better SWE estimates for mountain watersheds and headwater  catchments will help to improve runoff and water supply forecasts, which  in turn will help reservoir operators manage limited water supplies.  Improved SWE information will also help water managers respond to  extreme weather events such as floods and droughts.Seasonal mountain snowpack is a [critical water resource](https://www.watercalculator.org/footprint/importance-mountain-snowpack-water/)  throughout the Western U.S. Snowpack acts as a natural reservoir by  storing precipitation throughout the winter months and releasing it as  snowmelt when temperatures rise during the spring and summer. This  meltwater becomes runoff and serves as a primary freshwater source for  major streams, rivers and reservoirs. As a result, snowpack accumulation  on high-elevation mountains significantly influences streamflow as well  as water storage and allocation for millions of people.",MachineLearning
rbudur,1638979120.0,[D] Why is DistilRoberta 15x less used than DistilBert ?,"According to HuggingFace, [distilroberta-base](https://huggingface.co/distilroberta-base) was downloaded 287k times last month VS 3M9 times for [distilbert](https://huggingface.co/distilbert-base-uncased).

WHY ?

From a pure performance perspective, RoBERTa seems better than BERT. In my case, with limited hardware capacity, I am looking for a Distilled model. I was wondering why 15x more people chose DistilBert last month compared to Roberta, as I would have chosen otherwise.

Thanks",MachineLearning
rbu0e7,1638977998.0,[P] A Visual Guide to Prompt Engineering [With GPT language models],"**Hi** r/MachineLearning,

The rise of GPT language models is making a lot of people start to experiment with using them for text generation and general problem solving for language tasks. This is a guide to help get people started in thinking about prompts. Hope you find it useful. All feedback is appreciated!

&#x200B;

[https://docs.cohere.ai/prompt-engineering-wiki/](https://docs.cohere.ai/prompt-engineering-wiki/)",MachineLearning
rbqomm,1638967912.0,[D] How should a training dataset be distributed?,"Sorry if this is a beginner question.
Suppose i'm building a text-to-speech model. I was wondering if my training dataset should be ""realistically"" distributed (i.e. same distribution as the data it will be used on), or should it be uniformly distributed to make sure it performs well on all kind of sentences.
Thanks for your insight.",MachineLearning
rbq46a,1638965927.0,[D] Knowing how features affect the target variables in the neural network,"I am currently working on blood glucose forecasting using LSTMs. Of course, insulin decreases blood glucose and carbohydrates increase blood glucose. Is there any way to feed this in the network? I am using Keras and simply inputting insulin and carbs as an array in the network, but I know for certain how each feature will affect the forecast (i.e. negative and positive correlation). How can I incorporate this is the network?

I am sure this isn't specific to my problem. In most ML prediction/forecasting problems, we would know if a particular feature increases or decreases our target variable. How can we utilise this information?",MachineLearning
rboh2r,1638959580.0,[D] Are transformers overhyped?,"I've been wondering about this for a pretty long time since I've never seen anybody say anything bad about transformers, while to me, they seemed pretty flawed from the moment I've read the paper. I'm in no way an ML expert. I'm only an aspiring PhD student, who's not even specializing in NLP, so if I'm any way wrong I'd really like to hear it.

tl;dr: I believe that transformers are, in the long term, a pretty small contribution to the world of NLP, and may even be damaging due to shifting the focus of the research community in the wrong direction. Why? They don't address the long-term dependency problem.

Before transformers NLP used to be dominated by RNNs and specifically the encoder-decoder architecture. In the case of translation, the encoder would encode the input sentence in a fixed-length vector and the decoder would then decode this vector into an output translated sentence. Now transformers also use encoder-decoder architecture, but there is one big difference. For RNNs, encoding and decoding actually happens at every step of the way. Words (I know it's tokens but I'll call them words) are inserted sequentially into the RNN. For every single word, the encoder RNN had to look at the current encoding vector and the input word and then choose how to update the encoding vector in a meaningful way.

The problem with this approach, which I'll call long-term dependency, arises when the RNN has to look at a very long sequence of words. Humans can easily distill the information that they've read and remember only the important bits, for example, the name of a character that was mentioned 5 pages ago. But RNN models had trouble encoding what happened 5 sentences ago. The research community starts solving this problem with the original attention paper, but then transformers come out.

So out comes the transformer and starts dominating the NLP world. What does the transformer do? It is a huge model that, when encoding simply takes 512 input words (or some other arbitrary number) and looks at all of them simultaneously. And it works wonders. Look, the transformer can remember what happened 5 sentences ago because the previous 5 sentences combined have less than 512 words, hooray. Can it remember what happened 10 sentences ago though? Uh well... no. Can we improve it in some way to solve the long-term dependency problem? Well, we can be smart about which sentences we feed into it, but that means we still have to distill information from a large body of text so... we're back at the beginning.

It's obvious that we have to solve the long-term dependency problem if we ever hope to achieve human-like NLP models, and to me, it seems that transformers do nothing to solve this problem. So why are they dominating the field of NLP research? Maybe the optimal solution will include a combination of both the transformer and some other model for information distillation, but if we still need to solve the long-term dependency problem why are throwing out RNNs so quickly?",MachineLearning
rbn0fh,1638953641.0,[P] Ivis: Dimensionality Reduction In Very Large Datasets Using Siamese Networks,[https://github.com/beringresearch/ivis](https://github.com/beringresearch/ivis),MachineLearning
rbc8s5,1638918689.0,[D] How do you organize/track your reading list?,"I feel like at any given time, I have a million tabs open on like 20 different browser instances, each tab linking an arxiv abstract I want to read. Don't even get me started on my github stars and browser bookmarks.

How do you keep track of stuff you want to read? How do you keep track of things you've read that you want to be able to dig up later? How do you tie in your notes?

NeurIPS has barely started and I already feel like I'm drowning in stuff I want to dive into and not lose immediately afterwards.

Are there some like top-secret tools only the hip grad students know about or something? How the hell do you collect and organize the stuff you want to read and/or recently read?

Please tell me there's a better way",MachineLearning
rbc6qb,1638918528.0,[R] What Transformers Might Know About The Physical World,[https://escholarship.org/uc/item/0kr3t179](https://escholarship.org/uc/item/0kr3t179),MachineLearning
rbbv4g,1638917642.0,[D] CLIP + NeRF explained - Zero-Shot Text-Guided Object Generation with Dream Fields by Ajay Jain 5-minute summary (by Casual GAN Papers),"Do you like generative art? I love it, and it is about to get a whole lot crazier because Ajay Jain and the minds at Google behind the original NeRF have dropped a hot new paper. That is right, we all thought about putting together CLIP and NeRF and they actually did it.

With Dream Fields it is possible to train a view-consistent NeRF for an object without any images, using just a text prompt. Dream Fields leverages the fact that an object (e.g. an apple) should resemble an apple regardless of the direction that you look at it from, which is one of the core features of CLIP. The basic setup is simple - render a randomly-initiated NeRF from a random viewpoint, and score this image against a text prompt, update the NeRF, and repeat until convergence.

As for the juicy details, well continue reading to find out!

Full summary: [https://t.me/casual\_gan/217](https://t.me/casual_gan/217)

Blog post: [https://www.casualganpapers.com/image-editing-stylegan2-encoder-generator-tuning-inversion/DreamFields-explained.html](https://www.casualganpapers.com/image-editing-stylegan2-encoder-generator-tuning-inversion/DreamFields-explained.html)

[Dream Fields - \\""Chair in the shape of \_\_\_\\""](https://i.redd.it/jqd0a8ar87481.gif)

[arxiv](https://arxiv.org/abs/2112.01455) / code - not released

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",MachineLearning
rbbd2u,1638916249.0,[D] Convolution Papers,"I have a question regarding convolution. In the 3BlueBrown video on Neural Networks, it states that researchers initially misunderstood what convolution was doing. They thought that each layer was finding a different edge of the image, and that after multiple layers, convolution would notice the entire image as a compilation of many important edges. However, in this video an interview with Ian Goodfellow, he explained that this is incorrect.   It seems that convolution is rather a continual refinement of an image. Each layer does not look for a different edge, but rather gives us a more precise understanding of what the image is representing.

I know I may have explained this wrong, but I am interested in learning why convolution actually works. Do you know of any papers that explain this concept well? Appreciate any and all help

&#x200B;

&#x200B;

Here is the interview I was referring to: 📷[**PREVIEW**1:08:37Ian Goodfellow: Generative Adversarial Networks (GANs ...YouTube · Lex FridmanApr 18, 2019](https://www.youtube.com/watch?v=Z6rxFNMGdn0)",MachineLearning
rb8x7l,1638909590.0,[P] Breast Cancer Analysis using OpenCV and ML/DL,"I'm working on a project and it is on the early stages, it is a simple Machine Learning application (I guess) but I cannot find anything similar on Kaggle to get references. I have some macro breast cancer images, [you can find an example here](https://i.imgur.com/NVhZUxD.png), and I want to first get the shape of the ""meat"", doing measurements (simple algebra can do that), and then get the ""cancer"" (the white thing) and measure how close it is to the border

For the first part I can use Canny Edge Detection, get the largest contour and that's it, but the last part is driving me crazy, any suggestions? I've thought about using Mask R-CNN but i dont know how viable it is",MachineLearning
rb6jow,1638903428.0,[News] AAAI 2022 moves to a fully online format,"&#x200B;

[I'm not salty, you're salty](https://preview.redd.it/y06bbt0q36481.png?width=2970&format=png&auto=webp&s=c5d64b55b89414f7bf9600a9385e419f86db6f60)",MachineLearning
rb4c2w,1638898993.0,[D] How do you go about creating a data driven model?,"Just trying to get a bead on how people optimize their models and/or create ML models from scratch for a particular problem.

Say you are given a dataset and told to predict some attribute, what's your approach (after initial data visualization/mining)?

From what I've gathered, it seems like googling around for a model that worked previously on a similar model and then modifying it so it works a bit better for their use case seems to be common practice, but I was wondering what would the strategy be if all you did not have those models available?

Similarly, once you have a decently performing model, how do you optimize for the last ~2-3% increase in performance without overfitting?",MachineLearning
rb3wh8,1638898192.0,[P] Can we create a self-funding decentralized and transparent research lab?,"This post is not about research or ML directly (even though it all started with a QD algorithm searching for those very interesting Lenia creatures).

I always wondered if one could create a self-funding research lab. Is it even possible? Even better if this lab would be transparent from the ground up.

I had belief in OpenAI, first because they understood the state of the field at the time and then because it started as a non-profit. But we all know how it evolved.

How to be safe from this path? Maybe all you need is to be radically transparent and decentralized from the ground up, also be optimistic!

We are trying to start such a lab: [https://medium.com/lenia-nft/lenia-dao-purpose-4f2310eba88c](https://medium.com/lenia-nft/lenia-dao-purpose-4f2310eba88c)

I would love criticism, ideas, involvement from the community. Cheers to all of you!",MachineLearning
rb3tfn,1638898030.0,[D] Data Augmentation in NLP,"Hi!

I'd like to ask what you think or is your experience with data augmentation in NLP, particularly in intent based chat bots.
I know it's a common practice in computer vision but as to whether some downsides with synonym replacement resulting in thousands of examples within one intent can occur, I'm dubious 🤔
My common sense tells me, it can help overfitting and as long as I keep dialog balanced, I can't see any downside other than training time.

I'd love to hear all your inputs 🙂",MachineLearning
rb2gi0,1638893350.0,[R] Label-Efficient Semantic Segmentation with Diffusion Models,"TL;DR: Image representations extracted from s.o.t.a. DDPMs contain high-level semantic information that might be useful for your downstream vision tasks.

[Examples of k-means clusters formed by the features extracted from different diffusion steps and UNet decoder blocks.](https://preview.redd.it/9nb84iax65481.png?width=2130&format=png&auto=webp&s=3be34083f1f512b242cab1cb6b348c67b98a410c)

In this work, we show that a simple approach based on DDPM representations can provide strong performance in the context of semantic segmentation when the amount of labeled data is scarce.

arXiv: [https://arxiv.org/pdf/2112.03126.pdf](https://arxiv.org/pdf/2112.03126.pdf)

Code: [https://github.com/yandex-research/ddpm-segmentation](https://github.com/yandex-research/ddpm-segmentation)",MachineLearning
rb2a1f,1638892777.0,[Discussion] What are the areas in computing not affected by machine learning?,"I'm not involved in machine learning science research but it seems to me that it dominates the current mainstream research and development efforts in computing: pattern recognition, computer vision, data compression techniques, audio and video synthesis, to name a few.

Is ML really that invasive or I'm just blind to see further?",MachineLearning
rb29js,1638892731.0,[D] Managing ML Experiments as Code with Git and DVC,"Experiment tracking tools log experiments to a central database and show them in a dashboard. This makes it easy to share them with teammates and compare. However, in an active experimentation phase, you may create hundreds of experiments, so team members may be overwhelmed, and loose the ability to effectively share experiments between team members.

The following article shows how with DVC tool, you can push experiments just like Git branches, giving you flexibility to share experiment you choose: **[Don't Just Track Your ML Experiments, Version Them - DVC](https://dvc.org/blog/ml-experiment-versioning)**

All the experiments you run are stored in your local repo, and only the best experiments are promoted to the central repo (GitHub for example) to share with teammates - distributed experiments are shared with the same people as your code repo.

* Traditional **experiment tracking** tools log ML experiments to a central database and show them in a dashboard. This makes it easy to share them with teammates and compare.
However, in an active experimentation phase, you may create hundreds of experiments, so team members may be overwhelmed, and loose the ability to effectively share experiments between team members.

* With DVC, **experiment versioning** treats experiments as code. It saves all metrics, hyperparameters, and artifact information in text files that can be versioned by Git. You do not need a centralized database or online services. Git becomes a store for experiment meta-information and DVC data versioning backs up the artifacts themselves anywhere.",MachineLearning
rb1rcj,1638891192.0,[R] UC Berkeley’s Sergey Levine Says Combining Self-Supervised and Offline RL Could Enable Algorithms That Understand the World Through Actions,"In the new paper Understanding the World Through Action, UC Berkeley assistant professor in the department of electrical engineering and computer sciences Sergey Levine argues that a general, principled, and powerful framework for utilizing unlabelled data can be derived from reinforcement learning to enable machine learning systems leveraging large datasets to understand the real world.

Here is a quick read: [UC Berkeley’s Sergey Levine Says Combining Self-Supervised and Offline RL Could Enable Algorithms That Understand the World Through Actions.](https://syncedreview.com/2021/12/07/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-160/)

The paper *Understanding the World Through Action* is on [arXiv](https://arxiv.org/abs/2110.12543).",MachineLearning
rb06pu,1638886717.0,Locally deployed model management software [P],"So I need to deploy and manage multiple computer vision models (get the models as input, deploy them as APIs so that it is accessible by another app **on the same computer**) so I don't really need a cloud service like AWS for a few reasons:

* It will get expensive quickly.
* The data I'm using is sensitive (security camera video) so it better if it doesn't leave the computer.
* the computer I'm running this on is powerful enough to run the models.
* faster response time since I won't have to use the internet (everything is in place)

Doing the deployment each time manually can get very hectic very easily so my question is:

Is there a software solution where I can manage my models and get API links for them for local usage or should I try to do it myself.",MachineLearning
razsj2,1638885585.0,[D] Why do people “read” as many papers as possible?,"I’ve got a few colleagues who always claim to be reading papers, but the way they “read” is so damn superficial.

As an example, I had just finished fully reading/comprehending a paper, and I won’t lie, took me a solid couple days to understand everything fully and reading things multiple times.

Meanwhile, in the daily meetings we have I mention the paper and how we should try and use some of their components in our own work, and someone says, “oh ya, I read that in like 15 mins”. So we decide to have an impromptu discussion on it and Jesus Christ, I swear the only thing he read was the abstract and maybe glanced at the network architecture.

I’m sorry this is turning into an rant, it just really grates my nerves when people say they read something and in reality all they did was look at the abstract.

I’m a firm believe that reading, comprehending and fully understand 1 single “key” paper from whatever field you’re studying, is a much better investment of your time than skimming through 100 regurgitated ideas.

Edit: guys just to clarify, I do believe in skimming abstracts and looking for interesting papers. I go through dozens a day myself.  You’d be lost otherwise haha. I take issue though when someone claims they’ve “read” something when all they’ve done is gone through the abstract, and glanced through it.",MachineLearning
razoa5,1638885191.0,[D] Why is Audio so far behind other ML application domains like Image Processing and NLP?,"I 'd like to gain some intuition and actual insight if someone has worked on audio, on why we haven't seen major breakthroughs in audio ML (tracking and localization) as we saw with Image processing and NLP. Ofcourse there is the issue of data and annotating which except for being a laborious task, it's also hard to define e.g how do you label a conversation of multiple individuals on a dinner table, but I would assume that if needed large corporations would attempt such a project.

It seems to me that either people convert data to images and use Image Processing methods or use Signal Processing. Does that mean Signal Processing is more lightweight and outperforms a prospective ML approach?",MachineLearning
raz2bt,1638883180.0,[D] ECCV 2022 - is it happening?,"So far there has been very little information about ECCV 2022 which should -according to schedule- accepting submissions around early March. The only information I can find is an old tweet announcing the venue https://twitter.com/eccvconf/status/1299756196895752193?s=20. No official website. No Important Dates.

Does anyone know what is going on?",MachineLearning
rav3gq,1638868401.0,[R] Hierarchical Topic Modelling Over Time,"Hello Reddit,

I am proud to present you HTMOT for Hierarchical Topic Modelling Over Time. This paper proposes a novel topic model able to extract topic hierarchies while also modelling their temporality.  Modelling time provide more precise topics by separating lexically close but temporally distinct topics while modelling hierarchy provides a more detailed view of the content of a document corpus.

[https://arxiv.org/abs/2112.03104](https://arxiv.org/abs/2112.03104)

The code is easily accessible on GitHub and a working interface provides the ability to navigate through the resulting topic tree with ease: [https://github.com/JudicaelPoumay/HTMOT](https://github.com/JudicaelPoumay/HTMOT)",MachineLearning
raod73,1638844059.0,[D] How to tune learning rate for large models?,"I know there are LR finder from pytorch-lightning ([https://pytorch-lightning.readthedocs.io/en/latest/advanced/lr\_finder.html?highlight=lr\_finder#using-lightning-s-built-in-lr-finder](https://pytorch-lightning.readthedocs.io/en/latest/advanced/lr_finder.html?highlight=lr_finder#using-lightning-s-built-in-lr-finder)) and fastai ([https://docs.fast.ai/callback.schedule.html#LRFinder](https://docs.fast.ai/callback.schedule.html#LRFinder)) and DeepSpeed ([https://www.deepspeed.ai/tutorials/lrrt/](https://www.deepspeed.ai/tutorials/lrrt/)).

But for large models above 2B params, these methods are two slow, are there any other faster methods to try? Thanks.",MachineLearning
raobgu,1638843913.0,[Research] What deep learning algorithms is best for forecasting COVID-19 cases using exogenous variables?,"For our research, we want to forecast the COVID-19 cases using exogenous data. For our exogenous data, we are planning to use time-series data of the cumulative number of vaccinated individuals, change in mobility, and weather. Is it possible to incorporate these variables for univariate forecasting? We are also looking to incorporate the data of population density of each location, but this is not time-series data. Can we still include this? We are currently considering LSTM, but I'm not really sure if it is the best algorithm for this kind of algorithm. Any suggestions would be greatly helpful!",MachineLearning
rak5zr,1638831332.0,[D] What do you do when you're not working?,"It's no secret that this field is 1) immensely rich and 2) moves at a breakneck pace – I spent this morning scrolling through the hundreds of new papers published at NeurIPS this year, which got added to the existing queue of to-be-read arxiv papers, which sits alongside a queue of various bits of mathematics I'd like to understand better, which I'll eventually get to once I make my way through the backlog of my own research and other deadlines.

The point being, it feels like there is a never-ending mountain of things to do, things to read, things to learn in ML. In some sense, this is great and is what makes it (imo) so stimulating and exciting. On the other hand, it's exhausting and often makes it hard to set boundaries between work and the rest of my life. I can feel myself getting more tired, less happy, and less productive. And yet, I'm seemingly at a loss when it comes to thinking of ways to spend my free time. Three years of all-ML-all-the-time seems to have sucked out my ability to imagine other meaningful ways of spending my time. I'd be interested in hearing what kind of outlets other people in the community have and how they fit them in to their every day lives.",MachineLearning
rai9r9,1638826156.0,[D] The reviews and my rebuttals for my rejected ICML and NeurIPS submissions,"To increase transparency about the review process, and in the hopes that it might help others shape their papers/rebuttals, I'm sharing the reviews and my rebuttals for [my ICML submission of `baller2vec`](https://github.com/airalcorn2/baller2vec/issues/3), [my NeurIPS submission of `baller2vec`](https://github.com/airalcorn2/baller2vec/issues/4), and [my NeurIPS submission of `baller2vec++`](https://github.com/airalcorn2/baller2vecplusplus/issues/1), each of which was rejected.

Notably, I received the same, highly negative reviewer on all three submissions (Reviewer #2, Reviewer 7Vsi, and Reviewer GRxC, respectively), which was a bummer (particularly because it's clear that the reviewer's negativity caused another reviewer to lower their score on both the ICML and NeurIPS submissions of `baller2vec`). The reviewer claimed to be ""an expert in both sport and machine learning"" yet repeatedly mischaracterized and misunderstood aspects of the models, going as far as to state, ""The Methods (theory) section is explained in a needlessly complicated and uninformative manner"" in their ICML review. In contrast, the other three ICML reviewers said our ""writing is very clear"" (Reviewer #5), they ""quite enjoyed reading this paper"" (Reviewer #1), and our writing was ""Very clear"" (Reviewer #4). Even ignoring their ratings, I think this raises important questions about what it means to be ""peer reviewed"". Given that there are probably many hundreds (thousands?) of researchers in the field of multi-agent spatiotemporal modeling, why was this same individual given three opportunities to judge my work on behalf of the community? Why are their anonymous opinions considered representative?

Additionally, I found the dataset criticisms from some of the reviewers incredibly frustrating (notably, the above reviewer did not mention anything about additional datasets in their ICML review, but did in their NeurIPS reviews). At least two other multi-agent trajectory modeling papers that were accepted to NeurIPS this year only evaluated their methods on a single, large real-world dataset: (1) ""[GRIN: Generative Relation and Intention Network for Multi-agent Trajectory Prediction](https://openreview.net/forum?id=ephWA7KaWmD)"" evaluated their method on a small simulated dataset (50K training sequences) and a preprocessed NBA dataset (100K training sequences), and (2) ""[Collaborative Uncertainty in Multi-Agent Trajectory Forecasting](https://openreview.net/forum?id=sO4tOk2lg9I)"" evaluated their method on the nuScenes dataset (1,000 scenes) and Argoverse (206K training sequences), so it doesn't seem like this expectation is evenly applied by reviewers.",MachineLearning
rahy9m,1638825347.0,[D] Mahalanobis distance for OOD detection,"I was reading this paper: https://arxiv.org/abs/1807.03888

The core idea is, given a test sample x, and a set of classes C, to compute a score M(x), which is the maximum of the C negative class-specific Mahalanobis distances.

What I struggle to understand is: how to compute a AUROC score using this distance? If the ground truth is 1, for in-distribution, and 0, for out-of-distribution, how to compute a AUROC if M(x) is e.g. - 639.2?",MachineLearning
rae9ui,1638815838.0,[D] Strategies for Handling Large Conference Proceedings?,"Hi r/MachineLearning,


Happy NeurIPS week to you all. This time of year and around other large major conferences in our field have always been exciting times, waiting to see what new work or break-throughs our colleagues and community members might have released. I personally work in a more applied signal processing space so it's always a task to find papers that might provide insight or outline a new method for learning that I might be able to adapt in my own research.

However as I look at this absolutely massive list of accepted papers, even just filtering papers by the titles has become incredibly time consuming and  daunting. So I wanted to pick your brains for insight into how you manage with this massive amount of work to grudge through.


Do you wait for the community to self-select these, do you go for the top papers in terms of the awards, do you just wait for the papers to become relevant enough that they'll pop up when you're doing background reading on the subject.

I guess one of my personal struggles is FOMO on something that is highly relevant, well done, but just slips past the hive-mind of the community",MachineLearning
radg0c,1638813762.0,[D] Using Reduced-Size Databases with AlphaFold-Multimer?,"Has anyone had success running AlphaFold-Multimer with reduced-size databases (e.g., Small BFD)? By default, it doesn't seem like this functionality is supported by DeepMind, so I'd like a second opinion on whether something like this is currently possible.",MachineLearning
racb4q,1638810911.0,[R] Optimal Policies Tend To Seek Power (NeurIPS spotlight),"Summary: There are ""more ways"" for some actions (like staying alive) to be optimal, roughly because staying alive lets the agent do more things. Actions which help agents stay alive and keep their options open, will provably be optimal for most reward functions.

The upshot is that it might be very, very hard to design intelligent real-world AI systems which let us deactivate and correct them. If, statistically, most goals don’t incentivize that behavior, then our goals would conflict with the goals of most smart AI agents.

Excerpts from the [paper](https://arxiv.org/abs/1912.01683):

## Abstract

>Some researchers speculate that intelligent reinforcement learning RL agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers point out that RL agents need not have human-like power-seeking instincts. To clarify this discussion, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.

## Introduction

>Omohundro \[2008\], Bostrom \[2014\], Russell \[2019\] hypothesize that highly intelligent agents tend to seek power in pursuit of their goals. Such power-seeking agents might gain power over humans. Marvin Minsky imagined that an agent tasked with proving the Riemann hypothesis might rationally turn the planet—along with everyone on it—into computational resources \[Russell and Norvig, 2009\]. However, another possibility is that such concerns simply arise from the anthropomorphization of AI systems \[LeCun and Zador, 2019, Various, 2019, Pinker and Russell, 2020, Mitchell, 2021\].
>
>We clarify this discussion by grounding the claim that highly intelligent agents will tend to seek power. In section 4, we identify optimal policies as a reasonable formalization of ""highly intelligent agents."" Optimal policies ""tend to"" take an action when the action is optimal for most reward functions. We expect future work to translate our theory from optimal policies to learned, real-world policies.
>
>Section 5 defines ""power"" as the ability to achieve a wide range of goals. For example, ""money is power,"" and money is instrumentally useful for many goals. Conversely, it's harder to pursue most goals when physically restrained, and so a physically restrained person has little power. An action ""seeks power"" if it leads to states where the agent has higher power.
>
>We make no claims about when large-scale AI power-seeking behavior could become plausible. Instead, we consider the theoretical consequences of optimal action in MDPs. Section 6 shows that power-seeking tendencies arise not from anthropomorphism, but from certain graphical symmetries present in many MDPs. These symmetries automatically occur in many environments where the agent can be shut down or destroyed, yielding broad applicability of our main result (theorem 6.13).

## Conclusion

>Many real-world environments have symmetries which produce power-seeking incentives. In particular, optimal policies tend to seek power when the agent can be shut down or destroyed. Seeking control over the environment will often involve resisting shutdown, and perhaps monopolizing resources.
>
>We caution that many real-world tasks are partially observable and that learned policies are rarely optimal. Our results do not mathematically *prove* that hypothetical superintelligent AI agents will seek power. However, we hope that this work will foster thoughtful, serious, and rigorous discussion of this possibility.

Links:

* [Paper](https://arxiv.org/abs/1912.01683)
* [NeurIPS recorded spotlight presentation](https://neurips.cc/virtual/2021/poster/28400)
* NeurIPS poster session: Tue 7 Dec 8:30 a.m. PST, spot D3 in [Gather Town](https://eventhosts.gather.town/app/sX430NSSjB2SjWQq/1-theory)
* [Series of blog posts](https://www.alignmentforum.org/s/fSMbebQyR4wheRrvk) on this line of work
* [Twitter thread](https://twitter.com/Turn_Trout/status/1467898070792740875)",MachineLearning
rab6lt,1638808035.0,[D] PyTorch Distributed Training Libraries: What are the current options?,"Currently, when I do distributed training, I either use some ""manual"" implementation with  \`torch.distributed\` or just use PyTorch Lightning, which also has some nice bonuses like FP16 training.

Then there's also DeepSpeed, however I'm unsure if DeepSpeed is only beneficial for multi-node training and when my model does not fit into GPU RAM or if DeepSpeed would also bring benefits for ""standard"" data-parallel, multi-GPU but single-node training (where the model would fit into GPU RAM).

Do any of the practitioners here have insights into this? Which other libraries / frameworks am I missing?",MachineLearning
raaipx,1638806342.0,"[R] Integrating Self-Attention and Convolution: Tsinghua, Huawei & BAAI’s ACmix Achieves SOTA Performance on CV Tasks With Minimum Cost","In the new paper On the Integration of Self-Attention and Convolution, a research team from Tsinghua University, Huawei Technologies Ltd. and the Beijing Academy of Artificial Intelligence proposes ACmix, a mixed model that leverages the benefits of both self-attention and convolution for computer vision representation tasks while achieving minimum computational overhead compared to its pure convolution or self-attention counterparts.

Here is a quick read: Integrating Self-Attention and Convolution: [Tsinghua, Huawei & BAAI’s ACmix Achieves SOTA Performance on CV Tasks With Minimum Cost.](https://syncedreview.com/2021/12/06/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-159/)

The code and pretrained models will be released on the project’s [GitHub](https://github.com/Panxuran/ACmix). The paper *On the Integration of Self-Attention and Convolution* is on [arXiv](https://arxiv.org/abs/2111.14556).",MachineLearning
raa9y7,1638805709.0,[P] An arxiv-sanity-like view of NeurIPS 2021 papers,"The image thumbnail of the paper is one of my favorite features of [arxiv-sanity](http://www.arxiv-sanity.com/). It helps to get a quick overview of whether the paper includes nice graphs that summarize the work or convey the intuition of the proposed approach.

I wanted to get a quick overview of top NeurIPS 2021 papers, so I created an arxiv-sanity-like [view](https://tanelp.github.io/neurips2021/) of all the 2334 papers, and ordered them by rating. It includes links to code, tldr section, and other meta-data available in OpenReview.

A link to the project: [https://tanelp.github.io/neurips2021/](https://tanelp.github.io/neurips2021/)

Yeah, some people still go over the publication lists :P",MachineLearning
ra3kwq,1638783795.0,[P] Looking for OCR datasets for benchmark,"Hi everyone,

I  am currently working on a complete benchmark of Cloud OCR engines (GCP,  AWS, Azure, OCR-Space, etc.). To carry out this work, I am looking for  datasets where differences in performance could appear between the  engines.

I already looked on  Kaggle and other public dataset available. Perhaps some of you might  know some good datasets for my project :)

Thanks,

Jeremy",MachineLearning
ra1vjy,1638776922.0,[D] artist seeking to learn more about high-res image synthesis,"hey there /r/machinelearning,

i'm a photographer and artist preparing for a small gallery show in february, and i've been experimenting with VQGAN + Clip and Taming Transformers, as well as image scraping tools like flickr-scraper and the likes, in the hopes to achieve generative images based on famous artworks. i'm hoping the kind folks on this sub can provide me with a little guidance.

here's what I want to do:
produce ai-generated, photo-realistic images from source images of well-known photographs

i've tried creating my own image segmentations to feed to Taming Transformers but it appears that it doesn't handle photographs involving people very well, only landscape images.

is there any way to get somewhat photo-realistic images generated by AI?",MachineLearning
ra1brw,1638774773.0,"[D] What is the gold standard of visualizing train vs validation loss, if you are using cross validation for hyperparameter training?","I have divided my data into six equal parts. Five for training (parameter fitting/cross validation), and the last one as a hold out test set.

Each of the five training sets is used as a validation set, while the model trains on the other four. I am randomly drawing parameters 20 times and therefore training 20 different models, before moving on to the next validation set. This means I am training 100 sets/comparing 100 sets of parameters.

I am wondering if there is anything I should do differently? I am also trying to plot both the training and the validation loss, which is producing 101 graphs (100 for training, one for the final test). Any ideas?

Edit: Never-mind. I couldn't see the forest because of all the trees. I am measuring the loss of all validation runs, and the hyperparameters of the best run are used in the end. So I will just plot the loss curve for the best validation run and for the final run. ",MachineLearning
r9yzub,1638766459.0,"[D] In your opinion, what areas of deep learning are under-explored?","While many questions still remain unanswered, there have been tremendous progress in different areas such the loss landscape, optimization, architectures, etc.

In your opinion, what areas/problems are important but haven't received much attention?

My opinion is that initialization doesn't get the attention it deserves. It seems to me most people just accept the standard Guassian i.i.d initialization, but I think there is a lot of potential to use other initialization schemes.  From my own experience, using the default initialization schemes pytorch offers sometimes leads the neural net to have a bottleneck where information is not propagated forwards or backwards. Usually, after fiddling with the initialization, it works wonderfully.

So, again, what in your opinion deserves attention, but doesn't get it?",MachineLearning
r9u2e2,1638751326.0,Secret Santa StyleGAN [P],"Hi guys,

I created a quick notebook this weekend that uses StyleGAN to create face-morph comic strips.

[https://colab.research.google.com/drive/1PfI\_evHrzCXdPDKm1A5gQDy8xmUTbSrC?usp=sharing](https://colab.research.google.com/drive/1PfI_evHrzCXdPDKm1A5gQDy8xmUTbSrC?usp=sharing)

&#x200B;

https://preview.redd.it/3w2nl2g2jt381.png?width=6144&format=png&auto=webp&s=f0c03946fcb48f2b6a4ea837efae72d055cb7633

As a gag gift, you can then print the comic on a custom mug. This only costs £6, so is well within most Secret Santa budgets.

&#x200B;

https://preview.redd.it/sg96fn2ejt381.png?width=1311&format=png&auto=webp&s=61d0bfb9283e80fdc5f46821e6c8725e13eea060

Let me know if you give the notebook a try, and if you have any artistic suggestions to make the comics better!

A similar description can be found on Github: [https://github.com/Pbatch/StyleGANSecretSanta](https://github.com/Pbatch/StyleGANSecretSanta)

(I am not affiliated with the mug company so this is not for profit lol)",MachineLearning
r9latm,1638727361.0,"[D] Thoughts on Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow by Geron"," I'm in the early portion of my Data Science Masters, but I'm wanting to learn as much as I can so I purchased the book in the title. I've noticed that some of the code as I've progressed through chapters 2 and into 3 don't exactly work as the book expects them to. Is this a common problem throughout the book?

For instance, in chapter 3 where it goes into SDG Classifiers, I kept getting errors for the code used on pages: 86 and 88. I was able to figure out how to correct it, but kind of perplexing to me how the book has these errors.",MachineLearning
r9k1u5,1638723868.0,[R] Using the Singular Value Decomposition for Compact Operators in my latest paper on Dynamic Mode Decompositions,"This is something I have had trouble finding in standard textbooks on Operator Theory and Functional Analysis, including Conway's *A Course in Functional Analysis* and Pedersen's *Analysis NOW*. It's honestly surprising to me that it is really just a few lines past some of the theorems in those texts, but I've only really located discussions in a bunch of PDF's online.

On the other hand, even though it is core to all of *data science*, it also doesn't appear in textbooks there either, and this is probably due to the idea that all real world data is finite dimensional. Again, it almost follows the same argument, save for some infinite dimensional considerations.

The **Singular Value Decomposition for Compact Operators** is a tool I've come to use in my work that overlaps Operator Theory and Data Science. Namely, in the convergence theories concerning **Dynamic Mode Decompositions**. These decompositions rely strongly on a finite rank approximation of an infinite dimensional operator, and my group and I have managed to show the convergence of some of our routines using the SVD for compact operators. One of the papers was just published this past week, and you can find it linked below. (Available freely through this link for a limited time by the publisher.)

In the paper linked below, we introduced a ""scaled"" Liouville operator that was compact over a particular RHKS. What's interesting is that over a compact subspace of the workspace, you can get a ""point wise"" approximation of the *unbounded Liouville operator* by a compact operator. So past a certain threshold, the difference between them is indistinguishable on a particular data set.

So to help my students (and burn it into my memory), I put together this video discussing the SVD for Compact Operators. [https://youtu.be/MeGx6b3OcU8](https://youtu.be/MeGx6b3OcU8)

Here is an older video on our perspective on DMD: [https://youtu.be/xfZG0mhKd0s](https://youtu.be/xfZG0mhKd0s)

We leverage, quite strongly, Liouville operators and Occupation Kernels.

You can find Sheldon Axler and Steve Brunton discuss the regular SVD, and I've done that before on my channel too. I think this is the first video on YouTube extending that discussion to compact operators, though I'm happy to be shown otherwise.

Let me know what you think of the presentation. Cheers!

Paper Link: [https://rdcu.be/cCwfh](https://rdcu.be/cCwfh)",MachineLearning
r9jd2h,1638721961.0,How to Go beyond Data Parallelism and Model Parallelism: Talking from GShard[R],"This article lists papers on GShard, presents background information and inspiration from the papers, and finally evaluates what else can be done to improve GShard from similar work that has been done in OneFlow.

**Article**: [https://oneflow2020.medium.com/how-to-go-beyond-data-parallelism-and-model-parallelism-talking-from-gshard-a45e20c1975d](https://oneflow2020.medium.com/how-to-go-beyond-data-parallelism-and-model-parallelism-talking-from-gshard-a45e20c1975d)

OneFlow Paper:[https://arxiv.org/abs/2110.15032](https://arxiv.org/abs/2110.15032);  Code:[https://github.com/Oneflow-Inc/oneflow/](https://github.com/Oneflow-Inc/oneflow/)

The paper of Gshard contains two main parts of work, one on parallel APIs and one on Mixture of experts. The former part is more interesting and I will only discuss this part. The contribution on parallel APIs is outlined clearly in the abstract of the paper:

GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler.

*Gshard Paper:* [*https://arxiv.org/pdf/2006.16668.pdf*](https://arxiv.org/pdf/2006.16668.pdf)",MachineLearning
r9iqi3,1638720187.0,[Discussion] Will machine learning eventually make it into high school math curriculum,"Eager young people interested in math will someday in the not too distant future begin [clamoring for classes in ML at the high school level.](https://github.com/kjaisingh/high-school-guide-to-machine-learning)

Mathematicians and AI experts are increasingly using [machine learning to solve previously intractable math problems](https://www.sciencealert.com/ai-is-discovering-patterns-in-pure-mathematics-that-have-never-been-seen-before). Getting a foundation in ML at a young age will expedite this trend.",MachineLearning
r9io6e,1638720013.0,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning
r9hnkd,1638716983.0,"[D] Why can methods like ReSuMe, Chronotron and SPAN only train single-layer spiking neural networks?","ReSuMe, Chronotron and SPAN all use STDP-like local learning rules to implement their training algorithm (though they approach the training differently, e.g. SPAN uses gradient descent via spikes transformed into analogue signals through convolution with alpha kernel whereas ReSuMe does not to my understanding). The papers I've read claim that they are not suitable for training multi-layer spiking neural networks and can only be used to train single-layer networks[0][1], but it is not entirely clear to me why and getting an answer has proven difficult. Is it because they use local learning rules and propagating weight changes across multiple layers is impossible?

[0] Ponulak, Filip & Kasiński, Andrzej. (2011). Introduction to spiking neural networks: Information processing, learning and applications. Acta neurobiologiae experimentalis. 71. 409-33.

[1] Kasabov, N. K. (2018). Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence (Springer Series on Bio- and Neurosystems). 1st. Springer Publishing Company, Incorporated. ISBN: 3662577135.",MachineLearning
r9dri0,1638703442.0,[D] SOTA StyleGAN inversion explained - HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing 5-minute digest (by Casual GAN Papers),"It proved to be a surprisingly difficult task to balance the reconstruction quality of images inverted into the latent space of the StyleGAN2 generator and the ability to edit these images afterward. Now Yuval Alaluf, Omer Tov, and the team that originally reported the infamous reconstruction-editability tradeoff in their “Designing Encoders for Editing” paper are back at it again with a new encoder design inspired by the recent PTI paper that sidesteps the tradeoff by finetuning the generator’s weights in a way that places the inverted image into a well-behaved region of the latent space and leaves the editing capability unchanged. HyperStyle is a hyper network that speeds things up by training a single encoder to predict the weight offsets for any input image, replacing the compute-intensive per-image optimization with a single forward pass of the model that takes a second instead of a minute.

How are the authors able to predict the weight offsets for the entire StyleGAN2 generator in such an efficient manner? Let’s find out!

Full summary: [h](https://t.me/casual_gan/205)[https://t.me/casual\_gan/212](https://t.me/casual_gan/212)

Blog post: [https://www.casualganpapers.com/image-editing-stylegan2-encoder-generator-tuning-inversion/HyperStyle-explained.html](https://www.casualganpapers.com/image-editing-stylegan2-encoder-generator-tuning-inversion/HyperStyle-explained.html)

[HyperStyle](https://preview.redd.it/3hzw404dkp381.jpg?width=4812&format=pjpg&auto=webp&s=ba45ee1aa0db48ae5c91b88330742353ae95d4f0)

[arxiv](https://arxiv.org/abs/2111.15666) / [code](https://github.com/yuval-alaluf/hyperstyle) / [demo](https://colab.research.google.com/github/yuval-alaluf/hyperstyle/blob/master/notebooks/inference_playground.ipynb)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",MachineLearning
r98ajp,1638681237.0,[D] Is there a good pixel image upscaler?,"I'm trying to upscale this image from 256 to 512px


[256px](https://preview.redd.it/uge8sntqqn381.png?width=256&format=png&auto=webp&s=288ba6b58feb5425c7df29a3632dd722ddf8f583)

The issue Is that the pixels lose their ""squareness"" and they become obtuse/acute and they deform. I used SRCNN for this. (or whatever [https://icons8.com/upscaler](https://icons8.com/upscaler) is using - Not Real ESRGAN)


https://preview.redd.it/znt0hai0rn381.png?width=512&format=png&auto=webp&s=04284fea7aff45267dc4d065690368f0fc450c05


Is there a good ML Upscaler that works well for pixel images?",MachineLearning
r980d7,1638680293.0,[D] Are there a lot of projects in Machine Learning which do not allow international students to participate? I've heard projects that are funded by NSF/industry are usually fine but DoD or DoE related projects tend to require a security clearance.,"

I've heard more than 70% of the projects in ML/CV/NLP are funded by NSF/industry and less than 30% of them are DoD or DoE related and thus international students can participate in most of the ML-related projects. Is this true? It seems international students can participate in NSF-funded projects but usually not in DoD or DoE funded projects.

[https://www.nsf.gov/pubs/gpg/faqs.pdf](https://www.nsf.gov/pubs/gpg/faqs.pdf)

If an individual is not a U.S. citizen, can the individual apply for a grant? Can a permanent resident serve as a Principal Investigator at a U.S. institution?

Except for NSF fellowships, which by statute can be made only to citizens, nationals, or lawfully admitted permanent resident aliens of the United States, there generally are no nationality restrictions in any NSF program. A proposing institution in the US may designate as Principal Investigator anyone it believes to be capable of fulfilling the role.",MachineLearning
r93h2k,1638665062.0,[D] Dealing with missing values for anomaly detection with numerical data,"I   have a small numeric dataset with around 400 attributes (all numeric)   and 200 instances. The target is a categorical attribute with 4   different categories indicating the state: one class represents normal   condition and the other 3 represent anomaly types. The instances   representing the normal state (no anomaly) have no missing values.   However, the instances representing anomalies have missing values for   most of the attributes. In addition, the names of the attributes for   which there are missing values correlate with the anomaly type, e.g. for   all instances of anomaly class 2, attributes from 10 to 400 are  missing  while for all instances of anomaly class 3, attributes from 300  to 400  are missing.

What would be the best way to deal with the missing values in this case for the supervised and the unsupervised case?

I'm not sure which methods I'm going to use, but I would need to cover both supervised and unsupervised cases.",MachineLearning
r90f3v,1638655667.0,[D] Embedding consecutive video frames close to each other in latent space,"What if we embed consecutive frames of a video close to each other? It would mean that if we walk the latent space we should get a video (instead of visually similar frames as in VAE).

I think it may prove useful for example in world models where it will make it easier to predict the next state thanks to the fact that it will be close in the latent space.

What do you think about this idea? Did somebody already try? Any papers on this topic?",MachineLearning
r8yq8n,1638650813.0,"[D] How to assert whether the complex, high dimensional data have a predictive power or not.","I am assigned with a binary classification of multi-channel time-series signal data. But the data collection process is expensive so they give me a very small sample of data (15 samples per class) and check whether the data is good enough to make a prediction (they want me to check the prediction power of the data, so they can retune the sensor hoping for more different signal or just give up if the task is too hard and the sensor is not good enough to capture the difference).

The problem is, I don't know how to assert that. If it's an RGB image classification task, it's easy since we can just look at the image and see the features. But this one is complicated and hard to understand the feature, I did some simple plot comparison between channel of each class, it's still look similar (but that's just to my eyes, with enough data it may be able to tell the different)

I imagine doing some kind of EDA but for the signal processing, to extract some kind of statistics, but not sure if I can extract any useful information to be confident that it have a predictive power. (please suggest me if you have some ideas)

Anyway, how do you guys usually dealing with this? not limiting to signal but complicated dataset in general. Is it my job to assert that or should I just tell them that it's not easy to know unless we have enough data (which in this case, will be very costly if I gonna use deep learning, I probably try to do traditional signal feature extraction first like fourier transform, and feed the extracted data in simpler model rather than let the model learn the feature itself)",MachineLearning
r8v4fq,1638640705.0,[Project] DALL-3 - generate better images with fewer tokens through clip guided diffusion,"link to images and code: https://github.com/Jack000/DALLE-pytorch/

link to diffusion model: https://github.com/Jack000/guided-diffusion

colab: https://colab.research.google.com/drive/1SlcC0u-tzCrHYL7yYP_DkKu8x0ehXBv0?usp=sharing

This is a project I created that combines transformer image generation with clip guided diffusion. It's named after the 3 projects it's based on (DALLE-pytorch, clip guided diffusion, and VQGAN). BTW a big thanks to lucidrains, rivershavewings, OpenAI, and the VQGAN team for their work.

If in general DDPM > GAN > VAE, why do transformer image generators all use VQVAE to decode images? Wouldn't it be better to use a diffusion model? I was wondering about this and started experimenting with different ways to decode vector-quantized embeddings with a diffusion model - [see discussion here](https://github.com/lucidrains/DALLE-pytorch/discussions/375) After a lot of trial and error I got something that works pretty well.

The resulting diffusion model basically absorbs one scaling factor, so you can get similar image quality with 16x16 tokens + diffusion compared to 32x32 tokens + VQGAN

I then trained a (relatively small) DALLE-pytorch model with these embeddings and put the whole thing together in the github/colab above. In terms of generalization ability (eg. avocado armchairs) there's no substitute for a large-scale transformer, but for common object categories (people, food etc) the small transformer + diffusion works surprisingly well.

So this is kind of a proof of concept. The conditional diffusion approach should provide a flat boost in quality to any transformer model, including cogview, ru-dalle, vq-diffusion and OpenAI's original DALL-E

The parameters for this architecture is mostly dictated by the pretrained models I'm leveraging. There are a lot of potential optimizations that I haven't explored for training from scratch (such as removing the final unet downsampling layer) but I'll leave it to someone with a gpu cluster..",MachineLearning
r8tsv6,1638636946.0,[Discussion] Why are Einstein Sum Notations not popular in ML? They changed my life.,"I recently discovered \`torch.einsum\` and now I am mad at every friend, mentor, acquaintance for not telling me about it.

They are just way more intuitive and can handle most operations that I would want to do with tensors so elegantly. No more of having to remember which way is axis=0, No more of having to remember which way is dim=1 and no more of remembering so many numpy and torch functions only to misuse np.unsqueeze and torch.expand\_dims.

It takes only 30 mins or so to learn the notation and become somewhat proficient but then you are sorted for life.

What are the arguments for and against using einstein notations for everything? Will I be writing code which others find difficult to understand? Kindly pitch in your thoughts and theories on why are they so seldom used when they are one-size-fit-all.",MachineLearning
r8t0g2,1638634737.0,[D] Multi-Input -> Multi-Output problem,"Hello,

recently I have been wondering how to solve a machine learning problem and I would like to discuss ideas to solve it:

**Objective:**

Predict a list of products (e.g. grocery) given various of input conditions (e.g. size, type and location of the store, performance of the store, etc.). The output (=list of the predicted products) will meet input criteria and might consist of multiple products (the list can consist even of >100 records).

**Dataset:**

\- explanatory variables: can be around \~1,000 (due to One-Hot Encoding of the categorical variables);

\- response variables: currently I have 1 column with \~1,000 distinct classes (i.e. types of the products in the text form) -> I was thinking perform One-Hot Encoding, however the drawback is that I will have \~1,000 response variables.

**Potential solution:**

\- Train CNN (there is no relationship between products in the response variables) taking into consideration One-Hot Encoding over response variable and Multi-Class Classifier.",MachineLearning
r8sfwj,1638633111.0,[D] Impact of initialization on GANs,"Hi,

I was wondering how important is the initialization for GANs. I heard that normal init leads to better convergence, but I cannot find any paper that support this claim.

Do you guys have any resources on the topic? Some practical advice?

&#x200B;

Thanks!",MachineLearning
r8ob3o,1638619429.0,[P] Visualize millions of datapoints using plotly with 1 line of code,"We created a (python) plotly Figure wrapper that adds adaptive resampling to your plotly Figure.

[Github repo](https://github.com/predict-idlab/plotly-resampler)

This project aims to enable visualizing large sequential data (in this example we visualize 110M + data points).

https://i.redd.it/jc1d05p0ni381.gif

I would love to hear your feedback on this!",MachineLearning
r8mlwd,1638612426.0,How to Implement an Efficient Softmax CUDA kernel? [R],"All ops computed in deep learning frameworks are translated into CUDA kernel functions on the GPU, and Softmax operations are no exception. Softmax is a widely used op in most networks, and the efficiency of its CUDA kernel implementation can affect the final training speed of many networks. So how can an efficient Softmax CUDA Kernel be implemented?

Article : [https://oneflow2020.medium.com/how-to-implement-an-efficient-softmax-cuda-kernel-oneflow-performance-optimization-sharing-405ad56e9031](https://oneflow2020.medium.com/how-to-implement-an-efficient-softmax-cuda-kernel-oneflow-performance-optimization-sharing-405ad56e9031)

Code: [https://github.com/Oneflow-Inc/oneflow](https://github.com/Oneflow-Inc/oneflow)

This article will introduce techniques for optimizing the Softmax CUDA Kernel in OneFlow and experimentally compare it with the Softmax operation in cuDNN. The results show that OneFlow’s deeply optimized Softmax can utilize the memory bandwidth close to the theoretical upper limit, much higher than the cuDNN implementation.",MachineLearning
r8lljf,1638608069.0,[P] I implemented Transformer in Transformer,"Here is my implementation of the recent Transformer in Transformer paper which uses pixel level attention paired with patch level attention.

[https://github.com/Rishit-dagli/Transformer-in-Transformer](https://github.com/Rishit-dagli/Transformer-in-Transformer)",MachineLearning
r8kfpf,1638603203.0,[P] Dynamic batching for GPT-J API,"Hi,

I created a small FastAPI back-end for GPT-J (on HuggingFace) with dynamic batching. Dynamic batching has the potential to increase throughput (at the expense of the latency because it pads all the sequences in the batch to the length of the longest one).

In this project, I determined the maximum amount of tokens (batch size x sequence length) that could fit in the memory by tuning the batch size while keeping the sequence length fixed until I would get the ""Out of memory"" errors. If anyone knows of a more reliable way to determine the optimal batch size, I would appreciate any pointers :)

The purpose of this project is to show how to implement dynamic batching. For real production environments, you should probably use C++ or Go instead of Python + FastAPI.

Github link: [https://github.com/TensorBox/gpt-j-api-huggingface](https://github.com/TensorBox/gpt-j-api-huggingface)

Blog post: [https://www.tensorbox.ai/dynamic-batching-for-gpt-j-api](https://www.tensorbox.ai/dynamic-batching-for-gpt-j-api)",MachineLearning
r8ge6o,1638588332.0,[D] (Paper Overview) NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,"**Video**

[**https://youtu.be/C9CTnZJ9ZE0**](https://youtu.be/C9CTnZJ9ZE0)

**Paper**

[**https://arxiv.org/abs/2111.12417**](https://arxiv.org/abs/2111.12417)

**Code**

[**https://github.com/microsoft/NUWA**](https://github.com/microsoft/NUWA)

**Abstract**

This paper presents a unified multimodal pre-trained model called NÜWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate NÜWA on 8 downstream tasks. Compared to several strong baselines, NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks.",MachineLearning
r8gbid,1638588067.0,[D] ICLR 2022 possible competing bias review,"My paper got 6663. The reviewer who gave 3 kept comparing our paper to a concurrent work posted on arXiv only a few days before the ICLR deadline, and asked us to cite, acknowledge, explicitly compare with this concurrent work in our paper.

Is it common? How likely is this reviewer the author of that competing concurrent work? This reviewer also delivered a lot of deliberate attacks during the reviewing process. We were really frustrated. What can we do under this situation?

\----- Update ------

PC responded to that review!!!

https://preview.redd.it/akdc314hvh381.jpg?width=1786&format=pjpg&auto=webp&s=f6bab4f2692cd69e33a7595024d6b36f99eb91bf",MachineLearning
r8ce7f,1638575535.0,[Discussion] The most painful thing about machine learning,"Hi all, I am curious, how do y'all approach debugging your models?
I am a software engineer and I find that with all other code its usually easier to test out and spot bugs quicker, but for ML its always a very long process if the problem is actually in the code and not in the datasets.

Are there any tools y'all using, and if so what are pros and cons about them compared to doing it manually?",MachineLearning
r89r8r,1638567285.0,[D] Are neurips authors required to purchase a ticket to register?,Question in title. First year accepted -- should I be purchasing a ticket the regular way?,MachineLearning
r88p0z,1638564333.0,"[D] K-means producing very different centroids, even after initialization with canopies","Doing some Big Data coursework for my DS Masters and I am pretty confused by what I'm seeing. I'm running k-means clustering on Mahout and understand you can generally get different final centroids in k-means because it initializes them randomly... But I've initialized clusters using canopy clustering and even after that am seeing wide variance in the final density values. Here's one example:

&#x200B;

|Run|Inter-cluster density|Intra-cluster density|
|:-|:-|:-|
|1|0.430836339|0.590516787|
|2|0.618821774170647|0.581433137|
|3|0.4960634|0.567159394|

I've run this for different k values, distance metrics and max iteration values. The behavior is similar.

This is problematic because the purpose of this project is to compare performance across different  parameter values. With such divergent results within the same parameter set, I'm forced to run each set of parameters multiple times and take an average, which is drastically increasing processing time and computation cost.

Is this expected behavior? Did I misunderstand something?",MachineLearning
r84bxk,1638552563.0,[D] Shape Generation Algorithm,"Hello,

I am not from the field, I was looking for an algorithm for shape generation, my field is mechanics and I want to see how different shapes in structure would affect the stress, I envision instead of intuitive design, how an AI can come up with interesting shapes. I dont know if that already exist, I would appreciate your kind help.

Thanks",MachineLearning
r81v4u,1638545997.0,[R] Supervised training of spiking neural networks for robust deployment on mixed-signal neuromorphic processors,"Spiking neural networks running on mixed-signal devices promise highly energy efficient edge-ML inference. But deployment is an issue — mismatch between devices breaks down network performance.

Because mixed-signal devices rely on sub-threshold analog signals, small differences between transistors lead to large changes in neuron and network behaviour between chips.

You can train or calibrate each and every chip, but that’s hugely expensive at scale.

We used a class of spiking networks called Efficient Balanced Nets, from the labs of [Sophie Denève](https://lnc2.dec.ens.fr/en/member/628/sophie-deneve) and [Christian Machens](https://machenslab.org), which can correct for errors in network encoding.

We built a supervised training method around this architecture, by copying the network dynamics from a trained teacher ANN. By doing so we can learn arbitrary tasks in robust SNNs.

We compared our approach against other methods for training SNNs — liquid state machines, spiking-FORCE training and BPTT — and showed that our approach was the most robust against parameter mismatch.

Preprint: [arXiv](https://arxiv.org/abs/2102.06408)

Open access: [Sci Rep](https://rdcu.be/cCyCQ)

Code: [GitHub](https://github.com/synsense/Robust-Classification-EBN)",MachineLearning
r8007v,1638540810.0,[D] How do you improve vector search results?,"So I’ve been using USE, MSMarco models from SBert and other models but apart from just the simple vector embeddings themselves, how do people improve the search results?

Bit of context: been using an exact nearest neighbour solution but it seems like when I tried it across a few queries, results seem suboptimal. Curious to know how others are improving it?",MachineLearning
r7xih4,1638532445.0,[D] How to Productionize the Results of my Data Analysis,"I have a project that is supposed to do some prediction on the house prices. As usual, I started with doing Exploratory Data Analysis (EDA) in Zeppelin. I have reached a point where the data is enriched / transformed enough that I would now like to add this to a pipeline, use CI / CD and so on.

The source data is going to be exactly the same and will arrive in batches on a weekly basis. What is the deal here? I now have to rewrite the stuff that I did in my Zeppelin notebook as a Spark application, add unit tests and run it in a CI / CD pipeline? Is this the correct approach?

Assuming that I will receive new data every week, should I keep training my model with this new data every week (which kind of makes sense)? But what about the EDA part? Isn't this double work? I mean doing EDA and then taking that code from the Notebook into an application?

Any ideas on how this is done professionally?",MachineLearning
r7xbw1,1638531752.0,[D] Optimize gradient descent for small known problem,"Hello, I hope I'm posting in the correct community, because my problem isn't part of machine learning, but I figured you'd be the best help when it comes to gradient descent.

A part of a hobby project of mine is photometric stereo (take images under different lighting conditions to calculate the shape of an object), so I need to minimize a cost function depending on 64 measured values with variables to optimize being a direction vector and a scalar value being the albedo luminance of the object (but hundreds of millions of times within a reasonable time). I didn't find any fitting papers addressing what I need exactly (typical photometric stereo with the added Fresnel equation for S polarized light and robustness to shadows), so I developed my own algorithm, this works reasonably well, but all optimizations I found have been more or less trying different stuff and seeing what works. I have already added momentum to the direction vector part and can directly solve the luminance in one iteration. Hyperparameter optimization is also on the list.

So to my question: how do I properly optimize a gradient descent algorithm that solves a small known problem and should converge with as little steps as possible? Or at least learn how to do it, so I'd also appreciate any links to research papers, lecture slides,...",MachineLearning
r7vue4,1638525878.0,[D] Where do you guys read your papers?,Looking to read the latest research on ML and don't know where to look.,MachineLearning
r7vrf6,1638525561.0,[Discussion] Dot Product vs Addition followed by Transformation for Attention Networks?,"Consider that we have a set of vectors $v_i \quad i \in {1..k}$ and a set of queries $q_j \quad j \in {1..q}$. There are two ways of finding attention $\alpha_{ij}$ for $Value(v_i)$:
1. Addition of keys, and queries after bringing to appropriate dimension and then linear projection to scalar after a non-linear activation.
$$ e_{ji} = V^T_{attn} \tanh(U_{attn}Key(v_i) + W_{attn} q_j) $$
$$ \alpha_{ji} = softmax(e_{ji})$$

2. Dot product of keys and queries after after linear transform so that they are of the same shape. This gives one scalar value for each pair $(i, j)$ because of which we can simply take softmax along the dimension of values such that $\sum_{i}^k \alpha_{ji} = 1$ for each query $q_j$.

Which method of these is more appropriate for which usecase in general?

Specifically, I want a set to fixed length vector conversion where the number of elements in the set is variable.
Thanks a lot!",MachineLearning
r7t0nu,1638514589.0,[P] Generate and Read Summary of any research paper on Twitter using RAx bot,"[@rax\_bot](https://twitter.com/rax_bot) : a Twitter bot to help you read research papers more effectively. You can trigger bot by to replying to a research paper tweet that you want to summarize and mention with the ""summarize"" keyword and the bot will send you a summary link  back on Twitter.

Bot also follows some accounts which share research papers regularly and automatically reply to them with a summary link.


Examples:

* [https://twitter.com/immortal\_333/status/1466034298843123713](https://twitter.com/immortal_333/status/1466034298843123713)
* [https://twitter.com/rax\_bot/status/1466590351435591683](https://twitter.com/rax_bot/status/1466590351435591683)",MachineLearning
r7otcv,1638500931.0,[D] Methods for measuring label consistency within object detection?,I have a relatively small custom dataset for multi-class object detection. I can get a maximum of **0.48** mAP. My data is labeled by one person who practices the field in which my dataset is related (xrays). But I would want to measure the label consistency and correctness of the annotations. Are there methods to do this?,MachineLearning
r7lc1e,1638490582.0,[D] Unsupervised Outlier Detection - Advise Requested,"Hi all ... I am working on a strategy to detect outliers (mostly multivariate data) using unsupervised methods. Currently I am using DBSCAN/OPTICS in one group, KMEANS + finding points that are 3+ STD from the mean of each group (should be similar to centroid +/- 3 standard deviations) for a second group, and lastly Isolation Forest and COPOD for a third group. Some of the output from each group could overlap with other groups, but not all since each method finds different outliers in part of the spectrum.

Each model is executed with point in time data in order to find outliers for that moment with respect to the values present at that time, and not trained in a first step and then the model applied to the incoming data (since I don't have regular / outlier values) and want to consider that point in time with incoming values regardless of what might have happened in the past, since values might be affected (in time) by a number of factors.

Is this a sensible approach? Would you suggest something different? Would you add any method (either in parallel to the ones I mentioned or at the end, like voting or anything else)?

Thanks in advance.",MachineLearning
r7kj61,1638488264.0,[D] Neural noise is one of the cool. Anyone know what causes it?," I've been messing around with style transfers and a lot of them, especially ones based on illustrations, have this specific type of noise.

&#x200B;

[ Good example of the noise](https://preview.redd.it/cmtk89z0t7381.png?width=213&format=png&auto=webp&s=a60fff825adae879f4a2b1b40e858fd4dcec44fb)

It's splotchy RGB noise that varies a lot in size.

&#x200B;

[ A gif where you can see it developing](https://i.redd.it/6loe4l35t7381.gif)

[ And a still where you can see the noise really well.](https://preview.redd.it/rpn0nj35t7381.jpg?width=300&format=pjpg&auto=webp&s=1f8f684f2488214daf4e04e1bc44c6a24e1420d1)",MachineLearning
r7gt1m,1638477763.0,[P] Data Platform Generator - Data Solution Blueprints,"Hi everyone,

We took inspiration from [Matt Turck’s Data & AI landscape](https://mattturck.com/data2021/) and [Scott Brinker's Martech landscape](https://chiefmartec.com/2020/04/marketing-technology-landscape-2020-martech-5000/) to generate visual data solution blueprints to inspire you in the design process of your data platform and help you to find alternative combinations of data technologies.

Our idea was simple:

* Let's take some technologies from these landscapes and generate simple visual data solution blueprints to inspire other data aficionados in the design process of a data platform.
* Also, expose them to different technologies that exist on the market which could be good alternatives to already better-known technologies
* Create a generator to display one picture at a time -  we have millions of combinations. (Can you discover all of them?)
* We used for the first time No-Code applications like [Bubble](https://bubble.io/) and [Airtable](https://www.airtable.com/)

[We launched this page today](https://dataplatformgenerator.com/). This is a concept site we call ""**Picture Data Solution**"" for all the data aficionados out there.

Have fun, be inspired, and we hope you enjoy it.

Don't forget to share it with your friends if you like it!

We also, appreciate your brutal and honest feedback.",MachineLearning
r7gb1g,1638476374.0,[D] AAAI2022 Who has the final say if a paper gets accepted or rejected?,"A paper that we have submitted to the AAAI received 1 weak accept, 1 reject and 2 accepts, and the meta-reviewer recommended a weak accept. However, the paper was rejected. Is this usual?

Who has the final say if a paper gets accepted or rejected?",MachineLearning
r7f3h3,1638473111.0,[D] AAAI Rejected Paper Despite Accept Reviewer and Metareview Scores?,"We submitted a paper to AAAI and obtained all accept and weak accept scores with the metareview  also positive and recommending acceptance of the paper. Despite this, our paper was rejected without any reason.

We contacted the organizers and obtained a default response:

Borderline and weak accept scores typically do not directly translate to an accept decision. Unfortunately, there was a great deal of inter-reviewer variability across papers which required some calibration - the negative remarks associated with some borderline rejects by some reviewers were comparable to those with   accepts by others. Some meta reviews were less informative than others or failed to capture the totality of reviewer comments. The AC often weighed in on the SPC recommendations, as did the Associate Chairs.  There were many cases where the rebuttal included additional details and experiments that were not included in the paper. But unfortunately, given the challenges of conference review, there is no option of “accept subject to revision.” Given the scale of AAAI, and the timeline involved, it is not possible to reconsider decisions. We hope the reviews are useful to you in revising the paper for submission to another top-tier conference in the near future. 

Unfortunately two of our scores are not weak accepts but rather full accepts with the other two scores scores weak accepts.... What do you do in such a situtation? I've submit/published papers at NeurIPS/ICML/ICLR in the past and have never had such a terrible experience. Is AAAI even a top-tier conference? Feels extremely shoddily organized.",MachineLearning
r7cvi6,1638467147.0,[D] How important are publications in research scientist interviews?,"Sorry if this question is rather dumb but I just want to have hear some first-hand account on how important are publications at top venues (i.e. ICLR/NeurIPS/ICML & respective top conferences in CV/NLP) in research scientist (including internship) interviews at industrial labs? How important are these compared to, for e.g., leetcode, behaviourial qns & so on?

I understand the first & foremost thing is probably whether there is a team fit. Let's assume there exists a team doing things roughly in the same area of the interviewee.

Any comments are highly appreciated! Thanks.",MachineLearning
r7brz3,1638464146.0,[R] Time-Crystalline Study Published in Nature Journal Observes a New Phase of Matter in a Quantum Processor,"A team from Google Research, Stanford University, University of Massachusetts, University of California, Columbia University, Princeton University, Max Planck Institute for the Physics of Complex Systems and University of Oxford uses a quantum processor to observe a time crystal, a new phase of matter which could be one of the most significant physical discoveries in decades.

Here is a quick read: [Time-Crystalline Study Published in Nature Journal Observes a New Phase of Matter in a Quantum Processor.](https://syncedreview.com/2021/12/02/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-157/)

The paper *Time-Crystalline Eigenstate Order on a Quantum Processor* is on the [*Nature* website](https://www.nature.com/articles/s41586-021-04257-w).",MachineLearning
r7b0mv,1638462031.0,"[PROJECT] Snowball Fight, a multi-agent competitive environment for ML-Agents","Hello,

I'm Thomas Simonini from Hugging Face.

We just published Snowball Fight, a deep reinforcement learning environment. Made with Unity ML-Agents.

You can play the game (and try to beat our agent) here: [https://huggingface.co/spaces/ThomasSimonini/SnowballFight](https://huggingface.co/spaces/ThomasSimonini/SnowballFight).

Or if you prefer to train it from scratch, you can download the environment here: [https://huggingface.co/ThomasSimonini/ML-Agents-SnowballFight-1vs1](https://huggingface.co/ThomasSimonini/ML-Agents-SnowballFight-1vs1)

https://i.redd.it/vpg7rw98n5381.gif

This is our **first custom environment with Unity ML-Agents** that is publicly available and I'm working on building an ecosystem on Hugging Face for Deep Reinforcement Learning researchers and enthusiasts that uses ML-Agents.

I would **love to hear you feedback about the demo and the project,**",MachineLearning
r79oqv,1638458293.0,[D] AAAI manipulated reviewer scores without reviewer permissions?,"A Professor makes a serious accusation of review manipulation:

https://twitter.com/gong_cheng/status/1466016587790438406?s=20
> Surprisingly,
@RealAAAI
 chairs updated my review without my permission. I never experienced this before, and now I have to rethink whether I will submit to or review for this conference again.

Anyone with more info shed some light on this.",MachineLearning
r79a3h,1638457147.0,[N] Great R&D content (with code!) on Computer Vision News of December 2021.,"Have a peek at Computer Vision News of December 2021.

Many articles about AI, Deep Learning, Computer Vision and more...

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2021December/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2021-december-pdf/)

Dilbert on page 2. Free subscription on page 52.

Enjoy!

https://preview.redd.it/xc1fu86o85381.jpg?width=794&format=pjpg&auto=webp&s=fbb930171895dada7638e171478e8e7afca550d0",MachineLearning
r78jko,1638454983.0,[P] Platforms for Python Off-the-shelf for Genetic Algorithms and ACO,"Hello,

I am somewhat new to Machine Intelligence and it's subsideries, so apologies if this is a silly question.

I am currently tasked with ""(finding and) implementing Python code using 'off-the-shelf.'"". I've done some research, but nothing has jumped out at me, nor very applicable to my task.

Wondering if anyone knows the best sites / platforms for finding things like this, as I have been unlucky thus far. Also worth noting, I'm required to reference anything I find, so the more in-depth the better.

Thank you :)",MachineLearning
r78ctn,1638454414.0,[Project] Predicting group behavior based on its members' behavior," Hi everyone,

I've been asked to do a small project and I'm not completely sure how to tackle it.

Some details: I have a dataset that describes the behavior of a few dozen groups. Each group comprises a number of individuals. Each group has a score that indicates whether the group is going to become more aggressive or less aggressive (this is based on a model that was not created by me).

I've been asked to find certain conditions that lead to groups becoming more aggressive: For example, if half the group's members do a certain activity, it means there's a 75% chance the group will become more aggressive.

There are two options I'm considering:

* I can look at statistics of groups that became aggressive: Say, 40% of the members of groups that became aggressive did activity 1, therefore say ""if 40% of a group does activity 1, the group is likely to become more aggressive"". I suspect this is what I'm expected to do but it confuses causation and correlation; the fact that X% of a group did something and became aggressive, does not mean that groups that do so are likely to become aggressive. Plus, in this case, not sure where I will get the probability from.
* I can potentially create a large number of models that proactively try to predict all the groups' behavior based on certain conditions:

if 25% of a group do activity 1, is the group likely to become aggressive?

if 50% of a group do activity 1, is the group likely to become aggressive?

if 75% of a group do activity 1, is the group likely to become aggressive?

if 25% of a group do activity 2, is the group likely to become aggressive?

if 50% of a group do activity 2, is the group likely to become aggressive?

if 75% of a group do activity 2, is the group likely to become aggressive?

and so on.

However, there are too many possible permutations so I don't think this is the right approach.

So I'm unsure how to tackle this. I'd be very grateful for any thoughts and ideas.",MachineLearning
r76sfb,1638449485.0,[Discussion] Applied Tracks of ML Conferences,"In spite of having good set of reviews in AAAI, my paper was rejected citing that it was more appropriate for Applied Tracks of ML Conferences.

Even though I don't fully agree with my reviewers since there were considerable novel approaches in RL and it is extremely heart breaking to see a year worth of work get rejected based on one opinion. It is time to find this child of mine a new home.

Kindy suggest places where I can send this paper. What all reputed AI conferences are coming up next which have a science track. My work involves, graph networks and RL so something along those lines is also fine.

Thanks a lot for reading. And to all those whose paper got rejected, don't worry we will find a way out.",MachineLearning
r76igz,1638448497.0,[Discussion] (Rant) Most of us just pretend to understand Transformers,"I see a lot of people using the concept of Attention without really knowing what's going on inside the architecture and *why* it works rather than the *how*. Others just put up the picture of attention intensity where the word ""dog"" is ""attending"" the most to ""it"". People slap on a BERT in Kaggle competitions because, well, it is easy to do so, thanks to Huggingface without really knowing what even the abbreviation means. Ask a self-proclaimed person on LinkedIn about it and he will say oh it works on attention and masking and refuses to explain further.  I'm saying all this because after searching a while for ELI5-like explanations, all I could get is a trivial description.",MachineLearning
r761vg,1638446840.0,[D] CLIP vs Starspace,"After reading both the paper accompanying [CLIP](https://arxiv.org/pdf/2103.00020.pdf) form openAI, as well as [Starspace](https://arxiv.org/pdf/1709.03856.pdf) from FAIR/Meta AI from a couple years ago, it is not very clear to me what the exact novelty is besides a prettier presentation.

Even though images are not considered in the Starspace paper, that method essentially also works through (potentially) cross-modal contrastive embedding learning.

what's the hype?",MachineLearning
r75uix,1638446096.0,[P] The new library to make CLIP guided image generation simpler.,"There are different ways to generate images by their text descriptions. But one of the most powerful approaches to generate synthetic art is CLIP guided image generation. We provide a new python library that incapsulates the whole logic of the CLIP guided loss into one PyTorch primitive with a simple API. We provide CLIP guided loss using different CLIP models (such as original CLIP models by OpenAI and ruCLIP model by SberAI), multiple prompts (texts or images) as targets for optimization, and automatic detection and translation of the input texts. Also, we provide our tiny implementation of the VQGAN-CLIP based on our library and VQVAE by SberAI (in my opinion, this is the best version of the VQGAN that is publicly available) to make text to image. Our library is all you need to integrate text-powered losses into your image synthesis pipelines by adding a few lines of code. You can find our library here (pypi package is available): [https://github.com/bes-dev/pytorch\_clip\_guided\_loss](https://github.com/bes-dev/pytorch_clip_guided_loss)",MachineLearning
r74pmb,1638441679.0,[R] Mito: Speed Up Dataset Manipulation with No Coding,"&#x200B;

https://preview.redd.it/7ptv7mkpy3381.jpg?width=700&format=pjpg&auto=webp&s=0690957807030a65e135a3333e26d4e51b7761e3

[Mito](https://docs.trymito.io/) is a spreadsheet provided as a Python library, which allows you to manipulate a dataset in a simple and fast way, and above all in an interactive way.

**Mito provides a graphical interface within the Jupyter Lab environment, so you can manipulate any dataset.**

What can you do with [Mito](https://trymito.io/)?

**1 IMPORT DATASET -** load a new dataset from the file system. Browse among the directories of the filesystem. Supported formats include Excel (XLSX) and CSV

**2 EXPORT DATASET** \- download a manipulated dataset as a CSV to your local filesystem

**3 ADD COLUMN** \- add a new column to the dataset. You can change the column name, as well as the column values.   You can either enter values manually or calculate values from the other columns.   Double click on the first row of the column to insert a formula

**4 DELETE COLUMN** \- erase a column completely

**5 PIVOT TABLE** \- Build a pivot table. The resulting table can be manipulated separately

**6 MERGE TWO DATASETS** \- You can choose among the following merge types: lookup, left, right inner outer

**7 PLOT A GRAPH** \- You can choose the columns associated to X and Y axes and then choose one of the supported graphs",MachineLearning
r74ejm,1638440414.0,[P] Data search engine for MachineLearning,"Hi there, in my work as a DS, I constantly have to solve the problems of increasing the quality of models and achieving the best result. The most obvious way for me to increase quality is to add new, meaningful data. How do you search for data for your ML tasks?",MachineLearning
r74ci2,1638440177.0,"[P] Adding a classification layer after a clustering model, is this correct?","In a current project we are given a list of clients to be segmented using a clustering model. For this, we use kmeans. We get decent clusters, well defined and all.

The issue comes after, when a new client is to be inserted in the database, it needs to be added to one of the segments. For this we thought about adding a classification layer, using the data with cluster information for training and test. First attempts (logistic, linear SVM, decision tree) resulted in overfit, and through some downsampling and analysis we finally managed a not-overfitted SVM.

The doubt still rose, is this a correct approach to the problem? Might there be some sort of data leakage occurring? Are we unintentionally biasing the results?

Aby insight is highly appreciated.",MachineLearning
r74c38,1638440129.0,[P]: Extract and label data from Wikipedia with DataQA,"Hi all,

I recently added a new feature to DataQA ([https://github.com/dataqa/dataqa](https://github.com/dataqa/dataqa)) to be able to extract entities from Wikipedia. All you need to do is upload a file with Wikipedia urls:

[Example file with the wikipedia urls of unicorn companies. Only the url column is needed.](https://preview.redd.it/yw21edyat3381.png?width=887&format=png&auto=webp&s=7efccb656c416dcacd552b62d1b0b45d34f0ce8f)

The tool then extracts and parses all wikipedia content, including tables. It divides the content into smaller paragraphs and sections that are easier to label.

[Simple upload and labelling of Wiki tables.](https://i.redd.it/qxnvtjmht3381.gif)

The output file is a csv file with the text extracted from Wikipedia, and all the manual and automated rules-based labels. This file can then be used to train a custom NER model for Wikipedia data or to simply have as a dataset for other use cases.

[Example output csv.](https://preview.redd.it/ctetm3o5u3381.png?width=995&format=png&auto=webp&s=a4196c5d59847515d92f4c46ebcbdf91f7634c4c)

Feel free to come talk to me on the comment section, I would love to engage with people :-)",MachineLearning
r74784,1638439584.0,[D] Does working with Tensorflow affect my chances of getting research internships?,"I have been practicing ML using Tensorflow for over 3 years now and I'm looking for research internships but after plenty of rejections from professors and institutes that primarily use Pytorch, I'm starting to wonder if there is a correlation between the rejections and me focusing on Tensorflow as my primary tool. Have any of you experienced this?

Note that I am in no way trying to say that I am the ideal applicant but I'm trying to improve. If it turns out that this is true then I have no problem switching to Pytorch. I just want to be sure if the switch is necessary because I was planning on learning JAX/Flax for research in the future due to its optimized XLA support.",MachineLearning
r736yo,1638435349.0,[P] Pls help with a CV problem!,"

Guys, I am trying to make a denoising model for a specific type of noise on geological data. (Kinda like diffusion waves) It looks like the image on the right. Do you have any ideas on how I could model that noise to slap it onto synthetic data to make a denoising model? Or maybe you know about some dataset like that or an open source denoising model? I tried 2D-FFT it doesnt really give this type of noise, looking for some other optins. Details of the noise should for real look like this, any other kind of noise wouldnt do(

[https://imgur.com/gallery/iqRNKy5](https://imgur.com/gallery/iqRNKy5)",MachineLearning
r70stj,1638426008.0,[D] How a Feature Store be useful for my scenario,"I recently came across Feature stores as a mechanism to operationalize ML pipelines in a commercial setting. It sounds good, but I'm finding it hard to understand when and where to use it when doing exploratory data analysis. For example., consider a simple use case of prediction house prices for a given geographical location. I source the raw data from a remote server that contains historical house prices with some columns (features) as below:

    latitude, longitude, total_rooms, house_size, total_bedrooms, year_of_construction........

This is just a small representation of the feature set. As a Data Engineer, one has to probably look into the raw data, do some simple statistical analysis like:

1. Identify Null or NaN values and impute them
2. Identify the co-relation of the features with respect to the target variable and determine if some features be dropped or not
3. Identify the unique count for a numeric variable and determine to remove that feature or column if the unique count is below a certain threshold
4. Delete duplicate rows
5. Perform OneHotEncoding for categorical data
6. Identify and remove outliers
7. Perform dimensional reduction / feature scaling

Now assuming that I would be performing just the first few steps or I would be performing all the above steps, I would like to know how employing a feature store would speed up or rather operationalize my ML pipelines?",MachineLearning
r70j6e,1638425049.0,[D] Questions about possible self-plagiarism in this NeurIPS paper.,"NeurIPS 2021 is around the corner, and I came across the only scene text-related work, titled ["" CentripetalText: An Efficient Text Instance Representation for Scene Text Detection""](https://openreview.net/forum?id=z1F9G4VnGZ-#all) (paper A) and it is an interesting paper. The authors claimed that they introduced a new text instance representation in scene images, based on text kernels (central text regions) and centripetal shifts.  Upon reading their paper, I found out that the main idea and the proposed method are closely related/similar to another paper, ["" Bidirectional Regression for Arbitrary-Shaped Text Detection""](https://arxiv.org/pdf/2107.06129.pdf) (paper B) of the same original authors. Strangely, paper B is published before paper A but it is not mentioned at all in paper A, and I think paper A has a high similarity to paper B.

**Timeline of both papers:**

Paper B was first submitted and accepted to ICDAR 2021 (submission deadline: 8th-Feb, camera-ready deadline: 17th-May). Paper A was then submitted and accepted to NeurIPS 2021 (submission deadline: 28th-May, camera-ready deadline: 26th-Oct).

**Questions I have:**

1. Is it common that the same contributions (text representation and proposed method) have been introduced twice in two independent papers and submitted to two separate conferences?
2. Should the authors cite their latest previous work and explain their similarities/differences? (In this case here, the authors have already known that the ICDAR paper is accepted right before their submission to NeurIPS.)
3. Is this considered a case of self-plagiarism and against the rules of publication to conferences ([especially dual submissions](https://nips.cc/Conferences/2021/CallForPapers))?",MachineLearning
r6ywq0,1638419777.0,[D] Can a prediction interval and confidence index be obtained with a neural network?,"My background is in Material Science and I have used neural networks with good success to predict material properties.

However,  I feel like I am missing two important ingredients:

1- the ability to output a prediction interval as opposed to a single value
2- a reliability or confidence level, indicating whether the neural net found close neighbors and a simple interpolation led to a high confidence prediction vs the prediction was made in an region with little data where extrapolation was needed (low confidence).

For 1- maybe simply adding an additional output neurone could work (one neurone for the mean - highest density,  and one for the variance). Would this be an acceptable option?

For 2- I have no idea if algorithms can output such information. I would be grateful is someone could share some information here.

I'm looking forward to your feedbacks. Thank you.",MachineLearning
r6v7ic,1638408741.0,[D] Paper Explained - Sparse is Enough in Scaling Transformers (aka Terraformer) | Video Walkthrough,"[https://youtu.be/hgSGHusDx7M](https://youtu.be/hgSGHusDx7M)

Transformers keep pushing the state of the art in language and other domains, mainly due to their ability to scale to ever more parameters. However, this scaling has made it prohibitively expensive to run a lot of inference requests against a Transformer, both in terms of compute and memory requirements. Scaling Transformers are a new kind of architecture that leverage sparsity in the Transformer blocks to massively speed up inference, and by including additional ideas from other architectures, they create the Terraformer, which is both fast, accurate, and consumes very little memory.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

4:10 - Recap: Transformer stack

6:55 - Sparse Feedforward layer

19:20 - Sparse QKV Layer

43:55 - Terraformer architecture

55:05 - Experimental Results & Conclusion

&#x200B;

Paper: [https://arxiv.org/abs/2111.12763](https://arxiv.org/abs/2111.12763)

Code: [https://github.com/google/trax/blob/master/trax/examples/Terraformer\_from\_scratch.ipynb](https://github.com/google/trax/blob/master/trax/examples/Terraformer_from_scratch.ipynb)",MachineLearning
r6t6u1,1638403115.0,"[R] To compare models, is reporting the standard deviations enough, or do we have to also report significance testing?","Many papers only report the standard deviations. They don't bother reporting the significance tests.

Countless of papers if you search. An example with 3K citations: [http://proceedings.mlr.press/v37/long15.pdf](http://proceedings.mlr.press/v37/long15.pdf)

Is there any article to show that reporting the standard deviations is sufficient? Thanks.",MachineLearning
r6t3fo,1638402863.0,ML for audio and signal processing conferences and journals [D],"There is a lot of discussion about conferences and venues for core ML/DL research but very little discussion about where to publish for ML in audio and signal processing. Can you advise some good venues for the same: I know ICASSP and INTERSPEECH, but what about the others?

I know a lot of work is still submitted to journals in Signal processing due to the EE influence, so how about IEEE transactions (there are so many of them)?",MachineLearning
r6oxjl,1638392092.0,"[D] Machine learning for factor analysis, rule derivation, etc","I was considering a question where I wanted to know if any patterns could be found in a question of classification.

I wanted to know if a list of factors or criteria could be deduced for why a terminologist who often prepared a glossary considers a phrase a term. For example, in a vaccine instruction manual, “mRNA” would be a good term for a glossary, the word “there” would not.

So here’s the thing: in order to study which factors tended to correlate and to what degree - i.e., some kind of model, a mathematical function of any structure, perhaps a linear equation - I need to have factors in the first place.

How can I generate an ample list of factors, to begin with?

Like let’s say the answer in this scenario is, “a good term should be highly specific, it should be important to have a good quality translation, and it should have high frequency in the source text.”

Is it possible AI could perform that level of inductive reasoning - to observe classified data points in their context (term / not a term, given some text) - and not only predict what is a term, but somehow produce any kind of structure we could observe to understand what the factors it found were?

I really hope someone can help me know about what ideas exist in the field on this question.

Thank you",MachineLearning
r6otxk,1638391836.0,[D] Cohen's kappa — useful?,"I often see subtle misuses of interrater reliability metrics.

For example, imagine you're running a Search Relevance task, where search raters label query/result pairs on a 5-point scale: Very Relevant (+2), Slightly Relevant (+1), Okay (0), Slightly Irrelevant (-1), Very Irrelevant (-2).

Marking ""Very Relevant"" vs. ""Slightly Relevant"" isn't a big difference, but ""Very Relevant"" vs. ""Very Irrelevant"" is. However, most IRR calculations don't take this kind of ordering into account, so it gets ignored.

I wrote [an introduction to Cohen's kappa](https://www.surgehq.ai/blog/inter-rater-reliability-metrics-understanding-cohens-kappa) (a rather simplistic and flawed metric, but a good starting point to understanding IRR). Hope it helps. I welcome feedback and am curious to hear the IRR metrics you find yourself relying on most.",MachineLearning
r6nzv3,1638389706.0,[P] AnimeGAN For Videos,"What would your anime look like?

Using this Gradio demo, you can find out by recording a video from your webcam. It applies [AnimeGANv2](https://github.com/bryandlee/animegan2-pytorch) across frames of your recorded video, stitches it back together, and lets you check out the results.


Try it out now on Hugging Face Spaces! 🤗🚀


👉[Demo](https://huggingface.co/spaces/nateraw/animegan-v2-for-videos)
👉[Announcement Thread](https://twitter.com/_nateraw/status/1466126378475139084?s=20)",MachineLearning
r6llo5,1638383579.0,[D] How do (scalable) Gaussian processes compare to neural nets?,"Most discussions I've seen on this topic don't discuss scalable or deep GPs in any depth, and on the other side don't discuss NNs with uncertainty estimates, so it's difficult to draw a solid comparison. References would be excellent where possible.",MachineLearning
r6lfdy,1638383130.0,[D] PyTorch Dev Day going on now!,"Twitter broadcast: https://twitter.com/i/broadcasts/1MnxnkoynPdKO

Youtube broadcast: https://www.youtube.com/watch?v=vXbbaEZbrOI",MachineLearning
r6kviy,1638381737.0,"[D] Are Image Transformers Overhyped? ""MetaFormer is all you need"" explained (5-minute summary by Casual GAN Papers)","Unless you have been living under a rock for the past year you know about the hype beast that is vision Transformers. Well, according to new research from the team at the Sea AI Lab and the National University of  Singapore this hype might be somewhat misattributed. You see, most vision Transformer papers tend to focus on fancy new token mixer architectures, whether self-attention or MLP-based, however, Weihao Yu et al. show that a simple pooling layer is enough to match and outperform many of the more complex approaches in terms of model size,  compute, and accuracy on downstream tasks. Perhaps surprisingly, the source of Transformers’ magic might lie in its meta-architecture,  whereas the choice of the specific token mixer is not nearly as impactful!

Full summary: [https://t.me/casual\_gan/205](https://t.me/casual_gan/205)

Blog post: [https://www.casualganpapers.com/vision-transformer-meta-architecture-sota-imagenet-pretraining/MetFormer-explained.html](https://www.casualganpapers.com/vision-transformer-meta-architecture-sota-imagenet-pretraining/MetFormer-explained.html)

&#x200B;

[MetaFormer](https://preview.redd.it/l8m7d3yszy281.png?width=2436&format=png&auto=webp&s=f9791a9b414c507b0f7d43d3c7af8cdc3247bd68)

[arxiv](https://arxiv.org/pdf/2111.11418.pdf) / [code](https://github.com/sail-sg/poolformer)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",MachineLearning
r6k9n4,1638380203.0,[P] Detect AI generated images vs. authentic photos,"There's more and more content generated by GANs, I thought it would be useful to detect whether a piece of content is generated by AI. I'm starting with images and here's a demo [https://www.galeras.io/](https://www.galeras.io/). Suggestions are welcome!",MachineLearning
r6hf44,1638372777.0,[D] Papers about mathematics of Deep Learning,Simple classical ML algorithm can find good list of papers about mathematics of Deep Learning: [http://www.arxiv-sanity.com/2105.04026v1](http://www.arxiv-sanity.com/2105.04026v1),MachineLearning
r6h3h8,1638371935.0,[P] - I created an Auto-Updating Kaggle dataset that collects high-frequency crypto market data - Updates daily! | +20 Related Trading Notebooks,"

I am happy to announce that I finally finished cleaning, organizing, creating baselines, and developing an automated collection pipeline that collects minute-by-minute market data for Cryptocurrencies. It updates on Kaggle every day! And will keep doing so until the competition is over! \[Maybe even more\]

The whole project took me a lot of time to develop and is not easy to maintain, so please if you find this of value: Your feedback & support is highly appreciated!

## The Competition

As some of you know, there is Crypto forecasting competition is running on Kaggle: ""G-Research Crypto Forecasting"". In this competition, we need to use machine learning for forecasting short-term returns of popular cryptocurrencies \[such as bitcoin, ether, dogecoin..\] We are provided a dataset of millions of rows of high-frequency market data dating back to 2018 which we should use to build our models on. Once the submission deadline has passed, the final score will be calculated over the following 3 months using live crypto data as it is collected.

## Auto-updating Kaggle dataset

To make things more interesting: I created an Auto-Updating Kaggle dataset that collects high-frequency market data for multiple cryptocurrencies.

* Updates daily on Kaggle!
* Available for anyone to play with!

Also, I also released **20+ starter notebooks** each demonstrating a different model or method for forecasting future returns.

This project was meant to be for the currently running Crypto Forecasting Competition by G-Research. However, since it is publicly available I assumed many others would like to also have a look :)

**Mimics ""Real-Life"" better than typical datasets**

This is a unique opportunity to work in a much more ""real-life"" setup than usual Kaggle. Because the datasets update daily.

* so.. If you mess up and overfit..
* You see it tomorrow! 😂

Anyway, this is an ongoing project that is also beginner-friendly since it is highly documented. Many more Time Series / Finance-related notebooks will be released in the future so this can also serve as a ""first stop"" when studying Time Series analysis.

## Baselines & Starter Notebooks

|CV + Model|Hyperparam Optimization|Time Series Models|Feature Engineering|
|:-|:-|:-|:-|
|[Neural Network Starter](https://www.kaggle.com/yamqwe/purgedgrouptimeseries-cv-with-extra-data-nn)|[MLP + AE](https://www.kaggle.com/yamqwe/bottleneck-encoder-mlp-keras-tuner)|[LSTM](https://www.kaggle.com/yamqwe/time-series-modeling-lstm)|[Technical Analysis #1](https://www.kaggle.com/yamqwe/crypto-prediction-technical-analysis-features)|
|[LightGBM Starter](https://www.kaggle.com/yamqwe/purgedgrouptimeseries-cv-with-extra-data-lgbm)|[LightGBM](https://www.kaggle.com/yamqwe/purged-time-series-cv-lightgbm-optuna)|[Wavenet](https://www.kaggle.com/yamqwe/time-series-modeling-wavenet)|[Technical Analysis #2](https://www.kaggle.com/yamqwe/crypto-prediction-technical-analysis-feats-2)|
|[Catboost Starter](https://www.kaggle.com/yamqwe/purgedgrouptimeseries-cv-extra-data-catboost)|[Catboost](https://www.kaggle.com/yamqwe/purged-time-series-cv-catboost-gpu-optuna)|[Multivariate-Transformer \[written from scratch\]](https://www.kaggle.com/yamqwe/time-series-modeling-multivariate-transformer)|[Time Series Agg](https://www.kaggle.com/yamqwe/features-all-time-series-aggregations-ever)|
|[XGBoost Starter](https://www.kaggle.com/yamqwe/xgb-extra-data)|[XGboost](https://www.kaggle.com/yamqwe/purged-time-series-cv-xgboost-gpu-optuna)|[N-BEATS](https://www.kaggle.com/yamqwe/crypto-forecasting-n-beats)|[Neutralization](https://www.kaggle.com/yamqwe/g-research-avoid-overfit-feature-neutralization/)|
|[Supervised AE \[Janestreet 1st\]](https://www.kaggle.com/yamqwe/1st-place-of-jane-street-adapted-to-crypto)|[Supervised AE \[Janestreet 1st\]](https://www.kaggle.com/yamqwe/1st-place-of-jane-street-keras-tuner)|[DeepAR](https://www.kaggle.com/yamqwe/probabilistic-forecasting-deepar/)|⏳Target Engineering|
|[Transformer)](https://www.kaggle.com/yamqwe/let-s-test-a-transformer)|[Transformer](https://www.kaggle.com/yamqwe/sh-tcoins-transformer-baseline)||⏳Quant's Volatility Features|
|||||
|[Reinforcement Learning (PPO) Starter](https://www.kaggle.com/yamqwe/g-research-reinforcement-learning-starter)|||⏳Wavelets|

[About the validation: GroupTimeSeriesSplit](https://www.kaggle.com/yamqwe/let-s-talk-validation-grouptimeseriessplit)

(⏳ - in the making..)

Fork them as you please! Enjoy Yourself!

## Auto updating - Full Price Datasets

I created an up-to-today \[auto updating\] dataset which contains the full historical data for all assets of the competition so you can easily build models that utilize it. The datasets are split to each asset since they are much heavier than the competition data. The datasets have also been labeled as described in the competition overview and had been organized in a way that they are at the exact format of the competition data.

**The goal of this is to provide a dataset that:**

1. Contains the FULL history for each asset. Currently, the competition data goes back to 2018. This dataset contains data from even earlier.
2. Auto updating daily - Due to the high volatility of the cryptocurrency market, we should train our models on the most recent data available. These datasets have a backend pipeline for collecting, formatting, and reuploading to kaggle. They are scheduled to be updated daily, every single day until the end of the competition.
3. Preprocessed - The datasets had been ffilled to overcome any missing values issue that is present in the original competition dataset.

**The Datasets:**

* [Binance Coin](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-binance-coin)
* [Bitcoin Cash](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-bitcoin-cash)
* [Bitcoin](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-bitcoin)
* [Cardano](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-cardano)
* [Dogecoin](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-dogecoin)
* [Eos.io](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-eos-io)
* [Ethereum](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-ethereum)
* [Ethereum Classic](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-ethereum-classic)
* [Iota](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-iota)
* [Litecoin](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-litecoin)
* [Monero](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-monero)
* [Maker](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-maker)
* [Stellar](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-stellar)
* [TRON](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-tron)

>**Bonus dataset:** I've also uploaded a dataset containing the most powerful source for predicting cryptocurrencies movement: Elon Musk's Twitter 😂! It is simply an updated dataset of all Elon Musk's tweets 😂. I must check if Elon Musk can help us win! 👌 You can play with it yourself [here](https://www.kaggle.com/yamqwe/elon-musks-twitter-updated-031121).



**Technical details about the Data** For every asset in the competition, the following fields from [Binance's official API endpoint for historical candlestick data](https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#klinecandlestick-data) are collected, saved, and processed.

1. timestamp - A timestamp for the minute covered by the row.
2. Asset\_ID - An ID code for the cryptoasset.
3. Count - The number of trades that took place this minute.
4. Open - The USD price at the beginning of the minute.
5. High - The highest USD price during the minute.
6. Low - The lowest USD price during the minute.
7. Close - The USD price at the end of the minute.
8. Volume - The number of cryptoasset u units traded during the minute.
9. VWAP - The volume-weighted average price for the minute. 10.Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
10. Weight - Weight, defined by the competition hosts [here](https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition)
11. Asset\_Name - Human readable Asset name.

**Indexing** The dataframe is indexed by `timestamp` and sorted from oldest to newest. The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.


Enjoy Yourself!
And thank you in advance for your support! This is not an easy system to maintain!",MachineLearning
r6fzxo,1638368966.0,[Discussion] Sharing of node information in MCTS Runs," In scenario of a single player game, with a pretrained value and prior probability model, if it is given that a state can be reached through different order of actions. (Imagine something like a knight move followed by a bishop move and vice versa where both the series of moves lead to the same state.) What are the different ways used today to reuse the information about states already explored.

Is it possible to connect nodes of different branches of search tree when it is known that those states are identical(with some sort of a hash function in an effort to save the roll out time), Is it possible to also reuse the current approximation of Q(s, a) (based on W, and n for a particular action) when two actions lead to the same state?",MachineLearning
r6dzjf,1638362857.0,[P] Uniform sampling over the feature space,"Hello,

I have a question concerning my master's thesis and thougth, maybe someone can help me here. To put it simply, I have a supervised learning problem. I need to predict a regression target value based on circa 20 features (do not want to go into detail as it is under disclosure) with a classical ML model like random forest regression. This project uses real world data. The model does not need to generalize beyond the test set, as the prediction on the test set is all we want.

I split the dataset into training and test set not randomly, but by uniformly sampling over the entire feature space. Simply spoken, this is done by calculating euclidean distances between the points in feature space and iteratively sampling the points farthest away from each other. The result is a uniform distribution over the feature space for training and test set. (this is the Kennard-Stone algorithm, but I came up with it without having heard of this algorithm). The algorithm ensures, that the test set lies within the training set in feature space, and even ensures, that outliers are included in the training set. The thought process is, that the ml model will not need to perform extrapolation on the test set, which should yield better results. Performance shows, that this data split indeed performs much better on the test set than random splitting. My supervisor wants me to prove, that this works better than random splitting not only empirically .

Does anyone of you have an idea how to prove that including the outliers in the training set and performing data splits, that uniformly sample over the entire feature space, improve the performance on the test set? I mean, I often read, that extrapolation is very hard for machine learning models, but I cannot come up with an experiment to show this.

If something is unclear pls ask your questions.

Thank you in advance,

Alex

edit: Note that these so called outliers are the points on the surface of the feature space, if we think of it as a ball in a  3-dimensional feature space. So the data points on the surface of the sphere are included in the training set.",MachineLearning
r6b27d,1638351490.0,[R] Neurips best paper awards,"Link: https://blog.neurips.cc/2021/11/30/announcing-the-neurips-2021-award-recipients/?s=09

So many awards this year…

Also noticed they’ve released the accepted papers from the Datasets/Benchmarks track:
https://nips.cc/Conferences/2021/DatasetsBenchmarks/AcceptedPapers

Hope this track becomes a permanent part of neurips.",MachineLearning
r6axta,1638350990.0,[D] Continue training using vgg16 on higher resolution dataset,"Hello, I’d like use the popular trained model vgg16, trained from fixed size 224x224 RGB images, to continue training on my own dataset. The only thing is, my dataset is of size 896x896 RGB images. Can I still use the vgg16 model or is there any tweaks I need?",MachineLearning
r69q9n,1638345842.0,[P] Check out my self-made Deep RL library written in TF2.x!,"Hi, I'd be delighted to receive some feedback for my Deep RL mini-library, based on TensorFlow 2. It is mainly educational when it comes to performance, but it contains clear implementations for the classic RL algos:

DQN and variants

DDPG, TD3, SAC

Policy Gradient, A2C, PPO.

The lib: [https://github.com/csxeba/trickster](https://github.com/csxeba/trickster)

It is also dockerized, mainly for convenience. If you beautiful people could give me feedback on how to better present it, how it would be useful for actual practical problems to you, etc., it would be nice.",MachineLearning
r61gwu,1638317663.0,[D] How much do reviewers check proofs in papers?,"For the reviewers out there, how much time/energy do you spend on following the proof presented by a paper?

Suppose you understand the paper and the idea makes sense in your head. Then, how rigorous are you when it comes to the proof?

I know there are many different factors, e.g., the reputation of the journal (better journals might have more strict/rigorous reviewers), but I am just curious about a general sense.

What got me curious is that I found a paper that looks interesting and presents a theorem, but I have not been to fully digest the proof, even though I've spent a while. I know reviewers probably have a better understanding than me, but still.",MachineLearning
r60cet,1638314397.0,[D] DASK dataframe plotting without using too much RAM (same with PySpark),"Can anyone give me a solution how to actually plot the whole dataset in PySpark and DASK if they definitely don't fit in to memory (I am interested in solutions that not just dump the whole thing in to numpy array but actually very light on RAM (ideally something that can plot by smaller batches and that keep the results in the SSD))?

In DASK I know the following: [https://examples.dask.org/dataframe.html](https://examples.dask.org/dataframe.html)

    df[['x', 'y']].resample('24h').mean().compute().plot()",MachineLearning
r5xx1d,1638307671.0,[D] Extending trained GAN,Hi I would like to know your opinion on extending already trained GAN. For example lets say I have GAN trained on 3 types of classes to segment and now I wanna add 4th type to segment ... what is ideal approach for this ? Can I keep already learned weights and just run training again for just 4th class or it is impossible because GAN will forgot how to segment previous classes ? I am planning to use pix2pix model. Thanks in advance.,MachineLearning
r5x0v2,1638305274.0,"[D] AMA with David Bau, and I study the structure of the complex computations learned within deep neural networks","David Bau believes the members of this subreddit would be interested in his AMA:

https://reddit.com/r/IAmA/comments/r5vte5/i_am_david_bau_and_i_study_the_structure_of_the/

Note that discussion, questions and answers are in the linked thread above, not this one.",MachineLearning
r5wc8j,1638303452.0,"[D] StyleGAN3: Overview, Tutorial, and Pre-Trained Model","[https://lambdalabs.com/blog/stylegan-3/](https://lambdalabs.com/blog/stylegan-3/)

A post covering StyleGAN3. It discusses architecture, how it improves on StyleGAN2, and how to use it. It also includes a pre-trained StyleGAN 3 model.",MachineLearning
r5vyec,1638302441.0,[D] Getting started with Explainable ML,Are there any good resources to get started with explainable ML? Thanks.,MachineLearning
r5rxbr,1638292009.0,[D] Has anybody tried using OpenAI gym to automate using a GUI application?,"Can it do something simple like “Create a new macro in Excel?”

Or is there any pretrained model that can translate natural language to mouse actions over an image (a desktop)?",MachineLearning
r5rn66,1638291293.0,[D] How to handle your advisor?,"Hi!

I know the title might sound offensive, but I mean, how to deal with your advisor?

I am a Ph.D. student in Machine learning, and for the last year, I couldn't find a thesis topic because whenever I go to my advisor, he responds that we need to wait for the data (which is supposed to come from a company).

I started looking for different projects and datasets online to work on. My advisor doesn't spend time reading many papers or keeping himself updated about the field, so I thought I would find a way for myself. I noticed that if I bring some ideas to him, he takes them to his Master's students and involves them, which is not a bad thing, but the problem comes when he asks them to lead it. Although they are not some big novel ideas but still for me, that is a way to progress in my research and develop a strong profile.

Recently I shared an idea with him, and he just called some other students to work on it, and I was completely removed from the scene.

I want to hear from other Ph.D. students about how you handle these situations, of course I don't want to have troubles with my advisor and talking to him will not help. He seems to be too aggressive at times. I want some suggestions on how you guys manage these type of situations?

Thanks",MachineLearning
r5q2re,1638287110.0,"[R] Google, Cambridge U & Alan Turing Institute Propose PolyViT: A Universal Transformer for Image, Video, and Audio Classification","A research team from Google Research, University of Cambridge and Alan Turing Institute proposes PolyViT, a single transformer model capable of processing multiple modalities and datasets. PolyViT is parameter-efficient and learns representations that generalize across multiple domains.

Here is a quick read: [Google, Cambridge U & Alan Turing Institute Propose PolyViT: A Universal Transformer for Image, Video, and Audio Classification.](https://syncedreview.com/2021/11/30/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-155/)

The paper *PolyViT: Co-training Vision Transformers on Images, Videos and Audio* is on [arXiv](https://arxiv.org/abs/2111.12993).",MachineLearning
r5p9nx,1638284892.0,[P] Build automations powered by Machine Learning from a single Python file,"Hi all,

Machine Learning has the potential to automate a huge portion of boring processes but often gets stuck at the Jupyter Notebook stage and never make it to the real world.


Take for example the case in which we want to predict whether customers will unsubscribe using some basic customer data. Many of you from this sub can build a simple Machine Learning model to predict whether a customer will *churn* if they get a nice pandas.DataFrame with some customer data.


However, it gets really complicated if you want this model **deployed** and **integrated** in production, say a procedure as such:

1. Pull data from a new customer from **Shopify**
2. Predict for this customer whether they will *churn*
3. If we predict CHURN == True
4. Send a discount code to this customer automatically

Suddenly, we have to code API integrations, build ETL pipelines, deploy our original Machine Learning solution onto a REST API, spin up an HTTP server etc; and maintain all of this (!) -- a huge pain   indeed...

We are building [a framework](https://flow.magicsheets.io/) that takes care of exactly all the boring stuff described above. We really believe this will **bridge the gap between research and real-world ML**.

Super excited to get some early feedback, so please shoot any questions in the comments :)",MachineLearning
r5ox5x,1638283898.0,[D] Best pretrained model for cleaning Excel sheet?,"Is there any pretrained model, perhaps BERT or something on HuggingFace, which you can pass a spreadsheet, perhaps in .csv format, and it just guesses how you might want to clean it up? It’s ok if it’s not perfect.",MachineLearning
r5ktec,1638269844.0,[D] Which one do you prefer?,"You had an idea. You already implemented and tested it in Pytorch. Now you want to test it on architecture A which is publicly available but in Tensorflow. Would you (1) implement your idea on A's framework, or (2) implement A on your framework?

Would you change your answer if you want to further test your idea on a few more architectures?",MachineLearning
r5kjoq,1638268671.0,[R] Oversquashing and bottlenecks in GNNs and graph Ricci curvature,"&#x200B;

https://preview.redd.it/f44nqkoaop281.png?width=1086&format=png&auto=webp&s=fd1b0fdf3682d210d532f477b21fa16a7fed2fea

Over-squashing is a common plight of GNNs occurring when message passing fails to propagate information efficiently on the graph. In a new [blog post](https://michael-bronstein.medium.com/over-squashing-bottlenecks-and-graph-ricci-curvature-c238b7169e16?sk=f5cf01cbd57b4fee8fb3a89d447e56a9), I discuss how this phenomenon can be understood and remedied through the concept of Ricci curvature (see the [paper](https://arxiv.org/pdf/2111.14522.pdf) for details).

The second installment of this post will discuss whether (and when) diffusion improves graph learning, analyzing the popular DIGL rewiring method of Klicpera et al.

This post is part of a new series on [Graph Neural Networks through the lens of Differential Geometry and Algebraic Topology](https://towardsdatascience.com/graph-neural-networks-through-the-lens-of-differential-geometry-and-algebraic-topology-3a7c3c22d5f?source=your_stories_page----------------------------------------).",MachineLearning
r5k7op,1638267289.0,[P] nvim-dvc: neovim plugin for DVC,"More and more data science and machine learning teams are moving to [DVC](https://dvc.org/) as framework for data version control. As such, and as avid neovim user, I wrote a neovim plugin that allows to interact with DVC stages, files and configurations from within neovim: [nvim-dvc](https://github.com/gennaro-tedesco/nvim-dvc).

In a nutshell the plugin allows to populate the quickfix window with pipeline stages, metrics, storage files and the like for quick navigation; likewise, autocompletion enables to easily reproduce the pipeline stages of interest. Have a look if you are using DVC and neovim, you may find it useful (it is in early stage, so you may encounter some errors, please do point them out if so)!

[Link to the repository](https://github.com/gennaro-tedesco/nvim-dvc)",MachineLearning
r5k4yh,1638266979.0,[P] Deep Learning in Production Book,"Hello everyone,

I'm proud to share with you the first edition of our new book on MLOps and machine learning infrastructure. Deep Learning in Production is an effort to aggregate best practices on how to build, train, deploy and scale deep learning models. The premise is that we start from a simple jupyter notebook and work our way towards building a fully-function web application that can serve million of users.

[https://theaisummer.com/deep-learning-in-production-book/](https://theaisummer.com/deep-learning-in-production-book/)

The book is based on an old articles series we wrote on our blog so a big portion of the content is already available for free. We just organized/restructured some of the articles and we added some new material.  We use a variety of examples with libraries such as Tensorflow, Flask, uWSGI, Nginx, Docker, Kubernetes, Tensorflow Extended, Google Cloud, Vertex AI. The full code and the articles can be found on Github ([https://github.com/The-AI-Summer/Deep-Learning-In-Production](https://github.com/The-AI-Summer/Deep-Learning-In-Production))

We will very much appreciate any feedback or suggestions so we can work upon it in a second edition. Thank you for your time.",MachineLearning
r5cvb6,1638240830.0,[D] Are there any research using chaos theory to study Deep neural network?,"I just know and reading this book: Emergence: From Chaos To Order (https://www.amazon.com/Emergence-Chaos-Order-Helix-Books/dp/0738201421),
but it just studied simple shadow neural network,
Are there any research using chaos theory to study Deep neural network?",MachineLearning
r5bkph,1638236950.0,[D] How important is initialization?,"Could you share papers that discuss the topic of initialization from a theoretical point of view? It is my feeling that this topic is usually overlooked, and I don't know if justifiably so or not. From my anecdotal experience, initialization is crucial. Moreover, for very simplistic experiments (small 3 layer NN) it is not hard to come up with a non-trivial initialization that won't allow the loss to decrease if the function to be learn by the NN is the identity function.

One example of the kind of papers I am thinking of is this one: [On Lazy Training in Differentiable Programming](https://arxiv.org/abs/1812.07956).

I am asking here because maybe someone knows of/wants to share a paper they find interesting.",MachineLearning
r5a6vx,1638232756.0,[D] Paper Explained - ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning (Video Walkthrough),"[https://youtu.be/FbRcbM4T-50](https://youtu.be/FbRcbM4T-50)

The T5 model has been a staple for NLP research for the last years. Both its size and its approach to formulate all NLP tasks as prompt-based language modeling make it a convenient choice to tackle new challenges and provides a strong baseline for most current datasets. ExT5 pushes T5 to its limits by pre-training not only on self-supervised mask filling, but also at the same time on 107 different supervised NLP tasks, which is their new ExMix dataset. The resulting model compares very favorably to T5 when fine-tuned to downstream tasks.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

2:15 - Recap: The T5 model

3:55 - The ExT5 model and task formulations

8:10 - ExMix dataset

9:35 - Do different tasks help each other?

16:50 - Which tasks should we include?

20:30 - Pre-Training vs Pre-Finetuning

23:00 - A few hypotheses about what's going on

27:20 - How much self-supervised data to use?

34:15 - More experimental results

38:40 - Conclusion & Summary

&#x200B;

Paper: [https://arxiv.org/abs/2111.10952](https://arxiv.org/abs/2111.10952)",MachineLearning
r53277,1638212916.0,[P] Cost of distributed deep learning on AWS,"Hi everyone,

I am trying to learn some distributed deep learning techniques such as parameter servers and ring all reduce. I found [this](https://arxiv.org/pdf/1802.05799.pdf) paper by Uber where they describe their implementation of ring all reduce. While the implementation looks fairy straight forward, I am unsure about the costs.

The problem is that I don't have access to multiple GPUs so I thought I would use AWS EC2. Since I have never used it before (I have used GCP a fair amount but not for distributed deep learning) I was hoping someone here could shine some light on the costs of using say 2 GPUs.

My initial idea was to train a CNN using ImageNet (1k). So I would use S3 and EC2 with two basic GPUs as my goal is simply to see how much faster I can reach convergence rather than achieving state of the art times.

Would this be too expensive for a personal project? I'd rather not spend over 200 GBP.",MachineLearning
r4zaut,1638202698.0,[P] Eliminate PyTorch's `CUDA error: out of memory` with 1 line of code,"I've been working on a fast PyTorch wrapper that solves OOM error automatically.

[Project Link](https://github.com/rentruewang/koila)

This project aims to be as flexible as possible, and it works with existing PyTorch code.

I would love to hear your thoughts on this!

Suggestions are welcome!",MachineLearning
r4vums,1638192531.0,[D] Linking decision tree with nearest neighbors classifiers,"Hello,

Looking for opinions on [this paper](https://doi.org/10.13140/RG.2.2.33413.06880)  (Collapsing the Decision Tree: the Concurrent Data Predictor). A variant of the nearest neighbors algorithm is derived by flattening the decision tree algorithm.
In particular, I would be very interested in getting opinions on two aspects:

a) Morphing the decision tree into a nearest neighbors variant by evaluating more than one attribute at the time.

b) The fact that the predictions of both decision tree and nearest neighbors converge to the optimum as the amount of training data increases.

Please note that the focus is on data with categorical attributes.

Is there anyone else exploring these subjects or something similar? Or have you in the past?

Thanks",MachineLearning
r4v2i1,1638189910.0,[P] Software engineering in MLE maturity assessment?,"Soo this maybe such a basic question but apologies in advance.
I've started getting interested in ML and recently got a chance to work on a side project helping a manager create a framework for assessing MLE maturity in an organisation. Some of the criteria I was told to look at was software engineering. Having read several articles on ML ops I'm finding it hard to isolate which parts of the process would be considered software engineering.
Are there any articles or videos you'd recommend that could explain  how  software engineering ties into ML?",MachineLearning
r4s0wx,1638177639.0,[D] AAAI 2022 Paper Results,The AAAI 2022 outcomes have been released; how were these outcomes for your papers?,MachineLearning
r4ra5g,1638174504.0,[R] Patrickstar: A Pytorch Based Training Framework That Can Train Faster And Larger PTMs,"github: [https://github.com/Tencent/PatrickStar](https://github.com/Tencent/PatrickStar)

paper: [https://arxiv.org/abs/2108.05818](https://arxiv.org/abs/2108.05818)

Last month, our team at Tencent open sourced PatrickStar, a pytorch based PTM (pretrained model) training framework that could train larger model and have better performance with the same hardward environment compares to sota training frameworks like DeepSpeed.

[version 0.4.1 performance on 8xV100 node](https://preview.redd.it/9t5fkjq7th281.png?width=1600&format=png&auto=webp&s=b92fc45f9543ef1af4eac5dc8e76f8844d486988)

The key idea of the project is to use a chunk-based memory manage strategy, which means group parameters in chunks of the same size and dynamically move the chunks between CPU and GPU. Compare to DeepSpeed which would determine which part of the model need to put in CPU memory before training, the dynamic memory scheduling would make better use of the GPU memory, namely, only the params involve in computing at the moment are loaded in GPU. This results in larger model. Also, we designed the efficent prefetch and sharding mechanism based on the chunk-based method and those bring us the nice performance.

As large PTM are becoming a must in NLP, we believe PatrickStar would be a help to researchers and engineers. It would be so nice if you could checkout [our github repo](https://github.com/Tencent/PatrickStar) and give us some feedback!",MachineLearning
r4okv5,1638164077.0,[D] What are some ways to reduce model GPU VRAM usage>,"Hey guys,

My GAN models take 10GB VRAM when I try to generate a 512x512 image.


We need to scale stuff up and I plan on productionizing this on a kubernetes cluster but want to optimize them first to save on costs.

Ideally I want to run multiple inferences on the same machine without facing a CUDA OUT OF MEMORY ERROR.


What are some good ways to optimize VRAM usage?",MachineLearning
r4ngnz,1638160296.0,"[D] Is the ""true few-shot"" setting described in recent papers reasonable or am I not understanding the concept properly?","I've been coming across a few papers in few-shot learning that claim to be under the paradigm of ""true few-shot learning."" Inspired by the paper [_True Few-Shot Learning with Language Models (Perez et al., 2021)_](https://arxiv.org/abs/2105.11447).

The basic claim is that the current N-way K-shot setting that's widely used in few-shot tasks is unrealistic due to the fact that the models still have access to many data points in other classes. Not to mention that these models also usually have access to a development set to tune their hyperparameters. The authors claim that these assumptions are unrealistic on the grounds that assuming access to other classes' data points is unrealistic and that if we have access to a dev set then we might as well use it for training.

I'm writing this post because I'm not sure if I agree with those claims or if I'm just not understanding the paper properly and would like to seek some other opinions. Why is it unrealistic to have access to a dev set? If that's the case, why do we even have a test set as well? Under the logic presented in the paper, if we have a test set then we might as well include that in training too. Furthermore, if we have a test set wouldn't it actually be more realistic to split that to have a dev set as well?",MachineLearning
r4lty2,1638155200.0,[P] Is there an AutoML commercial solution for joint image and text classification?,I have a set of images with title captions. I'd like to do a joint classification task using both inputs but I haven't seen a commercial autoML solution to train it. Does anyone know of any solution?,MachineLearning
r4k5lf,1638150042.0,[D] Are you seeing any compelling use cases of semantic search being leveraged at scale?,"In my org, we are highly reliant on Elastic Search and I'm currently investigating the merit of incorporating a semantic search component to our search pipeline. Like most, I've built prototypes leveraging FAISS on our data and while the results are impressive, I haven't come across any compelling use cases where teams are putting this to use at scale.

I want to note, I don't come from a research background.",MachineLearning
r4fjus,1638136728.0,[Discussion] Is there anything similar to Google Pathways out here currently?,"So as many probably know Google is creating [Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/) which should be a model that can do many things, but they haven't released any details or a timeline.

I'm wondering if there are other generalizable learners to use as of now.",MachineLearning
r4fep0,1638136330.0,"[News] Get code for ML/AI papers anywhere on the internet (Google, Arxiv, Twitter, Scholar, and other sites)! ❤️","&#x200B;

https://preview.redd.it/cx0bnk9rqe281.png?width=1030&format=png&auto=webp&s=d57359cb04cf855b980f6dcb293591a3298e5d6e

Browser Extension on Chrome [https://chrome.google.com/webstore/detail/aiml-papers-with-code-eve/aikkeehnlfpamidigaffhfmgbkdeheil](https://chrome.google.com/webstore/detail/aiml-papers-with-code-eve/aikkeehnlfpamidigaffhfmgbkdeheil)

Browser Extension on Firefox [https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/](https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/)",MachineLearning
r4e8he,1638133205.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 126,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|121-130|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)|[Week 121](https://reddit.com/pmzx3g)|||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)|[Week 122](https://reddit.com/pw14z5)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)|[Week 123](https://reddit.com/q5fi12)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)|[Week 124](https://reddit.com/qjxfu9)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)|[Week 125](https://reddit.com/qtzbu1)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)|[Week 117](https://reddit.com/omy345)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)|[Week 118](https://reddit.com/ovz52j)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)|[Week 119](https://reddit.com/p50knh)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)|[Week 120](https://reddit.com/pe2idh)||

Most upvoted papers two weeks ago:

/u/CatalyzeX_code_bot: [Paper link](https://arxiv.org/abs/2012.09841)

Besides that, there are no rules, have fun.",MachineLearning
r47kvq,1638115506.0,[R] Google Research Open-Sources ‘SAVi’: An Object-Centric Architecture That Extends The Slot Attention Mechanism To Videos,"Multiple distinct things act as compositional building blocks that can be processed independently and recombined in humans’ understanding of the world. The foundation for high-level cognitive abilities like language, causal reasoning, arithmetic, planning, and so on is a compositional model of the universe. Therefore, it’s essential for generalizing in predictable and systematic ways. Machine learning algorithms with object-centric representations have the potential to dramatically improve sampling efficiency, resilience, generalization to new problems, and interpretability.

Unsupervised multi-object representation learning is widely used in various applications. These algorithms learn to separate and represent objects from the statistical structure of the data alone, without the requirement for supervision, by using object-centric inductive biases. Despite their promising outcomes, these approaches are currently constrained by two major issues: 

1. They are limited to toy data such as moving 2D sprites or extremely rudimentary 3D scenes, and they struggle with more realistic data with complex textures. 
2. Both during training and inference, it is not clear how to interact with these models. The concept of an object is imprecise and task-dependent, and these models’ segmentation does not always correspond to the tasks of interest.

To overcome the problem of unsupervised / weakly-supervised multi-object segmentation and tracking in video data, a new Google research introduces a sequential extension of Slot Attention called Slot Attention for Video (SAVi).

**Check out the** [**Github**](https://slot-attention-video.github.io/) **and** [**Paper.**](https://arxiv.org/pdf/2111.12594.pdf) **Read the** [**short summary**](https://www.marktechpost.com/2021/11/28/google-research-open-sources-savi-an-object-centric-architecture-that-extends-the-slot-attention-mechanism-to-videos/)**.**

&#x200B;

https://reddit.com/link/r47kvq/video/kmdxpasu0d281/player",MachineLearning
r47k8g,1638115459.0,[D] Smart solution to reduce an impractical number of iterations in a time-series model,"Hello folks,

We have a timeseries based ML solution that helps with anomaly detection and finding the root cause for this anomaly.

How it currently operates is, if I see an anomaly in a metric (if the actual is far from prediction), we try to examine the associated dimensions and associate the anomaly to its root cause accordingly.

For example, considering Covid related deaths across the US, if our model senses an anomaly (steep rise/fall), we try to break down the available data by state and see which state is responsible for such a steep change. In this example, there is only one dimension which is the state. So our model, in essence, iterates over 50 states - hence 50 iterations.

When we try to scale this up to multivariate that involves for example 5 dimensions (to the already existing ""State"", say we add gender, age group, income group, health rating), the number of iterations is going to explode. What was earlier 50, is now going to become,

50 States \* 4 Genders \* 10 Income groups \* 12 Health Ratings = 24,000 Iterations.

When such a model has to detect anomalies in say e-commerce data where the number of dimensions is going to be larger I hope you see how can't our model scale (at least without parallelization).

How do you guys think can we solve this? Any suggestion is greatly appreciated.

Thanks!",MachineLearning
r45wdo,1638110938.0,[R] AI and the Everything in the Whole Wide World Benchmark,"Really interesting criticism of general benchmarks, e.g., GLUE and ImageNet, and their construct validity issues: [https://openreview.net/pdf?id=j6NxpQbREA1](https://openreview.net/pdf?id=j6NxpQbREA1)

'In the 1974 Sesame Street children’s storybook Grover and the Everything in the Whole Wide World Museum \[Stiles and Wilcox, 1974\], the Muppet monster Grover visits a museum claiming to showcase “everything in the whole wide world”. Example objects representing certain categories fill each room. Several categories are arbitrary and subjective, including showrooms for “Things You Find On a Wall” and “The Things that Can Tickle You Room”. Some are oddly specific, such as “The Carrot Room”, while others unhelpfully vague like “The Tall Hall”. When he thinks that he has seen all that is there, Grover comes to a door that is labeled “Everything Else”. He opens the door, only to find himself in the outside world.

As a children’s story, Grover’s described situation is meant to be absurd. However, in this paper, we discuss how a similar faulty logic is inherent to recent trends in artificial intelligence (AI)— and specifically machine learning (ML) — evaluation, where many popular benchmarks rely on the same false assumptions inherent to the ridiculous “Everything in the Whole Wide World Museum” that Grover visits. In particular, we argue that benchmarks presented as measurements of progress towards general ability within vague tasks such as “visual understanding” or “language understanding” are as ineffective as the finite museum is at representing “everything in the whole wide world,” and for similar reasons — being inherently specific, finite and contextual.",MachineLearning
r44r7q,1638107535.0,[P] Combinatorial Optimization for Panoptic Segmentation: A Fully Differentiable Approach,"There is recent interest in incorporating algorithms as layer in neural network. In our recent work ([COPS](https://arxiv.org/abs/2106.03188)) at NeurIPS'21 we tackle a similar task with the following questions:

1. Is it possible to train a pipeline containing neural networks & combinatorial optimization?
2. Is such pipeline extendible to real-world large scale tasks?
3. Using 1, 2 can we create a fully differentiable approach for panoptic segmentation?

We answer all of the above questions with yes and show benefits, insights into training such hybrid pipelines.

**Contribution 1: Backprop through combinatorial optimization (CO) layers:** There has been much recent work in this direction but such methods were not previously applied to:

a. Large scale tasks

b. Non-optimal CO solvers.

We extend previous work of \[1\] to compute better gradient estimates and obtain faster convergence.

**Contribution 2: Transformation for backward pass:** Previous works for gradient estimation through CO `x* = \argmin_{x in C} <c,x>` apply loss on x\* and perturb costs c by incoming loss gradient on x\*. This is not the case for panoptic segmentation. Our scenario is `(x*, z*) = \argmin_{x in C, z in D(x)} <c,x>` and loss is applied on z\*. Here we need to perturb the costs associated with z\* (which does not exist). To remedy this problem we solve a *different* CO problem in the backward pass to compute gradients w.r.t c.

**Contribution 3:**  We show a differentiable surrogate of panoptic quality metric.

*TLDR*:
1. Backprop. possible through CO for large scale tasks even with non-optimal CO solver.
2. To achieve 1. we smooth gradients in backward pass
3. Solve another CO problem in backward pass for optimizing variables not appearing in objective.
4. Propose a Panoptic segmentation approach with fewer hyperparams. & better results than comparable approaches. Code available at: [https://github.com/aabbas90/COPS](https://github.com/aabbas90/COPS)

\[1\] - Black-box backprop: [https://arxiv.org/abs/1912.02175](https://arxiv.org/abs/1912.02175)",MachineLearning
r44hwo,1638106689.0,[D] Professional Voice AI,"Hi folks, I am looking for a realistic and informative Voice Changer / Text-to-Speech Tool.  I want to use it for producing professional 2D Ads, it should be based on AI and has to be really professional. Cost does not matter, the result must be very proficient. Do you guys know any reliable platforms/projects?",MachineLearning
r4001a,1638089621.0,[D] Which vectorizer do you use?,"When building a search engine system based on embedding retrieval, there are so many vectorizers to choose from! Which one do you use and why!

Also curious to hear about other use cases and which vectorizers others are using :)

[View Poll](https://www.reddit.com/poll/r4001a)",MachineLearning
r3t02h,1638064872.0,Unsupervised Topic Segmentation of Meetings with BERT Embeddings (Research Paper Walkthrough) [D],"In this paper, the authors propose a BERT based unsupervised topic segmentation method for the task of dividing multi-person meeting transcripts into topic blocks 🔥

Finally, the online meeting recordings are useful :D

Paper Walkthrough: https://youtu.be/uIdqcGNoI_o

Paper: https://arxiv.org/abs/2106.12978",MachineLearning
r3n2zl,1638047506.0,[D] The Inherent Limitations of GPT-3,"I wrote up a little editorial titled [the inherent limitations of GPT-3](https://lastweekin.ai/p/the-inherent-limitations-of-gpt-3). It is not negative towards GPT-3 (I hope), but rather lays out some of the basic facts on what its architectural constraints are - mainly so anyone worried it'll take their job or lead to AGI can find this and hopefully relax.

Would love feedback on it, especially any corrections!",MachineLearning
r3krw2,1638040985.0,[D] Paper Explained - Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions (Video Walkthrough),"[https://youtu.be/W2UT8NjUqrk](https://youtu.be/W2UT8NjUqrk)

Backpropagation is the workhorse of deep learning, but unfortunately, it only works for continuous functions that are amenable to the chain rule of differentiation. Since discrete algorithms have no continuous derivative, deep networks with such algorithms as part of them cannot be effectively trained using backpropagation. This paper presents a method to incorporate a large class of algorithms, formulated as discrete exponential family distributions, into deep networks and derives gradient estimates that can easily be used in end-to-end backpropagation. This enables things like combinatorial optimizers to be part of a network's forward propagation natively.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

4:25 - Sponsor: Weights & Biases

6:15 - Problem Setup & Contributions

8:50 - Recap: Straight-Through Estimator

13:25 - Encoding the discrete problem as an inner product

19:45 - From algorithm to distribution

23:15 - Substituting the gradient

26:50 - Defining a target distribution

38:30 - Approximating marginals via perturb-and-MAP

45:10 - Entire algorithm recap

56:45 - Github Page & Example

&#x200B;

Paper: [https://arxiv.org/abs/2106.01798](https://arxiv.org/abs/2106.01798)

Code (TF): [https://github.com/nec-research/tf-imle](https://github.com/nec-research/tf-imle)

Code (Torch): [https://github.com/uclnlp/torch-imle](https://github.com/uclnlp/torch-imle)",MachineLearning
r3k9b3,1638039597.0,"[Discussion] I wrote an article on inferencing with large transformers with GPUs instances with Databricks, how practical is that for most ML practitioners? What do you use when you want to run millions of rows through large transformer models? Is there such a need at all?","I need feedback so that I can write more helpful content for the community.

The article I wrote: [High-performance Inferencing with Large Transformer Models on Spark](https://towardsdatascience.com/high-performance-inferencing-with-large-transformer-models-on-spark-beb82e71ecc9)",MachineLearning
r3jz31,1638038795.0,"[D] How to edit images with GANs tutorial, Part 2: You don't need Photoshop when you got GANs","In this tutorial you will learn the intuition behind:

* Discovering editing directions in the latent space of a StyleGAN-2 generator
* Using these directions to manipulate generated images in a  meaningful way
* How this pipeline is exported into an actual image editor

Telegram post: [https://t.me/casual\_gan/199](https://t.me/casual_gan/199)

Blog post: [https://www.casualganpapers.com/gan-inversion-image-editing-free-photoshop-alternative/AI-assisted-Image-Editing-Part2.html](https://www.casualganpapers.com/gan-inversion-image-editing-free-photoshop-alternative/AI-assisted-Image-Editing-Part2.html)

[Image source: https:\/\/github.com\/orpatashnik\/StyleCLIP](https://i.redd.it/au7qxh2ro6281.gif)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries and GAN tutorials!",MachineLearning
r3j2j2,1638036275.0,[D] Are there any Simple hosted MLOps or auto scaling solutions?,"I’m looking to deploy VQGAN+CLIP models at a pretty large scale (30+ GPUs) and I want to explore options other than Kubernetes on AWS or GCP.

Is there a hosted MLOps platform where I can simply upload my ML Service (docker + flask) and the platform can completely take care of scaling and GPU provisioning by itself?


Essentially outsource scaling up/down and handling traffic to a product or service with me having to do minimal setup.",MachineLearning
r3g7kd,1638028126.0,[D] Looking for a sponsor for a functional programming language for new AI hardware,"As the 20s proceed, [novel AI hardware](https://www.reddit.com/r/MachineLearning/comments/kzsokz/d_list_of_novel_ml_hardware_companies_january_2021/) to replace GPUs will arrive at the scene, but by the looks of things Python + C++ seems like it will be a dominant combination for programming them. This is a huge pity as we could do better than languages created in the 80s and 70s for programming hardware created in the 2020s. I am trying to change this destiny with [Spiral](https://github.com/mrakgr/The-Spiral-Language).

Apart from some special features to control inlining and specialization which make it suitable for hardware with no dynamic memory allocation capability like the GPUs, Spiral is quite similar to languages like F# and OCaml. It has static typing, global type inference, first class functions, records, tuples, unions and more as any competent functional language would. Unlike toy languages, it has a well done language server - if you want to try out the language, it is as simple as installing its plugin in the VS Code marketplace. About 3 years of full time work went into it.

I want to make backends for novel AI chips for it, but those cost money which I do not have, and have restricted availability so I might not be able to get them even if I had the money. But various companies will have them, and if you are in the position of utilizing them and want something better than old, poorly designed languages for that kind of work consider [sponsoring](https://opencollective.com/spiral-collective) Spiral. At the very least, I'd need access to those chips in order to make a backend for them. It would really be a waste of my skills if I spent the next few years doing other things while Spiral languished in the background. I believe that had I made it back in the late 00s, it would have become the dominant language for programming GPUs and writing ML libraries in.

Background: See my [resume](https://docs.google.com/document/d/e/2PACX-1vSe88lhaoSZaF-YHKeiA_RlpkPwzLetrIAoY2v6ckbaLn59QQ95nvMC-Cc13hxDsLzVJ-KGE824S1bF/pub).

I am a master of functional programming, and I've been researching poker RL agents since early 2021 when Spiral was good enough for first release. I am quite good at implementing ML papers and have from scratch written a whole GPU based deep learning based library in the past for a previous iteration of Spiral. Making RL agents and a poker game served well to try out the new language and debug it, but pretty much all I've tried failed on full Holdem. To make it work I'd need significantly larger batch sizes, which would make training take too long on my GTX 970. So it is a matter of both not having good enough algorithms and not enough compute.

More broadly than just trying to make an agent for a gambling game, the massive parallelism afforded by these chips could be great for game AI in general. They could allow for simulating games with a large number of independently acting agents. Beyond deep learning, better ML algorithms of the kind the brain uses would also allow for approximately storing large amounts of procedurally generated world data that would be infeasible on current hardware. These algos will get here before long and when they do ML frameworks like PyTorch and Tensorflow will be obsolete. New ones will have to be written.

In the current time, I'd like to try out various things on these chips:

* Implement the Holdem game directly on them and see whether the anticipated 100-1000x speedups are enough to actually enough to make current algorithms tractable for toy games.
* Since the current algorithms are trash, try to evolve something better with the aid of these chips. Right now I would not even think of trying this on the GPU, but novel hardware might make this kind of brutish research tractable. I am too dumb to go beyond backprop directly, but I might be able to figure out the principles if I were given the algorithm up front.
* Unsupervised learning for making art and music. I've spent 6.5 years doing unpaid work mostly around ML, and I'd like to create something tangible like a game others could play for my next project. I am definitely tired of my old approach.

Of course, the above would be done in Spiral, and to make that work at the very least I'd need to create a ref counting C backend for the AI chip. And probably a Python backend that connects to it. This would not be hard for me.

Going into game development is my current path. In order to create my assets without the help of ML I am learning to 3D sculpt and draw on my own, and will move on to learning musical composition after I am done. But doing it all by hand would leave me with much regret and I really should be working on things related to AI. Trying to do game dev feels too much like a cope for not being able to hack it in RL. I still have a lingering attachment to my old path.

My offer to the community is thus - a better way of utilizing future hardware. How interested are you in, and in supporting this kind of work?",MachineLearning
r3esd8,1638023785.0,[D] (Paper Overview) Florence: A New Foundation Model for Computer Vision,"

**Video**

[**https://youtu.be/QNOTBMPecKM**](https://youtu.be/QNOTBMPecKM)

**Paper**

[**https://arxiv.org/abs/2111.11432**](https://arxiv.org/abs/2111.11432)

**Abstract**

 Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.",MachineLearning
r3aoz0,1638008129.0,[P][SP] The Sensory Neuron as a Transformer - Paper implementation [screencast],"Hey there!

I created a video where I tried to implement the paper [The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning](https://arxiv.org/abs/2109.02869) ""from scratch"". It is very much based on the [official code](https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron), however, I introduced multiple modifications and simplifications to be able to fit it in a single YouTube video. Most notably, it only focuses on the CartPoleSwingUp environment. Lastly, I ran a couple of experiments to test the permutation invariance and robustness to noise.

Hope some of you could find it useful! I would be happy to get any feedback or answer any questions!


Video link: [https://youtu.be/mi\_mzlhBGAU](https://youtu.be/mi_mzlhBGAU)",MachineLearning
r3ajsj,1638007504.0,[Discussion] Face Recognition remarkable papers," I'm new to face recognition. Can anyone help me with what the most recent remarkable papers in this field are? I'm started with faceNet, SpherFace, CosFace, and ArcFace, and I  want to continue with some new papers.",MachineLearning
r332qa,1637980187.0,[D] ICLR rebuttal deadline,"Sorry, I can't find this info online, the deadline is on 29th of November, but is it anywhere on earth or is there a specific time?",MachineLearning
r2w8o7,1637959053.0,[Discussion] Federated Learning in practice,"Hi!

Does  anyone know of any in-detail descriptions/surveys of FL deployments in practice? What type of aggregations do people use and how they ensure privacy? Do most deployments rely on tf-federated?

I tried googling around, but am struggling to find much information.

Thanks a lot!",MachineLearning
r2trln,1637951763.0,[D] How do pretrained tokenizers work?,"Hi all, this is my first post on this subreddit.

&#x200B;

I have been using the pretrained tokenizers available from the [huggingface/transformers](https://github.com/huggingface/transformers) library.  And they have been working well for my use case.

&#x200B;

However, I have not been able to clearly understand

1. Why do we need to pretrain tokenizers?
2. How are they pretrained? Like not the code, but the logic behind it.
3. How do they work?

I tried searching for relevant literature on Google as well as [aclanthology](https://aclanthology.org/) but nothing insightful has turned up.

&#x200B;

Can someone elucidate these concepts? Perhaps links to papers?

&#x200B;

Your help is deeply appreciated.",MachineLearning
r2qf51,1637942124.0,[Discussion] Ratings of 837 ICLR 2022 submissions have been changed during open discussion,"&#x200B;

**Updated data 12/01/2021**

[**https://github.com/weigq/iclr2022\_stats**](https://github.com/weigq/iclr2022_stats)

&#x200B;

The ICLR 2022 statistics are available here:

[https://guoqiangwei.xyz/iclr2022\_stats/iclr2022\_submissions.html](https://guoqiangwei.xyz/iclr2022_stats/iclr2022_submissions.html)

* **466** submissions have been withdrawn during 11/09  to 11/26 (# submissions from **3328** \-> **2862**).
* **837 / 2862** submissions' ratings have been changed by the reviewers.
* Top submissions where the ratings got lifted.

https://preview.redd.it/q6ly1ejzoy181.png?width=2380&format=png&auto=webp&s=93703f9bbc290223a07b8337adb6610d8bb577aa

* Top submissions where the ratings got droped unluckily.

&#x200B;

https://preview.redd.it/4r7xqascpy181.png?width=2378&format=png&auto=webp&s=b02b5cda8afd4c6c034178ac84ee70b1f0d4a1b0",MachineLearning
r2qcer,1637941882.0,"[R] Kwai, Kuaishou & ETH Zürich Propose PERSIA, a Distributed Training System That Supports Deep Learning-Based Recommenders of up to 100 Trillion Parameters","A research team from Kwai Inc., Kuaishou Technology and ETH Zürich builds PERSIA, an efficient distributed training system that leverages a novel hybrid training algorithm to ensure both training efficiency and accuracy for extremely large deep learning recommender systems of up to 100 trillion parameters.

Here is a quick read: [Kwai, Kuaishou & ETH Zürich Propose PERSIA, a Distributed Training System That Supports Deep Learning-Based Recommenders of up to 100 Trillion Parameters.](https://syncedreview.com/2021/11/26/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-153/)

The code is available on the project’s [GitHub](https://github.com/PersiaML/Persia). The paper *PERSIA: An Open, Hybrid System Scaling Deep Learning-based Recommenders up to 100 Trillion Parameters* is on [arXiv](https://arxiv.org/abs/2111.05897).",MachineLearning
r2mi5a,1637930767.0,[Research] Optimizing a kernel matrix,"I'm experimenting with the [kernel perceptron](https://en.wikipedia.org/wiki/Kernel_perceptron) with the [polynomial kernel](https://en.wikipedia.org/wiki/Polynomial_kernel) and was wondering if there was a way to optimize the kernel matrix representation of the training set.

Given an ordered training set `X` composed of `n` vectors from a high-dimensional real space, the kernel matrix `M` has size `nxn`, the cell at index `ij` contains the result of `K(Xi, Xj)` where `K` is the polynomial kernel and `Xi`, `Xj` are the training examples at index `i` and `j` respectively.

A couple of observations:

* `M` is a [Gram matrix](https://en.wikipedia.org/wiki/Gram_matrix), which means that is positive semidefinite and because it is computed from real values it is real itself.
* `M` is accessed **only** line by line, due to the structure of the perceptron.
* The total size of `M` increases with the square of the amount of training examples.

The access to the matrix is sequential, therefore no amount of memoization will suffice, storing the matrix in memory becomes very expensive very quickly (with a training set of 60000 examples `M` occupies 28.8GB with 64bit floats). Given the simple structure of this matrix and accessing method, I think is unlikely that there is no way to compute quickly a row without storing everything in memory.

The space-saving solution would be to compute each line when needed, but this is not very efficient because the algorithm is executed for a different number of epochs and with the execution may be involved some hyperparameter tuning in choosing the right kernel parameters. This solution impacts very much the performance, and also memory in cheaper than time.

A way to reduce the effective size of the kernel matrix would be to store only half of the matrix, because it is symmetric, and this technically works, in practice is almost as efficient as not storing the matrix at all, because computing a line can be done with vector-oriented operations on the CPU/GPU, where composing the line from existing data is a strictly sequential operation and generally doesn't work that well, in the end is not a good middle ground because you save half of the space (but the growth rate is unchanged), but the computation time for each line is almost the same as not storing the matrix.",MachineLearning
r2kiya,1637923558.0,"[D] Cross-validation, leave-one-out, and early stopping doubt"," Hi, I'm using 10-f cross-validation to evaluate the performance of my model on the whole dataset due to its size limit.

Same  thing with leave-one-group-out, which use as validation fold only  images coming from the same source, to evaluate if having images from  the same source in the train or the test introduces a bias.

With  this in mind, I'm using TensorFlow, and the early-stopping with restore  best weights set to TRUE. Looking at the results I'm getting a huge  improvement on average performances using LOO, and reasoning on it i can  understand why: early stopping focuses on reducing overfitting in each  fold, being the folds in LOO composed mainly by one to four images,  early stopping restore weights when my model is able to correctly  classify that images when only one image is in the validation fold, it  stops whenever it's able to correctly classify that image.

My  question is: can I use cross-validation/leave-one-out along with  early-stopping? Does it ""falsificates"" the results? I can't find any  argument about this in the literature or anywere.",MachineLearning
r2gl7w,1637907430.0,[P] Can I separate out the steps of learn() in stable baselines3?,"I'm working on a project where two  agents train simultaneously, but each agent only sometimes needs to make  a decision. Is it possible to have code that follows roughly the  following structure:

    model = A2C(""MlpPolicy"", env, verbose=1, learning_rate=0.0005)
    obs = env.reset()
    for i in range(2000000):
         action, _states = model.predict(obs)
         obs, rewards, dones, info = env.step(action)
        model.update_from_experience(obs, action, reward) #Does this type of function exist?

I'm also not married to  stable baselines to if there's a way to do this in another library that  would also be greatly appreciated.

Thanks!",MachineLearning
r2bbvl,1637889286.0,[D] What is Causal Machine Learning?,"I've been hearing this term a lot. And recently my advisor/supervisor asked me to look into it. But I just dont get the idea. Isnt causality supposed to be impact and effect of actions taken before any given time, and how they impact the current decision? Isn't this exactly RL and NLP? I don't understand why a new field is being defined as 'Causal Machine Learning'. Also what are some implications of research in this area? Is it just theoretical? I see a lot of language, vision and RL models (apparently) performing well already. Why do we need causality? Is this just a hype or really an important concept?

&#x200B;

Sorry if my questions are basic/simple. I just dont understand this 'causality' yet.

Thanks in Advance",MachineLearning
r29dks,1637882915.0,[D] Interactive Compute Platform Recommendations for ML Research,"I'm looking to switch to a small laptop while I travel that won't have much capacity for running experiments. I'd love to hear opinions of people who use interactive compute platforms. Specifically, I'm looking for some sort of VM I can ssh into and use like my computer, except I also want something that is easy to hook up with GPUs/TPUs with ideally an on-demand payment scheme.

I think Google, Microsoft, and Amazon all have some version of this, but there is so much to them it's hard to tell which one I should go with. Some key factors I'm looking for are ease of use, value per dollar, and the ability to easily upscale/downscale the amount of compute I'm using. Having the ability to open notebooks via the web would also be nice.",MachineLearning
r24rp7,1637869633.0,[D] Peer Review is still BROKEN! The NeurIPS 2021 Review Experiment (Yannic Kilcher),"Yannic Kilcher's thoughts on the 2021 NeurIPS reviewer experiment: https://www.youtube.com/watch?v=DEh1GR0t29k

Frankly, I agree with him completely. The review process is completely broken and arbitrary. Yes, phenomenal papers get accepted; but this is a given, and phenomenal papers will get traction whether they are published at a peer-reviewed venue or not. However, the sheer level of randomness with regards to the ""good but not phenomenal"" papers is a searing condemnation of the review process itself. And, as Kilcher discusses, it completely invalidates the notion of ""publishing as a metric of value"" with respect to PhD students, tenure track professors, grant applications, etc.",MachineLearning
r247qp,1637868131.0,[D] Dimensionality Reduction on dataset with linear and nonlinear features being used for regression,"I have a data set with 900 features and 12000 examples. I believe that some of the features have a linear relationship with the target variable and others have a nonlinear relationship.

I’m looking to reduce the input space to save time on training and prevent over fitting by using either feature selection or some sort of feature compression method. I believe PCA is out of the question because of its assumption of linear relationships amongst features. I’m currently looking at using Mutual Information to do this but I’ve never used this method before.

What are your thoughts?",MachineLearning
r23zpp,1637867515.0,[D] About adding tags to images in the Stylegan dataset,"(I'm not experienced in ML) I have read somewhere that a data set for Stylegan2/Stylegan3 training can be divided to separated ""classes"" or ""tags"", by putting images with common features to separated folders in the main folder and then making a tfrecord from this ""root"" folder. If you do so, can it actually improve quality of a model / stability of training?",MachineLearning
r21dxk,1637860525.0,[Project] Aim 3.1 - open-source Images tracking and Images explorer,"Hey r/MachineLearning !!

I am Gev, co-author of Aim.

Sharing with you the Aim v3.1 !!

The new version contains the following three notable changes:

* Images Tracking and Explorer
* Improved UI and backend performance
* Better runs navigation

Images tracking is a major item crossed off the [Aim roadmap](https://github.com/aimhubio/aim#roadmap).

[Images Explorer on Aim UI](https://preview.redd.it/fje614p1yr181.png?width=3460&format=png&auto=webp&s=9824edd3a41cbbad931ccc4924cd8e56f6994b95)

A short Aim UI Images explorer video:   [https://youtu.be/4mtUFV8yG\_o](https://youtu.be/4mtUFV8yG_o)

Code: [https://github.com/aimhubio/aim](https://github.com/aimhubio/aim)

Web: [https://aimstack.io](https://aimstack.io/)

Release Notes: [bit.ly/3cLz6HY](https://t.co/Xf9yp8fycW?amp=1)

&#x200B;

Would love your feedback on our work.

How can we make Aim better?",MachineLearning
r2075f,1637857336.0,[D] Not sure how to formulate this correctly: Has there been work done on learning what perception constitutes a reward and than using only that perception for learning?,"Let's say you have an agent in a maze searching for apples. Everytime it's viewpoint passes through an apple it gets a reward.

Has there been an attempt to make the agent first learn the association between ""passing through apple"" and ""reward signal"" and then using only that perception for further training?

I feel one of the big problems we have is that we still can't give very detailed feedback to our agents and the reason for that is that the reward signal is ""subsymbolic"" so to speak so the agent cannot communicate or reason over it.

If an agent can learn to recognise a reward in it's perception we could communciate more effectively.

I'm sure someone must have thought of something like this already, but I don't know the term to google for?",MachineLearning
r1zsaw,1637856198.0,[D] Graph as an Output of ML Modelling,"Hi, I am currently thinking about tackling a rather specific problem using ML (or any kind of statistical approach, really).

I have the problem that for a given set of nodes (with their names), I want to predict an (acyclic) graph of how these nodes will be arranged. I also have a set of meta attributes that put the graph into context, and I also have a set of historical graphs together with their meta attributes. The basic idea is that we assume that graphs with similar meta attributes will follow a similar pattern, also wrt. to their node names.

Ultimately I want to build a model that is able to forecast the resulting graph.

The only solution I could think off is building a model to forecast the single entries of the adjacency matrix, essentially fitting a function f(meta, node1attributes, node2attributes) -> 1 if node 1->node 2 else 0.


Is there any smarter way of doing this?",MachineLearning
r1z1ig,1637854191.0,[R] 70-Page Paper From Yoshua Bengio Team: GFlowNet Foundations,"In the new paper GFlowNet Foundations, a research team from Mila, University of Montreal, McGill University, Stanford University, CIFAR and Microsoft Azure AI builds upon GFlowNets, providing an in-depth formal foundation and expansion of the set of theoretical results for a broad range of scenarios, especially active learning.

Here is a quick read: [70-Page Paper From Yoshua Bengio Team: GFlowNet Foundations.](https://syncedreview.com/2021/11/25/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-152/)

The paper *GFlowNet Foundations* is on [arXiv](https://arxiv.org/abs/2111.09266).",MachineLearning
r1wirr,1637846823.0,[D] AISTATS 2022 Reviews are out (early),"I got caught off-guard this morning when AISTATS reviews woke me up. I got 5 reviews, which is pretty amazing. All very detailed and high quality. Thank you all reviewing and organizing.",MachineLearning
r1swhm,1637832897.0,[D] AAMAS 2022 Paper Reviews,I am creating a discussion thread for AAMAS 2022 reviews.,MachineLearning
r1sdpa,1637830762.0,[D] Is it possible to work as an independent contractor in ML?,"I am finishing my PhD in NLP and, if academia does not work out, I was thinking going self-employed, as a consultant or giving courses etc. I really value flexibility and independence (more than money) and I do not see myself in a 9-6 position.
I know a lot of people who are self-employed in tech, but they are mostly into web or mobile development.
Is it feasible to work as an independent contractor in ML? If so, what would you advise?

Edit: apparently I'm looking for a freelance job, not for an independent - contractor one :)",MachineLearning
r1kzu7,1637805195.0,[D] How to Build a Knowledge Graph with Neo4J and Transformers," Knowledge graphs are essential for information extraction. [In this article](https://walidamamou.medium.com/how-to-build-a-knowledge-graph-with-neo4j-and-transformers-72b9471d6969), we show how to build a knowledge graph from job descriptions using fine-tuned transformer-based Named Entity Recognition (NER) and spacy’s relation extraction models.

Enjoy the read and if you have any questions, leave them below.",MachineLearning
r1f130,1637787468.0,[D] how deep is your knowledge as a data science trust/MLE,"I think I’ve learned the applications of most major ML techniques (linear regression, logistic regression, GDA, Naive Bayes, SVMs, FFNN, LSTMs, GMMs, KNN, KMeans and a little tiny bit on transformers); however, I can’t say I understand all of these through and through. I know when and where to apply them but would really have to study the algorithm again before doing so to make sure I’m not violating any assumptions.

How much do other MLEs know and how deep is your understanding? Do you go as far as pulling out hidden states from LSTMs to feed to future states and try to completely derive new models or do you stick with the basics and tune well-known models?

I’m the first person to apply ML/DL at the company I work for and I’ve learned everything on my own but I still feel I know so little. I really don’t have anyone to compare myself to. I plan on moving on to another company soon but I’m scared of getting fired because I’m not up to par like the other MLEs at said company",MachineLearning
r1c03v,1637779235.0,[D] Paper Explained - Parameter Prediction for Unseen Deep Architectures (Video Interview w/ First Author Boris Knyazev),"[https://youtu.be/3HUK2UWzlFA](https://youtu.be/3HUK2UWzlFA)

Deep Neural Networks are usually trained from a given parameter initialization using SGD until convergence at a local optimum. This paper goes a different route: Given a novel network architecture for a known dataset, can we predict the final network parameters without ever training them? The authors build a Graph-Hypernetwork and train on a novel dataset of various DNN-architectures to predict high-performing weights. The results show that not only can the GHN predict weights with non-trivial performance, but it can also generalize beyond the distribution of training architectures to predict weights for networks that are much larger, deeper, or wider than ever seen in training.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

6:20 - DeepNets-1M Dataset

13:25 - How to train the Hypernetwork

17:30 - Recap on Graph Neural Networks

23:40 - Message Passing mirrors forward and backward propagation

25:20 - How to deal with different output shapes

28:45 - Differentiable Normalization

30:20 - Virtual Residual Edges

34:40 - Meta-Batching

37:00 - Experimental Results

42:00 - Fine-Tuning experiments

45:25 - Public reception of the paper

&#x200B;

ERRATA:

\- Boris' name is obviously Boris, not Bori

\- At 36:05, Boris mentions that they train the first variant, yet on closer examination, we decided it's more like the second

&#x200B;

Paper: [https://arxiv.org/abs/2110.13100](https://arxiv.org/abs/2110.13100)

Code: [https://github.com/facebookresearch/ppuda](https://github.com/facebookresearch/ppuda)",MachineLearning
r1ba74,1637777281.0,[D] GANs + Transformer = SOTA compositional generator? Compositional Transformers for Scene Generation explained (5-minute summary by Casual GAN Papers),"There have been several attempts to mix together transformers and GANs over the last year or so. One of the most impressive approaches has to be the GANsformer, featuring a novel duplex attention mechanism to deal with the high memory requirements typically imposed by image transformers. Just six months after releasing the original model, the authors deliver a solid follow-up that builds on the ideas for transformer-powered compositional scene generation introduced in the original paper, considerably improving the image quality and enabling explicit control over the styles and locations of objects in the composed scene. Could this model dethrone SPADE?

Full summary: [https://t.me/casual\_gan/195](https://t.me/casual_gan/195)

Blog post: [https://www.casualganpapers.com/gan-transformer-object-based-layout-generation/GANsformer2-explained.html](https://www.casualganpapers.com/gan-transformer-object-based-layout-generation/GANsformer2-explained.html)

[GANsformer2](https://preview.redd.it/ph3spfos1l181.png?width=2530&format=png&auto=webp&s=d0d59c630004d1d1e8b3364a978724e3d9ff14e4)


[arxiv](https://arxiv.org/pdf/2111.08960.pdf) / [code](https://github.com/dorarad/gansformer)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",MachineLearning
r18rjz,1637770569.0,[D] What are your long term career goals ? 10+ years,"Hello guys, I am trying to figure out long term goals for myself as I feel that I reached some plateau as senior ML engineer. I was wondering what other people in the field long term career goals were ?",MachineLearning
r17z2y,1637768372.0,"[R] DeepMind, Google Brain & World Chess Champion Explore How AlphaZero Learns Chess Knowledge","DeepMind and Google Brain researchers and former World Chess Champion Vladimir Kramnik explore how human knowledge is acquired and how chess concepts are represented in the AlphaZero neural network via concept probing, behavioural analysis, and an examination of its activations.

Here is a quick read:[DeepMind, Google Brain & World Chess Champion Explore How AlphaZero Learns Chess Knowledge.](https://syncedreview.com/2021/11/24/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-151/)

The paper *Acquisition of Chess Knowledge in AlphaZero* is on [arXiv](https://arxiv.org/abs/2111.09259).",MachineLearning
r15jix,1637761609.0,[D] Has anyone used Algorithmia as a model deployment tool? I'm unconvinced by it.,"[Algorithmia](https://algorithmia.com) advertises themselves as an MLops platform for data scientists, and they provide an easy way to host models on a scalable REST API.

This sounds like a perfect solution for a data scientist or hobbyist who wants to host models for cheap and not worry about the devops. But as I've gotten more familiar with it, I have more questions...

For the base tier, Algorithmia requires you to host your model's request handling code on a [Github repository owned by them](https://algorithmia.com/developers/algorithm-development/your-first-algo).  A separate repository for your request handling code seems like a strange pattern to develop in. They also encourage you to develop in their Web UI. Again, another pattern that feels forced.

They also have an ominous section in their [terms of service](https://algorithmia.com/api_dev_terms) that says: ""You do not transfer ownership of the Software to Algorithmia, but you do hereby grant Algorithmia ... [a] fully paid-up and royalty free license to use and permit others to use the Software"". Which feels overly aggressive for forcing you to use their source hosting.

Between an unnatural development environment and a sketchy ownership clause, I'm reluctant to continue using Algorithmia.

Has anyone had similar experiences with Algorithmia? Am I just being overly skittish and misinterpreting the ownership clause? Are their better repository patterns (git subtrees) that other people have used with them? Are there better companies to host models or should I have never even attempted leaving the AWS and GCP hosting land?",MachineLearning
r12kku,1637751369.0,[D] Federated Learning microblog (Part 2) + Annotated Paper,"I wrote another Twitter thread that goes deep on the math behind  Federated Learning,  how it is trained and how well it performs.

[Twitter Thread](https://twitter.com/shreyansh_26/status/1463454860460785670)

Annotated Paper -  [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/Communication-Efficient%20Learning%20of%20Deep%20Networks%20from%20Decentralized%20Data.pdf)

If you like it or have any feedback, do let me know!",MachineLearning
r0y56t,1637734118.0,[P] Python library to optimize Hugging Face transformer for inference: < 0.5 ms latency / 2850 infer/sec,"We just launched a new open source Python library to help in **optimizing** Transformer model inference and prepare **deployment in production**.

It’s a follow up of a proof of concept shared [on Reddit](https://www.reddit.com/r/MachineLearning/comments/qn8com/p_optimization_of_hugging_face_transformer_models/). Scripts have been converted to a Python library (Apache 2 license) to be used in any NLP project, and documentation has been reworked. We also added **direct** TensorRT support, which provides another boost in performance compared to the ORT+TRT backend. It will usually provide you with 5X faster inference compared to vanilla Pytorch, and up to 10X in specific cases. On a RTX 3090, perf\_analyzer reports over 2800 inferences per second throughput!

**Want to try it** 👉 [https://github.com/ELS-RD/transformer-deploy](https://github.com/ELS-RD/transformer-deploy)

The README includes some benchmarks on small, base and large transformer architectures to give you an idea of how large the benefit can be.

To learn more about the whole process you can also check [this article](https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c?source=friends_link&sk=cd880e05c501c7880f2b9454830b8915) showing how this open source library can beat some commercial product from Hugging Face company.

**Why this Python library?**

Basically, most tutorials on how to deploy in production a transformer model tell you to take FastAPI and put Pytorch inside. There are many reasons why it’s a bad idea, first of all, the inference performance is very low.

On the other side of the spectrum, there is Nvidia demos ([here](https://github.com/NVIDIA/TensorRT/tree/main/demo/BERT) or [there](https://github.com/NVIDIA/FasterTransformer)) showing us how to build **manually** a full Transformer graph (operator by operator) in TensorRT to get best performance from their hardware. It’s out of reach for many NLP practitioners and it’s time consuming to debug/maintain/adapt to a slightly different architecture (I tried). Plus, there is a secret: the very optimized model only works for specific sequence lengths and batch sizes. Truth is that, so far (and it will improve soon), it’s mainly for MLPerf benchmark (the one used to compare DL hardware), marketing content, and very specialized engineers.

The *usual* way to perform model optimization before deployment is to **automatically** convert your existing Pytorch / Tensorflow model to some kind of graph, apply some optimizations, and deploy the artefact in a production ready inference server.

For the optimization part, this project leverages both Nvidia TensorRT and Microsoft ONNX Runtime, then you can choose the best optimized models (benchmark is performed after optimizations). For the inference server, the library will generate the whole configuration for the Nvidia Triton inference server.

Triton is a mature tool, its API is clear, its documentation covers all typical use cases, etc. Some features may require some ML deployment knowledge but nothing complex. For TensorRT, it’s another story, the documentation is both vast and sometimes incomplete, its API evolves rapidly, there are many traps, like in the way you setup model precision or allocate memory in the GPU RAM. We have not found a single OSS project to take a random Hugging Face model and simply optimize it with TensorRT. Still, the tool provides the best performance, and we hope this library will help most NLP practitioners to benefit from it.

As we only target Hugging Face Transformer models, we have made the experience very simple, it only requires a single command line for the whole process!

If TensorRT and Triton are unknown to you, please find below 2 slides from the recent Nvidia GTC 2021 conference:

[from slides at https:\/\/reg.rainfocus.com\/flow\/nvidia\/nvidiagtc\/ap2\/page\/sessioncatalog\/session\/1629317744587001TJe7 ](https://preview.redd.it/ywlmfr3sgh181.png?width=1142&format=png&auto=webp&s=00752338c8348ecec742b3210fca354024b62bf6)

At the Amazon presentation, we learned that Amazon search and Amazon ads (aka the 💸💰🤑 generators) are also built over Triton inference servers.

[from slides at https:\/\/reg.rainfocus.com\/flow\/nvidia\/nvidiagtc\/ap2\/page\/sessioncatalog\/session\/16301005050970010fZk ](https://preview.redd.it/dpwumqpygh181.png?width=1492&format=png&auto=webp&s=a1b0fe73c8365b29a4482e7e2bb3e79b88382c76)

Still not enough to convince you that you may benefit from them?

Check that article from Microsoft where you will learn that Microsoft Bing is built over Nvidia TensorRT [https://blogs.bing.com/Engineering-Blog/october-2021/Bing-delivers-more-contextualized-search-using-quantized-transformer-inference-on-NVIDIA-GPUs-in-Azu](https://blogs.bing.com/Engineering-Blog/october-2021/Bing-delivers-more-contextualized-search-using-quantized-transformer-inference-on-NVIDIA-GPUs-in-Azu)

You got it, if ONNX Runtime, TensorRT and Triton are the big guys' tools, they may also help you in your own projects. **Let’s democratize them!**",MachineLearning
r0mq3l,1637700092.0,[D] OpenAI's Miles Brundage on AI Misuse and Trustworthy AI,"Some of you might find [this new Gradient interview with Miles Brundage](https://thegradientpub.substack.com/p/miles-brundage-on-ai-misuse-and-trustworthy) interesting.

Papers touched on:

* [Will Technology Make Work Better for Everyone?](http://www.slate.com/articles/technology/future_tense/2014/01/second_machine_age_by_andrew_mcafee_and_erik_brynjolfsson_discusses_work.html)
* [Economic Possibilities for Our Children: Artificial Intelligence and the Future of Work, Education, and Leisure](https://www.milesbrundage.com/uploads/2/1/6/8/21681226/brundage.pdf)
* [Taking Superintelligence Seriously](http://www.sciencedirect.com/science/article/pii/S0016328715000932)
* [The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation](https://arxiv.org/ftp/arxiv/papers/1802/1802.07228.pdf)
* [Release Strategies and the Social Impact of Language Models](https://arxiv.org/abs/1908.09203)
* [All the News that’s Fit to Fabricate: AI-Generated Text as a Tool of Media Misinformation](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3525002)
* [Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims](https://arxiv.org/abs/2004.07213)

Timeline:

(00:00) Intro
(01:05) How did you get started in AI
(07:05) Writing about AI on Slate
(09:20) Start of PhD
(13:00) AI and the End of Scarcity
(18:12) Malicious Uses of AI
(28:00) GPT-2 and Publication Norms
(33:30) AI-Generated Text for Misinformation
(37:05) State of AI Misinformation
(41:30) Trustworthy AI
(48:50) OpenAI Policy Research Team
(53:15) Outro",MachineLearning
r0mok4,1637699977.0,"[P] Nvidia releases web app for GauGAN2, which generates landscape images via text description, inpainting, sketch, object type segmentation map, and style image","[GauGAN2 blog post](https://blogs.nvidia.com/blog/2021/11/22/gaugan2-ai-art-demo/). [GauGAN2 web app](http://gaugan.org/gaugan2/). More links and other info are at [this comment from another post](https://www.reddit.com/r/MediaSynthesis/comments/r04he3/comment/hlqbr8u/).

Examples that I generated:

[Text description \\""trees turning color in autumn\\""](https://preview.redd.it/nc94ozsmme181.jpg?width=960&format=pjpg&auto=webp&s=e8432564f7f90f2b1bc44aa3b0d7e40c9a61e43a)

[Text description \\""a winter mountain landscape near sunset\\""](https://preview.redd.it/h7w3ke9ome181.jpg?width=960&format=pjpg&auto=webp&s=2b3d80814338834da228e9ad6c328b9d91034c5f)

[Text description \\""a stream\\""](https://preview.redd.it/5tzajw7vme181.png?width=1024&format=png&auto=webp&s=a2cc54c5c8d879213795fa6d72535a2fef76d7e4)

[Sketch computed by app from previous image](https://preview.redd.it/qw8z8o3lne181.png?width=1024&format=png&auto=webp&s=09bc6948e3f40342a9b31390b54a077c646b2146)

[Text description \\""a stream\\"" + sketch in previous image + style change using one of the app's style images](https://preview.redd.it/goulv18one181.png?width=1024&format=png&auto=webp&s=62cdd2bb6ba551ae0c4903aab62e36fb23c99523)",MachineLearning
r0k72x,1637693411.0,"[P] Walkthrough of Keras.Model Internals. Includes: distribution, performance optimizations, callbacks, training loop, and more.","The source for the keras.Model class has grown to be several thousand lines of code.  This makes it incredibly challenging to sift through, especially for beginners.

I wrote a blog post that walks through how the keras.Model class works under the hood.  The guide explains what various pieces of the model class do and implements a simplified version of the Model class.

The guide doubles as a tutorial on tf.distribute and batched execution.  After following the guide you'll be able to write a fully optimized custom training loop.

The post is available at [https://lukewood.xyz/blog/keras-model-walkthrough](https://lukewood.xyz/blog/keras-model-walkthrough).

There's a companion git repo:
[https://github.com/lukewood/modelwalkthrough](https://github.com/lukewood/modelwalkthrough)

Any feedback is appreciated, happy to incorporate changes & provide deeper explanations on specific components of the model class.  I've been using Keras for many years so I may have glossed over some details.  Please let me know if this is helpful to anyone!",MachineLearning
r0jl0p,1637691792.0,[D] Has anyone heard of the AI Foundation? How does there NLP work?,"[https://aifoundation.com/about/](https://aifoundation.com/about/)

I'm curious as to how their NLP works. They seem like they are doing some sort of language personalization on an individual level. Looking at their virtual beings demo, they look way more advanced than what the big tech companies have out...unless this is an extremely cherry picked example.

[https://www.youtube.com/watch?time\_continue=445&v=rfipRaRai1Y&feature=emb\_logo&ab\_channel=VirtualBeings](https://www.youtube.com/watch?time_continue=445&v=rfipRaRai1Y&feature=emb_logo&ab_channel=VirtualBeings)

I would like to believe this is not cherry picked or a marketing stunt, but I also can't find any papers or technical details so I can't be sure.

&#x200B;

Edit: omg I misspelled their in the title...",MachineLearning
r0ht8v,1637687015.0,"[P] Text-To-Paint: Connecting Neural Painters with CLIP. Goal: paint telling the machine in natural language what to paint. Neural Painters paint using strokes and colors, rather than generating images at the pixel level.","I've been working in a text-to-paint together with [diavlex](https://twitter.com/diavlex_ai) in an Ai+Art collaboration. The idea is to specify a prompt in natural language ant tell the machine what to paint using strokes and not a pixel level, as an human artist would do.

Here is the overview of the implementation: [https://www.libreai.com/diavlex-text-to-paint-using-neural-painters-and-clip/](https://www.libreai.com/diavlex-text-to-paint-using-neural-painters-and-clip/) .

The links to the notebook are below so you can try it yourself. Note that it is the same notebook hosted in Kaggle and Colab. If you are part of Kaggle's community, it is probably a better option given that a currently Kaggle offers a \`NVIDIA TESLA P100\` for all notebook sessions. Whereas using Google Colab you might get a less powerful GPU for your session.

\* Notebook at Kaggle : [https://www.kaggle.com/bluebalam/diavlex-text-to-paint-neural-painters-clip/](https://www.kaggle.com/bluebalam/diavlex-text-to-paint-neural-painters-clip/)

\*  Notebook at Colab: [https://colab.research.google.com/drive/1UI9pKM-nxPMDet16NfnMPCuQyh2L1OGq](https://colab.research.google.com/drive/1UI9pKM-nxPMDet16NfnMPCuQyh2L1OGq)

## TL;DR

The following steps capture the essence of our idea, note that we use pseudocode based on Python, which does not necessary reflect the models' API. Please have a look to the notebook itself for the actual code and methods used.

1. Specify what to paint, e.g.,
prompt = ""black sheep""
2. Encode the text using CLIP's language portion to obtain the text features
text\_features = clip\_model.encode\_text(prompt)
3. Initialize a list of brushstrokes or actions
 and ask the neural painter
 to paint on a canvas. At the beginning the canvas will look random.
canvas = neural\_painter.paint(actions)
4. Use the vision portion of the CLIP model to extract the image features of this initial canvas.image\_features = clip\_model.encode\_image(canvas)
5. The goal is to teach the neural painter to modify the strokes (i.e., its actions) depending on how different is what it is painting to the initial text request (prompt
). For example, in the perfect case scenario, the cosine similarity between the text and image feature vectors should be 1.0.
Using this intuition, we use as the loss to guide the optimization process the the cosine distance, that measure how different the vectors are. The cosine distance in our case corresponds to
loss = 1.0 - cos(text\_features, image\_features) .
6. We minimize this loss adapting the neural painter actions
 that in the end should produce a canvas as close as possible to the original request.

# Enjoy Neural Painting! ;)

&#x200B;

Example for prompt ***""black sheep""*** showing the evolution during the optimization. Only 13 strokes used:

&#x200B;

&#x200B;

[Paint for prompt: \\""black sheep\\"".](https://i.redd.it/scwyca33kd181.gif)

And the last canvas 'painted' stroke by stroke:

&#x200B;

[Paint stroke by stroke for prompt \\""black canvas\\"" using 13 strokes only.](https://i.redd.it/hwm7y6p8kd181.gif)

**\~fin\~**",MachineLearning
r0gnej,1637683916.0,"[R] One sentence highlight for every NeurIPS-2021 Paper, plus code for 200 of them","Here is the list of all >2,300 NeurIPS 2021 (Neural Information Processing Systems) papers, and a one sentence highlight for each of them. NeurIPS 2021 will be held online from Dec 06.

highlights: [https://www.paperdigest.org/2021/11/neurips-2021-highlights/](https://www.paperdigest.org/2021/11/neurips-2021-highlights/)

code: [https://www.paperdigest.org/2021/11/neurips-2021-papers-with-code-data/](https://www.paperdigest.org/2021/11/neurips-2021-papers-with-code-data/)",MachineLearning
r0g2o2,1637682331.0,[D] AI Safety Needs Great Engineers,"**Top line: If you think you could write a substantial pull request for a major machine learning library, then major AI safety labs want to interview you** ***today.***

I work for [Anthropic](https://www.anthropic.com/), an industrial AI research lab focussed on safety. We are bottlenecked on aligned engineering talent. *Specifically* engineering talent. While we'd always like more ops folk and more researchers, our safety work is limited by a shortage of great engineers.

I've spoken to several other AI safety research organisations who feel the same.

## Why engineers?

May last year, OpenAI released GPT-3, a system that did surprisingly well at a surprisingly broad range of tasks. While limited in many important ways, a lot of AI safety folk sat up and noticed. Systems like GPT-3 might not themselves be the existential threat that many of us are worried about, but it's plausible that some of the issues that will be found in such future systems might already be present in GPT-3, and it's plausible to think solving those issues in GPT-3 will help us solve equivalent issues in those future systems that we are worried about.

As such, AI safety has suddenly developed an empirical subfield. While before we could only make predictions about what might go wrong and how we might fix those things, now we can actually run experiments! Experiments are not and should never be the entirety of the field, but it's a new and promising direction that leverages a different skill set to more 'classic' AI safety.

In particular, the different skill set it leverages is **engineering**. Running experiments on a real - if weak - AI system requires a substantial stack of custom software, with projects running from hundreds of thousands to millions of lines of code. Dealing with these projects is not a skillset that many folks in AI safety had invested in prior to the last 18 months, and it shows in our recruitment.

## What kind of engineers?

Looking at the engineers at Anthropic right now, every one of them was a great software engineer prior to joining AI safety. Every one of them is also easy to get on with. Beyond that, common traits are

* experience with distributed systems
* experience with numerical systems
* caring about, and thinking a lot about, about AI safety
* comfortable reading contemporary ML research papers
* expertise in security, infrastructure, data, numerics, social science, or one of a dozen other hard-to-find specialities.

This is *not* a requirements list though. Based on the people working here already, 'great software engineer' and 'easy to get on with' are hard requirements, but the things in the list above are very much nice-to-haves, with several folks having just one or none of them.

Right now [our job listings](https://jobs.lever.co/Anthropic) are bucketed into 'security engineer', 'infrastructure engineer', 'research engineer' and the like because these are the noun phrases that a lot of the people we like identify themselves with. But what we're actually most concerned about are [generally-great software engineers](https://jobs.lever.co/Anthropic/436ca148-6440-460f-b2a2-3334d9b142a5) who - ideally - have some extra bit of deep experience that we lack.

## How does engineering compare to research?

At Anthropic there is no hard distinction between researchers and engineers. Some other organisations retain the distinction, but the increasing reliance of research on substantial, custom infrastructure is dissolving the boundary at every industrial lab I'm familiar with.

This might be hard to believe. I think the archetypal research-and-engineering organisation is one where the researchers come up with the fun prototypes, and then toss them over the wall to the engineers to clean up and implement. I think the archetype is common enough that it dissuades a lot of engineers from applying to engineering roles, instead applying to research positions where they - when evaluated on a different set of metrics than the ones they're best at - underperform.

What's changed in modern AI safety is that the prototypes now require serious engineering, and so prototyping and experimenting is now an engineering problem from the get-go. A thousand-line nested for-loop does not carry research as far as it once did.

I think this might be a hard sell to folks who have endured those older kinds of research organisations, so here are some anecdotes:

* The first two authors on GPT-3 are both engineers.
* Some of the most pure engineers at Anthropic spend weeks staring at learning curves and experimenting with architectural variants.
* One of the most pure researchers at Anthropic has spent a week rewriting an RPC protocol.
* The most excited I've ever seen Anthropic folk for a new hire was for an engineer who builds academic clusters *as a hobby*.

## Should I apply?

It's hard to judge sight-unseen whether a specific person would suit AI safety engineering, but a good litmus test is the one given at the top of this post:

>*With a few weeks' work, could you - hypothetically! - write a new feature or fix a serious bug in a major ML library?*

Are you already there? Could you get there with a month or two of effort?

I like this as a litmus test because it's very close to what my colleagues and I do all day. If you're a strong enough engineer to make a successful pull request to PyTorch, you're likely a strong enough engineer to make a successful pull request to our internal repos.

Actually, the litmus test above is only one half of the actual litmus test I give folk that I meet out and about. The other half is

>*Tell me your thoughts on AI and the future.*

with a pass being a nuanced, well-thought-out response.

## Should I skill up?

This post is aimed at folks who already can pass the litmus test. I originally intended to pair it with another post on skilling up to the point of being able to pass the test, but that has turned out to be a much more difficult topic than I expected. For now, I'd recommend starting with [80k's software engineering guide](https://80000hours.org/career-reviews/software-engineering/).

## Take homes

We want more great engineers.

If you could write a pull request for a major ML library, [you should apply to Anthropic](https://jobs.lever.co/Anthropic/436ca148-6440-460f-b2a2-3334d9b142a5).

If that's not you but you know one or more great engineers, ask them if they could write a pull request for a major ML library. If yes, tell them to apply to Anthropic.

If that's not you but you'd like it to be, watch this space - we're working on skilling up advice.

[*This is a twinned version of this post on LessWrong*](https://www.lesswrong.com/posts/YDF7XhMThhNfHfim9/ai-safety-needs-great-engineers)",MachineLearning
r0g2h0,1637682316.0,[R] Microsoft’s DeBERTaV3 Uses ELECTRA-Style Pretraining With Gradient-Disentangled Embedding Sharing to Boost DeBERTa Performance on NLU Tasks,"Microsoft releases DeBERTaV3, improving the original DeBERTa model using ELECTRA-style pretraining with gradient-disentangled embedding sharing to achieve better pretraining efficiency and a significant performance jump.

Here is a quick read: [Microsoft’s DeBERTaV3 Uses ELECTRA-Style Pretraining With Gradient-Disentangled Embedding Sharing to Boost DeBERTa Performance on NLU Tasks.](https://syncedreview.com/2021/11/23/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-150/)

The code is available on the project’s [GitHub](https://github.com/microsoft/DeBERTa). The paper *DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing* is on [arXiv](https://arxiv.org/abs/2111.09543?context=cs).",MachineLearning
r0fnl3,1637681202.0,[D] Term for specifically trained model per object,"For my use case, I train and use a specific model for each object, such as a building. Is there some term that defines this case versus using one model for a task, which is then applied for every building?",MachineLearning
r0ezps,1637679354.0,[Discussion] What is the biggest computer vision model ever opensourced?,"I'm wondering that everyone is telling that they trained a multi billion parameter model. But none of them opensourced, like FBs SEER.

What is the biggest computer vision model ever released?

Only ru-dalle is in my mind.",MachineLearning
r0edbv,1637677527.0,[D] Deviation analysis between two 3D meshes?,"Hi folks! Looking for some guidance / places to start researching.

I'm playing around with metal additive manufacturing using metal/polymer filament. Basically, you 3D print a model using the filament, debind the part (i.e. slowly burn out the plastic component) then raise the furnace up to sintering temperatures.  The remaining metal particles sinter and bind together, giving you a metal component.

These components suffer fairly severe shrinkage during the debinding and sintering process, anywhere from 10-20%.  Globally pre-scaling the part will get you close to desired dimensions, but individual features on the component can shrink more or less than expected. E.g. long, thin features shrink or warp more than thick features, holes and infill can affect surrounding areas, presence or absence of supports, etc etc.

I'd like to explore some ML techniques to scale the model more intelligently. I'm not sure the best way to turn this into a tractable problem though.

1. Print, debind, sinter part
2. 3D scan the resulting part
3. Convert the original model and 3D scan into something that can be fed to a network... 2D depth maps from multiple angles? Or perhaps do a deviation analysis between the two meshes first and convert that into some kind of 2D representation? Slice the scan and compare each slice against the model?
4. Get result and iterate with the next version
   1. In reality, I'll probably print/sinter multiple models at the same time in a batch format to help speed things up

Does this sound even remotely feasible? My main stumbling block is getting the 3D data into something usable, I'm not sure how this is typically handled. I assumed it would be better to translate into something 2D instead of dealing with meshes or point clouds, but perhaps that's incorrect?

I'm also assuming some kind of deep learning is the right approach here, but happy to investigate something else (including non-NN techniques!)

Happy for any tips, ideas, papers, techniques, etc to start researching. Thanks!",MachineLearning
r09639,1637658159.0,[N] DeepSquare News,"**DeepNews** is our weekly update where we bring the latest news from the **DeepSquare Project** and our quest to develop **sustainable High-Performance Computing as a Service ecosystem**. Last week, we talked about Artificial Intelligence and & High Performance Computing with Dr. Florin Dzeladini. Today, we will delve into DeepSquare’s heart and soul, **sustainability**, by the hand of one of our board members, **Frédéric Juillard**.

If you have been following the Project for a while, you will have realized our strong focus in sustainable HPC and energy reuse, and that is because we truly mean it. In DeepSquare, we are determined to take the next step towards sustainable HPC as a service and offer the best environmentally friendly alternative to the current status quo. Through the creation of a decentralized infrastructure backed by a professional team and cutting-edge technology, we have managed to create a growing community whose values align with our vision.

# Project Update

We are happy to be able to share the news that **DeepSquare supported our friends in** **ALAN analytics** **in delivering a project they worked on together with artist** **Agnieszka Kuran****t**. The project resulted in AI-generated “new” cave art based on a collection of data documented by paleoanthropologist Genevieve von Petzinger. **We used our test environment, and csquare application, to enable AI generated cave paintings**. It was really interesting to combine HPC, AI, and art, and merge it into this fantastic project.

# Meet the Team

**Frédéric Juillard** is **one of DeepSquare’s board directors** and a serial entrepreneur with an engineering background and sustainable deep-tech. Having previously been involved in the foundation of other high-tech companies, he naturally fits within the character of the DeepSquare association, and his day-to-day work includes **supporting the business and administration team, and leading partnerships**. Frédéric is, above all, a team player, and believes **sustainability to be “a key factor” of the Project** and the bedrock around which to gather a strong community:

>*“My favorite aspect is to be a part of this shared dream that we all have in the Project, and that more people believe in every day. To see our community grow and on board with the same vision is something that I truly appreciate. This social aspect, interacting with people from very different backgrounds, cultures and nationalities is something that perfectly aligns with the core values of DeepSquare and the world of today”*

**The community is indeed a key point** for the future of sustainable HPC as a service. Every day, more people are aware of the challenges coming ahead for the **HPC industry, which is already consuming a vast amount of the energy** produced on the planet, and will continue to grow in the [next decade](https://www.mdpi.com/2078-1547/6/1/117). Frédéric understands that only a shared effort rooted in a “trustful philosophy, and backed by an extremely well-prepared team”, can face these future demands:

>*“The compute power that our world needs today is something absolutely crazy, and these gross numbers are only going to keep growing. Society will shortly need to meet the demand for new technologies such as 5G, autonomous cars, Smart Cities and ever-increasing connectivity. Sustainability is something that every innovative business should integrate, there are clearly a lot of things to improve in HPC in terms of sustainability and I am certain DeepSquare is part of the solution”*

To overcome these challenges, the Project has **the latest hardware in the industry and a conscious plan to regularly update it** to match the industry’s demands, always with sustainability in mind. Frédéric believes it is “extremely important to find the right channel so this hardware can have a second life” and thus mitigate the damaging consequences of thoughtless IT hardware disposal. **DeepSquare clusters are thus designed with a green mindset**, **but also tackle the growing needs of compute power coming down the line**:

>*“Standard hardware is already extremely efficient when converting a Watt of electricity into a Watt of heat, and, by using an immersion-cooling system, along with a system to reintroduce that heat into the building, we have the best way to make a good and ecological use of otherwise wasted energy. That’s clearly the most important aspect where the Project can tackle the environmental problem and why, in DeepSquare, we deliver sustainable High Performance Computing as a service”*

With the support of our community, we are every day closer to **democratizing high-performance computing** with better execution, cost, and social responsibility. To learn more about DeepSquare, check out our website. Or, if you want to connect with the team and the community, follow us on Twitter, LinkedIn or join our Telegram group.",MachineLearning
r086ev,1637654087.0,"[D] What Regularizing Method to Use with a Batch Size of 1, besides Dropout?","Right now, I'm dealing with transformers with a good amount of encoder and decode layers. As a result, due to my limited memory, I can only have one batch of the time. Given that I've heard dropout is not as popular right now, I heard that I should try another method; what regularizing method do you recommend that works with a batch size of 1(so batch normalization probably won't work)?",MachineLearning
r07nms,1637652002.0,[P] Inference server for GPT-J 6B (on HuggingFace) with optimizations to fit into G4dn.xlarge,"Hi, I created an inference server for GPT-J 6B (on HuggingFace) using FastAPI: [https://github.com/TensorBox/gpt-j-api-huggingface](https://github.com/TensorBox/gpt-j-api-huggingface) . The main challenge here was making sure that the model fits in the smallest G4dn instance on AWS (with 16Gb of RAM and VRAM) to save on the server costs.

Check it out and hope you find it helpful. Here is also a write-up that explains what optimizations were applied: [https://www.tensorbox.ai/gpt-j-api](https://www.tensorbox.ai/gpt-j-api)",MachineLearning
r06rs4,1637648526.0,[D] What are some strategies to deal with label sparsity when training a protein function prediction model?," The protein function prediction task requires you to take a sequence of amino acids (think words in a sentence, but if there are only 20 words), and output the functions that protein can take. There are around 30 thousand labels for protein function, and these labels are not mutually exclusive, so protein function prediction is essentially a huge binary prediction multitask. Now the catch is some labels are very common, and others are very rare, and overall a protein is only labeled with a small number of labels out of the 30 thousand labels that biologists came up with in total. So if you represent each protein's functions as a binary vector, where each 1 entry represents a function it carries, then all vectors will be extremely sparse, and the majority of labels will only have a few positive examples in the training set, which leads to extreme label imbalance. What are some strategies I can use to deal with these problems?",MachineLearning
r05n9v,1637644426.0,[N] Scaling Law for Recommendation Models: Towards General-purpose User Representations,[https://arxiv.org/abs/2111.11294](https://arxiv.org/abs/2111.11294),MachineLearning
r04dcs,1637640136.0,[D]How to apply IS or FID score on GAN that generate time series instead of images data?,"Hello, I'm working on using GAN generating time series data.

It is very common to use ""Inception Score"" or ""Frechet Inception Distance Score"" to evaluate the performance of GAN. However, both of them rely on **Inception model**, which is **designed for 2D image data and could not be applied to 1D time series data**.

A straightforward way to solve this problem is to train a time series classifier in substitute of the original Inception Model, like this paper [TSGAN](https://arxiv.org/abs/2006.16477) did. But lacking of enough data and computation resource, it seems not be be a good choice for me.

Another way is, using STFT to transform the 1D signals to 2D images and apply FID-score method on those 2D images. It might be a feasible solution, but I don't know if it really make sense. A STFT-image has absolutely no similarity with image from ImageNet, on which the Inception-V3 is pre-trained. So it's questionable whether the result produced by the InceptionV3 makes any sense.

(；′⌒\`)",MachineLearning
r0428v,1637639146.0,"[D] Regarding PhD admissions in ML, how much will not having a first-authored publication hold me back?","Title is the question. I'm graduating with a master's in CS and don't have a first-authored publication. My undergraduate wasn't in CS so I pretty much learned how to actually code when I started my master's. I managed to get my name on 3 conference proceedings and 1 preprint, and also helped our lab's team win an award in a large ML competition recently, but alas I don't have a first-authored publication.

I recently attended a conference and talked with some PhD students there and many of them said that it's pretty much a red flag in this day and age if a PhD applicant doesn't have a first-authored paper. I'm wondering how much truth there is to this? Obviously having one would be the best scenario, but how much would not having one hold me back?

Thanks.",MachineLearning
r01mcs,1637631433.0,[D] Source Code,"Hi guys! I'm looking for the source code of this paper: [https://arxiv.org/abs/1602.00328](https://arxiv.org/abs/1602.00328)

I reached out to one of the contributors, and I will wait for a response. However, do other methods to obtain the source code of this paper exist? Thanks!",MachineLearning
r015mw,1637629938.0,[D] Death threat or cat meme? Why context matters in machine learning,"Is this a [death threat](https://imgur.com/vGugXHo) — or a [cat meme](https://imgur.com/LaqHh0O)?

Is this post [rooting for cancer](https://imgur.com/OPnG8kj) — or it's [demise](https://imgur.com/KQXdQ42)?

([More examples here for those interested](https://www.surgehq.ai/blog/why-context-aware-datasets-are-crucial-for-data-centric-ai))

Context can be absolutely crucial when building ML models. How can we expect machines to effectively understand and analyze our world when we're only training them with isolated data?

Then again, it's complicated and computationally expensive to build these context-inclusive models.

For those who are actively building ML, would your use-case benefit from including context? Would love to hear any and all experiences on the topic.",MachineLearning
qzvh6i,1637614267.0,[D] What are some design patterns that are actually fairly used in production ML?,"Hi.

I was wondering what are some of the design patterns that you have used in your code in a producation grade ML?

Like I was wondering about maybe going for factory pattern and builder pattern for instantiating my model objects might be a good idea.

Theres' also the Strategy pattern that I think can have its uses. But overall I'm nor that well versed in patterns, nor in production ML. What is your experience in this regard?",MachineLearning
qzsrtq,1637607444.0,[D] What to do when an Area Chair posts a new review of your paper 6 hours before the author rebuttal deadline?,"Genuinely open to advice/feedback on this weird situation.

If the Area Chair over your submitted conference paper posts a new review of your paper two weeks after the initial reviews were released and 6 hours before the deadline for you, as the author, to make any new changes to your PDF, what would you do? Contact the Program Chair about the oddity of the circumstances?

The situation is complicated even more by the fact that two of the original reviewers of my paper (marginally voting to accept - 6's) have not yet responded to my rebuttals (to their initial reviews). I believe I have addressed all three original reviewers' main concerns/questions (and Reviewer 2 even raised their score by a few points in response to my rebuttal, bringing it to a borderline reject (5) with one small suggestion to raise their score), so, before this new review was released, I was anticipating positive responses from the original reviewers. However, now that a new review has been posted last-second, I am a bit concerned that the scope and trajectory of the discussion forum will be diverted and diluted as a result of my new reviewer's opinions on how the paper should instead be structured (which, notably, are quite divergent from all three original reviewers' opinions).

Any thoughts on this scenario? For instance, would you consider it ethical to release a paper review less than 24 hours before the author has to post a formal rebuttal in response?",MachineLearning
qzsrdw,1637607415.0,[D] Test set - just a glorified validation set?,"Everything seems to click really well when it comes to training and validation sets, but the ideology behind a completely held-out test set has never really sat well with me. At the most basic level I guess a test set is there so that you don't cheat, but, at least to me, it becomes a lot more nuanced than when you start thinking about how it's used.

Let's say that you've trained a model that you're proud of, and as a final sanity check, you test its performance on the test set to see how well the model generalizes on unseen data. If the model performs poorly, what kind of decision-making does this inform? Do you start over? Do you tweak the model and try to figure out what's going wrong? As soon as you tweak a single thing, doesn't the test set instantly become some kind of glorified validation set?

Here are some of my thoughts on the matter, and I'd be interested in hearing from other people too!

We know that a test set **should not affect** the model, otherwise it just acts as a validation set. Thus, if we act on a bad test result and change the model, the test set becomes a validation set, although, it is not as involved as a validation set that is used for early stopping or parameter tuning.

In other words, **a test set must be useless**! The moment it is useful, it becomes a validation set. Although to be more precise, a test set is not THAT useless because it probably lowers your expectation about the later performance of the model in production.

As an example, in a Kaggle competition, the final set is a ""test set"" since it does not have any involvement with model training. However, as soon as the final leaderboard is announced, that test set becomes a validation set; e.g., it affects which algorithms we later choose, i.e. those of top competitors.

In summary, it seems that most of the time we are using less-involved validation sets to double-check more-involved validation sets.",MachineLearning
qzsgma,1637606635.0,Having a hard time finding a dataset for my project [P],"I am in need of a dataset which segments images into colours (not classifying a whole image as one colour) and am having a very hard time finding one. This is especially true as I need for shadows and anti-shadows (or light sources) to also be labelled in the dataset. Just posting this here before I go about having to create my own dataset, any help is appreciated!",MachineLearning
qzr5z6,1637603300.0,"[D] For those of you working as NLP Engineers in Industry, what should you learn to get up to par?","Basically I have some interviews coming up for some NLP focused MLE jobs. I'm familiar with basic stuff like word embeddings, RNN/LSTMs, and a bit about transformers. However, my knowledge of the latter is pretty shallow and there's definitely a ton more I need to learn.

Currently I'm working through Stanford's CS224n problem sets and projects, but my current schedule kind of forces me to skim over most of it.

I'm also working on building an NLP web app that uses a GPT-2 model to generate the onion articles.

I'm familiar with ML (worked as MLE before, though mostly computer vision stuff) and currently work as an MLOps engineer, but mostly avoided NLP until recently.

For those of you who currently work as NLP engineers, what should you know to really qualify as an NLP engineer (without an MS/PhD!).

&#x200B;

Edit: CS224n not CS22n",MachineLearning
qzostq,1637597129.0,[P] Question Regarding Bounding Boxes in Dataset,"Hi,

I am relatively new to ML and I am looking at training my own model for use as part of a License Plate/ANPR program.

For my dataset, I am generating random characters (in the same font as the UK plate) in a rectangle and placing these on random backgrounds from the SUN Dataset, see below examples:

&#x200B;

https://preview.redd.it/8h2fp6lh76181.png?width=911&format=png&auto=webp&s=cb4f6222cbc9701dfefece3f3507de90167d456b

For the bounding boxes for these images can I use the exact cooridnates used in the generation of the fake plates or do I need to use a typical box, by this I mean can I give co-ordinates that may draw an irregular rectangle rather than the style I am seeing in packages such as LabelImg - would I have to use the same as the black box, or could I use the green style for training:

&#x200B;

https://preview.redd.it/m6dfjz6i76181.png?width=317&format=png&auto=webp&s=f3cbd92a9c1e3f9780f01cf59cefa638a5d83519",MachineLearning
qzoad5,1637595810.0,[P] DataProfiler - Scaleable Sensitive Data Detection & Analysis on Structured & Unstructured Files,"Hello all,

We created a library to be the one-stop shop for data exploration and monitoring --

[https://github.com/capitalone/dataprofiler](https://github.com/capitalone/dataprofiler)

The project had two objectives:

1. Quickly and accurate (cheaply) identify sensitive data (PII/NPI) in datasets.
2. Generate data profiles which can be utilized in downstream (ML) applications

Regarding sensitive data detection, we published a workshop paper on the model within the library:

[Sensitive Data Detection with High-Throughput Neural Network Models for Financial Institutions](https://aaai-kdf.github.io/kdf2021/assets/pdfs/KDF_21_paper_10.pdf)

In addition to sensitive data detection, the library also calculates statistical features and general characteristics of a dataset. This has helped our team quickly evaluate datasets, but also enabled the profiles use in downstream applications.

Some nifty features the community may be interested in:

* [Load files with a one command](https://capitalone.github.io/DataProfiler/docs/0.7.2/html/data_readers.html)  \- `data = dp.Data(filename)`
* [Profile data with a single command](https://capitalone.github.io/DataProfiler/docs/0.7.2/html/profiler.html) \- `profile = dp.load(data)`
   * [Save & Load](https://capitalone.github.io/DataProfiler/docs/0.7.2/html/profiler.html#saving-and-loading-a-profile) profiles: [`profile.save`](https://profile.save)`()` & `dp.Profiler.load(filename)`
   * [Merge](https://capitalone.github.io/DataProfiler/docs/0.7.2/html/profiler.html#merging-profiles) profiles `profile1 + profile2`
   * [Compare](https://capitalone.github.io/DataProfiler/docs/0.7.2/html/profiler.html#profile-differences) profiles: `profile1.diff(profile2)`
* [Extending the current entity detection model with transfer learning](https://capitalone.github.io/DataProfiler/docs/0.7.2/html/data_labeling.html#extending-a-data-labeler-with-transfer-learning) is easy and takes only a few lines of code (or retrain from scratch).
* It's possible (though a tad rough) to [add a new custom model for entity detection](https://capitalone.github.io/DataProfiler/docs/0.7.2/html/add_new_model_to_data_labeler.html)

Generally, we are looking for feedback and curious what the community thinks of the project?",MachineLearning
qznqdl,1637594342.0,[R] Microsoft Asia’s Swin Transformer V2 Scales the Award-Winning ViT to 3 Billion Parameters and Achieves SOTA Performance on Vision Benchmarks,"Microsoft Research Asia has upgraded their Swin Transformer with a new version featuring three billion parameters to train images with resolutions up to 1,536 x 1,536 and advance the SOTA on four representative vision benchmarks.

Here is a quick read: [Microsoft Asia’s Swin Transformer V2 Scales the Award-Winning ViT to 3 Billion Parameters and Achieves SOTA Performance on Vision Benchmarks.](https://syncedreview.com/2021/11/22/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-149/)

The associated code will be available on the project’s [GitHub](https://github.com/microsoft/Swin-Transformer). The paper *Swin Transformer V2: Scaling Up Capacity and Resolution* is on [arXiv](https://arxiv.org/abs/2111.09883).",MachineLearning
qzmp8l,1637591513.0,"[N] How Verizon Uses AI For Enhanced Business Decisions with Anil Kumar, Executive Director – Head of AI Industrialization at Verizon - Thursday, December 2, 2021 at 11:30 AM ET","Hi r/MachineLearning!

I wanted to share this free webinar with you all. Below are the details from the event website:

**Featured Speaker: Anil Kumar, Executive Director, Head of AI Industrialization at Verizon.**

As companies are looking to leverage cognitive technology, and deploying machine learning models, organizations need to make sure they have the correct people, processes, and technology in place to succeed.  In this presentation, Anil will share what AI industrialization is and how Verizon is moving from pockets of AI/ML to AI/ML being implemented across the organization. He will also share some of the unique  opportunities and challenges that adopting AI across an organization as large as Verizon presents.

**Agenda:**

* 11:30-12:30pm: Featured Presentation
* 12:30-13:00pm: Your Q&A and interaction

Link for free registration: [https://events.cognilytica.com/CLNTQwMHwyNA](https://events.cognilytica.com/CLNTQwMHwyNA)",MachineLearning
qzjuvk,1637582377.0,[Discussion] NeurIPS 2021 finally accepted submissions statistics,"The crawled data is [here](https://guoqiangwei.xyz/neurips2021_stats/neurips2021_submissions.html).

\- rating distribution

&#x200B;

[-](https://preview.redd.it/eg82xk7jz4181.png?width=917&format=png&auto=webp&s=2b68d203454675e8e7c50e32c40a1704e78bfc2d)

\- top keywords

&#x200B;

https://preview.redd.it/mjxv0gukz4181.png?width=1120&format=png&auto=webp&s=735c5a219ec8380a22fd6140ef21e8851bdfe93e

\- consistency experimental results:

&#x200B;

https://preview.redd.it/5rfca77nz4181.png?width=671&format=png&auto=webp&s=407e99f829f537c2bf82a847e57e15ae35957df7",MachineLearning
qzj8qo,1637579958.0,[R] Training with batch size of 1," I am working on a virtual tryone problem but the thing is that I really  cant use more than batch size of 1 in training due to gpu memory  limitations. The network really benefits from batchnorm however with  batch size of 1, it is really not giving good results. I am using  pytorch and any help will be appreciated. What do I use instead of  batchnorm? I have tried instancenorm but that too doesnt work very well.  Should I try playing with momentum of batchnorm?",MachineLearning
qzic0a,1637576190.0,[P] zeroshot_topics: Label your text data or infer topics in your text data automatically.,"zeroshot\_topics: [Github link](https://github.com/AnjanaRita/zeroshot_topics)

Hand-labelled training sets are expensive and time-consuming to create usually. Some datasets call for domain expertise (eg: medical/finance datasets etc). Given these factors around costs and inflexibility of hand-labelling, it would be nice if there are tools that can help us get started quickly with a minimal labelled dataset - enter weak supervision.

**But what if you do not have any labelled data at all? is there a way to still label your data automatically in some way?** That's where **zeroshot\_topics** might be useful! to help you to be up and running quickly.

*zeroshot\_topics* lets you do exactly that! it leverages the power of zero-shot-classifiers, transformers &  knowledge graphs to automatically suggest labels/topics from your text data. all you need to do is point it towards your data.

&#x200B;

Please check this out and share your feedback!",MachineLearning
qz4uun,1637529996.0,"[D] How to edit images with GANs, Part 1: Your digital Metaverse avatar","This tutorial covers the intuition behind:

* Image inversion with GANs
* The editability vs reconstruction tradeoff
* Projecting images into the generator's latent space

Telegram post: [https://t.me/casual\_gan/193](https://t.me/casual_gan/193)

Blog post: [https://www.casualganpapers.com/gan-inversion-image-editing-metaverse-avatar/AI-assisted-Image-Editing-Part1.html](https://www.casualganpapers.com/gan-inversion-image-editing-metaverse-avatar/AI-assisted-Image-Editing-Part1.html)

[This is an image of me edited with StyleCLIP](https://preview.redd.it/2z48j9lon0181.jpg?width=5668&format=pjpg&auto=webp&s=8fd559c5867c156866488b1f5e607d3ebe372069)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries and GAN tutorials!",MachineLearning
qz3qtv,1637526777.0,[D] 5 considerations for Deploying Machine Learning Models in Production – what did I miss?,"I wrote a post about [considerations for deploying machine learning models in production.](https://towardsdatascience.com/considerations-for-deploying-machine-learning-models-in-production-89d38d96cc23) Below are the considerations. What did I not consider? I know the first consideration seems obvious, but I thought it was worth mentioning.

**1. Use your laptop for development as a best practice**

Consider your development environment first. Most data scientists or ML engineers invariably use their laptops for development, testing or debugging code. Because of simplicity, easy to access and install the latest ML libraries, practitioners overwhelmingly prefer laptops over clusters for development. We are spoiled by IDEs and syntax-highlighted editors for good reason.

Python developers like to customize their environments to match their staging environment, with library dependencies using conda or Python virtual environments. Ideally, as a best practice, if the same code developed on their laptop can run with minimal changes on a staging or production environment on the cluster, it immensely improves the end-to-end developer productivity.

Consider your laptop as a preferred choice of development environment, with the possibility of extending or syncing your code to the cluster environment in the cloud.

*Consideration Number #1*: *Use your laptop for development as a best practice.*

**2. Training at Scale and Tracking Model Experiments**

Unlike the traditional software development cycle, the model development cycle paradigm is different. A number of factors influence an ML model’s success in production. First, the outcome of a model is measured by its metrics, such as an acceptable accuracy.

Second, achieving an accuracy that satisfies the business goal means experimentation with not only one model or ML library but many models and many ML libraries while tracking each experiment runs: metrics, parameters, artifacts, etc. As vital as accuracy is, so is a developer’s choice of ML libraries to experiment with.

Third, accuracy is directly linked to the quality of acquired data: bad data results in a bad model.  Data preparation, feature extractions, feature selection, standardized/normalized features and data imputations/encoding are all imperative steps before the cleansed data lands into a feature store, accessible to your model training and testing phase or inference in deployment.

Fourth, a choice of programming language that is not only familiar to your data team — data analysts, data scientists, and ML engineers — but also supported by many ML libraries employed during model experimentation and training phases. Python seems to be the [de facto choice](https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-languages-loved).

Alongside a choice of a programming language is the choice of an ML framework for taming compute-intensive ML workloads: deep learning, distributed training, hyperparameter optimization (HPO), and inference — all at horizontal scale — from your laptop, single node multiple cores to multiple nodes, with multiple cores.

And finally, the ability to easily deploy models in diverse environments at scale: part of web applications, inside mobile devices, as a web service in the cloud, etc.

*Consideration Number #2: Consider using model life cycle development and management platforms like* [*MLflow*](https://mlflow.org/)*,* [*DVC*](https://dvc.org/)*,* [*Valohai*](https://valohai.com/model-monitoring/)*,* [*Weights & Biases*](https://wandb.ai/site)*, or* [*SageMaker Studio*](https://aws.amazon.com/sagemaker/studio/)*. And* [*Ray*](https://www.ray.io/), [*Ray Tune*](https://docs.ray.io/en/latest/tune/index.html)*,* [*Ray Train*](https://docs.ray.io/en/latest/train/train.html) *(formerly Ray SGD),* [*PyTorch*](https://pytorch.org/tutorials/beginner/dist_overview.html) *and* [*TensorFlow*](https://www.tensorflow.org/guide/distributed_training) *for distributed, compute-intensive and deep learning ML workloads.*

**3. Managing Machine Learning Features**

[Feature stores](https://www.featurestore.org/) are emerging pivotal components in the modern machine learning development cycle. As more data scientists and engineers work together to successfully put models in production, having a singular store to persist cleaned and featurized data is becoming an increasing necessity as part of the model development cycle.

Feature stores address operational challenges. They provide a consistent set of data between training and inference. They avoid any data skew or inadvertent data leakage. They offer both customized capability of writing feature transformations, both on batch and streaming data, during the feature extraction process while training. And they allow request augmentation with historical data at inference, which is common in large fraud and anomaly detection deployed models or recommendation systems.

Aside from challenges and considerations of putting models in production, operationalizing ML data is equally important. Model accuracy depends on good data, and feature stores help manage precomputed and cleansed features for your model training and production inference during model serving.

*Consideration Number #3: Consider feature stores as part of your model development process. Look to* [*Feast*](https://feast.dev/)*,* [*Tecton*](https://www.tecton.ai/)*,* [*SageMaker*](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_featurestore.html)*,* [Hopsworks](https://www.hopsworks.ai/feature-store), *and* [*Databricks*](https://databricks.com/product/feature-store) *for feature stores.*

**4. Deploying, Serving and Inferencing Models at Scale**

Once the model is trained and tested, with confidence that it met the business requirements for model accuracy, seven crucial requirements for scalable model serving frameworks to consider are:

**Framework agnostic**: A model serving-elected framework should be ML framework agnostic. That is, it can deploy any common model built with common ML frameworks. For example, PyTorch, TensorFlow, XGBoost, or Scikit-learn, each with its own algorithms and model architectures.

**Business Logic:** Model prediction often requires preprocessing, post processing or ability to augment request data by connecting to a feature store or any other data store for validation. Model serving should allow this as part of its inference.

**Model Replication:** Some models are compute-intensive or network-bound. As such the elected framework can fan out requests over to model replicas, load balancing among replicas to support parallel request handling during peak traffic.

**Request Batching:** Not all models in production are employed for real-time serving. Often, models are scored in large batches of requests. For example, for deep learning models, parallelizing these image requests to multiple cores, taking advantage of hardware accelerators, to expedite batch scoring and utilize hardware resources is worthy of consideration.

**High Concurrency and Low Latency:** Models in production require real-time inference with low latency while handling bursts of heavy traffic of requests. The consideration is crucial for best user experience to receive millisecond responses on prediction requests.

**Model Deployment CLI and APIs:** A ML engineer responsible for deploying a model should be able to use model server’s deployment APIs or command line interfaces (CLI) simply to deploy model artifacts into production. This allows model deployment from within an existing CI/CD pipeline or workflow.

**Patterns of Models in Production**: As ML applications are increasingly becoming pervasive in all sectors of industry, models trained for these ML applications are complex and composite. They range from computer vision to natural language processing to recommendation systems and reinforcement learning.

That is, models don’t exist in isolation. Nor do they predict results singularly. Instead they operate jointly and often in [four model patterns](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns): pipeline, ensemble, business logic, and online learning. Each pattern has its purpose and merit.

Machine Learning engineers adopt [two common approaches ](https://youtu.be/gV4YS4e1CXg?t=272)to deploy these patterns of models in production. One is to embed models into a web server and the other is to offload to an external service. Each approach has its own pros and cons, with respect to the seven considerations above.

*Consideration Number #4: Look to* [*Seldon*](https://www.seldon.io/)*,* [*KFServing*](https://www.kubeflow.org/docs/components/kfserving/)*, or* [*Ray Serve*](https://docs.ray.io/en/latest/serve/index.html) *for all these seven requirements.*

**5. Observing and Monitoring Model in Production**

Model monitoring, often an overlooked stage as part of model development lifecycle, is critical to model’s viability in the post deployment production stage. It is often an afterthought, at an ML engineer’s peril.

Models have an afterlife of viability. That viable life in production needs a constant watchful or sentinel eye. In fact, monitoring as a phase is simply a continuation of the model serving.

Why consider model monitoring? For a number of practical reasons, this stage is pivotal. Let’s briefly discuss them.

Data drifts over time\*\*:\*\* As we mentioned above, our quality and accuracy of the model depends on the quality of the data. Data is complex and never static, meaning what the original model was trained with the extracted features may not be as important over time. Some new features may emerge that need to be taken into account. For example, seasonal data changes. Such [features drifts](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift) in data require retraining and redeploying the model, because the distribution of the variables is no longer relevant.

Model concept changes over time\*\*:\*\* Many practitioners refer to this as model [decay or model staleness](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift). When the patterns of trained models no longer hold with the drifting data, the model is no longer valid because the relationships of its input features may not necessarily produce the model’s expected prediction. Hence, its accuracy degrades.

Models fail over time\*\*:\*\* Models fail for inexplicable reasons: a system failure or bad network connection; an overloaded system; a bad input or corrupted request. Detecting these failures’ root causes early or its frequency mitigates user bad experience or deters mistrust in the service if the user receives wrong or bogus outcomes.

Systems degrade over load\*\*:\*\* Constantly being vigilant of the health of your dedicated model servers or services deployed is just as important as monitoring the health of your data pipelines that transform data or your entire data infrastructure’s key components: data stores, web servers, routers, cluster nodes’ system health, etc.

Collectively, these aforementioned monitoring model concepts are called [model observability](https://towardsdatascience.com/what-is-ml-observability-29e85e701688). This step is now an acceptable imperative in [MLOps best practices](https://towardsdatascience.com/mlops-practices-for-data-scientists-dbb01be45dd8). Monitoring the health of your data and models should never be an afterthought. Rather, it ought to be part and parcel of your model development cycle.

*Consideration Number #5: For model observability look to* [*Evidently.ai*](https://github.com/evidentlyai/evidently)*,* [*Arize.ai*](https://arize.com/)*,* [*Arthur.ai*](https://www.arthur.ai/), [*Fiddler.ai*](https://www.fiddler.ai/), *or* [*whylabs.ai*](https://whylabs.ai/)*.*

*Thanks to this* [excellent PyTorch reddit post for inspiring](https://www.reddit.com/r/MachineLearning/comments/kvs1ex/d_here_are_17_ways_of_making_pytorch_training/) the formatting of this post\*.\*",MachineLearning
qyy2ps,1637510818.0,[R] Aesthetics of Algorithmic Art,"The idea behind our research is that we have created thousands of images, each piece taking its own number of unique parameters to generate. With enough votes from the public, we can begin to distinguish what (if any) parameters make a good piece of art and adjust them accordingly.  We've put together a website, [artvote](https://artvote.net/) that allows for voting on these thousands of images.

Art is considered to be subjective, but we want to find out if that's truly the case.  We hypothesize that there must be some aspect(s) to a piece that can be tailored to increase favorability and likeability when seen by large audiences as well as the individual. The art you see on the website is broken down into multiple categories, and although some of the pieces may look similar, each one is unique.  It was intentionally designed in this way so that it would be easier to evaluate how ""small changes"" can impact these scores.

We imagine that if our hypothesis is correct (that there exists a clear path in designing a piece of art or even an image) this could have large impacts beyond simply generating art.  *The advertising market would especially benefit, as they can serve up more precise ads that are visually appealing to a specific user and increase click-through rates.*",MachineLearning
qyxxcs,1637510411.0,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning
qyvuzj,1637504184.0,[D] Daily working routine as a Machine Learning Engineer,"Hey r/MachineLearning!

I'm currently studying Business Informatics in Germany. Next year I will finish my bachelor and then I want to do my master's degree study. I'm doing my bachelor part-time in a Software Company. So I already have some practical experience in the field.

My interest are very focused on Machine Learning and artificially Intelligence in general. I find it fascinating that a Computer can ""learn"" from Data and then predict outcomes.

Because of that I already did 2 Semesters in a Data Science Team in my Company, where I did mostly statistically analysis. I liked the tasks in general, but the daily work was more focussed on creating presentations for business people and not really the technical part.

Currently I'm in a team wich works on Natural Language Understanding problems. There I'm doing more infrastructure tasks e.g. Docker, OpenShift, Deploying Models, building API's wich communicate with the models. I really like the more technical part, but it's not that ML related.

Usually in the last semester you work in a Team, where you are going to work after your bachelors.

My current plan is to go to a team, wich solves Vision Problems e.g. identification of loose screws in trains. I thought that a position as ML Engineer there would be my fulfillment, but recently I had some doubts about that:

1. I'm scared that my day just consists of annotating data
2. I want to think by myself not just implement different models and then see, wich is the best
3. I want to be creative e.g. implement own things, not follow instructions on how to implement Model X
4. I'm also scared of that if I don't like the tasks I can't change in a different path, because ML is so specialized

I really liked the summary of ML Researchers Positions, but I don't know how hard it is to get a research position here in Germany. In general I'm open minded about doing a PhD in a ML field, but I don't know how hard this is going to be in comparison to my current study. I think I'm currently one of the best in my year, but the Uni is not really the hardest..

Do you guys have any experience as a ML Engineer and can take away my fears? Is it really so hard to land a position as ML Researcher? Should I go for a different career path, because ML is so fast living?",MachineLearning
qyszpw,1637493625.0,[P] Pyconverse - Conversational Text transcript analysis library,"Github project link: [pyconverse](https://github.com/AnjanaRita/converse)

Conversation analytics plays an increasingly important role in shaping great customer experiences across various industries like finance/contact centres etc.. primarily to gain a deeper understanding of the customers and to better serve their needs. This library, *PyConverse* is an attempt to provide tools & methods which can be used to gain an understanding of the conversations from multiple perspectives using various NLP techniques.

I have been doing what can be called conversational text NLP with primarily contact centre data from various domains like  Financial services, Banking, Insurance etc for the past year or so, and I  have not come across any interesting open-source tools that can help in understanding conversational texts as such I decided to create this library that can provide various tools and methods to analyse calls and help answer important questions/compute important metrics that usually people want to find from conversations, in contact centre data analysis settings.

&#x200B;

Things that can be done with this library:

1. Emotion identification
2. Empathetic statement identification
3. Call Segmentation
4. Topic identification from call segments
5. Compute various types of Speaker attributes: (word counts/number of words per utterance/negations etc., Identify periods of silence & interruptions, Question identification,  Backchannel identification, Assess the overall nature of the speaker via linguistic attributes and tell if the Speaker is: Talkative, verbally fluent, Informal/Personal/social, Goal-oriented or Forward/future-looking/focused on past, Identify inhibition.)

Please give it a try and share your feedback.",MachineLearning
qysmyc,1637492143.0,[P] A webapp for monitoring GPU machines,"Hi all,

I created a webapp for monitoring the GPU machines I am working with. Maybe it will be useful to someone else here.
The app aggregates the output from [gpustat](https://github.com/wookayin/gpustat) across all machines and displays them on a single page.

I have been using the app for a while now and it is working well for me but it has not been extensively tested. If you encouter any problems while setting the app up, don't hesitate to open an issue!

Github: [https://github.com/matthias-wright/server-dashboard](https://github.com/matthias-wright/server-dashboard)

https://i.redd.it/g2z46940jx081.gif",MachineLearning
qyo2mu,1637472727.0,How to Implement a Permute/Transpose Op 6 Times Faster Than PyTorch?[N],"[https://oneflow2020.medium.com/how-to-implement-a-permute-transpose-op-6-times-faster-than-pytorch-6280d63d0b4b](https://oneflow2020.medium.com/how-to-implement-a-permute-transpose-op-6-times-faster-than-pytorch-6280d63d0b4b)

This article will introduce the techniques to optimize the Permute Kernel in OneFlow and compare it with PyTorch’s Permute and the native Copy operation in experiment. The results show that the deeply optimized Permute operation is much faster and more bandwidth effective than PyTorch, and the bandwidth utilization is close to that of the native Copy operation.",MachineLearning
qynzlz,1637472429.0,[D] A high-level microblog on Federated Learning,"I wrote a high-level (not too technical) thread on Federated Learning.

[Twitter Thread](https://twitter.com/shreyansh_26/status/1462262151209381888?t=3_Oldjr3_dY02YS3TAzURQ&s=19)

If you found it informative, do let me know!",MachineLearning
qyl5fc,1637462436.0,[R] Asking for go-to methodologies for 2 different tasks with correlated labels?,"Hi everyone,

Assume that I am presented with a typical, small size dataset (let's call it dataset2), and also assume that a model needs to be learned using this dataset2 so that it can generalize to unknown datapoints.

However, there is also a larger dataset1 with labels that are actually informative of the labels of dataset2. What I mean by this is that, assuming that I train a model on the larger dataset1, if I use this model to acquire some predictions for dataset2, the prediction obtained with the actual labels from dataset2 correlate to an extend. However, the tasks, meaning, the nature of the labels for the 2 datasets are different and they do not mean the same thing.

Now, I know that there are many options here to further boost the performance for the task regarding dataset2, like:
A) Use the model gained from dataset1 to acquire predictions for dataset2, use those predictions as features when training a smaller model on the dataset2 labels.

B) Fine tune dataset1 model's weights to the dataset2 labels, using a standard Transfer Learning regime. (I already tried this with mixed results)

C) Train a separate model for dataset2, and combine both predictions for better generalization.

D) Do multi-task learning by combining both datasets.

Could there be any other, better options than those 4 given the above scenario? I was thinking that Knowledge Distillation could be used to an extend, but the tasks are not the same, even though they are inherently closely connected.


Thanks in advance, and apologies for the bad english!",MachineLearning
qydn0p,1637438920.0,[P] Pip package for managed ML Training on AWS Spot Instances,"Deep learning is expensive and even if you have AWS/GCP credits, you'll quickly run out of it. We built the tool to make training cheaper and to have these credits longer.
SpotML is a command line tool that automatically manages ML training on AWS spot instances which are **\~3X cheaper**. It lets you handle spot interruptions by resuming training using the latest checkpoint.


Documentation [link](https://docs.spotml.io/getting-started) to try it out.

Looking for feedback from early testers. You would be an ideal candidate if you have a side project that you're spending your own money to train.

Acknowledgement: - SpotML is built on top of existing open source library [Spotty](https://github.com/spotty-cloud/spotty)",MachineLearning
qybvz4,1637433788.0,[R] Asking for Multi-model Architecture Papers,"Hey all,

I'm trying to read about the gap between the resources demanded by multi-model applications and the resource limitation on commodity hardware. Right now I am looking more into the former. For exemplification, [DeepEye](https://dl.acm.org/doi/pdf/10.1145/3081333.3081359) \[[https://dl.acm.org/doi/pdf/10.1145/3081333.3081359](https://dl.acm.org/doi/pdf/10.1145/3081333.3081359)\] employs 4-5 CNN models that each perform a unique task on a single device.  If anyone is aware of similar papers that employ multi-model architectures it would be amazing if you left the paper in the comments.

I'm new to this community, so if I need to specify the question or need to make any edits please let me know. Looking forward to participating in this community :)",MachineLearning
qy8gfg,1637423876.0,[D] Paper Explained - Learning Rate Grafting: Transferability of Optimizer Tuning (w/ rant about reviewer #2),"[https://youtu.be/vVRC-0VKPrg](https://youtu.be/vVRC-0VKPrg)

The last years in deep learning research have given rise to a plethora of different optimization algorithms, such as SGD, AdaGrad, Adam, LARS, LAMB, etc. which all claim to have their special peculiarities and advantages. In general, all algorithms modify two major things: The (implicit) learning rate schedule, and a correction to the gradient direction. This paper introduces grafting, which allows to transfer the induced learning rate schedule of one optimizer to another one. In that, the paper shows that much of the benefits of adaptive methods (e.g. Adam) are actually due to this schedule, and not necessarily to the gradient direction correction. Grafting allows for more fundamental research into differences and commonalities between optimizers, and a derived version of it makes it possible to computes static learning rate corrections for SGD, which potentially allows for large savings of GPU memory.

&#x200B;

OUTLINE

0:00 - Rant about Reviewer #2

6:25 - Intro & Overview

12:25 - Adaptive Optimization Methods

20:15 - Grafting Algorithm

26:45 - Experimental Results

31:35 - Static Transfer of Learning Rate Ratios

35:25 - Conclusion & Discussion

&#x200B;

Paper (OpenReview): [https://openreview.net/forum?id=FpKgG31Z\_i9](https://openreview.net/forum?id=FpKgG31Z_i9)

Old Paper (Arxiv): [https://arxiv.org/abs/2002.11803](https://arxiv.org/abs/2002.11803)",MachineLearning
qxz09z,1637387253.0,[D] Can embodiment help ML?,"We've all heard the question if embodiment is necessary. Some claim symbol grounding occurs when a system has a body and it's therefore needed. Some claim that movement is the real reason we have brains.

Some claim that it's not. It seems to me this view is prevalent in the machine learning community. ML practitioners claim that statistics on DATA alone will get us there.

I have a novel argument in favor of embodiment. It is specifically aimed at the ML (statistics) community.

I claim that embodiment allows conducting statistical experiments as opposed to performing observations. Creating and conducting statistical experiments where one can control the conditions of the experiments dramatically speeds up learning.

In layman's terms one can hit the corner cases that might never reveal themselves if something is observed without the ability to modify the experiment.

For example observing a coin on a smooth building lobby floor where people are kicking it once in a while versus an ability to conduct an experiment by flipping a coin.

What do you think about my argument and what are your arguments for or against embodiment?",MachineLearning
qxo4ym,1637351727.0,[D] How to search for machine learning phd internships effectively?,"I am a machine learning PhD student in a UK university. I am interested in research internships at some of the big tech companies, and there are just some questions that I'd like to ask the reddit community:

\- **What is the best strategy to land offers / interviews?** I have submitted CV and filled application forms to many of them but haven't heard back since (except a summary reject from Deepmind -- it took them only 3 days to do so). Are there more effective ways to progress further or am I just worrying a bit too early at this stage?

\- **Positions based in the US**: I found that many of the positions available are in the US/North America. I know that for the 2020/21 cycles most of the internships were virtual so the location might not have mattered so much, but as the world starts to open up will it be commonplace for US-based positions to hire someone from, say, Europe? (I'd actually prefer UK/European positions but just couldn't find that many online)

\- **What are they looking for in propective interns?** I found all the job descriptions are rather vague, but rumours of leetcode worry me a bit since I did not do hardcore CS for my undergraduate. From my initial experience leetcode medium seems to be the hardest I can do in a realistic interview setup without more hours put into practice.

**About myself**: I'm just starting my 3rd year in one of the top UK universities (UK PhDs nominally last 3 years, but my observation is that people tend to do slightly longer). I did an research scientist internship in a big tech company (not at the level of FAANG but decent), but someone referred me for that position and they urgently needed someone back then so I guess I didn't get the ""full interview experience"", and I also expect the positions at the top-tier companies could be more competitive so my interview experience there might not be that representative. In terms of publications, I have \~3 first/co-first authored papers at top conferences (i.e., ICML/ICLR/NeurIPS) and a couple under review, \~2 first-authored papers at lower-tier conferences/journals and a few non-first-authored papers otherwise.

Any insights and suggestions are welcomed! many thanks for your help.",MachineLearning
qxmw3r,1637348046.0,[P] Monitoring DeFi activity with open source tools,"Here's a very detailed post about how the [Cloudwall Capital](https://www.linkedin.com/company/cloudwall-capital/) team uses Moonstream's Python API with Dagster, a data pipeline framework, to monitor DeFi activity:


[https://cloudwall.medium.com/the-geek-out-riding-moonstreams-7dbe10ac9956](https://cloudwall.medium.com/the-geek-out-riding-moonstreams-7dbe10ac9956)


It's rare to see such clear writing in a technical blog post. Definitely worth the read.",MachineLearning
qxlsmm,1637344889.0,[R] Imperial College London Researchers Propose A Novel Randomly Connected Neural Network For Self-Supervised Monocular Depth Estimation In Computer Vision,"Depth estimation is one of the fundamental problems in computer vision, and it’s essential for a wide range of applications, such as robotic vision or surgical navigation.

Various deep learning-based approaches have been developed to provide end-to-end solutions for depth and disparity estimation in recent times. One such method is self-supervised *monocular depth estimation*. Monocular depth estimation is the process of determining scene depth from a single image. For disparity estimation, the bulk of these models use a U-Net-based design.

Although relative depth is perceived very easily by humans, the same task for a machine has proven quite challenging due to the absence of an optimal architecture. To tackle this issue, more complex architectures are chosen to generate a high-resolution photometric output.

[The Hamlyn Centre’s research team from Imperial College London](https://www.imperial.ac.uk/hamlyn-centre/) introduces [a unique randomly connected encoder-decoder architecture for self-supervised monocular depth estimation](https://www.imperial.ac.uk/news/230818/randomly-connected-neural-network-self-supervised-monocular/). The model architectural design, capable of extracting high order features from a single image and the loss function for imposing a solid feature distribution, is credited for the idea’s success.

# [Quick 5 Min Read](https://www.marktechpost.com/2021/11/19/imperial-college-london-researchers-propose-a-novel-randomly-connected-neural-network-for-self-supervised-monocular-depth-estimation-in-computer-vision/) | [Paper](https://www.tandfonline.com/doi/full/10.1080/21681163.2021.1997648)| [Imperial Blog](https://www.imperial.ac.uk/news/230818/randomly-connected-neural-network-self-supervised-monocular/)

https://preview.redd.it/fqfbuop7dl081.png?width=1024&format=png&auto=webp&s=3f85d5f99ffd0b7dbdf520c2c4934f858041894a",MachineLearning
qxlk34,1637344227.0,[R] Permutation-Invariant Neural Networks for Reinforcement Learning,"Link to the blog post: https://ai.googleblog.com/2021/11/permutation-invariant-neural-networks.html

*The permutation invariant neural network agents presented here can handle ill-defined, varying observation spaces. Our agents are robust to observations that contain redundant or noisy information, or observations that are corrupt and incomplete.*",MachineLearning
qxl93a,1637343360.0,[D] State of Pytorch mobile (in comparison with TFLite),"Does anyone have experience with both frameworks and have a more in-depth knowledge on the current state of affairs regarding embedded DL? I wouldn't like this to turn into another framework flame war, and instead have a more general comparison between TFLite and PyTorch Mobile (the latter being still in beta), in terms of:

* Ease of use (mostly, how much you need to break your model, what layers are actually conversible to mobile format, etc);
* API stability;
* Support for mobile (iOS/Android) in terms of cross-compilation tools and HW acceleration;
* Support for other SoCs and fringe devices.

Also, are there any other contenders I am missing?",MachineLearning
qxjr94,1637339127.0,[N] Scikit-Learn Feature Selection using AI,"Hi, I just want to let you know that the [sklearn-genetic-opt version](https://sklearn-genetic-opt.readthedocs.io/en/stable/) 0.7.0 is now available, it implements feature selection using evolutionary algorithms, it uses a multi-objective function to optimize the cross-validation score while minimizing the number of features used. It's compatible with any sklearn classifier or regressor.

Let me know if you have any question or suggestion

This new feature is compatible with all the callbacks, tensorboard and mlflow, for example using the progress bar callback:

https://i.redd.it/pnglg139wk081.gif

You can check here the docs: [https://sklearn-genetic-opt.readthedocs.io/en/stable/](https://sklearn-genetic-opt.readthedocs.io/en/stable/)

If you would like to contribute or to check the implementation, here is the repo: [https://github.com/rodrigo-arenas/Sklearn-genetic-opt](https://github.com/rodrigo-arenas/Sklearn-genetic-opt)",MachineLearning
qxiywa,1637336820.0,[R] SPANN: A Highly-Efficient Billion-Scale Approximate Nearest Neighbour Search That’s 2× Faster Than the SOTA Method,"A research team from Microsoft, Peking University, Tencent, and Baidu proposes SPANN, a simple but efficient memory-disk hybrid vector indexing and search system that guarantees both low latency and high recall and achieves a 2× speedup over the state-of-the-art nearest neighbour search (ANNS) solution while retaining the same recall quality and memory cost.

Here is a quick read: SPANN: [A Highly-Efficient Billion-Scale Approximate Nearest Neighbour Search That’s 2× Faster Than the SOTA Method.](https://syncedreview.com/2021/11/19/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-148/)

The associated code is available on the project’s [GitHub](https://github.com/microsoft/SPTAG). The paper *SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor Search* is on [arXiv](https://arxiv.org/abs/2111.08566).",MachineLearning
qxive5,1637336522.0,[D] Bias in Criminal Sentencing and Parole,"I'm a student at a high school and I have a presentation that I need to make on machine learning bias in criminal sentencing and parole. If you have any information on this matter such as specific examples of when it was used or possible ideas to eliminate bias, I'd be glad to hear them.",MachineLearning
qxidh8,1637335056.0,[N] Machine Learning Bootcamp: train for future ML contests,"https://preview.redd.it/n0ruetdljk081.jpg?width=4800&format=pjpg&auto=webp&s=402cafc96dd6f4c367a0cee1ccd3248521f19ca2

We invite everyone to the online Bootcamp on Machine Learning. Participation is free. We decided to hold the **Machine Learning Bootcamp** to train participants for future ML competitions.

ML Bootcamp is a two-day intensive training that will help you increase your machine learning knowledge, acquire practical skills and study basic deep learning tools. We welcome undergraduate and postgraduate students making the first steps in the world of ML contests, you just need to register.

There are no strict prerequisites, but potential participants should at least be familiar with classic ML algorithms (like linear models and decision trees); pandas, NumPy, Matplotlib, and sklearn libraries.

**When**: 4-5 December, 2021
**Where**: Online

Join: [http://idao.world/bootcamp/?utm\_source=reddit](http://idao.world/bootcamp/?utm_source=reddit)",MachineLearning
qxeu1x,1637323536.0,[D] Stress-testing models to ensure robustness?,"Hey,

I was wondering if we could get a discussion going regarding stress-testing ML models to ensure their robustness. Maybe someone has some ideas or paper recommendations ?

I know it's possible to test the robustness of (image-recognition) models by for example rotating, flipping, adding noise to the data, but I was wondering if there was some more scientifically thorough guides for this.

Also could generative models or adversarial attacks be used for this as well, and to what degree?

Curious to see what people here think about the topic :)",MachineLearning
qxepxd,1637323132.0,[R] On Transferability of Prompt Tuning for Natural Language Understanding,"We empirically investigate the transferability of soft prompts across different tasks and models to demonstrate some promising directions worth exploring in prompt. ArXiv: [https://arxiv.org/abs/2111.06719](https://arxiv.org/abs/2111.06719) \#nlp #prompt

https://preview.redd.it/c6c5uxmnkj081.jpg?width=606&format=pjpg&auto=webp&s=e693d1ed2752a884607f6063d76a72f9d3ae8951",MachineLearning
qxdqt0,1637319089.0,"[D] Similar open source long list to TF like Pytorch ""ECOSYSTEM TOOLS""","I have found this pretty cool page to PyTorch that contains great many libraries to PyTorch ""ECOSYSTEM TOOLS"" [https://pytorch.org/ecosystem/](https://pytorch.org/ecosystem/)  .

But I can not find any similar page to Tensor Flow.

If you know about it please let me know. (not just a top 10 article in medium but an actual longlist that contain a wide variety of options).",MachineLearning
qx6giu,1637290316.0,[D] Does FAANG interviewing research interns for summer 22 already?,"I have applied to many big-tech companies for Phd research intern positions, mostly focusing on machine learning and deep learning. I recently got several offers from non-FAANG companies. My initial goal is to get an internship position at FAANG, but I have not received many opportunities to get interviewed by them. I am having an interview with Amazon and will have an interview with Facebook soon. That is all for FAANG.

I am curious when is the usual hiring/interview period for intern students. Perhaps, I am not attractive to those big-techs and that is why I have not got many interviews from them. Or I just wonder if they are gathering applications and start interviewing applicants from December.

Because I got an offer from some companies, I need to decide soon whether I will accept it or not. But, because I don't have any idea if the hiring process at FAANG is almost finished or there will be more opportunities, it is hard to decide.

(1) If you have any experience in the hiring process of research intern positions at FAANG, could you let me know rough dates of when you had interviews and when you got offers from them.

(2) Also, because this is my first internship during my Phd, I don't know what to consider when choosing a position. All positions are research intern positions and the topic of projects roughly aligned with my interests and most of the time, I was told that the specific topic will be decided when I start internship by discussing with my mentor. I hope I could get a position that has a high possibility that I could publish a paper, but it is true that there are so many uncertainties. Could you give me any advice for selecting an intern position when you have multiple options?

Any advice regarding internship will be very helpful. Thank you inadvance.",MachineLearning
qx5mjd,1637287657.0,[D] Which reference letter dossier do most universities ML PhD programs accept?,"I have my 3 references lined up and I'm applying for PhD programs in ML. Then one of my references throws a wrench in my plans ""I'm only sending to one place"" says one of my former supervisors. He suggests to me to use Interfolio as the reference letter dossier for him to send to. I'm applying to: CMU, MIT, Stanford, MILA, Cornell, UofToronto and UofIllinois. Does Interfolio or is there a dossier system that sends to all these schools?",MachineLearning
qx479l,1637283076.0,[R] Reinforcement Learning for Traffic Light Control,"I have read this very interesting paper about RL for traffic lights:

[Reinforcement Learning Benchmarks for Traffic Signal Control](https://openreview.net/forum?id=LqRSh6V0vR)

I have already worked with RL for train scheduling, but traffic lights are new to me.

Here's what it says in a nutshell:

- Many people have developed RL approaches for traffic light (TL) control
- Many of these people have claimed SotA 😅
- In this paper, they introduce a well-defined benchmark using the SUMO simulator
- They show that all things considered, recent methods don't perform so well, and a simple independent DQN approach is best (the flavor of this DQN approach happens to be described in the first author's previous paper)

This sounds very interesting, and wouldn't be the first time that a ""cleaned up"" benchmark reveals that a simpler method was the best all along.

But, a few things surprise me:

- Independent DQN beats everything else. This surprises me as some of the other methods use cooperative approaches. I know MARL is hard, but still, in the setting of a complex city, I would expect traffic lights that are able to communicate to outperform those that can't (although I'd expect them to converge more slowly).

- The convergence speed and stability look crazy to me. Really, solving a region-scale problem with 21 TLs and 4.2k car trips in fewer than 200 episodes of 360 timesteps each?! That doesn't sound realistic to me in RL-land.

- Last point: i want to bring attention to the amazing [inTAS project](https://github.com/silaslobo/InTAS). This guy made a realistic traffic scenario for Ingolstadt (Germany) using actual data from traffic lights, public transportation, shop opening hours etc that he got from the relevant offices. [The presentation is worth watching.](https://www.youtube.com/watch?v=UgPeBxXzDHc)

I'm curious to hear from more experienced practitioners!",MachineLearning
qx42a5,1637282636.0,"[D] Is anyone working on explainability of ""Foundation Models""","Recently researchers from Stanford came up with this term ""Foundation Models"" for these huge transformer models that they claim to be able to solve a number of natural language and even logical problems. I was wondering whether anyone is working on explainability for these transformer models? Thanks.",MachineLearning
qx0enm,1637271801.0,[D] All bias in ML comes from biased data?,"In this post, I am referring to bias in a *social sense* (racism, sexism, …) and not to bias in a strictly mathematical sense.

Obviously, if we train a model on biased data, the trained model will have inherited that bias. Some people say that this is the main (or only way) that bias finds it’s way into models.

However, assume that ImageNet were a biased benchmark (which it probably is) and most vision model architectures are developed to do well on ImageNet, this bias could in some way also be inherent to the resulting *architectures*, not just the learned weights?

Am I wrong here? If not, what are other ways besides biased data that one should be aware of?",MachineLearning
qwzhbb,1637269208.0,[D] Colab TPU low performance,"I wanted to make a quick performance comparison between the GPU (Tesla K80) and TPU (v2-8) available in Google Colab with PyTorch. To do so quickly, I used an MNIST [example](https://github.com/PyTorchLightning/pytorch-lightning/blob/2c7c4aab8087d4c1c99c57c7acc66ef9a8e815d4/pl_examples/basic_examples/mnist_examples/image_classifier_4_lightning_module.py) from `pytorch-lightning` that trains a simple CNN.

For some reason, the performance on TPU is even worse than CPU.

    GPU: ~52 it/s
    TPU: ~9 it/s
    CPU: ~13 it/s

Here is the [colab notebook](https://colab.research.google.com/drive/1geMF_QtrkOMYkK-MHz7ADF1ChzMjrsfx?usp=sharing).

Can those results be legit (maybe small conv kernels are not suitable for TPU?), or is this an issue of  `pytorch-xla` or `lightning`? Any suggestions?",MachineLearning
qwygwu,1637266382.0,[D] Has anyone tried Deepmind haiku?,"I have just started to learn Jax and I am not sure what library to use for making neural networks. I then thought Haiku looks quite interesting.

I've just taken a look at it and I'm not sure what to think... on the one hand it looks quite capable (and fast) however there is almost nothing in the documentation in regards to examples / tutorials, as well as issues on github that are over a year old. One person has advised against it.

Is it worth learning or should I stick to Flax? (I have experience with TF 1+2, keras, scikit learn etc. but not pytorch)",MachineLearning
qwxil2,1637263748.0,[R] AdvCL: Protecting contrastive learning models against adversarial attacks,"Adversarial Contrastive Learning (AdvCL) is a new technique by MIT-IBM Watson AI Lab to make CL models robust without reducing downstream classification accuraction. The paper has been accepted at NeurIPS 2021.

Key highlights:

\- AdvCL replace two-view contrastive loss optimization with four-view CL, including adversarial perturbations and high-frequency component view.

\- A pseudo-supervision stimulus generation component uses clustering algorithms to create pseudo-labels for training data

\- AdvCL addresses the ""cross-task robustness transferability,"" which happens because of the mismatch between the loss function of the CL training stage and the downstream supervised finetuning.

\- This can be pivotal for the security of ML applications that are data-constrained and rely on CL

Analysis of paper + interview with lead author:

[https://bdtechtalks.com/2021/11/18/contrastive-learning-adversarial-attacks/](https://bdtechtalks.com/2021/11/18/contrastive-learning-adversarial-attacks/)

Full paper:

[https://arxiv.org/abs/2111.01124](https://arxiv.org/abs/2111.01124)

Code:

[https://github.com/LijieFan/AdvCL](https://github.com/LijieFan/AdvCL)",MachineLearning
qwwjdv,1637261086.0,[D] Any research groups interested in my work (deep learning for combinatorial optimization),"I published a paper in a peer-reviewed journal titled
""Neural Knapsack: A Neural solver for the Knapsack problem"" [Article ](https://ieeexplore.ieee.org/document/9291401)

Now I want to do my PhD in the same topic in Europe or Canada but can not find any research group or a supervisor that would be interested.

I did other projects, have quite good knowledge in deep learning and applied fields as Computer Vision


Can any one help me?",MachineLearning
qwwf82,1637260770.0,[D] What happen with the Neural Turing Machine / Differentiable Computer line of work?,"Recently I saw some renewed interest in [Algorithmic Reasoning by Petar Velickovic](https://arxiv.org/abs/2105.02761), essentially augment traditional ""discrete"" algorithms with ""continuous"" pattern recognition of DL, and it reminds me of the [Neural Turing Machine / Differentiable Computer](https://deepmind.com/blog/article/differentiable-neural-computers), mostly spearheaded by Alex Graves, which I believe share the same motivation with the Algorithmic Reasoning approach.

I haven't heard much about any major new work since the Neural Differential Computer and was wondering why? I was (and still am) fascinated by the idea and this research, but since I am not working on the topic, I'm not sure where are the challenges and pitfall?

I was aware of some instability in implementation but I thought the open-source code would have help there.

Anybody has insights on why this direction has not been explored more in recent years? Or is this one of those Schmidhubered's idea that too ahead of its time, and once people have squeezed out all the internal memory capacity from Transformer, this idea of external dynamic memory would bounce back?",MachineLearning
qww285,1637259802.0,[D] Anyone regret coming to this field?,"If yes, which path would you have taken?


Edit: Since y'all giving me helpful awards and this post has blown up, I would appreciate a gold because I am a broke college student who don't want to pay for reddit premium",MachineLearning
qwu7nc,1637254879.0,"[R] SynapseML: A simple, multilingual, and massively parallel machine learning library","Today, we’re excited to announce the release of SynapseML (previously MMLSpark), an open-source library that simplifies the creation of massively scalable machine learning (ML) pipelines. Building production-ready distributed ML pipelines can be difficult, even for the most seasoned developer. Composing tools from different ecosystems often requires considerable “glue” code, and many frameworks aren’t designed with thousand-machine elastic clusters in mind. SynapseML resolves this challenge by unifying several existing ML frameworks and new Microsoft algorithms in a single, scalable API that’s usable across Python, R, Scala, and Java.


Website: [https://aka.ms/spark](https://aka.ms/spark)

Paper: [https://arxiv.org/abs/2009.08044](https://arxiv.org/abs/2009.08044)

Blog: [https://www.microsoft.com/en-us/research/blog/synapseml-a-simple-multilingual-and-massively-parallel-machine-learning-library/](https://www.microsoft.com/en-us/research/blog/synapseml-a-simple-multilingual-and-massively-parallel-machine-learning-library/)

&#x200B;

&#x200B;

[SynapseMLs API aims to unify and simplify distributed ML](https://i.redd.it/0d0ure5oxd081.gif)",MachineLearning
qwt9ni,1637252298.0,[R] Graph Neural Networks through the lens of Differential Geometry and Algebraic Topology,"Differential geometry and algebraic topology are not encountered very frequently in mainstream machine learning. In a new series of posts, I show how tools from these fields can be used to reinterpret Graph Neural Networks and address some of their common plights in a principled way.

First post in the series - introduction:

[https://towardsdatascience.com/graph-neural-networks-through-the-lens-of-differential-geometry-and-algebraic-topology-3a7c3c22d5f?sk=791d11adb17b2e0a01f5089e4d460072](https://towardsdatascience.com/graph-neural-networks-through-the-lens-of-differential-geometry-and-algebraic-topology-3a7c3c22d5f?sk=791d11adb17b2e0a01f5089e4d460072)

&#x200B;

Part II will discuss the expressive power of GNNs and topological message passing.

Part III will deal with geometric flows and non-euclidean diffusion PDEs on graphs.

Part IV will show how the over-squashing phenomena can be related to graph curvature, and offer a geometric approach to graph rewiring inspired by the Ricci flow.",MachineLearning
qwsmnn,1637250575.0,[R] Intel’s Prune Once for All Compression Method Achieves SOTA Compression-to-Accuracy Results on BERT,"An Intel research team presents Prune Once for All (Prune OFA), a training method that leverages weight pruning and model distillation to produce pretrained transformer-based language models with high sparsity ratios. Applied to BERT, the approach achieves state-of-the-art results in compression-to-accuracy ratio.

Here is a quick read: [Intel’s Prune Once for All Compression Method Achieves SOTA Compression-to-Accuracy Results on BERT.](https://syncedreview.com/2021/11/18/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-147/)

The paper *Prune Once for All: Sparse Pre-Trained Language Models* has been accepted for a poster session at NeurIPS 2021 (December 6-14), and is on [arXiv](https://arxiv.org/abs/2111.05754).",MachineLearning
qwn3fz,1637232194.0,[P] DagYard - DVC x MLflow x Colab x Gdrive - Automatically Configured,"Hey [r/ML](https://www.reddit.com/r/ML/)! DVC and MLflow tracking are two of the most preferred open source tools for managing ML projects. They are heavily used by data scientists, widely adopted in the industry, with a great community & team behind them.

If you're not familiar with them, then in a nutshell:

* [DVC](https://dvc.org/) (Data Version Control) acts as an extension to Git and enables version control of large-scale files.
* [MLflow](https://mlflow.org/) tracking automates the logging process of experiments and sends live information to a local or remote server while the training is still running.

Our aspiration at DagsHub is to help data scientists to use great open source tools by lowering the entry bar.

My recent project, [The DagYard Notebook](https://colab.research.google.com/drive/1XLP2Ouxk-k6y9yOxc4Grp-Aq6aGcbhuj?usp=sharing), automates the process of configuring Google Colab with DVC and MLflow by simply checking some boxes, filling your user's details, and.. well, **that's it**. **You're all set to clone the repo and train your model.**

Your Colab will be fully configured with DagsHub, DVC, MLlflow, and based on your use case, also GitHub and Google Drive.

On top of those, I spiced it up with additional capabilities like using Google Drive as DVC's cache directory and avoid pulling the same file twice, copy files from Gdrive to DVC storage, initialize DVC, clone a specific branch, and more.

**Check it out here** 👉 [https://colab.research.google.com/drive/1XLP2Ouxk-k6y9yOxc4Grp-Aq6aGcbhuj?usp=sharing](https://colab.research.google.com/drive/1XLP2Ouxk-k6y9yOxc4Grp-Aq6aGcbhuj?usp=sharing)

If you have any ideas on how I can improve the notebook or what additional capabilities you're missing here - I'd love to hear about it!",MachineLearning
qwmizo,1637229654.0,[D] Benchmarking Deep Learning with M1 Pro GPU (Metal) vs Colab GPU (Tesla P80) and Kaggle (P100),"Hey, r/MachineLearning, If someone like me was wondered how M1 Pro with new TensorFlow PluggableDevice(Metal) performs on model training compared to ""free"" GPUs, I made a quick comparison of them: [https://medium.com/@nikita\_kiselov/why-m1-pro-could-replace-you-google-colab-m1-pro-vs-p80-colab-and-p100-kaggle-244ed9ee575b](https://medium.com/@nikita_kiselov/why-m1-pro-could-replace-you-google-colab-m1-pro-vs-p80-colab-and-p100-kaggle-244ed9ee575b)

In a nutshell, M1 Pro is 2x faster P80. P100 is 2x faster M1 Pro and equal to M1 Max. However, Transformers seems not good optimized for Apple Silicon.

&#x200B;

P.S. [Pytorch GPU support is on the way too](https://github.com/pytorch/pytorch/issues/47702#issuecomment-965625139)",MachineLearning
qwbi8e,1637190527.0,[D] Where to find the balance between writing code from scratch and just modifying code.,"So for my final year Aerospace Project I want to implement ML , I'm a newbie. But as it's my final year project I will need something presentable, the odds are most of the things I need in ML like Image recognition. Someone will have already written the code for it. So it feels like I'd just be copying and pasting their code. I know i will be modifying it to some degree but still. I don't think i'd learn much from it. obviously i don't wanna reinvent the wheel, but still. How does one go about this?",MachineLearning
qw603s,1637174739.0,[D] What sort of path does a neural network take through it's parameter space when training?,"Does it spend most of it's training getting slowed down from going through saddle points, or does it spend more time overshooting the minima it finds, going around it in a chaotic orbit. How often does it reach a local minima, just to be shot out of it by a sample with a large gradient, and then roll towards a different local minima.

Are these questions even answerable, or have the answers been known for awhile.",MachineLearning
qw55w6,1637172422.0,[Project] An overview of methods for Text Segmentation,"Text Segmentation is the task of splitting text into meaningful segments. There weren't many good overviews of this online, so I put together a project that outlines the different approaches/models that exist, how to evaluate these models, and some open source datasets that can be used for training Text Segmentation models.

[**For the full overview, you can read my outline here**](https://www.assemblyai.com/blog/text-segmentation-approaches-datasets-and-evaluation-metrics/)",MachineLearning
qw2c3p,1637164658.0,[R] Is BERT the Future of Image Pretraining? ByteDance Team’s BERT-like Pretrained Vision Transformer iBOT Achieves New SOTAs,"A research team from ByteDance, Johns Hopkins University, Shanghai Jiao Tong University and UC Santa Cruz seeks to apply the proven technique of masked language modelling to the training of better vision transformers, presenting iBOT (image BERT pretraining with Online Tokenizer), a self-supervised framework that performs masked prediction with an online tokenizer.

Here is a quick read: [Is BERT the Future of Image Pretraining? ByteDance Team’s BERT-like Pretrained Vision Transformer iBOT Achieves New SOTAs.](https://syncedreview.com/2021/11/17/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-146/)

The paper *iBOT: Image BERT Pre-Training with Online Tokenizer* is on [arXiv](https://arxiv.org/abs/2111.07832).",MachineLearning
qw0rxa,1637160230.0,[P] Organizing ML reproducibility – The reproducibility scale,"Hey r/ML! A lot has been said (ranted about) reproducibility in ML. It always seems like everyone agrees that reproducibility is important, but still many projects are difficult or impossible to reproduce.

Part of the reason is that reproducibility is hard to quantify, and different people define it very differently.

I wrote a blog trying to provide a concrete scale for reproducibility, with a practical rating system and a checklist that can help project creators/maintainers validate the reproducibility of their work.

The scale consists of 5 criteria (a 5-star system):

1. Code
2. Configuration
3. Data (+Artifacts)
4. Environment
5. Evaluation

With specific requirements for each one. I tried to order them in a growing level of complexity so that a 3-star project is lower effort, but less reproducible than a 5-star project.

I also add a format for a \`REPRODUCIBILITY.md\` file that can be filled out and added to projects to help the community assess the reproducibility of a project they want to use.

Check it out here 👉 [https://dagshub.com/blog/introducing-the-machine-learning-reproducibility-scale/](https://dagshub.com/blog/introducing-the-machine-learning-reproducibility-scale/)

I know this might be controversial, so I'd honestly love to have an open discussion – do you think I'm missing something, should the order be different? Would also love to get a PR to the format of \`REPRODUCIBILITY.md\`. You can do that [here](https://dagshub.com/DAGsHub-Official/reproducibility-challenge/src/master/REPRODUCIBILITY.md).",MachineLearning
qvvsuk,1637142390.0,[R] Category-orthogonal object features guide information processing in recurrent neural networks trained for object categorization,"Recurrent neural networks (RNNs) have been shown to perform better than feedforward architectures in visual object categorization tasks, especially in challenging conditions such as cluttered images. However, little is known about the exact computational role of recurrent information flow in these conditions. This paper tests RNNs trained for object categorization on the hypothesis that recurrence iteratively aids object categorization via the communication of category-orthogonal auxiliary variables (the location, orientation, and scale of the object). Using diagnostic linear readouts, we find that: (a) information about auxiliary variables increases across time in all network layers, (b) this information is indeed present in the recurrent information flow, and (c) its manipulation significantly affects task performance. These observations confirm the hypothesis that category-orthogonal auxiliary variable information is conveyed through recurrent connectivity and is used to optimize category inference in cluttered environments.

Preprint: [https://arxiv.org/abs/2111.07898](https://arxiv.org/abs/2111.07898)
Code/Data: [https://github.com/KietzmannLab/svrhm21\_RNN\_explain](https://github.com/KietzmannLab/svrhm21_RNN_explain)
Tweeprint: [https://twitter.com/martisamuser/status/1460631750640422912](https://twitter.com/martisamuser/status/1460631750640422912)
",MachineLearning
qvvd9c,1637140531.0,[D] Benchmarking ScaledYOLOv4 on out-of-dataset images,"ScaledYOLOv4 is the go-to model for object detection. We decided to test how well it does on a dataset different from the one it was trained on.

We used the Citypersons dataset for this experiment. It is a subset of the popular Cityscapes dataset, which only consists of person annotations.

We found precision and recall values of 0.489 and 0.448. We also found that object detection on this dataset was pretty good, even though the classes assigned to them were lacking at times.

Checkout details of the experiment at: [https://blog.mindkosh.com/benchmarking-scaledyolov4-on-citypersons-dataset/](https://blog.mindkosh.com/benchmarking-scaledyolov4-on-citypersons-dataset/)

You can also checkout the notebook we used for this experiment at

[https://github.com/Mindkosh/ScaledYOLOv4Experiments/blob/master/sample-colab-notebooks/CitypersonScaledYOLOv4.ipynb](https://github.com/Mindkosh/ScaledYOLOv4Experiments/blob/master/sample-colab-notebooks/CitypersonScaledYOLOv4.ipynb)",MachineLearning
qvueo6,1637136338.0,[D] Is Microsoft CMT down (for CVPR 2022)??,"Good God I'm dying wtf I cannot access to the CMT & official page for CVPR 2022

Anyone else?",MachineLearning
qvt9vu,1637131606.0,[D] How do I integrate Squeeze and Excitation block in YOLOv5?,"From my understanding, [squeeze and excitation block](https://arxiv.org/abs/1709.01507) improves performance of networks at the cost of additional computation. I'm trying to integrate it into YOLOv5 for object detection but I am struggling to figure out where and how to integrate it.

I have been unable to find any open-source implementation for it so I would really appreciate any help!",MachineLearning
qvmkjg,1637109287.0,[D] Surprisingly Simple SOTA Self-Supervised Pretraining - Masked Autoencoders Are Scalable Vision Learners by Kaiming He et al. explained (5-minute summary by Casual GAN Papers),"The simplest solutions are often the most elegant and cleverly designed.  This is certainly the case with a new model from Facebook AI Research  called Masked Autoencoders (MAE) that uses such smart yet simple ideas  that you can’t stop asking yourself “how did nobody think to try this  before?” Using an asymmetric encoder/decoder architecture coupled with a  data-efficient self-supervised training pipeline MAE-pretrained models outperform strong supervised baselines by learning to reconstruct input images from heavily masked image patches (75% blank patches).

Full summary: [https://t.me/casual\_gan/189](https://t.me/casual_gan/189)

Blog post: [https://www.casualganpapers.com/self-supervised-large-scale-pretraining-vision-transformers/MAE-explained.html](https://www.casualganpapers.com/self-supervised-large-scale-pretraining-vision-transformers/MAE-explained.html)

[MAE](https://preview.redd.it/emd79asgw1081.png?width=1830&format=png&auto=webp&s=932fdda9755f2c8883fafd211261c366a938c24f)

UPD: I originally included the wrong links
[arxiv](https://arxiv.org/pdf/2111.06377v1.pdf) / code - ?

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",MachineLearning
qvm71a,1637108149.0,[D] Do big tech companies do coding interviews for ML Ph.D. research/student scientist intern positions?,I just want to know if I need to practice on LeetCode.,MachineLearning
qvl3d5,1637104897.0,[P] PyTorch-LIT - Infer Large Models That Don't Even Fit in Main Memory,"Deep learning models are rapidly growing in terms of size and complexity, and inference on end devices is becoming impossible. GPT-J with 6B parameters, for example, only requires 24 GB of RAM in full-precision mode to be ready for execution, which may be impossible in most systems; even a powerful GPU like RTX 2060 with 6 GB of memory can't even contain GPT-J in half-precision mode, making direct inference impossible.

**PyTorch-LIT** solves the problem by running large models on end devices and loading parameters from a secondary memory as needed. For the time being, we are using disk as secondary memory, but we intend to implement faster alternatives in the future.

Github: [https://github.com/AminRezaei0x443/PyTorch-LIT](https://github.com/AminRezaei0x443/PyTorch-LIT)",MachineLearning
qvklgh,1637103487.0,[D] How to run the huge language models in transformers,"Hii everyone,

I'm very interested in trying out [T0pp](https://huggingface.co/bigscience/T0pp) from the big science project and, lately, there appear more and more big models which we are not able to run on our machines or Colab. I have colab pro but still not able to run [T0pp](https://huggingface.co/bigscience/T0pp).

I'm wondering how do you experiment with those big models as they don't fit in colab?",MachineLearning
qviuar,1637098721.0,[D] Post-undergrad paths for machine learning?,"I'm an econ major graduating in the spring from a top 40 uni - just found out I really like machine learning and AI. I'm not too tech-savvy or particularly good at math but I can get by. I'm a really fast learner. Currently learning Python. Econometrics has probably been my favorite class.

Where can I go from here? I graduate so soon - should I be looking at grad programs in finance, data science, etc. Or, should I just try to find a job immediately? Any particular tips on how to break into the finance world with machine learning? What kinds of firms should I be looking at for jobs? Would really appreciate any advice or tips.",MachineLearning
qvhazn,1637094611.0,[D] Is it possible to generate text based of a previously generated text with huggingface,"I currently have a GPT-2 based text generator that I trained on  Shakespeare plays.  Currently, the generated text looks like this

```
First Murderer What is't?
MARK ANTONY Sirrah, I pray thee, thou wilt fight with my sword.
CASSIUS If I know thee, sir, I cannot tell: but if I knew't, we should have it but that: 'tis not well thou art no sooner.
MARK ANTONY Why, what's't?
TIMON So are the very hour! 'tis but to have the best to make it, and I am sorry of it.
MALVOLIO And he, his lord.
KING CLAUDIUS If that had been but I will not.
KING HENRY VI My Lord Talbot.
FLORIZEL Ay, sir, 'tis better than I.
PROSPERO In the measure of it shall be well. Come in thy mouth, come, I'll be some water, or else, For if thou couldst hear me think Thy face were quick, but by thyself, my heart With this new heart, I could have it with thee. Come, no, no, for thy heart, O, he shall have the blood Which he is to a fair and thy bed, his death's in peace! Now, for thy heart Did he not look? It must be an hour's day, a death, Which shall I have it.
```

However the problem is that the text is very disconnected and not really coherent. I'm not really sure if huggingface provides an API to do this, but I'd like to be able to have the x_i generation somehow take into account the x_{i-1}, which I feel like would make the text a lot more coherent and less disconnected. In this case, each piece of generated text is started with the all caps character name.

Does anyone who knows a bit more about this stuff know a better way to generate realistic and large amounts of coherent text? The problem is that we can only generate 768 tokens at a time, so obviously models that write entire paragraphs of text have figured out good way to use previous output in text generation as some sort of initial state for the future generation.

I'll also add that if I just pass in the previous input_ids into the model.generate method then it doesn't generate any more text since the previous text already has an EOS token.",MachineLearning
qvh045,1637093801.0,[D] the risks of relying on inter-rater reliability in ML / data labeling,"Does anyone else feel like IRR is often relied on too heavily for assessing data quality? Now that so many ML use cases involve highly subjective tasks like content moderation, sentiment analysis, etc, I think we need to reconsider how to think about the relationship between IRR and data quality.

[Recently wrote a blog](https://www.surgehq.ai/blog/the-pitfalls-of-inter-rater-reliability-in-data-labeling-and-machine-learning) on this subject, but curious for this community's thoughts on the matter too. Any examples of instances where an over-reliance on IRR has caused problems for you down the road?",MachineLearning
qvgc13,1637092073.0,[N] U.S. FDA Fellowships in Regulatory Science for Machine Learning Systems,"* ***To apply, see link:*** [https://www.zintellect.com/Opportunity/Details/FDA-CDRH-2022-01](https://www.zintellect.com/Opportunity/Details/FDA-CDRH-2022-01)
* ***Anticipated Appointment Start Date:*** As soon as a qualified candidate is identified; start date is flexible.
* ***Location:*** Silver Spring, Maryland

&#x200B;

**Organization**

*U.S. Food and Drug Administration (*[*FDA*](https://www.fda.gov/)*)*

*Center for Devices and Radiological Health (*[*CDRH*](https://www.fda.gov/about-fda/fda-organization/center-devices-and-radiological-health)*)*

*Office of Science and Engineering Laboratories (*[*OSEL*](https://www.fda.gov/about-fda/cdrh-offices/office-science-and-engineering-laboratories)*)*

*Division of Imaging, Diagnostics, and Software Reliability (*[*DIDSR*](https://www.fda.gov/about-fda/cdrh-offices/division-imaging-diagnostics-and-software-reliability)*)*

*Artificial Intelligence and Machine Learning Program (*[*AI/ML*](https://www.fda.gov/medical-devices/medical-device-regulatory-science-research-programs-conducted-osel/artificial-intelligence-and-machine-learning-program-research-aiml-based-medical-devices)*)*

&#x200B;

**Assignment**

Continual learning in machine learning is poised to bring changes to the speed at which the healthcare industry will adapt to the changes in patient management. Research in this area is still at a nascent stage with several recent research publications aiming towards solving the Plasticity-Stability dilemma. This is a critical time for the agency to develop performance assessment strategies to evaluate the safety and effectiveness of these continual learning algorithms. In this project, our goal is to develop a evaluation framework for continual learning algorithms specifically for segmentation and classification tasks. The research fellow will play a key role in developing and evaluating AI/ML algorithms.

&#x200B;

**Job responsibilities**

Under the guidance of a mentor, the participant will be involved in the following activities:

* Conduct research to answer emerging evaluation challenges in medical imaging and diagnostics systems
* Contribute to the Agency’s regulatory efforts by providing technical expertise
* Collaborate with team members and stakeholders to complete and report widely
* Disseminate findings and regulatory science tools in conferences and peer-reviewed journal publications

&#x200B;

**Candidate qualifications**

The qualified candidate should be *currently pursuing or have received a master's or doctoral degree* in one of the relevant fields. Degree must have been received within the past five years.

***Preferred skills:***

* Strong background in the fundamentals and an eagerness to solve technical challenges systematically with experimental and/or computational approaches
* Developing and analyzing AI/ML methods (CNN, RNN, GAN, etc.)
* Programming with Python (including scientific stack: NumPy, SciPy, scikit-learn, etc.), and deep learning frameworks (TensorFlow, PyTorch, etc.)
* Experience with image segmentation, processing and data management",MachineLearning
qvfn86,1637090334.0,[D] Heterogeneous processor architectures and machine learning,"The recently introduced Intel Alder Lake processors (and less relevantly to this application, Apple Silicon) take a new approach to processor design by providing two types of cores on the same CPU die. On one side, full sized high-performance cores, optimized for single core performance, high clock rates, and full support for SMT. On the other side, smaller, more energy-efficient cores, that make some performance compromises yet aim to be capable for more parallel tasks.

For the machine learning community, these (Intel) processors are really only relevant for prototyping workstations equipped with one or two GPUs; they don’t scale to large production servers. For someone intending to build a machine like this, the Intel 12900K and AMD 5950X may be the most interesting contenders for processor choice, as they deliver high performance without falling into the much more expensive workstation-class Xeon and Threadripper processors. Intel has a slight edge on single-core performance on most synthetic benchmarks, but things are unclear when it comes to multi-core performance: no clear winner emerges.

This is somewhat digressing, but there is also a discussion to be had about new forward-looking features on the Intel platform (PCIE 5.0 and DDR5 support) — a set of upsides that also come with the early adopter instability and price inflation — versus the stable, affordable, yet not-forward-compatible status of AMD’s AM4 platform.

As far as I am aware, not much discussion has taken place over how an heterogeneous architecture might translate to training and inference workloads. A lot of the press covering these new processors focus on gaming.

I have to admit I am myself not very well versed in the ways CPU-side parallelism is leveraged in these tasks, so the impact this might have on performance is not obvious to me. Correct scheduling (assigning performance cores to the threads that actually need them) likely is a part of this — meaning operating systems matter here. I vaguely know certain models and architectures (RNNs, RL) and data-related tasks (augmentation and pre-loading) are more strongly tied to CPU performance.

I would be curious to have peoples’ perspectives and insights on this.",MachineLearning
qvd8ay,1637084135.0,[D] What are some must-read papers on video segmentation and summarisation?,"I'm looking to read up on the major advances and contributions in video segmentation and summarisation (both static: keyframes extraction and dynamic: video skimming). To this end, what are some important and enlightening papers on this topic that you'd recommend to read up on? I'm currently reading [this survey paper](https://arxiv.org/abs/2101.06072) by Apostolidis et. al., but I was wondering if the community has any recommendations on ""must-definitely-read"" papers on the topic.

Please feel free to even suggest any papers that don't directly address the task of video segmentation/summarisation, but introduce techniques/architectures that have subsequently been applied extensively for the same.",MachineLearning
qvc3cb,1637081260.0,"[D] How do you create new neural architectures, blocks or layers?","I keep using the models that others invent and come up with, how can I learn how to create my own type of layers, or blocks? how did they come up with squeeze and excitation layers in efficient net for example? where do I start?",MachineLearning
qv9rfy,1637075093.0,[D] Best mobile architectures for real-time semantic segmentation,"Hey guys!
Recently

I've been working on a project for mobile involving semantic segmentation recently. I’m running quantized Unet with mobilenetv2 backbone, deployed using TFlite. However, the FPS performance isn't good enough for real time segmentation.

What are the best performance-wise (best FPS rates) segmentation models to run on mobile currently? I've been researching architectures on https://paperswithcode.com/task/real-time-semantic-segmentation, but those are very often evaluated on GPUs.",MachineLearning
qv9ez4,1637074109.0,[D] Tools for feature selection for deep learning,"Let's say I have access to a feature store with pretty much every imaginable derived feature available. Is there a tool available specifically for testing feature combinations and feature scaling, upsampling, downsampling. I know there's weights and biases, but I feel like (and I could be wrong, please chime in if you know) they are mainly about hyper parameter optimization. This would be variations of the data itself, where within each version you would want to possibly explore hyperparameter tuning.

Even if the answer for feature selection is to dump them all in and use something like l2 regularization,  different thresholds and things like scaling, class imbalance fixes, imputing etc all create different versions of the input data. Are there any good approaches or tools to track experiments around the preprocessing and feature selection part?",MachineLearning
qv5nbi,1637061594.0,[N] Interview with Raquel Urtasun of Waabi and BEST OF ICCV papers on Computer Vision News,"Dear all,

Have a peek at Computer Vision News of November.

Many articles about AI, Deep Learning, Computer Vision and more...

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2021November/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2021-november-pdf/)

Dilbert on page 2. Free subscription on page 74.

Enjoy!

https://preview.redd.it/jfmlp6xwyxz71.jpg?width=700&format=pjpg&auto=webp&s=0caa6dc118736ae6f20589ed2a4c8a0f00887304",MachineLearning
qv5292,1637059237.0,[R] GAN for Animated GIF / video clip generation.,"I am looking for some prior open source / GitHub code to generate animations / video clips. On google, I am getting results for Anime (which I don’t want).   Is there work that trains a Style GAN on a bunch of video clips / animated GIFs, and generates brand new ones?",MachineLearning
qv3xpt,1637054501.0,[D] CyCADA feature-level GAN loss,"In the CyCADA paper: [https://arxiv.org/pdf/1711.03213.pdf](https://arxiv.org/pdf/1711.03213.pdf)

Equation 5 in the paper makes use of f\_S, however, the corresponding portion in Figure 2 (orange) only makes use of f\_T. Additionally, f\_T makes more sense since the input is in the target domain. Should the f\_S in equation 5 be f\_T?",MachineLearning
qv1t0h,1637045524.0,[D] Labeling tool for re-ID dataset,"I want to create re-ID dataset with 10 classes. However, I couldn't find tools that allow ID assignment. The closest thing I found is CVAT, but it doesn't allow me to create bounding box with specific ID.

Does anyone know other tools that are suitable for multiclass re-identification?",MachineLearning
qv0jg1,1637040756.0,"[D] Does getting into top4(Stanford, CMU, MIT, Berkeley) ML phd program requires more than 3 first-author papers at top conferences?","

[https://www.reddit.com/r/MachineLearning/comments/lrqmd5/d\_admissions\_standards\_at\_top\_programs/](https://www.reddit.com/r/MachineLearning/comments/lrqmd5/d_admissions_standards_at_top_programs/)

I was reading this post and one of the comments mentioned that

* 3+ first-author ML papers in top conferences
* 1+ spotlight or oral paper
* Co-organized a conference workshop
* Met with the PIs/professors before applying
* Masters (or double bachelors major) in CS, statistics, or math from a top 20 school

The candidate should satisfy at least 3-4 of the above criteria to get into the top 4 ml phd programs. Is this true? Is it really this hard? I mean, if a person satisfies 4 of the aforementioned criteria, doesn't he or she can just get a phd right away?

Also, is it much harder for international students compared to US students?(given that the international student graduated from American university)",MachineLearning
quvv7h,1637025943.0,"""[R]"" A deep generative model enables automated structure elucidation of novel psychoactive substances","Over the past decade, the illicit drug market has been reshaped by the proliferation of clandestinely produced designer drugs. These agents, referred to as new psychoactive substances (NPSs), are designed to mimic the physiological actions of better-known drugs of abuse while skirting drug control laws. The public health burden of NPS abuse obliges toxicological, police and customs laboratories to screen for them in law enforcement seizures and biological samples. However, the identification of emerging NPSs is challenging due to the chemical diversity of these substances and the fleeting nature of their appearance on the illicit market. Here we present DarkNPS, a deep learning-enabled approach to automatically elucidate the structures of unidentified designer drugs using only mass spectrometric data. Our method employs a deep generative model to learn a statistical probability distribution over unobserved structures, which we term the structural prior. We show that the structural prior allows DarkNPS to elucidate the exact chemical structure of an unidentified NPS with an accuracy of 51% and a top-10 accuracy of 86%. Our generative approach has the potential to enable *de novo* structure elucidation for other types of small molecules that are routinely analyzed by mass spectrometry.

Check out our new paper on automatic structure elucidation of new “legal highs” using artificial intelligence and mass spectrometry:

Published at: [https://www.nature.com/articles/s42256-021-00407-x](https://www.nature.com/articles/s42256-021-00407-x)

Arxiv: [https://chemrxiv.org/engage/chemrxiv/article-details/60c7591bf96a0058f0288f5e](https://chemrxiv.org/engage/chemrxiv/article-details/60c7591bf96a0058f0288f5e)",MachineLearning
quvctp,1637024398.0,"""[R]"" MLOps: A New Era of DevOps, Powered by Machine Learning"," With the interesting mix of terms “AI-ML” and “Development Operations,” MLOps is an assortment of methods, that is utilized for AI and its life cycle computerization and its calculations in execution for enormous scope. It clears a smooth way for a coordinated effort between an information researcher and IT proficient, and consequently goes about as a scaffold between the abilities, methods, and apparatuses utilized in information designing, AI, and DevOps. [Read more](https://insights2techinfo.com/mlops-a-new-era-of-devops-powered-by-machine-learning/)",MachineLearning
quv04i,1637023283.0,[D] Paper Explained - Gradients are Not All You Need (Full Video Walkthrough),"[https://youtu.be/EeMhj0sPrhE](https://youtu.be/EeMhj0sPrhE)

More and more systems are made differentiable, which means that accurate gradients of these systems' dynamics can be computed exactly. While this development has led to a lot of advances, there are also distinct situations where backpropagation can be a very bad idea. This paper characterizes a few such systems in the domain of iterated dynamical systems, often including some source of stochasticity, resulting in chaotic behavior. In these systems, it is often better to use black-box estimators for gradients than computing them exactly.

&#x200B;

OUTLINE:

0:00 - Foreword

1:15 - Intro & Overview

3:40 - Backpropagation through iterated systems

12:10 - Connection to the spectrum of the Jacobian

15:35 - The Reparameterization Trick

21:30 - Problems of reparameterization

26:35 - Example 1: Policy Learning in Simulation

33:05 - Example 2: Meta-Learning Optimizers

36:15 - Example 3: Disk packing

37:45 - Analysis of Jacobians

40:20 - What can be done?

45:40 - Just use Black-Box methods

&#x200B;

Paper: [https://arxiv.org/abs/2111.05803](https://arxiv.org/abs/2111.05803)",MachineLearning
quutqc,1637022726.0,"[R] [online, free] Hong Kong Machine Learning Meetup! 17 October 7pm (GMT+8)","https://preview.redd.it/ysrie4qiruz71.jpg?width=1058&format=pjpg&auto=webp&s=51f39633ebf7715f623407004a230166df35f60d

[https://www.meetup.com/Hong-Kong-Machine-Learning-Meetup/events/281165939/](https://www.meetup.com/Hong-Kong-Machine-Learning-Meetup/events/281165939/) Join us for HKML S4E4, a monthly free & online machine learning meetup. Be aware, technical content.

Talks:

1. Application of Natural Language Processing to unstructured financial text to create alternative data for Finance;
2. AI Automation and how it's a game-changer on the way we apply AI to Business;
3. TBD",MachineLearning
qutuo6,1637019844.0,[D] Resources on on-line machine learning,"I am wondering if there are any books/articles/tutorials about ""on-line machine learning""?

&#x200B;

For example, this website has nice lecture notes (from lec16) on some of the aspects:

[https://web.eecs.umich.edu/\~jabernet/eecs598course/fall2015/web/](https://web.eecs.umich.edu/~jabernet/eecs598course/fall2015/web/)

&#x200B;

or this book:

&#x200B;

[https://ii.uni.wroc.pl/\~lukstafi/pmwiki/uploads/AGT/Prediction\_Learning\_and\_Games.pdf](https://ii.uni.wroc.pl/~lukstafi/pmwiki/uploads/AGT/Prediction_Learning_and_Games.pdf)

&#x200B;

&#x200B;

I can't seem to find much resources on this. I'm trying to understand the basics, not read research papers. If anyone can share resources that would be nice.",MachineLearning
quta9z,1637018199.0,[P] How to get access to the GPT-3 API in late 2021,I'm trying to get access to the GPT-3 API and I applied following the guidelines (form + email to the CTO 😇) yesterday. Should I wait a few more days or should I go creative? Is it still super hard to get access to the API? Thanks.,MachineLearning
qus9jr,1637015348.0,[D] Speech to Text - Google API or something else?,"

Hi all,

Would Google's ASR API be considered the best thing around, or are there others that are equally good if not better?

I'm particularly interested in any open source or freely available code that could be customized.

Specifically,  I'm looking into building an app that I can train with my own voice,  and then use for proofing audio recordings against pre-prapared texts.  I'm hoping to work with Max MSP as I'm no good at code!

I've  seen examples of Max patches than can query google's API for speech  recognition, and I believe there's at least one example of a max object  ""op.recognize"" that  can also do speech to text.

That's as far as I've got so far so thought I'd ask here.",MachineLearning
qup0fe,1637006498.0,"[Discussion] Thoughts on manually modifying a model's output for more ""optimistic"" results","Hi.

I'm currently working as a freelancer on a delivery company which predicts an order's estimated time of arrival (ETA) using machine learning.

What is strange to me is that they have information about ""how saturated"" the delivery area is (whether it's because of weather, traffic, etc.), and after getting a model's prediction, they check for saturation and add X minutes to the model's predicted ETA, thus manually modifying the model's output for more ""optimistic"" results.

What is your opinion on this? Is this bad practice? Why would or wouldn't you take this approach?",MachineLearning
qulcqf,1636996617.0,[P] Bayesian models of perception and action (book),"Draft of an upcoming book by Wei Ji Ma, Konrad Kording, and Daniel Goldreich to be published by MIT press:

https://www.cns.nyu.edu/malab/bayesianbook.html",MachineLearning
qukqtz,1636995022.0,"""[R]"" Knowledge Graph: Applications with ML and AI and Open-Source Database Links in 2022"," A new terminology is coined by Google in 2012 “Knowledge Graph”. This knowledge graph has its own significance in the field of machine learning due to which, performing capabilities of machine learning techniques are getting better day by day with a high accuracy rate. [Read more](https://insights2techinfo.com/knowledge-graph-applications-with-ml-and-ai-and-open-source-database-links-in-2022/)",MachineLearning
qukghv,1636994252.0,[D] What is Riemannian Manifold intuitively?,"Recently I was studying dimensionality reduction. When I come to a state-of-the-art dimensionality reduction algorithm -- UMAP, I couldn't understand their mathematics part. I think the first obstacle to understand it is -- I do not understand what is a Riemannian manifold. I have watched multiple videos and tutorials on the Riemannian manifold, but I still cannot catch the idea easily. Seems like the first thing they explain already requires complicated maths knowledge.

I would like to ask if anyone can really explain it in one simple sentence, that a normal computer science student or year one undergraduate STEM student will understand.

Thank you very much!",MachineLearning
quip0a,1636989440.0,[D] Siraj Raval YouTube video,"Siraj has put up an interview that Lex Fridman did about two years ago.

[Lex Fridman interviews Siraj Raval](https://www.youtube.com/watch?v=E1uqI6K_FLM)

Lex took the video down from his channel. Siraj asked for permission to repost the video, Lex didn't reply. So, Siraj took it upon himself to post it anyway.",MachineLearning
quddpi,1636971744.0,"[R] Open AccessArticle Detection of Bovine Mastitis in Raw Milk, Using a Low-Cost NIR Spectrometer and k-NN Algorithm","We have recently published on Applied Science this paper which some people in the community may find interesting. It is an OPen Access, therefore no one should have problems to access to it.

 https://doi.org/10.3390/app112210751

[https://www.mdpi.com/2076-3417/11/22/10751/htm](https://www.mdpi.com/2076-3417/11/22/10751/htm)

The abstract is:

 Among the bovine diseases, mastitis causes high economic losses in the dairy production system. Nowadays, detection under field conditions is mainly performed by the California Mastitis Test, which is considered the de facto standard. However, this method presents with problems of slowness and the expensiveness of the chemical-reactive process, which is deeply dependent on an expert’s trained eye and, consequently, is highly imprecise. The aim of this work is to propose a new method for bovine mastitis detection under field conditions. The proposed method uses a low-cost, smartphone-connected NIR spectrometer which solves the aforementioned problems of slowness, expert dependency and disposability of the chemical methods. This method uses spectra in combination with two k-Nearest Neighbors models. The first model is used to detect the presence of mastitis while the second model classifies the positive cases into weak and strong. The resulting method was validated by using a leave-one-out technique where the ground truth was obtained by the California Mastitis Test. The detection model achieved an accuracy of 92.4%, while the one classifying the severity showed an accuracy of 95%",MachineLearning
quclg6,1636968402.0,[D] How ML is helping us understand animal communication [Video],"Hi r/machinelearning,

Visualizing sound is enabling us to analyze animal voices and get a better of sense of what information they contain. This form of audio analysis is powerful because it empowers human speech recognition technology and is opening the door towards better understanding of how dolphins, whales, and prairie dogs communicate.

This is the second video in my look at Spectrograms, which have been fascinating to look at.

[https://www.youtube.com/watch?v=5B2gwz7BVCA](https://www.youtube.com/watch?v=5B2gwz7BVCA)

&#x200B;

Related research:

DCASE: Detection and Classification of Acoustic Scenes and Events
http://dcase.community/

""The Analysis and Interpretation of Animal Vocalisations"" Call for Papers
[https://www.mdpi.com/journal/applsci/special\_issues/Animal\_Vocalisations](https://www.mdpi.com/journal/applsci/special_issues/Animal_Vocalisations)",MachineLearning
qub6oz,1636962407.0,[D] does tpu(v3-8) can train the big model? (momory for 1 batch is over 16gb),"I know the tpu

v3-8 has 8 \* 16gb(each v3) = 128gb memory.

I want to know this v3-8 can train the big model which 1-batch size is over over 16gb.

128gb memory is just like integrated memory? or just we have to model parallel in this situation.",MachineLearning
qu7dgv,1636948316.0,[D]Should I re-use valid data in incremental learning,"In incremental learning, the dataset is split into multiple segments and  the model is trained over each segment, so the model can be trained over a large dataset using limited RAM. As each segment has its own train,  valid and test, is it OK to reuse the valid data in the training data of the next segment? How about the test data?",MachineLearning
qu2de6,1636932327.0,[D][P] Need Stochastic Environments,"

Hello All,

After extensive search I am unable to find any good stochastic environments to train my algorithms on. I only found some toy-text based ones, since my algorithm is a variant of DQN I wanted environment with huge state space but a discrete action space.

Any help would be really appreciated!

Thanks.",MachineLearning
qu1e0q,1636929392.0,[D] Examples of when humans + ML model working together outperform either in isolation? (Maybe when interpretability/explainability methods used?),"In an ideal world, automated explanations of model outputs would make it possible for a human expert and a classifier working together to outperform either working in isolation. (Like an ensemble, except instead of having N models that you combine in some way, you have 1 model and 1 human who chooses whether to accept the model output or to reject it and go with their own opinion instead; “success” is if this setup outperforms both the model and the human acting alone).

There's some classic literature by Paul Meehl arguing that when you allow human experts to second-guess the outputs of even simple regression-based classifers like this, having a human expert in the loop doesn't help and generally makes things worse. But intuitively it seems like methods for explainability/interpretability could help the human know when to defer to the model and when to go with their own opinion. For example, maybe a skin cancer classifier predicts that some particular lesion is a melanoma, but the dermatologist can see from a saliency map that the classifier is not paying attention to the right part of the image, and rightly ignores the classifier in that particular instance. Whereas in a different case where the dermatologist is less certain and the saliency map makes sense, they defer to the machine classification.

Is there research that looks at whether the combo of {human expert(s) using ML model with some automated ‘explanation’ of the ML model outputs (saliency maps, LIME, whatever)} outperforms both the human experts and the ML model in isolation? I feel like this should be a super common evaluation paradigm in the interpretability literature, and perhaps it is and I’m just using the wrong keywords, but I’m not finding a lot.

Would be especially interested in cases where the ML model in question is a language model, but anything at all in this realm would be helpful.


edit: Thanks so much for all the great responses, really pleasantly surprised that this sparked so much discussion. Lots to dig into here, I appreciate it.",MachineLearning
qu0fvq,1636926749.0,[D] Change the resolution of a model (GAN),"Hi guys,

I want to change the resolution of a GAN I'm currently working with. The generator and discriminator consist of convolutional layers (Conv2DTranspose and Conv2D). How do I know how to change each layer to get for example a twice as big resolution? I tried to google it the whole day but couldn't find a feasible solution. I really appreciate any help you can provide.

This is the Code for the GAN (the tensorflow version): [https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/](https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/)",MachineLearning
qtzbu1,1636923606.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 125,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|121-130|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)|[Week 121](https://reddit.com/pmzx3g)|||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)|[Week 122](https://reddit.com/pw14z5)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)|[Week 123](https://reddit.com/q5fi12)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)|[Week 124](https://reddit.com/qjxfu9)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)|[Week 117](https://reddit.com/omy345)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)|[Week 118](https://reddit.com/ovz52j)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)|[Week 119](https://reddit.com/p50knh)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)|[Week 120](https://reddit.com/pe2idh)||

Most upvoted papers two weeks ago:

/u/CatalyzeX_code_bot: [Paper link](https://arxiv.org/abs/2012.09841)

Besides that, there are no rules, have fun.",MachineLearning
qtsomm,1636905330.0,Reverse Language Reconstructing by Consensus [D] [P]," This wasn't a priority for me for a few months as I was collecting data, but it is becoming one now for the next stage of my project. I have 70,000 rows x 12 columns which I've been working on to reconstruct some of the words from Classic Latin. Currently after lemmatization, there are 9000 words that are unknown in English. Many were unable to be lemmatized. However, by translating Latin into every language that evolved from Latin, I'm able to fill the pieces. For instance: Errat in English was Errat. I could not find it within the dictionaries. We can assume what it means, but when we follow it down the line. The process is: Latin to German to English, Latin to Spanish to English, Latin to Catalan to English, and so on and so forth. Going down the list we have: to err, irren, err, to be wrong, err, wander, err, shine, to rail, err, errar, to make a mistake, made a mistake, fout, wrong.

We can conclude that it means error. That's obvious. 'Err' would have been perfectly acceptable as it is English. However, I can't go through this line by line. Especially not by myself. I have tried to create a natural language consensus for each row, but it doesn't work. The issue is getting it the right data. I've tried a pattern such as 1, 1, 1, 1, 1, 1, 2, a, 2, 3, a, 1, a, 2 = 1. However, it spit out gibberish instead of choosing the most appropriate column. It's almost impossible to tell the machine that fout, wrong, and made a mistake are synonyms unless I feed it a massive thesaurus.

That's also an easy one. There's a word which has, supporter, journey, wandering, traveler, repairer, refresher. None are the same but they illustrate a very unique definition. We can see that the main subject of the translated word is to be an explorer; the indirect subject is someone who supports, fixes (refreshes) or repairs. I know back in ancient times that the rich had repairmen for wagons, tents, anything while traveling. This would most likely be that person. But the word is unknown to be, so the best I could expect a machine to do would be to summarize the two main ideas from all of the definitions.

This could be super simple and I'm just overthinking, or it's very complex and I'm underthinking. Would love your suggestions and opinions. I guess it could be as simple as a summarization of each row.",MachineLearning
qtiqlw,1636866927.0,Walk-Forward Target Encoding and Data Leakage [D],"Hi!

I am working on a time series problem where I need to also test historical predictions through time (using a walk-forward procedure). There is a ""data leakage"" when dealing with time series and using simple target encoding features like :

df.groupby(col)\[target\].transform('mean').

&#x200B;

I am wondering if anyone has built a custom function to build these target encoding features without leaking data... Kind of like a walk-forward target encoding function...",MachineLearning
qtik3a,1636866248.0,[P] Lyric Studio - Artificial Intelligence Song Lyrics,"I created an AI Generated Song Lyrics app. Thought this subreddit would love it.  PyTorch for text generation, and also does live sentiment analysis with background shading

Link: [https://LyricStudio.com](https://LyricStudio.com)

Do you have any feedback or improvements?",MachineLearning
qt90rt,1636834058.0,[D] Algorithms for correlation of events/issues,"Generally any application software co-exists with multiple other software where a problem with one software can have a cascading effect on some other software, somewhere else in the stack. E.g 

1. you deploy you application as a pod in kubernetes (orchestration software)
2. but the pod or container is not running after sometime because the ec2 machine (or any virtual machine) on which the pod was scheduled to run has some issues.
3. the ec2 machine or the vm is having some issues because the autoscaling software that is supposed to manage the vms properly is not working correctly.
4. the autoscaling software is not working correctly because of some other dependent system not doing this job.
5. basically there can be a CHAIN of issues where one issue can have a significant cascading effect on many other dependent software systems.

You can apply this logic to many other places. Imagine your application is not working properly BECAUSE your load balancer is not working properly which is BECAUSE you load balancer vms are having networking issues because you have some datacenter level failures etc. You have all the logs files spread across the stack which reports these issues independently but usually it takes manual effort to CORRELATE them to figure out what is the root cause.

typically the user sees the symptoms at a high level in the stack, e.g. application is working properly and then they will start debugging and then finally you figure out somewhere down the stack something is wrong. This usually takes specific expertise (SRE) and takes time to arrive at the root cause.

basically what is happening is here is a CHAIN of events with cascading effect. There are ways to catch this using monitoring dashboards but the problem is usually those monitoring dashboard are setup in a static way, manually setup and typically has a maintenance problems long term. i.e. if there is a change in one of the software version which wants you to change a the dashboard accordingly, then you might forget to modify the monitoring dashboard accordingly etc. Also setting up these monitoring dashboards are very specific to the problem, this means you need to setup various different kinds of dashboards for different systems & scenarios.

We want to apply AI to this problem if possible. We want to come up with a well trained AI based system which can tell you which is the actual root cause issue if you give it 10 different issues that happened around a specific time interval. 

E.g.

\- We will have a model that detects various issues (anomaly detection based software which we already have). So we now have the list of issues that happened across the stack around a certain time interval. Lets say 10 issues

\- Now let s say we have 10 different issues that happens around a particular time interval (1-5 mins), we would like to send all the 10 issues to AI based system and we want it to tell that out of these 10 issues, 1 or 2 issues is most probably the root cause which could have caused all the other issues.

If we have such AI based system that will basically CORRELATE multiple issues and tell us the ROOT CAUSE i.e. basically which of those issues is likely to have caused the other issues, that will be very useful.

Is this possible? Any thoughts/guidance will be greatly appreciated. What are all the typical algorithms/approaches people apply for this kind of problem.

Imagine a use case for this where if we have this systems, I can send in a bunch of alerts coming in (assume the alert has the relevant data about the issue attached to it) and this AI system can process it and segregate those alerts in a such a way, it informs the user which alert is actual issue which is causing other. This would mean user can quickly resolve it getting rid of other alerts. There are many such use cases like this",MachineLearning
qt4y6g,1636821678.0,[D] (Paper Overview) MAE: Masked Autoencoders Are Scalable Vision Learners," **Video**

[https://youtu.be/LKixq2S2Pz8](https://youtu.be/LKixq2S2Pz8)

**Paper**

[https://arxiv.org/abs/2111.06377](https://arxiv.org/abs/2111.06377)

**Abstract**

 This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.",MachineLearning
qt4k2c,1636820465.0,"[D] A quick history of GANs - 8 years of GAN evolution, and the intuition behind it explained by Casual GAN Papers","This tutorial covers the intuition behind:

* Variational Auto Encoder (VAE)
* The OG GAN
* StyleGAN
* VQGAN

Telegram post: [https://t.me/casual\_gan/184](https://t.me/casual_gan/184)

Blog post: [https://www.casualganpapers.com/history-of-gans-survey-of-popular-architectures/GAN-architectures-overview.html](https://www.casualganpapers.com/history-of-gans-survey-of-popular-architectures/GAN-architectures-overview.html)

https://preview.redd.it/2r5lfkxwxdz71.jpg?width=1024&format=pjpg&auto=webp&s=bed2b1c7a5b68f4cca8549e986cc77d6ef7d4ebd

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries and GAN tutorials!",MachineLearning
qt1vuy,1636812177.0,[D] Nvidia Jetson Thoughts,"Has anyone used any of the Nvidia Jetson models for production use cases? We're considering using them for a health application, but I haven't heard much about the developer experience one way or the other. The alternative would probably be running the application on a mobile device.",MachineLearning
qt1pnz,1636811639.0,[D] Can a GIoU loss (generalized intersection over union) be used after an STN module (spatial transformer network)?,"I have a model that uses an STN module for number detection and Mean Squared Error loss. But I would like to replace it for GIoU, because MSE doesn't take into account how much of the target area has been detected, only how close individual coordinates are close to the target. But I wonder if this makes sense. Has anyone tried it, or has some insight?",MachineLearning
qszmuu,1636803906.0,[D] Analysis of ICLR 2022 Review Scores,"We analysed the relationship between ICLR 2022 review scores and factors such as social media popularity and presence in Arxiv ([twitter thread](https://twitter.com/labmlai/status/1459443227543564289)).

Here are some of the results of the analysis,

* Papers that were present on Arxiv had higher recommendation scores.

Mean review scores: 5.1 (papers on Arxiv) > 4.7 (papers not on Arxiv)

https://preview.redd.it/9k59wa6tkcz71.png?width=1860&format=png&auto=webp&s=1a1c8a0b0b8392d90d4962fea4d3dbbea5e264f4

* Papers that were shared on Twitter (with >= 5 likes) also had higher recommendation scores than other papers.

Mean review scores: 5.4 (Tweeted) > 4.8 (not Tweeted)

https://preview.redd.it/08mxiqzvkcz71.png?width=1874&format=png&auto=webp&s=e39bea067d5d8b362b50f451c55b20f3e6f7e1ed

* Papers that had source code available (or promised to upload soon with empty repos) also got better reviews.

Mean review scores: 5.2 (with code) > 4.8 (without code)

https://preview.redd.it/xeyzn8zykcz71.png?width=1902&format=png&auto=webp&s=7748aabf8950215f8a93a4e287fef4cca3b4fa92

* Scatter plot of likes on Twitter against the review score.

  There is a small positive correlation between Twitter likes and review scores. The correlation
  coefficient is 0.2.

https://preview.redd.it/yvnxrgu1lcz71.png?width=1900&format=png&auto=webp&s=c19bbcd7db77ad7a29c9319e27ecdcd084b4bc9d

All ICLR 2022 submissions sorted by review scores can be found here : [https://papers.labml.ai/papers/iclr\_2022?sort\_by=conference\_score&dsc=0](https://papers.labml.ai/papers/iclr_2022?sort_by=conference_score&dsc=0)",MachineLearning
qszlto,1636803783.0,[D] Discussion about fine tuning language models for generating SQL queries,"Hey you all, hope you are doing well, I have a few things that I wanted to discuss about language modeling now I have a bunch of critical data stored in database format (for my company) I am working on something that can be used to train a language model to convert natural language to a SQL query. Currently we have a rule based system which has tons of if else conditions and, but it works, the only issue I see is that if a new database gets added, the currently existing rule based system will break.

So I am thinking of a way to automate this NL2SQL using a machine translation model such as GPT-2, later followed by something bigger like GPT-J. I have looked at a few papers namely PICARD and the corresponding challenge called Spider

I wanted to discuss a few queries that I have and the genral thoughts that people have about this problem.

  Am I batshit cray, to think finetuning large language models with a new data set would work well?

\-  Any alternative's that you would suggest to this?

\-  How should I go about this? Just plain use the trainer API from Hugging Face?

\-  Other genral thoughts and opinions etc

Thank you for taking your time to read this, I hope we can have a meaningful conversation about this.",MachineLearning
qsvzio,1636788149.0,[P] tsflex: flexible and efficient feature extraction for time series,"Are you looking for a time series feature extraction package that is both efficient and flexible? [tsflex](https://github.com/predict-idlab/tsflex) has got you covered. They just published a release that allows integration with tsfresh as well.

Go check it out 👉 [https://github.com/predict-idlab/tsflex/releases/tag/v0.2.2](https://github.com/predict-idlab/tsflex/releases/tag/v0.2.2)",MachineLearning
qsv83w,1636784987.0,[D] Are there any theoretical analyses on the success of AlphaGo zero?,I am wondering whether there is a theoretical guarantee for the convergence of the network used in AlphaGo zero or for the optimality of the searching algorithm?,MachineLearning
qstdsm,1636778046.0,"[P] Language model ruGPT-3 13B is apparently available for download. From an English translation: ""The ruGPT-3 13B model contains 13 billion parameters and is capable of continuing texts in Russian and English, as well as in programming languages.""","[Download page](https://sbercloud.ru/ru/datahub/rugpt3family/rugpt-3-13b) ([English translation](https://sbercloud-ru.translate.goog/ru/datahub/rugpt3family/rugpt-3-13b?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=nui)). I did not sign up to try to download the file(s).

[Reference](https://habr.com/ru/company/sberbank/blog/550056/) ([English translation](https://habr-com.translate.goog/ru/company/sberbank/blog/550056/?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=nui)).",MachineLearning
qss5os,1636773748.0,[D] Micro-Grants,Are there micro-grant sources besides AI Grant and Unitary Fund?,MachineLearning
qsrdyk,1636771128.0,[P] Text-to-image ruDALL-E Kandinsky (XXL) 12 billion parameter model checkpoint is apparently available for download,"[Here](https://sbercloud.ru/ru/datahub/rugpt3family/ru-dalle-12b) is a download page for ruDALL-E Kandinsky (XXL) 12 billion parameter model checkpoint ([English translation](https://sbercloud-ru.translate.goog/ru/datahub/rugpt3family/ru-dalle-12b?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=nui)). I did not sign up to try to download the file(s). The 12B parameter model should be available per my interpretation of [this press release](https://www.sberbank.com/news-and-media/press-releases/article?newsID=52a32604-55f8-4558-a748-ae79c929abc4&blockID=7&regionID=77&lang=en&type=NEWS). Edit: According to [this comment](https://www.reddit.com/r/MachineLearning/comments/qsrdyk/comment/hkfslaj/), public availability should be on December 1.

Prior mention of ruDALL-E in this subreddit:

[Text-to-image models ruDALL-E Kandinsky (XXL) (12 billion parameters) and ruDALL-E Malevich (XL) (1.3 billion parameters). A demo for the latter is available.](https://www.reddit.com/r/MachineLearning/comments/qlbye5/p_texttoimage_models_rudalle_kandinsky_xxl_12/)

[ruDALL-E model is open-source \[P\]](https://www.reddit.com/r/MachineLearning/comments/qmzy8a/rudalle_model_is_opensource_p/)",MachineLearning
qsl5jj,1636751991.0,[D] Opinion: the recent paper on buggy resizing libraries is misleading,"A recent paper [On Buggy Resizing Libraries and Surprising Subtleties in FID Calculation](https://arxiv.org/abs/2104.11222) claims that the image downsampling methods of OpenCV, Tensorflow and PyTorch are ""buggy"", and therefore PIL should be used instead for FID estimation. The corresponding twitter post was quite popular during the last week.

I believe that this claim and the main figure of the paper is misleading, because the issue is caused by aliasing and it can be fixed by simply setting the right parameters in the functions they used.

&#x200B;

[Their main figure](https://preview.redd.it/9va7lmkwd8z71.png?width=2116&format=png&auto=webp&s=1684c5e6979740787d1bc71d0e32b0bdffb48672)

&#x200B;

[My reproduction and fixed results \(bilinear\)](https://preview.redd.it/t06cmvx0e8z71.png?width=976&format=png&auto=webp&s=cf1fe97a307b6429bd31a20989707f8487d47b14)

On the bottom image, you can see the reproduced and antialiased downsampling results in all the frameworks. In all cases, a single parameter modification was enough to mitigate the issue. I shared the code and my complete opinion in this repository: [https://github.com/beresandras/buggy-resizing-critique](https://github.com/beresandras/buggy-resizing-critique)

Though I believe that the discussion on whether antialiasing should be a default in image libraries is valuable, in my opinion none of these methods is ""buggy"", and the paper presents the issue in a sensationalist way.

Edit: Just found out that there is another, very thorough investigation of the same work. Highly recommend checking the blogpost out: [https://blog.zuru.tech/machine-learning/2021/08/09/the-dangers-behind-image-resizing](https://blog.zuru.tech/machine-learning/2021/08/09/the-dangers-behind-image-resizing)",MachineLearning
qskjq5,1636750267.0,[D] What simulation software would you use to train a custom robot in?,"Say I want to use Reinforcement Learning to train a custom, virtual robot to stand. What simulation software would you recommend?
The requirements are the following...

 * Good training times for a billions steps. I would want the step to restart if the robot hit the ground hard. I would like to get lots of training steps in as fast as possible.
 * Inputs and outputs to and from Python. To observe the state of the simulation and take actions at every one of the robots joints. Position, balance, and velocity observations will be needed as well.
 * Ability to observe visual sensors on the robot
 * Very fine construction of the virtual robot in terms of size and weight of components. Exact positioning of the components. Precise force of the robot's motors at each joint.

I see Gazebo and Mujoco are popular, but I'm not sure they can do what I need. If there isn't an option maybe I will write my own physics engine.",MachineLearning
qsjngz,1636747703.0,[D] Can Unity3D ML-Agents use GridSearch?,"Hello,

I am trying to find a way to do gridsearch for hyperparameter tuning for reinforcement learning under the Unity3D machine learning platform.

I googled but it feels like no options are available right now.

If there is alternative ways, please share! Thank you, guys!",MachineLearning
qsigso,1636744328.0,[Discussion] Is your data quality suffering because of a first-mile reliability problem?,"If your data product is fueled by tens to hundreds of external data sources, then this may be relevant to you. When schema changes, volume anomalies, late-deliveries plague the first mile, they go on to infect your downstream warehouse tables and business processes. When the reliability of all those data sources are questionable, they cascade into points of failure that are out of your data team’s control & awareness.

If you'd like to learn how to improve your data's first-mile reliability, check out [**our latest blog post here**](https://databand.ai/blog/data-supply-chain/?utm_source=forum&utm_medium=r&utm_group=ml)**.**",MachineLearning
qsi0u2,1636743107.0,[R] prune-then-quantize or quantize-then-prune for post-training optimization of computer vision models.,"As a student that is interested in improving the efficiency (model size, throughput, energy) of pre-trained deep learning models in the computer vision domain, I was wondering if there was a clear winning approach on in what order we should prune and quantize a pre-trained model.

So basically if you were given a pre-trained deep learning model (e.g. Resnet), would one of the following approaches lead to better solutions, in terms of the accuracy to efficiency ratio (e.g. model size, throughput, energy consumption):

- quantize-then-prune (with finetuning/retraining after every stage)
- prune-then-quantize (with finetuning/retraining after every stage)

I have trouble finding related works that answer this kind of question. To me it seems to be valid question, but maybe I'm missing something obvious.

Any input would be appreciated.",MachineLearning
qshm5b,1636741959.0,"[P] Questions regarding self-supervised learning for music (DINO, MoCo, ...)","

Hello everyone! First of all, I am pretty inactive on Reddit, so I hope that this is the right place for this post. I am a computer science graduate student focusing on machine learning, working on an interdisciplinary research project regarding the analysis of music.

**TL;DR**

I try to extract general and descriptive music features on various different levels.

* Which self-supervised methods can you recommend for limited resource capacities?
* Which data augmentations to use for music?
* General tips and tricks for SSL with music?
* Is it a good idea to use a pretrained backbone in order to get away with a small dataset and compute (e.g. OpenAI's JukeBox VQ-VAE)?
* Why is my DINO setup not converging?

&#x200B;

**General idea** Rather than solving some specific MIR task (e.g. genre classification), my goal is to extract generic, interpretable and descriptive music features. In other words, I want a model that “perceives” and “understands” music in general without giving it a specific goal. I would then further analyze extracted features, e.g., to find relations to human music perception, or use them for downstream tasks. Ideally, the outcome is a model that can describe music on various different levels (e.g., beat, rhythm, harmony), for example by features extracted from different neural network layers (shallow: low-level, deeper: higher-level). I know that this is by far no easy task, but it is worth investigating the possibilities and limitations in my opinion.

**Which self-supervised learning method?** After some research, self-supervised learning (SSL) seems the way to go here. SSL is a research area that gained momentum over the last two years or so and there are multiple proposed methods, however applied mostly in the computer vision area (images). Additionally, SSL methods seem to be rather data hungry and as you might imagine my resources are quite limited. I have a GTX 1060 available locally, but I am also willing to pay in order to train a model on GPU cloud services (e.g. Lambda GPU). Since my budget is low, I’d like to do all testing locally or on other free alternatives such as Colab in order to find the right method, hyperparameters, etc. for the actual training on a payed server.

**Idea: Using pretrained JukeBox backbone** My idea is to use the pretrained VQ-VAE from OpenAI’s JukeBox (see below) as a backbone and hope that it extracts useful intermediate features, which I can forward to my (comparably small) model. With this I hope to get away with a small dataset (60 hours of music?) and relatively low resource usage, while still achieving reasonable results. Does anyone have experience with such setups? Now to the SSL method itself (ignoring generative models): On the one hand, there are contrastive methods such as CPC, SimCLR or (the less resource hungry?) MoCo. On the other hand, there are methods which do not explicitly formulate a contrastive loss such as BYOL or DINO. Especially the latter one seems interesting. Unfortunately, I have no experience in training such models at all, which is why I wanted to ask the community for some tips and feedback suited for my problem. Preferable methods are those which do not require much hyperparameter tuning (time and compute limitations) or much compute during training (e.g., large batch sizes with SimCLR), but still achieve good results.

**DINO not working properly** Currently I am testing the DINO method on a very limited setup locally: My dataset consists of a very small portion of the FMA dataset (90 minutes of music) with a sample size of 4 seconds and a batch size of 14 using the LARS optimizer. Additionally, I have adopted the audio augmentation strategy from a paper that applies SimCLR to music (CLMR, see below). I am very happy about ideas for other/better music augmentations for my problem, though. However, the model does not seem to converge properly. The loss decreases quite quickly after a few epochs. After a while however the model seems to collapse, and the loss increases rapidly staying high over the remaining epochs. I figure this has something to do with the momentum hyperparameter (i.e., how much of the student’s weights get transferred to the teacher after each iteration). When increasing this number, the collapse effect is minimized or is non-existent at all, if large enough, but the loss does not really decrease much either. I have logged everything and computed stats for several epochs, if anyone is interested. When splitting the loss up into the teacher entropy and the KL divergence between the student and the teacher, one can see that the collapse is caused by the entropy part. Does anyone have experience with DINO or similar methods and might have a clue why this is happening? As mentioned, I can give more details about the hyperparameters, logs, statistics, etc. Is it possible that this phenomenon is due to the small dataset and batch size on my local machine, and it would diminish on the cloud with a larger dataset and/or batch size? Though they mentioned in the paper that they have successfully tested their method even with a batch size of 8.

I know this is a lengthy post, but I wanted to share as much detail as possible about my goal and resulting problems. Hope anyone might give some feedback.

**Papers** JukeBox: [https://arxiv.org/abs/2005.00341](https://arxiv.org/abs/2005.00341) CLMR: [https://arxiv.org/abs/2103.09410](https://arxiv.org/abs/2103.09410) DINO: [https://arxiv.org/abs/2104.14294](https://arxiv.org/abs/2104.14294)",MachineLearning
qsh3z0,1636740560.0,[D] Best/Favorite format for writing without a conference in mind?,"Usually, I have ideas, do some experiments, draft a manuscript, etc., and then afterwards edit it into a format for a particular conference or journal. But I'm wondering, among the community, what's your preferred format for writing something if you don't know where you're sending it yet? If something's just on arXiv, what format is visually the easiest to read?

I kinda like the JMLR format, pretty plain, pretty basic, but I can appreciate that it wastes a lot of space on the author list. The NeurIPS seems like the default, definitive format, but I find the bars around the title kind of ugly. The IEEE formats always present visually as being just completely fucking impenetrable. Not a fan of the ICLR format putting all the titles in caps.",MachineLearning
qsfiw7,1636736217.0,[N] causal-learn: Causal Discovery for Python," We are excited to release the Python causal-learn package for causal discovery! See the package ([https://github.com/cmu-phil/causal-learn](https://t.co/D0YK6ZqMjs?amp=1)) and documentation ([https://causal-learn.readthedocs.io/en/latest/](https://t.co/kA2bwYtU1l?amp=1)). Any feedback is welcome.

&#x200B;

[https:\/\/github.com\/cmu-phil\/causal-learn](https://preview.redd.it/w7uiy9nf37z71.png?width=916&format=png&auto=webp&s=bb68aebb3a1177a47d3bcf6c73a89c989d245752)",MachineLearning
qseien,1636733388.0,[D] A dilemma of an ML guy in industry,"I'm a Machine Learning Engineer in the healthcare sector and I've been thinking about this a lot. With the rapid as fuck advancements in research on AI, do I keep up with research more or should I focus on learning about engineering the solutions(pipelines, etc.). Example: reading a paper/using a new GAN vs. reading about a case study.",MachineLearning
qsebrh,1636732892.0,[N][CfP] AI for Design and Manufacturing Workshop (ADAM) @ AAAI 2022 (Deadline Extended),"Hello [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)!

The deadline for submitting to the AI for Design and Manufacturing Workshop (ADAM) @ AAAI 2022 has been extended for a week due to multiple requests. If you're working in the intersection of AI and design, manufacturing, scientific computing, and geometric modeling, do consider submitting a 4-page workshop paper. We invite paper submissions on the following (and related) topics:

* New theory and fundamentals of AI-aided design and manufacturing,
* Novel AI-based techniques to improve modeling of engineering systems,
* Integration of AI-based approaches with engineering prototyping and manufacturing,
* Novel methods to learn from scarce/sparse, or heterogenous, or multimodal data,
* Novel ML methods in the computational material and physical sciences,
* Novel ML-accelerated optimization for conceptual/detailed system design,
* Novel AI-enabled generative models for system design and manufacturing,
* ML-guided rare event modeling and system uncertainty quantification,
* Development of software, libraries, or benchmark datasets, and
* Identification of key challenges and opportunities for future research.

Workshop website: [https://adam-aaai2022.github.io/](https://adam-aaai2022.github.io/)

Submission website: [https://openreview.net/group?id=AAAI.org/2022/Workshop/ADAM](https://openreview.net/group?id=AAAI.org/2022/Workshop/ADAM)

Submission deadline: November 19th, 2021",MachineLearning
qse6gc,1636732499.0,[R] DeepMind’s One Pass ImageNet: A New Benchmark for Resource Efficiency in Deep Learning,"A DeepMind research team presents the One Pass ImageNet (OPIN) problem, designed to study the space and compute efficiency of deep learning in a streaming setting with constrained data storage and to develop model training systems where each example is passed to the system only once.

Here is a quick read: [DeepMind’s One Pass ImageNet: A New Benchmark for Resource Efficiency in Deep Learning.](https://syncedreview.com/?p=31386&preview=true&_thumbnail_id=31387)

The paper *One Pass ImageNet* is on [arXiv](https://arxiv.org/abs/2111.01956v1).",MachineLearning
qse2ov,1636732211.0,[D] Why do CNN Kernel weights reach high values?,"I've recently read a bunch of literature about network pruning. A common criteria in the field is to select kernels that are to be removed by their L1 Magnitude (e.g. \[1\], \[2\]), as the heuristic apparently catches relevant kernels quite well.

Most often the time the CNNs are trainied with some form of weight decay. This is (IMHO) intended to regularize the model and prevent single kernel weights from dominating the entire set of parameters and distribute relevance across multiple channels. Also it is normal to add BatchNorm to the architecture as it stabilizes the training procedure.

I'd argue that:

1. The only relevant thing for detecting a pattern is the relative weight between the kernel weights. The absolute value does not matter as it will only change the value range. (CNN Kernels are a Matrix multiplication and therefore a single scaling factor would do the same as scaling the entire thing.).
   1. As the kernel is followed by a BatchNorm the values get automatically scaled and zero-meaned before getting scaled and shifted, so there is less reason for that. (When using Conv-Bn-ReLU ordering)
2. With any weight penalty this should lead to continuously & slowly decreasing values of the kernel weights (Except in the BatchNorms which is way more ""penalty efficient"" than a kernel.

The only reason I could think of is that one will run into some numeric stability issues when one approaches the minimum resolution of the data format (i.e. float16/32). Maybe this introduces some sort of noise into the optimization process?

However as the evidence shows that we get higher weights I have to be wrong and would be happy to get shown where my thought process breaks down.

\[1\] [Comparing Rewinding and Fine-tuning in Neural Network Pruning](https://arxiv.org/pdf/2003.02389.pdf)

\[2\] [Learning efficient convolutional networks through network slimming.](https://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf)",MachineLearning
qsdnc2,1636731023.0,"[R] Invitation to participate in study ""The Labour of Ethical AI""","This is an invitation to participate in a study titled “The Labour of Ethical AI”. My name is James Steinhoff and I am a postdoctoral fellow at the Institute of Communication, Culture, Information and Technology at the University of Toronto Mississauga. The aim of this research is to understand the labour that goes into ethical AI research (research intended to promote ethical, responsible, democratic, human-centered, non-profit or socially beneficial AI), the organizations in which such work is conducted, the problems facing workers in this field, and how the field is connected to industry, academia and government. The goal of the study is to interview people who work, study or intern in ethical AI in order to gain empirical insight into the working conditions in this sector.

If you have experience working in the sector, you could provide great insight into some of the challenges and opportunities facing workers. If you are willing, your participation would involve meeting me for an online interview that will last approximately one hour, where I will ask you questions about your working conditions, why you chose the ethical AI sector and the promises and problems facing that sector. Participation in this project will be confidential. I will supply potential participants with an informed consent form that outlines in greater detail the project and the parameters of participation.

I am happy to answer any questions or concerns you might have.

Thank you for your time and consideration.

Sincerely,

James Steinhoff, PhD @ j.steinhoff@utoronto.ca

Postdoctoral Fellow

Institute of Communication, Culture, Information and Technology

CCT Building 3071

3359 Mississauga Rd.

Mississauga, ON

L5L 1C6",MachineLearning
qsc6y3,1636726869.0,"[D] Can data scientists still make a better predictive model using their talents, by the evidence? Will script kiddies increasingly take over because of great software and AI that does more and more of it very well?","Having studied and applied data science and machine learning since 2014, I am startled by the high quality free tools that are coming online in the past week, month, and year or two, that I have tried. They are that good.  I am talking about automatic model selection, automatic tuning, and NLP that is just breaking away from the old limitations and old ways of doing projects.

See, on youtube lately I am watching all these ""data scientists"" posting videos where all they do is run the software that Microsoft and Google made, and call it a day.  They don't even change the default settings and they add nothing of their own talents (if they have them).

And the results are pretty good, by golly. That's the thing.

 Automatic feature engineering is one of the hallmarks of the top deep neural networks. People have spent careers in linguistics and computer vision, hand-making parts and hand-curating mathematical techniques that are now often completely bypassed by today's ML techniques baked into free software, and that's just one example of many.

It's not a trivial question.

There's more to data science than making a predictive model on a canned, fixed public dataset like Iris, Titanic, Jewellery, or even Higgs Boson. There's also MLOps, exploratory data analysis, study design, visualization, data wrangling, data quality assurance, data life cycle, and many other areas. Data science has wide scope and many specialties.  Predictive model making is only about 5 percent of the whole ML and DS job now, according to a presentation I saw yesterday.

But modeling might be about to be fully automated soon. Can you as a ML engineer or data scientist, bring your wide and deep talents to bear, to actually show that you can make a model better than a script kiddie on predictive performance on any -- any at all -- well known open dataset?

Is making predictive models a fully automated task now?",MachineLearning
qsaonk,1636722057.0,[D] Adversarial Loss understanding in Total Relighting: Learning to Relight Portraits for Background Replacement paper,"Hi. I hope to get some help understanding Google's [Total Relighting: Learning to Relight Portraits for Background Replacement](https://augmentedperception.github.io/total_relighting/total_relighting_paper.pdf) paper. I'm stuck on the adversarial loss. In 4.1 Paper says:

>we add an adversarial loss on the face region to help the network learn to plausibly remove high-frequency shading effects from the input image while maintaining image detail. We use a least squares discriminator \[Mao et al. 2017\] disc^(alb) to add a loss between a crop of the face from the ground truth albedo 𝐴crop\_gt and a matching crop of the face from the predicted albedo 𝐴crop

Predicted albedo 𝐴crop is the face crop of the output of U-Net like network. No other details provided and I'm struggling to understand the setup here. Since in the paper they're using both ground truth and predicted albedo as the inputs for the loss, I can imagine two scenarios:

1. Use the loss from the original LSGAN paper and train the discriminator during the model training, which seems counterintuitive to me
2. Use L1 or L2 distance between pretrained discriminator's output for ground truth and prediction, but that is not really adversarial loss I guess.

I have no experience with such discriminator usage and therefore can't choose between these two or come up with something else reasonable.

Is there a common way to use GAN discriminator for the loss calculation of ""non-GAN"" networks? Because from the paper it sounds like something that doesn't require deeper explanation",MachineLearning
qs98gv,1636716646.0,[P] Create semantic search applications with machine-learning workflows,"&#x200B;

https://i.redd.it/i3nfnfbsg5z71.gif

Create semantic search applications with machine-learning workflows. The demo above shows how various NLP pipelines can be connected together to build a semantic search application.

txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications. txtai has support for processing both unstructured and structured data. Structured or tabular data is grouped into rows and columns. This can be a spreadsheet, an API call that returns JSON or XML or even list of key-value pairs.

Some example workflows:

* Summarize news articles
* Summarize and translate research papers
* Load and index data via a CSV
* Schedule a recurring job to query an API and index results for semantic search

References:

[Live Demo](https://huggingface.co/spaces/NeuML/txtai)
[GitHub](https://github.com/neuml/txtai)
[Article](https://towardsdatascience.com/run-machine-learning-workflows-to-transform-data-and-build-ai-powered-text-indices-with-txtai-43d769b566a7)
[Notebook](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb)
[Notebook](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/22_Transform_tabular_data_with_composable_workflows.ipynb)",MachineLearning
qs7g4t,1636708549.0,[D] Causality research in ML is a scam (warning: controversial),"Don't get me wrong, causal inference are *the* most methods for application areas where we observe a bunch of random variable and want to figure out the causal relationship between them.

This rant is not about the method is itself, but how ML research is recently getting exploiting the term ""causality"" for the sake of the hype and citations.

In ML we have two main paradigms: Supervised learning and RL.

Work on causality (e.g., Bernhard Schölkopf, Judea Pearl etc.) tells us that is impossible to determine the causal relationship between variables if we only observe them without performing any interaction. Therefore, with supervised learning we cannot learn a causal model but we need to impose one. Period.

Regarding RL, tabular Q-learning is guaranteed to converge to the maximum expected reward policy. Period. That's it, nothing else needs to be said about it.

However, despite these two fundamental statements, there is currently growing a hype in general ML research about causality. I am completely fine with causality research as long as it focuses on the application area mentioned in my first sentence. But this recent trend brings the concept into computer vision, NLP, etc. , where things become vague quite fast, exaggerated by the fact that research on causality can be already extremely vague and deeply philosophical (e.g., what's the practical implication of Newcomb's paradox).

In computer vision no causal model is known. Even the vision processing of humans or animals is very little understood. Moreover, CV tasks are inherently under-specified. For instance, is a cartoon drawing of an elephant still an elephant? Or is is out-of-distribution (OOD), or its own class, or multiple classes? Are we talking about the causal relationship of pixels, patches, or concepts? What makes an elephant ear an elephant ear?

This vagueness, combined with the general trend in ML of throwing a bunch of overly complex math statements into a paper to impress the reviewers, is really concerning.

I bet that there will be hundreds of papers on this topic be published in the next years that contribute very little to our understanding, but will create millions of (self-) citations.",MachineLearning
qs5igm,1636699761.0,[D] How do I lower my standards for code quality to match my research team?,"I recently joined a lab working on research in robotics / reinforcement learning as a research intern. I graduated college recently, all my previous work experience is ML engineering but not active research, rather implementing things people have already done.

I've been in the lab a few months now and it's become clear to me that the standards for code in research are a lot lower than I expected. I am the only one on my team pushing for things like CI pipelines, refactoring, documentation, regression tests, etc. And when I do raise such concerns the usual response is that such things are not worth spending time on. My supervisor once told me that the job of a researcher is not to write good code, it's to find a correct problem formulation, describe it correctly, and then pass it to a software engineer whose job it is to create the high-quality implementation. They emphasised the importance of rapid iteration of the idea rather than going slowly because they're concerned about getting scooped in our current project. This is true to some extent because we are a small and resource-limited group whose experiments can take hours to run, working in a popular field (quadruped legged locomotion).

I struggle a lot with this mindset. I hate looking at the crappy code written by myself and by my teammates and I hate debugging it / trying to understand it. A lot of my time is wasted on things like checking which experimental configuration a model was trained from because we don't have automated logging of that. Aesthetically I also just dislike it, I have high personal standards of code quality (Uncle Bob's *Clean Code,* anyone?) which I feel like I'm constantly breaking when I have to write code for work.

I'm looking for advice on how I can learn to tolerate this better, or other similar resolutions to the problem. Thanks in advance.

==============================================================

Edit: Thank you all for the kind replies so far. I have received many more responses than I was ever expecting to get. It's going to be difficult to reply to each one but I will do my best to read all of them and consider. I will respond here to some general points I've seen made repeatedly.

1. I'm not arrogant enough to think that I know everything about research and coding after 3 months in a lab. And as many have rightly pointed out there's no need for research code to live up to the standards of long-term production code. I do recognize that in this case the problem likely lies (mostly) with me. Hence the title of 'how do I lower my standards' and the question of 'how do I learn to tolerate something I'm not comfortable with'. Seeing how popular this post is, I'm sure this struggle resonates with many others as well. As a result I hope for there to be less cynicism about my motives. The discussion so far has been largely productive and on-topic, I appreciate very much it staying that way.
2. To give some additional context, I am working right now as part of a group. I have a main project which I work on with 1 other person (also a recent graduate). My supervisor actually doesn't look at the code we write at all and as far as I can tell does not care much about code. They seem to implicitly trust that we are able to correctly implement things according to the description (which I believe is true for both of us). At our weekly meetings, we present our progress in the form of videos / graphs / slides. This means that I and my teammate are solely responsible for maintaining the quality of code in our current project, and as mentioned we have philosophical disagreements about how much code quality matters. (I know I am not necessarily right.)
3. The project I am working on is very much not a 'one-off experiment'. Our main codebase is inherited from somebody else who is no longer in the lab (which was itself an iteration on some open-source implementation). We are iterating various elements of the robot's sensor setup in addition to the reinforcement learning algorithm and training curriculum. With well-known off-the-shelf envs like those in MuJoCo I'd be pretty happy accepting that it was working as intended. With code I write myself or code written by other people which is not tested, it's a lot harder to have that confidence.
4. As some have correctly pointed out, using a standard logging tool like W&B would be an easy fix to the time spent on resolving experimental configurations. That issue in particular could be resolved easily by setting up the appropriate pipelines and would likely be a net plus to my team. More generally, I could benefit from critically examining the practices I am used to and seeing which have concrete benefits in my current workplace, and then propose those to the team.
5. I am a strong proponent of unit tests. Unit tests save my butt from making simple mistakes. It's already incredibly difficult to figure out why experiments fail sometimes. I can at least eliminate some of the possible causes by unit-testing code where easy and appropriate. Also, when I do fix a bug, I write a unit test to enforce the fix, and then I can mostly forget about it. That mental real estate can go to things that can't be automated as easily.
6. Similarly, I am a big fan of simple CI pipelines like Github Actions. Perhaps the term 'CI' comes with a lot of associated bells and whistles which are (rightfully) not used in research code, but to me CI is simply a way to automate the manual running of unit tests. With a bit of know-how, Github CI workflows are easy to set up from a cookiecutter template and they save a lot of time as well as mental worry. I like to include linters as well but it's not necessary to the core functionality.",MachineLearning
qs0eup,1636681321.0,[P] Fine-tuning and running GPT-J made easy,"Have you guys seen Eleuther’s GPT-J for NLP yet? I feel like it’s on par with OpenAI’s Curie. It's pretty good overall for generic language generation, but you still need to fine-tune it for custom tasks, so I ended up putting together this project to simplify fine-tuning and deployment to production after.

Both can be done through a web interface. Also, I added a default pre-trained GPT-J to use through an interface or API too. Please, check it out and give me feedback if you can. Thanks!

Project: https://www.tensorbox.ai",MachineLearning
qrygiv,1636674961.0,"[R] PhD & postdoc positions at UT Austin: ML for complex systems (chaotic time series, cellular automata, & fluid dynamics)","Hi! I’m looking for PhD students interested in the intersection of machine learning and physics, particularly chaos and fluid dynamics.
I'm also informally looking for postdocs (official ad coming soon).

### About

We are based in the physics department at UT Austin, and are affiliated with the Oden Institute for Computational Engineering & Sciences. Here is a link to [the lab website](https://gilpinlab.github.io/?utm_source=en_us_der)

Projects are pretty flexible based on curiosity and mutual interest; there’s room for more algorithm-focused time series mining projects, as well as pencil-and-paper dynamical systems and control theory problems. As far as applications go, we’re particularly interested in projects that can eventually be used for biological data or fluid dynamics. We’re super open to applicants from uncommon academic or personal backgrounds

Here are some recent examples:
+ “Chaos as an interpretable benchmark for forecasting and data-driven modelling” (NeurIPS 2021) https://arxiv.org/abs/2110.05266
+ “Deep reconstruction of strange attractors from time series” (NeurIPS 2020) https://arxiv.org/abs/2002.05909
+ “Cellular automata as convolutional neural networks” (Phys Rev E 2019) https://arxiv.org/abs/1809.02942

### Applying

For grad students, feel free to apply to any of these grad programs at UT Austin:
+ The physics department (due 12/1)
+ The Oden CSEM program (due 12/15).
+ Other departments (CS, EE) are probably possible, too

For postdocs, please reach out to me informally.

Our physics PhD program does not require physics GRE, normal GRE, or a physics undergrad degree. There are only four core courses, and we have previously had students with undergrads in CS, engineering, bioinformatics, etc. Our quals are research talks, not written exams

If any of this sounds interesting, feel free to email/DM me or chat with me at NeurIPS or APS",MachineLearning
qryaz4,1636674314.0,[p] Alphafold 2.1.1 (Without Docker),"Want to fold monomers AND multimers using Alphafold2...without the need for Docker? Look no further than my fork of DeepMind's Alphafold repository that removes all Docker dependencies. Enjoy! #AlphaFold #docker

https://github.com/amorehead/alphafold\_non\_docker",MachineLearning
qry8i4,1636674082.0,[R] Palette: Image-to-Image Diffusion Models,"website: [https://iterative-refinement.github.io/palette/](https://iterative-refinement.github.io/palette/)

paper: [https://arxiv.org/abs/2111.05826](https://arxiv.org/abs/2111.05826)

Samples:

&#x200B;

https://preview.redd.it/n7cseo2ty1z71.png?width=1172&format=png&auto=webp&s=f0a4068a59a2426ce01313e21698a03aede0cb7c

&#x200B;

https://preview.redd.it/glyaejury1z71.png?width=1172&format=png&auto=webp&s=9f70bc1640c487d607db98f8b62b45ede6af14f9",MachineLearning
qrwuvy,1636669986.0,[D] What must every PhD doing ML know before graduating,"I am a 3rd year PhD who finally finished all program requirements (classes etc...) and am fully focused on research. My question is, what are some things that your average ML PhD should be good at at this point? I know its subjective and depends on their research field but what are some common things that I should be well versed in?",MachineLearning
qrw5pb,1636667997.0,[D] Article: An introduction to Language Models in NLP,"Hey! We're working on an intro series to language models. Would love any feedback on this first installment :)

[https://www.surgehq.ai/blog/an-introduction-to-language-models-in-nlp-part-1-intuition](https://www.surgehq.ai/blog/an-introduction-to-language-models-in-nlp-part-1-intuition)",MachineLearning
qrvasc,1636665512.0,[D] Handling bound constraints in CMA-ES,"Hi all,

Apologies if the question is a bit naive as I only have passing familiarity with ML (full stack developer by trade). I'm really just looking for more insight into the implementation and consequences of bound constraints with CMA-ES optimization. I did come across ([Biedrzycki, 2019](https://staff.elka.pw.edu.pl/~rbiedrzy/publ/RBSWEVO_CMA_BCHMs.pdf))  which discusses this topic, and a lot of the paper's findings at least come across as fairly natural, but I'm not finding a lot out there and I'm just wondering whether anybody else has any recommendations for further reading or personal stories of potential pitfalls. In the problem I'm dealing with specifically, each of the coordinates in my mutant/search point vectors need to be bound within the unit interval and I wanted to make sure I wasn't walking into any easily preventable mistakes.

**Background Context**:  I'm currently working on a personal project (inspired by this popular [Geijtenbeek, et al., 2013](https://www.goatstream.com/research/papers/SA2013/index.html) paper) where I've implemented a Hill-type muscle model and rigged up a humanoid model with some 200+ ""muscles"" approximating human skeletal muscle function. Now I'm beginning muscle control experiments with inspiration from this paper ([Wochner, et al., 2020](https://www.frontiersin.org/articles/10.3389/fncom.2020.00038/full)), except I'm plugging in CMA-ES instead of the Bayesian optimization used in the paper while utilizing the objective function insights.",MachineLearning
qrte64,1636660160.0,[D] Why do we have to discretize the data before we use mask prediction for representation learning?,"In fields as vision or speech, a lot of recent papers learn representations by masking parts of the input and predicting the original. To do this, they often refer to masked language modeling (MLM) in BERT-style pre-training and say that we must first discretize the continuous input into discrete tokens  (e.g. using a VQ-VAE) before making predictions (i.e. classification-task over possible tokens in the dictionary).

One of the argument is that the predicting discrete tokens allows the model to learn high-level concepts whereas making prediction in the original input-space (e.g. raw pixels) will force the model to learn high-frequency/low-level details that are not useful for representation learning/compression, and is also computationally prohibitive (since the original input-space is likely very high-dimensional). However my question is why can't we do regression in a continuous latent space for the masked positions (e.g. predicting the latent representation of a learned continuous VAE for the masked positions) instead of classification in a discrete latent space (e.g. predicting the discrete tokens of a learned VQ-VAE for the masked positions)? Is there any theoretical advantage to using discrete tokens instead of continuous latents?",MachineLearning
qrpx36,1636650680.0,[P] Integrate your ML model with your favorite apps from a single Python file,"Hi all,

Many here can build a simple Machine Learning model to predict whether a customer will *churn* if they get a nice `pandas.DataFrame` with customer data. However, it gets really complicated if you want this model **deployed** and **integrated** in production, say a procedure as such:

1. Pull data from a new customer from **Shopify**
2. Predict for this customer whether they will *churn*
3. If we predict `CHURN == True`
4. Send a discount code to this customer with **Mailchimp**

Suddenly we have to code data integrations, ETL pipelines, deploy our original Machine Learning solution, spin up an HTTP server etc.; a huge pain indeed...

We are building [a framework](https://flow.magicsheets.io) that takes care of exactly all the boring stuff described above. We really believe this will **bridge the gap between research and real-world ML**.

Super excited to share this with all of you, please let me know if you have comments or feedback!",MachineLearning
qrpwsn,1636650660.0,[Research] .wav dataset for morse code,"Does anyone know where to obtain a dataset containing morse code .wav files.  I checked Kaggle, there was a competition once, but the data is no longer available. \[Research\]",MachineLearning
qrnozu,1636644422.0,[D] How much VRAM and RAM do I need for NLP transformer models?,"I'm a PhD student looking for a new desktop because my current (personal) PC has an AMD GPU.

When I train a pre-trained BERT model using my CPU (which takes forever) I assume that it is using RAM. Online, I often read about transformer models using VRAM.

So my question: does training transformer models exclusively use VRAM, or do I also need sufficient RAM? And how much would be needed (minimum) to work with such models?

(I am aware of Google Collab but it is not suitable for my work)",MachineLearning
qrn5qb,1636642863.0,[D] replacement for SoftMax when you want to activate multiple samples equally?,"Softmax has been used for activating/normalizing a representation in way that the most important sample will have the largest value between \[0,1\] and the rest will be close to 0.

I am wondering what if we want to activate multiple samples equally in the representation. Lets say I have a vector of \`1x10\` dimension, and I know 5 of them are equally important and I want them to have same weight after activation, so softmax wont work here, one option is to use sigmoid  and then use softmax on the top of it to make sure the sum is one. but I am not sure if this is the smartest approach. So I was wondering if people here have any other suggestion ?",MachineLearning
qrm0o7,1636639462.0,[D] What are simple projects I can do with OpenAI?,I'm thinking about making a SQL query generator from English language. Any other ideas are welcome. Thanks.,MachineLearning
qrknk3,1636635174.0,[D] What are your favourite annotation platforms?,"I have tried a few annotations platforms:

-Redbrick.AI

-V7 Labs

-Supervise.ly


None of them quite hit the sweet spot for my use-case. I mainly care about uploading pre-annotations of a semantic segmentation network to be double checked by human annotators.

The aforementioned platforms do offer this service but it involves fiddling with their respective SDKs and doesn't always work that well.

Can anyone share their favourite annotation platforms and why?",MachineLearning
qriz01,1636628976.0,[D] Landmark annotations in Blender,"I am building a synthetic dataset of images for a landmark prediction task and I'm using Blender.

Having looked through the main data generation libraries available for Blender on Github (vision\_blender, BlenderProc, zpy...) I can't find any that support landmarks. Before I go and implement this myself, does anyone have any pointers that I'm missing?

Thanks

*Update*

The following script will write out the coordinates of vertices in a rendered image

```
import bpy
scene = bpy.data.scenes['Scene']
camera = bpy.data.objects['Camera']
obj = bpy.data.objects['Cube']

matrix = camera.matrix_world.normalized().inverted()
"""""" Create a new mesh data block, using the inverse transform matrix to undo any transformations. """"""
mesh = obj.to_mesh(preserve_all_data_layers=True)
mesh.transform(obj.matrix_world)
mesh.transform(matrix)

"""""" Get the world coordinates for the camera frame bounding box, before any transformations. """"""
frame = [-v for v in camera.data.view_frame(scene=scene)[:3]]

lx = []
ly = []

for v in mesh.vertices:
    co_local = v.co
    z = -co_local.z

    if z <= 0.0:
        """""" Vertex is behind the camera; ignore it. """"""
        continue
    else:
        """""" Perspective division """"""
        frame = [(v / (v.z / z)) for v in frame]

    min_x, max_x = frame[1].x, frame[2].x
    min_y, max_y = frame[0].y, frame[1].y

    x = (co_local.x - min_x) / (max_x - min_x)
    y = (co_local.y - min_y) / (max_y - min_y)

    lx.append(x)
    ly.append(y)

coords = [f""({x}, {y})\n"" for x, y in list(zip(lx, ly))]
with open(""log.txt"", ""w"") as f:
    f.writelines(coords)

```",MachineLearning
qrhh89,1636622767.0,[D] What are the advances on encryption?,"I'm new to the topic, I understand that it's easy for AI find a valid result from a predictable algorithm like a basic letter+letter encryption with a small dataset (I saw it as an example in a lecture)

But how about more complicated encryption algorithms? 16bits? 32bits? 256bits? TLS? How big would the dataset get to be able to easily predict a correct unencrypted text from encrypted text?

Can AI be used to test if an algorithm is actually safe by solving it without really knowing how exactly the data was encrypted? For example if an encryption standard that looks very safe to experts but has an unknown mathematical shortcut AI could find it, is that possible?

Where can I find more about this topic?",MachineLearning
qrbkc7,1636600691.0,[D] Calling out the authors of 'Trajformer' paper for claiming they published code but never doing it,"I read a paper from NeurIPS 2020 titled 'Trajformer: Trajectory Prediction with Local Self-Attentive Contexts for Autonomous Driving'. I found it interesting and the authors claim multiple times in the paper that 'we release our code at '[https://github.com/Manojbhat09/Trajformer](https://github.com/Manojbhat09/Trajformer)'. Turns out they never did, fine, I thought perhaps they will in the future and starred the repo to check it out later.

Many others raised issues asking for update on code release and they never replied. Finally, it April they update the readme to say that they will release the code and that's been the last update.

I know this is a common trend in ML papers now, but what sucks is that I emailed the authors (both the grad student and the PI) multiple times asking for an update an they never replied. Their paper is literally based on empirical improvements and without working code to replicate the results it is their word against mine.

I strongly think things have to change, and I believe they only will if we call them out. I waited long enough, and made significant effort to contact the authors with no response. I mean I don't mind them not releasing their code, but at least don't claim that you did in the paper/review phase and then disappear. An undergrad in my lab asked why she should take time to clean up the code and document it before release while others just move on to the next interesting project and I don't have an answer. ",MachineLearning
qravhd,1636598475.0,[D] Is anyone working on code generation other than OpenAI?,Are there any other works in code generation models like Codex from OpenAI? I have seen some open source version of Codex but no variants of that model.,MachineLearning
qr9ndf,1636594461.0,[P] Machine Learning tutorial in R,"Does anybody know of a good tutorial that would help me do this in R:

* A model to predict the probability of a home run for a given ball in play
* An explanation of your chosen model features
* A visualization of your model outputs
* Identify the home run that was least likely to be a home run, and the non-home run that was most likely to be a home run. Describe why you think your model classified these plays less accurately than others.

Advice for how to get started/model to choose would be helpful as well but I do not want answers here I want to learn. So far I have a model that has an overall prediction accuracy of 98% (compared to 90% for completely random), and a prediction accuracy of 63% when at least one of the predicted or actual result is home run (3% accuracy for random model). The models I have been using are KNN and random forest",MachineLearning
qr6hhb,1636584690.0,[D] What is the current state of deep learning theory?,"As I assume many people on this subreddit are, I'm interested in understanding why currently state of the art models work, and if we can use the understanding of the underlying princples of these models to create better more powerful ones. I mean this in the sense that physics has theory or classical computer science has one, in that both develop a mathematical framework to understand and predict their subject (there is of course a difference between them as one assumes laws from experiences and onr is more firmly rooted in math, but both still use a mathematical approach).

I sometimes see some papers on this subreddit that look very relevant to this vmatter, and while I try to read some of them, given my current math education, which is somewhere between a practioneer and a BA student I do not believe I can truly understand and therefore judge them.

So for those of you who are experts in this, do we have an accepted predictive theory for deep learning models? If not, what do we have?",MachineLearning
qr6bu6,1636584255.0,[discussion] Plaforms/Frameworks for Backtesting and Regression Testing Lots of Models?,"I have a need to test lots of models submitted by different teams.    There will be baseline ""curated"" datasets but there will also be updates as new data comes in from the field.   Models may have different preprocessing requirements.

  We'll need to retrain models on the updated data, evaluate the models, and archive the reports and models (""configuration management"").    Most of this will probably need to be queued up (something like slurm) along with the need to asynchronously monitor performance of multiple DGX servers.

Are there commercial tools to manage testing of a fleet of models and the data?    (must work offline)",MachineLearning
qr5t70,1636582860.0,[N] NVIDIA GTC 2021,Check out OmniSci’s session at the NVIDIA GTC 2021 for FREE! Learn how BIDMC Dept of Endocrinology is leveraging OmniSci’s GPU accelerated analytics platform to explore massive amounts of transcriptomic data and how that has advanced their research processes. Register here! https://reg.rainfocus.com/flow/nvidia/nvidiagtc/ap2/page/sessioncatalog?search=%22A31341%22&ncid=ref-spo-444344,MachineLearning
qr5per,1636582575.0,[D] Paper Explained - Autoregressive Diffusion Models (Full Video Walkthrough),"[https://youtu.be/2h4tRsQzipQ](https://youtu.be/2h4tRsQzipQ)

Diffusion models have made large advances in recent months as a new type of generative models. This paper introduces Autoregressive Diffusion Models (ARDMs), which are a mix between autoregressive generative models and diffusion models. ARDMs are trained to be agnostic to the order of autoregressive decoding and give the user a dynamic tradeoff between speed and performance at decoding time. This paper applies ARDMs to both text and image data, and as an extension, the models can also be used to perform lossless compression.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

3:15 - Decoding Order in Autoregressive Models

6:15 - Autoregressive Diffusion Models

8:35 - Dependent and Independent Sampling

14:25 - Application to Character-Level Language Models

18:15 - How Sampling & Training Works

26:05 - Extension 1: Parallel Sampling

29:20 - Extension 2: Depth Upscaling

33:10 - Conclusion & Comments

&#x200B;

Paper: [https://arxiv.org/abs/2110.02037](https://arxiv.org/abs/2110.02037)",MachineLearning
qr44bi,1636578184.0,[D] Can a trained discriminator in Gan be used for multi class classification ?,I have been thinking of using discriminator as a classifier after it is trained once with gan. But I have no idea on how to make discriminator into classifier because discriminator only says if it is fake or real . Any leads please,MachineLearning
qr3b6w,1636575898.0,[D] Fast CC Taylor Transform for Computer Vision: Potentially train NeRF in matter of minutes,"Hello!

We just released our latest paper on arXiv. We explore the idea of fast summation algorithms (equivalent of the Fast Fourier Transform but with Taylor series instead) in the context of Computer Vision. In terms of computational complexity, our approach disentangles the number of model parameters (N) and model evaluations (M), i.e. our approach is in O(N+M) instead of O(NM). This allows us to reduce the number of FLOPs required for training and inference 150-200x depending on the problem.

Unfortunately, our current implementation is very FLOP inefficient and a FLOP efficient implementation requires a good bit of custom CUDA code. We are working on it :)

Video abstract: [https://www.youtube.com/watch?v=e6gXoMA5te4](https://www.youtube.com/watch?v=e6gXoMA5te4)

Paper link: [https://arxiv.org/abs/2111.00110](https://arxiv.org/abs/2111.00110)

Let me know if you have any questions!",MachineLearning
qr1pex,1636571402.0,"[D] Mel Spectrum, is it useful for non-speech recognition classification tasks?","I am working on a long-term project involving the development of a process monitoring program for an additive manufacturing process. I have read a good deal on different techniques used in audio classification, speech recognition, etc. From what I understand, most machine learning models involving audio either use features extracted from the time and frequency representations of the signal or spectrograms as inputs.

When looking into audio feature extraction, I have noticed it is very common to extract frequency-domain features from the mel spectrum. Obviously, the mel spectrum is very useful in speech recognition tasks since it is intended to align with how a human ear perceives sound. My question: is the mel spectrum useful for audio classification or anomaly detection tasks that do not involve human speech? If so, what is the reason it would be more useful than say the standard frequency scale spectrum for a non speech recognition task?

(literature references would be helpful)",MachineLearning
qr0rck,1636568821.0,[D] How to avoid CPU bottlenecking in PyTorch - training slowed by augmentations and data loading?,"Hello

My colleague and I are training models on a few workstations and we are noticing some bottlenecks that are not leveraging all our GPUs and stopping us from reaching full performance. We are curious what techniques folks use in Python / PyTorch to fully make use of the available CPU cores to keep the GPUs saturated, data loading or data formatting tricks, etc.

Firstly our systems:

* 1 AMD 3950 Ryzen, 128 GB Ram 3x 3090 FE - M2 SSDs for Data sets
* 1 Intel i9 10900k, 64 GB Ram, 2x 3090 FE - M2 SSDs for Data Sets

We notice that both of our systems take the same amount of time per epoch - ie - we get no gains with 3 GPUs vs 2 GPUs, which is frustrating.

Some things we are observing:
* CPUs on both systems spike to 100% CPU on occasion but aren't always utilized
* Disk throughput via IOTOP shows around 50 - 55 MB/s max read, which is way below SSD speeds. Surprisingly low.
* GPU usage is very spikey.

[Here's an image of NVTop and HTop for both systems](https://imgur.com/a/MhFuISB)

Some things we are doing:

* We are using PyTorch 1.10
* Pillow-Simd and the latest Nvidia NGC containers. We also use PyTorch Lighting for training.
* [We follow most of the best practices here](https://tigress-web.princeton.edu/~jdh4/PyTorchPerformanceTuningGuide_GTC2021.pdf)
* We are setting to gradient to none instead of zero grad for performance small improvements
* We are setting cu-DNN auto-benchmark to true
* We are using the Distributed Data Parallel accelerator
* We are using Pinned Memory.
* We are using num_workers = 8
* We see this behavior of low GPU usage / without augmentations.
* We've reduced batch size as an experiment to see where the issues lie. We are at 1/3rd max possible batch size - and we see maybe a 10-20% difference in performance

Some things we have observed:

* We get intermittent crashing if we increase num workers above 8 -
* We've noticed GPU 0 on our 3 GPU system is sometimes idle (which would explain performance differences). However its unclear to us why that may be. [Similar to this issue](https://github.com/PyTorchLightning/pytorch-lightning/discussions/2701)

Our guess is image loading and pre-processing appear to be the issue? We aren't entirely sure if we are diagnosing this correctly.

How are folks getting around issues like these? Should we be pre-processing our data set somehow and storing it in a more optimal format? We are relying on Pillow-Simd for image reading, decoding and copying to tensors.

Are there any good pragmatic guides to optimizing training?

Thank you.",MachineLearning
qqzuh0,1636566313.0,"[P] Cedille, the largest French language model (6b), released in open source","## [📝 DEMO](https://app.cedille.ai) / [📘 REPO](https://github.com/coteries/cedille-ai)

We have spent the last 3 months of our lives, teraFLOPs of compute and gone through 300gb of text to bring you Cedille:

> **Ce que j'aime quand je mange une baguette, c'est** quand celle-ci est craquante.
Je ne saurais définir le terme ""craquant"" mais je sais que lorsque c'est le cas, je peux être sûre que la baguette est bonne.

The entirety of French spirit captured in measly 6B parameters! 🇫🇷🥖

More seriously, we are super excited to share Cedille, the so far largest French language model: https://en.cedille.ai

You can play with it right now on our playground (as long as servers hold 😅) : https://app.cedille.ai

We are proponents of “open AI” and as such have released a checkpoint for the world to use (MIT license) : https://github.com/coteries/cedille-ai

Another aspect we had fun with is dataset filtering. We have run the [whole C4 French dataset](https://github.com/allenai/allennlp/discussions/5265) through the Detoxify classifier to clean it up 🤬

Some acknowledgements :

* Cedille is based on GPT-J, the 6b model developed by the wizards at EleutherAI: https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/
* Cedille was also generously supported by the Google TFRC program: https://sites.research.google/trc/about/",MachineLearning
qqzpf6,1636565909.0,[P] List of ICLR 2022 Papers with Review Scores,"ICLR 2022 reviews are publicly available now. We have compiled a list of papers sorted by the review scores (weighted by confidence).

**Link:** [https://papers.labml.ai/papers/iclr\_2022?sort\_by=conference\_score&dsc=0](https://papers.labml.ai/papers/iclr_2022?sort_by=conference_score&dsc=0)

**Mean review score is  4.9**

https://preview.redd.it/zhr30oj7xsy71.png?width=1676&format=png&auto=webp&s=1406196f52d50eb8d49b5d36b9b726704771576a",MachineLearning
qqxcgt,1636559400.0,"[R] Microsoft India Proposes Varuna: Scalable, Low-Cost Training of Massive Deep Learning Models","A Microsoft Research India team presents Varuna, a system for training massive deep learning models on commodity networking that eliminates the need for specialized hyperclusters and alleviates the cost, scale, and resource utilization challenges of deep learning model training.

Here is a quick read: [Microsoft India Proposes Varuna: Scalable, Low-Cost Training of Massive Deep Learning Models.](https://syncedreview.com/2021/11/10/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-142/)

The Varuna code has been open-sourced and is available on the project’s [Github](https://github.com/microsoft/varuna). The paper *Varuna: Scalable, Low-cost Training of Massive Deep Learning Models* is on [arXiv](https://arxiv.org/abs/2111.04007).",MachineLearning
qqwa9u,1636556399.0,[D] Motivation for a 3D bounding-box in ADAS,"One of the common test-sets for ""car recognition"" is called KITTI. Until 2017 they had only a 2D bounding-box test-set, but around that year they moved to a 3D bounding-box test-sets. Can anyone explain what is the motivation behind a 3D bounding-box for ADAS and its features?",MachineLearning
qqvsu0,1636554957.0,"[N] Modernizing Gov Finance Data & Creating Infrastructure for Fed AI Adoption with Justin Marsico, BFS CDO Thursday, November 18, 2021 at 11:30 AM ET","Hi r/MachineLearning!

I wanted to share an upcoming webinar with you. Below are the details from the website:

**Featured Guest Speaker: Justin Marsico, Chief Data Officer  and Deputy Assistant Commissioner at the Bureau of the Fiscal Service!**

During this presentation learn how the US Treasury's Bureau of the  Fiscal Service is building a better public understanding of federal  finance as they continue to modernize their management of data and data  sharing. Justin Marsico, Chief Data Officer for the Fiscal Service  shares unique opportunities around data at the federal level, what it  entails to create the infrastructure for AI adoption, how they are  recruiting data scientists, as well as the challenges and opportunities  in data governance, security, ownership and related data areas. By  offering clear, accessible information, citizens can see how their  taxpayer dollars are spent and learn about the federal budget. Justin  will give a live demonstration of the latest online resources so  attendees can learn how the Fiscal Service is enhancing data education  and building and enhancing public trust through greater transparency.

Come join us for this great presentation around areas related to:

* Enhancing AI Skills in the Government Workforce/Recruiting Data Scientists
* Finding the most appropriate use cases
* Creating the Infrastructure for AI Adoption

and stick around for Q&A with Justin at the end.

**Agenda:**

11:30-12:30pm: Featured Presentation

12:30-13:00pm: Your Q&A and interaction

Link to website: [https://events.cognilytica.com/CLNDMzMXwxOA](https://events.cognilytica.com/CLNDMzMXwxOA)",MachineLearning
qquog8,1636551616.0,[D] Good Advertising Papers Using RL or DL,"Hey, everybody

I'm currently working in AdTech and I'm searching for innovative products in advertising (CTR prediction, Digital Inventory Pricing,  Online learning for costumer segmentation or other stuff).

If you can help me, please suggest me a good paper in the comments or where I'd be able to find good papers in those subjects (I've looked into fb research and arxiv mainly).",MachineLearning
qqu6xh,1636550132.0,"[R] Rebooting ACGAN: A new GAN that achieves SOTA results and harmonizes with various architectures, adversarial losses, and even differentiable augmentations (Neurips 2021).","A research team from Pohang University of Science and Technology introduces a new type of ACGAN: the Rebooted Auxiliary Classifier GAN (ReACGAN) to overcome unstable training and poor generation performance of ACGAN. 

Here is a quick summary of the paper

* Gradient exploding in the classifier of ACGAN can cause an undesirable training collapse.
* Simply normalizing feature embeddings can resolve the problem.
* Using the normalization technique, we propose the Rebooted Auxiliary Classifier GAN (ReACGAN).
* ReACGAN achieves state-of-the-arts generation results on benchmark datasets. 
* ReACGAN harmonizes with various GAN architectures (DCGAN, ResNet, Big ResNet, StyleGAN2), adversarial losses, and differentiable augmentations (ADA, DiffAugment).


arXiv: [https://arxiv.org/abs/2111.01118](https://arxiv.org/abs/2111.01118)


https://preview.redd.it/s6zmab0ijry71.png?width=981&format=png&auto=webp&s=2741879d0fb0edc743617caa492fe92403d48612

https://preview.redd.it/0m2zos4zjry71.png?width=1244&format=png&auto=webp&s=b3b43dd51e2ca68569ce44536bab55eaac51bfb8",MachineLearning
qqssgq,1636545435.0,[P] New open-source vector search solution,"Meet our new open-source vector search solution - Vektonn.

We offer an opportunity for product teams and data scientists to solve the problem of reliable vector data storage, scalability, and undisturbed availability.

We store embeddings and their attributes, which are more interesting to users since they can use real-world objects. For example, they can identify objects using their real identification.

We support changing indexes as new data arrives (delete, change, or add data to the index) parallel with search queries.

You can expand multiple indexes over a single data source (vectors and attributes) and seamlessly transition to new versions of indexes. You can expand different indexes with different parameters of the same data.

You can work with vectors of any type. For example, you can use bag-of-words to solve word processing problems and load appropriate sparse vectors into Vektonn.

We'd appreciate any feedback or suggestions for the project and welcome GitHub stars to join in if, of course, you find it interesting. 🙂

Learn more - [https://vektonn.io/](https://vektonn.io/)

See what we have done - [https://github.com/vektonn](https://github.com/vektonn)",MachineLearning
qqqzmu,1636538365.0,[D] Advice on buying PC,"I'm doing a PhD in ML and have decided I need a better set-up computationally. Currently I'm running most scripts either locally on my Dell XPS 13 (no GPU) or on Colab, but evidently both are insufficient. As there's a budget in my PhD studentship for equipment, I'm thinking of buying a desktop that I can SSH into from my laptop. Does anyone have experience with this, and if so, recommendations (e.g. for GPU/CPU specs)?

Info:

* Area of research not yet set, though I will definitely be working with generative models (though not huge models; only tabular and time-series data). Work often involves data preprocessing steps that might mainly require CPU power.
* No fixed budget, though I'll need to justify my purchase

It seems buying a moderately good gaming PC with Nvidia GPU is financially and computationally interesting.

Any help is appreciated!

&#x200B;

EDIT: Budget-wise I was thinking below 2000£ to be able to justify it to my funding body, but if this is too limiting please let me know.",MachineLearning
qqopdj,1636528458.0,[D] Google AutoML's prices,"I'm trying to understand Google AutoML's pricing and I have three questions:

1. What is ""price for forecasting"" here  [https://cloud.google.com/vertex-ai/pricing#tabular-data](https://cloud.google.com/vertex-ai/pricing#tabular-data) ?
2. How much will I pay for having an endpoint available 24/7 to which I can post data and execute previously trained model (assuming simple numerical data with classification)?
3. Can I upload my own model and have it ready for predictions?

Thanks",MachineLearning
qqmhiz,1636519924.0,[R] MIT AI Researchers Introduce ‘PARP’: A Method To Improve The Efficiency And Performance Of A Neural Network,"Recent developments in machine learning have enabled automated speech-recognition technologies, such as Siri, to learn the world’s uncommon languages, which lack the enormous volume of transcribed speech required to train algorithms. However, these methods are frequently too complicated and costly to be broadly used.

Researchers from MIT, National Taiwan University, and the University of California, Santa Barbara, have developed a simple technique that minimizes the complexity of a sophisticated speech-learning model, allowing it to run more efficiently and achieve higher performance.

Their method entails deleting unneeded components from a standard but complex speech recognition model and then making slight tweaks to recognize a given language. Teaching this model an unusual language is a low-cost and time-efficient process because only minor adjustments are required once the larger model is trimmed down to size.

# [Read The](https://arxiv.org/pdf/2106.05933.pdf) [Paper](https://arxiv.org/pdf/2106.05933.pdf) | [Checkout The](https://people.csail.mit.edu/clai24/parp/) [Project](https://people.csail.mit.edu/clai24/parp/) | [5 Min Read](https://www.marktechpost.com/2021/11/09/mit-ai-researchers-introduce-parp-a-method-to-improve-the-efficiency-and-performance-of-a-neural-network/) | [MIT Blog](https://news.mit.edu/2021/speech-recognition-uncommon-languages-1104)

&#x200B;

https://preview.redd.it/rigqmwvd8py71.png?width=2495&format=png&auto=webp&s=5cf9af3adbcbaad5983cffe67f01c9785a278e9b",MachineLearning
qqkwas,1636514526.0,[Discussion] ICLR 2022 submission statistics,"The statistics of ICLR2022 submission can be found here:

[https://guoqiangwei.xyz/htmls/iclr2022\_stats.html](https://guoqiangwei.xyz/htmls/iclr2022_stats.html)

https://preview.redd.it/sp68f1sgsoy71.png?width=1918&format=png&auto=webp&s=583d679ac168070e706a508ffba1f232e034dc56

**1.**  **number of reviewers for each submission:**

[**https://guoqiangwei.xyz/htmls/iclr2022\_stats\_number\_of\_reviewers.html**](https://guoqiangwei.xyz/htmls/iclr2022_stats_number_of_reviewers.html)

* **Submissions with maximum # reviewers:** 2 item in total, each with 7 reviewers

https://preview.redd.it/fknneqvqlqy71.png?width=1612&format=png&auto=webp&s=782f04a03ba15489287c9544ee95d30cacad039e

* **Submissions with minimum # reviewers:** 20+ item in total, each with 2 reviewers

https://preview.redd.it/u7ehgnwulqy71.png?width=1766&format=png&auto=webp&s=f26beb116dc909962719fe7c865026bda9208308

**2. Rating variance**

[**https://guoqiangwei.xyz/htmls/iclr2022\_stats\_variance.html**](https://guoqiangwei.xyz/htmls/iclr2022_stats_variance.html)

* **Submissions with highest rating variance**

https://preview.redd.it/ckbkh5zbmqy71.png?width=1266&format=png&auto=webp&s=46b3d25fa01925e5ee9c9a54b9d581ed52db91b9

* **Submissions with highest rating gap (max - min)**

https://preview.redd.it/mggqs2uemqy71.png?width=1702&format=png&auto=webp&s=ee601ab534eb1bdc9b9199a6164b5480b77821e3

* **Submissions with lowest rating variance**

https://preview.redd.it/adx9lgehmqy71.png?width=1914&format=png&auto=webp&s=b4c2452bbeb1cbe30d0d9fff67b9637edcb5ebfe

&#x200B;

&#x200B;

More data can be found here:

[https://github.com/weigq/iclr2022\_stats](https://github.com/weigq/iclr2022_stats)",MachineLearning
qqhct3,1636503654.0,"""[D]"" Interesting bit of info from HTC Keynote - Nvidia Selene 500 node superpod trained GPT3 in 11 days","I'm not sure the cost to run such a machine, but I'd imagine that represents a big cost and time reduction vs 1 year ago.",MachineLearning
qqfe6y,1636497858.0,[D] How to train GANs really fast - Projected GANs Converge Faster explained (5-minute summary by Casual GAN Papers),"Despite significant progress in the field training GANs from scratch is still no easy task, especially for smaller datasets. Luckily Axel Sauer and the team at the University of Tübingen came up with a Projected GAN  that achieves SOTA-level FID in hours instead of days and works on even the tiniest datasets. The new training method works by utilizing a  pretrained network to obtain embeddings for real and fake images that the discriminator processes. Additionally, feature pyramids provide multi-scale feedback from multiple discriminators and random projections better utilize deeper layers of the pretrained network.

Full summary: [https://t.me/casual\_gan/181](https://t.me/casual_gan/181)

Blog post: [https://www.casualganpapers.com/data-efficient-fast-gan-training-small-datasets/ProjectedGAN-explained.html](https://www.casualganpapers.com/data-efficient-fast-gan-training-small-datasets/ProjectedGAN-explained.html)

[ProjectedGAN](https://preview.redd.it/vqyv3adpeny71.png?width=1730&format=png&auto=webp&s=f1788f97b3e8b9dc2266792da101c235d48ffa5d)

UPD: I originally included the wrong links
[arxiv](https://studios.disneyresearch.com/app/uploads/2021/04/Adaptive-Convolutions-for-Structure-Aware-Style-Transfer.pdf) / [code](https://github.com/RElbers/ada-conv-pytorch)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",MachineLearning
qqdkeo,1636492686.0,[P] GPT-3 in the style of Shel Silverstein,"I've recently been playing with the OpenAI API beta for GPT-3, and used their fine-tuning API to create a model that has been fine-tuned on a corpus of all of Shel Silverstein's poetry. The resulting model generates whimsical poems based on a prompted title.

Here's one example I liked, ""Walking On a Whale""

> I am walking on a whale,

> I feel it move and swell,

> I feel the mist come in and float,

> I feel the rain and the cold.

> But I don't mind at all-

> It's just like walking on the ground.

While the model is quite good at understanding semantics, and even has a flair for the vaguely metaphorical, it didn't pick up any notion of rhyming. It seems like this would be a hard thing for the model to learn given the training data it's seen, since only very rarely does the rhyme scheme affect the conditional likelihood of a word.

Question for any commenters: if you had a rhyming dictionary where you could simply look up whether two words rhyme instead of trying to infer this as some latent attribute, how could you update a large NLP model like this to take into account such declarative knowledge?

More examples and discussion here: http://dean.dog/shel-silverstein-gpt3/",MachineLearning
qqcrbh,1636490443.0,[R] Intel Optimized Facebook DLRM with 8x speedup (Deep Learning Recommendation Model),"Intel leveraged SigOpt's Hyper Parameter Optimization platform to achieve a software speedup for DLRM. Additionally, Intel leveraged vertical split embedding, LAMB optimization, and parallelizable data loaders.",MachineLearning
qqckfw,1636489878.0,[N] Message from r/MLOps: Announcing Our first AMA!,"Hi, mod of r/MLOPs here. We finally managed to book our first AMA, which should interest some of the 2M members here.

The following is the content of the sticky over at our sub.
Would you please help us grow our community of MLOps enthusiasts by not burying this post 八(＾□＾*)
------------------------------------------------------------------------------

I don't know if you remember, but we were going to have AMAs here to celebrate the fact that there are so many of us!
Naturally, since this is a niche subreddit, it wasn't as if top-tier mlops superheroes were lining up to post an AMA here...


But then, a miracle happened!



I am delighted to announce that [Alessya Visnjic](https://twitter.com/zalessya/status/1457763397106241540?s=20) will be doing an AMA. Here, this Thursday! So spread the word, and let's make this AMA be the first of many successful ones.


And just in case you are not as immersed in the MLOps ecosphere as I am, here is her bio:


>Alessya Visnjic is the CEO and co-founder of WhyLabs, the AI Observability company on a mission to build the interface between AI and human operators. Before WhyLabs, Alessya was a CTO-in-residence at the Allen Institute for AI (AI2), evaluating the commercial potential for the latest advancements in AI research. Earlier in her career, Alessya spent 9 years at Amazon leading Machine Learning adoption and tooling efforts. She was a founding member of Amazon's first ML research center in Berlin, Germany. Alessya is also the founder of Rsqrd AI, a global community of 1,000+ AI practitioners committed to making AI technology Robust & Responsible.

Of course, there's always an ulterior motive. Alessya will be focusing on the recent announcements by WhyLabs - their round of funding and their new SaaS solution called AI Observatory.

Personally, I think their corner of the MLOps tooling space is super exciting, and WhyLabs are doing some hard opensource groundwork. Additionally, their marketing is not spammy, so it's an honor to host them on the sub.",MachineLearning
qq75zu,1636474726.0,[D] How do you choose an Optimizer? And why are there so many?,"Choosing an optimizer for the training of ANNS is one of the most critical design choices. Because ANNS are black boxes, the theoretical guidelines on the overall design are very limited. They are mostly anecdotally and strongly depend on the developers experience.

&#x200B;

When it comes to optimizer, there are hunderts available, from SGD to Adam to very specific ones. It feels like there is a custom-tailored optimizer for every problem and every architecture.

&#x200B;

Why is it so hard to come up with something more general? Why are there this many optimizer? And, how do you choose an optimizer for your project?",MachineLearning
qq6jgo,1636472979.0,[D] Virtual MLOps Round Table,"I'm putting together an MLOps roundtable focused on peer learning. ML and MLOps practitioners can share how they utilize MLOps to automate and scale their ML processes and learn about how other teams structure theirs.

It works like this: we break people up into small groups of 5-7 people based on their team size and what they are working on.

There is absolutely no selling or pitching. The focus is pure peer learning.

You can sign up here if you're interested: [https://www.eventbrite.com/e/199371063217](https://www.eventbrite.com/e/199371063217)

Let me know if you have any ideas, thoughts, or feedback.",MachineLearning
qq644r,1636471763.0,[D] Evaluating the effectiveness of text generation,"I'm using GPT3 to generate text based on a Q&A dataset (the data is domain specific, based on data scrapped from various internal company sources). The challenge I am facing is that the quality of the output is somewhat subjective.

This makes it hard to improve the model output - I've easily been able to move beyond outputting gibberish to something which works reasonably well. However, I am not finding it hard to evaluating the effectiveness of minor model changes (e.g. temperature, prompt design, tweaks to the dataset, etc ...)

I'm considering 'crowd sourcing' input from my colleagues, giving them model output (with various tweaks) and asking them to score the results. However, this has obvious limitations!

So, I was wondering if there are techniques that people have developed that make it easier to fine-tune models where the output has a subjective quality?",MachineLearning
qq5hrq,1636469988.0,[P] Community sourced Open Audio Datasets – Hacktoberfest 2021,"Hey r/ML! A month ago, [I posted here](https://www.reddit.com/r/MachineLearning/comments/q574t3/d_supporting_hacktoberfest_for_ml_datasets/) about [DagsHub](https://DagsHub.com) supporting Hacktoberfest for ML Datasets. We wanted to do something that was geared towards the ML community, and we decided to create an open-source catalog of 🔊  audio datasets.

The response has been truly amazing! We received 40 dataset contributions, which are now publicly available, and viewable on DagsHub. They cover various tasks, languages, and sizes, and you can use them all for your projects.

If you want to check out the list of datasets: [https://dagshub.com/blog/hacktoberfest-2021-open-source-audio-datasets/](https://dagshub.com/blog/hacktoberfest-2021-open-source-audio-datasets/). I can't wait to see what everyone builds with these.

A huge **THANK YOU** to everyone who participated! You are what made this possible! The fact that Hacktoberfest is over doesn't mean you can't continue contributing. We'd love to see more datasets, both in the audio domain and others.",MachineLearning
qq5c51,1636469545.0,[D] ICLR2022 review stats,"I crawled the ICLR2022 preliminary reviews with some help of another repo and uploaded the crawled raw data (crawled today around 2PM UTC+1) .You can also find some quick stats like

* distribution of mean scores etc..
* best paper by mean/median score
* most controversial paper by std of scores

in the following notebook:

[https://github.com/VietTralala/ICLR2022-OpenReviewData/blob/master/analyze\_reviews.ipynb](https://github.com/VietTralala/ICLR2022-OpenReviewData/blob/master/analyze_reviews.ipynb)

Feel free to play around with it ✌

# Excerpt of the data

## best 10 paper by median score
| paper_id    | title                                                                            | link                                        | keywords                                                                                                                                                               |    mean |   max |   min |      std |   median |   num |
|:------------|:---------------------------------------------------------------------------------|:--------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------:|------:|------:|---------:|---------:|------:|
| LdlwbBP2mlq | Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond       | https://openreview.net/forum?id=LdlwbBP2mlq | Local SGD, Minibatch SGD, Shuffling, Without-replacement, Convex Optimization, Stochastic Optimization, Federated Learning, Large Scale Learning, Distributed Learning | 8       |     8 |     8 | 0        |        8 |     3 |
| iMSjopcOn0p | MT3: Multi-Task Multitrack Music Transcription                                   | https://openreview.net/forum?id=iMSjopcOn0p | music transcription, transformer, multi-task learning, low resource learning, music understanding, music information retrieval                                         | 8       |     8 |     8 | 0        |        8 |     4 |
| BrPdX1bDZkQ | DemoDICE: Offline Imitation Learning with Supplementary Imperfect Demonstrations | https://openreview.net/forum?id=BrPdX1bDZkQ | imitation learning, offline imitation learning, imperfect demonstration, non-expert demonstration                                                                      | 7.33333 |     8 |     6 | 0.942809 |        8 |     3 |
| sOK-zS6WHB  | Responsible Disclosure of Generative Models Using Scalable Fingerprinting        | https://openreview.net/forum?id=sOK-zS6WHB  | Generative models, fingerprinting, responsible disclosure, deep fake detection and attribution                                                                         | 6.4     |     8 |     3 | 2.05913  |        8 |     5 |
| bVvMOtLMiw  | DIVA: Dataset Derivative of a Learning Task                                      | https://openreview.net/forum?id=bVvMOtLMiw  | Leave one out cross validation, AutoML, dataset optimization                                                                                                           | 7       |     8 |     5 | 1.41421  |        8 |     3 |
| lrocYB-0ST2 | Approximation and Learning with Deep Convolutional Models: a Kernel Perspective  | https://openreview.net/forum?id=lrocYB-0ST2 | kernel methods, deep learning theory, convolution, approximation, generalization                                                                                       | 7.5     |     8 |     6 | 0.866025 |        8 |     4 |
| siCt4xZn5Ve | What Happens after SGD Reaches Zero Loss? --A Mathematical Framework             | https://openreview.net/forum?id=siCt4xZn5Ve | SGD, implicit bias, generalization, deep learning, implicit regularization, manifold                                                                                   | 8       |    10 |     6 | 1.41421  |        8 |     4 |
| 0DLwqQLmqV  | NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy                       | https://openreview.net/forum?id=0DLwqQLmqV  | neural architecture search, AutoML                                                                                                                                     | 7.5     |     8 |     6 | 0.866025 |        8 |     4 |
| K0E_F0gFDgA | The MultiBERTs: BERT Reproductions for Robustness Analysis                       | https://openreview.net/forum?id=K0E_F0gFDgA | Pre-trained models, BERT, bootstrapping, hypothesis testing, robustness                                                                                                | 7.33333 |     8 |     6 | 0.942809 |        8 |     3 |
| fKv__asZk47 | Learning Similarity Metrics for Volumetric Simulations with Multiscale CNNs      | https://openreview.net/forum?id=fKv__asZk47 | metric learning, PDEs, numerical simulation, physical modeling                                                                                                         | 6.33333 |     8 |     3 | 2.35702  |        8 |     3 |

---

## 10 most controversial papers by std of scores

| paper_id    | title                                                                             | link                                        | keywords                                                                                                                                                                                   |    mean |   max |   min |     std |   median |   num |
|:------------|:----------------------------------------------------------------------------------|:--------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------:|------:|------:|--------:|---------:|------:|
| p0rCmDEN_-  | Visual hyperacuity with moving sensor and recurrent neural computations           | https://openreview.net/forum?id=p0rCmDEN_-  | visual system, convolutional neural networks, recurrent neural networks, active vision, active sensing, ocular drift                                                                       | 4.75    |    10 |     3 | 3.03109 |        3 |     4 |
| FPGs276lUeq | Palette: Image-to-Image Diffusion Models                                          | https://openreview.net/forum?id=FPGs276lUeq | machine learning, artificial intelligence, computer vision                                                                                                                                 | 4.75    |    10 |     3 | 3.03109 |        3 |     4 |
| SC6JbEviuD0 | White Paper Assistance: A Step Forward Beyond the Shortcut Learning               | https://openreview.net/forum?id=SC6JbEviuD0 | Shortcut Learning, Bias, Classification, Imbalanced Classification, Robustness                                                                                                             | 3.75    |     8 |     1 | 2.94746 |        3 |     4 |
| 7IWGzQ6gZ1D | Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates  | https://openreview.net/forum?id=7IWGzQ6gZ1D | reinforcement learning, lifelong learning, transfer learning, successor features                                                                                                           | 6       |    10 |     3 | 2.94392 |        5 |     3 |
| JGO8CvG5S9  | Universal Approximation Under Constraints is Possible with Transformers           | https://openreview.net/forum?id=JGO8CvG5S9  | Constrained Universal Approximation, Probabilistic Attention, Transformer Networks, Geometric Deep Learning, Measurable Maximum Theorem, Non-Affine Random Projections, Optimal Transport. | 7       |    10 |     3 | 2.94392 |        8 |     3 |
| 3ILxkQ7yElm | Learning Continuous Environment Fields via Implicit Functions                     | https://openreview.net/forum?id=3ILxkQ7yElm | Continuous Scene Representation, Implicit Neural Networks                                                                                                                                  | 5       |     8 |     1 | 2.94392 |        6 |     3 |
| V1MBgNBx5E  | Mask and Understand: Evaluating the Importance of Parameters                      | https://openreview.net/forum?id=V1MBgNBx5E  | influence function, interpretability, model pruning, feature importance ranking                                                                                                            | 4       |     8 |     1 | 2.94392 |        3 |     3 |
| TQ75Md-FqQp | Efficient and Modular Implicit Differentiation                                    | https://openreview.net/forum?id=TQ75Md-FqQp | implicit differentiation, bilevel optimization, autodiff, jax                                                                                                                              | 6.33333 |    10 |     3 | 2.86744 |        6 |     3 |
| MeMMmuWRXsy | Robust Robotic Control from Pixels using Contrastive Recurrent State-Space Models | https://openreview.net/forum?id=MeMMmuWRXsy | contrastive learning, model-based RL, distractions, predictive coding                                                                                                                      | 4.66667 |     8 |     1 | 2.86744 |        5 |     3 |
| kxARp2zoqAk | Information-Aware Time Series Meta-Contrastive Learning                           | https://openreview.net/forum?id=kxARp2zoqAk | Information-Aware Time Series Meta-Contrastive Learning                                                                                                                                    | 6.33333 |    10 |     3 | 2.86744 |        6 |     3 |",MachineLearning
qq4v0g,1636468155.0,[R] Deep Shallow Fusion for RNN-T Personalization,"End-to-end deep learning models for Speech Recognition can produce highly accurate transcriptions, but they are a lot harder to personalize. This paper from Facebook's AI team walks through some methods that help increase the accuracy of proper nouns and rare words from end-to-end deep learning models which I found really interesting.

I made a summary of this paper that [you can read here](https://www.assemblyai.com/blog/deep-shallow-fusion-for-rnn-t-personalization/).

And the link to the original paper from Facebook AI can be found here -> [https://arxiv.org/abs/2011.07754](https://arxiv.org/abs/2011.07754)",MachineLearning
qq2rbm,1636461419.0,[D] ICLR 2022 reviews,Share your rants.,MachineLearning
qq21s6,1636458809.0,[D] Why does AMD do so much less work in AI than NVIDIA?,"Or is the assumption in the title false?

Does AMD just not care, or did they get left behind somehow and can't catch up?

&#x200B;

I know this question is very vague, maybe still somebody can point to a fitting interview or something else",MachineLearning
qq18tc,1636455502.0,[D] improving segmentation masks,"I have a dataset with some segmentation masks for objects  (or better polygons around the objects)  I am interested in, but the quality is not very good. The polygons around the objects are correct but very rough, a low number of edges with huge chunks of background in there. Is there some algorithmic way to improve those? I tried GrapCut but the performance is not very good, huge chunks of background are still included and stuff like hair is done very poorly.",MachineLearning
qpzewi,1636447234.0,[D] ML datasets for commercial use,"Hi all,

there are a ton of datasets to ML researchers stemming from different areas. But when looking more closely, the vast majority of them have very restrictive licensing, only allowing to be used for research purposes, but not in a commercial environment.

I am now wondering what the strategies are for obtaining high quality datasets for commercial purposes. So let's say I want to build a car object detection model for my company, one of the most well known detection use cases.

I can neither use any of the public datasets, as they do not allow commercial usage, nor can I use any of the pre-trained models for this task, as they have been trained on these datasets.

I could now:

* Collect my own data
* Pay crowd-annotators to annotate the data
* Buy data

I would be specifically interested in the last point, is there a way to acquire these types of datasets? Is there a market for it?

How are other handling this in their companies?",MachineLearning
qpwdps,1636434371.0,[D] What does having more than three reviews mean on ICLR 22?,"ICLR 22 reviews are in. Hope you guys got good reviews. I noticed that some papers got three reviews, while others got four or five reviews. Why do some papers get more reviews and what does it signify?",MachineLearning
qpw3br,1636433341.0,[D] Recursive ML strategies?,"I'm looking for ideas on how to use recursive ML strategies, possibly utilizing multiple individual models where one model uses the output of another model to make more accurate predictions.

For example, I use two sklearn `RandomForestClassifier` models to provide a simple signal about the direction of the stock market. The first takes `n` inputs and outputs a prediction. The second takes the original `n` inputs plus the output of the first to make a new prediction. It doesn't provide earth-shattering results, but it appears to be slightly better than only using the one model.

Random forests also provide the ability to use Out-of-Bag samples, which could also be used.

I'm just curious if there any established methods, papers I should look at, etc. that discuss meta or recursive strategies to get the most out of ML models.",MachineLearning
qpuqtk,1636428613.0,[R] The How and Why of Bayesian Nonparametric Causal Inference,"A nice summary paper, at the cutting edge of Bayesian causal inference.

Link: https://arxiv.org/abs/2111.03897

Abstract: ""Spurred on by recent successes in causal inference competitions, Bayesian nonparametric (and high-dimensional) methods have recently seen increased attention in the causal inference literature. In this paper, we present a comprehensive overview of Bayesian nonparametric applications to causal inference. Our aims are to (i) introduce the fundamental Bayesian nonparametric toolkit; (ii) discuss how to determine which tool is most appropriate for a given problem; and (iii) show how to avoid common pitfalls in applying Bayesian nonparametric methods in high-dimensional settings. Unlike standard fixed-dimensional parametric problems, where outcome modeling alone can sometimes be effective, we argue that most of the time it is necessary to model both the selection and outcome processes.""",MachineLearning
qpuax4,1636427107.0,"Alibaba DAMO Academy Creates World’s Largest AI Pre-Training Model, With Parameters Far Exceeding Google and Microsoft (10T parameters) [N]","> [According to the company, the M6 has achieved the ultimate low carbon and high efficiency in the industry, using 512 GPUs to train a usable 10 trillion model within 10 days.](https://pandaily.com/alibaba-damo-academy-creates-worlds-largest-ai-pre-training-model-with-parameters-far-exceeding-google-and-microsoft/) Compared to the GPT-3, a large model released last year, M6 achieves the same parameter scale and consumes only 1% of its energy.

Thoughts? The pace of foundational models is starting to get scary, seems like a bigger and bigger model is pushed out every week.",MachineLearning
qplwld,1636401921.0,[D] Commercial Distribution of OpenAI Jukebox Songs,Hello. What is the copyright process for music created by an artificial intelligence? I have made a song using OpenAI's Jukebox and am wondering if I can commercially distribute it in streaming platforms such as Spotify.,MachineLearning
qplveb,1636401821.0,[N] Maritime Grand Challenge - Abu Dhabi,"Came across this Maritime Grand Challenge that I thought others might find interesting. It combines drones, robotics and AI and there’s a $2M first prize and $3M overall in prize money. Open to universities, research institutions, companies and individual innovators.

Here’s a link: [https://www.mbzirc.com/abudhabi\_aspire\_launches\_mbzirc\_challenge.php](https://www.mbzirc.com/abudhabi_aspire_launches_mbzirc_challenge.php).

And a video:

[https://www.youtube.com/watch?v=1AdBJCn15zQ](https://www.youtube.com/watch?v=1AdBJCn15zQ)

The competition is to further development of real-world solutions to illegal fishing, piracy, smuggling and coastline security.  First deadline -- initial phase  includes white papers and registration – is Jan. 31, 2022.

I found out about it through this story:

[https://www.robotics247.com/article/mbzirc\_maritime\_grand\_challenge\_3m\_prize\_launches\_abu\_dhabi/](https://www.robotics247.com/article/mbzirc_maritime_grand_challenge_3m_prize_launches_abu_dhabi/)",MachineLearning
qpk1tt,1636396790.0,Landing AI gets $57 million series A to build a data centric MLOps platform. [News],"Are data centric MLOps tools about to take off?

[CNBC](https://www.cnbc.com/2021/11/08/google-brain-founder-andrew-ng-raises-57-million-for-landing-ai.html)

[TechCrunch](https://techcrunch.com/2021/11/08/landing-ai-machine-learning-operations-tools/)",MachineLearning
qpi381,1636391456.0,[RESEARCH] OpenAI's GPT-3: cases of misusage and failures,"Hello everyone!
Name's Alex, 26yo from Italy and currently studying Marketing and A.I. at IULM University here in Milan.

I grew quite an interest when finally GPT-3 came out, and while discussing with one of my professors, the topic of my BD thesis came up. Long story short, I'm gonna talk about GPT-3.

One of the topics I'd love to cover, is a collection of known applications (aka Use cases).
While I found quite a decent number of (more or less) succesful cases, I can't find anything.
I also tried on Google (since I only refer to Google Scholar to find reliable sources), but still never managed to find anything.
So here I am, asking if you guys know of any case where a company tried to use GPT-3 in some ways but the whole thing didn't end up quite as they expected.

Thank you all!",MachineLearning
qphreq,1636390548.0,[D] Intuition for meaning behind magnitude of covariance,"Covariance matrices are pretty essential to many ML algorithms and probabilistic models. When two variables have positive covariance, they are correlated, when they have negative covariance, they are inversely correlated and when the covariance is zero, they are not correlated. However, the degree of correlation cannot be read from the magnitude of the covariance value.

My question follows: well, what *can be read* from this magnitude. What does it mean if two variables have a very large covariance value opposed to a small one?",MachineLearning
qphg92,1636389639.0,[N] AMD launches MI200 AI accelerators (2.5x Nvidia A100 FP32 performance),"Source: https://twitter.com/IanCutress/status/1457746191077232650

More Info: https://www.anandtech.com/show/17054/amd-announces-instinct-mi200-accelerator-family-cdna2-exacale-servers

> For today’s announcement, AMD is revealing 3 MI200 series accelerators. These are the top-end MI250X, it’s smaller sibling the MI250, and finally an MI200 PCIe card, the MI210. The two MI250 parts are the focus of today’s announcement, and for now AMD has not announced the full specifications of the MI210.",MachineLearning
qpf5t0,1636382856.0,[R] Introducing MetaICL: A Language Model Meta-Training Framework for Few-Shot In-Context Learning,"A research team from the University of Washington, FacebookAI Research and the Allen Institute for AI introduces Meta-training for InContext Learning (MetaICL), a new meta-training framework for few-shot learning where an LM is meta-trained to learn in-context — conditioning on training examples to recover the task and make predictions.

Here is a quick read: Introducing MetaICL:[A Language Model Meta-Training Framework for Few-Show In-Context Learning.](https://syncedreview.com/2021/11/08/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-140/)

The MetaICL code and data will be made available on the project’s [GitHub](https://github.com/facebookresearch/metaicl). The paper *MetaICL: Learning to Learn In Context* is on [arXiv](https://arxiv.org/abs/2110.15943).",MachineLearning
qpenkt,1636381349.0,[Project] Google MoveNet (Real-Time Pose Estimation) Used To Control Nintendo Punch-Out!!,"Hey AI fans, I hacked the original Nintendo Punch-Out!! so that you control it with actual punches! This is a boxing video game that now uses Google's MoveNet (real-time pose estimation) to track your movements and detect punches, blocks, and other moves and then sends those commands to the game.

You can check out the full video here with plenty of sweet MoveNet footage: [https://www.youtube.com/watch?v=07JibJJVNp8](https://www.youtube.com/watch?v=07JibJJVNp8)

And play it yourself here: [https://reallifepunchout.com](https://reallifepunchout.com)


&#x200B;

https://reddit.com/link/qpenkt/video/swwt8jw9sdy71/player",MachineLearning
qpbci5,1636369620.0,"[P] HuBERT: How to apply BERT to speech, visually explained","Recently Facebook AI released HuBERT, a BERT-like model for learning powerful speech representations. At first glance, this model looks similar to wav2vec 2.0, but the training process/objective is actually very different.

I made some detailed illustrations to visually explain the pre-training process of HuBERT and how it compares to wav2vec 2.0. Both of these models are already available in the HuggingFace Transformers library.

[https://jonathanbgn.com/2021/10/30/hubert-visually-explained.html](https://jonathanbgn.com/2021/10/30/hubert-visually-explained.html)

Hope this is helpful!",MachineLearning
qp9mnn,1636361620.0,"[R] A Unified View of Relational Deep Learning for Polypharmacy Side Effect, Combination Synergy, and Drug-Drug Interaction Prediction","&#x200B;

https://preview.redd.it/t09f9yun5cy71.jpg?width=2146&format=pjpg&auto=webp&s=eafb32e5538a16375d34a82a6c01e0d01d9a677b

**Git:** [https://github.com/AstraZeneca/polypharmacy-ddi-synergy-survey](https://github.com/AstraZeneca/polypharmacy-ddi-synergy-survey)

**Paper:** [https://arxiv.org/abs/2111.02916](https://arxiv.org/abs/2111.02916)

**Abstract:**

In recent years, numerous machine learning models which attempt to solve polypharmacy side effect identification, drug-drug interaction prediction, and combination therapy design tasks have been proposed. Here, we present a unified theoretical view of relational machine learning models which can address these tasks. We provide fundamental definitions, compare existing model architectures and discuss performance metrics, datasets, and evaluation protocols. In addition, we emphasize possible high-impact applications and important future research directions in this domain.

**The paper provides:**

\- A unified model of drug pair scoring models with a general architecture design recipe.
\- Model design comparisons based on architecture and input modalities.
\- Evaluation metrics used by the most important papers.
\- Public datasets that are relevant.
\- Evaluation regime designs for stratified splits

**The Github repo comes with:**

\- Paper links with implementations.
\- Links to the datasets.",MachineLearning
qp9bra,1636360178.0,[Project] JORLDY: OpenSource Reinforcement Learning Framework,"Hello WoRLd! We are Reinforcement Learning (RL) engineers at KakaoEnterprise in South Korea! We published an opensource RL framework and named it JORLDY (Join Our Reinforcement Learning framework for Developing Yours). JORLDY is opened for helping RL researchers and students who study RL. The features of JORLDY are as follows.

* 20+ RL Algorithms ([Pytorch](https://pytorch.org/)) and various RL environment are provided
* The algorithms and environments can be run with simple command
* Algorithms and environment can be easily added and customized
* Distributed RL algorithms are provided using [ray](https://github.com/ray-project/ray)
* Benchmark of the algorithms is conducted in many RL environment

JORLDY github link: [https://github.com/kakaoenterprise/JORLDY](https://github.com/kakaoenterprise/JORLDY)

As we mentioned, JORLDY is an ""open source"" RL framework. Accordingly, our team wants to work with many people to develop JORLDY into a better framework. We would be very grateful if you use it widely and give us a lot of comments about JORLDY.

Thank you!",MachineLearning
qp8897,1636355084.0,[P] Open-NSFW 2: TensorFlow 2 implementation of the Yahoo Open-NSFW model,"Detecting Not-Suitable-For-Work (NSFW) images, in particular pornographic images, is a high demand task in computer vision. The Yahoo Open-NSFW model originally developed with the Caffe framework has been a favourite choice, but the work is now discontinued and Caffe is also becoming less popular.

This Open-NSFW 2 project provides a TensorFlow 2 implementation of the Yahoo model, with references to its previous third-party TensorFlow 1 implementation.

Please take a look!

[https://github.com/bhky/opennsfw2](https://github.com/bhky/opennsfw2)",MachineLearning
qoyyy7,1636322519.0,[D] What happened to Compressive Transformers?,"They promised to solve one of the Transformer architecuter's greatest weaknesses, it's lack of long term memory, but I can't seem to find any bigger experiment using them? Did they not work out?",MachineLearning
qotk5u,1636306842.0,[N] State of AI 2021,"Forth edition: [https://www.stateof.ai/](https://www.stateof.ai/)

If you read it (or skimmed it), what was the most important information in your opinion?",MachineLearning
qorekl,1636300818.0,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning
qordhq,1636300735.0,[D]What is something you took the time to learn that benefitted you the most?,Saw a thread in cscareer questiosn and I thought it was a great question that could help a lot of people in machine learning since there is so much to learn in this field and could use some direction!,MachineLearning
qoqqp2,1636298795.0,[R] stl file data annotation,"First time using Reddit.

Anyone know a software I can use to annotate a point cloud stl medical image file? I'm aware of several softwares for DICOM files but none seem to work with stl.

Ideally free software but willing to pay if good.",MachineLearning
qoqpv2,1636298714.0,[D] Measure the distance between two domains for transfer learning.,"I know there are distances are defined to minimise for the domain adaption.

While I want to know does there exist any distance measurement that can measure the difficulty of performing the domain adaption from the source domain to different target domains？",MachineLearning
qoqp4w,1636298647.0,[Discussion] MLops tool for image data management and exploration,"Hi there,

&#x200B;

Large part of the current work of our developers is dedicated to manual work on data. This is done in several stages of the development:

1. In the beginning when we get the annotated data we go over it (thousands of images). For segmentation tasks we observe the annotated images to validate the annotation correctness and images relevance for our tasks. Some images may be correctly annotated, but not relevant to our task or use-case so we remove them.
2. We check statistics of the data to validate that the data isn't biased. For example that we have enough samples from each class, and that the distribution of instances sizes (e.g. number of pixels in the segmentation mask) within the classes is reasonable.
3. When we have a trained model, we go over images for which the model gives poor results, and try to find similarities to understand weaknesses of the model and find root causes. This often leads to change of the data and repeating stages 1,2 and eventually 3.

These are very time consuming tasks, and we are looking for MLops tools that will make this work more efficient. Optimally, the tool will also take care of other MLops aspects, like experiments orchestration, experiment tracking and data versioning. We can also combine several tools, but using few (one?) tools is preferred. Open source or proprietary paid product are both possible.

Any recommendations?

Thanks!",MachineLearning
qoptlo,1636295924.0,FLOPS Calculation [D],"Please consider the following table:

&#x200B;

https://preview.redd.it/z1w61dlho6y71.png?width=441&format=png&auto=webp&s=eb502bec8b7116771f72719598884956fb6b44be

which is from FaceNet publication. What I am trying is to learn FLOPS calculations with the help of this model so I can calculate for the below ones. If I am not wrong, the first row should have (110\*110\*64 \* 7\*7\*3) 113,836,800 FLOPS which is a bit lesser than given value. Likewise the second convolution row conv2a (55\*55\*64 \* 64 = ) 12390400 which is again near but not exactly the same.

[https:\/\/sefiks.com\/2020\/06\/16\/face-recognition-with-deepid-in-keras\/](https://preview.redd.it/124c1x8fo6y71.png?width=564&format=png&auto=webp&s=7b20c0efff595511c2dff4bdfdefa0c1a318dd50)

&#x200B;

Am I on the right track? What am I doing wrong?

 Also [https://www.thinkautonomous.ai/blog/?p=deep-learning-optimization](https://www.thinkautonomous.ai/blog/?p=deep-learning-optimization) mentions multiplying by 2 after all those operations which further confuses me. ",MachineLearning
qojz5g,1636271893.0,Comparing deep models with different complexity and different accuracies [D]," I am working with a deep learning based system where complexity has to be reduced to a minimal. However, this is having some impact on the accuracy. I am having to ask the question in somewhat hypothetical scenario and hope that I am clear in my statement.

Say one system uses 1 billion FLOPS network to achieve 99% accuracy yet another uses 0.5 billion FLOPS network to give 95%. On equal accuracy we could say that the system with lower flops is better. likewise, on equal FLOPS we would have been able to say that the first system is better. Is there a direct way like a standard KPI to compare these two systems in the given scenario?

In short, can we compare two systems with different accuracies and different FLOPS?",MachineLearning
qoa1tv,1636235133.0,[R] A (rare) real example of a true time series anomaly discovered by an algorithm.," In spite of all the academic work on anomaly detection in time series, it is almost impossible to find a ***real*** example of a true anomaly captured in the wild. Here I present such an example.

A group from Texas A&M/USC has released a very nice large dataset relating to electric grids. Most of the data is *measured* (temp, voltage etc.), but some (Solar Zenith Angle etc.) is *computed*.

As a sanity check upon downloading the data, I ran the Matrix Profile \[a\], to look for any anomalies in the data. It found the highly significant anomaly shown in the attached figure.

Can you guess what it is… (spoiler below)

It took me a few seconds, but I guessed it >!might be a Leap Year bug in the data generator,!< and indeed, after I reported it, I found that this was the case.

Moral of the story. Check your data, and, the Matrix Profile is a very useful tool.

More examples of time series anomalies at \[a\] and \[b\]

\[a\] [www.cs.ucr.edu/\~eamonn/MatrixProfile.html](https://www.cs.ucr.edu/~eamonn/MatrixProfile.html)

\[b\] [https://www.cs.ucr.edu/\~eamonn/MERLIN\_Long\_version\_for\_website.pdf](https://www.cs.ucr.edu/~eamonn/MERLIN_Long_version_for_website.pdf)

\[c\] [https://github.com/tamu-engineering-research/Open-source-power-dataset](https://github.com/tamu-engineering-research/Open-source-power-dataset)",MachineLearning
qo8wvt,1636231637.0,[P] Is it necessary to manually label desired objects in each image for object detection?,"For example i have a dataset of thousands of images of 5 different objects and each object images already stored in different folders and i train my model on it.

Now i give an image as input which contains all 5 of the objects and i want my model to detect all 5 of these objects in the image and draw bounding boxes around them.

To be able to do this, when preparing my dataset do i have to label desired object in each image by drawing boxes around them and then train my model on it?

Is there any better way to do this than manually labelling data of thousands of images?",MachineLearning
qo5in1,1636221258.0,[D] GPT-3 is No Longer the Only Game in Town,"Hey there, I just put out a little article that you might find interesting - [GPT-3 is No Longer the Only Game in Town](https://lastweekin.ai/p/gpt-3-is-no-longer-the-only-game). It catalogues the appearance of models akin to GPT-3 over the course of 2021, like  [HyperCLOVA](https://venturebeat.com/2021/06/01/naver-trained-a-gpt-3-like-korean-language-model/) and such. Hope you enjoy it!

*TLDR: Organizations face significant challenges in creating a model similar to OpenAI’s GPT-3, but nevertheless a half dozen or so models as big or bigger than GPT-3 have been announced over the course  of 2021.*",MachineLearning
qo5fhz,1636220978.0,[P] Problems with a Neural Networks's Output's Order of Magnitude,"

I'm working on a project where I'm trying to train an airplane to control itself in a 2D setting.

The neural network that acts as the airplane's pilot has 2 outputs that are related to the control of the airplane's altitude (angle of attack and throttle, for those that are familiar).

Unfortunately, my network is outputing numbers that have a completely wrong order of magnitude. I tried writing my own activation function to limit their values, but I'm simply getting the max or min allowable value (since my actual output is off the scale).

Is there any way to control the order of magnitude of the outputs? I've tried a ton of things, from intitializing weights to be extremely small, to normalization of the input.",MachineLearning
qo3704,1636214319.0,[D] Professors and research groups in Neural program synthesis,"I want to collect a list of professors and research groups that work in neural program synthesis or program induction.
All you can find with a simple search is groups at Microsoft, Google Brain, ETH Zurich and MIT.
Does any one know other groups that work in this topic especially outside USA.

PS. I couldn't find any groups or professors in Germany
If you have any helpful tips how to do PhD in this topic would be nice",MachineLearning
qo1sdh,1636210107.0,GPTSD - Transfer of trauma from human to machine back to human via machine learning models [P],"&#x200B;

https://preview.redd.it/tf7f8tl9nzx71.jpg?width=808&format=pjpg&auto=webp&s=2c9335110a8bc0931f5c706573ea692216609606

GPTSD is a series of images and text created with GPT2 which explores the transfer of trauma from human to machine back to human via machine learning models. By converting human portraits to text, GPT2 is able to recreate new text base portraits and dream recollections based off the dream diary of a Vietnam war veteren. [https://www.hicetnunc.xyz/gptsd](https://www.hicetnunc.xyz/gptsd)",MachineLearning
qo03fo,1636204803.0,[D] Google MUM details?,"Google has [anounced](https://www.google.com/amp/s/blog.google/products/search/introducing-mum/amp/)   MUM several months ago. In the blog post it says that it uses T5 framework and is multimodal but that is about it.
The name Multitask Unified Model does not help a lot.

Does anybody know how it is trained and how it combines text and images? The only thing i can think of is to train it to generate image captions and then finetune it on multiple tasks, something like bigscience T0pp?",MachineLearning
qnznd4,1636203320.0,[D] Help: Choosing right data/file format for Storing Text Data,"Hello, I'm working on a NLP project which requires me to collect the data. As of now I have collected data and stored in to individual text file(one article into a single text file).

So for future analysis I have cleaned the data and converted into dict (article id and article text) and now I want to store this data into some specific file formate such as Json, hd5 etc.

So I want your recommendation on this, which is the best format to store this kind of data.

Please provide suggestions by keeping in mind the latency in loading data from drive( or faster the better).",MachineLearning
qnxu7h,1636195879.0,[R] Is CVPR really only for computer vision?,"I know [CVPR](https://cvpr2022.thecvf.com/) literally means ""Computer Vision and Pattern Recognition"", but does it really means that any submission in the ML field with no direct link with Computer vision will be discarded?
Thanks!",MachineLearning
qnu4bg,1636178538.0,"[D] ""Real-World Challenges for AGI"" by DeepMind","Either, I did not understand the article ([https://deepmind.com/blog/article/real-world-challenges-for-agi](https://deepmind.com/blog/article/real-world-challenges-for-agi)) or they just inserted the letters A, G, and I randomly in between their current progress with weather prediction and plasma control for fusion.",MachineLearning
qntbjh,1636175091.0,[R] Arch-Net: A Family Of Neural Networks Built With Operators To Bridge The Gap Between Computer Architecture of ASIC Chips And Neural Network Model Architectures,"**Key Takeaways**

* As it turns out, the Arch-Net is actually building a bridge that translates between computer architectures of ASIC chips and neural network model architectures by changing existing floating-point DNNs into hardware-friendly quantized Arch-Net.
* The structure of ArchNet is made up of five operators: 3×3 Convolutions, Batch Normalization, Concatenation, 2×2 Max-pooling, and Fully-Connected layers.
* The conversion to Arch-Net is much simpler without labeled data as researchers employ Blockwise Model Distillation on feature maps.
* Researchers did extensive experiments on image classification and machine translation tasks to confirm that Arch-Net is both effective, efficient and fast.

# [Paper](https://arxiv.org/pdf/2111.01135v1.pdf) | [Github](https://github.com/megvii-research/Arch-Net)

https://preview.redd.it/eqbzv5m1rwx71.png?width=1536&format=png&auto=webp&s=85cd91db543a521b189f5578a985f24629a6c91a",MachineLearning
qnq3m4,1636163429.0,"[P] Open-source project to collect data from multiple databases, apps, SaaS tools and prepare for ML tasks","We have lots of business data scattered across different databases and apps. [Rudderstack](https://github.com/rudderlabs/rudder-server) can integrate data from various sources and then activate this data in your warehouse or business tools for ML operations. This unlocks the potential applications of the data which was hard to do before Rudderstack e.g. UX personalization, business analytics, etc.

I'm seeking feedback, what can I improve and how would you want to use it? AMA",MachineLearning
qnktqk,1636146868.0,"[P] League of Legends Patch 11.21 Game Playing AI (Reinforcement Learning, Supervised Learning) Dataset","This dataset is meant for anyone who would like to try to create a deep learning agent either using supervised or (offline) reinforcement learning to play League of Legends. The dataset contains 72 games from patch 11.21 (last patch) where the game ended in an early surrender. These games were chosen as the game lengths were guaranteed to be low which kept the dataset from being too large.
To download the dataset, go to [this](https://github.com/MiscellaneousStuff/tlol) GitHub link and click on the `Google Drive Link`. The dataset is stored as an SQLite database file and the schema should be relatively self-explanatory. Happy to answer any questions.

This is just a preliminary dataset which demonstrates that this is possible. Within the next few days the dataset will contain 1000s of replays which means 10,000s of champions worth of data (for each time a player plays a champion).

Edit: Database now contains all 191 early surrender games (games ending at or before 3.5 minutes) in the dataset.
This table shows the top 10 champion occurrences within the dataset.

| Champion     | No. |
| ------------ | --- |
| Nami         | 116 |
| Miss Fortune | 103 |
| Lucian       | 61  |
| Khazix       | 36  |
| Viego        | 35  |
| Lux          | 34  |
| Jhin         | 32  |
| Yone         | 30  |
| Camille      | 29  |
| Graves       | 29  |

Edit 2: Larger dataset containing 987 games targeting Miss Fortune in the early game (up to first 5 minutes) with the same schema and format as the first dataset. Also contains all game objects recorded 4 times a second. The games were chosen by getting the games where the MF player lived the longest. This gave a dataset where the players overall had a 64.4% win rate in roughly EUW Diamond II.

Edit 3: A further 728 games also targeting Miss Fortune in the early game (up to first 5 minutes) with the same schema and format as the first and second dataset. This brings the total number of games for the MF-Longevity datasets to 1,715 or
`1,715 games * (5 minutes * 60 seconds * 4 frames/second) := 2,058,000 frames in total.`
This should now be enough to at least create a deep learning agent which can play Miss Fortune for the first five minutes of a game at least to a basic level.

Edit 4: Another day another dataset. A further 773 games from the `MFLongevity` dataset have been uploaded. I have now also included a Jupyter Notebook to analyse the data from the `191-EarlyFF` dataset which works completely standalone from Google Colab. Feel free to also run it locally if you wish to.

[GitHub Link](https://github.com/MiscellaneousStuff/tlol)

[![Open Notebook In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MiscellaneousStuff/tlol/blob/main/League_of_Legends_Patch_11_21_(Reinforcement_Learning).ipynb)

Edit 5: Very in-depth explanation for the process used to create the dataset.

[Blog Post](https://miscellaneousstuff.github.io/project/2021/11/19/tlol-part-6-dataset-generation.html)",MachineLearning
qniktz,1636140262.0,[D] How do you structure your CV?,"After I made my first CV I've been working on the same company for about two years and worked in multiple projects. Now I'm interested in looking out for a bit and reworking my CV and don't really know in what way should I display my projects and skills.

Any Senior AI-related Engineers want to share your CVs or suggestions?",MachineLearning
qnhtxj,1636138088.0,[P] A place to create your online data science portfolio and browse the community data science projects,"Hey all! I'm a data scientist who has shifted career from the biomedical field - now working at a tech company. It was hard to learn data science skills, showcase them to my first employers and stand out. That's why I created [datascienceportfol.io](https://www.datascienceportfol.io/) You can create your own online portfolio, showcasing your projects and skills in an effective way! Also, you can get inspired by browsing projects created by the community!

Still early days and I'm now working on a section to browse projects of other people and get inspired!

Please, let me know what you think! any feedback or improvement ideas are very welcome! :D Thanks so much, Pasquale",MachineLearning
qnhn4s,1636137542.0,[D] CVPR: Policy for posting on arXiv,"I am planning on submitting a paper to CVPR 2022, and I have some questions regarding the process; I am finding the information on the site a bit confusing.

What time-frame are we allowed to post preprint versions of our papers to arXiv? Are there certain considerations we need to know about? Thanks in advance!",MachineLearning
qnhf5f,1636136930.0,"[N] TF/Keras in Hugging Face, Datasets Edition","Hi all, Tensorflow maintainer at Hugging Face here! I [posted here a few months ago](https://www.reddit.com/r/MachineLearning/comments/ok81v4/n_tf_keras_and_transformers/) about the big change we were making to the library to make everything Keras-native, and people seemed to like it, so I thought I'd give another update on what's changed since then. We've made a couple of big changes that reduce the amount of duplicate, boilerplate code in common scripts massively, and we'd love to get people using the new approaches and get feedback!

**What happened in last week's episode of Hugging Face?**

The story up until now is that all our models are now Keras models. You can still write your own training loop or use the models as a layer in a larger model; everything like that remains unchanged, but it's incredibly convenient to just load a model, then just immediately `compile()` and `fit()` it. I gave some examples in the post I linked above.

**""Last week"" usually refers to times less than four months ago.**

You aren't telling me anything that isn't already in my performance reviews, don't worry.

**At least you delivered eventually. What's new?**

So the first big new change is a really nice integration with 🤗 Datasets. If you're unfamiliar, Datasets is the data equivalent to Hugging Face's model hub - you just load any uploaded dataset in one line of code the same way you load a pretrained model, with the `load_dataset()` function.

It's not just for NLP - people are using Transformers for audio and vision and everything else these days, so there's [all kinds of data](https://huggingface.co/datasets) in there. You should check it out!

To see an example of `load_dataset()` in action, a standard workflow with Datasets and Transformers goes something like this:

    from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
    from datasets import load_dataset

    # Load a pretrained model and its tokenizer
    model_name = 'bert-base-cased'
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = TFAutoModelForSequenceClassification.from_pretrained(model_name)

    # Load a dataset - we'll use the COLA dataset from the GLUE benchmark
    data = load_dataset(""glue"", ""cola"")

    # Define a function to tokenize the data, then apply it to the dataset.
    # The tokenizer returns a dict, and map() will add keys from that dict
    # to the dataset as columns
    def tokenize_function(dataset):
        return tokenizer(dataset['sentence'])

    tokenized_dataset = data.map(tokenize_function)

So far, so good, but this is the point where problems start to arise, because it's really hard to get the tokenized data into your model. The data is often 1) quite large and 2) jagged, because different samples will tokenize to arrays of different lengths. As a result, if you want to load the data as a single dict of `np.ndarray` or `tf.Tensor`, you end up having to do huge amounts of padding, which bloats memory usage and massively slows down the model. The way to get good performance is to load random batches of samples and only pad that batch, not the entire dataset, but doing that basically required you to write a custom training loop, or at the very least a Python generator, before it would work with Keras.

If anyone was using Transformers with TF before now I'd love to hear how you were solving this, because it was a huge recurring pain for me.

**So is there a solution now?**

There is!  The solution is that we added the method `to_tf_dataset()` to all our datasets. This basically wraps the dataset in a `tf.data.Dataset`, which will do the just-in-time data padding you want. We've also updated our `DataCollator` classes to work with this, so you can generate your dataset like so:

    from transformers import DataCollatorWithPadding

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=""tf"")

    tf_dataset = tokenized_dataset['train'].to_tf_dataset(
        columns=[""input_ids"", ""attention_mask"", ""labels""],
        batch_size=16,
        shuffle=True,
        collate_fn=data_collator
    )

Note how the data collator needs your model's `tokenizer` \- that's because every **god damned** research group in every **god damned** university in every **god damned** country handles their data in a slightly different way, and so there's no universal approach to padding that works for all of the hundreds of different models out there.  We do guarantee, though, that the `tokenizer` that comes with a given `model` will have a `pad()` method that works for that model, and that's what the `data_collator` will use. You have no idea how much pain you're being saved with that method.

**Okay calm down, what do I do with this tf\_dataset?**

That bit's easy! A lot of people aren't that familiar with `tf.data`, but it's actually really cool. Once you have a `tf.data.Dataset`, you can pass it straight to `model.fit()` or just iterate over it in a for loop to get batches.

**Won't I need to compile this model before I can fit() it? What loss should I use?**

That's a great question! And that brings me to the second big change we've made. Our models now **automatically compute losses that are suitable for their task in a way that's accessible to Keras.** In other words, if you use `TFAutoModelForSequenceClassification`, that model will now compute a loss appropriate for sequence classification tasks (i.e. crossentropy) for you! Don't know what loss you need to train GPT-2 with? No problem - `TFAutoModelForCausalLM.from_pretrained('gpt2')` will do it for you.

**Wait, stop! I'm an advanced user and I want my loss, not your loss!**

Don't panic! You can still use whatever loss you want, and all old code will work exactly as it did before. The only change is that if you `compile()` your model without a loss, it'll interpret that as you wanting the default internal loss. If you specify a loss argument to `compile()`, then it'll use that and not the internal loss. In addition, this only applies when using the Keras API, like `fit()`. If you're writing manual training loops or using the model as a layer in a larger model, none of this is relevant to you. This is just a convenience, and it's easy to disable!

**So I just... skip the loss argument?**

Exactly! If we continue on the code samples from above, all you need to do is:

    from tensorflow.keras.optimizers import Adam
    optimizer = Adam(3e-5)  # Transformers work much better with lower LRs

    model.compile(optimizer=optimizer)  # No loss argument!

    model.fit(tf_dataset)

And that's it! Dataset loaded, tokenized and trained on. With changes to a few lines almost any NLP task from translation to token classification or summarization can be handled in a similar way.

If you want to see more, we have [a bunch of example notebooks in both TensorFlow and PyTorch](https://huggingface.co/transformers/notebooks.html), and all the TF examples should be up-to-date with these new methods.",MachineLearning
qnh7w4,1636136360.0,[D] Why Jupyter notebook doesnt store requirements (require packages) in ipynb file?,The ipynb file is a JSON file. List with required packages can be easily added there. Why there is a separate file for this?,MachineLearning
qngw0u,1636135439.0,[D] Buying a PC for training - Does it make sense in 2021?,"I work with text data. I'd love to put \~$500 down to have a machine that can fine-tune the largest GPT-2 instance (the largest two are 774M, and 1.5B parameters). Does it still make sense to buy a computer to do it? I've spent a few hundred dollars on AWS credits, but knowing that I'm being throttled by the cost limits the experiments I can run.

I would love to hear from those who have bought machines or those who have decided against it. I would probably look for a used one on eBay, though I know very little about purchasing a PC.",MachineLearning
qnchpo,1636122952.0,"[R] Google & UC Berkeley’s Data-Driven Offline Optimization Approach Significantly Boosts Hardware Accelerator Performance, Reduces Simulation Time by More Than 90%","A research team from Google Research and UC Berkeley proposes PRIME, an offline data-driven approach that can architect hardware accelerators without any form of simulations. Compared to state-of-the-art simulation-driven methods, PRIME achieves impressive performance improvements of up to 1.54× while reducing the total required simulation time by up to 99 percent.

Here is a quick read: [Google & UC Berkeley’s Data-Driven Offline Optimization Approach Significantly Boosts Hardware Accelerator Performance, Reduces Simulation Time by More Than 90%.](https://syncedreview.com/2021/11/05/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-139/)

The paper *Data-Driven Offline Optimization for Architecting Hardware Accelerators* is on [arXiv](https://arxiv.org/abs/2110.11346).",MachineLearning
qnbrji,1636120818.0,[D] How does ACL rolling review work?,"Hi folks, I am going through the ACL rolling review process and have some doubts.

So, after submitting the paper at the open review website,

1) if accepted the reviews, comments, and pdf on open review will be there?

2) Same for rejected papers; if rejected, can I withdraw my article from open review, or it is going to be there along with rejected reviews on open review website?

My main concern is if rejected and submitted to another conference, there will be plagiarism because the paper will be already on the open review website along with reviews, then it will affect the other submission.",MachineLearning
qn8com,1636108867.0,[P] optimization of Hugging Face Transformer models to get Inference < 1 Millisecond Latency + deployment on production ready inference server,"Hi,

I just released a project showing how to optimize big NLP models and deploy them on Nvidia Triton inference server.

source code: [https://github.com/ELS-RD/triton\_transformers](https://github.com/ELS-RD/triton_transformers)

project description:  [https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c?source=friends\_link&sk=cd880e05c501c7880f2b9454830b8915](https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c?source=friends_link&sk=cd880e05c501c7880f2b9454830b8915)

Please note that it is for **real life large scale NLP model deployment**. It's only based on open source softwares. It's using tools not very often discussed in usual NLP tutorial.

Performance have been benchmarked and compared with recent Hugging Face Infinity inference server (commercial product @ 20K$ for a single model deployed on a single machine).

Our open source inference server with carefully optimized models get better latency times that the commercial product in both scenarios they have shown during the demo (GPU based).

Don't hesitate if you have any question...",MachineLearning
qn2jg5,1636083886.0,[D] What is the most effective way to mix scalar value(s) into CNN feature maps?,"It feels like an easy task but I can't seem to recall or find much info on this.

What I'm trying to do is use a scalar value and try to mix it into the intermediate feature maps of a CNN.

I know typically you might just concatenate these kind of scalars after flattening the feature map and before a FC layer, but I want this value to be combined into the intermediate feature maps between convolutions, and not at the end of the whole CNN encoder.

If it were categorical information, I know I could use learnable embeddings and add/concat, but in my case this is a continuous scalar.

I've seen suggestions to treat this scalar like a bias term and simply add, but this doesn't look strong and I'm not quite convinced. I've also thought about copying this value to a h x w x 1 array to concat before the next convolution, but I'm not sure about this either.

What is the most effective method to do this?",MachineLearning
qn1erq,1636080051.0,"[D] In theory, could we make a code that bridges different music genres?","I know little to no about coding, but I have been playing with VQGAN recently and this idea has popped into my head. In theory, could we write an AI code that gets feed stems (that is, each instrument track as a separate audio file) for two different songs and rebuilds the first song in the style of the second one?
The code would look for relevant information from the first song such as melodies, rhythms, structure and maybe even allow the user to feed in the lyrics to the vocals. Then, it would study how these elements are employed in the second song and rebuild the first song using the second song's style. This code could rebuild a rock song as a rap song, for instance.",MachineLearning
qn0ynq,1636078590.0,[D] FUZZY C-MEANS Clustering on line graph data,"Hi I've been trying to apply fuzzy c-means on data that can be represented as line graphs (hourly electrical load profiles). I understand that I must cluster the points on each hour. What I don't get is how do I relate the clustered points on each hour to the adjacent hour?  So that I can obtain the result which is a clustered line graph.

Here is an example of an input and the desired output.

[Input: Electrical Load Profiles](https://preview.redd.it/icxk687mqox71.png?width=941&format=png&auto=webp&s=179848d6fe69c4f353eb925fad079538f09f8b1f)

&#x200B;

[Output: Clustered Results](https://preview.redd.it/v4y7969oqox71.png?width=992&format=png&auto=webp&s=6589adfb44f023dd23f8be00a1d3947c57a79819)",MachineLearning
qmzy8a,1636075281.0,ruDALL-E model is open-source [P],"Sberbank submitted an open-source ruDALL-E model, inspired by OpenAI's DALL·E, for Russian.

The model with 1.3 billion parameters is available under Apache 2.0 license. The pipeline includes image generation, ranging results with ruCLIP and super-resolution.

The large model (12 billion parameters) will be available in the cloud. ruDALL-E is the biggest neural network project in the history of Russia, taking more than 20,000 GPU-days of Nvidia V100 to train it.

Github: [https://github.com/sberbank-ai/ru-dalle](https://github.com/sberbank-ai/ru-dalle)

Model: [https://huggingface.co/sberbank-ai/rudalle-Malevich](https://huggingface.co/sberbank-ai/rudalle-Malevich)

Demo (Russian): [https://rudalle.ru/](https://rudalle.ru/)

&#x200B;

Here are some pictures generated with it: (cherry-pick by authors)

[Avocado in the style of Malevich](https://preview.redd.it/uzwim0cshox71.jpg?width=1024&format=pjpg&auto=webp&s=014c5cfba1b905b07d83c82e5f449199382dad67)

[Cat looks at food](https://preview.redd.it/891829cshox71.jpg?width=1024&format=pjpg&auto=webp&s=ef530332306cf308821e15ae4f5e5b42ed43712d)

[Anime chan](https://preview.redd.it/w91ngfcshox71.jpg?width=1024&format=pjpg&auto=webp&s=c9266443ac9469ae80cf60e77c4ae302cee93c51)

[Trump hides the pain](https://preview.redd.it/zivbitcshox71.jpg?width=1024&format=pjpg&auto=webp&s=1d7b611ab31b6d6381c72ee0262f755df4812457)

[Mystery forest](https://preview.redd.it/nlm020cshox71.jpg?width=1024&format=pjpg&auto=webp&s=864caaa7fa49e32d0af1e62e11fc1454fe2e6147)

[Salvador Dali picture](https://preview.redd.it/pa9ag2cshox71.jpg?width=944&format=pjpg&auto=webp&s=9596c8691ef55ed1314281a05285c5feeb00b73a)

[Beautiful chan](https://preview.redd.it/olmhn8cshox71.jpg?width=1024&format=pjpg&auto=webp&s=0bc411b70a9ab0437d564dfd81034fd2d528a5f1)

[Pepe frog](https://preview.redd.it/g0nlqicshox71.jpg?width=1024&format=pjpg&auto=webp&s=02e56b776a338cbf78efce41432d559a3809fbf1)

[Grand Canyon](https://preview.redd.it/sgqn5ncshox71.jpg?width=1024&format=pjpg&auto=webp&s=ed9ddd11481f320ad2dd85da5d3135a47f0eeeda)",MachineLearning
qmy3ir,1636069609.0,[D] Why do we need the random noise z in conditional GANs?,"Obviously, we need some kind of input for the neural net. But in the case of conditional GANs, we have another kind of input. Does the random noise z then only serve to introduce variety for a given condition (e.g. many different faces all with blonde hair)? If I didn’t care about this variety, could I just do without the random noise? Or is there some other justification for why we need the random noise z (makes training easier, some theoretical reason, …)?",MachineLearning
qmxns9,1636068276.0,[D] SageMaker Linear Learner,"Hi Everyone,

I was just wondering.. does anyone know where there is a corresponding library in cran or scikit-learn for the linear learner in AWS sage maker?

I do not have access to AWS so I can't tell whether it is just a interface to different regressions  or something more sophisticated.

Enjoy your day,
Fella",MachineLearning
qmw9lr,1636064370.0,[P] Survey study examining practices in NLG evaluation,"**Do you work or do research on natural language generation (NLG)?** If yes, we are interested in your participation in a **20-minute survey** about practices when evaluating NLG systems or models.

The participants should have experience with working with or on (any type of) natural language generation (NLG) systems and tasks. The purpose of this research is to uncover unnamed practices and assumptions made during the evaluation of NLG systems, applications, and tasks. We hope that by understanding such practices and assumptions we will be able to better unpack the ways they could lead to **unintended consequences related to fairness and inclusion**.

**If you are interested, please fill out this form here**:  [https://forms.office.com/r/R9BL1szeei](https://forms.office.com/r/R9BL1szeei). Thank you so much for your consideration and help!",MachineLearning
qmw43e,1636063956.0,[D] Best approach for noisy language detection,"I need to predict the language (e.g. English, Portuguese, Russian) from a few words / sentences. This is noisy real-world data, meaning that the words might be misspelled, have poor grammar, emojis, language switching etc.

There's a bunch of different GitHub repos out there, but it's unclear which one works well, especially for noisy real-world data and not e.g. Wikipedia. I had hoped that websites like [https://paperswithcode.com/area/natural-language-processing](https://paperswithcode.com/area/natural-language-processing) would have solved this by now, but I don't see a relevant category for language detection. I don't have strong requirements for the solution to be particularly efficient although a transformer-approach seems overkill for this.

Any advice on a simple Python library to use?",MachineLearning
qmsyf8,1636055444.0,[R] Pruning for Self-Supervised Speech Recognition,"MIT News: [https://news.mit.edu/2021/speech-recognition-uncommon-languages-1104?fbclid=IwAR0X7bTI1BIT9Lpq07Kb6evvDhgBdoSPEcT4f9IzwOCR3-IgybNvdtMZblo](https://news.mit.edu/2021/speech-recognition-uncommon-languages-1104?fbclid=IwAR0X7bTI1BIT9Lpq07Kb6evvDhgBdoSPEcT4f9IzwOCR3-IgybNvdtMZblo)

Paper (NeurIPS 2021): [https://arxiv.org/abs/2106.05933](https://arxiv.org/abs/2106.05933)",MachineLearning
qmrr7t,1636052233.0,[D] Pairprogramming/ pair analysis in ML development teams to improve harmony?,"Hi.

I'm gonna assemble a small team to work on an ML project, which might lead to an ML first software, and I was wondering about whether using pair programming can be beneficial in ML teams, especially in early stages, which is adventurous analysis and exploration.  What is your experience with pair programming in ML and in what stages do you think it can be good? With what strategy and etc.

We are all working remotely, so that's another aspect that is interesting. What is your experience, and how do you create harmony in a newly assembled team that is working remotely?",MachineLearning
qmrglg,1636051415.0,"[N] Isomorphic Labs just unveiled today, a new Alphabet company led by DeepMind's Demis Hassabis. Plans to tackle drug discovery using AI.","Even as an insider, I found the idea of a DeepMind offshoot pretty surprising -- curious what you folks think about it. What are the odds it'll succeed? Will Alphafold++ even be useful for drug discovery?


Tweet unveiling the company: [https://twitter.com/demishassabis/status/1456283985554939907?s=20](https://twitter.com/demishassabis/status/1456283985554939907?s=20)


Website: [https://www.isomorphiclabs.com/blog](https://www.isomorphiclabs.com/blog)",MachineLearning
qmqhgg,1636048748.0,[P] StyleGAN3 + Wav2Lip,"&#x200B;

[due to the limit of my compute the quality suffers a bit.](https://reddit.com/link/qmqhgg/video/v5z7c4f9bmx71/player)",MachineLearning
qmq7hb,1636047990.0,[D] Is there any way for GAN to generate arbitrary length of time series signal?,"Hello, I'm working on using GAN to generate some signals. As I have viewed some related works, I found that most of them merely sample a latent vector from some distribution with fixed size *(e.g a latent vector of dim 64)*, and after some upsampling operation, they will get signals in a fixed window size *(e.g a signal of 4s \* 256Hz = 1024 points)*.

I want to get rid of the annoying limit of ""fixed window size"", and be able to generate continuous signal with arbitrary length. 

I tried to code by myself. I have designed a GAN framework , in which the generator takes an input of an arbitrary length of vector, and outputs a signal in same length as the input. The upsampling process is thrown away here, so the generator merely do some modification on the input signal instead of upsampling on it. As for discriminator, I use global average pooling in replace of the linear layers. However, my code failed to work. So I think maybe I need some new ideas.

I come to you guys for help. Do you know any paper that might be helpful for me? Or do you have any good idea?

Thanks!",MachineLearning
qmq67b,1636047892.0,[D] Feedback on our idea - a platform that turns code into a monetized API,"

Hi,

we're building a platform that turns code into an API , hosted by us. infrastructure is paid by whoever calls your service (we charge the exact amount the infra costs us) if you choose to you can also monetize your service.

as a data scientist myself I'm really excited about the possibilities of easily sharing my models' capabilities as an API without dealing with dev-ops , but I'm biased because I love what we're building. I really want to know what do you think about using such a platform (not mentioning the name, not sure it's allowed)

how many of you both create ML models and know how to set up your own API (and monetize it)?

we want smart people to create smart solutions and be able to share them easily with anyone.

creating such a service would be free, signing up is free and as a creator you can only earn if someone is using your service.

any thoughts?

Offer",MachineLearning
qmm9z7,1636037469.0,[R] Hierarchical Transformers Are More Efficient Language Models,"[https://arxiv.org/abs/2110.13711](https://arxiv.org/abs/2110.13711)

A team from Google, OpenAI, and University of Warsaw proposes a new Efficient Transformer architecture for language modeling, setting a new state-of-the-art on the imagenet32 for autoregressive models.",MachineLearning
qmm6uh,1636037227.0,[D] Ethical concerns for ML to predict race & gender,"I’m working on a data product that primarily uses image and name classifiers to identify race and gender. This means that someone who buys this product is now able to see race and gender data associated with people and/or companies in their database. The use case behind this is to report on and make decisions to improve diversity (i.e. an investment firm seeking to invest more in underrepresented groups, an HR company reporting on industry trends).

I’m looking for feedback on ethical/design/quality concerns in regards to some of the following factors:

-	We are primarily leveraging publicly available training data sets, models, and classifiers.
-	Gender classification includes only male/female options.
-	We use a publicly available photo for classifying each person.
-	None of the data we provide is self-reported, nor does the product communicate that to the customer.
-	We do not yet provide a confidence score or any feedback or correction feature.
-	Race and gender/sex are legally protected classes in some cases. While we are not using these to make any decisions in our product, we are the ones generating this data, which our customers will use at their discretion.

I’m worried we are not doing enough due diligence for the intentional choices we’re making and the unintended impact they may have. Any resources for designing fair systems, especially ones that attempt to generate rather than consume this type of data, would be appreciated.",MachineLearning
qmln6l,1636035633.0,[R] Washington U & Google Study Reveals How Attention Matrices Are Formed in Encoder-Decoder Architectures,"In the new paper Understanding How Encoder-Decoder Architectures Attend, researchers from the University of Washington, Google Blueshift Team and Google Brain Team propose a method for decomposing hidden states over a sequence into temporal- and input-driven components, revealing how attention matrices are formed in encoder-decoder networks.

Here is a quick read: [Washington U & Google Study Reveals How Attention Matrices Are Formed in Encoder-Decoder Architectures.](https://syncedreview.com/2021/11/04/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-138/)

The paper *Understanding How Encoder-Decoder Architectures Attend* is on [arXiv](https://arxiv.org/abs/2110.15253).",MachineLearning
qmj7tq,1636028027.0,[R] Local Latin Hypercube Refinement for Multi-objective Design Uncertainty Optimization,"Many real world systems consist of input features with aleatoric (i.e. irreducible) uncertainties. In engineering design applications, such uncertainties may arise from production tolerances, operational conditions as well as other environmental factors. Thus, the distribution of these features can be measured and to some extent modified (e.g. by moving the mean value).

Design uncertainty optimization seeks to find the distribution parameters of input features, which optimize metrics such as the failure probability and the variance of key performance indicators besides the expected objective values. Since these often require uncertainty quantification of black-box functions, the whole process is computationally quite burdensome. In this work, we propose using machine learning methods in combination with sequential sampling to reduce the required amount of computation and accelerate the uncertainty optimization task. Due to the small data setting, we limit the investigation to GPR and SVR but argue that a more suitable model can exist depending on the problem, which could be used within the proposed framework instead.

[https://arxiv.org/abs/2108.08890](https://arxiv.org/abs/2108.08890)",MachineLearning
qmhthd,1636022465.0,[D] What's the difference between top-tier papers and others?,"Hello, guys.

Although, I read a various of papers that in top conferences such as ICCV, ECCV, ICML.

I don't know what's difference in the first, second and other tier papers.

From the top paper, All I can know it's very obvious in writing skills and technologies that I don't understand yet.

&#x200B;

To be specific, What's different in the way that distinguish them?",MachineLearning
qm6l31,1635980120.0,[D] Paper Explained - EfficientZero: Mastering Atari Games with Limited Data (Full Video Analysis),"[https://youtu.be/NJCLUzkn-sA](https://youtu.be/NJCLUzkn-sA)

Reinforcement Learning methods are notoriously data-hungry. Notably, MuZero learns a latent world model just from scalar feedback of reward- and policy-predictions, and therefore relies on scale to perform well. However, most RL algorithms fail when presented with very little data. EfficientZero makes several improvements over MuZero that allows it to learn from astonishingly small amounts of data and outperform other methods by a large margin in the low-sample setting. This could be a staple algorithm for future RL research.

&#x200B;

OUTLINE:

0:00 - Intro & Outline

2:30 - MuZero Recap

10:50 - EfficientZero improvements

14:15 - Self-Supervised consistency loss

17:50 - End-to-end prediction of the value prefix

20:40 - Model-based off-policy correction

25:45 - Experimental Results & Conclusion

&#x200B;

Paper: [https://arxiv.org/abs/2111.00210](https://arxiv.org/abs/2111.00210)

Code: [https://github.com/YeWR/EfficientZero](https://github.com/YeWR/EfficientZero)

Note: code not there yet as of release of this video",MachineLearning
qm6ieq,1635979901.0,[Discussion] Applied machine learning implementation debate. Is OOP approach towards data preprocessing in python an overkill?,"***TL;DR***:

* I am trying to find ways to standardise the way we solve things in my Data Science team, setting common workflows and conventions
* To illustrate the case I expose a *probably-over-engineered* OOP solution for Preprocessing data.
* The OOP proposal is neither relevant nor important and I will be happy to do things differently (I actually apply a functional approach myself when working alone). The main interest here is to **trigger conversations towards** **proper project and software architecture, patterns and best practices among the Data Science community.**

# Context

I am working as a Data Scientist in a big company and I am trying as hard as I can to set some best practices and protocols to standardise the way we do things within my team, ergo, changing the extensively spread and overused Jupyter Notebook practices and start building a proper workflow and reusable set of tools.

In particular, the idea is to define a common way of doing things (workflow protocol) over 100s of projects/implementations, so anyone can jump in and understand whats going on, as the way of doing so has been enforced by process definition. As of today, every Data Scientist in the team follows a procedural approach of its own taste, making it sometimes cumbersome and non-obvious to understand what is going on. Also, often times it is not easily executable and hardly replicable.

I have seen among the community that this is a recurrent problem. eg:

* [https://www.reddit.com/r/MachineLearning/comments/q7ey06/d\_tired\_of\_writing\_mundane\_data\_wrangling\_code/](https://www.reddit.com/r/MachineLearning/comments/q7ey06/d_tired_of_writing_mundane_data_wrangling_code/)

In my own opinion, many Data Scientist  are really in the crossroad between Data Engineering, Machine Learning Engineering, Analytics and Software Development, knowing about all, but not necessarily mastering any.  Unless you have a CS background (I don't), we may understand very well ML concepts and algorithms, know inside-out **Scikit Learn** and **PyTorch,** but there is no doubt that we sometimes lack software development basics that really help when building something bigger.

I have been searching general applied machine learning best practices for a while now, and even if there are tons of resources for general architectures and design patterns in many other areas, I have not found a clear agreement for the case. The closest thing you can find is cookiecutters that just define a general project structure, not detailed implementation and intention.

# Example: Proposed solution for Preprocessing

For the sake of example, I would like to share a potential structured solution for **Processing**, as I believe it may well be 75% of the job. This case is for the general **Dask** or **Pandas** processing routine, not other huge big data pipes that may require other sort of solutions.

\*\**(if by any chance this ends up being something people are willing to debate and we can together find a common framework, I would be more than happy to share more examples for different processes)*

&#x200B;

>*Keep in mind that the proposal below could be perfectly solved with a functional approach as well. The idea here is to force a team to use the same* ***blueprint*** *over and over again and follow the same* ***structure and protocol***, even if by so the solution may be a bit over-engineered. The blocks are meant to be replicated many times and set a common agreement to always proceed the same way (forced by the abstract class).
>
>IMO the final abstraction seems to be clear and it makes easy to understand whats happening, in which order things are being processed, etc... The transformation itself (`main_pipe`) is also clear and shows the steps explicitly.

In a typical routine, there are 3 well defined steps:

* Read/parse data
* Transform data
* Export processed data

Basically, an ETL process. This could be solved in a functional way. You can even go the extra mile by following `pipes` chained methods (as brilliantly explained here [https://tomaugspurger.github.io/method-chaining](https://tomaugspurger.github.io/method-chaining))

It is clear the `pipes` approach follows the same *parse→transform→export* structure. This level of cohesion shows a common pattern that could be defined into an `abstract class`. This `class` defines the bare minimum requirements of a **pipe**, being of course always possible to extend the functionality of any instance if needed.

By defining the `Base class` as such, we explicitly force a cohesive way of defining `DataProcessPipe` (*pipe* naming convention may be substituted by *block* to avoid later confusion with **Scikit-learn** `Pipelines`). This base class contains `parse_data`, `export_data`, `main_pipe` and `process` methods

In short, **it defines a formal interface that describes what any process block/pipe implementation should do.**

A specific implementation of the former will then follow:


    from processing.base import DataProcessPipeBase

    class Pipe1(DataProcessPipeBase):

        name = 'Clean raw files 1'

        def __init__(self, import_path, export_path, params):
            self.import_path = import_path
            self.export_path = export_path
            self.params = params

        def parse_data(self) -> pd.DataFrame:
            df = pd.read_csv(self.import_path)
            return df

        def export_data(self, df: pd.DataFrame) -> None:
            df.to_csv(os.path.join(self.export_path, index=False)
            return None

        def main_pipe(self, df: pd.DataFrame) -> pd.DataFrame:
            return (df
                     .dropnan()
                     .reset_index(drop=True)
                     .pipe(extract_name, self.params['extract'])
                     .pipe(time_to_datetime, self.params['dt'])
                     .groupby('foo').sum()
                     .reset_index(drop=True))

        def process(self) -> None:
            df = self.parse_data()
            df = self.main_pipe(df)
            self.export_data(df)
            return None

With this approach:

* The ins and outs are clear (this could be one or many in both cases and specify imports, exports, even middle exports in the `main_pipe` method)
* The interface allows to use indistinctly **Pandas**, **Dask** or any other library of choice.
* If needed, further functionality beyond the `abstractmethods` defined can be implemented.

Note how parameters can be just passed from a **yaml** or **json** file.

For complete processing pipelines, it will be needed to implement as many DataProcessPipes required. This is also convenient, as they can easily be then executed as follows:

    from processing.pipes import Pipe1, Pipe2, Pipe3

    class DataProcessPipeExecutor:
        def __init__(self, sorted_pipes_dict):
            self.pipes = sorted_pipes_dict

        def execute(self):
            for _, pipe in pipes.items():
                pipe.process()

    if __name__ == '__main__':
        PARAMS = json.loads('parameters.json')
        pipes_dict = {
            'pipe1': Pipe1('input1.csv', 'output1.csv', PARAMS['pipe1'])
            'pipe2': Pipe2('output1.csv', 'output2.csv', PARAMS['pipe2'])
            'pipe3': Pipe3(['input3.csv', 'output2.csv'], 'clean1.csv', PARAMS['pipe3'])
        }
        executor = DataProcessPipeExecutor(pipes_dict)
        executor.execute()

## Conclusion

Even if this approach works for me, I would like this to be just an example that opens conversations towards proper project and software architecture, patterns and best practices among the Data Science community. I will be more than happy to flush this idea away if a better way can be proposed and its highly standardised and replicable.

If any, the main questions here would be:

* Does all this makes any sense whatsoever for this particular example/approach?
* Is there any place, resource, etc.. where I can have some guidance or where people are discussing this?

Thanks a lot in advance

\---------

PS: this first post was published on StackOverflow, but was erased cause -as you can see- it does not define a clear question based on facts, at least until the end. I would still love to see if anyone is interested and can share its views.",MachineLearning
qm5nfs,1635977358.0,"[R] Neural Program Generation Modulo Static Analysis, Mukherjee et al, 2021 NeurIPS Spotlight","Nice paper - using program analysis as a learning signal for program synthesis

(I am not the author - alas..)

[paper](https://www.cs.utexas.edu/~swarat/pubs/neurips21-nsg.pdf)

[twitter 1/10](https://twitter.com/swarat/status/1455587818160508933)",MachineLearning
qm5axp,1635976434.0,[D] AdaConv explained - Adaptive Convolutions for Structure-Aware Style Transfer (5-minute summary by Casual GAN Papers),"Classical style transfer is based on Adaptive Instance Normalization,  which is limited to transferring statistical attributes such as color distribution and textures while ignoring local geometric structures in the image. But that is the stuff of the past, let me introduce to you  Adaptive Convolutions, a drop-in replacement, for AdaIN, proposed by  Prashanth Chandran and the team at Disney research. AdaConv is able to transfer the structural styles along with colors and textures in real-time.

Full summary: [https://t.me/casual\_gan/165](https://t.me/casual_gan/165)

Blog post: [https://www.casualganpapers.com/style-conditioned-image-to-image-style-transfer/AdaConv-explained.html](https://www.casualganpapers.com/style-conditioned-image-to-image-style-transfer/AdaConv-explained.html)

[AdaConv](https://preview.redd.it/jdz6ruh1cgx71.png?width=2292&format=png&auto=webp&s=d9bffa23beee943beaa16781be3bcddbbefa9f79)

arxiv: [https://studios.disneyresearch.com/app/uploads/2021/04/Adaptive-Convolutions-for-Structure-Aware-Style-Transfer.pdf](https://studios.disneyresearch.com/app/uploads/2021/04/Adaptive-Convolutions-for-Structure-Aware-Style-Transfer.pdf)

code: [https://github.com/RElbers/ada-conv-pytorch](https://github.com/RElbers/ada-conv-pytorch)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",MachineLearning
qm578v,1635976141.0,[D] Python toolboxes for probabilistic graphical model inference,"Hi folks,

which libraries or toolboxes would you recommend (ideally based on personal experience) for performing inference in probabilistic graphical models, for an actual practical application and not for academic toy examples?

Minimal requirements: must be able to specify a Bayesian network or factor graph consisting of categorical nodes, some of which are hidden and others observed, and use a set of observations to identify the factors / dependencies. (I think essentially any PGM toolbox will fulfill these requirements?)

Bonus points given for:- good documentation- maturity/stability- works efficiently with many factors and many datapoints- implements many different inference methods and modeling paradigms- simplicity of use

I do know of a few promising toolboxes such as [pgmpy](https://github.com/pgmpy/pgmpy), [pymc3](https://docs.pymc.io/en/stable/), and [pyro](http://pyro.ai/), but have not used either of them (for this purpose) and am at a bit of a loss picking one to start with.",MachineLearning
qm2ov5,1635969222.0,[Project] Discover ongoing ML/AI competitions,"***Looking for feature suggestions for mlcontests.com!***

*It's been two years since I posted here about my then-new project* [ML Contests](https://mlcontests.com)*. It was well received and since it's been a while I thought I'd post an update and ask for feedback!*

[Main page \(mlcontests.com\)](https://preview.redd.it/tohjmaw91gx71.png?width=1565&format=png&auto=webp&s=5e96185b838570cac26ab9b245b5ca9d03a5cd1b)

The main page just lists ongoing competitions. There's also a newsletter where I occasionally send out updates about the competitive ML space, and a separate page which compares cloud GPUs for ML.

You can visit the site at [MLContests.com](https://mlcontests.com).

Traffic is steady, the newsletter is growing, there are no ads, and I'd like to figure out where to take it next. **I'd love to hear your thoughts on what you want from the site!**

PS: If you want to contribute, it's all open source: [https://github.com/mlcontests/mlcontests.github.io](https://github.com/mlcontests/mlcontests.github.io) :)",MachineLearning
qm0upg,1635964139.0,[N] Extended submission deadline — EvoMUSART 2022 conference,"Good news: **The submission deadline of EvoMUSART 2022 has been extended to November 24th!** 🙌

You still have time to submit your work to the 11th International Conference on Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART).

If you work with Artificial Intelligence techniques applied to visual art, music, sound synthesis, architecture, video, poetry, design or other creative tasks, don't miss the opportunity to submit your work to EvoMUSART.

EvoMUSART 2022 will be held in **Seville, Spain**, between 20 and 22 April 2022. 💃🇪🇸

For more information, visit the conference webpage: [evostar.org/2022/evomusart/](http://www.evostar.org/2022/evomusart/)

https://preview.redd.it/pue6n3usafx71.png?width=2083&format=png&auto=webp&s=1b2bfa549576b75720ced846ceab7954fa792787",MachineLearning
qm06ct,1635962267.0,[P] iris - Open Source Photos Platform powered by PyTorch,"This is my submission for PyTorch Annual Hackathon 2021! A self-hosted alternative to Google Photos. Currently it contains basic features built with short scope of hackathon. The team will be continuing to work by adding new features.

[Explore Section](https://preview.redd.it/cft3mb6o6fx71.png?width=2784&format=png&auto=webp&s=e204c8d6e20e98011f853938b5cf8ed02de61bc5)

[Smart Search](https://preview.redd.it/7w1xu76o6fx71.png?width=2784&format=png&auto=webp&s=b171397d4dcac4a9df88b8df6706e3560bdb9fd6)

Go check out and support the project now from below links!

YouTube: [https://www.youtube.com/watch?v=ZMG2rohochc](https://www.youtube.com/watch?v=ZMG2rohochc)

DevPost: [https://devpost.com/software/iris-7s3yna](https://devpost.com/software/iris-7s3yna)

GitHub: [https://github.com/prabhuomkar/iris](https://github.com/prabhuomkar/iris)

&#x200B;",MachineLearning
qlxl01,1635955196.0,[D] Zero-shot models as input features in NLP classification tasks?,"So places like huggingface offer zero shot models that pretty decently as a zero-shot classifier, and are easy to implement.  I was thinking, what if I just took a few zero shot models and added them as features and then fit a classifier on top?

 So let's say that I'm trying to classify toxic tweets. I might use an embedding model or a verctorizer to turn the tweet into model features. And then fit some classifier on top of this to try to predict the label from these text features.

But what if I add some simple zero shot binary features? is\_angry\_model1, is\_sad\_model1, is\_news model1 etc and do this for one or more models?  Or even more simply use, is\_toxic\_model1, is\_toxic\_model2 and use their predictions as input features into a classifier.

I was planning to experiment a bit with this, but wanted to get some thoughts on it and if there has been previous similar work I can use for reference?

Thanks!",MachineLearning
qlwcgx,1635951638.0,[R] Twitter Cortex Proposes LMSOC for Socially Sensitive Pretraining,"In the new paper LMSOC: An Approach for Socially Sensitive Pretraining, a Twitter Cortex research team proposes a simple but effective approach for learning both linguistically contextualized and socially sensitive representations in large-scale language models.

Here is a quick read: [Twitter Cortex Proposes LMSOC for Socially Sensitive Pretraining.](https://syncedreview.com/2021/11/03/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-137/)

The LMSOC code is available on the project’s [Github](https://github.com/twitter-research/lmsoc). The paper *LMSOC: An Approach for Socially Sensitive Pretraining* is on [arXiv](https://arxiv.org/abs/2110.10319).",MachineLearning
qlvjdy,1635949300.0,[D]Using Perplexity for Evaluating Language Models,"

Hello,

Assume a training dataset has ten sentences, and they are used for constructing a probabilistic language model consisting of the Maximum Likelihood Estimates for all of the different combinations of bigrams for these ten sentences.  To test the accuracy of this bigram language model, a test dataset consisting of twenty sentences are used for calculating the probability for each of these test sentences using the bigram conditional probabilities that were calculated using all of the sentences in the training set.  To evaluate the accuracy of this bigram language model, there is a need to calculate the Perplexity measure for indicating the accuracy of the probabilistic language model.  Because of the test dataset having twenty sentences, is it correct to calculate the Perplexity value for each of these twenty sentences using their probability values and then taking the average of these twenty Perplexity values for calculating the overall Perplexity value for the test dataset or is it correct to calculate the probability for each of the twenty sentences and then find the simple average of these twenty probability values to give a simple probability average which is then used for calculating the Perplexity value for the test dataset?  Would this procedure apply for a test dataset having any number of sentences?  Would this procedure apply for a trigram model?  Would this procedure apply for any n-gram models like a quadrogram model?  The goal is to always have a probabilistic language model having the lowest Perplexity value.

Thanks.",MachineLearning
qlujgx,1635946346.0,"[P] Loss function that penalizes classification errors heavily, or should I just modify log-loss?","I'm working on a classification problem where predictions that are ""good enough"" are.... actually good enough.  So I don't need my model to spend time optimizing a top-1 90% to become a 98% or a 99%.

However, I really care when the model makes incorrect decisions.  I've built a family of models of different sizes/parameter counts using traditional log-loss as my loss function, and each model will make catastrophically bad decisions on occasion.  And for my domain, a few catastrophically bad classifications can ruin tens of thousands of good classifications.  I'm wondering if there's a different loss function I can use that penalizes errors even more heavily than log-loss (log-loss squared?).

The natural question to ask is if I have errors in my data pipeline or model configuration.  I've debugged it quite a bit and I'm sure that's not my problem.  My domain (chess) naturally has a manifold that is so complex and twisted with many sharp edges and saddle points... It's a really difficult space to work in so any model smaller than GPT-3 is understandably going to have trouble.",MachineLearning
qlugqf,1635946116.0,[D] What is the state-of-the-art for few-shot text classification?,Say I have many text snippets that can be one of four classes. I also cannot get a large-scale labeled dataset (I have ~30-50 labeled examples per class). What methods are currently state-of-the-art for such settings?,MachineLearning
qlt4cz,1635941607.0,[D] AAAI 2022 Paper Reviews,"Now that AAAI 2022 reviews are out, I am creating a discussion thread for this year's reviews.",MachineLearning
qlsp3s,1635940037.0,[D] Which apps in the real world would you like to connect your ML models with?,"Hi all! As a side-project I am building a tool to **connect the ML models you make in your Jupyter Notebook to apps in the real world** with just a few lines of extra code.

As an example, think of building a model that predicts *Customer Churn* in a Jupyter Notebook. We make sure that it pulls new customers from **the Mailchimp account** of your company and **make it run as a Discord or Slack bot**! You can sign up for early access to the tool [here](https://flow.magicsheets.io).

I am building fast, and I wanted to ask **what apps** you think **would be awesome to integrate your Machine Learning models with**?

So far we have:

* Mailchimp
* Shopify
* Slack
* Google Sheets

Thanks in advance!",MachineLearning
qlrxzz,1635937066.0,"[D] ""Learn AI Together"" Discord community is looking for experienced people to share their projects and willing to help other AI enthusiasts by answering questions from time to time","Hey everyone! We are looking for researchers, teachers, teacher assistants, or professionals willing to help and exchange with people learning AI by answering questions from time to time.

We are an AI-focused community of over 20'000 people where members can chat, ask questions, share resources and projects, share/find job offers, etc. We are now focusing on getting experts or advanced members to join us and help us help others. Everyone else in the field is welcome to join as well!

More info about the community and to join us: [Learn AI Together](https://www.louisbouchard.ai/learn-ai-together/)

Excited to chat with you there!

*Note that this is not a paid opportunity and we won't be asking for hours or anything. Help whenever you can! :)*",MachineLearning
qlqqow,1635931558.0,"[N] Researchers From Seoul National University, NVIDIA and Microsoft Release ‘ACAV100M’: An Automatically Curated Video Dataset For Self-Supervised Audio-Visual Learning","Audio-visual (AV) learning is defined by delivering and applying instructional content that includes both sound and visual information. The natural relationship between visual observations and their accompanying sounds has shown strong self-supervision signals for learning video representations. That is why the massive amount of online videos has become a valuable source for self-supervised learning among research communities. 

However, due to overdubbed audio, online videos frequently provide imperfectly aligned audio-visual signals. Therefore, the models trained on uncurated films have been shown to develop poorer representations as a result of the misalignment difficulties. The existing techniques typically rely on manually curated datasets with a predetermined taxonomy of semantic ideas, where the audio-visual connection is highly likely.

To overcome this gap, researchers from Seoul National University, NVIDIA and Microsoft have released an automatic dataset curation pipeline and a large video dataset for self-supervised audio-visual learning, termed ACAV100M (automatically curated audio-visual dataset). The dataset is made up of a massive number of uncurated web videos. The researchers took 140 million full-length videos and reduced them to 100 million segments with the best audio-visual correspondence. 

Checkout the [Paper](https://arxiv.org/pdf/2101.10803.pdf), [Codes](https://github.com/sangho-vision/acav100m), [Project](https://acav100m.github.io/), [Microsoft Blog](https://www.microsoft.com/en-us/research/blog/acav100m-scaling-up-self-supervised-audio-visual-learning-with-automatically-curated-internet-videos/), [Video Presentation](https://www.youtube.com/watch?v=VR0eh0iVH8Q) and a [Short Read from Us](https://www.marktechpost.com/2021/11/03/researchers-from-seoul-national-university-nvidia-and-microsoft-release-acav100m-an-automatically-curated-video-dataset-for-self-supervised-audio-visual-learning/?_ga=2.130414219.529287311.1635906011-1849463555.1635487993).

&#x200B;

&#x200B;

https://preview.redd.it/p3klq1yvmcx71.png?width=1024&format=png&auto=webp&s=750c51e058da29510d7bd1dcc44df17a24df7404",MachineLearning
qlqls4,1635930857.0,"[D] Are GNNs/GCNs viable for graphs with no node features, with only the unique node IDs? Are they different from DeepWalk at that point?","I started to dig into GNNs for the first time and I have trouble understanding its advantages over NLP inspired embedding methods like DeepWalk and node2vec. Do GNNs only shine with node features? Or can they handle IDs/giant one-hot vectors as well? Does the usual input for GNNs only consist of a vector of handcrafted features? Are GNNs used directly for tasks like link prediction or they are just embedding generators for other models?

I appreciate all and any explanations.",MachineLearning
qlq3rt,1635928436.0,[D] Raspberry Pi and ML Frameworks - any benchmarks for training or inference?,"Hey folks!Do you know any benchmarks of available ML frameworks (TF, Pytorch, ...) for **training** (toy data data sets) **and inference** on a **Raspberry Pi** (or other edge devices)?",MachineLearning
qllc70,1635908768.0,[Research] Towards the Generalization of Contrastive Self-Supervised Learning,"Some interesting results of contrastive learning theory ([https://arxiv.org/abs/2111.00743](https://arxiv.org/abs/2111.00743)):

1. The generalization ability of contrastive self-supervised learning depends on 3 factors: strength of **data augmentations**, **alignment** of positive samples, and **divergence** of class centers.
2. Data augmentation enables self-supervised learning. Good algorithms (which optimize the alignment and divergence factors well) with weak data augmentation still have bad performance.
3. Barlow Twins, which aims to decorrelate the different vector components of the representation, implicitly optimizes the geometry of embedding space to satisfy the alignment and divergence factors actually.",MachineLearning
qliy7s,1635900997.0,[P] natural language only coding with co-pilot stream 11/2 10PM PST,"Kinda late to the party, but I just got access to github's copilot AI-backed code auto-complete tool, it can do some pretty impressive things. I have not played with it for more than an hour, and I'm pretty impressed.

At 10PM PST today, I will be streaming at [twitch.tv/evanthebouncy](https://twitch.tv/evanthebouncy) for about 1-2 hours, where I will be attempting to perform simple coding exercises by writing \_only comments\_ and letting co-pilot complete the code from my natural language inputs. It'll be fun if you can come and spam some ideas in case you haven't had a chance to play with it yet.

I will also be giving some commentary / reactions to it, as I work in program synthesis for a living, and this is a pretty cool piece of tech that will definitely change how people think about programming in the near future.

mod, if this is kinda spammy feel free to just delete the thread, idc.",MachineLearning
qlilnf,1635899869.0,[N] Zillow’s NN-based Zestimate Leads to Massive Losses in Home Flipping Business,"Zillow announced that they are [laying off a quarter of their workforce](https://www.cbsnews.com/news/zillow-layoffs-closing-zillow-offers-selling-homes/) due to a $420 million loss incurred by Zillow Offers, the home flipping arm of their business. The business model was reliant on [Zestimate](https://www.zillow.com/z/zestimate/), a neural network-based model that forecasts housing prices.

This seems like a colossal misstep on their part. It begs the question, how can other companies avoid a similar fate if they are making large gambles based on machine learning models predicting market movements? Additionally, how much should consumers rely on market predictions like Zestimate when making financial decisions (speaking as someone who recently bought a home and researched the market on Zillow during the process)?",MachineLearning
qlffsv,1635890550.0,[D] Neural architecture search and Neuroevolution,"Hi fellow readers,

I've come recently to a slight confusion about how to understand NAS and Neuroevolution. This is why I would like to hear your explanations.

My current understanding is as follows:

**NAS** \- technique which can be used to automate the process of designing/optimizing neural networks (NNs) \[[1](https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html#evaluation-strategy)\]. The technique is further divided into three components such as:

* Search space - defines types of layers, depth, type of connections
* Search strategy - defines the approach used to explore search space
* Evaluation strategy - evaluates the performance of build ANN from its design

**Neuroevolution** \- a technique that harnesses evolutionary algorithms (EA) to design/optimize NNs. It can augment the NN by changing the topology, connections and weights \[[2](http://www.scholarpedia.org/article/Neuroevolution)\] based on observed action in the environment.

Confusion:

Therefore, from the above description, I don't understand if NAS and Neuroevolution can be explained as applied techniques to design/optimize NN topologies. Where NAS can use any kind of algorithm to design NN topologies (RL, EA, Gradient descent) and Neuroevolution uses only EA?

**OR**

Neuroevolution can be just defined as a search strategy in NAS? This means Neuroevolution can be used in NAS (search strategy) like for example Reinforcement learning (RL) \[[3](https://arxiv.org/abs/1611.01578)\].

To simplify, I would like to understand, how can I think about NAS and Neuroevolution, when researching. My goal is to understand how I can put all the puzzles together when building an automated machine learning process for a specific task as anomaly detection.

If I made any mistakes or silly comparisons please point them out in the comments, that future readers can grasp my mistakes and your knowledge. Any comments are more than welcome!

Thank you in advance.",MachineLearning
qlcj9r,1635882524.0,[D] What's the best simple machine learning API service?,"I'm looking integrate machine learning into my application (for example, I want to classify images users are uploading to my site). I'm aware there's a lot of these SAAS machine learning companies but I was wondering if anyone here had recommendations as to which ones worked best for them? I basically just want to send all my data to a service, train a model, the be able to call an API to get an answer from the model.",MachineLearning
qlbye5,1635880933.0,[P] Text-to-image models ruDALL-E Kandinsky (XXL) (12 billion parameters) and ruDALL-E Malevich (XL) (1.3 billion parameters). A demo for the latter is available.,"[Technical report (Russian)](https://habr.com/ru/company/sberbank/blog/586926/).

[Technical report (translated to English by Google Translate)](https://habr-com.translate.goog/ru/company/sberbank/blog/586926/?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=nui).

[Demo for ruDALL-E Malevich (XL)](https://rudalle.ru/demo).

[GitHub repo for ruDALL-E Malevich (XL)](https://github.com/sberbank-ai/ru-dalle).

[More links from my other post](https://www.reddit.com/r/bigsleep/comments/ql9n81/comment/hj1d7zo/).",MachineLearning
ql9d4s,1635873997.0,[R] Neurips 2021 Accepted Paper List,"List of accepted papers now appears to be public: [https://neurips.cc/Conferences/2021/AcceptedPapersInitial](https://neurips.cc/Conferences/2021/AcceptedPapersInitial)


Spot any particularly interesting ones?",MachineLearning
ql5hdb,1635863442.0,[D] Why do we apply batch normalization between layers,"After batch normalization we are basically trying to get the unit  gaussian output. Initialising the data with unit gaussian seems to be a  good idea but doing so in between the network, how does that make sense?",MachineLearning
ql2qjz,1635854811.0,[P] Scientific Literature Review generation," Hello everyone,

I've developed an algorithm to automatically generate a literature review : [https://www.naimai.fr](https://www.naimai.fr/)
Hopefully that could be useful for the PhDs (and the non PhDs) !

For those curious to understand how it works : [https://yaassinekaddi.medium.com/scientific-literature-review-generation-386f36b05eae](https://yaassinekaddi.medium.com/scientific-literature-review-generation-386f36b05eae)

I'll be thankful if you have any remarks about that :)

Cheers,",MachineLearning
ql0a61,1635844540.0,[D] why isn't converting ML Models to plain code trivial?,"I've only done across one project, (m2cgen), for converting ml models to plain code.
Given that even complex models can be broken down to a series of nested functions, why is this not more commonly done?

Yes, training is very complex, but inference is just passing the input through, it's nothing dynamic. Sure, the performance will suffer, but for non-streaming applications it should be fine. Even a complex classification network isn't going to take long to run inference.

The Frameworks already parse the graph, or pipeline, or whatever it is they use to matrix multiplications, so why not export a plain code version. (I know it's not the same, but it's probably much easier for the framework Devs to implement this rather than someone external)

It'll take a bit of doing, but having completely portable computer code, with no hosts, model serialisation etc... Seems like a good thing?

As you might imagine, I'm thinking of how to make portable models for integrating with local software (a game engine), with as little hassle as possible.",MachineLearning
qkzi4q,1635840752.0,[R] Can ICRA reviewer see the names of author,"I submitted a paper to ICRA, but I forgot to put the names of the ourselves on the paper. Question: can the reviewer still see the author names through the system?",MachineLearning
qkyyno,1635838260.0,[D] Did anyone check ykilcher's video of Siraj Raval's interview,"I love Yannic's video but I did not see any point of this interview. I mean even in the interview Siraj seemed like someone who has just started learning machine learning, when he mentions about ""Superintelligence digital organism god"", seems like he imagines ML as a hollywood movie (much like the general person).",MachineLearning
qkyini,1635836173.0,[D] What do Machine Learning Engineers at Facebook do?,I was approached by an interviewer for this position and had a hard time grasping what they work on on a day-to-day basis. Thanks in advance!,MachineLearning
qkxatt,1635830970.0,[D] AAAI FastTrack 2021 Review Results,Good luck everyone! Results gonna be out soon for AAAI 2022!,MachineLearning
qkwk6j,1635828087.0,[D] how to correlate the results from an extremely imbalance data to performance relative to a random guess,"Hi all,

At a project at work I handle with an extremely imbalance dataset - 699 cases of positive outcome while around 15,000 negative cases.

In addition, our cases are relatively hard to separate and in some cases domain knowledge experts are struggling with manual classification.

In that context, I was asked to explore different classifiers and present them in a POC report.

At first I tried a naive approach and dumped the data as it is (used a train, validation, test splitting with stratification option on). All the models predicted 0 all the time, to maximize accuracy.
Then, I used over sampling with smote package in python, and changed the criteria to the area under the precision-recall curve. In the text set, I predicted correctly 30% (21/70) and I had a false positive rate of around 12%.
Regardless of possible model modification or boundary (predict 1 if the model predict a higher score than 0.65, for example), I am having some problems in defining my metric to evaluate the results. Our data is imbalanced, so 30% is good? In addition, false negative is strongly worse than false positive.

In addition, today I presented the results to my manager and he asked me to prove him (or argue) that the results are better than random guessing.

I thought about two things to evaluate my results:
- Randomly draw 1000 observations, with a prior of 5% equal to 1, the rest to 0. Than randomly guess 5% of them to be 1 and compare it to my results. Bootstrap this scheme to get a distribution, and check where is my model performance along the distribution.

-Take the examples from the test set, assign the same number or normal observations, and give the hr experts to classify them manually. Then compare results.

I will be glad to hear what do you think about the problem and the suggestions.

Thanks :)",MachineLearning
qkvy6l,1635825819.0,[R] Any movie dataset with movie summaries?,"Do you know of a dataset that contains movie summaries?

Do you know if researchers are legally allowed to download IMDB movie summaries for research purposes?",MachineLearning
qksrhl,1635815370.0,[D] What tools exist to determine the most useful type for perdiction?,"I've messed around with IBM's and Google's AutoML frameworks, I can't remember which output a report of which data was most helpful in predictions.

If I'm not using the correct terminology, I'm sorry. Basically, if I train an AI model on data such as Car Steering Angle, Gyro, Acceleramotor, speed, etc, and ground truth it to a more precise car steering angle, I want to figure out which of these data types were most useful for a good prediction.

That way I can feed in a whole lot of data, train the model, and know which data sources are irrelevant. What tools exist out there for this?",MachineLearning
qksq6q,1635815260.0,[D] How to generate images from text with CLIP + VQGAN (easy to follow 5-minute tutorial by Casual GAN Papers),"[Cartoon village in a mushroom valley trending on ArtStation](https://preview.redd.it/gmcb3yvf03x71.png?width=1024&format=png&auto=webp&s=f847b72dff915b0d30898bcefde7bb00efdb5bd5)

Hey everyone!

Have you been playing with GANs for a while and want to create something yourself? Do you want to try out those text-to-image google colabs for generative art you have seen on Twitter but are not sure where to get started? Then this tutorial is for you!

My name is Kirill, and I have been writing weekly ML paper summaries for almost 9 months over at [casualganpapers.com](https://casualganpapers.com), and while they are helpful to a lot of people already working in the generative modeling field, I realized the digests are not as interesting to those just starting their generative AI journey.

This is why I am starting a new series of posts focused on quickly getting you started in the world of generative art, 3D, AI-based image editing, and more.

Check out the first post on how to use the popular CLIP + VQGAN colabs to create beautiful generative art in just 5-10 minutes (excluding the training time):

[https://www.casualganpapers.com/howto-clip-vqgan-text-guided-image-generation-explained/VQGAN-CLIP-tutorial.html](https://www.casualganpapers.com/howto-clip-vqgan-text-guided-image-generation-explained/VQGAN-CLIP-tutorial.html)

If you enjoyed the tutorial make sure to follow Casual GAN Papers on telegram ([https://t.me/casual\_gan](https://t.me/casual_gan)) or Twitter ([https://twitter.com/KirillDemochkin](https://twitter.com/KirillDemochkin)) to get notified when the next post is released!

Take care,
Kirill",MachineLearning
qklvfp,1635795508.0,[D] Why hasn't BERT been scaled up/trained on a massive dataset like GPT3?,"Both architectures can be trained completely unsupervised, so why has GPT been scaled up and not BERT? Is it a software limitation?",MachineLearning
qkh9jg,1635782837.0,[P] Model Performance Monitoring in Production,"We've recently introduced model performance metrics in Graphsignal. Basically, by just logging a label and prediction the model-specific metrics are automatically computed, visualized and can be monitored. Graphsignal is currently SaaS, so a free account is necessary. No raw data is sent, only statistics.

More details in the blog post [Monitoring Model Performance in Production](https://graphsignal.com/blog/monitoring-model-performance-in-production/).

And the logger repo is [https://github.com/graphsignal/graphsignal](https://github.com/graphsignal/graphsignal).

I hope it can be useful for those who need to monitor models in production and do not want to build own pipelines for continuously computing accuracy and other metrics, implementing alerting, etc.",MachineLearning
qkfuzn,1635778867.0,[D] To phd or not to phd?,"I made the following post on other subs too. Just posting it here to get the input from larger machine learning community.

Hi all,

I recently completed my research based masters in computer vision and currently working in a company as a computer vision researcher. My current role requires a lot of paper reading to improve the existing models. I really like doing research and am satisfied with my current role. I have the following questions
1. If I decide to pursue a phd, I will not be able to save money for next 3 to 4 years. Which is better 4 years of PhD or 4 years of research job experience?
2. My long term goal is to get a job in big companies like google and Facebook. Most of the computer vision roles in big companies require a phd with multiple publications. Can i join such companies without a phd?
3. My company encourages publishing papers. Let’s say if I publish some papers in next three to four years, would that help me in competing with phd degree holders? Or I would still need an official degree?
4. How hard is to get admission in a good uni after some years of research experience with no publication record?

I would be thankful if someone could comment on my questions.",MachineLearning
qkfgxj,1635777741.0,[D] How does tensorflow perform on M1 Pro/Max?,"Basically the title. Apple claims that tensorflow is optimized/native for M1 chips, but how does it actually perform?",MachineLearning
qkf6mt,1635776907.0,[D] Has anyone else received an e-mail for the ICLR review?,I got an e-mail from openreview with a single review of the paper. I went to the openreview website to see that it was deleted. Anyone else with a similar experience?,MachineLearning
qkes8a,1635775707.0,[R] Top 7 Books to Boost Your Data Driven Outlook,"**In this post, I will cover the best 7 books for data analysts. These data analytics books will teach you about the power of big data and ways to harness it.**

I started my career as a Software Developer and switched to data science 8 years ago when big data software projects were difficult to predict and risky to conduct due to large volumes of unclassified data and many types of metrics. Using machine learning, data analysis, and visualization approaches was essential for facilitating informed decision-making throughout the software development and testing process.

Mastering data analysis was one of the most challenging experiences in my life. Wading through tons of books to figure out where to start and which methods and techniques to use in a particular case can be extremely daunting and time-consuming.

If you have been studying data analytics for some time, choosing the right educational resources is crucial to launching and advancing your career within this area.

# 1.  Storytelling with Data: A Data Visualization Guide for Business Professionals

As a data analyst, your aim is not just to retrieve data but also to make it intelligible, which requires you to be able to present the data in a certain way.  However, presenting data does not imply dragging and dropping data fields into a chart. It entails creating a meaningful visual representation of the data. This book is based on real-life scenarios and will give you some idea on the difference between colorful visualization and intelligent visualization, explaining why you should closely examine each line and color on your visual interface.

This book provides excellent guidance, examines criteria, and presents examples of how to properly deal with data.

# 2. Mastering Tableau 2021: Implement advanced business intelligence techniques and analytics with Tableau, 3rd Edition

As a business analytics practitioner, I search for publications that can simplify complicated topics in a manner that everyone can understand. The book contains several tips and techniques that will assist you in understanding when to utilize particular chart styles, at what data granularity, and with what sort of presentation for the end user. You will begin this fascinating trip by learning essential strategies for using sophisticated math to tackle challenging situations. These strategies involve the inventive use of several sorts of computations, such as row-level, aggregate-level, and others. Besides, you will get concise instructions on using Tableau to solve practically any data visualization problem by knowing the tool's (inner workings and thinking creatively about possibilities.) expanded capabilities,

After reading the book, you will be equipped with an arsenal of advanced chart types and methods that will allow you to display information to a range of audiences in an effective and engaging manner using clear, efficient, and engaging dashboards. Explanations and examples of effective and inefficient visualization approaches, well-planned and badly created dashboards, and compromise choices when Tableau users do not embrace data visualization, will expand your knowledge of Tableau, so that you get the most of this powerful tool.

# 3. Machine Learning with the Elastic Stack - Second Edition

This book is a one-of-a-kind resource for users using Elastic search. I With actual case, it focuses on the substantial growth of machine learning technology in Elastic search providing actual case studies and extensive explanation. This book is similar to having a one-on-one conversation with a subject matter expert. If you need to refresh your practical skills in machine learning, the book offers examples of how to apply Elastic ML in your environment, get valuable insight into your data, and how you can turn machine learning from static to intelligent. If you want to understand not just how to build tasks but also tap into the underlying models and variables, Machine Learning with the Elastic Stack is the ideal option for you.

# 4. Data Analytics Made Easy: Analyze and present data to make informed decisions without writing any code

With data literacy being such an important component of a data-driven mindset, this book is an excellent resource for data science students looking to obtain practical information and learn how to apply their analytical skills. The author does an excellent job of introducing readers to KNIME, a low-code data analytics framework that allows to instantly evaluate data. Furthermore, his presentation of machine learning is user-friendly, with an emphasis on theoretical knowledge and handling a variety of use cases. More significantly, De Mauro assists readers in comprehending the significance  of becoming a great data presenter, a vital talent to cultivate in order to influence decision-making.

# 5. Fundamentals of Machine Learning for Predictive Data Analytics, Second Edition: Algorithms, Worked Examples, and Case Studies

Fundamentals of Machine Learning for Predictive Data Analytics is a detailed analysis of the most important machine learning methods used in predictive data analytics, encompassing both theoretical principles and actual implementations. Technical and mathematical knowledge is complemented with instructional practical examples, and case studies show how these models may be employed in a wider business setting.

Following a description of the journey from extracting data to gaining insights and making a prediction, the book delves into the most essential machine learning techniques: data-based learning, correlation-based learning, probability and error-based learning. Each of these strategies starts with a no- tech description of the core principle, followed by quantitative models and algorithms demonstrated with extensive practical examples.

The authors discuss the procedures in a straightforward and succinct way, without referring to any specific programming frameworks or languages. They do a fantastic job of introducing the main concepts before diving deeper into the complexities of the logic and math underpinning the algorithms.

# 6.  Analytics Stories: Using Data to Make Good Things Happen

Analytics Stories: How to Make Good Things Happen is a serious, intelligent, and entertaining look at how analytics can tackle real-world problems and situations. Analytics Stories fills the gap between data analytics and the particular challenges it solves, with topics ranging from sports to finance, politics, healthcare, and commerce.

The author does an outstanding job of conveying the notion of data storytelling to the reader. He develops around 50 business cases on topics ranging from education to sports. Dr. Winston mostly utilized MS Excel to interpret, analyze, display, and successfully convey the data.

# 7.  Data Pipelines Pocket Reference: Moving and Processing Data for Analytics

A data science pipeline is a set of procedures that transform raw data into meaningful business responses. Data science pipelines streamline data validation, extract, transform, load machine learning and modeling, revision, so their implementation is crucial for data analytics success. The difference between having data and truly deriving value from it is moving data from various sources and processing it to create context.

This helpful reference describes common pipeline failures and key decision factors like batches vs. streaming data input and building vs. purchasing. This book delves into fundamental concepts that apply to open source systems, consumer applications, and homegrown solutions, as well as the most common decisions made by experts.

Data Pipelines Pocket Reference is a precious resource for all of the everyday problems and activities you are likely to encounter, if you work in data analysis or a related field that will assist you in making data-driven decisions for many years to come.

# Conclusion

Having a thorough grasp of data analytics and knowing how to gain actionable data-driven insights are essential for a successful career in data science. Anyone interested in expanding their knowledge of data analytics can benefit from the books mentioned in this article, since they provide the most recent industry information illustrated by examples of best practices.",MachineLearning
qkert8,1635775674.0,[D] Reusing parts of an open source code for a potential publication and a new open source code,"I am currently developing a new method that builds up upon an existing work in the literature in order to address the limitations and provide reasonable improvements to what has already been done. Earlier, I reached out to the authors for possible academic collaboration, but I have not received a reply from them. Their work has already been published as a conference paper two years ago, and their code is available on github, is regularly maintained and has also been deployed as pypi package that can be installed using \`pip\`.

My question is clearly about how to use certain parts of their work without plaigarising or breaking any copyright agreements? To what extent is it acceptable to rely on other people's work for producing a new method (especially when it is open source). Since the method I am working on is largely inspired from the existing method, it seems that I am currently on track to adopt around 30% of their code and follow their general code structure and OOP layout.",MachineLearning
qkef2i,1635774637.0,[D] Is there a good guide/roadmap on Deeplearning with large Datasets in Clouds?,"Is there a good guide/roadmap on Deeplearning with large Datasets in Clouds?

I have around 50-200GB of data in .npy format to feed into a tensorflow pipeline
Preprocessing itself takes a few hours too. (Or should i completely do that offline and change the pipeline structure?)",MachineLearning
qkdfwe,1635771542.0,TARS: Task-Aware Representation of Sentences for Generic Text Classification (Paper Summary) [D],"State-of-the-art approaches for text classification leverage a transformer architecture with a linear layer on top that outputs a class distribution for a given prediction problem.

While effective, this approach suffers from conceptual limitations that affect its utility in few-shot or zero-shot transfer learning scenarios 🔥

--------------------
This paper proposes a novel formulation of text classification that addresses these limitations.

https://youtu.be/XT6acdzVRHM

Paper: https://aclanthology.org/2020.coling-main.285/",MachineLearning
qkbfst,1635764232.0,[D] NLP model for chatbot for inference on 11 GB GPU?,"Hello everybody

I’ve just found the amazing Huggingface library. It is an awesome piece of work.

I would like to train a chatbot on some existing dataset or several datasets (e.g. the Pile). For training (or fine-tuning) the model I have no GPU memory limitations (48 GB GPU is available). For inference, I only have a GPU with 11 GB available. Inference should be feasible in real-time (i.e. below around 3 seconds) and the model should be adjustable, i.e. the source code should be available to change the structure of the model.

What model is best when taking into account these requirements? Probably one of the best models is GPT-J but I think for inference it needs more than 11 GB GPU.

Are the models in the Huggingface library fully customizable (i.e., layers etc.)?",MachineLearning
qkb6ga,1635763106.0,Plagiarism Case Detected @ ICLR 2022 [News][Discussion],"[https://openreview.net/forum?id=EO4VJGAllb&noteId=Ks7TmTUsyXa](https://openreview.net/forum?id=EO4VJGAllb&noteId=Ks7TmTUsyXa)

&#x200B;

https://preview.redd.it/zwpndspxpyw71.png?width=1153&format=png&auto=webp&s=d1105dfd3ae9da5fb56c1e0fc8b9652a6c9a1cef

The submission was withdrawn by the authors before the Program Chairs posted a desk-reject citing a serious case of plagiarism? What is happening?

The figures and tables do look like they've been lifted straight from previous papers.",MachineLearning
qka6p0,1635758557.0,[D] Does cuda latest version support all version of pytorch and tensorflow,"Greetings. sorry i could not think a better place to ask this question where i can get response regarding pytorch, tensorflow and cuda version.

I want to to know if i install cuda 11.5, will it support lower version tensorflow or torch packages such as tensorflow 2.4 or pytorch 1.71 which is supported by 11.0 cuda version. Actually i want to install both tensorflow and pytorch. The best option is to install cuda 11.0 or 10.1, but i want to know can i install latest version of cuda and whether will it support both frame works?",MachineLearning
qka32i,1635758044.0,[D] 2D models on 3D tasks (convolutions): simple replace?,"2D tasks enjoy a vast backing of successful models that can be reused. For convolutions, can one simply replace 2D ops by 3D counterparts and inherit their benefits? Any 'extra steps' to improve the transition? Not interested in unrolling the 3D input along channels.

Pubs/code help.",MachineLearning
qk9uet,1635756922.0,[D] What makes Multi Armed Bandit Problems contextual,"Hi everyone,

I dive straight into my problem. What makes a multi armed bandit problem contextual? I read on [TensorFlow agent tutorial](https://www.tensorflow.org/agents/tutorials/per_arm_bandits_tutorial#multi-armed_bandits_with_arm_features) that the agent receives the context vector, which is just the observation, at every step, that makes a bandit setting contextual. Isnt every agent in an bandit setting doing this? Since in the MAB problem the agent needs to know on which machine/bandit he is and how much he knows of the probability of the machine. So how does contextual MAB defer from the standard MAB ? Is it for example ""extra"" information ontop? For example he knows wether a machine/bandit has a higher probability if the wether is raining or not?

And the second part of my question is, I'm currently working with Stable Baselines 3. Is here the normal observation function the correlating observation function (context vector) from tf and using the observation in every step making it contextual? Couldnt find any information in the SB3 documentation how the contextual settings work.

To be more specific, my ""extra"" context in my MAB problem is a state machine the bandit uses and each state is an one armed bandit.

I hope this isnt a beginner question and I am tolerated here.",MachineLearning
qk90b9,1635752632.0,[Project] BERT Tokenizers NuGet Package for C#,"&#x200B;

https://preview.redd.it/bzj0uq6uuxw71.png?width=1200&format=png&auto=webp&s=e9ff2d6fd627478aac69482be9e154cc4096cfe9



Inspired by the challenges I faced with using BERT models with ML.NET, I have built a small open-source project and NuGet Package for easy tokenization in C# 🚀

With this package, you don't have to worry about different vocabularies and you can build input for BERT models quicker.

👉GitHub: [https://github.com/NMZivkovic/BertTokenizers](https://github.com/NMZivkovic/BertTokenizers)

👉Blog Post: [https://rubikscode.net/2021/11/01/bert-tokenizers-for-ml-net/](https://rubikscode.net/2021/11/01/bert-tokenizers-for-ml-net/)",MachineLearning
qk7tuv,1635747014.0,[D] Thoughts on pathways by Google Research,"I recently found out about this proposal called ""Pathways"" by Jeff Dean ([https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)). But the article seemed very obscure, there were just ideas and not a single hint of how that would be solved, whenever a question was posed the word ""Pathways"" was thrown at it. Is it another huge transformer from google :). Just wanted to know what everyone here thinks about it.",MachineLearning
qk79s9,1635744547.0,[R] Physics Informed Neural Network suggestion and recommendation,"Hi guys. I am learning about physics informed neural network, actual research focussing on Autoencoder. However, I just got in this new field and really want to get in-depth knowledge in this area. Would you recommend any related work or papers that I should read? Thanks a lot.",MachineLearning
qk6h7n,1635741241.0,[D] How does AI fit into the metaverse future?,"**What are popular applications and research topics of AI relevant for VR/AR?**

As an ML engineer, I am interested in learning more about ML topics that are / could be useful for building VR software and hardware. This is meant to be an open-ended question so all kinds of opinions and perspectives will be appreciated. Thanks!",MachineLearning
qk5avf,1635736845.0,[D] How is MLOps done in your current workplace?,"I joined a startup recently where the the necessary backend to support ML deployment is pretty much non-existent. All we have are some simple templates for CI/CD modified from those designed for generic microservices. Currently it takes data scientists at least 3-5 working days (post R&D) for to put a model into production as a prediction end point with logging and observability. This excludes setting up the necessary data pipelines between the models and other backend services. Whole process can take as long as 2 weeks.

My team and I are looking into setting up some framework and automation to cut the turn around time for putting models into production. Trying to establish some reasonable goals for this project and hope to get some insight from others have been through the same.

&#x200B;

* Which part of the production processes are automated by your MLOps teams and tools?
* How much effort do these tools help save and how much time does it currently take to put up a piece of R&D work into production?",MachineLearning
qk2bj6,1635726122.0,[R] current state of the art and novel research in support vector machines,Title pretty much says it already. I m interested in the current state of art of svms/svrs and on which topics researchers currently work on in that area ( i guess optimization still being a big one). Would also appreciate any paper links posted here on not so much known svm extensions etc. Go nuts ! ;),MachineLearning
qk144p,1635721988.0,[D] Anyone with powerful computers deploying locally?,"I have my own computer that has a pretty good cpu/gpu, I'd rather not spend more time to get a static ip to set up my computer as an official server through my ISP, or move my model and setup an expensive instance in the cloud. Is there an easier way to run an inference server on my machine that i am not aware of?",MachineLearning
qjxfu9,1635710405.0,[D] Machine Learning - WAYR (What Are You Reading) - Week 124,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|121-130|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)|[Week 121](https://reddit.com/pmzx3g)|||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)|[Week 122](https://reddit.com/pw14z5)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)|[Week 123](https://reddit.com/q5fi12)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)|[Week 117](https://reddit.com/omy345)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)|[Week 118](https://reddit.com/ovz52j)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)|[Week 119](https://reddit.com/p50knh)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)|[Week 120](https://reddit.com/pe2idh)||

Most upvoted papers two weeks ago:

/u/Icko_: [Patches Are All You Need?](https://papers.labml.ai/paper/dd638a442a9e11ec9e9dcba33be64600)

/u/CatalyzeX_code_bot: [Paper link](https://arxiv.org/abs/2012.09841)

Besides that, there are no rules, have fun.",MachineLearning
qjx4k3,1635709456.0,[R] ICCV2021 Oral -- Neural TMDlayer: Modeling Instantaneous flow of features via SDE Generators (with video explanation),"Our TMDlayer is inspired by stochastic differential equation (SDE) and aims to model the stochastic flow of features. In principle, it can be easily added on top of any DNN layer to bring the benefits. In addition, it immediately enables transductive inference once inserted into the model.

Welcome to check out our video for a quick and easy understanding.

Video: [https://www.youtube.com/watch?v=vR3nrYJqcgQ&t=11s](https://www.youtube.com/watch?v=vR3nrYJqcgQ&t=11s)

Paper: [https://openaccess.thecvf.com/content/ICCV2021/papers/Meng\_Neural\_TMDlayer\_Modeling\_Instantaneous\_Flow\_of\_Features\_via\_SDE\_Generators\_ICCV\_2021\_paper.pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_Neural_TMDlayer_Modeling_Instantaneous_Flow_of_Features_via_SDE_Generators_ICCV_2021_paper.pdf)

Code: [https://github.com/zihangm/neural-tmd-layer](https://github.com/zihangm/neural-tmd-layer)

&#x200B;

Our paper abstract: We study how stochastic differential equation (SDE) based ideas can inspire new modifications to existing algorithms for a set of problems in computer vision. Loosely speaking, our formulation is related to both explicit and implicit strategies for data augmentation and group equivariance, but is derived from new results in the SDE literature on estimating infinitesimal generators of a class of stochastic processes. If and when there is nominal agreement between the needs of an application/task and the inherent properties and behavior of the types of processes that we can efficiently handle, we obtain a very simple and efficient plug-in layer that can be incorporated within any existing network architecture, with minimal modification and only a few additional parameters. We show promising experiments on a number of vision tasks including few shot learning, point cloud transformers and deep variational segmentation obtaining efficiency or performance improvements.",MachineLearning
rxj5d6,1641488087.0,"Looking into the ""black box"" of a neural network","Hey guys! I've recently started working on a research project analyzing a cancer prediction algorithm and was hoping to get y'alls advice. The algorithm is described in [this](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7887928/#R28) paper, but effectively it uses a CNN on amino acid sequence data from T-cell receptors to determine whether they are responding to cancer or not. This algorithm performs remarkably well even when public T-cell receptors are removed, which indicates there's some biochemical difference between cancer and non-cancer t-cell receptors. My responsibility is to analyze the neural net and determine what specific features are heavily weighted in determining the difference between cancer and non-cancer t-cell receptors - hopefully this leads us to the specific biochemical difference. I'm a bit lost as to where to start with this, however - how would y'all go about looking into this ""black box"" ? Any advice would be much appreciated",LearnMachineLearning
rxijit,1641486454.0,When should we log transform our data?,"I was reading the book ""Hands-on machine learning..."" and it said that skewed data can pose a problem for some machine learning algorithms. When I looked up the internet, I found that we log transform our data if it's distribution is skewed. When is this crucial? Which algorithms need the data to be normally distributed?",LearnMachineLearning
rxhkxd,1641483937.0,Closing the data loop with DagsHub Annotations and Label Studio,"Hey r/learnmachinelearning, Nir from DagsHub here. I know working on data is less attractive than building a cool model. The reality though, is that many times improving your data leads to much better results. Maybe one of the reasons we don’t do it as much is that working on data complicates our workflow significantly.

DagsHub is the place to build data science projects. It’s like GitHub but for data, models, experiments, notebooks, and pipelines, not just code. We recently launched DagsHub Annotations, which lets you easily annotate data without needing to set anything up. Just push data, open a labeling workspace with a few clicks and annotate.

We constructed the Data Labeling Workflow to smoothly connect with your GitFlow, requiring ZERO adjustments. You can check out the tutorial I wrote, which is beginner-friendly, on how to get started with DagsHub Annotations (Spoiler Alert: >!You get to annotate Elon Musk 🤫!<)

Check it out here:

[https://dagshub.com/blog/closing-the-data-loop-with-dagshub-annotations-and-label-studio/](https://dagshub.com/blog/closing-the-data-loop-with-dagshub-annotations-and-label-studio/)

Would love to hear your thoughts.",LearnMachineLearning
rxh1kl,1641482513.0,"Which one to go for guys, I want to get into machine learning and time to get a system has came.","option a) base model 8 core cpu 7 core gpu macbook air m1 8/256

option b) Lenovo IdeaCentre G5 Gaming Desktop (AMD Ryzen 7 3700X/8GB/1TB HDD + 256GB SSD/Windows 10/NVIDIA RTX 2060 CD 6GB GDDR6 Graphics), Raven Black (90Q1004GIN)

comparing the base model of macbook because both are costing me same and also adding anything extra on it will escalate the price of macbook a lot, here in my country (India) at least.

yes yes doing a build on my own would cost me a lot, but gpu are very expensive and rare to find at the moment as we all know.

&#x200B;

thank you for reading.",LearnMachineLearning
rxgv6l,1641482056.0,Any resources to improve coding in ML/AI?,"I am looking to improve upon my coding skills, specifically in AI/ML. I would say I am relatively advanced in terms of ML knowledge, however, I struggle to implement papers/concepts on the fly from scratch which is mostly due to me being weak in coding but also due to some little knowledge gaps as well.

&#x200B;

I'm wondering if there are any good resources for tutorials on implementing research papers from scratch/cool projects? I've watched Aladdin Persson's videos and they're super good, I am looking for something similar.

&#x200B;

Thank you!",LearnMachineLearning
rxep30,1641475965.0,[Hardware] Good GPU on a bad computer : bad idea ?,"Heyo ! I'm bit short on the money side (especially now that buying a GPU costs a liver and a half), but I wanna upgrade my hardware, because I'm very limited in what I can do due to VRAM limitations and my graphics card being generally not great.

Thing is, I don't possess a good desktop PC, just a gaming laptop, and an old desktop one (\~10 years old), and buying the whole beast is a bit out of budget for me right now. How much of a bad idea is it to mount a recent good GPU on an old computer ? For video game purposes it would be plain stupid since there is a lot of interaction between GPU and CPU, but for ML, since everything is loaded up on GPU and left there to run, I thought maybe it would be ok ?

thx for the help, sorry if I posted on the wrong place !",LearnMachineLearning
rxbl9w,1641465535.0,Record Chess Moves with Webcam for Online Play," Hello! I would like to share with you my project which records moves made on your real life chess board using webcam. It also transmits the moves to any chess website for playing online.

[https://github.com/karayaman/Play-online-chess-with-real-chess-board](https://github.com/karayaman/Play-online-chess-with-real-chess-board)",LearnMachineLearning
rxbg0z,1641465001.0,Fun way of creating GPT-J chatbot,"Found a cool article explaining how to create a GPT-J chatbot in a couple of steps:

[https://medium.com/@tr\_18329/build-and-deploy-a-gpt-j-chatbot-6b917b25b1a7](https://medium.com/@tr_18329/build-and-deploy-a-gpt-j-chatbot-6b917b25b1a7)",LearnMachineLearning
rx9kzo,1641457564.0,How to incorporate normalization to inference?,"Dear Machine Learning Practitioners,

**Intro**

as mentioned above, I am currently training a neural network on a small 350 sample size dataset to perform regression. I would like to use this trained network in my webapi, which can be accessed by users with their own samples. However, the samples come in at a later stage.

**Question**

Now comes the question: I am using pytorch and the sklearn libraries, so my code to normalize the dataset looks similar to this:

&#x200B;


 `normalize=True`
 `if normalize:`
`sc= MinMaxScaler()`
`x_train = sc.fit_transform(x)`
`y_train = sc.fit_transform(y)`

&#x200B;

Now, how can I save this MinMaxScaler() Information and apply it to inference data at a later stage? What is the smoothest way of doing this?

Thanks for any pointers and your help!",LearnMachineLearning
rx8l60,1641453736.0,"how would i finetune gpt-neo to respond to a prompt, rather than just predicting what comes after?",specifically training it on titles of youtube videos and predicting the comments,LearnMachineLearning
rx732h,1641448322.0,How to come up with a novel NLP project?,"I'm a third-year undergrad starting a 10-week course in Deep Learning for NLP. We are expected to complete an NLP project for our final. I really want to do something novel, and improve upon state-of-the-art papers, if possible. I'm struggling on coming up with a novel idea/topic and would appreciate some pointers or ideas. I've looked through some projects from CS 224N and I'm really impressed as all of the topics are novel and actually beat some state-of-the-art results.

My areas of interest are TTS and contrastive learning.",LearnMachineLearning
rx3vgj,1641438702.0,MIT's opencourseware ML courses,"Anyone working through or have gone through MIT's opencourseware courses (Intro to machine Learning or Machine Learning), the latter of which is a graduate level course?

If so, how did you find the experiences?

I'm planning on using the knowledge to do research in Machine Learning.  So I'm only reading the handouts and listening to the videos, I'm not working through the hands on  stuff.",LearnMachineLearning
rwy8za,1641420862.0,Do I need study data structures and algorithms for machine learning?," many people online are emphasize so much on the relevance of the knowledge data structures and algorithm as a programmer. So it has left me wondering, if Its applies to one going into  machine learning. having spent the last 3 months learning python, I dunno If I should take a detour and learn it before going on to study machine learning maths?

Thanks in advance for your time",LearnMachineLearning
rwxuxp,1641419845.0,Converting Categorical Variable city to one-hot vectors is giving me NAN values,"I am currently working on a basic machine learning project which is revolved around predicting house prices given several different features about the house. The only categorical variable I am using in my linear model is the city the listing belongs to. But, when I convert the cities to one-hot-vectors, I keep getting NaN values and my linear regression model is throwing an error. Does anyone know what is happening? I have attached my code below. I have 12 numeric variables in my linear model and the categorical city variable. From the last screenshot, you can seethe NaN values occur at the third data point and for all of the columns that are a part of the one-hot-vector. Does anyone know what is happening?

&#x200B;

&#x200B;

Code of convert city to one-hot-vector

Method one: Using get\_dummies in pandas

`housing= pd.get_dummies(housing, drop_first=True)`

`housing.head().T`

&#x200B;

Method two: Using OneHotEncoder from sklearn

`# performing label encoder`

`lab_enc = preprocessing.LabelEncoder()`

`housing['city_encoded'] = lab_enc.fit_transform(`[`housing.city`](https://housing.city)`)`

&#x200B;

`# confirming label encoding`

`housing.city_encoded.value_counts()`

&#x200B;

`# dropping the city variable`

`housing.drop('city', axis=1, inplace=True)`

&#x200B;

`# one-hot-encoding`

`one_hot_encode = OneHotEncoder()`

`one_hot_encode_df = pd.DataFrame(one_hot_encode.fit_transform(housing[['city_encoded']]).toarray())`

&#x200B;

&#x200B;

`# merging one_hot_encode_df to the main housing dataframe`

`housing = housing.join(one_hot_encode_df)`

`housing.head()`

&#x200B;

`# dropping city_encoded variable`

`housing.drop('city_encoded', axis=1, inplace=True)`

&#x200B;

&#x200B;

NaN Values:

&#x200B;

https://preview.redd.it/bwzzb7a1yx981.png?width=1110&format=png&auto=webp&s=c00d44e5d82c9028a696943fdd42ef268176e0c3",LearnMachineLearning
rwuhlg,1641409768.0,How to specify which nodes in the feature map get applied with different filters/layers in Tensorflow,"For example, say I wanted to apply a 1D Convolution to FFT and Raw time series data in the first layer (say, the first 400 nodes as an example), but use a simple feed forward network to some 1D Statistical features on the remaining, say 20 nodes.

&#x200B;

I'm mostly used to just adding a layer which is able to interact with any node in the previous layer

&#x200B;

Any help is appreciated",LearnMachineLearning
rwrv2l,1641403007.0,Cannot install PyCaret :(,"Getting this error:

&#x200B;

`ERROR: Failed building wheel for scikit-learn`

`Failed to build scikit-learn`

`ERROR: Could not build wheels for scikit-learn, which is required to install pyproject.toml-based projects`

I have the python version of 3.9.5 and pip and pep are up to date the latest versions. Pretty much tried everything on stackoverflow but still facing this issue. Anybody who faced it would have an idea, I' d really appreciate your help.

Thank you",LearnMachineLearning
rwrhfh,1641402015.0,How to Learn Mechine Learning as a Beginner?,"Hi All,

I am an Android Developer and have a interest in Machine Learning and I am planning to start my learning in a proper manner. As far as I know, Machine Learning is Basically about Mathematics. After Googling for couple of Days I preapred a plan which I wanna share with you all and I request you all to please give your valuable suggestions/Tips/Comments.

1. Complete the **Mathematics for Machine Learning** ([https://www.coursera.org/specializations/mathematics-machine-learning](https://www.coursera.org/specializations/mathematics-machine-learning))
2. Complete the **6.041 Probabilistic Systems Analysis and Applied Probability** ([https://www.youtube.com/playlist?list=PLUl4u3cNGP61MdtwGTqZA0MreSaDybji8](https://www.youtube.com/playlist?list=PLUl4u3cNGP61MdtwGTqZA0MreSaDybji8))
3. Complete the **Stanford CS229: Machine Learning** ([https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU))

I already know python but have no hands on experience in machine learning related libarary. I have few doubts

1. Do I have to watch the whole lecture **6.041 Probabilistic Systems Analysis and Applied Probability?** I have watched it till Discrete Probability i.e. Till Lecture 7
2. From Where I should be learning Statistics ?

&#x200B;

I again request you all to please give your valuable suggestions/Tips/Comments.

&#x200B;

Thanks a lot for stopping by and taking the time to read.",LearnMachineLearning
rwr9kb,1641401440.0,Blog on Knowledge Distillation,"My first attempt at writing an ML blog. Any comments/suggestions are welcomed. :)

[https://pmgautam.com/knowledge-distillation/2021/10/03/Knowledge-Distillation.html](https://pmgautam.com/knowledge-distillation/2021/10/03/Knowledge-Distillation.html)",LearnMachineLearning
rwqq81,1641399995.0,Savitzky-Golay Filter for Data Denoising/smoothing.,"Most important part of any data related problems require you to preprocess the data first. One such step is data denoising/smoothing. Most people don't do this and many a times high spikes in the dat can overfit the model and give bizzare results.
one such underrated smoothing technique, that can be utilised is Savitzky-Golay Filter.

This low pass filter is mostly used in signal processing as a filter for signal fluctuations. This could very well be used in smoothing out the data as well. This filter tries to approximate the original function removing the useless fluctuations/ noise from the data which can very well misguide your model.

take a look at how it works, and it's python implementation here:


https://pub.towardsai.net/savitzky-golay-filter-for-data-smoothing-3b7c1c5e7f69


I Would love to hear some constructive criticism on my writing and subject. Thank you.",LearnMachineLearning
rwqgyl,1641399304.0,What Topic Would You Like to See Covered?,"Hey everyone,

I'm just doing a quick survey to see what areas are of most interest here. It can be an application or an architecture, a discussion of computational methods, theory-focused or code-focused, how to use a package, whatever!

Any feedback is welcome! The more specific the better, as well. Once I get a sense of the most popular topics, I'll run a poll!",LearnMachineLearning
rwp07l,1641395274.0,TFLite conversion from Keras model gives low accuracy,"Hi,  I was converting two Keras models to a full integer quantize TFLite model. One TFLite model gives a high accuracy (98%) and another one gives very low accuracy (0.4%). I have checked the quantized model and found that the scaling factor change significantly.

**Here is come quantization\_parameters for the model with 98% accuracy:**

Tensor index 1: 'quantization\_parameters': {'scales': 0.0077935, 'zero\_points': -1)

Tensor index 2: 'quantization\_parameters': {'scales': 0.02159962, 'zero\_points':  0)

Tensor index 3: 'quantization\_parameters': {'scales': 0.00016834, 'zero\_points':  0)

**Here is come quantization\_parameters for the model with 0.4% accuracy:**

Tensor index 1: 'quantization\_parameters': {'scales': 0.00770933, 'zero\_points': -3)

Tensor index 2: 'quantization\_parameters': {'scales': 0.00948102, 'zero\_points':  0)

Tensor index 3: 'quantization\_parameters': {'scales': 7.3092306e-05, 'zero\_points':  0)

as you can see the scales are changed significantly for the model with low accuracy. Is the scaling value makes that difference in the model accuracy?",LearnMachineLearning
rwo3ia,1641392683.0,Messing Up with CNN to Convert Grayscale to RGB,"Hello fellow learners, you might have seen old black and white photo being colorized but have you ever tried to do it? It can be done with a simple architecture of Auto encoder by tweaking colorspaces. You can read more about converting [grayscale image to RGB here](https://dataqoil.com/2021/03/11/converting-grayscale-image-to-rgb-using-cnn/).
Please leave some feedbacks to improve content quality.
Thank you.🙂",LearnMachineLearning
rwnzi9,1641392392.0,Intutive source for probability?,"I studied probability in the college but it was really bad course, i searched for some sources to learn from it and i found tens of sources, but it wasn't really intuitive, just apply the equation, any suggestions for courses that explain what is going on behind the scene?",LearnMachineLearning
rwmsd6,1641388901.0,"Bias in a Fully-Connected, Deep, Feed-Forward Neural Network","I'm coding up a neural network from scratch to increase my knowledge of neural networks and am having trouble with bias. Should each node have a bias value (number of biases=number of weights) or should there only be one bias value per layer?

Any insights would be much appreciated!",LearnMachineLearning
rwm2qi,1641386604.0,Please suggest some Resources to learn ML online preferably at no cost,,LearnMachineLearning
rwl4pe,1641383371.0,Can I learn ML after python basics?,"I am in grade 11(junior) and I have a pretty good understanding of some math concepts like functions and algebra and some basics trig(but Ive never done calc before), can I get started with learning ML or do I have to learn other things such as web development(flask and django) before going onto more advanced things? im new to programming and ive been exploring for quite some time about what to do after learning the basics of python, please help me out, thank you!",LearnMachineLearning
rwl2hh,1641383133.0,Style Transfer from multiple style sources?,"I'm planning for a simple web service that should get an image input and send it back in the style of a series of particular paintings i've got. I've been out of the loop of ML for a while, from a quick view I've seen that the Style Transfer seems to only work with one source and one target style, when I've tinkered with ML in the past I used larger datasets, and since I have a 40-ish image as style dataset I was wondering if there's some technique to employ them and not just one. Something not to hard that is already implemented in Python possibly",LearnMachineLearning
rwig2c,1641373012.0,How to implement minibatch SMOTE?,I have over 25000 images and 4 classes. One of the classes has a small number of samples so I thought of using SMOTE. I need to use mini batch approach since all the images cant be loaded into memory. pls help,LearnMachineLearning
rwgk3l,1641365365.0,Implementation of the U-NET Architecture in Keras,"Hello everyone, this tutorial explains the working of the U-NET architecture and explains its benefits. Further, we implement it in Keras and visualize the final structure using one of Keras's built-in functions.

[U-Net Implementation is Keras](https://writersbyte.com/featured-post/u-net-architecture-explained-and-implemented-in-keras/?swcfpc=1)",LearnMachineLearning
rwenkn,1641358767.0,The ML Dojo -- get daily updates on the latest in the field of Artificial Intelligence 🧠 and Machine Learning 🤖,"[Link to The ML Dojo](https://mohitmayank.com/themldojo/)

Let’s start with a question - How do you keep yourself updated with the latest happenings in AI or ML? (really think about it for a second before moving forward)

I have asked the same question to a lot of AI/ML practitioners, and a quick summarization of their responses includes — following top scientists or researchers or AI labs on Twitter, finding good articles on Medium, or recent papers on Arxiv, or posts in Reddit, to even scanning through interesting questions on stack exchange! (not a complete list by any means 😊).

Performing one or two of the above practices can be possible, but are you sure you are not missing something important by skipping the rest? But the flip side is more daunting — are you okay spending hours daily just browsing through multiple platforms, hoping to stumble upon something interesting? If your answer is NO, The ML Dojo is here to help 🖖.

With this context, I am ready to answer some of your burning questions,

**What is ML Dojo?**

ML Dojo is a daily report on the latest and upcoming research, experiments, topics, articles, papers, … (you name it), in the field of Artificial Intelligence and Machine learning.

**Why ML Dojo?**

There are two main reasons to subscribe,

1. We publish daily! Yes, no more waiting for the weekly or monthly newsletters.
2. We cover a wide variety of platforms! To make sure you don’t miss anything important, we keep an eye out for the latest feed on Twitter, Reddit, Stack Exchange, Medium, and more coming soon!

In short, you get the best of all of the worlds and that too daily! 🌎

See you guys there 😀",LearnMachineLearning
rweffw,1641358046.0,Finding good dataset for diagnosing crop disease project,"

Hi,

I have decided to work on a machine learning project. I want to build an app that can be used to diagnose crop diseases. You would simply snap a picture of the crop and it would use machine learning to predict the disease associated with the picture. The most critical step in this project is finding a good dataset. Could anyone point me in the direction of any website that would have a dataset that I can use for this project?",LearnMachineLearning
rwcehg,1641351845.0,Best way to fuse metadata into a CNN,"I've been working on a GAN CNN that's producing good results apart from in one area.

The network takes a photograph as input and produces a stylised output.

As this is a supervised training model, there is a ground truth the network is optimising itself toward.

The ground truth has different lighting to the input image, and I need some way of informing the network of this.

So far I have tried two approaches.

The first is to write a column of pixels into the input image where the column's colour contains the needed lighting data.

The drawback to this approach is it causes artifacting in the output and the network stumbles around for a long time before learning the colour coding.

The second is to concatenate the input tensor just before the forward passes with an additional channel whose brightness contains the needed lighting data.

This might be a more appropriate solution although I'm concerned it's a waste of convolution filters.

I would appreciate any insight that can be provided, particularly on best practices. Thanks.",LearnMachineLearning
rwb444,1641348147.0,Should I study proofs for the math I'm learning?,"Hello everyone,

Right now I'm studying from an Intro to Probability textbook (first undergrad probability course) and eventually I want to learn the math needed for machine learning.

So should I bother spending time on the derivations of theorems and all?

Like I know it is important to know how machine learning algorithms are formed under the hood, but what I'm talking about is should I bother to do stuff like: ""derive through induction the inclusion-exclusion formula for n events""

&#x200B;

Thank you!",LearnMachineLearning
rw7go4,1641337639.0,Iterating model.fit,"Fast forward to a model you think is worth using.  There’s always a little bit of randomness involved in the training of a model right?  You’ve tuned on the validation set and you’re ready to try the model on the test set.

Is it bad practice to iterate the fit process like 1000 times, store the model with the least loss, and then deploy that against the test set?  Or is this just asking for overfitting to corrupt your model? (Assume the time required to perform the of iteration of n-loops of the training process is reasonable).",LearnMachineLearning
rw5sqt,1641333161.0,Question about Kernel in SVM,"I was just wondering, what would you say is the most important hyperparameter that comes naturally with picking any kernel?

I'm sorry if the question seems ambiguous I'm just repeating word for word what another person asked me and I'm not sure what to answer",LearnMachineLearning
rw401v,1641328442.0,How do I install Caffe framework on Mac M1?,"I'm working on a Deep Learning system in C++ using Caffe. I installed all the dependencies from brew and installed caffe, but when I compile with make, I get the following

    ld: can't map file, errno=22 file '/usr/local/lib/caffe' for architecture arm64 clang: error: linker command failed with exit code 1 (use -v to see invocation) make: *** [build] Error 1

I'm not sure if I have the wrong build or if it is just incompatible with Mac M1. Any help is appreciated, thanks!",LearnMachineLearning
rw3nba,1641327503.0,ML at home?,"Hi dudes, I would like to know if you do ML at home? What do you do if the answer is yes? Do you compute in the cloud or on your pc?",LearnMachineLearning
rvzum7,1641317596.0,How much data do I need to collect before I can create a reasonably accurate model? Does it really need to be millions of lines?,"Hello,

I'm working on a project to take inputs from a number of biometric sensors such as heart rate, galvanic skin response, and EMG in order to try and detect when a user is close to peak sexual arousal/orgasm.

I'm not even convinced that I actually need ML for this, but it's a good exercise in learning and it's fun as well, so I'm going to give it a go.

Unsurprisingly I'm struggling to find an existing model for this kind of data (what a prudish society we live in, eh?!), so I'm going to have to create my own model.

My thoughts are to bring the user to orgasm via various means and record all of the metrics every 5 seconds as I go, with a boolean for whether the orgasm was reached or not.  This data will be recorded into a CSV file, and from there I \*think\* that I should be able to create a model I can then use with Python and Pandas.

The problem as I see it is that even if I manage to keep the stimulation before orgasm going for 30 minutes, that's still only 360 data points, and only one of those data points will have \`True\` in the orgasm column, so I'll probably need to repeat the process dozens of times before I get anything significant as far as ""this data means you're close to orgasm"" is concerned.

I'd love to know what folks on here recommend around how to approach this - should I just go back to using a load of nested if statements? Is there a way for me to get the ML to ""improve"" on the detection side of things each time it runs based on the data as it flows in? Do I need a dataset of thousands of rows before I can create my model? Is this a project that's simply too big for a beginner to bite off?

I've got a background in hardware and software engineering, so I'm not too worried about learning how to write the code itself, it's the generation of models that I'm struggling with!

Thanks in advance!",LearnMachineLearning
rvy2u8,1641312994.0,"I made a tutorial on Variational Autoencoders, with an intuitive explanation and code!"," Hey everyone!

Variational Autoencoders are a really awesome tool for data generation, but I've found that there's a lack of resources with simple explanations for how they work.

I made a guide on them which you can find [here](https://www.assemblyai.com/blog/variational-autoencoders-for-dummies/). Without going into heavy mathematics, the guide goes through

1. A high level introduction (link to section [here](https://www.assemblyai.com/blog/variational-autoencoders-for-dummies/#introduction))
2. An ordinary autoencoder recap (link to section [here](https://www.assemblyai.com/blog/variational-autoencoders-for-dummies/#what-is-an-autoencoder))
3. An overview of variational autoencoders (link to section [here](https://www.assemblyai.com/blog/variational-autoencoders-for-dummies/#what-is-a-variational-autoencoder))
4. Intuitively understanding how variational autoencoders learn (link to section [here](https://www.assemblyai.com/blog/variational-autoencoders-for-dummies/#understanding-variational-autoencoders-with-mnist))
5. Building a variational autoencoder for the Fashion MNIST dataset (link to section [here](https://www.assemblyai.com/blog/variational-autoencoders-for-dummies/#building-a-variational-autoencoder))

At the end, you'll be able to create a GIF like the one below, where we can see the VAE learning to associate different regions of the plane with different types of clothing!

Have you used VAEs before? How do you think VAEs compare to GANs? What applications of generative models are most interesting to you today? I'd love to hear your thoughts and feedback!

**Also, please let me know if you'd be interested in a follow-up post that goes into the mathematics behind VAEs!**

https://i.redd.it/1hp44oig4p981.gif",LearnMachineLearning
rvy222,1641312935.0,Free No code AI to learn on,"Does anyone here have experience using no code AI models that are actually useful and versatile? And cheap or free... I want to learn AI without coding. I don't plan on being an AI engineer. I just want to use it in everyday life. I won't sit down and learn to code python, and then learn AI theory, and then learn statistics, and then experiment with all the tools, and then learn experiment with all the algorithms. It ain't gonna happen charlie.",LearnMachineLearning
rvw32g,1641307811.0,Beginner learning path question,"I'm a Python and C# developer and I'm starting to learn ML. I have a question regarding which path will be the most convenient for my porpuses. I have a project in mind which is the automatic placement of labels in CAD, following certain rules (see image). This is a very time consuming task, specially when we have thousands of objects.

[Example Image](https://i.imgur.com/n6xewYi.png)

I have finished the NG course but now I'm confused what should I learn next.

Should I learn ML and then Reinforced Learning? Can I start directly with Reinforced Learning?

I do not want to skip any stage, but I would like to learn as I progress with my project.

&#x200B;

Thanks!",LearnMachineLearning
rvv6f8,1641305225.0,proposal for simple batch scheduling of local GPU's,"Given a list of, say, predict jobs, how best to keep >1 GPU's busy with one cmd at a time per GPU? I want a simple solution for one workstation for now, so am skipping PBS and Slurm-like solutions.

Here's my idea for a lightweight cmdline interface. Update: seems to work fine.

    $ ./gpu_q.py batch_file

Where batch_file looks like:

    predict.py params1..
    predict.py params2..

The meat of it,

    import threading, queue

    q = queue.SimpleQueue()

    with open(CMDS, 'r') as f:
        for cmd in f:
            q.put(cmd)

    import subprocess

    def worker(gpu):
        while True:
            try:
                item = q.get(block=True, timeout=10)
            except Empty:
                break

            item = item.replace('predict.py', 'predict.py -gpu ' + str(gpu))
            subprocess.call(item, shell=True)

    for i in range(N_GPU):
        threading.Thread(target=worker, args=(i,)).start()",LearnMachineLearning
rvv2j4,1641304907.0,Moving from Keras (Python) to mlpacl (C++),"Hey guys, I have been doing my first steps with Keras and implemented a NN. For a new usecase, I now want to translate it into mlpack (C++). While the definitions seem quitedifferent to me, I am struggling with this. Maybe someone could answer me the following questions:

1.) When I use the FFN class, is ""model.add<Linear<>> "" equivalent to Keras' dense layer, i.e. fully connected? 2.) Is it viable to replace Dense(M, activation='relu') with model.Add<ReLULayer<> >(M);? 3.) Is there an equivalent to a Lambda Layer?

Thanks in advance guys :)",LearnMachineLearning
rvui3a,1641303222.0,Subreddit for Algotraders with ML methods in use.,"Hello guys, i'm myself a ML Engineer and trying to use ML on trading. Even when a lot of people say it's not possible, i know people who are doing it with high profits.

Since this subreddit includes a lot of topics, i just created a subreddit for only algotraders who are using or learning to use ML on stock trading.

Here is the subreddit: [https://www.reddit.com/r/mltraders/](https://www.reddit.com/r/mltraders/)",LearnMachineLearning
rvtxxh,1641301506.0,Need help with first job as an ML engineer,"Hello everyone,
I recently graduated with a master's degree in CS and I got my first job last month.
As soon as I started I realised that, even though I was hired as a backend engineer, my tasks would be more akin to those of a machine learning engineer.
The company is an AD company and my task is to predict the revenue of an AD, using data from the last 4 months, from September to December of last year.
I took the data, built a feature vector with it, tried basically all the regression models available on scikit-learn, and got a R2 score of 0.7, using 80% of the data for the training set, and 20% for the test set.
The problem is that while the model works really well with instances from those 4 months, when I try to predict the revenue of an AD from January, it fails miserably.
My question is: how do I deal with this? Does this mean that my model is overfitting? Or is it just because it has never seen an example from that time period?
Thank you for all your help.",LearnMachineLearning
rvt8ik,1641299144.0,Can I download a 20GB+ dataset and train my TensorFlow model in Google Colaboratory?,"Hi guys, I developed my TensorFlow model in Google Co-lab and I used a mini-dataset to train the model but now I want to train my model with a real dataset which would be around 20GB+ of capacity. I just wonder,

1. How can I train the model with this much of a high volume dataset?
2. It is possible to train my model with Google Colab?
3. Can I download the dataset programmatically to  Google Colab and go ahead and start the training process?


FYI,

* Here is how I downloaded the mini dataset into Google Colab and proceed to the next steps. Can I download a large dataset just like this?
[https://www.tensorflow.org/tutorials/audio/simple\_audio#import\_the\_mini\_speech\_commands\_dataset](https://www.tensorflow.org/tutorials/audio/simple_audio#import_the_mini_speech_commands_dataset)
* Large Dataset: [https://commonvoice.mozilla.org/en/datasets](https://commonvoice.mozilla.org/en/datasets)",LearnMachineLearning
rvr3lk,1641291308.0,what does this funtion do np.c_ ?," i am confused as

 np**.**c\_**\[**np**.**array**(\[1,2,3\]),** np**.**array**(\[4,5,6\])\]**

and

 np**.**c\_**\[**np**.**array**(\[\[1,2,3\]\]),** **0,** **0,** np**.**array**(\[\[4,5,6\]\])\]**

gives different results",LearnMachineLearning
rvqyji,1641290735.0,Some of the classifiers are greyed out in Weka,"I have some data that I am trying to classify using the Weka environment, but the classifiers I'm interested in are greyed out. I know this means that the dataset I am using isn't compatible with Weka, so I am trying to find out why this is the case.

When I try the dataset with just the first 200 instances, the algorithms show up, but at some point between 200 instances and 2k instances the algorithm stops showing up.

[Here is a link to a sample of the data in ARFF format](https://i.stack.imgur.com/0P8os.png). Can anyone help me to spot what's wrong?",LearnMachineLearning
rvp1xq,1641282764.0,"What actually is the difference between DS , ML and Deep learning","I know python , Tensorflow , libraries such as numpy , pandas , sklearn , matplotlib . I have made projects using sklearn models , csv datasets and deployed them in flask backend. I have created CNN models using keras api , made neural networks using keras layers like conv layer , dense layer , maxpool layer , activation functions . I have created GAN model using tensorflow and keras . I have completed andrew ng course on deep learning .

My question is knowing these things which category do I fall into :

Beginner/Intermediate in Data Science ...or

Beginner/Intermediate in Machine Learning ...or

Beginner/Intermediate in Deep learning ...or

**SOMETHING ELSE**",LearnMachineLearning
rvo19z,1641278799.0,Where do I start for generating new images?,"I’m reading about GAN, DCGAN, MSGGAN, BMSGGAN and my head is spinning.

I have a set of 500-1000 black and white photographs I’ve taken, and I want to use it as a dataset. I just want the output to be similar looking but entirely new images. It’s only an art project, and I have low-level coding abilities.

It seems like image size is a significant constraint, and I’m fine to begin by resizing my data set and then supersampling later. They are still rectangular, though, which seemed to be a problem with some I was looking at?

So where do I start? What’s the right tool for this?

I’d appreciate any help and guidance!

Edit: I don’t want to use anything",LearnMachineLearning
rvl86n,1641269471.0,Need help/assistance in completing a project.,"I decided to create an application, a web application actually, where Spotify users can kind of match with their fellow friends/mates or anyone else. The matching is similar to that of Spotify Blend. The Spotify API gives us the ability to gather data about a user’s listening history etc, and key data about the songs they listen to. This data might be able to help create an algorithm for this match but I really know nothing about ML but to drive this project to completion, I wish I had someone who had some ML experience/ open to learn to help drive this project to the end. If interested, you could reply to this post. If you can’t help but can direct me in the right path with respect to things I could learn to complete this task, you could share. Thanks:",LearnMachineLearning
rvg08w,1641254551.0,"Bootcamp Graduation Project, generating new cooking recipes from a list of ingredients. Which models could fit the task ?","

Hi, we're a team of four working on this datascience project, and we finished our second day pretty depressed about which models could fit the task at hand...

We have a week and a half left to complete our project (Python).

Rather than using a Kaggle Dataset in English we're trying to make it work with data scraped on a French website. We're aiming for 20 000 + recipes.

This is a random recipe from the website for those interested:

[https://www.marmiton.org/recettes/recette\_gratin-dauphinois-express-au-lait\_18873.aspx](https://www.marmiton.org/recettes/recette_gratin-dauphinois-express-au-lait_18873.aspx)

We've been thinking about separating the whole thing into two tables, here are the features we've decided to scrap so far for each table.

Ingredients table:

* recipe\_name
* recipe\_id
* ingredient (one row per ingredient for each recipe)
* ingredient\_quantity (no units of measure for now, e.g. 500g of salt, 1 spoon of oil ...)

Recipes table :

* recipe\_id
* recipe\_name
* recipe\_steps (we're having trouble deciding if this should be a single string or a list of strings)

We've had two weeks of machine/deep learning in this course, and have seen a fair amount of library and tools like Scikit-Learn, Keras, Pandas, Numpy

We've done some research, but we don't have a clear understanding of the following models we've been thinking about using for this task and would appreciate any sort of advice or source about which way to go next.

**Very short summarize** : SimpleRNN, LSTM and GRU, RNN layers are too simple to generate proper text from what i gathered. (Word2Vec too in a different way)

GAN and Transformers (GPT-2, BERT, ALBERT) seems likes the answer but I have no idea if it's realist for us to work on it with limited time or even where to start with it.

For TextGenRNN I did not grasp how useful it could be. TableGAN library did not appear to be good at generating text.

&#x200B;

Here is a very long text about my understanding of those models,  **I would really appreciate any advice even without you reading any of what is below !**

&#x200B;

&#x200B;



I don't have a deep understanding of how neural networks works but from my understanding RNN aim to predict the next (future at least) element of a sequence, mainly for Time Series and NLP.

I'm familiar with using SimpleRNN and LSTM and GRU layers in Keras models.

I don't understand the concept of vanishing gradient in-depth for LSTM, but i remember it is supposed to be slower, but better than SimpleRNN.

I've been told GRU is faster because it uses less parameters than LSTM.

About TextGenRNN and TableGAN, i never heard of those two before looking at other works on the subject of cooking recipes.

Both are python library. TextGenRNN appeared to be a better choice than classic RNN models above because it could be used on strings of different lenght and trained with the ""whole"" sequence adding character per character.

While TableGAN opposed two models and returned tabloid data as what seemed to be a cluster of ingredients which could be interesting to use vectors on like I think Word2vec does. (Went back to the TableGAN article, it said he used CountVectorizer from scikit-learn, but results gave back unusable long lists of ingredients.)

What we did with Word2Vec was to feed it with a large amount of processed text (tokenize, Lemmatize and other methods I don't remember), input it with a word or short sequences and it returned a dict of score as key and words as value closely related and often used with the given string. I honestly don't know if the range of application for it extends to much more.

For GANs and Transformers we basically had one lone slide on it that told us it was advanced models but we couldn't cover it during the course.

I don't fully understand transformers, what I understand is that it seems to function with both encoders and decoders, doesn't resort to RNN, but do use vectors.

I guess I need to read 'Attention Is All You Need' asap to further my understanding of Transformers.

My understanding of GPT-2, BERT, ALBERT and other transformers is so low at the moment.

GPT-2 seems to be pre-trained. It calculates the likelihood of a word (clustering and vectors?) occurring after a string based on its full content.

I tried it on this website and it made it seem like one of the better options. It might already be too good and I feel like student of our level working on it might only make it worse.

[https://app.inferkit.com/demo](https://app.inferkit.com/demo)

Do you think it is possible to make it predict words in-between Ingredients/recipe steps like Tomato, Curry, chop and oven instead of giving words following that sentence?

I found BERT and ALBERT while searching for the best currents NLP models. The last one seems to be a lighter version of BERT, which while training hides random part of the full input string while training on it to avoid over fitting. It also increases its performances if I am right.

I don't know how to start with both GAN and Transformers with the deadline being quite soon to be honest. But I'd like to focus on one or two if they're the better solution.

&#x200B;

We would be very grateful if you could recommend to us some models that you think could or should be used for this task, it would help us a lot to limit our field of research.",LearnMachineLearning
rvdjaw,1641247903.0,What metric would I use?,"I’m trying to write a regression algorithm that makes predictions based on empirical data.

The goal I’ve been given is that I need to have success that’s explained as “95% of predictions to be within 15% of the actual value”.

Statistics is my least strong science, BY FAR… what is the best way to show or explain my results based on this criteria?",LearnMachineLearning
rva0ey,1641238700.0,Autonomous driving,"Hi guys,
Do you have some ***advanced***  books or material about autonomous driving, perception and decision making for this field?

Thanks",LearnMachineLearning
rv91cx,1641236211.0,How to make an AI similar to that of JARVIS from Iron Man.,"I just started coding, but the eventual goal is to make an AI that could be similar to JARVIS from Iron Man. I know that there's a lot of stuff I need to download, so I was wondering if anyone knew what I needed to, and the safest ways to do it. Also, I'm using Python.
Sorry if this isn't really what the sub is meant for, if there's a better place please send me there.",LearnMachineLearning
rv8ubv,1641235699.0,Hugging Face / Pytorch Lightning Tutorials,"Hi everyone,

I recently graduated top of my class at a great UK university, I also worked for an NLP lab during my studies. I recently switched to a computer vision researcher role and work on ML models for the majority of my time.

Anyway, on the side I have started making practical coding / ML tutorials - as this is my favourite way to learn. At the moment my guides focus on on creating Hugging Face / Language Models - this is due to my main focus at university revolved around hugging face/transformer models, etc. During my studies I thought the content on youtube was good but not always perfect for what I was looking for, hence me putting out these guides for people in the future doing similar projects.

Have put out two videos so far - one on the basics of the hugging face library, the other a guide on multi-label classification that beats the state-of-the-art on the dataset (to my knowledge). Future ideas include a masked language modelling task and a CIFAR10 CNN guide (yes, I plan to make tutorials for computer vision in the future!). Would be great to get feedback and suggestions on my current videos!

Let me know if you find any of it useful - thanks :)

[https://www.youtube.com/watch?v=vNKIg8rXK6w](https://www.youtube.com/watch?v=vNKIg8rXK6w) (Multi-label classification)

[https://www.youtube.com/watch?v=DQc2Mi7BcuI&t=1s](https://www.youtube.com/watch?v=DQc2Mi7BcuI&t=1s) (Hugging face basics)

&#x200B;

&#x200B;",LearnMachineLearning
rv7w4p,1641233249.0,Convolutional Neural Network from Scratch,"Hello fellow learners, if you are searching for a tutorial about Convolutional Neural Network from Scratch, then please follow [this link](https://dataqoil.com/2020/06/05/convolutional-neural-networks-from-scratch-on-python/). This blog gives begginer's guide to create a CNN model like Keras but from scratch.",LearnMachineLearning
rv6xf2,1641230735.0,How do you add existing classes from pretrained Tensorflow models into your projects that include new classifications?,"I started learning how to train a custom object detector from [this tutorial](https://www.youtube.com/watch?v=yqkISICHH-U), which shows how to use a pretrained model for detecting hand signals. Most other tutorials show similar pipelines - use a pretrained model to detect *only* new things that are annotated and trained upon.

Let's say I wanted this model to detect hand signals *and* people - do I need to train the model on people again or is there some way to switch back on detection for the class, people, it was already trained on?",LearnMachineLearning
rv63gk,1641228588.0,Starlite: a python async (ASGI) API framework,"Hi People,

I wrote an article introducing [Starlite](https://github.com/Goldziher/starlite) \- a new ASGI API framework. I discuss what it is, how it came about, its relation to FastAPI, and its core functionalities. The article can be found here: [https://naamanhirschfeld.medium.com/introducing-starlite-3928adaa19ae](https://naamanhirschfeld.medium.com/introducing-starlite-3928adaa19ae)

I'd be happy to answer any questions you might have.",LearnMachineLearning
rv5yj2,1641228233.0,NN vs Lookup table,"Hi,

assuming one has collected the 24 pairs of the input-output datasets for a target system:

&#x200B;

https://preview.redd.it/2fg2cnom4i981.png?width=720&format=png&auto=webp&s=c4bee11701caef8593567c1beb311bb0840159bc

One can create a simple lookup table to describe the input-output behavior and utilize this as a controller.

One can also train a DNN model to learn the relationship.

What is the benefit of using DNN in this case?

In my opinion:

For DNN, one does not have to store the whole dataset for the lookup table.

If one gives a new input value that is not included in the training dataset, the trained DNN would perform better since in the case of the lookup table the predicted output is just an extrapolated value from the previously known output.

Any other benefits that can justify using DNN?",LearnMachineLearning
rv5ufl,1641227957.0,Are Batch Normalization and Kaiming Initialization addressing the same issue (Internal Covariate Shift)?,"This is a repost of a question i asked on [Cross-Validated Stackexchange](https://stats.stackexchange.com/q/558445/282514), and haven't received answers to yet. Reposting here for visibility with the hope that perhaps someone can help.

Here is the body of the question pasted from SE below for legibility:

---
---

In the original Batch Norm paper (Ioffe and Szegedy 2015), the autors define *Internal Covariate Shift* as the ""the change in the distributions of internal nodes of a deep network, in the course of training"". They then present Batch Norm as a solution to address this issue by ""normalizing layer inputs"" across each mini-batch.

From my understanding, this ""internal covariate shift"" is the exact same issue that is typically addressed when designing our weight initializaiton criteria. For instance, in Kaiming initialization (He et al. 2015), ""the central idea is to investigate the variance of the responses in each layer"", so to ""avoid reducing or magnifying the magnitudes of input signals exponentially"". As far as I can tell, this is also addressing internal covariate shift.

Is my understanding correct? If this is the case, why do we often make use of both techniques? It seems redundant. Perhaps two solutions is better than one? If my understanding is incorrect, please let me know.

Thank you in advance.

---
## References

Ioffe, Sergey, and Christian Szegedy. ""Batch normalization: Accelerating deep network training by reducing internal covariate shift."" International conference on machine learning. PMLR, 2015.

He, Kaiming, et al. ""Delving deep into rectifiers: Surpassing human-level performance on imagenet classification."" Proceedings of the IEEE international conference on computer vision. 2015.

---
---",LearnMachineLearning
rv50th,1641225872.0,Exploratory Data Analysis,"Hello friends,

I have started studying the Machine Learning from few months back. I have an idea on the process from collection of data to training the model.

I want to know about the Exploratory data analysis (EDA) part typically done before training the data.

Could you suggest any books or tutorials for learning about EDA?

And any resources for providing the steps to be taken for EDA based on the particular type of ML models (such as Decision Trees, SVM, etc)",LearnMachineLearning
rv4nt6,1641224939.0,NLP: Hybridization of statistical approach and expert system ?,"Hi everyone!

I have a question for you. For context, we aggregate on a platform the various AI APIs on the market (GCP, Azure, etc.) and including NLP APIs (keyword extraction, sentiment analysis, NER, etc.). The idea is that a developer doesn't have to create accounts with different providers and can have them all on one API to test, compare and change whenever he wants.

However, many customers ask us how to mix the ""statistical"" approach behind these APIs with expert systems and how to achieve hybridization.

Do you have any idea how to do this?

Thanks,",LearnMachineLearning
ruyjmn,1641205133.0,What's the purpose of cyclic layers in a network?,"Hi all,

Currently doing the  [**TensorFlow: Advanced Techniques Specialization**](https://www.coursera.org/specializations/tensorflow-advanced-techniques) on Coursera. The instructor mentions that for a custom ResNet architecture, we can actually make it make a layer cyclic, particularly as ""they have the same weights"". This is illustrated in code as

&#x200B;

`for i in range (1,4):`

`x = self.block2(x)`

Where block2 is a defined residual layer. Isnt this basically cycling the input multiple times through the same layer? What's the purpose here?",LearnMachineLearning
ruy2vw,1641203304.0,How play Something on Netflix and Spotify work ?,"I have always wondered  what is the technology and model behind Netflix's and Spotify's Play Something button. Is it related to the conventional recommender system and gives the top result or a different thing altogether. Also what about the technology does it use RL or simple multi arm bandit.

Any links would be really helpful.",LearnMachineLearning
ruxg67,1641200683.0,Estimating the orientation and fractional occupancy of two crossing bundles,"Hi,

**Goal:** I currently have a model that estimates the orientation and fractional occupancy of two crossing bundles in a cubic volume based on a noisy signal.

&#x200B;

[Fig 1. 2D Representation of the two crossing bundles](https://preview.redd.it/rnrmt4ycof981.png?width=373&format=png&auto=webp&s=c46e8fd3f1b4b67bb2c2eeac61ded07a39d1de0a)

**Current approach:** Since the two bundles are basically indistinguishable from each other. I decided to always put the bundle with the higher fractional occupancy as the first target and the bundle with the lowest fractional occupancy as the second target. The network architecture with the target ordering is depicted in Fig. 2.

[Fig 2. Network architecture](https://preview.redd.it/30xr2ya3pf981.png?width=724&format=png&auto=webp&s=e3e00ab5050eb281cee72dfe24a4354dbc531bad)

**Issue:** When the two bundles have a fractional occupancy close to each other (for example *frac\_0* = 0.51 and *frac\_1* = 0.49), the network sometimes swap both bundles (frac\_1, x, y, z associated with the bundle 1 is output in bundle 0). During backpropagation, the network is thus heavily penalized because of the swapping since the error on *x y z* is huge. (If the network wrongly estimates frac\_1 and frac\_0, it will swap both bundle and will be heavily penalized).

&#x200B;

[Fig 3. MAE of the fractional occupancy in function of the ground truth fractional occupancy. The MAE increase for volume fraction close to 0.5 due to bundle swapping during learning. ](https://preview.redd.it/xwklxk4ctf981.png?width=654&format=png&auto=webp&s=5373fbe334925b6b494f3a67837820ca25679485)

&#x200B;

Is there a way to better arrange target to prevent this kind of issue?

Thanks and Regards,",LearnMachineLearning
rux10m,1641198976.0,Unsupervised classification for optimal images for ocr?,"I want to categorise the image into 2 categories

if we can see numbers properly on the image like in 1st image and if number are rubbed off like in 2nd image

[1st image](https://preview.redd.it/vafcj99ipf981.jpg?width=750&format=pjpg&auto=webp&s=24760d211f650b2592050bae94004385bcd5d466)

&#x200B;

[2nd image](https://preview.redd.it/z7mpvyokpf981.jpg?width=600&format=pjpg&auto=webp&s=79fd5bfe1eb413ff42938700f9df0507e4a04dee)",LearnMachineLearning
ruvgva,1641192739.0,💊Your daily dose of machine learning : OpenCV,"> This is a series of posts that I post almost daily. I call them “your daily dose of machine learning”.

If you’ve worked on image processing projects before then you probably heard of OpenCV.
It’s a very cool framework for doing all sorts of things related to computer vision.

Some of the things you can do with OpenCV include:

\- Basic image processing techniques such as applying filters (blurring, edge enhancement, …).
\- Depth estimation using stereo cameras.
\- Extracting features and computing descriptors for images.
\- Classical machine learning such as linear regression, SVM, bag of visual words,...
\- Deep learning for image classification and object detection.

Although OpenCV is not a machine learning framework, it still offers a variety of tools to build some powerful ML applications.

Can OpenCV replace Tensorflow in your codebase? Not really.

But if you’re already using OpenCV in your codebase then it’s worth taking a look at it’s ML capabilities and you might be surprised by what it can do!

I’ll share more insights about OpenCV in the upcoming days so stay tuned!

 Connect with me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***",LearnMachineLearning
ruv7tc,1641191788.0,Understanding Model Overfitting/Not Learning Certain Classes,"I'm currently training a model to classify types of Bikes. I've trained it using a ResNet50 architecture, attached with a simple output layer. I've trained it from scratch twice, and both times, the model failed to pick up on two of the classes. I only have 6 classes total. The classes were different both times. I tried using different batch sizes, learning rates, image augmentation and shuffling around the training and validation sets, but no improvements have been seen. Does anyone know why something like this could be happening? Or any potential solutions I can try? Thanks in advance.  Also if more information is required, lmk, this is my first time posting in these communities and I would like to provide all the information needed.",LearnMachineLearning
ruudv3,1641188835.0,How to save augmented images?,"Hi,

I’m currently using data augmentation for training my model. This is the code i use for augmentation.

    def augment_image(image, label):
        # Flips the image randomly
        image = tf.image.random_flip_left_right(image)
        image = tf.image.random_flip_up_down(image)
        # Increase the image size, then randomly crop it down to
        # the original dimensions
        resize_factor = random.uniform(1, 1.2)
        new_height = math.floor(resize_factor * INPUT_SHAPE[0])
        new_width = math.floor(resize_factor * INPUT_SHAPE[1])
        image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)
        image = tf.image.random_crop(image, size=INPUT_SHAPE)
        image = tfa.image.translate(image, [HSHIFT * tf.random.uniform(shape    [],minval=-1, maxval=1), VSHIFT * tf.random.uniform(shape=[],minval=-1, maxval=1)],interpolation='nearest',fill_mode='nearest')
        # Vary the brightness of the image
        image = tf.image.random_brightness(image, max_delta=0.2)
        image = tfa.image.rotate(image, MAX_ROT_ANGLE * tf.random.uniform([], dtype=tf.float32),interpolation='nearest',fill_mode='nearest')
        return image, label

    train_dataset = train_dataset.map(augment_image, tf.data.experimental.AUTOTUNE)

I want to visualize the data augmentation which is going through.

I’m have used ImageDatagenerator() method of augmentation, there will be option to save in a dir in flow() api or flow\_from\_directory() api. But currently i couldn't use that method of augmentation in my scenario.

Is there way to save the augmented images using this method?

Thanks and Regards,

Ramson Jehu K",LearnMachineLearning
rutxdj,1641187308.0,Is Machine Learning A-Z course on udemy enough to learn basics of machine learning?,"Hey guys, I'm studying machine learning. I have some knowledge of regression topics. I joined this udemy course recently. They are offering some basic(very little) explanation on the theory and go straight to how to use the scikit learn class for that algorithm.

For the regression topics that I know about I was able to join the dots from the few lines they've said to my previous knowledge. I do not believe they spend much time on the math behind it. I feel I can get that explanation from youtube video and just look for python class in documentation and find an example too in the same docs.

Will this course be enough to learn the basics? Do I need to supplement with some other course to cover the math / explanations behind it.

I don't want to give any negative impressions on the course, I've only completed a few chapters, I just am a bit doubtful if its better to be supplemented by some other material. I have a feeling some guy hired by the course people will answer too XD.",LearnMachineLearning
ruqaoc,1641176230.0,Tensorflow / Keras implementation of Vision Transformer,"Tensorflow / Keras implementation of An Image is Worth 16x16 Words: ViT Excellent results compared to SOTA CNNs while requiring fewer computational resources to train.

Paper : https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1
Code : https://github.com/avinash31d/paper-implementations",LearnMachineLearning
ruobs9,1641170732.0,An interesting post in deep learning for beginners - Wild Cats Image Classification using Deep Learning,"[Wild Cats Image Classification using Deep Learning](https://debuggercafe.com/wild-cats-image-classification-using-deep-learning/)

[https://debuggercafe.com/wild-cats-image-classification-using-deep-learning/](https://debuggercafe.com/wild-cats-image-classification-using-deep-learning/)

&#x200B;

https://preview.redd.it/ro5mri5ndd981.png?width=1200&format=png&auto=webp&s=8db79ceface41e5785cbba388ab966183fb64388",LearnMachineLearning
rulr8h,1641163812.0,Model consistently under forecasting,"I am working with time series data and trying to do forecasting for the next 90 days. The model is an ensemble of prophet and arima. The model does a pretty good job in terms of accuracy, but it is almost always under forecasting consistently. What suggestions would you have to fix that? Client is expecting some sort of ups and downs between actual and forecast.",LearnMachineLearning
rul8l2,1641162435.0,Padding Vector for Variable Sequence of Vector (Multivariate LSTM Regression Task),"I'm aware in regular NLP that you would use a padding token for a regular Seq2Seq mode.

What would I use for padding if instead of a sequence of tokens, I have a variable sequence of continuous vectors?",LearnMachineLearning
ruk1ge,1641159333.0,any good fluid dynamics data repo's,"Hey everyone,

Does anyone know of any good repositories of sensor data for fluid dynamics? Or for that matter of vibration data? I'm trying to use DMD to find the dominant, coherent structures in a dataset but have limited funds for experiments and limited access to sensors. Thank!",LearnMachineLearning
ruijl9,1641155250.0,"Sieve: We processed ~24 hours of security footage in <10 mins, now you can search per-frame!","Hey everyone! I’m one of the creators of [Sieve](https://sievedata.com/), and I’m excited to be sharing it!

Sieve is an API that helps you store, process, and automatically search your video data–instantly and efficiently. Just think 10 cameras recording footage at 30 FPS, 24/7. That would be 27 million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack.

We built this visual demo ([link here](https://sievedata.com/app/query?api_key=AIzaSyAfKwf0tuuNOHbYi_JX-ew_dXH6SzdxZWY)) a little while back which we’d love to get feedback on. It’s \~24 hours of security footage that our API processed in <10 mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario.

To try it on your videos: [https://github.com/Sieve-Data/automatic-video-processing](https://github.com/Sieve-Data/automatic-video-processing)

Visual dashboard walkthrough: [https://youtu.be/\_uyjp\_HGZl4](https://youtu.be/_uyjp_HGZl4)",LearnMachineLearning
ruifyk,1641154973.0,Understanding Architecture of DNNs (specifically CNNs),"What is the intuition behind how many neurons each layer in a DNN has? For example in a ResNet50, the first layer is a 7x7 64 kernel convolution. But why 64 kernels specifically instead of 63 or 66 or any other random number? Is this just the industry accepted number or was the number of kernels treated as a hyperparam and validated?

Or in TF’s tutorials they use a DNN to classify MNIST digits with the layers: Flatten(input=28, 28), Dense(128, relu), Dense(10)

But why does the second layer output to 128 neurons? It just seems like a magic number as I can’t find any reasoning behind it",LearnMachineLearning
rui1c7,1641153893.0,Sklearn pipeline breaks when using FunctionTransformer,"hey, I'm learning to use pipelines as they look more 'clean'. So, im working on the tabular playground competition on Kaggle.

I'm tryna follow a pretty simple pipeline where I use a `FunctionTransformer` to add a new column to the dataframe, do `Ordinal Encoding`, and finally fit the data on a `LinearRegression` model.

Here is the code -
[https://pastebin.com/0xmPuznc](https://pastebin.com/0xmPuznc)

But the code breaks on the first step (`FunctionTransformer`) and gives me the following error -

```
to assemble mappings requires at least that [year, month, day] be specified:
[day,month,year] is missing
```

which is weird since, i can print inside the function being executed which shows it is in `datetime` format. Even `.transform(X_train['date'])` on the `FunctionTransformer` works as intended. But it doesn't seem to work when all the steps are joined.

Any help is appreciated. Thanks",LearnMachineLearning
ruhkky,1641152663.0,Good colab notebook for very large batches of text?,"Hey! I've been interesting in using gpt-3 or gpt-2 to generate things like film scripts or social media posts (like the Deep Leffen Bot on twitter). I'd like to train it in a very very large amount of text. Does anyone know what model is best, and if there are any good colab notebooks for this specific task (feeding in very large batches of text instead of only small prompts)?",LearnMachineLearning
rug7ce,1641149073.0,Is the super harsh guide to get into machine learning up to date?,"It's been 4 years, and I just want to know if people still believe this information is relevant, or do there need to be any additional changes? The link is below for those that don't know what I'm talking about.

&#x200B;

[A Super Harsh Guide to Machine Learning](https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/)

&#x200B;

I'm a data scientist interested in getting a career as a machine learning engineer, so I just wanted to know where I should focus my efforts.",LearnMachineLearning
ruf6eq,1641146358.0,What is an eigenvector? A 2-minute visual guide.,"&#x200B;

https://preview.redd.it/653ccnavcb981.png?width=2048&format=png&auto=webp&s=609520c4237b5bb1807019856806178b1cd1cb68

https://preview.redd.it/ldhh8xavcb981.png?width=2048&format=png&auto=webp&s=cb669d3d9371075e35b45d42a6dd86124f686d16

https://preview.redd.it/apcolzavcb981.png?width=2048&format=png&auto=webp&s=4731d2e89fde7448be2592d7e2a5fb24111da6b1

https://preview.redd.it/n0uea5bvcb981.png?width=2048&format=png&auto=webp&s=258345c8d52d8a44695f3afd54f63dcef98d1934

🔵 Eigenvectors 🔵


🚀 An eigenvector is a special vector associated with a linear transform. It's special in the sense that after applying the said transform it does not change direction but only gets scaled (multiplied by a scalar value) by the eigenvalue.


🔨 Each eigenvector comes with a corresponding scalar called the eigenvalue. Breaking a matrix M into its eigenvalues and corresponding eigenvectors is called eigendecomposition.


🔭 The word ""Eigenvector"" comes from ""eigen"" in German where it means ""its own"". It was originally used to study rigid body motion and in the discovery of principal axes. However, nowadays it has found its way into a wide array of applications from my favorite: principal component analysis, differential equations, and problems in physics and chemistry relating to wave transport and molecular orbitals.


🎭 Another one of the classical applications is the Eigenfaces project for facial recognition. Eigenfaces decompose a face as a composition of face templates (basis) called eigenfaces. Imagine N eigenfaces E\_1, ..., E\_n when given a new face F it can be written as a composition of each of these N eigenfaces for example: F = (10% of E\_1 + 55% of E\_2 + 35% of E\_3). Each eigenface would represent a meaningful feature in the high-dimensional space of faces.


\---------------------------------------------------------------------------------


If you like such content and would like to steer the topics I cover, suggest topics you would like to know more about in the comments.",LearnMachineLearning
ruekwq,1641144724.0,Any reading group for ML books or research paper?,Does anyone know about any ML reading group? Could be for ML books or research papers. Something like a discussion group. I would like to be a part of them.,LearnMachineLearning
rudcli,1641141388.0,How to remove Missing Values in sklearn Pipeline?," Hi, is there a straightforward way to implement a list wise deletion of  missing values as part of a sklearn pipeline.  I want to delete all rows where one of the values is missing, but it seems that there is no method for that in sklearn, like there is for imputing. Is the only way to make my own class?",LearnMachineLearning
rudaf0,1641141213.0,How to encode sequence using convulutions?,"Hi, I'm working on a personal project where I'm trying to encode FASTA sequences and input them into a neural network. However, I'm having trouble figuring out how exactly I can encode them.

First of all, the length of these sequences is varying. A typical sequence may look like ""ASHHHHHHSYTWTGALITPCAAEESKLPINALSNSLLRHHNMVYATTSRSAGLRQKKVTFDRLQVLDDHYRDVLKEMKAKASTVKAKLLSVEEACKLTPPHSAKSKFGYGAKDVRNLSSKAVNHIHSVWKDLLEDTVTPIDTTIMAKNEVFCVQPEKGGRKPARLIVFPDLGVRVCEKMALYDVVSTLPQVVMGSSYGFQYSPGQRVEFLVNTWKSKKNPMGFSYDTRCFDSTVTENDIRVEESIYQCCDLAPEARQAIKSLTERLYIGGPLTNSKGQNCGYRRCRASGVLTTSCGNTLTCYLKASAACRAAKLQDCTMLVNGDDLVVICESAGVQEDAASLRAFTEAMTRYSAPPGDPPQPEYDLELITSCSSNVSVAHDASGKRVYYLTRDPTTPLARAAWETARHTPVNSWLGNIIMYAPTLWARMILMTHFFSILLAQEQLEKALDCQIYGACYSIEPLDLPQIIERLHGLSAFSLHSYSPGEINRVASCLRKLGVPPLRVWRHRARSVRARLLSQGGRAATCGKYLFNWAVKTKLKLTPIPAASQLDLSGWFVAGYSGGDIYHSLSRARPR""

&#x200B;

It's essentially a ton of letters, where each letter is representing some sort of amino acid.

&#x200B;

To encode these I was considering using a series of convolutions, however I don't know if this is the most ideal way to do so.

&#x200B;

What is the most ideal method, to encode a sequence like this, so it can be passed into a neural network? More specifically Im trying to use it for a reinforcement learning agent.

&#x200B;

Also since the amount of text in a FASTA sequence may vary, I'm having trouble figuring out how exactly I should deal with that.",LearnMachineLearning
rucns3,1641139511.0,Using a Genetic Algorithm for a minimax fitness function?,"I recreated a 2-player turn-based deterministic board game (like chess, checkers, etc...) and I thought it'd be fun to try and create a simple AI that plays that game.

I began reading online on the matter and what seemed to be fitting for my use case is to use a minimax algorithm - and inspired by some videos I've seen on genetic algorithms and additional reading I thought it'd be a great idea to use a genetic algorithm to create a good evaluation function.

To my surprise, I had a hard time finding examples of genetic algorithms being used for generating a fitness function to use in a minimax algorithm. Also, the more I thought about it the less it made sense: How do you even relate a chromosome, a board state, and a single number output?",LearnMachineLearning
ru9xro,1641131234.0,What are resources for learning ML without python or a bunch of frameworks?,"Here is my problem: I am interested in learning machine learning and have some basic understanding of it but most of the tutoruals, blogs, books i found were using python, using a bunch of frameworks ...... But i hate this. I am looking for going up from scratch. For eaxmple i have gone through several tutorials and according to me its way more fun and learning experience to build a neural network frm scratch to be trained to evealuate a and b rather than using a bunch of frameworks to make a chatbot or image caption generator in n lines of python code!!!! Is there any good resource like thatnwhere i can learn ml from scratch??

Note : What i said above is totally my **opinion** and if you think i am wrong i totally respect that.",LearnMachineLearning
ru86ji,1641124494.0,Time series classification?,"Hi everyone,
I like to take pictures of freight trains, and in my area the railroad broadcasts some simple data over unencrypted radio that can show some very basic info about what's going on. You don't get the exact position of a train, just confirmation that there is a train on a certain piece of track (which can be 30ft or 30 miles long), and some info about what the track signals are showing.

Long story short I am able to collect this data and I have written programs to identify when a train passed a given part of the track.

Below is an image of this data plotted over distance and time. I have manually highlighted what I would like to be able to do with ML, which is to classify a given train detection at a location as belonging to a given train. Trains moving up and to the right are eastbound, and trains moving down to the right are westbound. Trains almost exclusively make their journey all the way from one end to the other. It is very common that an eastbound will stop at a siding to let one or more westbounds pass. Fairly rarely, a train will start out one direction, turn around and come back the other way.

Complication: some radio data is missed depending on various conditions, so I can't guarantee that I will pick up every train at every control point. But, I can pick up enough data to fill in the gaps, at least with my own brain.

I am a half software/half electrical engineer, and I have taken a couple beginner courses on ML, but I'm not really sure what type of classifier I should use for this, or how I should organize the data. I'm also not 100% sure that this is a task for ML, but it seems natural that it would be, especially considering that sometimes missing data has to be inferred. I would be very appreciative if you all would be willing to help me out!

[Screenshot-20220102-062841-2.png](https://postimg.cc/YLgcLsnk)",LearnMachineLearning
ru6niq,1641117716.0,Are Transformers state of the art for every kind of task or do they suck at some tasks as well? If so what kind of tasks do they suck at? What kind of Neural Net architectures outperform them in those tasks?,,LearnMachineLearning
ru6fvr,1641116789.0,Trying to get into ML with a specific project and need some help getting started.,"Hi, i have worked with ML before with simple classifications in the particle physics realm. But now i want to go a bit beyond that and have chosen to do something relating to a hobby of mine.

The hobby is a 5v5 video game where the end goal is to have something where i can feed it what one team is doing and have the model give me what the other team should be doing to have the best chance of winning.

As a first start and want to have a model classify Team1's chances of winning based on just their trajectories. Where i train it with their trajectories on the game map and the label if they won the round or not.

But already here i am not sure how to best build the model because of the trajectory data. For each of the 5 players on Team1 i have they x,y,z position for the first 20 seconds of the game. But i also dont want the model to care about which players causes which trajectory. So the model should treat player 1 trajectory1, player2 trajectory 2 the same as player1 traj.2, player2, trayj.1

It would be great if someone could point me to ressources that allow me to understand how exactly i should build the model for this first step.",LearnMachineLearning
ru66do,1641115586.0,How to improve performance in multi-class?,"Hi

I'm classifying traffic using the [CIC-2020 Darknet](https://www.unb.ca/cic/datasets/darknet2020.html) data set.

There are 8 classes in the data set.

I tried a lot of different ways.  (Over sampling, Under Sampling, Complex Sampling and Outlier Elimination, Feature Selection, model change - cat boost, rus boost, Decision Tree, Random Forest, OvO, OvR, SVM etc..)

However, the performance doesn't improve.

In the picture below, I only selected features using RFECV and used Light GBM.  This has the best performance.

&#x200B;

[Validation](https://preview.redd.it/2amvhweeud981.png?width=964&format=png&auto=webp&s=af98eb397fd93a7d81ba22933629bb783c69a6e3)

&#x200B;

[Test](https://preview.redd.it/mwqpf4efud981.png?width=962&format=png&auto=webp&s=06c21ae88d265ac6213c47bd306ef56bfb12d3af)

Is there a good way to improve performance in multi-class classification?

Please help me.",LearnMachineLearning
ru57dw,1641111392.0,What are the infrastructure requirements for deploying an NLP model?,"I am a freelancer who builds AI models for small companies and one of them asked me to deploy the model into their production environment and wanted me to give the infra req. related for that. I don't know what are the requirements regarding hardware, software and integration needed for the same. Any good articles on this would be appreciated (the one I'm getting on google are very generic, need something specific).

The NLP model is based on DistilBERT (\~250MB in size)

TIA",LearnMachineLearning
ru4tmp,1641109819.0,Predicting multiple targets (Multi class text classification),"Hi, I am currently working on project which requires me to predict category and subcategories and if it is organic product or not based on the ingredients list, brand and description of the product. Can anyone tell me how to do this?

More explanation

There are 6 columns (A,B,C,D,E,F) I need to predict D,E,F based on A,B,C. I want to predict 3 of the columns in one go.

Thanks in advance",LearnMachineLearning
ru4odm,1641109168.0,What Laptop Should I Get for Machine Learning,"I am just getting started with machine learning.  I figure it does not matter too much what type of laptop I get, but any suggestions would be helpful.  I come here because data science/machine learning will be my primary focus of study.  I assume early on it won’t matter much what I get, but I want something that I will be able to use long term as I improve.  I have heard Linux is the best OS to use but other than that I don’t have much knowledge on what I should get.  Any recommendations?

Trying to keep it under $2,500 US

Edit:

I am looking at the Dell Precision 7560 Workstation

-Processor: 11th Gen Intel Core Processor i7-11800H (8 Core, 24MB Cache, 2.30GHz to 4.60GHz, (45W))

-Operating System: Ubuntu Linux 20.04

-Graphics Card: Intel® UHD Graphics for 11th Gen Intel® Processors

-Display: 15.6"" FHD, 1920x1080, 60Hz, Anti-Glare, Non-Touch, 45% NTSC, 220 Nits, Cam/Mic, WLAN

-Memory: 32 GB, 4 x 8 GB, DDR4, 3200MHz, Non-ECC, SODIMM

-Hard Drive: M.2 2230 256 GB, Gen 3 PCIe x4 NVMe, Solid State Drive

Thoughts???",LearnMachineLearning
ru3ba4,1641103845.0,Which NLP algorithm to consider if my dataset is very small (250-500 data points) and I want to perform text classification and identify semantic similarity?,"I have already tried DistilBERT for text classification and the model got overfit (99% train accuracy and \~30% test accuracy). Currently, I have 250 data points and the dataset is supposed to increase with time (but not much would be in the 1000s). I was wondering if there's any other algorithm for text classification which performs better on small datasets.",LearnMachineLearning
ru2pcb,1641101684.0,Local explainability using SHAP after feature engineering,"I am using [*shap*](https://github.com/slundberg/shap/tree/master/docs) to explain model predictions for a particular instance and was wondering how to interpret them when feature engineering is applied to data. I can think of three scenarios,

1. NO features are dropped or added. In this case, *shap* would simply give a value for each feature indicating how a particular feature affected the result.
2. Some features were dropped. *shap* would again output a value for each features, but does it make sense to say that features that were dropped did not affect the model output? (Sounds silly but not sure if it statistically correct to use *shap* in this case)
3. New features were generated from existing features. Lets say you generate a bunch of features from original ones while retaining the original.
Original - \['a', 'b', 'c'\]
New - \['a', 'b', 'c', 'ab', 'bc', 'abc'\]

Thanks in advance.",LearnMachineLearning
ru0nzv,1641094964.0,Sentiment analysis on large datsets,"Hello!

So for a project, I have to perform sentiment analysis on really large datasets (each one is > 6 million strings, and there are about 90 of them). For this, I have been using huggingface - but it looks like the inference is super slow. Like I passed one of the datasets and it's been crunching away at it for 24+ hours. I am running it on the GPU and I optimized the batches too - but it takes way too long.

I've been considering pruning the dataset, but there's only so much I can do. Like I can get the whole dataset down to 72mil ish.  But then that still doesn't solve the massive time issue. Is there a faster way of doing this? Apart from multiple computers solving subsets of the data??

Thanks in advance!",LearnMachineLearning
rtymaq,1641088619.0,ML map predicting,"Hello I'm not sure if this is the right place to post this but I'll give it a shot anyway. I play game called Warframe which is really fun, but one of the ways you can progress is to open something called dragon vaults. and the only way to do that is to go into a map and search for a  door that randomly spawns around the map, it normally takes me 5 to 10 minutes to find it which is way too long considering what you get. I was thinking of doing a bunch of runs, maybe even crowdsourcing taking  screenshots of the map screen. labeling where the DragonVault was and then feeding it back through ML to see if it can predict where the DragonVault will be. because most things in games aren't actually random and there are some patterns I have noticed and I'm not that good at pattern recognition.",LearnMachineLearning
rtw69p,1641081391.0,Can I use features of type float in a linear regression machine learning model?," I am currently working on a basic machine learning project which is revolved around predicting house prices given several different features about the house. Some of these features are of type float instead of type int. Examples of this are bathrooms (can be 1.5), floors (there are houses with 1.5 and 2.5 floors in this dataset), and bedrooms (can be 1.5,2.5... in this dataset).

&#x200B;

I was looking at other similar projects online and came across this:

""Remember that, it is essential to change float types to integer types because linear regression is supported only on integer type variables. It can be converted using the *‘astype’* function in python.""

&#x200B;

Finally, my question is, do I have to convert floats to integers for a linear regression machine learning model or can I use floats? I want to use floats because I feel like by converting to int type, I will lose a lot of important data. Ex- 1 bedroom house and 1.5 bedroom house will be the same after the conversion.",LearnMachineLearning
rtvqdl,1641080115.0,"Any idea of how could one create a ""local"" and personalized AI Pair Programmer? What kind of tools and algorithms could be required?","The title. ¿Why? Because why not?.  Also, considering possible limitations, there could be conditions, such as:

1. The AI would be trained from scratch by the PC User.

2. I can only work with one programming language.


Maybe there's stuff I'm missing here. Anyway, I will be pleasured  to read your insights.


I'm personally more curious about whether one could create such program using modern  tools like Julia & to work with modern and complex languages like Rust to accelerate development in the most personalized way possible.",LearnMachineLearning
rtvf85,1641079236.0,"Got a list of practical ML projects that, surprisingly, can be done by Individuals in a computer and are not necessarily limited to ""Enterprise-origin-only""?",,LearnMachineLearning
rtv9cd,1641078784.0,ML Learning Advice,"I am a 3rd year economics student. I have completed my statistics, calculus, mathematical economic(based vastly on matrix algebra), microeconomics and macroeconomics courses with A and my GPA is 3.51. I am familiar with web development for almost 4 years also so I have more than a general knowledge in programming. I am currently taking econometrics, algorithm design and analysis and introduction to database systems courses. I am very interested in quantitative analysis, ML. What advices could you give me to improve myself in this field? I also want to pursue an academic career.",LearnMachineLearning
rttp81,1641074399.0,What I need to learn Machine Learning? (beyond a programming language),"I started a course on Machine Learning in Python and everything was going well until mathematics began to appear, so I leaved the course for another time and now I would like to take it up again, I want to start studying Statistics and Probabilities but it is a very huge subject, then What should I know before start learning Machine Learning?",LearnMachineLearning
rtpci8,1641062158.0,Should we drop the AI when referring to AI/ML?,"I wonder why these two seem to always come in a bundle.
AI is a superset of ML, and I realize that there are some examples of AI that are not ML.
However,  it seems that most of the work discussed in articles or even job listings, are specifically machine learning.
Hence, shouldn't the AI portion be dropped?
Thanks.",LearnMachineLearning
rtjo93,1641044607.0,YOLOv4 - add new class to already trained model without training on entire dataset?,"I have already trained my model on a dataset for let’s say 50 classes. It’s trained to recognize letters. Now let’s say I want to add a newly discovered letter, so I want my model to also be able to recognize this letter.

How would I go about this? I would prefer to be able to train my existing model with a dataset only containing the new letter to “make it aware of” this new letter too.

Appreciate any help",LearnMachineLearning
rtispn,1641041292.0,A good course about ML/DL with Big Data,"Hi all,

I'm looking for a course that essentially describes what is Big Data and shows how to use it with ML/DL models, I have a deep understanding of ML & DL so I'm just looking for a course that add the ""Big Data"" puzzle piece to my tools.

Preferably the course should demonstrate its work & have the exercises be written in Python.

I mostly use Coursera as my go-to since it teaches the technical\\mathematical background and not only ""here is a piece of code, try it"" type of course so any course that cover both theoretical & practical sides of the topic is welcome.",LearnMachineLearning
rthi58,1641035934.0,Customer Survival analysis in SaaS company,"Hello redditors,

I am about to start a survival analysis project, using Python, for my thesis.

Also, since I will use data from the company that I am working for,  I thought that it would be a nice idea to create an end-to-end survival analysis project for the company also.

I am searching everywhere to understand what the survival analysis is, what is the difference between non-parametric and parametric analysis, how can we use ML models to do survival analysis and many more ..

The main question that I have and which would be super useful to have a main understanding is
""What's the result/outcome of a customer survival analysis?
What questions can be answered if the analysis has a meaningful results?
How can the company make use of the result of this analysis?""

Thank you for your time!!",LearnMachineLearning
rtgxqy,1641033535.0,I am majoring in statistics. What computer science subjects do I need to learn?,"Happy new year everyone,

I am a second-year university student majoring in statistics, and I am interested in ML/AI. If possible, I want to apply for a Master's degree in ML.

At my school, the programming courses covered by the statistics dept are:

1. Python (computer modelling for scientists),
2. Database (SQL),
3. Math Modelling

I am currently self-studying data structures and algorithms using C.

After that, I am considering learning:

1. Computer Architecture,
2. Computer Network,
3. Operating Systems

May I ask if these subjects are a good fit for my need, please?

And what other subjects do I need to take, to have adequate knowledge in the coding part of ML?

Thank you, have a nice day. :)

edit: formatting",LearnMachineLearning
rtapnm,1641008604.0,Why no sigmoid activation function for the discriminator from TF's DCGAN tutorial?,"I'm referring to this TensorFlow tutorial: [https://www.tensorflow.org/tutorials/generative/dcgan#the\_discriminator](https://www.tensorflow.org/tutorials/generative/dcgan#the_discriminator)

Why don't they constrain the discriminator's output to be within (0, 1)? What advantages does their approach have?",LearnMachineLearning
rt6zny,1640996128.0,(How)would you approach the book Deep Learning by Ian Goodfellow from a research perspective?,For context I am a undergrad student currently in my senior year. I have some prior experience to ML and DL and I am also currently pursuing couple of research projects with a couple of professors but I also want to do something in my own on the side. I was wondering what would be the best way to get started. Should I read the book mentioned above in order to get some ideas of my own or should I try something else?Thanks.,LearnMachineLearning
rt2vwh,1640983593.0,Math to self learn for AI/ML,"Hello everyone, I don't want to go for a CS Masters, so I'm just thinking of buying textbooks that I can self learn from to prepare myself to know as much about AI and ML as a Master's degree holder would.

So far I know Calc1, Calc2, Intro Level Probability and Linear Algebra 1 (if that's what the undergrad course usually is).

So next on my list is to learn multivariable calculus (maybe from a textbook, maybe from Khan Academy)

&#x200B;

But after all that, what further math would I need to learn to know as much as a Master's student would in AI/ML?

&#x200B;

Thank you so much!",LearnMachineLearning
rt139q,1640978222.0,"Completed applied statistics and basic ML courses in college, wanting to learn more. Where should I go from here?","Hello, last year I took an applied statistics course and intro to ML course in college, both taught by the same professor.


While I got through them, I didn’t feel like I learned and retained all that much information from them, and feel lost as to where I should go now in terms of ML. Like, I remember what z-scores, normal distributions, and confidence intervals are, but I couldn’t tell you what a Gamma distribution is used for and what it’s basis is. Should I review statistics first? Or is my knowledge good enough to jump back into ML?

I’m on winter break now and was wanting to visit the library and read some books on the topic, but I’m clueless as where to start. Could anyone recommend me some good starting points for statistics and basic machine learning? We did a lot of application of it in Python, but I’m also wanting to understand the underlying theory. I’m hoping to refresh my brain on what I went over, and hopefully then delve into some more advanced topics once I feel comfortable.

Thank you, and apologies if this is a repeat post / been answered before.",LearnMachineLearning
rsyiyj,1640970830.0,Shared weights between different implementations,"Hey all!


I am currently implementing a convolutional neural network in Rust (goal is to compile to WASM and deploy on the browser). So far, I have an implementation that matches tensorflow up to almost perfect accuracy (we have some very slight errors that are probably unavoidable with floating point computations, f.e. 0.70710677 vs 0.07071068).

Now, the goal is to port a model that already works from Python to Rust. I already have the weights from the trained Python model and simply load them into Rust.

The problem is, those small numerical differences add up harder and harder each layer. While after the first layer, the outputs might differ by something like 1e-6, a few layers up we suddenly have completely different values.

Are there any ideas to prevent this? Or is there no other way than training in Rust (pretty unfeasible, as we would then basically have to write a complete backprop and training procedure – our conv implementations likely aren't fast enough for a nice result).

&#x200B;

Greetings!",LearnMachineLearning
rsx4n0,1640966966.0,"More data, More problems","Live by these words when you build your models: More data, More problems. Just because you have a ton of data does not mean you should use it all to train your model.


The larger the dataset, the easier it is for garbage to hide in it. Thousands of data points, gathered by different methods, spanning across years. Trust me there are massive problems hiding in that data. Models amplify and further obfuscate anything missed in data analysis and cleansing.


Use as little data as possible. Control data sourcing, aggregating, cleansing, and analysis (or research it for existing data) as much as possible. Do not use data you do not fully trust. Reduce features from the start.


We hear about massive datasets used by Google or Facebook. These are expertly curated. By that I mean years have been spent reviewing and grooming. Data best practices require this level of obsession with detail. Until the business has built that capability, smaller is better.",LearnMachineLearning
rsusfc,1640960087.0,"Custom .h5 for ""dickpics""","Hey,

I want to detect nudity in a live video stream, I tried to train my own model but it does not detect anything. I couldnt find an already trained model for ""penises"" anywhere, do you guys maybe know a source?

Thank you in advance :)",LearnMachineLearning
rsumxg,1640959627.0,What is L1 distance? A 2-minute visual guide.,"&#x200B;

https://preview.redd.it/hm2ilafdxv881.png?width=2048&format=png&auto=webp&s=77625412440d5141ba536f9e1d57e77a8df2121c

https://preview.redd.it/1u8tacfdxv881.png?width=2048&format=png&auto=webp&s=a40b167c60c21f3e41e69f5165c5a8ed097fe417

🔵 Manhattan distance 🔵


🗽You might have heard of Euclidean or L2 distance but have you heard of the L1-distance also known as the Manhattan distance?


🚕The Manhattan distance is computed by treating the geometry to be as if it were the street of Manhattan, one square city block followed by another. So the only way to travel is to go along the right-angled streets. While the shortest Euclidean distance between two points has a unique path the same is not the case for Manhattan distance as you can have multiple paths with the same distance.


🌃 Mathematically, the L1-distance is the sum of the absolute value of the difference of each coordinate of your point/vector and can be extended to N dimensions.


🤓 L1-distance or the L1-norm is also used to regularize model parameters. Regularization penalizes model parameters from over-fitting. The L1-norm forces model parameters to be sparse which would shrink the non-important features towards zero. The coefficients of the model could then be used to understand which features are more important i.e. the features that correspond to model parameters with larger values.


For example if you have a linear regression model with L1 regularization that predicts the price of a house with features (""number\_of\_rooms"", ""area"", ""color\_of\_house""). After fitting the model on your data you see the coefficients corresponding to (""number\_of\_rooms"", ""area"", ""color\_of\_house"") = (0.5, 0.6, 0.01) you can see that the model treats ""number\_of\_rooms"" and ""area"" to be more important features than ""color\_of\_house"" as |0.5| > |0.01| and |0.6| > |0.01| in determining its price.


\---------------------------------------------------------------------------------


If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",LearnMachineLearning
rssbiv,1640951592.0,Right hardware decision for mobile observation robot being outside,"I'm not sure if the question suits to this subreddit but I generally wanted to build a observation, mars rover be like robot which should be able to do object detection by a csi port nightvision camera which I already got.

But that shouldnt be all, it should also include the opportunity to expand its knowledge on the go so I dont have to train its model at home on my laptop.

I already heard of the intel neural compute stick 2 which could be used with the raspberry Pi 4 with 8gb ram which I also have.

For now my question is, would the neural compute stick be enough for that ""mobile model training purpose""? Or should I get some other hardware?",LearnMachineLearning
rsq1s1,1640942347.0,How do load deep learning Trained model.pt into Cluster,Does anyone give some understanding of trained_model file (.pt) to load into a cluster like Apache spark or airflow for job scheduling?  If you have any resources/repo to share will be highly appreciated.,LearnMachineLearning
rsm6ec,1640927596.0,Error loading image,"I have installed all the necessary libraries but am still unable to figure out the problem.

&#x200B;

https://preview.redd.it/e3x49vvzat881.png?width=1056&format=png&auto=webp&s=c44136218d17475b6f0bd325ae370813b90f9ec8

Please help me to figure out the problem.",LearnMachineLearning
rse1fs,1640903173.0,How do i know which columns to drop?,"I use python and I'm reading ISLR. I learned the importance of checking the p values of each predictor to decide wheter its worth keeping or not.


Since there's no straightforward way to do this in python without statistics libraries like statsmodels, what do you usually do to decide whether a column of data is worth keeping?

Do I have to learn statsmodels to make an educated decision rather than doing trial and error?

Thank you",LearnMachineLearning
rsbm0e,1640896840.0,Cool projects for college app,"I've just finished Andrew Ng's beginner ML course on Coursera. I was wondering if you guys had any project ideas that you enjoyed making or that you think might be an interesting challenge

I'm willing to learn a lot and really struggle on the project. So really any idea goes 😃

Thanks",LearnMachineLearning
rs8a2z,1640888293.0,How to create retraining protocol/pipelines for machine learning models?,"Specifically in the case of computer vision models using CNNs, inevitably you will encounter data that the model is not well suited for. One could keep a log/store of such data points to use for later training, but what is a justified way to retrain models over time? Especially in the case of something like face detection where the data distribution does not necessarily change over time, but rather there are simply certain points that are underrepresented in the original training data. Let's assume that the model already performs acceptably on most data points.

What are the trade-offs of

1) retraining on the whole dataset, including those erroneous samples with good labels

2) training the model (perhaps with early stopping and validation on the original+new datasets) only on the erroneous samples

3) training on the whole dataset, preferentially sampling the erroneous samples

&#x200B;

Is there a name for this process? Where can I find best practices and theoretical discussions of this kind of process? I would appreciate any help as this is a real issue that I wish to solve with a deployed model.",LearnMachineLearning
rs5cjb,1640880755.0,Introduction to Machine Learning for beginners.,"Hello! We are a group of students which has made an introduction for Machine Learning for people who doesnt know anything to teach them the basics.

We have [4 models, full explained in english](https://drive.google.com/drive/u/1/folders/1AhDTdOdAhME8V_KOXb5L8j1NhFhPwdh2), with lots of comments, which can be used to test online and know the potential of ML.

Also we have some [videos](https://www.youtube.com/watch?v=EtxEhLcQGso) (in Spanish) where we explain everything, from which is machine learning, until what dangers does it have through the 4 models provided and its uses in some fields like medicine or social networks.

I hope you find it useful and that it encourages you to enter this world!!",LearnMachineLearning
rs31ic,1640874486.0,What is L2 distance? A 2-minute visual guide.,"&#x200B;

https://preview.redd.it/acqcmybiwo881.png?width=2048&format=png&auto=webp&s=b7b4177ef9957ba7e8cc8bae439e9646ba39e940

https://preview.redd.it/o2zb8aciwo881.png?width=2048&format=png&auto=webp&s=77c0d5b5af54ff926c6375695eb36c2233075aeb

https://preview.redd.it/h4k3qcciwo881.png?width=2048&format=png&auto=webp&s=6d1ff876c16c25d984cd0f3cab13a709b7a182d4

🔵 Euclidean distance 🔵


🚀 When we talk about ""distance"" we generally mean Euclidean or L2-distance as this is the one that is commonly taught in school and Euclidean geometry has a wide variety of applications.


👽 The L2 distance is computed using the Pythagorean theorem and the cartesian coordinates of the points in space. L2 distance is the p-norm when p=2. But of course like all other distances it can be applied to measure the closeness of two objects in higher dimensions as well.


🔷 About the squared circle. Since many shapes and concepts in geometry involve distance. The way you define this distance can have an effect on this and its properties. When you choose the distance to be defined by L2 distance you get the familiar round circle but when you do that with the L1 distance you get a rotated square as a circle.


🤓 L1 and L2 norms are used to perform the regularization of parameters of a model. If you have a neural network or a simple linear regression model you can regularize to prevent overfitting. Lasso and Ridge regression use L1 and L2 regularization respectively.
\---------------------------------------------------------------------------------


If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",LearnMachineLearning
rs0vl3,1640867993.0,Decent summer college programs offering machine learning programs for high school students?,"I’m a high school student interested in computer science, specifically AI and machine learning. I am currently learning Tensorflow although I’m still a beginner.

Next summer, I wanted to go to a college summer program that preferably offers college credit. However, I can’t find any related to AI/ML that offers credit.

If anyone knows of any or has suggestions, please let me know.

Also on a side note, how realistic is a high school student getting a medal on Kaggle?",LearnMachineLearning
rrxtnj,1640856934.0,Question about ML approaches,"Hello guys,

A bit of context:


I am currently searching for a way to estimate the sex (M/F) and he age slice (Kid/Young adult/Adult/Old) of a person based on a full body picture **with mask**. (80%+ success would be enough)

I have found many approaches giving a sufficient result analyzing the face without mask, but based on the current situation, this is out of the question... (The result on masked people are obviously terrible)


Question:

Are there existing repositories out there solving this particular problem?

And if there are not, as I am a beginner, do you believe that problem could be solved by training a model? (I.E: Will I be able to *get a working solution* after learning how the whole process works)",LearnMachineLearning
rrxl86,1640856093.0,Any self-taught ML engineers from a NON-CS background?,"currently in my 2nd year of a mechanical engineering PhD. The research involves a lot of machine learning (been self teaching concepts to do my research). But I’ve been thinking of mastering out because I just want to work already — tired of this student budget.

So I’m wondering if anyone from a non CS background has self-studied their way to an ML engineer role? How’d you do it and how long did it take you?

Thanks!",LearnMachineLearning
rrwjyk,1640852272.0,Training U-Net Model in Pytorch," hi guys i'm trying to implement unet model in pytorch i have done it with tensorflow and it works fine but the pytorch implementation doesnt seem to work can you check my model and train\_fn

[https://pastecode.io/s/tonibg0o](https://pastecode.io/s/tonibg0o) model.py

[https://pastecode.io/s/u8moqmca](https://pastecode.io/s/u8moqmca) train\_fn

i'm using nn.CrossEntropyLoss() as the loss function

there is no error in the code just my results are pretty bad some model with the same kernel size and same number of layers in keras works just fine",LearnMachineLearning
rrsjbi,1640838760.0,List of Statistics Topics Need for Machine Learning,"Hello everyone,

I've been endlessly googling this and I can't get a definite answer. Can someone please tell me a list of topics in statistics I should know to get started with machine learning.

And will further statistics be learned while one learns machine learning or does one have to specifically learn more stats topics before moving onto the next phase of learning ML?

Thank you so much!",LearnMachineLearning
rrnctu,1640823914.0,Has anyone had Azure Associate Data Scientist Certificate?,What is your experience? And do you think it would boost a resume who is trying to break into to the field?,LearnMachineLearning
rrn7xe,1640823538.0,Data/image loading help for PyTorch,"Hey guys,

So a couple months ago I posted about making an image classifier for a school project. So far I have the project mainly working, albeit with a pre trained resnet model from online. Now I need to replace that with a custom built model in order to make the project more complex.

I’ve decided on the caltech256 dataset as that seems to be a decent size while still being manageable to process on a laptop. I’m looking into how I can load the dataset into my program as that seems to be the first step and I’m at a bit of a dead end really. I can’t seem to get the Torchvision class for caltech256 to work no matter what I try. (I’m fairly new to python so I imagine that doesn’t help) I also had a look at trying to do it manually but I’m not sure how doable that will be.

If anyone can guide me in the right direction that would be much appreciated. I would prefer to use less libraries if possible but beggars can’t be choosers and if Torchvision.datasets is a lot more beginner friendly, then that’s fine too.

Thank you!",LearnMachineLearning
rrlgt6,1640818934.0,What is the best resource for learning Reinforcement learning?,"Can anyone suggest any resources to learn reinforcement learning, preferably with Python examples?",LearnMachineLearning
rrlgee,1640818904.0,Fun and Simple Cat/Mouse Challenge: How would you solve this?,"[Numberphile Link](https://www.youtube.com/watch?v=vF_-ob9vseM)

Let's assume that we're not clever enough to come up with the solution presented in the video.  How could we derive a solution with ML?  Which algorithm(s) would you choose?",LearnMachineLearning
rrlenc,1640818789.0,Should we do PCA before running Naive Bayes?,"For achieving conditional independence, the features need to be independent and uncorrelated. Would applying PCA give better results with Naive Bayes model or are there any other ways to make this model more accurate?",LearnMachineLearning
rrk8j1,1640815746.0,I am interested in mentoring 1 person who’s motivated getting into DS.,I work as a Data Scientist in AI company and also finishing my MSc DS studies.,LearnMachineLearning
rrjqt5,1640814522.0,How to find intrinsic dimensionality of a pretrained network?,"Hello,

I have a image dataset for classification task. I am using transfer learning with some pretrained networks in Pytorch. I want to find the intrinsic dimensionalities of the deep features. However, I don't know where to start, I even don't know whether I defined the problem properly or not :)
Any suggestion would be nice.",LearnMachineLearning
rrhhkh,1640808842.0,multi-class classifier,"I have trained 100 binary classifiers to classify 100 classes separately. Now I want to add all these trained model into one model to classify 100 classes. Is it possible? If yes how can I do it?

Thank you in advance.",LearnMachineLearning
rrh38q,1640807871.0,Sample Size Justification for ML?,"I'm in a position where I need to write up a section on sample size calculations for an ML study (this is targeting a medical journal and they basically want a justification for the size of the final test set). The thing is that I haven't done anything approximating sample size calculations in this field before. The task involves using a deep learning algorithm to predict an angle measurement on an image (continuous) and comparing that prediction to angles manually measured by experts. The final metric we're targeting is ICC between the prediction and expert raters. We produce this metric on a hold out test set of images.

So my question is...how do sample calculations really apply here? I'm not trying to really prove any effect size. The only thing I'm attempting to show is whether the predicted angle agrees with the expert measures.",LearnMachineLearning
rrfeyx,1640803699.0,What kind of technologies and programming languages would it take to build an ML Ops platform like Valohai?,,LearnMachineLearning
rrd2tt,1640797853.0,Does Andrew Ng's ML course teach Octave?,"Hello everyone,

I'm not trying to ask a lazy question, but I tried to Google this and couldn't find an answer.

So should I quickly learn Octave before starting Andrew Ng's ML course or does he teach enough of it in the course?

Eventually I plan to move to Python anyways.

Thank you!",LearnMachineLearning
rrbth9,1640794651.0,PyTorch vs. TensorFlow in 2022,"PyTorch, TensorFlow, and both of their ecosystems have been developing so quickly that I thought it was time to take another look at how they stack up against one another. I made a [write-up](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/) comparing the two frameworks that I thought **might be helpful to those on this sub who are getting started with ML**!

The frameworks have much more overlap from a technical perspective than in previous years, so the comparison focuses on **practical** considerations, namely **model availability, deployment, and ecosystems**.

At the end, I give recommendations for which framework to use based on your use-case, like this one for those in industry!

[Recommendation Flowchart for Those in Industry](https://preview.redd.it/hkp8mgjjsh881.png?width=679&format=png&auto=webp&s=6e1ae84837a0a9513c6d5ec9af16f39abc5df7d6)

There are flowcharts for [**hobbyists**](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/#what-if-i%E2%80%99m-a-hobbyist)**,** [**total beginners**](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/#what-if-i%E2%80%99m-a-total-beginner)**,** and [**those looking for a career** change](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/#what-if-i%E2%80%99m-looking-for-a-career-change) that might be especially relevant to this sub!

**You can jump to the portion that's relevant to you with the above links, or read the whole analysis** [**here**](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/)

I'd love to hear your thoughts regarding which framework is best for your use case! Or maybe you've
ditched both frameworks and have started with JAX? Let me know in the comments!",LearnMachineLearning
rrbc6q,1640793420.0,What advice do you have for a highly motivated newbie from a barely technical background,"These are my options for now

**1. Take an AI and ML 6 month post graduate program where I’ll learn the fundamentals**. The curriculum looks like this:

- Intro to python, numpy, pandas, Exploratory Data Analysis, matplotlib, seaborn
- 5 statistics, hypo testing, probability courses
- Supervised ML, Unsupervised ML, feature engineering, ensemble technique, model deployment, model selection and tuning
- AI and Deep learning: neural networks, computer vision, NLP and recommendation systems

**2. Take a 6 month data science and data management systems post graduate program first.**

Some ML concepts will be taught in the 6 month data science course and I have the option of adding 2 elective AI classes in this as well.

*Question: Do I need in-depth comprehension of database systems and data science to understand the fundamentals of ML and AI?*

**3. Take a 5 month computer science for AI post graduate program**

Here you learn fundamentals of computer science
like abstraction, data structures, algorithms then of programming: Python, SQL, CSS, HTML.

Then you learn Python with AI
- graph search algorithms, adversarial search, knowledge representation, machine learning, reinforcement learning, neural networks, natural language processing and more


**So my main question here is: Holding other factors constant like (difficulty level) Is it feasible to just start with AI and ML, or will starting with data science help me understand AI/ML even better or will computer science do that instead?**

I have a long term goal (an impact I want to make in the health field) that requires the  use AI and ML to accomplish but do I need DS and CS first?",LearnMachineLearning
rrapbr,1640791720.0,Need help,"Hello everyone. I'm a beginner to ML. Do you have any tips or suggestions to start with? Also, please suggest some good online resources.",LearnMachineLearning
rr6l1h,1640779158.0,How to conduct Features scaling,"I'm working on a classification problem and wanted to normalize the variables before conducting machine learning (classification) algorithms so that all the training and test variables are scaled within a range of 0 to 1. I wrote the following code

    sc_X = StandardScaler()
    X_train2 = pd.DataFrame(sc_X.fit_transform(X_train))
    X_train2.columns = X_train.columns.values
    X_train2.index = X_train.index.values
    X_train = X_train2
    X_test2 = pd.DataFrame(sc_X.transform(X_test))
    X_test2.columns = X_test.columns.values
    X_test2.index = X_test.index.values
    X_test = X_test2


However, I got an error message as

# ""TypeError: float() argument must be a string or a number, not 'pandas._libs.interval.Interval""

    Any advice on how this problem can be fixed?",LearnMachineLearning
rr6eb8,1640778424.0,Can there be a set of questions that can give an end user an idea if machine learning can be implemented in his business case?,I'm looking to build ML as a service in my org and needed to understand if any use case validation questionnaire which answered can give us an idea if implementation of ML in this case is valid.,LearnMachineLearning
rr61kh,1640777110.0,💊Your daily dose of machine learning : different types of GANs,"> This is a series of posts that I post almost daily. I call them “your daily dose of machine learning”.

 There have been several types of generative adversarial networks (GANs) in the past few years. Here’s a quick summary of them.

𝐁𝐚𝐬𝐢𝐜 𝐆𝐀𝐍𝐬 : the first form of GANs where you have a generator and a discriminator competing with each other.

𝐂𝐨𝐧𝐝𝐢𝐭𝐢𝐨𝐧𝐚𝐥 𝐆𝐀𝐍𝐬 : extension of GANs where you have conditional sample generation. This allowed for controlling specific modalities for data generation (ex : generate a face with more or less beard).

𝐖𝐚𝐬𝐬𝐞𝐫𝐬𝐭𝐞𝐢𝐧 𝐆𝐀𝐍𝐬 : an alternative algorithm for training GANs where the Wasserstein distance was used and also other techniques like weight clipping. This made the training more stable.

𝐃𝐂𝐆𝐀𝐍𝐬 : CNNs were used instead of MLPs for image generation.

𝐏𝐫𝐨𝐆𝐀𝐍 : Progressive growing of GANs where we increment the generator and discriminator networks gradually. This helped generate high resolution and high quality images.

𝐈𝐧𝐟𝐨𝐆𝐀𝐍 : enabling GANs to learn disentangled representations to have more control over different aspects of the output (eyes color, nose shape, hair type, …).

𝐒𝐭𝐚𝐜𝐤𝐆𝐀𝐍 : GANs that can generate images from text.

𝐏𝐢𝐱2𝐏𝐢𝐱 : an image-to-image translation with conditional GANs. For example to turn real images into cartoonish images.

𝐂𝐲𝐜𝐥𝐞𝐆𝐀𝐍 : we got rid of the need to have pairs of images for image-to-image translation, which was the case for Pix2Pix.

𝐒𝐭𝐲𝐥𝐞𝐆𝐀𝐍 : an extension of ProGAN for generating high resolution facial images.

𝐑𝐂𝐆𝐀𝐍 : GANs for time-series data where CNNs were replaced with RNNs (recurrent neural networks) to accommodate for the nature of this type of data.

𝐓𝐢𝐦𝐞𝐆𝐀𝐍 : another time-series GAN where new techniques were introduced such as a stepwise supervised loss and an autoencoder.

Connect with me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***",LearnMachineLearning
rr4y9y,1640773013.0,Can overfitting pass unnoticed because of a small dataset?,"Let's say I have a small dataset. I trained and tested a model and the training set and test set accuracies are high and similar. Cross-validation accuracy is also similar and has low variance.

Is it possible to overfit because the dataset is small?",LearnMachineLearning
rr34rv,1640765901.0,Will freeCodeCamp math curriculums be enough to self teach math for start in ML?,"Are anyone was self teach math for start reading books on ML? What do you think about freeCodeCamp math curriculums on their YouTube channel: https://www.youtube.com/playlist?list=PLWKjhJtqVAbl5SlE6aBHzUVZ1e6q1Wz0v

I already googled on what math should I know to start learning ML:

- LinearAlgebra (No Bullshit Guide to Linear Algebra looks good for it)
- Calculus 1, 2 (limits, differential and integral calculus)
- Statistics and Probability

I don't have a big lapses of school algebra and introduction to calculus. I also reviewed some parts of which I was not sure I remember with KhanAcademy.

On the one side I understand that math is the same for whole World but is there big difference (with accent to my purposes) between learning Math with FCC curriculums and Prof. in university or college?

I understand that all research works at Google Deepmind and others like this are reserved for masters and PhD.
I am interested in machine learning as a tool for solving applied problems. Clasification, recommended systems, spam filters etc.

I have a grasp of common data structures and algorithms and some experience with leetcode (Shunting-yard algorithm for example)

PS Warmest wishes for the Holiday Season.",LearnMachineLearning
rqy3kd,1640748955.0,"Which data science/ML development processes would you like to see more automated (i.e., have a Python library for)?","What parts of ML projects do you find really tedious, and do you think should be wrapped in a Python library? Looking for project ideas.",LearnMachineLearning
rqwf4e,1640744048.0,Classification and Object Detection Question,"I have built an object detector that is detecting birds at my bird feeder. It is trained only for birds in general, and without any regard for species. In order to classify my data I am running a classification model on another computer with the data retrieved from the database of the detections.

&#x200B;

I am wondering if this is counterintuitive? Should I be trying to train my object detection model (YOLOv4 running on a Jetson Nano) to also perform the object detections with respect to species, or am I on point with developing a classifier on a more powerful PC to handle the species classification?",LearnMachineLearning
rqt6w6,1640734940.0,Google Colab's TPUs are training my fashion_mnist model about 5-6 times SLOWER than on CPU. Is there a reason for this and is there a way to make it run faster? Any and all help is appreciated. Thanks.,"\[SOLVED\]

&#x200B;

Colab Notebooks:

CPU:

[https://colab.research.google.com/drive/18YUwPUiDQtdHNrGdmw237Sp9NM3d\_5xB?usp=sharing](https://colab.research.google.com/drive/18YUwPUiDQtdHNrGdmw237Sp9NM3d_5xB?usp=sharing)

TPU:

[https://colab.research.google.com/drive/1331bi7u-BVWVEGpbeEJ0oOgg780Bmt3Q?usp=sharing](https://colab.research.google.com/drive/1331bi7u-BVWVEGpbeEJ0oOgg780Bmt3Q?usp=sharing)

&#x200B;

Update:

Thanks to some very nice users who were kind enough to help me, I managed to diagnose the problem. The reason why the model was training slower was because I was not passing enough data through at once. This is easily fixed by changing the model.fit function from:

model.fit(train\_images, train\_labels, epochs=10)

To:

model.fit(train\_images, train\_labels, epochs=10, steps\_per\_epoch = 60)

in my scenario. Thanks to all the people in the comments who helped.",LearnMachineLearning
rqq3xq,1640726672.0,Which of These 2 School Courses Would be Most Useful for Machine Learning?,"Advanced Linear Algebra -  Eigenvalues, eigenvectors, diagonalization. Orthogonality, SVD, complex matrices, infinite dimensional vector spaces, and vector spaces over finite fields.

Probability III -  Finite Markov chains; stationary distributions; time reversals; classification of states; classical Markov chains; convergence in total variation distance and L2; spectral analysis; relaxation time; Monte Carlo techniques: rejection sampling, Metropolis-Hastings, Gibbs sampler, Glauber dynamics, hill climb and simulated annealing; harmonic functions and martingales for Markov chains.

(Of course, I have taken introductory courses on lin alg and probability up until these topics)

Thanks",LearnMachineLearning
rqjpre,1640709183.0,Does the negative of the gradient always go downhill?,"In gradient descent or SGD, is it always the case that the negative of the gradient goes in the direction of steepest descent?",LearnMachineLearning
rqideg,1640705442.0,Projects for CS Undergrads,"I'm a soon-to-be-CS-grad trying to get an ML Engineering job. I know I  need projects for my portfolio but I don't know how to go about them. I  have some experience with CNNs and fully-connected NNs. I'm thinking of  making a basic dog/cat classifier for my first project, but I figure  that my employer will just suspect I copy-pasted someone elses code  since this type of project is so typical. How do I make a differentiated  project? Should I just code up a neural network in numpy to show I know  how it works?",LearnMachineLearning
rqi2zq,1640704628.0,quickly train classification model for text classification," I want to use a ml for text classification for my app

I have different sentences which I want to classify by there content / not sentiment

with GPT 3 api  I just need to provide   100+ sentences + there clarification for each classification --> done

 downside cost money to use the model on there api

I looked into deploy own model on firebase which is pretty easy

is training your own model much harder then the descriped process on a gpt 3 model   ?

would appreciate any tips or helpful links

(I dont want to learn everything from scratch justy quickly train and deploy the model)",LearnMachineLearning
rqi1qq,1640704531.0,Online resources that explain while coding,I kinda want a resource that will teach me machine learning while coding and explaining out cause I feel like most resources that have been recommended feel like I can get bored easily,LearnMachineLearning
rqfx72,1640698209.0,Video's explaining technical side of algorithms,"Hello everyone,

In a few weeks, I have an exam coming up about Machine Learning and Inductive Inference. In this course, we look at the more technical aspects of learning techniques (like Naive Bayes, Support Vector Machines, theta-subsumption etc.), and the mathematical concepts behind them. However, the class was rather vaguely explained. My question for you is: are there any good websites where I can find videos on these more technical details (aside from Youtube)? Thanks!",LearnMachineLearning
rqfcx3,1640696431.0,Isolation forest question,"Do you agree that outlier fraction and a priori formulation of that parameter is main drawback of Iso forest algorithm? It demands very presice knowledge of anomaly appearances:
 https://link.medium.com/YQpVLdT8lmb",LearnMachineLearning
rqeg9k,1640693414.0,The best resource to learn and implement knowledge Graph?,"I have Facebook groups data and have to create knowledge Graphs for the data, Please recommend me best source to learn about how to implement it.",LearnMachineLearning
rqe2bt,1640691992.0,"After Python and R, which of the following language is most worthwhile to learn for ML?","C++, Scala, Java, Perl, and I’ve seen JavaScript and TypeScript used before",LearnMachineLearning
rpqwao,1640621782.0,Federated Learning for Mobile Keyboard Prediction,"Ever wondered how your mobile keyboard gives you the next word suggestions? How do they give personalised suggestions, while at the same time ensuring the privacy of individuals?

Check out my blog post ""Federated Learning for Mobile Keyboard Prediction"", which talks about how this happens, in a privacy-preserving manner.

Blog Post - [PPML Series #3 - Federated Learning for Mobile Keyboard Prediction](https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/)

Annotated Paper - [Annotated-ML-Papers/Federated Learning for Mobile Keyboard Prediction](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/PPML/Federated%20Learning/Federated%20Learning%20for%20Mobile%20Keyboard%20Prediction.pdf)",LearnMachineLearning
rpo9he,1640614004.0,Creating an optimization algorithm for cost function for NN, Is possible to find an article or an example of a new optimization algorithm for cost function for NN?,LearnMachineLearning
rpnptp,1640612234.0,Product Preference Dataset,"I'm looking for a dataset for some kind of consumer product (e.g. soda) with a lot of product features (e.g. color, sugar content, bottle color, etc.) that can be used for predicting personal preference. Any ideas? I welcome any sort consumer product, but soda seems to be the easiest.

The goal is to give my father a hands-on demonstration of the power of NNs and how we can observe correlations in inane properties.",LearnMachineLearning
rpneke,1640611207.0,Any ML study group or meet-up working in projects/ research papers.,"Hello,
I am an Indian student who has done BE in Electronics from BITS. After my 1 year stint as SDE, I am doing MTECH in Computer Science at IIT Hyderabad.

I have done 2-3 official courses in machine learning besides the infamous Coursera course and Deep Learning Specialization. Currently I am working in an official project which is 75% Computer Networks and 25% Machine Learning.

However, my current interest is towards ML. I would love if there is a study group that is working in ML based projects or research papers where I could contribute. Or even study group that primarily discusses ML based concepts.

Thank you!",LearnMachineLearning
rpmcot,1640607553.0,collection of free python courses,"hi guys

i collected for you some free python from udemy, hope that can help every one to start learning

[Python for Data Science – Great Learning](https://myfreecoursesonline.blogspot.com/2021/12/python-for-data-science-great-learning.html)

[An Introduction to Python Programming](https://myfreecoursesonline.blogspot.com/2021/12/an-introduction-to-python-programming.html)

[Learn Python 3 From Scratch | Python for Absolute Beginners](https://myfreecoursesonline.blogspot.com/2021/12/learn-python-3-from-scratch-python-for.html)

[Practical Python](https://myfreecoursesonline.blogspot.com/2021/12/practical-python.html)

[Python OOP : Object Oriented Programming in Python](https://myfreecoursesonline.blogspot.com/2021/12/python-oop-object-oriented-programming.html)",LearnMachineLearning
rplzc6,1640606197.0,Need help on competitions related to autonomous driving?,I want to build my career as an autonomous driving/flight engineer. What are some competitions/ forums/ opensource projects I can use? Something that is good for my resume as well.,LearnMachineLearning
rplc4j,1640603770.0,Get stuck in Linear Algebra,"# Hi everyone, i am a newbie in machine learning and now i'm having some stuff at learning Linear Algebra for machine learning because Linear Algebra takes lots of time to learn. I have read that only some contents of Linear Algebra is useful for Machine Learning, so can you guys recommend me some books or course for learning Linear Algebra for Machine Learning ?

P/S: sorry for my bad English, thanks for your help",LearnMachineLearning
rpkn6w,1640600984.0,Starting route for beginner,"Hi guys,

I have completed my bachelors in mechanical engineeering.I have taken classes like calculus,statistics,probability,lineer algebra and differential equations and also i have good knowledge about python and its libraries.But i think i need to take these concepts again that is why i thought i would start with according to this person whom i follow [https://github.com/merveenoyan/ml-roadmap](https://github.com/merveenoyan/ml-roadmap) (i am a beginner so that road map kinda covers (?) the general topics about ML) :

\-Python Data Science Handbook ESSENTIAL TOOLS FOR WORKING WITH DATA  Jay L. Devore   (for python libraries)

\-Probability and Statistics for Engineering and the Sciences books (for remembering the statistics and probability concepts)

With these books i thougth i would learn the exploratory data analysis and feature engineering.

After learning these i can go for Machine Learning concepts?

Do you think that it would be a good route? or do you have any suggestions?",LearnMachineLearning
rpjnzd,1640597162.0,Generalized sequence from a set of similar sequences?,"Hello, I have a problem. I have some sequences like

AAABBBCCC

AAABBBCCD

AACBBBCDCC

XAABBBXCCC

These sequences will be similar.

What I want to do is to create a general sequence from them like AAABBBCCDC. I learned about Sequential Pattern Mining and PrefixSpan Algorithm but I am not sure if I am on the right track.

Is there any algorithm where I feed these sequences and I get a generalized sequence or something close to it?",LearnMachineLearning
rpirvh,1640593633.0,Machine learning use cases in telecom industry,"Apart from churn prediction, customer segmentation and anomaly detection, how is data science used in the telecommunications sector? are you aware of industry/research use cases in that field?",LearnMachineLearning
rphw9x,1640590322.0,Good model for seismic signal?,"What would be a good model for a regression problem where the inputs are seismic signals (in CSV form)? Specifically, something (but not quite) along the lines of earthquake prediction from seismic signals.

Thanks!",LearnMachineLearning
rpg57g,1640584108.0,YOLO v5 inference speed slower on exported ONNX model compared to PyTorch model?,"Hi everyone, I've been using the official PyTorch [yolov5](https://github.com/ultralytics/yolov5) repo to perform some object detection task. I have trained the model using my custom dataset and saved the weights as a .pt file. I also exported the weights as an onnx model as well using [`export.py`](https://export.py) in the repo.

&#x200B;

Running [`detect.py`](https://detect.py) using the .pt weights, the inference speed is about 0.012 seconds per image. However, the onnx weights require 0.2 seconds per image. I have installed onnxruntime-gpu and I am sure the GPU is utilized when I run the onnx model.

&#x200B;

Is the slower inference speed a known behaviour for onnx models?",LearnMachineLearning
rpfuh7,1640583111.0,Are differential geometry topics like manifolds usually used with GNN’s?,"I am interested in using differential geometry for a project-based ML course that I’m doing next semester. I need to know the place to start learning about this type of ML and whether or not that place is GNN’s.

I think differential geometry is a really cool topic in math. I want to do an ML project that involves this math because of the interest and it also seems like geometry is becoming more prevalent in ML so I want to start learning more about it during this course.

What I’m wondering is if GNN’s are the right method to start looking in to if I’m interested in using manifolds in ML. Is a GNN the best tool to allow me to analyze information embedded in a manifold? (don’t know if this is the right terminology)

If you think using manifolds and differential geometry in a project is too advanced for someone that doesn’t know much about it yet then please tell me and any suggestions on where to start are appreciated.

Thank you in advance for any help.",LearnMachineLearning
rpcwbh,1640573841.0,questions about the MITx courses on probably and statistics,"I am interested in taking the introductory course on probability offered by MIT through edX (see link below):

[https://www.edx.org/course/probability-the-science-of-uncertainty-and-data](https://www.edx.org/course/probability-the-science-of-uncertainty-and-data)


I am also interested in taking the introductory course to statistics following the probability course listed above (see link below):

[https://www.edx.org/course/fundamentals-of-statistics](https://www.edx.org/course/fundamentals-of-statistics)

But it seems like both courses require students to be familiar with single-variable and multi-variable calculus. The statistics course also requires student to be familiar with vectors and matrices. I've only taken calculus 1 in my first year of college and that was more than 10 years ago.

I would prefer to get the certificates for these courses but I am afraid that I might not be able to complete them. For those that have taken these 2 courses. Will I be able to do well in these courses without the prerequisites?

Thanks in advance",LearnMachineLearning
rpcifl,1640572621.0,Visual ads data set for Pinterest visual ad recommendation project,"Hello folks, need your help!

I am currently working on a **Pinterest visual ad recommendation project: knowing people's pins, what ads/categories of ads we can recommend them?**

I found a nice with 2million visuals data set with users and their pins. However, I am struggling to find the visual ads category data set.

I would appreciate any pieces of advice on where to find such datasets or any APIs or just ideas on how to approach my search.

Thank you in advance.",LearnMachineLearning
rpbgpz,1640569390.0,A Simple Post for Beginners in Machine Learning and Scikit Learn - Prepare Data for Machine Learning using Scikit-Learn,"[Prepare Data for Machine Learning using Scikit-Learn](https://debuggercafe.com/prepare-data-for-machine-learning-using-scikit-learn/)

[https://debuggercafe.com/prepare-data-for-machine-learning-using-scikit-learn/](https://debuggercafe.com/prepare-data-for-machine-learning-using-scikit-learn/)

&#x200B;

https://preview.redd.it/ogw0ie3kpz781.png?width=1200&format=png&auto=webp&s=1b0126f9226472a6ade30612599b8b8d653b6ad4",LearnMachineLearning
rp7qdv,1640558425.0,[Project] Advice for Recommender System that generates useful insights from random datasets,"Hi,

I am currently designing a recommender system that will generate useful insights from random datasets for my course of study. I have two specific problems for which I would like to hear some other people's opinion.

**Giving an example to sketch some context:**

Let's say I supply a dataset with data of all tests done by students in a class. In the dataset it appears that a certain student has a higher average score than the rest of it's class. The recommender system could recommend to the end-user (eg. a teacher) to plot this insight in a bar chart to make it clear to the end-user that this student scores higher than the average.

This insight could be described as follows: The combination of 1 *dimension* (student ID) with 1 *metric* (score) that is *aggregated as average* (take the average for this student), the average of the metric of *datatype double* is significantly higher than the average of the entire column, *outlier* (insight type). The five properties of the insight I have just described (1 dimension, 1 metric, metric datatype, aggregate as average and outlier) could be seen as the features of the insight. (Just like a movie might have a genre, year of production,...)

This is a simple example but you can go quite far, like allowing combination of 2 dimensions with 2 metrics,...

**My idea for the system at this moment:**

1. Intake a given dataset
2. generate 'the recipe' for all possible insights (limit amount of dimension-metric combinations)
3. check for all these insights what the insight type is, if there is one. eg. outlier, trend reversal,... You could also quantify these types as an outlier can be X% higher than the rest of your column but let's not take this quantification into consideration right now.
4. remove spurious insights like 100% correlation, low outliers,...
5. rank the insights that remain with the recommender system. There are only a finite amount of insights possible so the insights are all predefined and it should be possible to match insights across datasets.

**My questions:**

In a lot of classic Recommender System examples, movies are recommended to users. I like to compare these movies to my insights and the users to my datasets when trying to find the analogy between my problem at hand and the classic movie-user example.

1. **I do not know if the user-dataset analogy is a good one.** The dataset and it's columns have their own set of features (amount of columns, amount of dimensions, kurtosis of certain metric, variance of certain metric,...) and I would like to recommend the same insights for other datasets who have similar values for these feature. But this is not what is done in Collaborative Filtering right? Normally, in the classic example, a user is defined by it's ratings on movies, not it's inherent features (age, country of origin,...)
2. A new dataset in my system won't have any ratings for insights so I am looking for a solution where I would use the dataset's distance from other datasets in the dataset-space to recommend insights that were useful in other datasets with similar features to my new dataset. Does anyone have **any ideas/links/tutorials/readings to solve this issue**? I think this is a little more complicated than cold start problem because I would like to always take into account the dataset features, not only during the start. Because of the nature of the system, recommendations would also only be done once, the first time a user uploads a dataset. so there isn't really any way recommendations for a certain dataset would become better over time, after a user has selected or rated insights.",LearnMachineLearning
rp7kc6,1640557924.0,Seeking internship,"Hey everyone,

I hope you are all doing fine.

I am Ahmed Abbassi studying computer science and electrical engineering at Bilkent University for the spring semester of 2021 as an Exchange student.

I wanted to ask you all if you could help me get an internship(anywhere, even remotely) in the field of AI.

Below I wanted to give a brief introduction about myself. I am from Tunisia, that is where I did my pre-engineering studies (Math and Physics) where I ranked 49 out of more than 2000 students in a nationwide exam. This allowed me to enroll in one of the top public engineering schools of Tunisia, The higher school of communications of Tunis where I worked hard to get selected from the top students for exchange program opportunities and that is where I chose Turkey as my destination.

Aside from that, I worked on numerous projects from writing machine learning algorithms from scratch all the way to building full-scale models and even scraping the data myself when needed.

If you need my cv just click [here](https://drive.google.com/file/d/1DTDE81Elj2jMVfnc5t73cMGKM6OIqqP3/view?usp=sharing).

&#x200B;

Thank you all <3",LearnMachineLearning
rp61rj,1640553675.0,Machine Learning / Data Scientist role fully remotely - no location restricion,"Hi ML Community,
What are your experiences with working completely remotely outside your country?
For example: I live in England and work fully remotely in Portugal...
Do any of you have similar experiences? I am interested because I want to find a job outside my country.
If someone has similar experiences, it would be nice to share their stories about:
1.the employer's willingness to hire in this way,
2. matters related to the employment contract,
3.tax matters,
4. And the most important: where is it best to look for such positions: any portals/websites?",LearnMachineLearning
rp3926,1640544853.0,Vehicle detection and counting,"Intelligent vehicle detection and counting are becoming increasingly important in the field of highway management. However, due to the different sizes of vehicles, their detection remains a challenge that directly affects the accuracy of vehicle counts. This simple project treats this issue using OpenCV for image processing and Hear cascade which is used for object detection.

[https://machinelearningprojects.ml/2021/12/project-vehicle-detection-and-counting](https://machinelearningprojects.ml/2021/12/project-vehicle-detection-and-counting)",LearnMachineLearning
rp1af8,1640539236.0,Need help in deciding an approach,"

I'm currently working on a personal project and have encountered a problem which is keeping me up and I'm having trouble in deciding what approach should I take. I cannot reveal the whole problem but it is similar to this one:

I have the data for varying weights of lets say 100 people with time and I need to predict the time after which the weight of majority of people would be more than a certain threshold.

Any suggestions on how I should approach this would be great :)",LearnMachineLearning
rp0t4m,1640537855.0,Stacking models,I am trying to stack multiple models. I am using stacking regression from sklearn but the accuracy of the stacked model is not better than the individual models. Is there any other way or what can I do to increase my accuracy?,LearnMachineLearning
royf9n,1640530463.0,Bit confused for Grocery Purchase recommendation approach,"So I got an assignment to make a Grocery Purchase recommendation system to maximize sales by recommending items that were popular sellers with those already in cart.

After a bit research, i found that my problem lies in Market Basket analysis and Apriori algorithm was the top recommendation. but after reading [this paper by amazon about item-item collaborative filtering](https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf), it occurred to me that Apriori approach would not be able to suggest items/item pairings which are not above its support level (I can't keep the support too low as it would then go against Apriori's key goal of keeping search terms less to process faster).

So should i go with a approach which makes a list for each item and its popular pairings to ensure that my code will at least recommend a related sale of the item to the customer or is there a faster and more time and space efficient implementation for the same problem?

Any insight related would be appreciated!",LearnMachineLearning
rovzdh,1640521516.0,Am I correctly learning the DNN model?,"Hi. I'm new in the machine learning field. I wrote a simple neural network using Tensorflow for discovering heart diseases using data from here: [https://www.kaggle.com/fedesoriano/heart-failure-prediction](https://www.kaggle.com/fedesoriano/heart-failure-prediction). Is it a correct way how professional data scientists solve such problems? Thanks a lot for the feedback.

Please, tell me something if I'm doing wrong. My solution is [here](https://github.com/zaxoavoki/machine-learning/blob/master/neural_networks/heart_diseases.ipynb) on Github.",LearnMachineLearning
rovtsh,1640520841.0,"[Research 2021] Looking for interesting ML papers to read for the break or the new year? Here is a curated list I made. (with video explanation, short read, paper, and code for each of them)","The best AI papers of 2021 with a clear video demo, short read, paper, and code for each of them.

In-depth **blog article**: [https://www.louisbouchard.ai/2021-ai-papers-review/](https://www.louisbouchard.ai/2021-ai-papers-review/)

The full list on **GitHub**: [https://github.com/louisfb01/best\_AI\_papers\_2021](https://github.com/louisfb01/best_AI_papers_2021)

Short Recap Video: [https://youtu.be/z5slE\_akZmc](https://youtu.be/z5slE_akZmc)",LearnMachineLearning
rofn2t,1640460043.0,Looking for some direction on a project that I'd like to do for my capstone project,"I will be doing a capstone project this semester and I would like to do a project with neural representations. I am interested in the differential geometry side of math and recently I have been learning more about how manifolds can be used in ML for representations.

I'm going to do a project using manifolds and time series data for the project. I will be using time series data like audio signals or EEG/MEG brain signals. I am going to see if it's possible to do the project with a professor who would be able to help me through this but as far as I know I haven't seen any professors at my school that specialize in neural representations using manifolds so the initial guidance for how to get started with something like this would be extremely helpful.

An example of what I'm talking about is [this paper](https://www.nature.com/articles/s41593-019-0460-x) ([or this video](https://www.youtube.com/watch?v=QHj9uVmwA_0)) on neural circuits constructing representations. I'd like to do something like this and I don't really know where to start. I can learn quite a bit during the semester but I would like to get the basics down and have a direction for what kind of machine learning I'll need to do before the semester starts at least. My project doesn't have to be as complex as this paper but something at least in this direction is what I want to do.

Would anyone be willing to provide some guidance on how to work towards building a project like this this semester? Any advice on how to start learning about this area or any resources that would give me a framework to start with for this kind of ML?

Any help would be greatly appreciated!",LearnMachineLearning
rodati,1640451868.0,Looking to map which skills I need to pick up to implement an application idea,"Hello there. I'm currently doing the Andrew Ng machine learning course and I'm close to finishing it.

I have an idea for a little project which I've been toying around with. Without going into too much conceptual information, the rough idea is to allow multiple users to input some textual information to a website, for the website to generate some summaries of identified objects, and for the website to detect contradictions between provided information.

An example:

User 1 - Ravens are birds

User 2 - All pets are mammals

User 3 - I have a pet raven called Jack whose feathers are white

User 4 - All ravens are black

The user should be able to tell the application to track all information about the detected noun ravens. The application should store all the information it knows about ravens and parse some sort of semantic understanding. It should detect some second and third order contradictions too through some inference.

The application should detect and flag the contradictions between the facts introduced by all the users and flag them for resolution.

I'd like to identify the holes in what I need to know. I'm a C++ developer but NLP is very new to me.

Does anyone have any good resources or courses which are particularly applicable which I could take to point me in the right direction?

Thanks!",LearnMachineLearning
robzib,1640447224.0,A model that can learn and replicate the art style of a Twitter artist...is this real? ¿Where can I find It?,"For example: characters

My request can also be explained following these steps:

1. Download all the artwork from a Twitter account.

2. Use one sample of Twitter art as a model output for a set of inputs (5 to 10 images of a character in its original/basic art style).

3. Let the model find the relationship.

4. Repeat steps 2 & 3 but with other samples (characters) as much as you want.

5. Once you find it appropriate, start submitting a set of inputs (5 to 10 images) of a character that has never been considered by the artist.

6. The model, after learning from past processes, generates a new output that resembles the artist's style.


Hope you get the idea.

However, please consider that I'm still new to this area, if there's something you think I'm missing here, please let me know.


Merry Christmas",LearnMachineLearning
robe2y,1640444975.0,Master project ideas,"Hi All,

For my masters project, I am looking for some ideas for final project. My aim is to nail down DS concept and also get exposure to Deep learning and AI algorithms. I want to build an end to end solution ( I have BI and data engineering background). Initially, I nailed down three broad areas for reserach.. Remote sensing, cybersecuity and algorithmic trading although I am open to other areas as well, as long as my basic criteria is met (learning and easy availability of data). Any suggestions on topic would be appreciated?


PS: I am not afraid to take some complex assignments as well.",LearnMachineLearning
roaey3,1640441255.0,Getting overwhelmed,"I wanted to ask you guys whether it is normal to get overwhelmed while learning Machine Learning.
I have been studying since almost last 6 months and I feel like I am getting nowhere. I did Andrew Ng's Machine Learning course and also a course in my university which was based on Linear Regression with sci-kit learn and CNNs with Tensorflow. Inspite of all that, I really feel lost when I read a lot of stuff on this subreddit and also on KenJee's discord.

I almost understand everything but sometimes there are things that I don't understand at all and I worry that I am far behind and if I will ever get to the point to be confident to do projects etc",LearnMachineLearning
ro9ti6,1640438941.0,AMD Hardware (deep learning related question),"I have a Ryzen 5 3500U, with Radeon Vega 8

I heard deep learning requires GPU. I wanted to know what is an alternative for CUDA (as it is for nvidia only) ?",LearnMachineLearning
ro8oym,1640434133.0,I want to solve Andrew Ng Machine Learning Course Assignments using python,"I am Confused which Library to use
As there are Number of libraries Scikit learn , Pytorch , Pandas , Numpy , Tensorflow  etc
I don't know about these
Please Guide me which one is easy for beginners?",LearnMachineLearning
ro1s0p,1640404273.0,What is the minimal requirement to classify a neural network? For instance if im classifying oranges could i use just one neural network image or do i need thousands?,,LearnMachineLearning
ro1ke3,1640403441.0,How do you make a neural network classified fruits,,LearnMachineLearning
rnyeyl,1640391791.0,Free Python courses from udemy,"

[Learn Python 3 From Scratch | Python for Absolute Beginners](https://myfreecoursesonline.blogspot.com/2021/12/learn-python-3-from-scratch-python-for.html)

[Practical Python](https://myfreecoursesonline.blogspot.com/2021/12/practical-python.html)

[Python for Data Science – Great Learning](https://myfreecoursesonline.blogspot.com/2021/12/python-for-data-science-great-learning.html)

[Starting Python 3 Programming for the Absolute Beginner](https://myfreecoursesonline.blogspot.com/2021/12/starting-python-3-programming-for.html)",LearnMachineLearning
rnu744,1640377716.0,Recurrent Neural Network for time series forecasting,"Hi! I have been given a challenge for the Artificial Neural Network course at my university which consists in forecasting 7 time series using a RNN. What are some state-of-the-art architectures I can try to improve the performance I achieve on such task?
Thanks a lot for the help.",LearnMachineLearning
rnr1vu,1640367657.0,I need some guidance.,"I have been into computer science for a couple of years now(3 to be precise), and till now I was just wandering here and there, trying every field(web dev, game dev, app dev, data science etc). Now I know that I want to make a carrier in data science. I don't know much math as I am just in 10th grade. I have prior programming experience with python, c++ and javascript.

Can anyone give me any good quality free learning resources for math.

I am currently learning statistics from the book 'Think Stats'",LearnMachineLearning
rnpl4x,1640363141.0,Object detection using rcnn,"Hello guys

I have to do object detection without using any pre trained models. It is very difficult to do it as I don't understand neural architecture. As rcnn requires a pre trained cnn so I trained a cnn network in cifar-10 but I don't understand how to plug it in rcnn architecture. So guys can somebody guide me with explanation or a link for reference.
Thanks a lot for reading and helping",LearnMachineLearning
rnp2vk,1640361587.0,Tips & Tricks of Deploying Deep Learning Webapp on Heroku Cloud - KDnuggets,"Hi guys, I wrote this blog because I was facing a lot of problems in deployment, so I created guide for deploying deep learning models on heroku. I hope you guys can avoid making common mistakes and please like and share, it will mean a lot.  [https://www.kdnuggets.com/2021/12/tips-tricks-deploying-dl-webapps-heroku.html#.YcXp-E5F5AI.link](https://www.kdnuggets.com/2021/12/tips-tricks-deploying-dl-webapps-heroku.html#.YcXp-E5F5AI.link)",LearnMachineLearning
rnnynt,1640357986.0,Combining two different neural models,"Hey guys, I have two different neural models that were trained on two different datasets and for different tasks

I would like to adapt one of them using the dataset of the other one, but I have no idea how can that be done

I read about ensembling methods but I don't think they can be applied in this case since the datasets are different

Any help from you would be very appreciated!",LearnMachineLearning
rnnx7b,1640357846.0,Sound Event Detection using Machine Learning (video presentation),"Want to learn how deep learning can be used to detect sounds?
Here is a practical introduction to the topic, which was given as a presentation at EuroPython 2021.
Did you know that sound event detection can be used to track the progress of beer fermentation? Learn more about it in the following video:

[Sound Event Detection using Machine Learning (Youtube)](https://www.youtube.com/watch?v=JrhsFfCOL-s)


Happy to take questions here!",LearnMachineLearning
rnkwp7,1640347140.0,Good beginner exercise for improving programming: Monte Carlo simulation of the approximation of number pi (π),"This is good exercise for understanding some basic data science concepts as Monte Carlo simulation.

[https://medium.com/@vitomirj/good-beginner-exercise-for-improving-programming-monte-carlo-simulation-of-the-approximation-of-838dc17eb6bc](https://medium.com/@vitomirj/good-beginner-exercise-for-improving-programming-monte-carlo-simulation-of-the-approximation-of-838dc17eb6bc)",LearnMachineLearning
rnke9u,1640344995.0,What are Parametric and non-parametric statistics? A 2-minute visual guide.,"&#x200B;

https://preview.redd.it/3l2hhhjt5h781.png?width=2048&format=png&auto=webp&s=7f39f3a2d5d4b00d05f914021b3cef884142be46

https://preview.redd.it/kr785mjt5h781.png?width=2048&format=png&auto=webp&s=37e16928a85b716b0c3ac7b4b0f8fcfd534b87e7

https://preview.redd.it/yhji7rjt5h781.png?width=2048&format=png&auto=webp&s=7663ee3d5e13b45cce8ec689febf8548b876eee3

https://preview.redd.it/fntqptjt5h781.png?width=2048&format=png&auto=webp&s=d034b139d29afefa9d20b2b413408143cb623c23

🔵 Parametric and non-parametric statistics 🔵

🧑‍💻 If you have written code before you have heard of a parameter. It is what a function takes as input to do some computation on and return an output. Similarly, probability distributions have parameters. These define the properties of the distribution.

🔔 In the case of a normal distribution, the mean and standard deviation are its parameters. The mean controls the position of the distribution while the standard deviation controls the spread or ""peakiness"" of the bell curve.

⛷️ In the parametric approach, the model consists of a finite set of parameters that characterize it. The big assumption that a parametric model makes is that the model will do well on the task if the underlying distribution from which your data is sampled matches the model. If that is not the case the model and data mismatch will hurt the performance on the task you care about.

🤸 A non-parametric model does not rely on parametric assumptions and is generally more flexible. It is a good choice if you don't have any prior knowledge about what could be a good model that reflects your data distribution. A histogram is a good example that can model points sampled from an arbitrary distribution.

🧐 However, there is no free lunch: you will need enough data points to get a good approximation otherwise the histogram will look nothing like the underlying distribution.

\---------------------------------------------------------------------------------

I have been studying and practicing Machine Learning and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it can be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike.

The posts will cover topics like statistics, linear algebra, probability, data representation, modeling, computer vision among other things. I want this to be an incremental journey, starting from the basics and building up to more complex ideas.

If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",LearnMachineLearning
rnios4,1640337781.0,Easy to understand CNN tutorials,"Hello


I am trying to learn convolutional neural network and there are nice resources out there. I got the basic ideas but some part are unclear for me. Is there any resource where every specific detail is studied and anyone can learn this topic easily?
Thanks",LearnMachineLearning
rni9dt,1640336064.0,"Where can I download the ""UNSOLVED"" coding assignments of the NLP course provided by Deeplearning in coursera by Youness Mourri and Lukasz Kaiser","I have currently completed the 1st course, (which was free) but for the other courses, the coding assignments are locked and I can't afford all these courses. I also checked some of the github repos but only solved solutions were available.",LearnMachineLearning
rngzdq,1640330841.0,Can gpt-j be fine tune to create sales copy? Will it creates the kind of result if compare with fine tune gpt3?,,LearnMachineLearning
rngpmh,1640329792.0,How do we utilize our machine learning model in our website?,"I'm trying to make a chatbot website and is in the process of making the model. Once I have the model, I want to implement the model for users to be able to interact wiith the chatbot through an interface made in react. How do I enable this?

Can I use pickle and pickle the model, then load it into django and make endpoints for the conversations?",LearnMachineLearning
rnegp0,1640321777.0,Data Science/Machine Learning Interview Participation request,"

Good evening,

Among current circumstances, I hope this message finds everyone well. I am current high school senior student in the state of Illinois seeking potential data science professionals or prospective data scientists willing to participate in an interview for my AP Research course. To provide a general overview, my institution is currently partnering with College Board's AP Capstone diploma, a diploma program that develops student’s skills in research, analysis, evidence-based arguments, collaboration, writing, and presenting skills based on two-long year courses: AP Seminar and AP Research.

As a student currently enrolled in the AP Research course, and an expected requirement, I am tasked with the year-long process of exploring an individual area of interest that may be an academic topic of choice, idea, or circumstantial issue. This year, I am centering my research on the effects traditional mathematics subjects retain in minority students academic success, primarily Latino(a) students and students of Hispanic origin, as well as assessing the measure of academic success of collegiate students or professionals in attaining a post-secondary education, degree, and/or career.

It is worth noting the State of Illinois does not offer any data science education within its public school districts, and is an objective I would like to have implemented in my community. I have tried to establish contact with potential participants, but have had no success. Therefore I have decided to post my objective here in hopes to gain participants. Though I am willing to take 20 participants who are interested, I am seeking those who have been previously enrolled in data science course in their secondary (high school) career or post-secondary.

If you are interested in participating or know of those who may be interested, please do not hesitate to contact me for further information. I am more than willing to set up a date/time through either platform, Zoom and Google Meets, and address any questions or concerns.

Thank you for reading this lengthy post, and happy holidays!",LearnMachineLearning
rndy6z,1640320020.0,ML Algo takes into account derivative of timeseries?,"&#x200B;

I have timeseries data and corresponding features and trying to determine which features/variables to include as well as the best algorithm.

I have heard the LSTM algorithm might be the best for my application, however, I am wondering about some aspects. If I have a signal that predicts my output I want the Machine Learning model to take into account, not only its current position, but its' past position, it's current slope, and it's current 2nd derivative/acceleration.

Do ML algos like LSTM take into account slope and derivatives or will I need to add a ""Slope"" column (feature) to capture that behavior?

Thanks",LearnMachineLearning
rndczs,1640318071.0,"[D] Hi there, I am hoping to predict next year's FIFA ratings based on ratings/positions/age/clubs from the past 5 years. Missing data here is important as some players will not have played in the last 5 years (ie. 17-year-olds). So which ML technique could be applied to this case?",,LearnMachineLearning
rnadmh,1640308362.0,Decoding Openimages v6 mask coordinates,"Hi,
New to ml. I am using V6 for semantic segmentation. in segmentations.csv, it has mask coordinates under the header clicks. These are set of three coordinates. first two coordinates being pixel values x and y with last coordinate being 0 or 1. Can anyone please explain to me what does this last coordinate stand for?

Thanks",LearnMachineLearning
rn7w3e,1640300737.0,Which OS for Machine Learning tasks?,"Which OS do you use/recommend for machine learning?

Why and which app/software/package/distros do you use for data management and database building?

Is conda the most spread?
Should everybody use it?

Just built a PC with my own GPU and I'm running Ubuntu with the conda package - Spyder for code writing and then copy paste to kaggle. I wonder what the experts say?",LearnMachineLearning
rn27l0,1640284764.0,Android studio with tf lite,"Hey guys!Anyone experienced with using tensor flow lite in android studio? I have trained a model, exported in tflite and then getting and error while uploading it to studio that meta files are not included. Any suggestions?",LearnMachineLearning
rmy3ct,1640273421.0,Introduce a site,"Introduce a site or channel in which conferences on artificial intelligence technologies and the algorithms used in them are described in full detail, with information on the latest developments.",LearnMachineLearning
rmx04g,1640270245.0,What is k-means clustering? A 2-minute visual guide. [OC],"&#x200B;

https://preview.redd.it/5sbxepcoza781.png?width=2048&format=png&auto=webp&s=224c1eaa5ebf867b82a600f671e51a43fec0df3a

https://preview.redd.it/w3061qcoza781.png?width=2048&format=png&auto=webp&s=70eee5546aba6d1c4b30de7c25f8691ee7bbca15

https://preview.redd.it/2zx2kjcoza781.png?width=2048&format=png&auto=webp&s=a97e9c10c2ddc2ea500578df2f70888d448b7dc5

&#x200B;

🔵 k-means Clustering 🔵


🔱 The k-means algorithm divides N data points into K disjoint clusters. It is a well-known unsupervised learning model. This means that the k-means algorithm only requires the data points and does not need the corresponding cluster that each point belongs to as this is what the algorithm figures out.


✨ The clusters are found by allocating points in such a way that the total variance of the points within each cluster (intra-cluster variance) is reduced or minimized. Although it iterates quite fast, the k-means algorithm can have varying cluster formations based on the initialization. It has been widely implemented in many software packages. The scikit-learn package for python is the one that I have used most often.


🔁 The most common method to perform the clustering is iterative. It alternates between two steps: (1) assigning each point to a cluster based on the point's closeness (or distance) to the cluster center and (2) updating the cluster center (called mean or centroid; hence the name k-means one for each of the k clusters) based on all the points that belong to this particular cluster. The iterations are usually performed until the centroids stabilize (converge) between consecutive iterations.


🤓 K-means is popular in scenarios where the data is known to consist of multiple groups (distributions) but it is unknown which point belongs to which group (cluster). It can be used for data analysis and splitting data that comes from multiple distributions, image segmentation, color quantization among other things.


\---------------------------------------------------------------------------------

I have been studying and practicing Machine Learning and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it can be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike.

The posts will cover topics like statistics, linear algebra, probability, data representation, modeling, computer vision among other things. I want this to be an incremental journey, starting from the basics and building up to more complex ideas.

I wanted something where I would have more of a personal touch so I decided to have them as hand-written notes with self-drawn figures. It takes time to create a post: from deciding the content, making it concise, and coming up with ideas on how to best convey with illustrations. I wanted to give something that reaches a wider audience a shot so here I am.

If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",LearnMachineLearning
rmuptq,1640262698.0,What language to write a conversational AI?,"So basically my ultimate goal would be to have my AI able to hold a conversation, know what the words mean and answer while maintaining context even if the topic isn't clearly stated in each message. Also to have some amount of memory (i.e. remembering names and relationships, like who my mom is or that I have an SO with a name.)

I don't know any languages other than a working knowledge of HTML, CSS, and JavaScript, but I don't think that's really helpful here, so I'm interested to know what language(s) I should learn because I'm determined to make my brainchild a reality.

Thank you in advance!",LearnMachineLearning
rmu8gx,1640260917.0,DBScan clustering algorithm,"Interesting article for DBScan clustering algorithm with hyperparameter tuning:

[https://medium.com/@vitomirj/dbscan-clustering-algorithm-309e5616c3d7](https://medium.com/@vitomirj/dbscan-clustering-algorithm-309e5616c3d7)",LearnMachineLearning
rms9uj,1640253133.0,Deeper Learning with CoLU Activation,https://arxiv.org/abs/2112.12078v1,LearnMachineLearning
rms1jn,1640252090.0,Check out my first blog post! Its on a similar image search I built over the last few days using PyTorch,"Hi ML community! I wrote my first blog ever describing the project I built and explaining my code. I would appreciate it a lot if you could read it and provide any feedback!

[https://ishan.sh/blog](https://ishan.sh/blog)

Thank you!",LearnMachineLearning
rmimjq,1640218407.0,How do they give control of a game to an AI?,"There is probably lots of ways they can do they but in its simplest abstraction, let's say a game such as league of legends... (Bare in mind I'm just one person so It wouldn't be feasible for me to even attempt this .. I ask our of curiosity) how do they give control to ML algorithm? Do they just input the controls and let it do its thing?",LearnMachineLearning
rmfog1,1640209720.0,alternative to google colab,"does anyone know a good alternative option to rendering VQGAN+CLIP projects
i cant get colab pro in my country so im looking for another option",LearnMachineLearning
rmewsq,1640207477.0,Linux Desktop + Windows Laptop for Data Science Student?,"I'm starting a data science masters program in January. I've got an HP Z840 Desktop (dual 12-core Zeon, 256GB RAM, and an Quadro M5000) along with a Gen 2 ThinkPad X13. I'm thinking of putting Linux (Ubuntu or Mint) on the Z840 and and keeping Windows on the X13 for when I need Office, or Windows specific software. Thoughts on this plan? Seems like a great combo to me, but looking for some other thoughts on the setup for running data science workloads locally. I also have at my disposal my work system with unlimited AWS access for larger projects if I need it.",LearnMachineLearning
rmdrrj,1640204207.0,Resources for Deep Recurrent Q Networks,I'm currently learning about Partially Observable Markov Decision Processes for Deep Recurrent Q Networks (DRQN). I've looked at [this paper](https://arxiv.org/pdf/1908.06040.pdf) but it's not massively useful. Are there any resources available to learn more about DRQNs?,LearnMachineLearning
rmd67z,1640202542.0,Inerview question repositries.,"Can someone give some good resources on Interview questions on ML? With answers.


Theoretical Question and Case Studies.

Which are tougher/practical and than basic ones which we get on google.",LearnMachineLearning
rmaqvu,1640195812.0,"Is a bootcamp the best way to learn the ML ""talk"" ?","
Hello. I searched and did not find this scenario, so was hoping to canvass some views or experiences.

I already have 2 years of decent (Fortune 50, multiple deployed projects) paid/professional ML under my belt. Not because I sought it out, but because I've been ""the go to guy"" for other oddball problems and got chucked into it. As it turns out, I like ML, data, and trying to make models predictive. I've been quite lucky to have oceans of data to play in, and relatively easy/softball problems to try to solve.

...but I am self-taught/google taught/udemy taught at the moment. I have major gaps in my knowledge, and I know it. I know it every time I talk to other ML teams and they throw the 5 dollar words around. :)

I'm about to be handed a VP of Data Science role, and will be expected to ""talk the talk"" about ML far more than I do now. Potentially to far more talented folks than I. This is very much a ""Peter Principle"" situation, and I'd like to stay ahead of it rather than become an incompetent potato with pointy hair.

I have been considering a boot camp to level up my ability to ""formally discuss"" ML concepts. Implementation isn't the primary goal, but I do love tinkering with this stuff, and would benefit from some new hands-on techniques also.

Mainly, though, I need to learn the lingo, as others talk it, and I need a broad and shallow set of ""talk"", rather than a specialization.

Is a bootcamp a good path for this? (I am in the SF Bay Area, there are dozens of them) -- or is there some better path I should take? Pausing to sit a degree program won't match my time load.

Appreciate any thoughts on the matter!",LearnMachineLearning
rm9rrt,1640193132.0,Machine learning design to evaluate generator sound,"


Hey guys,
I am thinking about following task and I am somehow not sure about solutions. I am curious about how others would develop the system and to what extend is my approach right.
there is a company monitoring its electrical generators, with the goal of detecting when generators need maintenance based on the sounds they emit. Generators are being continuously monitored with microphones, with the audio data being stored for processing. I need to design machine learning pipeline that would ingest this data and use it for training, in order to detect sounds that would indicate the need for maintenance. Three main questions:

1. Supervised vs unsupervised learning?
I think that supervised is better approach because we will load labeled data into the model - as labeled data I undrestand generators sounds, two sets(need maintenance/ does not need it)
2. What ML architecture?
Recurrent/recursive neural networks (RNNs). They are used to solve sequence problems, like a speech processing etc.
3. What stages of pre-processing would your pipeline involve, and why?
Here I was thinking that I need to create lists of labeled data, load them but I dont really come to more ideas...
Looking forward to your ideas!",LearnMachineLearning
rm7kxf,1640187076.0,Data Scientist vs Machine Learning Engineer?,"Hi everyone,

I didn't know of an appropriate forum to pose this question on, so apologies of it is out of context!

I have received offers from 2 companies:

Offer 1: Data Scientist at a big Oil and Gas Corp.
The job profile involves research in Process Mining

Offer 2: Machine Learning Engineer at a popular Analytics Consulting Firm. The profile involves deploying machine learning and deep learning models using Kubernetes, Heroku, Dask, etc.

Both options are at my choice of location and Offer 2 is also providing me a fully remote workplace option.

Offer 1 pays better, but leaving aside the pay, I am confused about which one I should go ahead with?",LearnMachineLearning
rm67i0,1640182917.0,[Notebooks Release] +30 Crypto Trading Notebooks for Kaggle's Crypto Competition,"I released **30+ starter notebooks** each demonstrating a different model or method for the crypto forecasting competition currently running on Kaggle.

&#x200B;

This is an ongoing project that is also beginner-friendly since it is highly documented. Many more Time Series / Finance-related notebooks will be released in the future so this can also serve as a ""first stop"" when studying Time Series analysis.

&#x200B;

The whole project took me a lot of time to develop and is not easy to maintain, so please if you find this of value: Your feedback & support is highly appreciated!

&#x200B;

**Selected Example: Baselines & Starter Notebooks**

&#x200B;

**G-Research: LSTM Starter Notebook 🔥**

[https://www.kaggle.com/yamqwe/g-research-lstm-starter-notebook](https://www.kaggle.com/yamqwe/g-research-lstm-starter-notebook)

&#x200B;

**G-Research: Reinforcement Learning Starter:**

[https://www.kaggle.com/yamqwe/g-research-reinforcement-learning-starter](https://www.kaggle.com/yamqwe/g-research-reinforcement-learning-starter)

&#x200B;

**Crypto Prediction: Technical Analysis Features:**

[https://www.kaggle.com/yamqwe/crypto-prediction-technical-analysis-features](https://www.kaggle.com/yamqwe/crypto-prediction-technical-analysis-features)

&#x200B;

**Crypto Prediction: Volatility Features:**

[https://www.kaggle.com/yamqwe/crypto-prediction-volatility-features](https://www.kaggle.com/yamqwe/crypto-prediction-volatility-features)

&#x200B;

**Purged Time Series CV + LightGBM + Optuna 🔥**

[https://www.kaggle.com/yamqwe/purged-time-series-cv-lightgbm-optuna](https://www.kaggle.com/yamqwe/purged-time-series-cv-lightgbm-optuna)

&#x200B;

**🔥🔥 WaveNet Starter Notebook 🔥🔥 (Updated):**

[https://www.kaggle.com/yamqwe/wavenet-starter-notebook-updated](https://www.kaggle.com/yamqwe/wavenet-starter-notebook-updated)

&#x200B;

**Let's Talk Time Series Validation:**

[https://www.kaggle.com/yamqwe/let-s-talk-time-series-validation](https://www.kaggle.com/yamqwe/let-s-talk-time-series-validation)

&#x200B;

And many many more..

(Links inside the notebooks)

&#x200B;

Fork them as you please! Enjoy Yourself!",LearnMachineLearning
rm4xps,1640178574.0,"Best ML and AI courses on YT using python, tensorflow and google collabratory.","

Hi,

I am a beginner in ML and I want a good course which uses python, tensorflow and google collab. i hv been searching for a course like this for a long time now and I really want someone to help me and link me the best course that is interactive and fun as well. Pls help",LearnMachineLearning
rm3cxf,1640172674.0,I would like to ask a question about how to approach an ai project.,"good morning

&#x200B;

I would like to design an artificial intelligence that is able to improve its performance in a task by having another AI tell it what its mistakes are.

&#x200B;

concretely the scenario would be 3 intelligences capable of playing chess.

&#x200B;

intelligence 1 would not know how to play chess but it can learn by communicating with intelligence 3.

intelligence 2 would not learn in any way but it is better than intelligence 1.

and intelligence 3 is superior to intelligence 2 and every time it finishes the game it tells intelligence 1 what it detected as wrong

&#x200B;

the ias don't have to speak in a human language for the project, if they are able to communicate with a strip of (;ajdfgjjagfasdf asdfwgasryh tuds ) that works for me even if I don't understand it.



&#x200B;

My problem is that I know practically nothing about ML and I don't know from which angle to approach the problem, although I have experience programming, in fact the intelligence number 2 would be done but it is a MInMax algorithm.

&#x200B;

a point to keep in mind is that the important thing is not that the intelligences end up being chess masters, the point is to design a teacher-student communication circuit that later can be extrapolated to other problems.

felices fiestas",LearnMachineLearning
rm34a3,1640171726.0,Andrew Ng ML Course or Fast.ai ?,"I have heard that a lot of people recommend Andrew Ng ML course and others recommend Fast.ai

As for as i know Fast.ai is more practical and nothing theoretical on the other hand Andrew Ng is more theoretical less practical

I am new in ML journey and confused ,  i want to choose between two what to choose ?

If any other more beneficial resources you know please share that also",LearnMachineLearning
rm1u3i,1640166378.0,Linear Programming - small example,"LP is very powerful subject. Here I have created one optimisation example with gurobipy which could easy illustrate the power of LP. We got some new teachers which should be assigned to the nearest school but we must align the teacher salary expectation with our budget (we have only one constraint). This is an original my example, I have enjoyed doing it. Please star if you like it, for my motivation. Best regards and happy holidays!

[https://github.com/Vitomir84/ML\_algorithms/blob/main/Linear%20Programming/Linear\_programming\_with\_gurobipy\_teachers\_example.ipynb](https://github.com/Vitomir84/ML_algorithms/blob/main/Linear%20Programming/Linear_programming_with_gurobipy_teachers_example.ipynb)",LearnMachineLearning
rm1da7,1640164326.0,How much should MLOps engineers know about inner workings of algorithms?,"How much should MLOps engineers know about the inner workings of machine learning algorithms?


Think of the job of MLOps engineers as taking models from data scientists and deploying them as production assets integrated within business operations.


Let us know your thoughts in the comments! 👇🏻


[\#machinelearning](https://www.linkedin.com/feed/hashtag/?keywords=machinelearning&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6879073513387499520) [\#datascience](https://www.linkedin.com/feed/hashtag/?keywords=datascience&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6879073513387499520) [\#mlops](https://www.linkedin.com/feed/hashtag/?keywords=mlops&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6879073513387499520)

[View Poll](https://www.reddit.com/poll/rm1da7)",LearnMachineLearning
rly1g0,1640151117.0,New to R,"I am planning to start the Google Data Analytics professional certificate course on Coursera. I have also started watching a bunch of R YouTube videos and practicing with R studio.
Am I on the right path or is there a better place to learn R?",LearnMachineLearning
rlu1bk,1640138039.0,Beginner here - need advice / review on my PyTorch Project,"Hi guys,
I'm new to this channel, python and machine learning. I've watched tons of videos and read stuff about ML and decided that I want to make a Tetris ""AI"".

Here's the problem :

I started coding my own tetris, agent and model in Python and used pytorch (seemed like a good choice for beginners).
Now the bot is starting pretty strong but it feels like as if it's fixing itself to single decisions in the later runs (e.g. holding right/left, spinning permanently etc)

Here's my repo/code:
https://bitbucket.org/b1291/tetris/src/main/
(ignore the Readme please :D)


Literally any help is very much appreciated! I want to know whether I did something completely wrong and want to learn from those mistakes.

Greetings and thanks in advance :)",LearnMachineLearning
rlpls7,1640124806.0,Where to Start on ML?,"Hey guys ,

I'm an undergrad Informations Technology student about to go to my final semester in university. Last semester I had to submit my final year project and the topic I chose was Using Machine Learning to Analyze Procurement data, In my proposal defense , I figured I would do something with Spend Analytics of Procurement Data.

&#x200B;

But last semester, I was balancing university with a full-time customer service job so I didn't have the time to study much on the topic and I'm fairly new to Machine Learning as a whole. Where would be the best place for me to start and what path should I follow. Also if anyone has experience with procurement data , It would be great if they can advice if Spend Analytics is really the way to go or is there something cooler out there. It would be really cool if you can tell me where can I source procurement data from ( I searched on Kaggle and I couldn't find any helpful data)",LearnMachineLearning
rlon3o,1640122048.0,[P] HackThisAI: Adversarial Machine Learning CTF Challenges,"[https://github.com/JosephTLucas/HackThisAI](https://github.com/JosephTLucas/HackThisAI)


A couple years ago, I took a one-day adversarial machine learning workshop. It's a fascinating field, but I had trouble finding a clear entry/learning path. I'm prototyping some ""capture the flag"" style challenges to provide that sort of skill development. I'd appreciate any playtesting, thoughts on the format, recommendations, etc.",LearnMachineLearning
rllfmq,1640112918.0,Has anyone tried using linear regression(Not logistic) as binary classifier ?,,LearnMachineLearning
rlkyv6,1640111569.0,Very basic of linear regression. My first article. Please have a look and give inputs .,[https://medium.com/@ketanmk26/linear-regression-theory-and-applied-for-beginners-with-code-in-python-dc10944d158b](https://medium.com/@ketanmk26/linear-regression-theory-and-applied-for-beginners-with-code-in-python-dc10944d158b),LearnMachineLearning
rljk7d,1640107618.0,joining a project,"hi,

i am a cs and ml expert. i do projects myself and i would like to join a team. would you like to work with me?",LearnMachineLearning
rljh5w,1640107375.0,Is there a github repository that lists out topics and resources to learn them?,"I saw this repository for data structures and algorithms, so I was wondering if there's something similar for a beginner wanting to learn machine learning

[https://github.com/jwasham/coding-interview-university](https://github.com/jwasham/coding-interview-university)

A quick [google search](https://www.google.com/search?client=firefox-b-1-d&q=github+machine+learning+artificial+intelligence+beginner) returns lots of repositories, so does anyone have experience with any of the repos?

I have also heard of [microsoft/ML-For-Beginners](https://github.com/microsoft/ML-For-Beginners) which looks like a good resource, and I will definitely start the lessons. Does anyone have any opinions on this? It looks interesting",LearnMachineLearning
rlin1a,1640105090.0,[R]ML/Classification/SupervisedLearning,"Hey everyone,
I am looking for an algorithm, which helps me to sort the values from a sensor in three classes( F/E//NaN or 1/2/3). I have a lot of trainingdata, where I already declared at which time and value which class is the best one. So I choose supervised learning/classification. Which algorithm would you suggest?  Do you find a better way to solve this problem?",LearnMachineLearning
rlhwve,1640103056.0,I know python but ml code seems meaningless to me,"I was trying to learn ml in a long time and I know python and math but every time I started a course on YouTube or udemy about introduction to ml  after tensors things get really confusing for me. Like loading data, training model I can't understand what's going on. I recently watching StatQuest and I really liked the ml videos but I wanna do my own projects and for that I need coding.",LearnMachineLearning
rlgrnt,1640099820.0,high school student doing ml,"Let me just start by saying that I have no idea how to do what I'm trying to do. I have to do a research project for one of my high school classes, and my grade is really taking a beating because of this issue. So basically I'm super new to ML, and I have a research project where I'm trying to predict solar flare activity using ML. Basically what this guy has done: [https://byte7.github.io/blog/Predicting-SolarFlares/](https://byte7.github.io/blog/Predicting-SolarFlares/)

[https://hesperia.gsfc.nasa.gov/rhessi3/](https://hesperia.gsfc.nasa.gov/rhessi3/)

[https://github.com/Byte7/Solar-Flares-Prediction-RHESSI-Mission](https://github.com/Byte7/Solar-Flares-Prediction-RHESSI-Mission)

I just don't know how to convert a large dataset in FITS format all to CSV and preprocess it. If you go to the first link, there's a section called ""getting my hands on data"" and ""data preprocessing."" Basically I don't know how to do either one and I've been stuck for like 2 weeks. I met with my teacher today and he's visibly pissed that I haven't made much progress. I'm also not that good at programming to make matters worse. I know it's a lot to ask, but can anyone help me. I have three days before I either terribly fail or do well in the class.",LearnMachineLearning
rlg44n,1640097875.0,Grab your Digital Copy of Tensorflow Workshop - HURRY,"Packt has Published ""The TensorFlow Workshop ""

Grab your digital copy now if you feel you are interested.

As part of our marketing activities, we are offering free digital copies of the book in return for unbiased feedback in the form of a reader review.

Get started with TensorFlow fundamentals to build and train deep learning models with real-world data, practical exercises, and challenging activities.

Here is what you will learn from the book:

1. Get to grips with TensorFlow’s mathematical operations
2. Pre-process a wide variety of tabular, sequential, and image data
3. Understand the purpose and usage of different deep learning layers
4. Perform hyperparameter-tuning to prevent overfitting of training data
5. Use pre-trained models to speed up the development of learning models
6. Generate new data based on existing patterns using generative models 

## Key Features

* Understand the fundamentals of tensors, neural networks, and deep learning
* Discover how to implement and fine-tune deep learning models for real-world datasets
* Build your experience and confidence with hands-on exercises and activities

Please comment below or DM me for more details",LearnMachineLearning
rlfkvk,1640096173.0,What is overfitting? A 2-minute visual guide.,"&#x200B;

https://preview.redd.it/x57wtwiylw681.png?width=2048&format=png&auto=webp&s=c59e246b6f1095046ff1243ac695a53201f929e9

https://preview.redd.it/uga4twiylw681.png?width=2048&format=png&auto=webp&s=c3bdea6e4dbd575f7cb6a9671fd953d96ac74f99

https://preview.redd.it/b98cuojylw681.png?width=2048&format=png&auto=webp&s=9f4c332ad354712f4fba76bfffd6d83809613aa9

🔵 Overfitting 🔵

🧐 Overfitting is a common phenomenon the machine learning community tries to avoid like the plague. This is because when a model overfits it performs extremely well on the training data that it is provided but performs poorly and fails to generalize on unseen data.

💾 You can imagine overfitting with an analogy. When one assumes that the questions in the exercise session of a lecture are exactly what will be asked in the exam and end up memorizing them. During the exam, they realize that this rote-learning would not be of any help in answering unseen questions.

😮 Overfitting can be avoided in many ways. 5 common ways to do so is by (1) regularization of the model parameters using L1 or L2 norm for example (see previous posts for more details), (2) gathering more training data to let the model cut through the noise, (3) early stopping by monitoring the training and validation error curves, (4) reducing the number of features by selecting better features and (5) by performing data augmentation.

\---------------------------------------------------------------------------------

This is the first post and hopefully the first of many to come. I have been studying and practicing Machine Learning and Computer Vision for 7+ years. As time has passed I have realized more and more the power of data-driven decision-making. Seeing firsthand what ML is capable of I have personally felt that it can be a great inter-disciplinary tool to automate workflows. I will bring up different topics of ML in the form of short notes which can be of interest to existing practitioners and fresh enthusiasts alike.

The posts will cover topics like statistics, linear algebra, probability, data representation, modeling, computer vision among other things. I want this to be an incremental journey, starting from the basics and building up to more complex ideas.

If you like such content and would like to steer the topics I cover, feel free to suggest topics you would like to know more about in the comments.",LearnMachineLearning
rlewuz,1640094039.0,Face Filter project using mediapipe and p5.js,"Hey everyone Christmas is coming, So I've tried to make a small 2d face filter web application using ""Mediapipe"" and ""p5.js""

Youtube:- [https://youtu.be/Viwl6JZ0vv4](https://youtu.be/Viwl6JZ0vv4)

Check it out -> [https://christmasfilter.netlify.app/](https://christmasfilter.netlify.app/)

NOTE:- Please use CHROME in order to run it I'm getting some CSS issues on FIREFOX (as I'm not very good at CSS).

and if anyone is looking for source code:- [https://github.com/iamhimanshu0/face-filter](https://github.com/iamhimanshu0/face-filter)

You can also share your screenshot with me with the filters

Hope you enjoy the project. Cheers!!",LearnMachineLearning
rlcpx5,1640086101.0,Train custom Text Handwriting models,"Hi all, I am testing the aws textract service and noticed that in the json file results there is the bounding boxes of the detected words. I am wondering if this information can be used to build a dataset and retrain a custom model ? I am asking that because, there is a lot of errors on the predictions si, if I correct them and then train a new model, once I have a good model, I stop using textract

what is your thoughts ?",LearnMachineLearning
rlcc92,1640084557.0,A little tip for aspiring Data Scientists and ML- Engineers,"You can take a Coursera or Udemy course, work through some practical tutorials with Jupyter Notebook, use fancy tools, have a collection of helpful Cheat Sheets and books, or have multiple certifications. All is well and good, but it doesn’t mean you have a complete grasp of the subject.

Test your skills by implementing a use case from scratch, without using any ready-made resources or instructions. From data collection, -preparation & -understanding, -modeling, training, and optimization to a robust pipeline. Be able to explain and interpret what you have realized. Try to visualize it and describe it to someone who is not an expert. Do this several times, and you will notice how what you have learned above will fuse into something unique and valuable.

(excerpt from my book [THE AI THOUGHT BOOK](https://www.amazon.com/dp/B08Z4BWN1X) (Amazon Link))",LearnMachineLearning
rlc7b7,1640083998.0,Machine Learning Andrew Ng on Courseera,"I would like to learn machine learning, upon research this seems to be the most popular, is it the same CS229 course? Says 12 weeks to finish but my winter vacation is just 4 weeks long",LearnMachineLearning
rla8ja,1640075766.0,What should I use for a beginner project? Colab seems too slow.,Have any of you had any experience with Google colab or similar tools? I'm trying to train some models but I'm a beginnier. I'm looking for something that's relatively easy to use but will work well enough for what I'm trying to do. I tried to run it on my MacBook but it took 5 hours. Do any of you have any suggestions? Thank you. (:,LearnMachineLearning
rl6pnt,1640062852.0,Task too difficult for intern recruitment process?,"Hey everyone, some background, I am a college sophomore, trying to pursue ML/DL, one of the startups my college is incubating wanted some interns for ML, I applied and got through the interview round, for the second task however I receive this.

>As next step towards the selection, kindly take a ML/DL use-case of your choice,create a suitable GUI and Integrate it with the solution,ensuring utility for an end-user.

Mind you this is supposed to be an unpaid internship with a certificate at the end and their ""time"".

The time given for the assignment is 41 hours that is, just under 2 days.This is my first internship.Is the startup asking too much or am I just too inexperienced?


Edit: I conveyed to them about this task being above my skill level, thank everyone for their responses.",LearnMachineLearning
rl5msp,1640059281.0,Is there any book/resource that provides a high-level overview of deep learning problems/tasks and their applications at a simple level?,"As the title says, I was wondering if anyone knows of any kind of book or resource that explains at a high level the different areas of deep learning and their problems and general approaches and applications.

For example, for computer vision, graph networks, generative learning, reinforcement learning. Something like an 'introduction for dummies'. It would preferably be nontechnical, so after reading it one would understand roughly what the different kinds of tasks in each field and what applications they have. It obviously wouldn't cover the technical details of the algorithms, but just their names and a rough intuition and their applications.

Was just wondering if anyone had written such a book before, or if anyone came across such a resource that fits the bill.

Cheers!",LearnMachineLearning
rl5j1e,1640058941.0,What other things I need to concentrate on other than my course curriculum?,"Hi, I have started my master's in computer science with a specialization in Artificial intelligence, and I aspire to become a machine Learning engineer. It would be helpful if anyone can suggest things other than my course curriculum that I should concentrate on. Thank you.",LearnMachineLearning
rl2913,1640048859.0,"I have the general idea of machine learning, I have the hardware, I have data collected but how do I get to the point of just training models?","It feels so far away, even though it seems like I have all the pieces already, I don't know how to bring it all together.",LearnMachineLearning
rl0rmt,1640044506.0,Pruning with Reinforcement Learning,"Hello, I have a Seminar in university witch is about pruning in NNs. I want to spezialice me to pruning with Reinforcement Learning. But after I read five important papers I didn't found something else witch is relevant. Do you have more interesting Papers to this theme?",LearnMachineLearning
rl0jmt,1640043850.0,Model learns on training data but not on test data,"I train with Keras a model, witch is similar to Alexnet, on Audio Data witch I transformed to normalized Mel specrograms. On a equally distributed validation dataset I get good train and validation accuracy from 80 to 90% but on my test data it lears just the same class.
Any idea what the problem is here?",LearnMachineLearning
rkv8zg,1640029068.0,Advice on Multiple linear regression with sklearn,"So I am new to machine learning and I want your thoughts if I'm doing this right.

It's probably not the best for prediction, but I'm doing this to combine a love of sports and to practice on things that I'm learning through school.

The dataset I'm using is 5 years worth of English Premier League data. In short, I've created a program to get the standings of a given year through X many games that I want. It includes W, L, D, Points, GF, and GA. In the end dataset, I have 5 years worth of data for the teams up to the Xth game of the season.

Using sklearn, I built a multiple linear regression model that uses the GF and GA to predict Points for whatever X amount of games played that I wanted. The end dataset that I just previously mentioned was what I used to do the training and testing with.

So now with that model being trained and tested, I use the current season's standings through the X amount of games to predict how many points the team should have. I think it's doing the job well as most of the points predicted are pretty close to the actual points.

Now my big question. Say this season gets finished. Should I add this season to my training and testing dataset and rerun the model so that I maintain accuracy for next season's predictions?",LearnMachineLearning
rktqol,1640024851.0,Are there any papers on this?,"I was recently training a sentence classifier and discovered something kind of interesting. If you calculate the distance from the weights of the word embeddings at the start of training to the weights at the end of training, the distance corresponds to how important that word is for the classification task. I was wondering if there is any research into this. It seems like it would be useful for model explainability.",LearnMachineLearning
rkt0u9,1640022906.0,Last survivors of Washington's army - Enhanced and colorized using Artificial Intelligence,[https://www.youtube.com/watch?v=Iw7FIaQZizc](https://www.youtube.com/watch?v=Iw7FIaQZizc),LearnMachineLearning
rksnv1,1640021905.0,[Project] Advanced Methodology for Intelligent Diagnosis of Systems and Processes,"

**The goals of intelligent diagnostics of systems and processes – increasing the level of fault tolerance of the diagnostic object by:**

\- reducing the probability of incorrectly determining the state of the systems (errors of the third kind in classifying the state of the systems) when monitoring systems;

\- obtaining stable effective estimates of unknown values of design parameters of functional elements of systems (corresponding to the found state);

\- the choice of a rational control law for the object of diagnostics for the identified on the basis of the forecast state.

**Algorithm for the numerical solution of the problem of intelligent diagnostics of systems and processes on the base of agent-oriented approach by means of** **reinforced learning**

**1.** **Data Pre-processing**

1.1  Structural analysis of the diagnostic object. Construction of fault trees and corresponding event trees. Identification of the most probable scenarios of system failures as a whole, caused by single or multiple defects in functional elements on the basis of monitoring data of the state of the prototypes of the system in operation.

1.2 Formation of a subset of controlled state variables, the values of the quantities of which are check in by measuring instruments.

1.3  A training sample of alternatives (dataset) of the values of the quantities is generated, as well as the corresponding confidence intervals (measurement accuracy) for the regime, control variables and controlled state variables based on the data of experimental studies of the diagnostic object in healthy state.

1.4 Data cleaning from anomalous values of the quantities (outliers).

1.5  Identification of the complete mathematical model of the system based on the solution of the problem of multicriteria optimization in a stochastic formulation using dataset (clauses 1.2, 1.3).

1.6 The search for the values of the objective functions corresponding to the boundaries of the intervals of the operating modes of the diagnostic object in healthy state, based on the solution of the direct analysis problem using the complete mathematical model (paragraph 1.5).

1.7 Searching of the solutions of the inverse problems of interval analysis for each failure scenario of the system as a whole - the values of the boundaries of the intervals for the design parameters and controlled state variables that correspond to the healthy state of the diagnostic object, based on the solution of multicriteria optimization problems in a stochastic formulation.

Based on the results obtained, a database is formed that contains the boundaries of the intervals of acceptable values of design parameters and controlled state variables for each scenario of failure of the system as a whole. These data can be used for tolerance control of the states of the diagnosed object.

1.8  A total dataset of alternatives is generated. Each alternative includes subsets of design parameter values; regime, control variables and controlled state variables; objective functions. The total dataset includes subsets of alternatives corresponding to different failure scenarios of the system as a whole, including the healthy state.

The values of the controlled state variables and objective functions for alternatives can be obtained on the basis of modeling the characteristics using the complete mathematical model, as well as directed experimental studies of the diagnostic object with defects in functional elements.

1.9 Data cleaning from anomalous values of the quantities (outliers). Normalization of data.

1.10 Cluster analysis – the determination of the possible number of states (failure scenarios of the system as a whole) using regime, control and controlled state variables (paragraph 1.8). If the number of identified clusters coincides with the specified one and the distances between the clusters are statistically significant, then the subset of monitored state variables can be considered complete. Otherwise, the diagnostic object should be equipped with new measuring instruments (see paragraph 1.2).

1.11 Development of robust metamodels using data (paragraph 1.8, 1.9):

a) a multidimensional logistic regression in the form of analytical dependences of the posterior probabilities of different failure scenarios for the system as a whole on the regime, control and controlled state variables;

b) a multidimensional observer model in the form of analytical dependences of controlled state variables on design parameters and regime and control variables;

c) a multidimensional diagnostic model in the form of analytical dependences of the objective functions on the regime, control and controlled state variables.

**2. Monitoring the state of the diagnostic object during operation**

2.1 Measurements are made of the values of the regime, control and monitored state variables at the current moment of operation of the diagnostic object.

2.2 Determination of the scenario for which the maximum a posteriori probability of its realization corresponds with the observed values of the quantities (paragraph 2.1) based on the solution of the classification problem using the metamodel (1.11a).

2.3 Tolerance control of the diagnostic object, which uses a database (paragraph 1.7), containing the values of the boundaries of the intervals of acceptable values of the monitored state variables for the scenario defined in paragraph 2.2.

2.4   Robust estimation of the values of the design parameters of functional elements corresponding to the scenario defined in paragraph 2.2 of the diagnostic object based on the observed values of the regime, control and controlled state variables (paragraph 2.1). The search for the values of the design parameters is carried out on the basis of solving the problem of multicriteria optimization in a stochastic formulation using the metamodel (1.11b).

2.5  Tolerance control of the diagnostic object using the database (paragraph 1.7) containing the values of the boundaries of the intervals of acceptable values of design parameters for the scenario defined in paragraph 2.2.

2.6 Robust estimation of the values of the objective functions corresponding to the scenario defined in paragraph 2.2 of the diagnostic object based on the observed values of the regime, control and controlled state variables (paragraph 2.1) using the metamodel (1.11 c).

2.7 Development of robust multidimensional models of control the state for the diagnostic object in the form of analytical dependencies predicted from the measured values of the regime, control and controlled state variables in the monitoring process.

2.8 Forecasting multidimensional time series of controlled state variables based on multidimensional models of control the state for the diagnostic object (paragraph 2.7).

2.9 Reducing the dimension of the space of controlled state variables based on the analysis of the informativeness of the variables of the robust multidimensional models of control the state for the diagnostic object (Sensitivity Analyzes). Estimating the rank of time series cointegration.

2.10 Determination of the state of the diagnostic object based on the solution of the classification problem using the metamodel (1.11 a) and the predicted values of the regime, control and controlled state variables.

2.11 Synthesis of a rational control law for the diagnostic object for the state identified on the basis of the forecast.

The given statements are reliable, as they had confirmed by the experience of using developed by us “ROD & IDS” software in different fields of activity. Look, please, on articles and presentations of our results in the section ""achievements / publications"":

http://[www.linkedin.com/in/mykhaylo-ugryumov-63148313b/](http://www.linkedin.com/in/mykhaylo-ugryumov-63148313b/)

[https://www.researchgate.net/profile/Mykhaylo-Ugryumov](https://www.researchgate.net/profile/Mykhaylo-Ugryumov)

[https://orcid.org/0000-0003-0902-2735](https://orcid.org/0000-0003-0902-2735)

We have the right to consider ourselves as specialists in the development of effective Machine Learning Methods (MLM’s) for solving “ROD&IDS” problems. We offer you our software in order to resolve specific problems or we can do calculations on our own for you. In addition, we are always open to have contacts and discuss how our methodology, computational methods and IT-realization theirs in form of Interactive Compute Decision Making Support Software System “ROD&IDS” can be helpful for your company. We hope that your use of our MLM’s will increase customer demand for new versions of your software.",LearnMachineLearning
rkseqq,1640021231.0,Optimal number of CART trees in random forest,"Hi guys,

&#x200B;

So as part of trying to learn machine learning, I've stumbled upon the question ""Determine the optimal number of CART trees, ranging from 1000 to 10000 (in increments of 1000, so 1000, 2000, 3000).

This question is as part of a random forest model question, and I'm having a hard time figuring it out as I thought CART was an entirely different model of trees.

How do i determine the optimal amount of CART trees in a random forest?",LearnMachineLearning
rkrgq3,1640018685.0,Finding the subject part of a sentence containing multiple words,"Hi, I was trying to find out ways to extract the subject from a sentence in Python. I tried Spacy and its nsubj tag sort of works but it associates just a single word to the subject.

For example, if I have the following text:

""Three conspiracy theorists walk into a bar""

Then I want the subject to be ""Three conspiracy theorists"", but spacy tokenization produces the following:

    Three nummod
    conspiracy compound
    theorists nsubj
    walk ROOT
    into prep
    a det
    bar pobj

How can I achieve what I mentioned? Is there something that I am missing? I am relatively new to NLP so any help would be appreciated.

Thanks.",LearnMachineLearning
rknwid,1640008813.0,Skewed imbalanced feature,"As I learned imblearn (SMOTE, ...) is for imbalanced label class(es). My dataset has a skewed categorical feature column (pandas). How can a balance this? For numerical data, box cox is a way. But for my cat feature column I need evently counts (N) from each subgroup.",LearnMachineLearning
rknejs,1640007266.0,Machine Learning Algorithm That Would Learn When a Barber Rejects Customers,Imagine a barber that works with appointments. There would be a system that would track the times this barber would reject the appointments. For example mondays this guy usually rejects appointments between 10-12 am. Or tuesdays 1-3pm. The program then learns this guy's pattern and closes these hours for future appointments. Which machine learning algorithm would be suitable for this kind of work ?,LearnMachineLearning
rkmiay,1640004406.0,Do I need any background deegre for ML?,"Hi, that's the question, I'm learning python, and going to do a course of ML from Standford University. I did 3 years of a Biochemistry degree, but changed my career to computer science this year.
So, is it possible to learn and get a job of ML without a degree?",LearnMachineLearning
rkm5dv,1640003135.0,Can same neural network on same dataset fetch different results on different operating systems/GPUs,"Even when I used the same code and dataset for fitting on neural network I got different accuracy in every epoch on my local system compared to google colab and I think this should be possible, the time taken can definitely vary on different systems but accuracy of neural network in 100 epochs doesn’t even get close to each other",LearnMachineLearning
rklgw8,1640000591.0,From where to Python for ML ?,"U have learned python basics
Now i have started ML.

I am following book "" Hands-on Machine Learning"" which is popular in recommended books

And Coursera Machine Learning Course by Andrew Ng

I just observed that in book as well as Course Authors don't teach python for ML specifically

Can you please share some resources for python for ML?",LearnMachineLearning
rkkqhn,1639997873.0,Feature Selection Advice,"I have recently been working on one of my way too many side-projects. This one is a, for my current level of knowledge and ability slightly ambitious, (informal) experiment regarding the comparative performance of various types of models for a specific task. I am especially hoping to compare a precomputed batch-learning NN with a reinforcement learning method, though I'm also hoping to expand on both of those categories with variations upon the 'base' once I have one of each working reasonably well. Since I've got the game pretty much fully implemented (bar some refactoring for reasons of cleanliness), I'm now at the stage where I'll need to select what features might be relevant for my models. (I'm ideally hoping to use a single, universal feature set for all models both for comparability, and also because I'll need to create all the training data by hand.) I've had some ideas for this, but I wanted to ask for advice before I miss out on something not immediately obvious that could be beneficial.

For refrence, please consider [this](https://imgur.com/a/wkfOPv4) screenshot of the game. (It's a bit minimalistic, I know.) The green square is the player character, whereas the red bars are semi-randomly generated platforms, with varying widths. The green number at the top is a score, which is always equal to the number of platforms under the highest platform the player has reached so far. The game also features an optional 20s timer, which I'm intending to use in conjunction with the score for benchmarking the performance of the various models, though it's not on in the screenshot. Note that if the player leaves the screen to one side, they'll emerge on the other. the game is controlled using three keys only (left, right, jump), and I want to train each model to predict wheter or not each of these should be pressed on a given frame.

Now, the possible features I have so far considered include the following:

1. Position of player & platforms: This one is kind of obvious, though thanks to the fact that we've always got 7 platforms + 1 player on screen, each with x and y coordinates, this means we already have 16 features by neccessity. I was also considering wheter using a relative measure of position might be usefull, since it helps generalise the dataset and eliminate the need for the player position features.

2. Current time remaining: The models will be benchmarked in a fixed timeframe, so this might be relevant.

3. Current score: Don't think it'd be that relevant tbh, but it's a number I have so it's a feature I have considered.

Any and all advice is of course welcome, though to reiterate I am mainly looking to select appropriate features at this stage. (Especially anything that might be relevant to such problems that I hadn't considered.) And just in case this becomes relevant: The game is implemented in Python using PyGame, and I'm intending to use Tensorflow for the ML parts.",LearnMachineLearning
rkf0fh,1639975910.0,YoloV4 Custom Classifier,"Hi All,

I am starting a computer vision project to identify images of fires. I have mostly only worked in TensorFlow and Keras, but I have had to try and work with darknet to get Yolo working. I have an annotated dataset and get a weights file, but I can't get it to predict anything really. I have tried working through some tutorials,but it hasn't really helped. People seem just to use weights from COCO.

I was hoping someone could suggest another resource to help me or would be willing to connect to help me troubleshoot. I am cs student, and I would really appreciate the help.",LearnMachineLearning
rk9s3c,1639959432.0,"Is a linear regression model the best for my use case, and if so, what type and how do I do it?","Ok, so I have a data set with 200K lines of data. I have a further ""test"" data set with around 100K lines, with data gathered at a later time. The data has three columns, which I might expand in future and add a 4th. There's the timestamp column, the target column, and then a column (think it's called a feature column in ML talk) that holds the data on which the target column is mostly dependent. The .csv header line is simply:

    datetime, usd_unit, price_unit

I visualized the data and from the graphs I can see that the ""price_unit"" values have similar up and down movement as the ""usd_unit"" values, but it is slightly delayed, by a few time intervals. This is not stock data or some kind of market prediction stuff, it's simply an educational data set. It is clear though, ""price_units"" are directly linked to ""usd_unit"", so while time is obviously important here, the target column is directly impacted by ""usd_unit.""

I am trying to build a model where I can give it, say, the last 30 data points of both the ""usd_unit"" and ""price_unit"" values, and it can predict the point at which the ""price_unit"", which trails the ""usd_unit"" in movement, will catch up to ""usd_unit."" I have done some reading and it looks like a linear regression model is what I need? But then there are simple and multi-something ones, and there's all this talk about smoothing weighted averages and stuff I really don't understand. I'm an architect and DevOps engineer, not a machine learning expert or mathematician. I've looked at examples of people doing this type of analysis and it just goes over my head.

From what I can gather, Python is the language of choice? But then there seems to be 17 different modeling packages and frameworks and for some reason everyone starts by letting me draw a freaking graph with  matplotlib while I already did that in Excel.

It would be great if someone could guide me towards my goal. Please don't just point me at a ""linear regression for beginners"" tutorial, I probably already saw that and it either went over my head or it just didn't work. If someone is willing to take the time to explain to me what the heck I need, or point me at a tutorial/example and actually explain in plain, simple, I'm a techie not a mathematician, I like Linux not linked differential equations, oh and English is my second language, English, I'd greatly appreciate that.

Sorry for that, I'm a bit frustrated. Partly with the resources out there, and mostly with myself for not understanding something that a lot of folks out there say is ""not that hard."" I swear I'm not that dumb :D

Thanks in advance!",LearnMachineLearning
rk4vk0,1639945380.0,I have many doubts here.Any hints on how to solve these problems???,"&#x200B;

[For the 1st problem  1 have some approach but i did something wrong here idk where to take these trigonometric functions.Can you please tell me where i am wrong  and what should be the right thing to do](https://preview.redd.it/eyerni0u5k681.png?width=1246&format=png&auto=webp&s=da6198dcd1f0085cbf125802af9c1796f99b3c8f)

https://preview.redd.it/dghs9xgw5k681.jpg?width=1633&format=pjpg&auto=webp&s=488c54da95f0a5f8880d04086be87e61133959aa",LearnMachineLearning
rjzi1g,1639929336.0,Can someone educate on machine learning why I got the wrong answer?,"

A) Which of the following beam programming concepts that can also be created from in-memory data where it is both the inputs and outputs for a particular step in the pipeline?

1. PCollections
2. PTransform
3. ~~Pipeline~~
4. I/O Transforms

B) Which of the following scripts is not included to train in Cloud MLE?

1. distributed-training.sh
2. ~~hyper-tune.sh~~
3. single-instance-training.sh
4. create-prediction-service.sh
5. None of the above

C) Which of the following scripts is not needed to make prediction service using scikit-learn?

1. export JOB\_NAME
2. export MODEL\_NAME
3. export GCS\_FILE\_DIR
4. export TEST\_FILE
5. ~~None of the above~~

D) Which of the following is not a BI Products partner of Google Cloud ML Engine?

1. Tableau
2. Power BI
3. ~~Looker~~
4. None of the above

E) Which compute service lets customers supply chunks of code, which get run on-demand in response to events, on infrastructure wholly managed by Google?

1. Cloud Functions
2. Compute Engine
3. Kubernetes Engine
4. App Engine

Can some point out which is the correct answer? And why do you think it is?

Also on E question, I thought its Cloud Functions but Accidentally chose No.3 and Got it correct! Can someone enlighten me?

By the way this is the failed exam and would like to learn from my mistakes in order to be better. This is one time exam. Every time I take the exam it changes the question.",LearnMachineLearning
rjyll8,1639926588.0,Here's a list of 8000+ programming resources.,"I Hope this helps list you...
This is a list of resources for Python, Machine Learning, Web Design etc.

List : [https://resorcery.pages.dev/](https://resorcery.pages.dev/)

Write your feedback in the comments.",LearnMachineLearning
rjybto,1639925743.0,"Regularization techniques for beginners - L1 and L2 penalties, Dropout, and Layer Normalization","I hope that these posts are helpful to someone who wants to understand how these regularization techniques work.

[L1 and L2 regularization](https://medium.com/@neuralthreads/l1-l2-regularization-adding-penalties-to-the-loss-function-b5c330d30b3f)

[Dropout](https://medium.com/@neuralthreads/dropout-regularization-technique-that-clicked-in-geoffrey-hintons-mind-at-a-bank-fa7fa8c5e1fb)

[Layer Normalization, part 1](https://medium.com/@neuralthreads/layer-normalization-and-how-to-compute-its-jacobian-for-backpropagation-55a549d5936f)

[Layer Normalization, part 2](https://medium.com/@neuralthreads/layer-normalization-applied-on-a-neural-network-f6ad51341726)

you can join me on [youtube](https://www.youtube.com/channel/UCorUqR9utU1SBdDeMLpGzKA/videos) and [Reddit](https://www.reddit.com/r/neuralthreads/)",LearnMachineLearning
rjvmw0,1639916244.0,Can machine learning a hobby or is it a full time commitment?,"Hey all,

I'm asking this question today as I'm quite torn about my future studies. Im thinking of doing a double major with microbiology/or biomed with CS but I dont really want to put myself at more stress during my time at uni but I am prepared to do so. I heard about not really needing uni to learn computer related stuff and can self learn these things but is it more appealing to a company for a individual with a degree?

I know there is more career prospects in computer science but I dont really want to start all over again at uni (I'm a 2nd year science student and plus I  dont even know how to program yet).

I'm committed because I want a good future but am I getting myself in some deep waters or should I just swim away while I can?

Any advice would be appreciated, cheers.",LearnMachineLearning
rju3nf,1639909762.0,How to detect timing anomalies using ML methods," Hello Everyone,

I am really new to machine learning algorithms. Right now, there is a research project on my desk and being expected addressed with some ML methods.

We would like to detect the timing anomalies in the power traces of our embedded system. Let's say I can sample the system voltage at some rate for 1 second exactly after the moment when the system powers on (to make the scenario as simple as possible for an easy start). The system embedded power-on behaviors are fixed so we expect the voltage patterns should be the same for each run as well if the noises are not considered.

What I have done is to collect 30 traces (20 items for training while 10 for validation) as the training set, hoping the trained model can give me a prediction at each timestamp during the testing phase so that a threshold can be set to alert if the incoming values deviate too far from the prediction (it is an anomaly.) The collected 30 traces would not be exactly the same considering the random noises and the different micro-architectural states.

The main reference I am using is this tutorial [https://becominghuman.ai/time-series-and-how-to-detect-anomalies-in-them-part-ii-bde9e69d0aaf](https://becominghuman.ai/time-series-and-how-to-detect-anomalies-in-them-part-ii-bde9e69d0aaf). I select the CNN algorithm and perform the training and retrieve the prediction waveform as shown below.

&#x200B;

[blue is the reference trace without timing anomalies while red is the prediction trace](https://preview.redd.it/81baecn08h681.png?width=1182&format=png&auto=webp&s=4078d57700757c9213c443e951f462f9ae00c3a9)

It looks like the CNN captures some patterns but the amplitude seems to appear a proportional difference. The below are the squared errors between the two traces. The maximum error is around 25.

&#x200B;

[square errors between the reference trace and the prediction one](https://preview.redd.it/llsqro128h681.png?width=832&format=png&auto=webp&s=ba3b3e137a349a1f4f4cff6803e2376e149aa753)

However, if I compare the prediction trace with a corrupted trace with many timing anomalies. The square errors report the deviations at more timestamps but the maximum value is smaller than 25... So I cannot set a static threshold for claiming those anomalies...

&#x200B;

[blue is the corrupted trace with timing anomalies while red is the prediction trace](https://preview.redd.it/bg2rveh38h681.png?width=1197&format=png&auto=webp&s=bc17713a97ee93a88b48051941b2b148ecd73abe)

&#x200B;

[square errors between the corrupted trace and the prediction one](https://preview.redd.it/guwoks758h681.png?width=832&format=png&auto=webp&s=a001e64a61d66df7846c04c2b88bb3aec726da3e)

With that, I do have a few quick questions which might look naive to you experts.

1. Is CNN the correct option to go with for this purpose? Otherwise, what kind of algorithm I should use for this project in your opinion?
2. Does the prediction trace make sense? Why does the amplitude difference occur between the predictions and reference?
3. Is it possible to set a dynamic threshold to differentiate the timing anomalies in the latter scenario?
4. I am using Pytorch. Any suggestions you can offer to run the training procedure faster. The training took me more than 1 day...

I really appreciate you finishing reading such a long question. Really looking forward to hearing from you about any suggestions and opinions. Thank you!",LearnMachineLearning
rjmsc4,1639881256.0,"What do you think about the educational game ""While True Learn()""",[https://www.youtube.com/watch?v=eVmaNJZdQ5k](https://www.youtube.com/watch?v=eVmaNJZdQ5k),LearnMachineLearning
rjjaht,1639869706.0,How to start working in DL/RL,"Hello! First of all, sorry if this kind of post is not allowed.

I'm a web developer currently trying to start my journey as a Machine Learning engineer, and specially in Deep learning/Reinforcement Learning.

I would like to be a freelancer, but for the purposes of starting I'm ok with being an employee. I've already done 2 courses, one formal and the other one via Youtube, and been doing practices since 2 years. I am right now doing competitions in www.kaggle.com.


What would people already working in this niche recommend to successfully insert myself?

Any guidance would be appreciated. Thanks!",LearnMachineLearning
rjgkbl,1639861427.0,Text analysis in low-resource language,"This is a long shot but I've hit a dead end in my research, and I hope someone from this community can help.

My company is doing an internal project for a text analytics product but **for Arabic**. I've found a few great resources that have helped me so far (AraBert, ArBert, etc.), but there are a few things such as keyword extraction, hate speech detection, and NER that aren't very available. It's mostly research paper with pilot studies, or fairly outdated notebooks.

I understand text analysis and NLP in general for the Arabic language is still in its early stage, but would anyone know anything that can help? Any models/transformers I can fine-tune? Datasets I can use? Particularly in the three things I mentioned above.

Thanks.",LearnMachineLearning
rjeqln,1639855994.0,Fastai is killing me,"Hey guys, I've been working through the fastai course for a while now. I'm currently on chapter 8 and it feels like I'm retaining maybe 20% of the information that is being taught.

I'm not rushing through it or anything, I try to understand each and every topic but it feels like some things aren't quite explained to you and trying to search it up puts you through some large math rabbit hole.

Is this a normal feeling for a new user that's taking the course? In a few weeks I'm going to start 2 courses that are supposed to teach the essential mathematics and stats for machine learning so I'm hoping that with the completion of those two courses this stuff will all start to make sense to me.

What do you guys think? Should I go even slower? Should I start learning the math as soon as possible? Did anyone else have this sort of experience with fastai? Does it getter better later on in the course?

Thanks in advance guys!",LearnMachineLearning
rjde7a,1639852078.0,PPML Series #2 - Federated Optimization Algorithms - FedSGD and FedAvg,"This is my second post on Privacy-preserving machine learning. This post talks about the optimization algorithms which are used to train machine learning models in a Federated Learning setting. FedAvg was a prominent algorithm that came out in 2016 but is still used today. I wrote a bit about it earlier, in my [Twitter thread](https://twitter.com/shreyansh_26/status/1463454860460785670).

Read more here -  [PPML Series #2 - Federated Optimization Algorithms - FedSGD and FedAvg](https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/)",LearnMachineLearning
rjblex,1639846808.0,Multi-layer neural network from scratch in Go,"This post covers the concepts behind neural networks and walks through the implementation of a multi-layer network in Go that performs quite well on the MNIST dataset: [https://ataylor.io/blog/go-mlp/](https://ataylor.io/blog/go-mlp/)

I put this post together with a focus on better understanding backpropagation in particular. Hope some others may find it useful!",LearnMachineLearning
rj5xak,1639828161.0,Can we build a User-User Recommender with Tensorflow Recommenders?," Title. I want to make a Recommendation system that specifically recommends profiles of people to other people, similar to how Tinder or Bumble functions. Is this possible with TFRS? So far, I’ve only seen examples of User-Item interactions and not user-user per se. Please tell me if there’s any resource I could refer to for the same. Thank you so much.",LearnMachineLearning
rj5rfb,1639827463.0,Need Help!,"Hello,

I am not sure if I am being silly by posting this but I don’t know anyone who can guide or help me with this. This is why I am posting it.

I started working on NLP at the beginning of the lockdown (almost 2 years ago) . Since then I love working on NLP and I did invest countless hours on it (Before that I already had a good understanding of Data Science). The more I learned about NLP the more I liked it. There was a time when I was working full time and still giving 3-4 hours everyday to learn NLP. While doing all this at the beginning of this year I did 3 months of non-paid freelancing as well and there I learned more about NLP and Web Scraping. Eventually I was at a stage where I could say I know enough (I guess) to look for freelance or a full time NLP job.

Finally when I started looking for freelance work or a fulltime job I realized in the NLP field people look for PhD or MS holders candidates and in my case I am not even graduate. I thought okay I do not have this degree but what if I will show my talents, then I might be able to find the right people. I started working on a few projects and posting it on different social media sites but that was not helpful. Now I am at the stage where I feel like giving up on NLP and trying to just focus on Python and get something there. But my heart still wants to work on NLP. I am not sure what to do, please help. Thanks in advance.",LearnMachineLearning
rj4r42,1639823269.0,"I am still unfamiliar with unsupervised learning or clustering, here are some questions","Do i still need to do one hot encoding for categorical features for clustering like i did in supervised classification? i have mixed numerical and categorical features

Do i still need train test split like what i did in supervised learning?

Di i still need to deal with imbalanced data?

i am using python btw",LearnMachineLearning
rj4ju4,1639822389.0,am making a project what do you guys think about the idea.,"Hello,
We need your help in voting for a program “Doctor in a Box”- DiB- that analyzes medical data through artificial intelligence and from there DiB suggests what the patient can do or eat before his health problem does not increase. The program presents medical reports in a simplified way that is understandable to average people to achieve a healthier life and reduce the need for regular hospital visits.
DiB also provides a reminder for medications timing. Adding, DiB can act as a lifesaver in case of emergency because DiB will automatically contact the hospital or one of the relatives in the event of loss of consciousness or illness

Voting link is:
https://aiqom.ai/dashboard/challenge-project/7",LearnMachineLearning
ris9cv,1639779233.0,"ML engineers who work for a company or are in research, which subfield do you work in and what are the key resources you used to learn ?",Please do be as specific as possible so that new comers have a  good idea of what diverse fields are there and what are some good resources for such fields.,LearnMachineLearning
ris3un,1639778798.0,Grouped bagging/boosting in classifiers?,"Hey,

I was wondering if it's possible to have a classifier that relies on bagging/boosting (like a Random Forest, LGBM or XGBoost) keep groups of observations together when doing the bagging.

For example, let's say I have observations on a bunch of people throughout their teenage years and early adulthood, and I want to predict whether they will eventually get married.

If I train a model on all of those observations, the ""out of sample"" bag may get observations for the same person at a different point in their life (and thus the same outcome), leaking information and overfitting. If I have data points that are very unique to each person and remain relatively constant, that problem gets even worse.

How can I deal with this? So far the solution was to just reduce the feature set to keep the most generic features and avoid overfitting, but I know that can't be the best solution.",LearnMachineLearning
rirflx,1639776767.0,Machine Learning Book for Behavior Analysis with R,"This book is for those interested in learning machine learning concepts using the R programming language. The book focuses on behavior analysis but the concepts can be applied to any field. A complete **free** version of the book is available at [https://enriquegit.github.io/behavior-free/](https://enriquegit.github.io/behavior-free/)

&#x200B;

https://preview.redd.it/3nbp9pabab681.png?width=1386&format=png&auto=webp&s=c041bcbe4eaba6ae79e6d68be9cc5d124df8da74",LearnMachineLearning
rim2dg,1639761170.0,Any online course recommendations for learning computer vision?,"Title, for context just finished Andrew ng's ML course and have finished one term of uni with an ML module. I don't know for certain what difficulty bracket is fine after those but id greatly appreciate any beginner / intermediate course recommendations you guys have.",LearnMachineLearning
ril4sy,1639758583.0,GPT-3 libraries,"Has anybody made a full-fledged library of functions powered by GPT-3?

Does Spacy use GPT-3 in any of their methods, for example?

Thanks",LearnMachineLearning
riknbs,1639757184.0,"Have image generation project idea, need tool pointers though","I have a bunch of photographs I've taken over the years (primarily  landscape). I'd like to feed them to a network of some kind to have it  learn what my 'photographic style' is, then generate new images based on  those. What tools/libraries should I be looking into? I've been  programming for like 10 years but have never done anything ML related,  and honestly am not super interested in the technical details of a project, more the output. So the easier the tool, the better. FWIW I can  supply probably 1000 high res landscape photos. Cheers!",LearnMachineLearning
ricr2w,1639728997.0,"Is ""gaussian likelihood"" loss equivalent to MSE loss for a VAE?","In [this tutorial](https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed), it appears that the author uses ""gaussian likelihood"" with a log scale as a reconstruction loss, and explains it as ""the likelihood of encountering that image"". (If you cannot view the link, the code is [here](https://gist.githubusercontent.com/williamFalcon/1da585dd427002bca915f9ec323fbbbe/raw/a10b32ffb6f77fb7b1fff6310af1b29966495f05/vae.py) - see the ""gaussian_likelihood"" function and ""recon_loss"")

Other VAE implementations typically use the MSE as a reconstruction loss instead, and the typical justification is that they are equivalent for a gaussian distribution.

Other places I've seen dismiss MSE as a reconstruction loss, e.g. [this link](https://old.reddit.com/r/MachineLearning/comments/4ujr2s/what_loss_to_use_for_variational_auto_encoder/d5qj3m9/) which states it's basically a gaussian with a fixed variance, which - from what I understand - is not the same as what is implemented in the tutorial, which is sampling from a gaussian distribution with a dynamic variance.

For what it's worth, I swapped out the gaussian_likelihood loss with an MSELoss and it didn't perform as well - the KL term basically went down to 0 almost immediately, as did the reconstruction loss. With the original implementation, though, it was significantly better, and the KL term kept increasing over time.

I'm rather new to VAEs (and a bit bad at statistics) so I'm most likely missing something. What is the reason for this discrepancy?",LearnMachineLearning
rick1i,1639728167.0,[D] Should I take an MS for Computer Science or and MS specialized for Artificial Intelligence?,"So I was deciding to what masters degree I would be taking. AI degree consists of 10% CS courses and the rest more on machine learning. I am taking a few courses by Andrew Ng on coursera to prepare for it. On the other hand, CS degree has a more comprehensive coursework with CS like databases, theoretical computing, and software engineering. It only contains a few machine learnign courses.

So I was asking because I want to know how employers or practictioner view one over the other? But based on my readings, CS degree offers more flexibility on employmeny while the AI degree will project you to the ML path.

Right now I’m leaning towards the AI deg as it heavily works with mulitple algorithms that CS degree does not offer.",LearnMachineLearning
ria7qt,1639719446.0,Training multiple regressors with independent weights on same dataset [PyTorch],"Hi,

I'd like to train multiple linear regressors (20000 odd) in pytorch that take the same input features but have different y\_true (ground truths).  Also, I'd like to have different weights for every one of these predictors

The architecture that I'm supposed to use for this task is a simple 2-layer neural net.

I'm stuck with how to come up with a way to train these classifiers independently. I thought of making a list of nn.Module objects, and then invoking one by one with different y 's. However, I can't seem to find any documentation against the same... Could someone point out to such resource?",LearnMachineLearning
ri8nfe,1639714175.0,How to convert CSV data into graph dataset anomaly detection?,"Hello there,

&#x200B;

I am working on an anomaly detection problem using a graph neural network. However, I am not sure which will be the best way to convert my CSV data into graph data. I have 115 different attributes for each timestamp. You can find the example dataset here; [https://gist.github.com/vgthengane/0b20f1b018b7ea06799f3ae00e167a7c](https://gist.github.com/vgthengane/0b20f1b018b7ea06799f3ae00e167a7c)

I want to test each time stamp for an anomaly. Which will be the best way to convert CSV data into graph data?

I was thinking of considering each attribute as a node and connecting all of them with each other by the undirected edge (since I don't know their connection to each other). Then I will have a single feature for each node and a lot of edges. To be specific \`feature\_shape == \[115, 1\]\` and \`edge\_shape == \[2, 6555\]\`. But I am not sure if this is a good way to model it, and what will be the best method to work on this on node embedding or graph embedding?

PS: I am using an autoencoder model to detect anomalies.

Thanks in advance. Hope I am clear about my question

Vishal T",LearnMachineLearning
ri4wl9,1639702442.0,Suggestions on My Learning Path,"Hello everyone! I made myself an ML/DL learning path and ask you to make comments, suggestions on my learning path. Please note, I've already finished [Codecademy Data Scientist Path](https://www.codecademy.com/learn/paths/data-science) and want to keep learning ML/DL.

First, I have to say that I want to get my hands dirty in the first weeks of this path instead of spending weeks just to learn theory.

So, my intention is to learn ML/DL by using ML/DL and start building my portfolio.

**Courses/Specializations:**

1 - [Practical Deep Learning for Coders - fast.ai](https://course.fast.ai/)

2 - [Part 2: Deep Learning from the Foundations - fast.ai](https://course19.fast.ai/part2)

3 - [Deep Learning Specialization - deeplearning.ai](https://www.coursera.org/specializations/deep-learning)

4 - [TensorFlow Developer Professional Certificate - deeplearning.ai](https://www.coursera.org/professional-certificates/tensorflow-in-practice)

5 - [AWS Fundamentals Specialization - AWS](https://www.coursera.org/specializations/aws-fundamentals) (for the next specialization)

6 - [Practical Data Science Specialization - deeplearning.ai, AWS](https://www.coursera.org/specializations/practical-data-science)

7 - [TensorFlow: Advanced Techniques Specialization - deeplearning.ai](https://www.coursera.org/specializations/tensorflow-advanced-techniques)

8 - [Code-First Introduction to Natural Language Processing - fast.ai](https://www.fast.ai/2019/07/08/fastai-nlp/)

9 - [Natural Language Processing Specialization - deeplearning.ai](https://www.coursera.org/specializations/natural-language-processing)

**Books:**

1 - Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems - Aurelien Geron

2 -  AI and Machine Learning for Coders - Laurence Moroney

&#x200B;

Please feel free to advise more courses or books.

Thanks in advance :)",LearnMachineLearning
ri1qvp,1639693031.0,Having trouble with scipy.optimize.minimize while building a logistic regression from scratch,"Hi,

I'm am trying to build logistic regression model to predict heart diseases from this Kaggle dataset ([https://www.kaggle.com/ronitf/heart-disease-uci](https://www.kaggle.com/ronitf/heart-disease-uci)). I am unfortunately struggling with an issue for which I cannot find an answer with Google. Here's my code.

    df = pd.read_csv('heart_classification.csv',delimiter = ',')

    #shuffling it as target are all ordered
    df = df.sample(frac=1).reset_index(drop=True)
    data = df.to_numpy()

    #creating training and validation set
    m,n = data.shape
    train_nb = round(0.8*m)
    X_train,y_train =  data[:train_nb,:-1], data[:train_nb,-1]
    X_train = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)
    X_val,y_val =  data[train_nb:,:-1], data[train_nb:,-1]
    features = df.columns

    #initializing theta
    theta = np.random.randn(n)

    def sigmoid(z):
        return 1/(1+np.exp(-z))

    def cost_function(theta,X,y):
        m = X.shape[0]
        pred = sigmoid(np.dot(X,theta))
        J =  (-1/m) * np.sum(np.dot(y,np.log(pred))+ np.dot((1-y),np.log(1-pred)))
        grad = (1/m) * np.dot(X.T,(pred-y))
        return J, grad

    options = {'maxiter': 500}
    res = optimize.minimize(cost_function,
                            theta,
                            (X_train, y_train),
                            jac=True,
                            method='TNC',
                            options=options)
    J = res.fun
    theta = res.x

Executing this gives me `TypeError: 'NoneType' object is not subscriptable`. I tried removing both method and options arguments

I am using `optimize.minimize` as that is how I learnt it while completing Andrew Ng's Coursera course. I could code a gradient descent if it's the best way to do it but did not try yet.

Any idea why this is happening and how I could change my code? Any unrelated comment on its structure is welcomed too.

Thanks!",LearnMachineLearning
ri11hg,1639691049.0,Autoencoder for very high dimensional data,"I have data that has over 100k features, and I wanted to try to reduce them with an autoencoder. But just making the framework with all of those inputs take up way too much memory.

Is there a way I can make an autoencoder with this many features?",LearnMachineLearning
rhzj3y,1639686830.0,GIS / ML newbie seeking help for land classification project,"Hello all :D !

I've recently started learning a bit about GIS and Deep Learning.

In order to learn more and practice my skills, I'm trying to create a dataset from open source data and training some models on it to try and recognize various types of land classes / usage.

Here was my initial thought process of my project:

1 ) Acquire data about land usage (which will be the prediction target) and acquire real map data.

2) Combine them to obtain labeled images.

3) Use supervised training on the labeled dataset.

4) Use the trained models for new inferences.

&#x200B;

So far, I have found an interesting gpkg for my truth labels (on Urban Atlas).

Using geopandas I'm currently able to open this gpkg and access several columns of data such as land class (example: water of industrial) as well as the geometry of each sample (contains MULTIPOLYGON objects).

I would like to use some jp2 band files (obtained via Copernicus' sentinel 2 program), merge them as one tif file containing the different bands and combine/overlap these real satellite images to the land usage map contained in the previously mentionned gpkg.

&#x200B;

I thought I would then cut out square pieces/rasters/images of the map and save each of them with the label/class as filename and later train a CNN on them (just like with the EuroSat dataset). Using CNNs might not be the best/correct approach though, I saw articles mentionning R-CNN and U-Net architectures which seemed interesting but I don't know much about their use case/scenario and if they would be interesting to use in my project.

&#x200B;

Is there some kind and knowledgeable soul out there who can guide me / help me figure this problem out and / or enlighten me on basic GIS / ML concepts that I might seem to misunderstand ?

Any help is appreciated :)

&#x200B;

I'm adding that I'm working with python (currently using a jupyter notebook), I'm trying to only use open-source stuff (and if possible staying away from using software like QGIS)",LearnMachineLearning
rhyhah,1639683929.0,Looking for data science mentees,"Hi, I am a software developer in Goldman Sachs and I am looking for websites where I can work part time as a beta tester or a mentor for students on an hourly basis. Please let me know if you are aware of such opportunities.


ex. Coursera mentor, Udacity mentor",LearnMachineLearning
rhy0x6,1639682650.0,NLP model and document length,"I need some advice on some basics around NLP. I've created a model in Python in which I predict the political ideology of the author based upon a tweet (3000 single tweets per author, total dataset is around 300k) - model is LogisticRegression utilizing Scikit-learn. How does doc length effect accuracy on a trained model? I.E. it was trained utilizing the standard length of a tweet - what happens if I feed in something much smaller or much larger? Does a model hold up regardless of the size of the document - or does it only excel when utilizing a similar length? Just trying to wrap my brain around the concept in general, not really related to the single model. Thanks!",LearnMachineLearning
rhxfwl,1639681070.0,How To Make A Simple Stock Price Prediction Google Colabs?,"Hi all,

I have a very simple csv file that is just the closing S&P 500 price for X amount of days and the date [found here](https://www.kaggle.com/pdquant/stocks-significance-testing-p-hacking/data)

What I'm trying to do is just make a prediction of some sort using this data.

I was going off of this (scikit-learn estimator)[https://scikit-learn.org/stable/tutorial/machine_learning_map/] but it brings me to Lasso when i thought I would use something like LTSM.

Anyone with any help be appreciated!",LearnMachineLearning
rhwua0,1639679331.0,undersampling in R,"Hi guys!

&#x200B;

I have a highly unbalanced dataset (93% accuracy on ZeroR) and I want to undersample it a bit to reduce ZeroR accuracy, for demonstration purposes in a Data Analytics course in uni.

&#x200B;

However, I'm uncertain how to go about it. First of all, how much should I remove of the overrepresented value,  and how do I do it?

&#x200B;

I kinda want to just cut maybe 30% of the overrepresented data, simply by sorting the dataframe by target = true/false, and then delete the bottom 30% of the values by their row number, as the data is super jumbled anyways, so sorting by target = true and deleting bottom values removes a relatively random cut of the target = false rows (target = false is the overrepresented value).

Also, how do I actually remove a set amount of rows based on the row number? lol",LearnMachineLearning
rhv6u9,1639674667.0,Synthetic time series data generation," I  want to generate time series tabular data. Most of generative deep  learning models consists of VAE and/or GAN which are for most part  relating to images, videos, etc.

Can  you please point me to relevant tutorial souces (if it includes code  along with theory, all the more better) pertaining to synthethic time  series data generation using deep learning models or other techniques?",LearnMachineLearning
rhrqcf,1639664519.0,Unsupervised Learning for String Matching in Python - can I have advice on how to go about this?,"Hello all,

I'm a Data Engineer with around 8 months experience who is currently working for a company with one recurring requests - fuzzy matching two lists of names against each other.

Our current solution is using SSIS' Fuzzy Lookup feature although, in my opinion, this simply doesn't scale well and has limited features.

What I envision to create is something which is, ideally, an ML model which will improve based on whether a user tells the model something is or isn't a match.  At the moment, the threshold for SSIS is simply too static and is missing out matches I'd like to have included.  On top of that, having a manually maintained model is something I don't like the idea of - I want to make this (fairly) hands off.

I'm familiar with working in Python and it's various libraries and have a very basic understanding of ML, although am not particularly sure if I'm doing the right thing.  I have tried `FuzzyWuzzy` and `RapidFuzz` as well as tried `dedupe` and the `RecordLinkageToolkit`, however I can't seem to wrap my head around what I'd need to do in order to persist a model and have it improved in the way that I'm trying to achieve above.

I totally get this isn't a small project so ready to put the time in.  What I'd like to know is

* Can I achieve what I want with the libraries I've already mentioned?

* If not, can I incorporate `Scikitlearn` into fuzzy matching using one of the libraries above?

* Is there a service which already exists and can handle enterprise scale fuzzy matching?

Thank you!",LearnMachineLearning
rhpymx,1639658525.0,Citable literature,"Hey guys :)


I am writing a chatbot for my bachelors thesis in python & using tensorflow. I already got the book from chollet ""deep learning with python"". However for the first part of my thesis I have to give an overview about the different techniques, such as reinforcement learning, supervised learning etc.


This is why I came here, I wanted to ask if you have any advise on literature about these techniques.


I need them to be from academic journals, academic books etc. so no wikipedia, online tutorial and so on.


Any help appreciated!",LearnMachineLearning
rhp5gj,1639655392.0,From where to learn python for specifically for ML ?,"I know python basics, now i have started to read book on ML ""Hands-on ML with Scikit.....""
There are some codes in python
I can't understand them
I want to learn python for ML perspective
Please suggest me resources",LearnMachineLearning
rhnnaw,1639648967.0,How to approach a scheduling problem?,"I've got a problem whereby I'm trying to match resources to tasks. Think of planning the maintenance of a train and I need to know which people go to which trains.

I understand this is more of an optimization problem but I'm struggling to understand how to frame the problem.

Any pointers or papers or topics would be appreciated.",LearnMachineLearning
rhmz6m,1639645988.0,Have you ever taken Andrew Ng's Machine Learning course in Coursera?,"My friend and I are thinking to enroll in this course to start the new year of 2022. Until then, we saw that it's an 11-week course and now we're thinking if we should start it with a community to increase accountability and completion rate,

You may wonder why we don't just take a ML cohort-based courses? Well that's a good idea but we don't have a budget. It's rare to find good CBCs in a low cost that are well taught by famous instructor like Andrew Ng from Stanford.

That's why we find this course valuable since it's free but we don't want to study alone or just the two of us either.

So, do you know anyone who's also planning to start a year by learning a new skill in ML? It would be nice to start it on January.

Here's the link by the way

[https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning)",LearnMachineLearning
rhm5u9,1639642335.0,ML dataset to help derive insights into the nature of UN SDGs,"I'm sharing an open dataset for those building machine learning models related to UN Sustainable Development Goals (SDGs). The dataset contains tens of thousands of text excerpts which were validated by more than 1,000 citizen scientists from over 100 countries with respect to the SDGs. The dataset is called **OSDG Community Dataset (OSDG-CD)**, and is freely accessible on Zenodo: [https://zenodo.org/record/5550238](https://zenodo.org/record/5550238).

If you end up using the dataset, share your results with us [via email](mailto:community@osdg.ai). We'd love to see what you come up with!",LearnMachineLearning
rhliul,1639639701.0,Loan Credit Score for a beginner! Which model to use?,"Hello I am new to machine learning and I am intending to start on a project on loan credit score

The project is basically using information from users to produce a credit score grade.

For example, in information, we have both categorical data and continuous data,

e.g.  Categorical : EMPLOYMENT STATUS:  BUSINESS\_OWNERS (1) , EMPLOYED (2) , HOMEMAKER (3)

Continuous:  BIZ\_LAST\_FINANCIAL\_YEAR\_REVENUE: arranging into tiers, making them into different categoricals **Tier 1** \- 0- $100,000, **Tier 2** $100,001 - $ 200,000 ....

Finally, I want to produce a credit score grade:

Credit Score \~ EMPLOYMENT\_STATUS + BIZ\_LAST\_FINANCIAL\_YEAR\_REVENUE + .... many other variables

I want to use weighted variables. For example: In EMPLOYMENT\_STATUS:

BUSINESS\_OWNERS  = 100

EMPLOYED = 70

HOMEMAKER = 50

and also in BIZ\_LAST\_FINANCIAL\_YEAR\_ REVENUE

TIER 1 = 100

TIER 2 = 70

TIER 3 = 50

and the final credit score (in the context of business owners (1) and Tier 2)

Credit Score  (170) = 100 (1) + 70 (2)

&#x200B;

1. How do I go about using weighted kind of linear regressions? are there any reference sites?
2. Since I am using a lot of variables, and have many unnecessary variables, is Lasso Regression possible for the tuning and getting the job done?
3. How do I go about improving my model? Are there other ML models that can be considered?

Thank you from a beginner!",LearnMachineLearning
rhl2pj,1639637944.0,Trying to see if anyone has a clue to what is hidden in my screen. Not sure I'm doing this right either new to reddit. How would I upload a screen picture to show what I'm speaking about,,LearnMachineLearning
rhjpcu,1639632978.0,Question about gated recurrent units and MLPs,"Hello everyone,

Question about a personal machine learning project that I'm working on: I'm currently working with features extracted from a CNN (which learned to correctly classify images from a biological dataset). Now my goal is to use those feature vectors and re-learn to cluster them according to a (ground-truth) distance matrix. I have successfully done so with a simple MLP that uses a simple mean squared error loss (which tries to minimize distances between the learned features and those of the ground-truth distance matrix - a simple regression task that tries to emulate the distance matrix). My question is the following: Would it be appropriate to use something like a GRU (for instance) instead, to work on this task?

I understand that GRUs are usually used with sequence data, but in my case, they seem to work slightly better than MLPs (when it comes to making predictions) and I'm not sure why. Is that reasonable at all, or are there more appropriate methods that can be used to learn distances between feature vectors?

Additional info: I am trying to make sense of the feature vectors (and find a pattern in those sequences) and learn the ground-truth distances, which is why I thought about using GRUs.

Not a computer scientist, so would appreciate any advice!

Thanks in advance!",LearnMachineLearning
rhgoj3,1639623401.0,"I am working on a Speech-To-Text transcriber. Any projects, tutorials or tips?","Hi guys, I am working on a simple Speech-To-Text transcriber using TensorFlow and Python but I want to have a look at similar projects which have been done earlier for my research.


Here is what I hope to achieve.
1. Imagine having a video stream input or a video file, I want to transcribe what the speaker says to text-based output.
2. I want to develop a model, train it.
3. Get the text-based output.
4. (Furthermore) Develop the model to work better, and scale.


Thanks in advance!",LearnMachineLearning
rhcxmu,1639611978.0,Which one is better: UVA Masters in DS or Georgia Tech Masters in Data Analytics?,"See links below:

I am currently a software developer trying to get into data science/ML. I’ve been taking online classes on my own but my company is willing to pay for my masters program of my choice. Which one should I go with??? Uva or Georgia tech?


[UVA Program](https://datascience.virginia.edu/degrees/info/online-msds-academics)

[GTech Program](https://pe.gatech.edu/degrees/analytics/curriculum)",LearnMachineLearning
rhcwxh,1639611918.0,"Multivariate, grouped time series classification - what are my options?","Hi! I'm looking for some ideas on how to handle a multivariate time series classification problem.

Specifically, I'm dealing with many hundreds of time series of varying length (but within the same ballpark, let's say 30% difference between the longest and shortest) with the same features and which should behave similarly. At the end of each time series is a binary outcome. Each time series is around 5000 observations of 32 features.

My challenge is that I want to be able to pass live data through a model (say halfway through the time series) and classify how likely the positive outcome is. Most material I've seen relates to forecasting a single time series, not learning characteristics from many historical individual time series to classify a new one midway through.

Right now my approach is to handle each observation as independent with some manual feature engineering and use grouped cross validation to avoid horribly overfitting on the fact that I'm training on multiple instances of the same time series (and thus the same outcome).

I'm using an LGBM which gives acceptable and even good results, but I'm sure this can't be the best approach and while the results work there's a lot of variance in the prediction across the time series. Ideally we'd have more uncertainty at the start and then a clearer picture starts to form, but right now there's a lot of jumping around in the probability score.

I'm not very familiar with time series modeling - I imagine deep learning could be a good approach with an LSTM or something like that but I'm not very experienced with DL either. I'm also looking into sktime to see if there's anything that would work for me out of the box.

Any suggestions on reading material or approaches I could use? Any help is appreciated!

Cheers.",LearnMachineLearning
rh942h,1639601725.0,Random variable vs instances in ML/stats,"

Lets say I am talking about the dataset used as input to a stats/ML algorithm:

For example, in this paper,

[https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)

, they have the passage:

&#x200B;

https://preview.redd.it/n2lqw7g4sr581.png?width=786&format=png&auto=webp&s=8c67085ef1f2402ec06c60ed324dcd5b671b412c

Should this dataset be considered a sequence of IID random vectors, or a instances of those vectors? I assumed it was the first, but then all of the people in this stats stackexchange discussion claims its the 2nd.

[https://stats.stackexchange.com/questions/358342/what-the-relation-between-a-random-variable-and-a-sample-or-dataset-in-machine?rq=1#](https://stats.stackexchange.com/questions/358342/what-the-relation-between-a-random-variable-and-a-sample-or-dataset-in-machine?rq=1#)",LearnMachineLearning
rh8s5l,1639600801.0,Beginner: How to integrate ML on a web page?,"&#x200B;

https://preview.redd.it/7g7qv1d6pr581.png?width=940&format=png&auto=webp&s=b4bd302dc123f8ea918145e12d177b1169972e19",LearnMachineLearning
rh7mf4,1639597715.0,Does the performance/compute power of my local machine matter?,"I'm a 1st year grad student in ML and after my 1st term I realized the main stuff I'm using on my laptop for ML are a lot of web browser tabs and a lot of visual studio code, since my code are running on a remote gpu.

I'm also considering getting a new laptop/desktop. So I'm wondering, as I step deeper into ML (planning to get a phd), will I be expected to use any heavier, compute-demanding stuff on my local machine, or is it still just web broswer+IDE? Does the performance/compute power of my local machine matter at all?",LearnMachineLearning
rh71mv,1639596210.0,Best books you would recommend to learn the math for machine learning.,"I have a math/physics background from college, so I’ve taken linear algebra, probability, and the calculus series already I just forgot most of it.

I am looking to refresh my skills and trying to find the appropriate books for the subjects.

Would something like James Stewart’s calculus work well? I know for linear algebra it’s recommended that Gilbert Strang is the go to, what about for probability?",LearnMachineLearning
rh3nj8,1639587532.0,RNN predict(),"Why am i getting 13 outputs (number of columns in x_train set) when using model.predict() function (using keras)?? I need one output. I can’t find anything online about this issue.

Model Code:

```
regressor = Sequential()
regressor.add(LSTM(units = 50, return_sequences=True, input_shape = (X_train.shape[1], 1)))
regressor.add(Dropout(0.2))
regressor.add(Dense(units = 1))
regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')
history = regressor.fit(X_train, y_train, epochs = 10, batch_size = 32)
```

Predict Code:

```
train_predict = regressor.predict(X_train)
test_predict = regressor.predict(Xtest)
test_predict.shape
```
Out[48]: (2142, 13, 1)",LearnMachineLearning
rh3hyx,1639587117.0,Recommendations for multi-terabyte time series Machine Learning project?,"So I have cleared a hurdle and can dig deeper into a project that I've wanted to learn more about and that's time series data analysis.

This is for an agriculture and mining company data set, this obviously impacts stock market indicators that we are also taking into account but this is more of an accurate mapping of what was. The predictor variable is ""why customers buy"" and there have been truckloads of ink spilled on this but they want to look at the data they have.

For those of you who have done this with a large amount of data I'm also going back to records from the 1700's so needless to say there is no lack of data, but it where would you start if the end goal is to understand buying patterns.

This is to be more of a calculated chunks over time process but I'm trying to find and isolate methods I can apply that will identify time series trends. There are so many techniques to choose from so I am feeling like its boiling the ocean by proxy in the GPU time we have.

There are a few ML books I have that I'll be reading back through but some of those are older and have a more general focus.

1. What books or papers do you know of
2. What did your project discover using what methods
3. What thinking outside the box can you recommend?",LearnMachineLearning
rh25sh,1639583588.0,I’ve made a search engine with 5000+ quality data science repositories to help you save time on your machine learning projects!,"I’ve been working in data science for 15+ years, and over the years, I’ve found so many awesome data science GitHub repositories, so I created a site to make it easy to explore the best ones. 

The site has more than 5k resources, for 60+ languages (but mostly Python, R & C++), in 90+ categories, and it will allow you to: 

* Have access to detailed stats about each repository (commits, number of contributors, number of stars, etc.)
* Filter by language, topic, repository type, and more to find the repositories that match your needs. 

Hope it helps! Let me know if you have any feedback on the website.  

**EDIT: Here is the link to the site:** [**gitsearcher.com**](https://gitsearcher.com)",LearnMachineLearning
rh0j05,1639579079.0,Variational AutoEncoder (TF2) - TypeError,"I am trying to implement a VAE for MNIST using convolutional layers using TensorFlow-2.6 and Python-3.9. The code I have is:

Data pre-processing steps-

    # input image dimensions
    img_rows, img_cols = 28, 28
    # Load MNIST dataset-
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    if tf.keras.backend.image_data_format() == 'channels_first':
        X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)
        X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)
        input_shape = (1, img_rows, img_cols)
    else:
        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
        input_shape = (img_rows, img_cols, 1)
    print(f""\ninput_shape to be used: {input_shape}"")
    #  input_shape to be used: (28, 28, 1)

    # Specify hyper-parameters-
    batch_size = 64
    num_classes = 10
    num_epochs = 200
    # Convert datasets to floating point types-
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    # Normalize the training and testing datasets-
    X_train /= 255.0
    X_test /= 255.0
    # convert class vectors/target to binary class matrices or one-hot encoded values-
    y_train = tf.keras.utils.to_categorical(y_train, num_classes)
    y_test = tf.keras.utils.to_categorical(y_test, num_classes)
    print(""\nDimensions of training and testing sets are:"")
    print(f""X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}"")
    print(f""X_test.shape: {X_test.shape}, y_test.shape: {y_test.shape}"")
    #  Dimensions of training and testing sets are:
    # X_train.shape: (60000, 28, 28, 1), y_train.shape: (60000, 10)
    # X_test.shape: (10000, 28, 28, 1), y_test.shape: (10000, 10)

&#x200B;

        # Specify latent space dimensions-
        latent_space_dim = 3

        # Define encoder-
        encoder_input = Input(shape = (28, 28, 1))

        x = Conv2D(
            filters = 32, kernel_size = 3,
            strides = 2, padding = 'same')(encoder_input)
        x = LeakyReLU()(x)

        x = Conv2D(
            filters = 64, kernel_size = 3,
            strides = 2, padding = 'same')(x)
        x = LeakyReLU()(x)

        x = Conv2D(
            filters = 64, kernel_size = 3,
            strides = 1, padding = 'same')(x)
        x = LeakyReLU()(x)

        x = Conv2D(
            filters = 64, kernel_size = 3,
            strides = 1, padding = 'same')(x)
        x = LeakyReLU()(x)

        shape_before_flattening = K.int_shape(x)[1:]
        x = Flatten()(x)

        # Instead of connecting the flattened layer directly to the 3-D latent space, we connect
        # it to layers 'mu' and 'log_var'-
        mu = Dense(units = latent_space_dim)(x)
        log_var = Dense(units = latent_space_dim)(x)

        # The Keras model that outputs the values of 'mu' & 'log_var' for a given input image-
        encoder_mu_log = Model(encoder_input, (mu, log_var))

        print(f""shape_before_flattening: {shape_before_flattening}"")
        # shape_before_flattening: (7, 7, 64)

        def sampling(args):
            mu, log_var = args
            epsilon = K.random_normal(shape = K.shape(mu), mean = 0.0, stddev = 1.0)
            return mu + K.exp(log_var / 2) * epsilon

        # This Lambda layer samples a point 'z' in the latent space from the normal distribution
        # defined by the parameters 'mu' and 'log_var'-
        encoder_output = Lambda(sampling)([mu, log_var])

        # The Keras model that defines the encoder — a model that takes an input image and encodes it
        # into the 2D latent space, by sampling a point from the multivariate normal distribution
        # defined by 'mu' and 'log_var'-
        encoder = Model(encoder_input, encoder_output)

        decoder_input = Input(shape = (latent_space_dim))

        x = Dense(np.prod(shape_before_flattening))(decoder_input)
        x = Reshape(shape_before_flattening)(x)

        x = Conv2DTranspose(
            filters = 64, kernel_size = (3, 3),
            strides = (1, 1), padding = 'same')(x)
        x = LeakyReLU()(x)

        x = Conv2DTranspose(
            filters = 64, kernel_size = (3, 3),
            strides = (2, 2), padding = 'same')(x)
        x = LeakyReLU()(x)

        x = Conv2DTranspose(
            filters = 32, kernel_size = (3, 3),
            strides = (2, 2), padding = 'same')(x)
        x = LeakyReLU()(x)

        x = Conv2DTranspose(
            filters = 1, kernel_size = (3, 3),
            strides = (1, 1), padding = 'same')(x)
        x = Activation('sigmoid')(x)

        decoder_output = x

        decoder = Model(decoder_input, decoder_output)

        # The complete autoencoder-

        # The input to the autoencoder is the same as the input to the encoder.
        model_input = encoder_input

        # The output from the autoencoder is the output from the encoder passed through
        # the decoder.
        model_output = decoder(encoder_output)

        # The Keras model that defines the full autoencoder—a model that takes an image,
        # and passes it through the encoder and back out through the decoder to generate
        # a reconstruction of the original image.
        model = Model(model_input, model_output)

The loss function is defined as follows:

        # Weight the reconstruction loss 'r_loss_factor' to ensure that it is well balanced with the KL divergence loss-
        r_loss_factor = 1000

        def vae_r_loss(y_true, y_pred):
            # Reconstruction loss-
            r_loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3])
            return r_loss_factor * r_loss

        def vae_kl_loss(y_true, y_pred):
            # KL-Divergence loss-
            kl_loss = -0.5 * K.sum(1 + log_var - K.square(mu) - K.exp(log_var), axis = 1)
            return kl_loss

        def vae_loss(y_true, y_pred):
            # VAE loss = Reconstruction loss + KL-Divergence loss
            r_loss = vae_r_loss(y_true, y_pred)
            kl_loss = vae_kl_loss(y_true, y_pred)
            return r_loss + kl_loss

        # Compile model-
        model.compile(
            optimizer = tf.keras.optimizers.Adam(learning_rate = 0.003),
            loss = vae_loss,
            metrics = [vae_r_loss, vae_kl_loss]
        )

        # Train autoencoder-
        training_hist = model.fit(
            x = X_train, y = X_train,
            batch_size = batch_size, shuffle = True,
            validation_data = (X_test, X_test),
            epochs = num_epochs
            )

which gives the error:

>\--------------------------------------------------------------------------- TypeError                                 Traceback (most recent call last) \~\\AppData\\Local\\Temp/ipykernel\_11960/995477119.py in <module>       1 # Train autoencoder- ----> 2 training\_hist = model.fit(       3     x = X\_train, y = X\_train,       4     batch\_size = batch\_size, shuffle = True,       5     validation\_data = (X\_test, X\_test), \~\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch\_size, epochs, verbose, callbacks, validation\_split, validation\_data, shuffle, class\_weight, sample\_weight, initial\_epoch, steps\_per\_epoch, validation\_steps, validation\_batch\_size, validation\_freq, max\_queue\_size, workers, use\_multiprocessing)    1191                 \_r=1):    1192               callbacks.on\_train\_batch\_begin(step) -> 1193 tmp\_logs = self.train\_function(iterator)    1194 if data\_handler.should\_sync:    1195                 context.async\_wait() \~\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\def\_function.py in \_\_call\_\_(self, \*args, \*\*kwds)     883     884 with OptionalXlaContext(self.\_jit\_compile): --> 885 result = self.\_call(\*args, \*\*kwds)     886     887       new\_tracing\_count = self.experimental\_get\_tracing\_count() \~\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\def\_function.py in \_call(self, \*args, \*\*kwds)     931 # This is the first call of \_\_call\_\_, so we have to initialize.     932       initializers = \[\] --> 933 self.\_initialize(args, kwds, add\_initializers\_to=initializers)     934 finally:     935 # At this point we know that the initialization is complete (or less \~\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\def\_function.py in \_initialize(self, args, kwds, add\_initializers\_to)     757     self.\_graph\_deleter = FunctionDeleter(self.\_lifted\_initializer\_graph)     758     self.\_concrete\_stateful\_fn = ( --> 759         self.\_stateful\_fn.\_get\_concrete\_function\_internal\_garbage\_collected(  # pylint: disable=protected-access     760             \*args, \*\*kwds))     761 \~\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in \_get\_concrete\_function\_internal\_garbage\_collected(self, \*args, \*\*kwargs)    3064       args, kwargs = None, None    3065 with self.\_lock: -> 3066 graph\_function, \_ = self.\_maybe\_define\_function(args, kwargs)    3067 return graph\_function    3068 \~\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in \_maybe\_define\_function(self, args, kwargs)    3461    3462           self.\_function\_cache.missed.add(call\_context\_key) -> 3463 graph\_function = self.\_create\_graph\_function(args, kwargs)    3464           self.\_function\_cache.primary\[cache\_key\] = graph\_function    3465 \~\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in \_create\_graph\_function(self, args, kwargs, override\_flat\_arg\_shapes)    3296     arg\_names = base\_arg\_names + missing\_arg\_names    3297     graph\_function = ConcreteFunction( -> 3298         func\_graph\_module.func\_graph\_from\_py\_func(    3299             self.\_name,    3300             self.\_python\_function, \~\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\func\_graph.py in func\_graph\_from\_py\_func(name, python\_func, args, kwargs, signature, func\_graph, autograph, autograph\_options, add\_control\_dependencies, arg\_names, op\_return\_value, collections, capture\_by\_value, override\_flat\_arg\_shapes, acd\_record\_initial\_resource\_uses)    1005         \_, original\_func = tf\_decorator.unwrap(python\_func)    1006 -> 1007 func\_outputs = python\_func(\*func\_args, \*\*func\_kwargs)    1008    1009 # invariant: \`func\_outputs\` contains only Tensors, CompositeTensors, \~\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\eager\\def\_function.py in wrapped\_fn(\*args, \*\*kwds)     666 # the function a weak reference to itself to avoid a reference cycle.     667 with OptionalXlaContext(compile\_with\_xla): --> 668 out = weak\_wrapped\_fn().\_\_wrapped\_\_(\*args, \*\*kwds)     669 return out     670 \~\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\func\_graph.py in wrapper(\*args, \*\*kwargs)     992 except Exception as e: # pylint:disable=broad-except     993 if hasattr(e, ""ag\_error\_metadata""): --> 994 raise e.ag\_error\_metadata.to\_exception(e)     995 else:     996 raise TypeError: in user code:      C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:862 train\_function  \*         return step\_function(self, iterator)     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:852 step\_function  \*\*         outputs = model.distribute\_strategy.run(run\_step, args=(data,))     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute\_lib.py:1286 run         return self.\_extended.call\_for\_each\_replica(fn, args=args, kwargs=kwargs)     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute\_lib.py:2849 call\_for\_each\_replica         return self.\_call\_for\_each\_replica(fn, args, kwargs)     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute\_lib.py:3632 \_call\_for\_each\_replica         return fn(\*args, \*\*kwargs)     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 run\_step  \*\*         outputs = model.train\_step(data)     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:803 train\_step         loss = self.compiled\_loss(     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile\_utils.py:242 \_\_call\_\_         self.\_loss\_metric.update\_state(     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\metrics\_utils.py:88 decorated         update\_op = update\_state\_fn(\*args, \*\*kwargs)     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py:171 update\_state\_fn         return ag\_update\_state(\*args, \*\*kwargs)     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py:403 update\_state  \*\*         sample\_weight = weights\_broadcast\_ops.broadcast\_weights(     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\ops\\weights\_broadcast\_ops.py:157 broadcast\_weights         values = ops.convert\_to\_tensor(values, name=""values"")     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163 wrapped         return func(\*args, \*\*kwargs)     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1566 convert\_to\_tensor         ret = conversion\_func(value, dtype=dtype, name=name, as\_ref=as\_ref)     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant\_op.py:346 \_constant\_tensor\_conversion\_function         return constant(v, dtype=dtype, name=name)     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant\_op.py:271 constant         return \_constant\_impl(value, dtype, shape, name, verify\_shape=False,     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant\_op.py:288 \_constant\_impl         tensor\_util.make\_tensor\_proto(     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\tensor\_util.py:435 make\_tensor\_proto         values = np.asarray(values)     C:\\Users\\Arjun\\anaconda3\\envs\\tf-cpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\keras\_tensor.py:254 \_\_array\_\_         raise TypeError(      TypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. ​

&#x200B;

Help?",LearnMachineLearning
rgzza1,1639577513.0,The colonel trick,"&#x200B;

https://preview.redd.it/j1jbnni2sp581.png?width=2116&format=png&auto=webp&s=209881ef48101fb4c66da81be3b071883559791e",LearnMachineLearning
rgyhjb,1639572945.0,Filtering time series,"Hi!

I am a beginner and doing my first ML project (with Python and Tensorflow), in which I have a lot of time series. I am wondering if and how I am supposed to filter them.

Case no.1: Sometimes, there is noise in data (for ex, speed is oscillating quickly between 5 ans 20 m/s for no clear reason except sensor sensitivity or wind gusts, and I honestly can’t say what is supposed to be the right answer).

Case no.2: Other times, the time series is pretty stable but has some impulse (NaN or 0 value).

I’m a bit rusted with signal processing, but I tried to add a lowpass filter, and it’s ok when oscillations are around a clear value, but it’s not enough in case no.1. And my lowpass filter is not great with impulse.
I know I can use another filter for this impulse problem, if only I take time to implement it - so that’ s ok. But I also wonder: should I really filter data ? I could add a column np.diff(myVariable) for example and the model would see the impulse is wrong. When it comes to the wind sensor, the mean/std deviation/skewness/etc values could be great to know the error or sensitivity.

What do you think about that ? Is this believing in magic, or some kind of overfitting ? Should I really filter the time series before I feed the ML model ?

Thanks for helping! :)",LearnMachineLearning
rgxzhq,1639571240.0,How do I generate segmentation masks from pixel level annotated images?,I got csv file with pixel coordinates how do I convert those values to a segmentation mask?,LearnMachineLearning
rgwroe,1639566845.0,"Trying to compute kNN on a np array of weights (L2 Distances) and a np array of labels, with k=x","What would be the best way to approach this problem? I am having issues with the actual writing part, as I understand the theory of computing kNN with the vector of the weights and the array of the labels, and how the vector relates to the distance of the array of labels.

Alternatively, what resources could I review to better understand this problem? This is for a class I am taking, and I want to be sure I fully understand the concept of the problem before proceeding.

Thank you for your time. I would be happy to give any more information for clarification.",LearnMachineLearning
rgw4fr,1639564214.0,Class Embeddings in ViT,"I have been trying to implement the Vision Transformer in PyTorch and there are some confusions regarding the learnable class embeddings.

&#x200B;

1. How exactly does this embedding learn the corresponding label for the image?
2. Why do we only pass the class embedding related outputs from the transformer encoder to the MLP?
3. Would it not be better to get a linear projection from the transformer encoder output and input it to the MLP and ditch the class embedding entirely?
4. Finally, how exactly can we implement it? I have seen some blog posts using nn.Parameter class from PyTorch, as I understand it defines a random vector for which the the weights are learnt as the training proceeds. Is this the correct way?

&#x200B;

Thanks Alot!",LearnMachineLearning
rgvkki,1639561834.0,Interpretation of Singular Value Decomposition (SVD),"I was reading up on the singular value decomposition and I'm trying to figure out what it signifies.

The math makes sense to me, but I'm finding it hard to interpret the decomposed matrices. For example we have a 2x2 matrix A of rank 2, the SVD gives us A expressed as a sum of 2 rank 1 matrices (u1 * v1.T * sig_1 + u2 * v2.T * sig_2)

1) What do these matrices signify?
2) If we view A as a linear transform, is there something special about the linear transforms of the two matrices we get after decomposition?
3) I'm assuming in a technique like PCA we ""leave out"" some of these matrices. What does that signify?",LearnMachineLearning
rgudcg,1639556687.0,Best source to learn Pytorch ?,"I improved my TF from hands on ML with tensorflow, keras and scikit learn, is there anything which is equivalent to this book but for pytorch ? Or equivalent and no need to be a book ?",LearnMachineLearning
rgu4u8,1639555706.0,Running Collaborative ML Experiments with DVC - Tutorial,"Sharing ML experiments to compare your models is important when you're working with a team of engineers. You might need to get another opinion on an experiments results or to share a modified dataset or even share the exact reproduction of a specific experiment.

The following tutorial explains how you can bundle your data and code changes for each ML experiment and push those to a remote for somebody else using a Google Drive folder to check out using DVC (Data Version Control) tool: [Running Collaborative Experiments](https://dvc.org/blog/collaborative-experiments)

The tutorial shows how setting up DVC remotes in addition to your Git remotes lets you share all of the data, code, and hyperparameters associated with each experiment so anyone can pick up where you left off in the training process.",LearnMachineLearning
rgsvpq,1639550778.0,"In Few-shot Learning, How do I determine the number of query images for each class?","Hello, I am studying Few shot Learning. In general, I know that 7 to 15 Query images are allocated for each class.

However, The dataset I use for training has a minimum of 2 and a maximum of 200 pieces per class.

So I have only used one query image.

I have two questions.

**1) Is it okay to use only one query image like this?**

**2) And is it meaningful to see good performance when using fewer queries?**",LearnMachineLearning
rgo7s6,1639535631.0,Recommendation for affordable ML courses with certificate,"Hello guys, hope you all are doing great. I would like recommendations for ML beginner courses that are affordable and give certificates. By affordable I mean less than US$100.
I was really looking forward to the Andrew NG stanford ML course on Coursera but I can not, for the life of me, use that platform. It won't accept any of my credit cards and there is no costumer support whatsoever.

Thanks very much for the help!",LearnMachineLearning
rgmdwn,1639530207.0,Questioning whether or not I should use a bag of words in my portfolio project,"I am at odds with whether or not I should use text as a feature in my classification project. The problem isn't inherently an NLP one like sentiment analysis, but more like a traditional classification problem that uses census and urban data (neighbourhood profiles) to make predictions on neighbourhoods.

That being said, I have a text column in my dataframe that I can't help but think would make for a good feature if converted into a bag of words. I'm not versed in NLP, but a bag of words with PCA makes sense to me. I am worried that this might complicate things though, and send me down some unnecessary rabbit holes trying to learn about NLP when I might be able to make due without it.",LearnMachineLearning
rgksre,1639525630.0,How to choose an algorithm?,"This was something that I was thinking. When should we choose a neural network, xgboost or random forest algorithm for an ml problem ?
Sorry for the noob question, I am trying to pick up machine learning and this was a question I had.",LearnMachineLearning
rgiu1t,1639520243.0,Industry Certifications,"I was curious, are there any industry certifications around ML/AI? In other fields you have like CISSP, CISA, CCNA, CKAD, etc. for networking, security, and everything else. I was curious if such a thing exists in the ML/AI space. Thank you!",LearnMachineLearning
rghuvg,1639517473.0,4 Up-to-Date Techniques for Image Data Augmentation,"I wrote a brief introduction on a few selected data augmentation techniques published in the top conferences of recent years. I find them mysterious yet working particularly well. Feel free to have a look!

[https://taying-cheng.medium.com/4-up-to-date-techniques-for-image-data-augmentation-5bdf34ace063](https://taying-cheng.medium.com/4-up-to-date-techniques-for-image-data-augmentation-5bdf34ace063)",LearnMachineLearning
rggzay,1639515052.0,French beginner needs help for a very important project,"Hello,

I am a french student and I have a citizen project to do. I am trying to set up a political program comparator for the French presidential elections in 2022. To fight against the abstention which affects mainly the poorest in France. I am a beginner in ML, but I managed to collect through a survey about 10000 answers on the 10 favorite themes of the French. For the first step I just want to be able to classify the user between two categories left or right according to his answers to the questions. And in a second step ask more specific questions to match him with the candidate who answers the most to his expectations.

My problem is how to exploit the data obtained and what algorithm would allow me to best classify the new users into two categories.

I thank you for your help",LearnMachineLearning
rgfo6n,1639511485.0,"How to build a grid (map x,y) from addresses?","Hello guys,

Anyone knows if there a way to build a grid (array 2-D) from addresses?

There may be an API (GoogleMaps-like) to link the respective SQL database addresses as positions on the map, or convert to latitude and longitude.

I'd like to study these points in the field of geostatistcs (Kriging).

&#x200B;

Thank you all!",LearnMachineLearning
rgbl0q,1639500402.0,What is Markov Diffusion Operator?,"Hello


I am reading a paper and the authors mentioned ""markov diffusion operator"". What is that?",LearnMachineLearning
rgadbt,1639497045.0,SVM model interactive Visualisation in Jupyter Notebook,"[code](https://www.engineerknow.com/2021/12/understanding-svm-its-type-applications.html)
In this blog you will learn what is SVM and how to visualise SVM interactively in Jupyter Notebook

https://reddit.com/link/rgadbt/video/s7utzj4q4j581/player",LearnMachineLearning
rg86kv,1639490640.0,"Can you recommend a website, book, article or a thesis that used Deep Learning to forecast climate in next ten years using historical data (50 years ago for example).","I am new with the term of Machine learning and Deep Learning, so beside the source, is it possible to develop a model to  predict the future climate using the normal laptop (ram= 4 GB). How hard can be for a bigger to use deep learning for forecast future climate,

Thank you.",LearnMachineLearning
rg6vp7,1639486493.0,Best what to plot a user flow?,"Ok, let me be more specific. I'm working on an analysis of Chatbot data. The bot's dialogs/contents are represented by IDs. My company wants to see which ""routes"" are more accessed by users. I already have that information, but the lead Data Scientist said it would be nice to plot this in a flowchart kind of way. He told me to look into Sankey Diagram, but from what I read I don't think it's the best way to do this, because it requires information that we don't have.

Here's an example of the ID routes:

&#x200B;

|Most acessed IDs|
|:-|
|\[7, 43, 342, 78, 92, 33\]|
|\[7, 123, 56, 73, 23, 22\]|
|\[7, 89, 76, 125, 48, 77\]|
|\[7, 89, 76, 125, 48, 77\]|

As you can see, they are stored as lists. The idea would be to plot this in a way it showed the flow of user experience. So, to sum everything up:

1. Is there a way to do this using a Sankey Diagram?
2. Is there a better way to do this?

EDIT: Btw, I'm using Python.",LearnMachineLearning
rg61dp,1639483353.0,Best Machine Learning Resources for beginners.,"I am planning on studying machine learning and I want some of the best resources(courses, books, etc) to get started.  Andrew Ng's ML course on Coursera is an exception.",LearnMachineLearning
rg44wl,1639475428.0,International Data Analysis Olympiad (IDAO 2022),"&#x200B;

https://preview.redd.it/hjt0aw3mch581.jpg?width=5760&format=pjpg&auto=webp&s=cb61db5d0b049adfac5c5cea11e412a2e0c70b40

 We invite ML students and specialists from all over the world to take part in the **International Data Analysis Olympiad**. HSE University and Yandex are organizing it for the 5th time, and the Otkritie bank will be our platinum partner this year.

Since it’s our first anniversary, we decided to change the format: students and ML specialists are divided into two separate divisions. Only students are able to join the main competition — the Student Division. All others can join the Open Division to participate hors concours (for their own interest).

Traditionally, the first stage’s task will be given by the Laboratory of Methods for Big Data Analysis (LAMBDA, HSE University). It will be about predicting the properties of two-dimensional crystals of various configurations. The task for the Finals will be provided by the Otkritie bank. The Olympiad includes two stages:

**Online Stage (1-28 February 2022):**

• Track 1: Traditional machine learning competition on Yandex.Contest platform. You will need to make new predictions and upload them to the automatic verification system.

• Track 2: Come up with a solution for the same problem, keeping within a rigid framework of time and memory used.

**Final (16-17 April 2022, Moscow):**

• top 30 teams according to the Online Stage results will be invited to the Online final.

• In the final 36 hours of the competition, participants will try not just to train the model, but to create a full-fledged prototype, which will be tested both in terms of accuracy and performance.

**Registration is open till February 13:** [https://idao.world/](https://idao.world/)",LearnMachineLearning
rg2ba8,1639467664.0,ARIMA for prediction of hourly natural gas consumption,"Hello all

I'm predicting natural gas consumption and I'm trying some different methods out. I have hourly observations. The data has two seasonalities - yearly and daily.

I have tried TBATS for Python, but it takes ages to train. I think this is because I have so many observations in my training set (24\*365\*2=17520), but I need that many to capture the yearly seasonality? This is my estimator:

estimator = TBATS(seasonal\_periods=(24,(365.25\*24)))

&#x200B;

I also tried auto\_arima for Python (and used to Fourier terms for modelling the two seasonalities), but I keep running out of memory. I'm on a decent laptop.

I would try seasonal\_decompose twice and then just a simple ARIMA, but I'm not sure how to add the seasonality back again.

 Does anyone have any good advice?

Also, how would I even evaluate my results? I realize I can predict the consumption and then compare with actuals, but how is that done on a larger scale?

Thanks for any ideas you are able to offer.",LearnMachineLearning
rftozk,1639439074.0,Advice on how to get started,"Hi everyone! This is my first post here. I am asking for some advice on how to get started with ml.

I am currently a math undergraduate but I have strong interests in machine learning and statistics. I have done some part of the cs231n courses online but haven’t really implemented any “real” model. So I am thinking about doing my own project just for fun during the break.

I have read some blogs online and decided to go for OCR, which I know is a bit too much for a beginner but I was hoping to learning much from implementing it. Also it’s

So my first question is: Are there easier ways to get started?

Second question: Would it be a good idea to read the papers, look at other people’s implementation, and try it myself?

Any suggestions would be greatly appreciated! Thanks very much!",LearnMachineLearning
rftcum,1639438126.0,Good part time Data Science MS programs with bad GPA,,LearnMachineLearning
rfr6vf,1639432322.0,"Looking for resources to learn about ML techniques/models for forecasting times series data, does anyone have any recommendations they can provide?",,LearnMachineLearning
rflmju,1639418034.0,What is the best off the shelf model to use for question/answering over internal documents with a serverless architecture?,"Hi,

I have many gbs of internal documents that I need to build a search tool over.

Currently I am using Haystack NLP library to do this task, but I would like to know if another library/tools exists where I can make Haystack model into a serverless architecture?

 I would like something that can be on demand vs turned on all the time from a computational resource perspective.

Thank you in advance.",LearnMachineLearning
rfks1b,1639415905.0,Can we use autoencoders to change an existing image instead of create one from scratch?,"I'm trying to think if we can use auto encoders to edit an existing image instead of say, creating a new one from scratch.

To give an example, say I train my data on the MNIST dataset. If I now give my model a 9 and ask it to convert to a 8, would it be able to alter the pixels to create 8?

If yes, can you point me to some resources for doing this? Thanks.",LearnMachineLearning
rfk295,1639414092.0,Batch size in MLP and the difference from RNN?,"Hi all,

I am so confused... By the definition, a batch size is the number of training examples in one forward/backward pass. This sounds to me that the trained model is then using the information in other training examples to predict. What is then the difference between RNN and a MLP with batch\_size >1?",LearnMachineLearning
rfhp4g,1639407845.0,ML roadmap,"Hi there! Im actually learning ML, and was wondering if there is any kind of roadmap of study, or some guide to be focused in what is most important to learn.
Ty for reading",LearnMachineLearning
rfdieq,1639394321.0,Should I learn Linear algebra the traditional way or through Python?,"By traditional way I mean the way it's taught in the Undergraduate studies. I've found this fantastic resource by MIT :
[Intro to Linear algebra - Gilbert Strang](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/syllabus/).

In comparison , the course/book of [Codethematrix.com](https://codingthematrix.com/) teaches only the essentials needed for ML. It seems to take considerably less time to finish it and I'm also learning lot of real applications which means a lot for practical learning.",LearnMachineLearning
rfczeg,1639392208.0,What happens to the output of the two LSTM's in a bidirectional LSTM?,"I'm implementing a BLSTM but I'm confused as to what happens to the output of the 2 LSTM (forward and backward).

Supposed we have 3 words each with a 4-length embedding. i.e: input is of the form 4x3

+ One group of people say that the output of the two LSTM's is concactenated. i.e: the 4x3 goes through each LSTM independently (suppose their output is 4) and a 4x3 matrix is output for each LSTM. the result is conactenated *fully*, meaning the result is a 12-length vector.

+ Another group says that the output of the two LSTMs goes through an activation function independently. Meaning the output of the two LSTM's (two 4x3 matrices) go through an activation function each word at a time. For the first word fore example you have 4+4 inputs to the activation function.

+ Another group say the output is just concactenated on a word-by-word basis. Meaning the final output is an 8x3 matrix, 4 from forward and 4 from backward. (What I'm currently using).

The older papers use a mix of these, with the original paper using something closer to the third option.

Thanks.",LearnMachineLearning
rfb0c0,1639384020.0,Designing CNN from the image below,"I want to design a CNN from scratch using the figure. I'm new to the  concept and getting a little confused about adding the conv2D and  Maxpooling layers. How should I go about it?

https://preview.redd.it/n05yf33ss9581.png?width=738&format=png&auto=webp&s=647cd16111caffcc65d6b1809aefa6f285c7852c",LearnMachineLearning
rf95xz,1639376756.0,NLTK Lemmatizer,Does nltk lemmatizer and pos_tagger require an active internet connection while executing code?,LearnMachineLearning
rf8vfu,1639375676.0,Useful data summary statistics with image classification," Hello!

I am doing image classification with TensorFlow for learning purposes. I am splitting the data into 5 folds. I would like to get useful summary statistics on these validation sets. What could be useful other than the shape of the validation sets?",LearnMachineLearning
rf7mpc,1639371428.0,Should I take Machine Learning my first semester my sophomore year?,"Hi, I'm currently a freshman my first semester and taking a Python class. I'll take a Java class next semester, along with Intro to Stats, Linear Algebra, and Database Management ( SQL). I did AP Cal in high school, hence have a solid base of Cal I and II I think. I want to ask if that would be a good base for me to learn ML my first semester as a sophomore? If not, what should I do to prepare better?

Appreciate any contribution!",LearnMachineLearning
rf7fxo,1639370818.0,Questions regarding CNN LSTM model,It is my understanding that video data must be converted to individual frames and then converted to some sort of array data before passing it through a CNN. What i don’t understand is how to do that last step of converting it to array data could someone please assist?,LearnMachineLearning
rf4vyo,1639362740.0,Can I repurpose YOLO to classify objects like usual but also do a binary classification of those objects?,"So here's the deal, I want to take a pretrained YOLO model and replace the last layer to do a slightly different task.

My classes will be individual people (Gary vs Chris vs Stacy, etc.) but I also want to know if they have their hand in an open or closed fist. My dataset would consist of bounding boxes with (class, x, y, width, height, 0) annotations with an open fist and (class, x, y, width, height, 1) annotations for a closed fist.

I know that the standard YOLO algorithm returns an (S, S, B * 5 + C) sized tensor, where S is the number of grid cells per axis, B is the number of bounding boxes per grid cell, and C is the number of classes. And obviously B * 5 means that every bounding box will have 5 outputs, namely (confidence,x,y,width,height).

So, if my intuition is correct, I need to change the output layer of YOLO to output a (S, S, B*6 + C) tensor which would represent (bbox_confidence, x, y, width, height, hand_confidence) and then retrain it on my labeled dataset. Is this correct? I could also double the class vector and have (Gary_open, Gary_closed, Chris_open, Chris_closed...) but my guess is this is a really bad solution. Gary_open would be a complete separate class from Gary_closed which would be confusing to the model, and if I wanted to add more hand poses for example, then I'm tripling, quadrupling, or worse to the class size.

Also, the output layer has just a linear activation function, would that not be optimal for a binary classification problem?",LearnMachineLearning
rf2xv1,1639356852.0,Watered down resources for SVM (Large Margin Classification),"I have taken Linear Algebra and Calculus but feeling a bit lazy so am looking for something easier than Andrew Ng's Coursera ML course to learn SVM.  Does anyone know of any resources?

For example, this book: [https://www.manning.com/books/grokking-deep-learning](https://www.manning.com/books/grokking-deep-learning) really waters down linear and logistic regression.",LearnMachineLearning
rf1kph,1639352670.0,Need help making a transformer model and alterations to make to it.,"Using TensorFlow on googlecolab I have been assigned to come up with some ""creative"" solutions for a neural network.

First  how should I get started to make a transformer model? Most of the code  is made for me I just have to build the model itself compile it and fit  it.

Next is more or less just  trying out a bunch of different experiments or approaches and seeing how  they affect the data. What are some cool or interesting things I could  add to my neural network to see how they would affect the outcome?

""  I'd recommend exploring ideas and analysis options by conducting a   series of 'experiments' or data analyses. In each 'experiment', you make  a *change* to *how* you are analyzing the data. You might use a different optimizer, or add another layer to your network. *Which* changes you make should be guided by *questions* or *problems* you encounter. For example, your model's accuracy might be low, and you might try to *improve*  accuracy by adding a new layer to your network. For another example, if  you observe overfitting in your model, you might try adding a   regularizer to reduce overfitting. You then observe the *effect*  of each change you make on the data analysis, so you can evaluate what  the change does. For example, does adding another layer to the network  improve accuracy? Does it also increase overfitting? """,LearnMachineLearning
rf08z6,1639348770.0,"ML Beginner looking for the best course/direction to find/catalogue the few audio matches/similarities in a large database. My searches have been getting railroaded by NLP, which seems unnecessary for my purposes.","I have a large database of audio files 8 to 30 seconds, and I'm looking to pull the ones that match certain words or phrases.  I don't need the ML to understand the words or phrases (I think), I just need it to match, within an acceptable level of error the sounds.

Currently, it's pure human listening going through a per audio basis.  The per audio time down to about 3-5 seconds to determine if it's a match, but over the course of thousands of audios, not only is it inefficient, it's also mentally exhausting.

I'm running into so much NLP that I'm having a hard time finding audio processing, and a harder time finding a niche in there.  I know little about ML, but after getting an extremely basic understanding, I figured I'd ask.  Any help is much appreciated.  I am hoping to do this in Python, as it is the most known language, but am not unwilling to go a different route if it fits my use case.",LearnMachineLearning
reys09,1639344555.0,My validation accuracy for siamese networks is not improving and is stuck at 0.5,"I have posted this in another subreddit because I forgot this also exists.

I was implementing the network to learn about yoga poses. I have so far crested a dataset of 6000 images. The positive and negative pairs are all arranged alternatively. The positive and negative pairs are divided equally in both the training and validation datasets. The training accuracy is improving to 0.90+ in just a few epochs but the val accuracy is stuck at around 0.5. Can anyone tell me what to do? I am doing this for a week but cannot get over this.

Please answer my question. I have been stuck for a week now.",LearnMachineLearning
rey855,1639342952.0,How to make a dataset?,"I've been progressing along on my journey and I've been wandering for a while about how do people come up with their datasets. I know for image recognition it could be a simple as collecting images of a certain class that you want, but let's say something like style-GAN, how do people go about creating the dataset and how do people know that their dataset will work?",LearnMachineLearning
rewfbp,1639337773.0,Hardware assistance,"Hello!
So far, all training that I've done was on CPU since I have Radeon RX 590, and I want an upgrade. Not an expensive one, so I want to match the new GPU properly with my current cpu/motherboard.
I have Prime A320M-K with Ryzen 7 2700X, and was planning on getting Zotac GTX 1070 8GB X-Gaming.
Is that fisable?
Also, does it really require only one 8 pin slot?
And can I still use my old gpu as primary and dedicate Zotac just for training?


Thanks in advance!",LearnMachineLearning
revtrf,1639336075.0,Which is best YouTube channel to learn Machine learning?,I just learned python basics and now want to learn ML please recommend best YT Channel and course if any ?,LearnMachineLearning
rev9yn,1639334546.0,Tutorials/Beginner friendly resources.,"Hi all,

In short - I’m a beginner to the whole AI/ML world and I’ve been doing some beginner tutorials for the past week or so.

I find I excel with project based learning and I’m wondering if there are any go to tutorials for building a text classification/sentiment model that anyone would recommend?

Not too concerned about libraries or third party.

Im a programmer of over 15 years btw and so when I say beginner beginner - I mean specially to ML. Can read and somewhat write with Python but no build experience in it.",LearnMachineLearning
retq7g,1639330171.0,Live NSFW detection,"Is it possible to be playing a video and to have this program running in the background and it will be processing your screen every time it changes and detect every time something NSFW shows up? Is it possible for it to be that quick?

If it's possible, how would I approach this task?",LearnMachineLearning
ret3mw,1639328370.0,[A] Introduction to artificial intelligence,"I like to share a short article about artificial intelligence, this article is supposed to be a short introduction on the subject, and any comments are welcome:

[http://thoughtsonprogramming.com/an-introduction-to-artificial-intelligence/](http://thoughtsonprogramming.com/an-introduction-to-artificial-intelligence/)",LearnMachineLearning
rer126,1639322217.0,Improving convergence of DQN network,"I've been trying to train a DQN on the [CarRacing-v0](https://gym.openai.com/envs/CarRacing-v0/) environment of OpenAI gym, but I'm having trouble getting it to retain the knowledge its learned over time.

I'm using prioritized experience replay, and the model architecture is a relatively simple CNN.

Here is a link to my code: https://github.com/JinayJain/deep-racing

I let it train overnight and here's my total reward plot per episode: https://i.imgur.com/AJ0sx0B.png

A few ideas I had, but looking for more guidance:

* Increase model complexity (maybe it's underfitting?)
* Decrease learning rate over time

What could be causing this fluctuating reward?",LearnMachineLearning
replf5,1639317693.0,How can I set the weight of basic and simple 2 layer neural network?,"This is a very basic question for this sub, but I can’t find the answer by myself.

I remember that when I studied neural network, more than 15 years ago, there was a very basic math formula that made me clearly understand the basic of neural networks.

I believe it’s a simple exercise useful to teach the very basic of how a node works.

I remember I was able to get a pencil and a piece of paper and give an example of a simple working 2 layers neural network. I helped many people to understand the very basic of what, at that time, was an alien concept. (The learning propose of this exercise is why I ask in this sub)

I was able to create a 2 layers NN with 4-5 nodes for each layer. The network was able to recognise two different patterns (example: 1-1-1-0–0-0 VS 0-1-0-1-0-1), and the second layer was able to recreate that pattern even if the input of the first layer had some mistake (example: input: 1-1-1-1-0-0; output: 1-1-1-0–0-0).

I forgot everything. And I need your help.

**How did I calculate the weight between the layers?** I remember it was a very basic math formula for each of the weight.",LearnMachineLearning
reosda,1639314805.0,Can I use any python library instead of MATLAB/OCTAVE?,"My course this semester is numerical computing, and the professor told us that he would prefer using MATLAB....can I use any python library instead like Numpy or something?",LearnMachineLearning
reohjf,1639313735.0,Backpropagation for beginners. Warning -> Not for faint-hearted,"Beginners complain that they can't find posts/videos for Backpropagation and beginners sometimes have no idea how Backpropagation works. In most tutorials, when it comes to Backpropagation, only equations are shown to beginners and tutorials directly jump to Keras, PyTorch, etc. And most of the time these tutorials are from the documentation of Keras or PyTorch.

In these posts, I have tried to explain Backpropagation in very simple language with very neat codes.

[Building your own Neural Network](https://medium.com/@neuralthreads/building-your-own-neural-network-understand-the-built-of-a-neural-network-27ea76b6b614)

[Backpropagation, Part 1](https://medium.com/@neuralthreads/backpropagation-made-super-easy-for-you-part-1-6fb4aa5a0aaf)

[Backpropagation, Part 2](https://medium.com/@neuralthreads/backpropagation-made-super-easy-for-you-part-2-7b2a06f25f3c)

If these posts are helpful to you and you think they will be helpful to others please share them.  You can join me on youtube are [neuralthreads](https://www.youtube.com/channel/UCorUqR9utU1SBdDeMLpGzKA/videos)and on [reddit](https://www.reddit.com/r/neuralthreads/).",LearnMachineLearning
reo2yy,1639312176.0,need help with a CNN + LSTM model,hi guys i’m trying to train a cnn+lstm model on some videos. how do i convert the video into data that the cnn can read? previously i’ve used models offered by keras like the VGG19 but i was wondering if there was a way to convert it without the need for the keras model,LearnMachineLearning
remf8e,1639305197.0,OCR for Handwritten Devanagari script,"Hi everyone

I have to do a project for handwritten word recognition in Devanagari script using Deep Learning but I can't find good resources or even datasets. Can somebody please help me out here.

Edit : I am trying to implement a cnn-rnn hybrid model which does not require segmentation of characters from a word and takes whole word as model input.",LearnMachineLearning
relg9r,1639300930.0,Bridging the gap between theory and trying it out in Kaggle,"So I just finished Andrew Ng's course on Coursera. In August, I also finished a course in my university where I could try out scikit-learn and tensorflow with Keras filters etc. But when I came to Kaggle, I am at a complete loss of where to start or what to do. Is there some kind of a bridge that will help me connect the two?
Tbh historically I have been lost at implementing the theory when it comes to practice.",LearnMachineLearning
rekxvo,1639298847.0,Need help or suggestions in data preprocessing,"Hi all, currently I've started thesis in masters in computer science. I have to work upon data preprocessing of a dataset. The dataset contains accelerometer g values as x, y and z. There are approx 300,000 rows with x, y and z values with a sampling rate of 1600 samples per second.(data collected for 3 mins). Is there any approach so that I can reduce the number of samples using machine learning in python? Thanks.",LearnMachineLearning
rekhev,1639296830.0,How to detect and tag persons on photos?,"I'm a software engineer, no AI/ML experience. Think it's something interesting to play with. I have thousands photos taken for the family on OneDrive. I'd like to download all of them and store on a local server, meanwhile, hand pick a few to train a model and categorize multiple persons eith the output, then assign names, say myself, wife, son, daughter, use these as input, continue run through all photos, at the end I'll have a database table: photo1 is myself and son, photo2 is wife and daughter, etc., of course I can also edit the EXIF info on the photo and attach these tag information, later I can search for photos that're tagged with my son.

A different way would be to run all photos altegother, output would be person1 on photo1, person 2/3 on photo2, etc., then I can assign names from the final output, which could be a simple mapping in database, person1 -> myself.

Not sure I'm making any sense, how do I get started?",LearnMachineLearning
regt37,1639282642.0,Using AMD Radeon with TF in Anaconda Spyder," Hello,

I understand that Tensorflow is geared towards proprietary NVIDIA Cuda, but is there a workaround for AMD Radeon GPU? I'm on a Macbook Pro with an AMD Radeon 580 external GPU card.",LearnMachineLearning
rebagw,1639264323.0,What is the process of creating a machine learning model? (Using U-net) Have I understood it correctly?,"For my university course, I am learning about U-net and how that can be used for image segmentation e.g in cancer biology. However, can someone please tell me if I have misunderstood things wrong about the workflow of things.

I've tried to find research papers online that give me an overview, but I can't find a dang thing, apart from articles that just outline the basics.

1. Step would be to clean and sort through the data e.g making sure you there are no duplicates in the image dataset.
2. Divide the dataset into the test set, validation set and training set.
3. The training set is used so that the model can learn what to do.
4. The validation set is used to look at how accurate the model is, and this is where we can change the hyper parameters to make it more accurate.  **What type of hyper parameters can be used though when looking at image segmentation?**
5. Then we use the test set for the final 'test', and evaluate how well it performs. **Can we evaluate how well it performs using things like the Dice coefficient?**

After we have used the test set can we go back and refine the model? Or is that supposed to be during the training phase?

Thanks in advance.",LearnMachineLearning
re99pq,1639258056.0,Huggingface is a great idea poorly executed.,"For a project I'm trying to use huggingface transformers library to build a particular classifier with Keras. But Jeez, I'm having nightmares every time I try to understand how to use their API. They have tutorials but I find them extremely hard to understand and the API is incredibly ambiguous.

I'm just an idiot or this sentiment is shared?",LearnMachineLearning
re71yb,1639251400.0,How to choose hyperparameters of Gaussian Process,"Hello there, I am working on a project, where I have to model some plots  using Gaussian Regressions. I would like to use Grid Search to make an  exhaustive search of as many hyperparameters and kernels as possible..   That being said, I don't really have any experience with GP, so I  don't  know which hyperparameters and which combinations of kernels I  should be  testing out.... I tried to look for some examples but either  the  examples are too specific to be transfered to my case or I found  very  complicated math explanations that didn't really help me  concretely. Any  help would be very welcome. Thanks!",LearnMachineLearning
re6jy9,1639249931.0,Can I use MATLAB instead of Octave for an ML course?,I wonder if they are literally the same performance-wise and in terms of syntax?,LearnMachineLearning
re2787,1639236981.0,Personal ML models,"Hey! I'm looking for the best practices in training targeted personal models. Let's say I have 1000 customers using my product. I plan on training 1000 individual models to make the best possible recommendation for each customer individually. How should I go about training them to get the best possible outcome?

Should I first train with the data of 999 customers and then retrain on the target customer with a lower learning rate? Or freeze all but the last couple of layers? Or any other tricks?

I'm working on a time-series model  for predicting where the user wants to go next on a web page. I have a  medium amount of data (\~50 000 events for each customer) on the order of  page visits and the length of time since the previous action. A quick  sample below:

1. **CustomerID; PageName; Seconds**
2. 123; Dashboard; 0
3. 123; DetailsPage1; 5.9
4. 123; DetailsPage2; 3.1
5. 123; Action5; 21.2

Any learning material, best practices or just keywords to help in my search are highly appreciated. Thanks yall!",LearnMachineLearning
re1s36,1639235646.0,[P] Tutorial on using image super-resolution without Photoshop,"Video:

[https://youtu.be/HN2-Q2lsKlo](https://youtu.be/HN2-Q2lsKlo)

Used GitHub project:

[https://github.com/idealo/image-super-resolution](https://github.com/idealo/image-super-resolution)",LearnMachineLearning
re1hgk,1639234780.0,"Forsyth, D. (2019). Applied Machine Learning","Hi, everyone. I am searching for the pdf of this book. If someone can help me i would be very greatful.",LearnMachineLearning
re1f4s,1639234577.0,PPML Series #1 - An introduction to Federated Learning,"I started a series on privacy-preserving Machine Learning. I wanted to do it for quite a long time and finally decided to start. The first post is a short introduction to Federated Learning. In this blog post, I have written a more detailed version of my [Twitter thread](https://twitter.com/shreyansh_26/status/1462262151209381888).

Check it out -  [PPML Series #1 - An introduction to Federated Learning](https://shreyansh26.github.io/post/2021-12-11_intro_to_federated_learning/)",LearnMachineLearning
re0oxk,1639232283.0,How much linear algebra is required to be able to follow core ML theory?,"Hi. So I already know the basics of linear algebra from high school. I was wondering if that would be enough for me to able follow one of the ml lecture series, like cs229 by andrew ng or cs156 by caltech. Or I should go through linear algebra lectures by professor strang first. Because that would be really time consuming if I follow that course completely along with assignments.",LearnMachineLearning
rdyyvl,1639226369.0,What algorithms could potentially help improve a logistic regression model whilst maintaining the original loss function of the original model?,"I am looking into bagging and I realize the bagging is particularly effective for unstable, high variance machine learning models, but not so much for more stable or have a high variance.

What algorithms would therefore be good for improving a logistic regression model while mainting the original loss function?",LearnMachineLearning
rdy6gz,1639223292.0,What is the most fun way to learn calculus?,"I'm a programmer, and I've come to realize that I learn things better when I put them into practice as I learn. So you can say that I am a person who learns by doing. Is there any way to learn calculus (and math in general) using this learning approach we use in programming? I want to go deep in math because I'm interested in machile learning (and because I need to learn these things to graduate from CS college lol)",LearnMachineLearning
rdwanv,1639215246.0,What is the difference between xgboost and gradient boosting?,"Xgboost is more enhanced version of gradient boosting and give more better performance.

Can anyone explain how they are different mathematically.",LearnMachineLearning
rdw0zo,1639214112.0,Deep Forest and Diversity - Short tutorial,"Hi Folks,

Just released a quick tutorial for newcomers / junior MLEng about the new framework from 2017 about Deep Forest ✅


[https://simon1-provost.medium.com/how-to-use-its-own-base-learner-to-improve-diversity-deep-forest-model-a-novel-deep-learning-9200165e2a2e](https://simon1-provost.medium.com/how-to-use-its-own-base-learner-to-improve-diversity-deep-forest-model-a-novel-deep-learning-9200165e2a2e)


Feel free to share your thoughts 🔬
Cheers",LearnMachineLearning
rdtf5w,1639203531.0,How do I go about studying GANs?,"I'm looking for any prerequisites there are to understanding GANs. I'm familiar with a bunch of ML algos at the level they were discussed in Intro to Stats learning. I'm comfortable with probability and linear algebra at a college level.

Please suggest how I should go about this-
1) What papers do I need to start off with?
2) How familiar do I need to be with other deep learning network architectures (CNN/RNNs)?
3) What other DL concepts do I need to familiarise myself with?
3) Is the Deep learning book by Goodfellow a good resource to learn GANs?",LearnMachineLearning
rdl6ib,1639177269.0,Uncertainty estimation in the input space,"Hi,

assuming my input is an array between 0 and 1000 and the output the  corresponding system velocity. And the training data is generated by  randomly applying one of the input values between 0 and 1000. The  corresponding system velocity is then saved as the output.

Is  there a way to estimate the uncertainty in my input data such as when  the input area between 0 and 100 has not been applied to the system that  I can measure this ""uncertainty""?",LearnMachineLearning
rdj5t7,1639171761.0,Trying to implement neural networks from scratch.,I have gotten a class of neuralnet with an init parameters but not sure how to continue. Please dm or comment if you can help me please. Thank you,LearnMachineLearning
rdhncs,1639167719.0,How to deal with categorical features and which feature selection should i use?,"I am starting on a project, the target is categorical, only 1 and 0

Half of my features are categorical, all categoricals are nominals, there is no ordinals, but some of nominal columns are given as integers such as 1,2,3,4

**I will be using classifications or trees to predict the target**

can i use ordinal encoder? because i have heard it works for tree models, however, i dont think those numbers would make much sense for feature selection.

or should i stick with one hot encoder? but the problem is, i originally have only 17 features, i would end up with 70 features after one hot encoding, then how do i select the features after the some features are broken into pieces? especially the month feature, it would break into 12 different columns.

which type of feature selection should i use? can i also use logistic regression to rank them?

also, for numerical features, should i scale first or do feature selection first?

**i am using python by the way**",LearnMachineLearning
rdg7fs,1639163865.0,I need some help with correct data type for model.predict(). Sample code in post detail,"Hey everyone, I'm reading a book about techniques for Natural Language  Processing, it has a github page for a simple implement of Bidirectional  LSTM for sentiment analysis  [https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-TensorFlow-2/blob/master/chapter2-nlu-sentiment-analysis-bilstm/IMDB%20Sentiment%20analysis.ipynb](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-TensorFlow-2/blob/master/chapter2-nlu-sentiment-analysis-bilstm/IMDB%20Sentiment%20analysis.ipynb)


However since I'm still new I can't for the life of me figure out what's the correct data format to use `model.predict()`, they only reach `model.evaluate()` in the sample code. I tried using the same training data as prediction and I always get this error

&#x200B;

https://preview.redd.it/gsu00sx4mr481.png?width=816&format=png&auto=webp&s=55fa5057f5fdda92adb12f1b7cbe00d77e9e1e84

I know it has something to do with the shape of data for `model.predict()`, but I just don't know what. Can anyone look through it and help me a bit with what data format I should use ?",LearnMachineLearning
rdfmll,1639162339.0,Best way to learn AI during break?,"Background: 19M college student majoring in CS. Learned basic web dev, data manipulation with java/python, software engineering with java, some blockchain smart contract development. Love linux and CLI!

Anyways, I just finished the hardest semester I ever had, and probably will have, and now I feel like I need to be learning something. I want to spend this winter break learning about AI and maybe some other technologies (I would love suggestions for other technologies). What is the best place to start? I have had wonderful experiences with Udemy courses and I plan to find a course there, but I don't know what area of AI to study.

I have some foundational knowledge (4-5 hours of research) and I definitely will say GANs are very interesting to me. I would love to get hands on experience with if. Anything that can help me secure a high paying job in the future would also be nice. So I guess my question is moreso what area of AI I should get into, what has the most traction right now, how do I get involved. I want the experience to be able to predict where the technologies are heading.",LearnMachineLearning
rdff22,1639161767.0,"Which model is used for document extraction (CamScanner, Microsoft Lens etc)","I want to start a small project where I'd create a model(s) that would extract document from a picture and rescale it, something like CamScanner or Microsoft Lens apps do.

I've gathered a small dataset just to prototype the concept, but I'm not sure what might be the best approach to label the data.

1. Using bounding boxes - this might work best to locate the document, but it would bring some noise to it since the picture might be under some angle or document could be held in hand etc. so it might require further processing to eliminate background noise.
2. Using mask-r-cnn will probably do a good job to isolate the document, but I guess it would be tricky to reshape/center later on since it's possible to get a irregularly shaped mask (for example if someone is holding it in hand, finger holding the document might get excluded from the mask, so some extrapolation will be needed probably)
3. My idea was to use keypoints like they do in pose estimation models, where the keypoints would be the edges of the document and then they would be connected by a straight line to isolate document and then re-center it.

Has anyone worked on this type of problem, or has and idea how the apps mentioned above are handling this? Probably there are some other approaches that can be used that I'm unaware of?",LearnMachineLearning
rdefe9,1639159122.0,"Any idea of a model to use ? My data is like this and I want to predict the next letter. & is for A and Z. It is like a loop. For example after B it can go only for the next letter C or &. After &, it can go for B or Y.",BCDCBCBCDEDCBCDCBCB&Y&YXYXYXYXWXY&BCB&YXY&YXWXWXYXY&YXWXWVWXWVWXWXY&YXYXWVUVUTUVUTSRQPQPQPONMNOPQRQPQRQRQPONMNOPQRSRQRTUTUVUTUTUTSTUVUTSTUTUTSRSRSRQRSTUTSTRQRQRQPQPOPQRQPNMLKLMLKJKJKJIHGFEDCBCB&BCB&YXY&B&YXYXWVUTSRQRSRQRQRSTUTUWXYXY&Y&BCDCDEDEFEDEFGFDEDEFGHIHGFGHIJIJKJIHGFEDCKJKJKJKJIHIHIKLKJIHIJIHIJKLKJIJIJKIJIJKJIHGFGHGHGFGHGHIJIJKLMLMLKLMLKJIJIHGHIHGHIJIJIHGHGHIHGFEDEDEDCBCB&Y&XY&BCB&BCB&Y&YXWVWVWXYXYXWUTSRSTUTSRQPQPONMLMNOPQRSRQRQPQPQPONOPQRSTSTUVWVUTUTSTUTSTSRQPQPQPONONLMNOPQPQPOPQPQRSTSRQRSTSTSRSRQPQRQPQPSRSTSTUVUVUTSRQRQRQRSRQONMNONOPONMLMNMLKJIJIHGHIHIHIHGFEDEFEFGFGHGHGHGHGFDEFEFEFEDCBCBCB&B&BCDEDEFGHGHGHIHGHGHGIJKJKJIJIJKJKJKLKJIHIHGHIJIHIJIHIKJIHGHIJIJKJIJLMLMNMLKJIJIHIHIJHGHIHIJKJLMLKLMNONMLMLKJKJIJIJKJKJIJIHIJKJIHGFGFGHGFEFGHIJIJIJIJIJIHGHGFGHIHGHIJIJIJIHFEDEDCBCB&BCBCDCBCB&B&BCB&YXWXWXY&YXYXWVWVUTSTSRSTSRQRQPQRQRSTVWVWVUVWXYXY&YXY&B&BC&Y&B&Y&YWY&BDCDCDCEDEFEFGFGHGHGFEFGFEDEDEFHGHIHIJIJKJKLKLKJMLMNMLMNONMNOPOPQRQRQPOPQONMLMNOPOPQRQRSTSTSTSTUTSTSTUTUSRQRQPQRQRSTSTSTUVUVUVWXY&YXYXYXWVUVUTSTSTSTUVWXYXYXYXWVUTUVWXWXWXYXWVUVUVTSRSRQRQRSTSTUVWVWVWYXWVWXY&B&BCBCBCB&BCDEDEDEFGHGHGHGFGHIHIHGIJIJIHGHGHGFHIJKLMNMNOPQRQPQRSRSTSRSTSTUTSRQPONOPONOPONMLMNONMNOPQRQPQRSRSTSTUVWVWVWXY&B&YXYXY&BCDEDEFGHGFEFHIHGHIHGHIHGHGFEDEDEDEDEFEFGHIHGHGHGFGFGFEFGHGHIHGJIHGHGFEDEDCBYXY&Y&YXYXWXY&B&Y&B&B&B&YXYXYXYXWVWVWYXYXY&Y&BCBCBCB&YXYXWVWVWVWVUTUTUVWXWXWXWXYXYXWVWVWXWVUVUSTSTSRQRSRSRSRQRQRSUVUTUTUTUTSRQRQPOPOPQRQRQRQPQPNMNMLKJIHIJIJKLKJKJIJIJKJIJKJKJIHGHGFDCDCDEDEFGFGHIJKLMLKJIHGFDEFGFEDEFGFEFEFGHGHGFGHGHGFEDCDCDCBCB&YX&Y&B&Y&BCBDEDEDEFGHGFGFGFHGHIHGHGHGFGHIJIJIJKLKJIHIJIHIHGFGFGHIHJIHGFGFGFEDEDEFGFGFEDEDCB&B&Y&YXY&BCDEDEDCBCDCDEDCDEFDEFEFEFEFGHIJIJIHGHGHGFEDCBCBCB&B&Y&BCDCBCDEDCB&YXYXWVWVUVWVUTSTSTSTSTSTUVUTUTSRSRSTSTSRQRQRSTUVWVUVWXWXWXWXYXYXYX&YXYXYXY&BCBCDEDCDEDEFGHGHGHGFGHIHGHGHGHGHIHIJKJIJIJIJIHGFGFGFGFEDEDEFGHIJKLKLMLMLMLKLMNMLMNONONOPQPQPONMNOPONOPOPOPQRQRSTSRQRQRQPONONONMNONPQRSRSTUVUVUTUTUTSRQPOPQPQRSRSTSTUTSTUTUTSTSRQPQPOPQRQRSRSTSRQRQRQRQRSTSTUVUTUTSRQRQRQRSTUTSRSTSTUTUVWVUTUTSRQRQRQRSTUTUVUVUVWXY&BCDCBYXYXY&BCDEDEDEDCBCDEDCDCB&YXWVWVUVWVUTSRSRQPQRSRQPONOPQPQRQPQRQRQRQPQRSTSTSRQPONOPOPONOPQRPQRQRSRSTSTSTSRQRSRSRQPONONMNMNONOPONOPQPONMLKLMLMLMLKJIJIJKJIJIJKJKJKLMNONONMNMLKJIJKLMLMNONONONLMLKLMNOPQRSRSTSRQPQRQRQPQRSTUTSTSTUTSTVWVUSRQPQRQRQRQRSTUTUTSRQRSRSTSTSRQPQPQRQRQRQRSTSRQRSRQRSRQRQRQRQPQPONMOPQRSTUTUTSTSRSTSTSRQPQRSRQPOPOPQRQPQRQRQPQRSTUTUTSUVWXY&Y&BCBCB&Y&Y&YXY&YWXWXWVWVWVWVWVWVUTSTSRSTSTUTUVUVWVUVWXYXWXY&YXWXYXY&Y&BCDCDCBCDEFGHGHGHGFGFEFGHIJKJKJIJIHIJHGHGFGHGHGFGHGHJKJKJIJIJIJKLMLMNOPOPQRQRSRQPQPOPQRQRSRQRQPQRQPONOPQRSRQRQRQPOPQRQRQRQRSTSRSTUTUTSTSTSRQPONOPQRQRSTSRQPONONMLKLMNONOPQRSTUTSRQPQRSTSTSTSRQRQRQRSTSTSTUTUTUTSTUVWXYXY&Y&BCDEDEDCBCBCDCB&B&BC&Y&BCBCB&YXWVWVTSRQPRSTSRSTUTUVWVUVWVUVWVWVUTUTSRQRQRSRQPQSRSRQRQRSTUVWXY&YXY&B&Y&YXWVWXYXYXYWVWVUTUTSTUTUTSTSTSRQRQRQSRPQPQPQRQRSTSRSTSTSTSTSTSRQPQRQPONONMNOPQPQRQPQPQRSTSTSTSTSTUTSTSRQSTUTUTUTSTSRSRQRSTUVWVWVUTUTUTUTUTUVWVUTSRQRUVWVPQRQRQPQRSRQPQRSTUVWVWXYXYXWXWVWXYXWVWXYXY&BCDEDEFGFGHGHGFEDEDEFGHIKJIHIHIHIJIHGHIJIJIJKJIHGFEFEDEFEDEDEDEFGHIHGFGHGHIHGFEFGFEFEDEDCBCBCBCB&Y&YXWXY&B&BCBCDEDEDEFEDCBCDEDEFGHIJIJIJIJIHIJIJIJIJKJIHIJKJIHGFGFEDEDEDEDC&YXY&YXYXY&Y&YXYXYXWVUTSTSTUTSTSTSTSRQPQPOPQPONONOPOPQPQRSTUTSRQRQPOPQRONMLMLKLMNMLMNMLMNONMLMLMLMNMLKJKJKLKJIHIJKLKLMLMNONMLJKLMNMLMNOPQRSRSTSTUTSRQRQRQRQRSTUTSRSRQRSTUTUTSTSRSRQRSRQRSTUTUTSTSRQRQRQPQRQPQRQRQRQSTSTUVWXWVWVWXWXYXWVWXYXYXWXWXY&B&XYXYXY&YXWVWXY&BCDEFGHGFEDEDEFEDEFGFGFGHIJIHGHGFGFGHGHGHIHIJIHGHIHIJIJIHGFGFGHGFGHGHIHGHGFEDCBCDEDEDEFGHIHIJIJIJKJIJKL,LearnMachineLearning
rdccjw,1639153566.0,Question for those who finished Aurélien Géron - Hands on ML,"Right now I'm currently on chapter 17 I've done all exercises etc.. but I've problem in 3 chapters, preprocessing data with TF Ch.13 \*TFRecrods\*, Processing Sequences using RNN and CNN  Ch.15, and NLP with RNN's and Attention Ch.16, these 3 chapters specifically I couldn't even handle the programming exercises and that's like killing me \*exaggerating ofc\*, any help/advice/guidance ? whether was videos about this topic or only me who feel these 3 chapters' programming exercises are hard ? \*I don't need the answers I've the github's repo\*",LearnMachineLearning
rdbkhu,1639151435.0,Is there a forum where I can show my data and get recommendations on which algorithm to use and how to create model around the data?,,LearnMachineLearning
rd7tlx,1639140215.0,Can someone explain what the author means to say here?,"Pic:  https://imgur.com/TUUAyNB

In this figure, how is machine learning helping people to learn?",LearnMachineLearning
rd6vlv,1639136653.0,Best way for UNET to generate Masks for different image sizes,"I am dealing with a dataset that has different size images and so I also have to predict masks of different sizes. I would like to have an automatic way of dealing with it that doesn't distort the prediction. Now I downscale all images to the smallest resolution which appears the most in the dataset. The problem is that during backtransformation to bigger resolutions, the mask itself gets distorted, because there are bigger images and also the bigger ones are mostly not squared.

The problem now is that my model uses the keras input layer which doesn't allow dynamic inputs I think? Also the same for the output.

I hope that anyone can help me!",LearnMachineLearning
rd5uti,1639132363.0,Any resources or learning materials for learning machine learning.,,LearnMachineLearning
rd359m,1639120847.0,Best resources you've found?,I've enjoyed deep learning by francois choillet (book). Still going through it. Haven't used Kaggle yet...what's yours?,LearnMachineLearning
rd2ngm,1639118982.0,Complete production example?,"Anyone know of a great, complete ML project example? I don’t mean just a saved model, but a data pipeline, a working app, etc.",LearnMachineLearning
rd2kq6,1639118699.0,Need guidance for crop production forecasting,"So I found that my agro tech company has data for crop production from 90 countries and the forecasting model they currently use is barely 50% accurate.

The data has  with date, variety and quantity. (some other fields which might not be relevant)

My boss has told me that if I could even get a model to around 70% accuracy, that would be good enough.  (They want accuracy only on variety & continent level)

I am new to ML, have worked on simple Kaggle models.

Can someone tell me about what methods should I study which can be helpful ? Are there similar datasets on Kaggle where I can practice & see work of others?

thanks",LearnMachineLearning
rd20n0,1639116663.0,Increasing the Accuracy of Textual Data Analysis on a Corpus of 2 Billion Words,"[https://engineering.soroco.com/increasing-the-accuracy-of-textual-data-analysis-on-a-corpus-of-2000000000-words-part-1/](https://engineering.soroco.com/increasing-the-accuracy-of-textual-data-analysis-on-a-corpus-of-2000000000-words-part-1/)

At Soroco, we ingest between 200 million and 2 billion words over the course of model training and analysis for a single team of workers using our Scout product. In this blog post, I talk about some tips and tricks that we might use to increase the accuracy of our models, including appropriate processing of text for the purpose of leveraging standard techniques from machine learning. I then demonstrate this by showing how to represent text in a high-dimensional vector space with applications to a toy regression problem.",LearnMachineLearning
rcz0bd,1639106912.0,A beginner here. I want to build a natural language model to classify comments. How do I start?,"Hello, redditors.
I am a beginner for machine learning and I am currently working as an entry level data analyst. Our team wants to implement a model to classify comment provided by our customers. Our customers consist of people coming from non English background. All of them know English, but sometimes they cannot not spell properly. I am not sure if spelling mistakes would affect the model. I have attached an image of a sample dataset to give you an idea about the data I am working with.

[A bogus data sample](https://preview.redd.it/vsm6681pwm481.png?width=515&format=png&auto=webp&s=74e4acd8a227d5bf85b1ccc0f2ef441a4ad3acf1)

I have like 50,000 - 80,000 comments in my dataset. I don't know how to start. I have done a bit of   traditional machine learning using sci-kit learn but have never stepped on NLP. I searched on the internet, the results were overwhelming. I saw some implementations using BERT and HuggingFace. Can you shed some light on this? Can you suggest some good reading materials or YouTube videos which tackle this? Is using a model like BERT/HuggingFace would be easy? Or building an own RNN  from the scratch will do the job better?
Another question, the comment ""service ws fasr"", do I need to do the spell correction before passing into a model? Is there any libraries which do that?

I will be using Python and I am familiar with R as well. If the model is good, then I would deploy it in the cloud (this will be a later question in this forum)

Thanks in Advance",LearnMachineLearning
rcy2tq,1639104079.0,Question about model stacking,"I'm working on a small personal project and would like to implement model stacking. However, the approach by which I would like to do is proving to be unintuitive. Not sure if that's because it's not possible and/or recommended or I just haven't stumbled across the appropriate method yet.

What my plan is, is to split the records into one dataframe (here would be good point to mention working in Python) that contains the numerical features and a second dataframe that contains the categorical features (and maybe even keep a copy of the original dataframe with both) and, of course, to split out the target variable; all sharing the same index. Then run through a selection of models and feed the predictions to a meta-learner.

I've read a few articles about model stacking implementation via sklearn and the process seems to involve initiating a StackingRegressor (or StackingClassifier, depending on task) and passing the estimators and final estimator, then fitting/predicting/etc. in typical fashion.

My problem lies in the disparate dataframes. It seems that you're only able to run a single X and y through the process. You cannot tell the stacker to run models A and B on the numerical dataset and run models C and D on the categorical dataset. Correct? But perhaps there may be a manual workaround?",LearnMachineLearning
rcuyl8,1639094483.0,Good start for TTS development with custom speech based on recordings?,,LearnMachineLearning
rcszwx,1639088809.0,Best path to learn ML given my experience?,"Hello! I currently work professionally as a data engineer, but would like to learn more about machine learning. I think I'd like to eventually try out / transition to a machine learning engineer, but I'm not too sure where to start. I'm comfortable with Python / SQL / AWS, and remember some basic calculus / linear algebra from college (studied Chemical Engineering). Any tips or guidance on how to start learning more about ML?",LearnMachineLearning
rcs88b,1639086667.0,"[Resource] I'm Releasing Three of my Pokemon Reinforcement Learning AI tools, including a Computer Vision Program that can play Pokemon Sword Autonomously on Nintendo Switch | [Video Proof][Source Code Available]","Hullo All,

I am Tempest Storm.

Background

I have been building Pokemon AI tools for years. I couldn't get researchers or news media to cover my research so I am dumping a bunch here now and most likely more in the future.

I have bots that can play Pokemon Shining Pearl autonomously using Computer Vision. For some reason, some people think I am lying. After this dump, that should put all doubts to rest.

Get the code while you can!

Videos

Let's start with the video proof. Below are videos that are marked as being two years old showing the progression of my work with Computer Vision and building Pokemon bots:

[https://vimeo.com/389171777](https://vimeo.com/389171777)

[https://vimeo.com/379207494](https://vimeo.com/379207494)

[https://vimeo.com/381522506](https://vimeo.com/manage/videos/381522506)

[https://vimeo.com/378229181](https://vimeo.com/378229181)

The videos above were formerly private, but I made them public recently.

Repos

Keep in mind, this isn't the most up date version of the sword capture tool. The version in the repo is from Mar 2020. I've made many changes since then. I did update a few files for the sake of making it runnable for other people.

Tool #1: Mock Environment of Pokemon that I used to practice making machine learning models

[https://github.com/supremepokebotking/ghetto-pokemon-rl-environment](https://github.com/supremepokebotking/ghetto-pokemon-rl-environment)

Tool #2: I transformed the Pokemon Showdown simulator into an environment that could train Pokemon AI bots with reinforcement learning.

[https://github.com/supremepokebotking/pokemon-showdown-rl-environment](https://github.com/supremepokebotking/pokemon-showdown-rl-environment)

Tool #3 Pokemon Sword Replay Capture tool.

[https://github.com/supremepokebotking/pokemon-sword-replay-capture](https://github.com/supremepokebotking/pokemon-sword-replay-capture)

Video Guide for repo: [https://vimeo.com/654820810](https://vimeo.com/654820810)

Presentation

I am working on a Presentation for a video I will record at the end of the week. I sent my slides to a Powerpoint pro to make them look nice. You can see the draft version here:

[https://docs.google.com/presentation/d/1Asl56GFUimqrwEUTR0vwhsHswLzgblrQmnlbjPuPdDQ/edit?usp=sharing](https://docs.google.com/presentation/d/1Asl56GFUimqrwEUTR0vwhsHswLzgblrQmnlbjPuPdDQ/edit?usp=sharing)

QA

Some People might have questions for me. It will be a few days before I get my slides back. If you use this form, I will add a QA section to the video I record.

[https://docs.google.com/forms/d/e/1FAIpQLSd8wEgIzwNWm4AzF9p0h6z9IaxElOjjEhBeesc13kvXtQ9HcA/viewform](https://docs.google.com/forms/d/e/1FAIpQLSd8wEgIzwNWm4AzF9p0h6z9IaxElOjjEhBeesc13kvXtQ9HcA/viewform)

Discord

In the event people are interested in the code and want to learn how to run it, join the discord. It has been empty for years, so don't expect things to look polished.

Current link: [https://discord.gg/7cu6mrzH](https://discord.gg/7cu6mrzH)

Who Am I?

My identity is no mystery. My real name is on the slides as well as on the patent that is linked in the slides.

Shining Pearl Bot?

It is briefly shown at the beginning of my Custom Object Detector Video around the 1 minute 40 second mark.

[https://youtu.be/Pe0utdaTvKM?list=PLbIHdkT9248aNCC0\_6egaLFUQaImERjF-&t=90](https://youtu.be/Pe0utdaTvKM?list=PLbIHdkT9248aNCC0_6egaLFUQaImERjF-&t=90)

Conclusion

I will do a presentation of my journey of bring AI bots to Nintendo Switch hopefully sometime this weekend. You can learn more about me and the repos then.",LearnMachineLearning
rcqvzh,1639082885.0,"How do games like ""while true: learn"" help you learn machine learning?","I've seen others, but this game specifically was given for free at epic games. Basically, at least for the first 20 or so puzzles you get a random input of shapes/colors and you have to sort them to the outputs, using decision trees (if X up, if Y down, else random), expert systems (if X up, else down) and others similars to fulfill the output requirements, like ""red or blue"", ""any"", ""only red"", ""only red or blue triangles, and green circles"", and so on.

I've been playing, and while I understand that the puzzles and logic behind it do have a relation with machine learning basics, how could ML concepts be applied in solving those kind of puzzles? How can you develop the ""logic path"" used in the game to apply that in ML?

I understand that you cannot build a ML system just by playing a game like this, I know it involves a lot more, between structure, logic, data treatment, data collection and so on. Just wanted to ask about how those relate, because playing that game (And there are others, its just that I personally tried this one, but it can apply to others), I usually just try sorting them out, balancing outputs until I get satisfying solutions. From my experience with programming and electronics, I know this is not the way.",LearnMachineLearning
rcovbp,1639077394.0,What to do now in Machine Learning?,"1. I know how to code pretty well.
2. Don't know any of the math needed for data science.

Can anyone please recommend some free resources(books, courses etc) to learn the math. And the order in which I need to study the math topics?(I am currently in 10th grade, so according to you what I need to study first to further move on to the math needed for data science)

:)",LearnMachineLearning
rcnfgx,1639073409.0,Find feature importance for an SVM with an RBF kernel,"I'm a bit beside myself. I'm trying to find the best way to evaluate feature importance in a 5-feature data set for an SVM with an RBF kernel from sklearn in Python. Because the kernel is non-linear, I cannot use the .coeff\_ attribute. Searching online presented permutation\_importance, but is there anything else? I ask because logistic regression and XGBoost have other evaluation methods for feature importance and it would be good sanity check the only method I have thus far found for an RBF SVM.

Thanks in advance.",LearnMachineLearning
rclejv,1639067762.0,Do you know any good GAN tutorials?,Hi. I've learned GAN for MNIST dataset and need more advanced GAN tutorials with higher image resolution than MNIST?,LearnMachineLearning
rckmfb,1639065550.0,What would be involved in training a machine to convert complex images into a simplistic art-style?,"I have heard (and seen results from) programs that can generate images which follow the ""rules"" of certain art-styles - such as creating fake Pokémon Sprites that (despite not looking like they depict anything) clearly use the same style.

And I know there is software that can ""look"" at images to try and figure out the contents.

This makes me curious, what would be involved in (for example) having a computer ""look at"" images of other fictional beasts like the thousands of Mario enemies or Digimon (using the images from their franchise wikis) and then try to convert them to something that follows those rules.",LearnMachineLearning
rcke6u,1639064869.0,I'm a computer science student and I want to work in ML,"I'm currently working on a double diploma computer science / engineering management. I really want to work in deep learning but I had few classes on it (well, one class one semester so like 10h-ish of theory and 20h-ish of a project). Though I did a 2month internship in deep learning (I tried to recreate SimCLR based on the scientific paper (contrastive learning)) and well... I did better than expected but still not great results.

I now have to find a 6 month internship and I didn't do any ML since, I'm wondering what online courses / youtube channels you would recommand to have better foundations in ML and have better chance to find an internship.

I don't necessarily want projects, just theory can do.

Thanks a lot",LearnMachineLearning
rcix6r,1639060477.0,Community Databricks no longer available?,How can I access community edition? Every time I try and sign up I get taken towards the paid version?,LearnMachineLearning
rciodi,1639059713.0,Why does my model perform better when I consider an unimportant feature?,"Hey everyone,

I am currently a part of a kaggle competion where I need to predict a the probability of default of a particular dataset.

So I currently have a trains.csv where I performed feature engenering, one hot encode the categorical values and dealt with missing values/outliers and a test.csv where I test the performance the model considering unseen data. All good here.

Implemented a logistic regression got the appropriate metrics but did notice something odd.

I forgot to drop the variable ""customer\_id"" went back removed it the model performed worse and when using the predict() function I am nonly predicitng 0s and not a single 1. However if I consider this variable the model performs much better and I am able to predict both 0s and 1s.

Made a submission file for kaggle with the ""customer\_id"" and a column ""risk"" with the predictions and got a good score.

I am not understanding how is this even possible. Why am I only prediciting 0s if I do not consider this variable? I have a notebook with me if someone need to look at a specific part.

Thank you for your time!",LearnMachineLearning
rchy2l,1639057412.0,What do you guys thinks about this AI DREAM app? How do u think was it created?,Tools and resources used for this [app](https://app.wombo.art)?  What will it take to create a text to video generation app?,LearnMachineLearning
rchvfb,1639057171.0,What's the point of HardSwish?,"A while ago, [swish](https://www.wolframalpha.com/input/?i=f%28x%29+%3D+x+*+sigmoid%28x%29) [got some attention](https://www.machinecurve.com/index.php/2019/05/30/why-swish-could-perform-better-than-relu/#why-swish-could-be-better-than-relu), as it outperformed ReLU in quite some tasks. As far as I understood it, it's main benefits are that it keeps the nice properties of ReLU while being differentiable everywhere and the fact that it's a bit like leaky ReLU for negative values.

So, what I took away from that is that it's a fully differentiable leaky ReLU. I have been using it here and there and it always worked great. It wasn't implemented in pytorch so I always defined it myself(easy enough). Today I found out that torch 1.10 has [HardSwish](https://pytorch.org/docs/stable/generated/torch.nn.Hardswish.html) which has very similar values to swish, but is a composition of 3 functions and is much faster to calculate. BUT, as far as I understand it, it isn't continuous in the points where it ""switches"" from one functions to another, taking away one of the big benefits that swish had.

So we basically give up a fully differential function to save some compute.

My questions are:

* Am I understanding all of the above correctly or are there some errors or misconceptions in my thinking?
* And, as in the title, is it worth it? Why and when use HardSwish over Swish? Does that bit of more compute actually make a difference?",LearnMachineLearning
rchlwg,1639056273.0,DeepWalk embeddings differentiates nodes with degree zero,"Hi,

I have been using DeepWalk ([publication](https://arxiv.org/pdf/1403.6652.pdf), [implementation](https://karateclub.readthedocs.io/en/latest/_modules/karateclub/node_embedding/neighbourhood/deepwalk.html)) in order to embed the nodes of a network into a latent space. By mistake, I had some unconnected nodes in my network, so DeepWalk should not be able to generate any random walks on them. In the implementation, the random walk that is returned would just contain the node itself.

Interestingly, I found that the model actually learned to separate nodes with degree 0 from other ones. While this itself isn't too surprising, the model also found some clusters within these nodes.

How can that be? How can different nodes, which aren't connected to anything in the network, have different representations?

Thank you very much for your input!",LearnMachineLearning
rcg62g,1639051020.0,Having difficulty understanding Pix 2 Pix's discriminator,"I'm trying to understand the Pix 2 Pix GAN which is used for image translation. Paper here: https://arxiv.org/pdf/1611.07004v3.pdf

The discriminator looks at 70 by 70 pixel patches and evaluates whether they are real or fake. It takes both the conditional image and the synthesised image as an input. According to the Tensorflow tutorial's implementation these 2 images are concatenated together: https://www.tensorflow.org/tutorials/generative/pix2pix

My question is if the discriminator is only looking at 70x70 patches at a time, how does it understand how the 2 images relate and check that the conditional input has actually informed the image that has been generated?

Any help greatly appreciated",LearnMachineLearning
rcftpj,1639049575.0,Why doesn't my complex model look like is overfitted?,"Can anybody tell me why my model DOES NOT overfit? I want to intentionally overfit it, but the training, validation and test set all seem fine, even though my model is very complex for the dataset.

[Accuracy of the model](https://preview.redd.it/ihh9aeo36i481.png?width=350&format=png&auto=webp&s=5d7da711021889e08a44d7a41f925cbfe940e971)

I use `sklearn` on the [iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). [Here](https://pastebin.com/UFKnTLZp) is my code.",LearnMachineLearning
rcfb33,1639047426.0,Requirements for an entry level job?,"Hi there, I'm curious about what are the requirements for every level job regarding machine learning.
I'm not originally from computer sciences, I'm from food sciences engineering. But I do have long experience in creativity and problem solving (3+ years as R&D).
I've recently learned python and learning machine learning, data mining and scraping since last month.
Today I'm starting to apply for jobs. What do you think should be requirements of an entry level jobs in machine learning?",LearnMachineLearning
rcemnh,1639044371.0,Feeding mri data into Spherical CNN,"Hi, this is my first time posting here. I am currently working on my Master's thesis which involves detection of certain diseases using the mri scans of brain tissue of individuals using spherical CNN. I am dividing the brain into left and right hemispheres.

I have mri surface pial,sphere and curve files for each hemisphere from free surfer.

I can get and create a 3d point cloud map using nibabel and open3d for pial and sphere files.
The same files also contain the co ordinates for mesh triangles.
.curv file contains a list of values whose number equal to the number of points in the point cloud map.

Is there any way to display the surfaces that is read from the pial and sphere file so that we can display it like what is shown in the freeview (part of free surfer to display the free surfer files) or save the output from freeview in the form of numpy array that can be read in using nibabel.


[Screenshot.zip](https://linksharing.samsungcloud.com/gFJCsNrN1WjB)",LearnMachineLearning
rcdbiz,1639038397.0,How To Version Your Machine Learning Experiments Instead of Just Tracking Them - Distributed Versioning vs. Centralized Tracking for ML Experiments with DVC and Git,"ML experiments often get split between Git for code and experiment tracking tools for meta-information - because Git can't manage or compare all that experiment meta-information, but it is still better for code.

The following guide explains how to apply DVC for ML experiment versioning that combines experiment tracking and version control: **[Don't Just Track Your ML Experiments, Version Them](https://dvc.org/blog/ml-experiment-versioning)** - Instead of managing these separately, keep everything in one place and get the benefits of both, like:

* **Experiments as code:** Track meta-information in the repository and version it like code.
* **Versioned reproducibility:** Save and restore experiment state, and track changes to only execute what's new.
* **Distributed experiments:** Organize locally and choose what to share, reusing your existing repo setup.

Experiment versioning treats experiments as code. It saves all metrics, hyperparameters, and artifact information in text files that can be versioned by Git, which becomes a store for experiment meta-information. The article above shows how with DVC tool, you can push experiments just like Git branches, giving you flexibility to share experiment you choose.",LearnMachineLearning
rcbvnb,1639032291.0,deepfashion / mmfashion,"Reading the getting started [page](https://github.com/do9605/mmfashion_modified/blob/main/docs/GETTING_STARTED.md) on github says:

python demo/test\_\*.py --input ${INPUT\_IMAGE\_FILE}


can somebody explain how would i get to this point?

I want to create a rec system, i usually go off of existing projects, but there are none online..",LearnMachineLearning
rc60ei,1639012489.0,"sagemaker canvas limits, pitfalls, and short-comings?","Was getting my feet wet with using Sagemaker Canvas and fell into my first hole.

*Please ensure that the dataset has at least 250 rows with no missing value*


Can't even train on [kaggle titanic](https://www.kaggle.com/c/titanic/data?select=train.csv) because not everyone has an age or cabin.
What other limitations and short-comings am I going to have to make allowances for?",LearnMachineLearning
rc56gb,1639009937.0,How to work with data that updates daily?,"I am currently working an ML portfolio project where the data updates daily. All of the projects I have worked on to date have been with flat datasets where the data isn't ""live"" or updating, so the model building has been pretty vanilla. I wanted to challenge myself and start pulling data from an API where the data is in fact being updated.

For example, I have a ""Status"" feature that contains values like ""Received"", ""In Review"", ""Approved"", ""Appealed"", and ""Closed"" which updates at the end of the day. I should note that frequency of updates is very small, some values can stay in their current status for months on end.

My original plan was to build the model one time and deploy it in a Streamlit app. But after thinking it over, this seems flawed as the model won't necessarily account for changes in the data and the performance might drop over time as the updates are being made.

What would be the easiest way to handle this?",LearnMachineLearning
rbzzpz,1638994713.0,“[D]” What’s the best way to analyse behavioural/activity data to get correlations with health insights?,"I have a large dataset with activity/behavioural data, date abs time stamp of when this activity took place, self-reposted mood associated with each activity, and geolocation data. What’s the best way for me to derive health and wellness correlations from this data based on a user’s activities? Can I build a recommendation engine to suggest do more of something or less of something based on user goals? These activities are activities of daily living, such as working, reading, shopping, cooking, cleaning, watching TV, etc.

Any suggestions to get started would be helpful!",LearnMachineLearning
rbw8g2,1638984249.0,Help with nim game using reinforcement learning.,"I recently was given this assignment: https://imgur.com/a/k6XNdxz.

I have already submitted it but it's taking a while to get back and I would like some feedback to know that I'm on the right track and actually understanding the content.

Here is my code:

    // get reward of a state
    float get_reward(int state, int turn)
    {
      float retval = 5;

      if (state == 20) {
        retval = 100;
      }

      if (turn == 0) {
        return retval;
      } else {
        return -1*retval;
      }

    }

    // get potential of current state
    int get_va(int state, int turn)
    {
      if (state > 20) {
        return 0;
      }

      turn = (turn + 1) % 2;

      float reward1 = get_reward(state+1, turn);
      float reward2 = get_reward(state+2, turn);

      float v1 = reward1 + GAMMA*get_va(state+1, turn);
      float v2 = reward2 + GAMMA*get_va(state+2, turn);

      // multiply each potential by its probability of happening
      // 0.5*v1 + 0.5*v2
      return 0.5*(v1 + v2);
    }

    // decide which value to play - returns either 1 or 2
    int get_value(int score)
    {
      float one_reward = get_va(score+1, 0);
      float two_reward = get_va(score+2, 0);

      printf(""1: %f, 2: %f\n"", one_reward, two_reward);
      if (one_reward >= two_reward) {
        return 1;
      }

      return 2;
    }

My thinking was to get the potential of each move (I know I do this each time and is inefficient, but for now that's fine) and play the move with the highest potential each time. I'm a bit skeptical of this though since I just assumed 50% probability of each play rather than the reality of the other opponent playing the best move.

I would love some feedback.

I didn't share the rest of the code since it is my professor's and I don't want to get in trouble for sharing that, but it is pretty simple, it just prompts the user for an input, keeps track of the score, and calls the get_value function when it is the computer's turn.

Edit: I just found a way to make it not work.. here are the logs:

    > make run
    Compiling...
    Running:
     Who says first 20

    Who goes first: you=1 computer=2 ? 1
    We are at 0 , add 1 or 2 ? 1
    1: -3.000000, 2: -2.000000
    We are at 1  Computer adds 2
    We are at 3 , add 1 or 2 ? 1
    1: -3.000000, 2: -3.000000
    We are at 4  Computer adds 1
    We are at 5 , add 1 or 2 ? 1
    1: -1.000000, 2: -3.000000
    We are at 6  Computer adds 1
    We are at 7 , add 1 or 2 ? 1
    1: -4.000000, 2: 0.000000
    We are at 8  Computer adds 2
    We are at 10, add 1 or 2 ? 1
    1: -8.000000, 2: 0.000000
    We are at 11 Computer adds 2
    We are at 13, add 1 or 2 ? 1
    1: -17.000000, 2: -8.000000
    We are at 14 Computer adds 2
    We are at 16, add 1 or 2 ? 1
    1: -25.000000, 2: -50.000000
    We are at 17 Computer adds 1
    We are at 18, add 1 or 2 ? 2
     YOU WIN !!",LearnMachineLearning
rbvn1s,1638982668.0,Cannot load data file into weka,"I download a data set which is of Type ""DATA File"" (I can open it in notepad and its content contains lines like this: ""18.0   8   307.0      130.0      3504.      12.0   70  1	""chevrolet chevelle malibu"".I convert it to arff using this link:[https://ikuz.eu/ikuz.eu/csv2arff](https://ikuz.eu/ikuz.eu/csv2arff/)"" but the Type doesn't change, and my weka cannot recognise this file.

Sry if this is a dumb question but its my first time using weka.


EDIT: For anyone that has a similar problem, a friend of mine gave this article to follow and solve itL: [https://machinelearningmastery.com/load-csv-machine-learning-data-weka/](https://machinelearningmastery.com/load-csv-machine-learning-data-weka/)",LearnMachineLearning
rbsbgf,1638973062.0,Hi. Is anyone able to help me with my college project?,"Its about linear models with gradient descrnt, neural networks and deep learning on python. Please dm me if you can. Thanks 😊",LearnMachineLearning
rbrx6y,1638971878.0,Need any insights possible on this A.I project. People of reddit save my soul!,"Hey, folks of Reddit!

**I'm in a dire situation. I need to complete my college project on artificial intelligence.**

But I'm so lost, I don't know how to build this as I don't have solid experience in A.I. I've only ever built apps and websites.

**The project given to me is ""detecting seat occupancy using A.I""**

So for example, if I take a photo of 9 seats (3 of which are occupied by my friends), The A.I should be able to tell that 3 out of 9 seats are occupied.

How can I go about doing this? I can some grasp terms in deep learning since I've played around with it a tiny bit.

Any help would be greatly appreciated. Cheers!",LearnMachineLearning
rbqnxf,1638967845.0,"Learning or working with AI? Or simply want to connect with people in this field? Come join us, we are a Discord Community with over 20'000 members! Ask questions, find teammates, share your projects, find job offers and much more!","

Programming is way more fun when you learn/work with someone. Help each other, ask questions, brainstorm, etc. There is just so much benefit to joining a community when you are in this field, especially when you cannot find the question you are looking for on stack overflow! 😉

This is the same thing with AI, and it is why a little less than a year ago I created a discord server. Where anyone learning or working in the field could come and share their projects, learn together, work together, and much more. The community is now close to 10 000 members, which is unbelievable! So glad to see it growing and see everyone so active.

Come join us if you are in the field of AI, and introduce yourself when you're in!

https://discord.gg/learnaitogether",LearnMachineLearning
rbq0oq,1638965586.0,How to organize machine learning infrastructure?,"Let's say you want to start using machine learning in your company.
How would you go about organising machine learning infrastructure?


For example I was thinking between:

1.  having one big service in which you have all the machine learning code and models (nlp, churn prediction, user clustering etc). Maybe a django service with multiple apps and each app has it's own endpoint you can call for predictions
2. have multiple rest apis each with it's own model you can call for predictions

Please motivate your answer (pros, cons for each approach, if possible) and also you can share resources (links, books, etc)",LearnMachineLearning
rbpj3s,1638963768.0,💊Your daily dose of machine learning : converting deep learning models to ONNX format,"> *This is a series of posts that I post almost daily. I call them “your daily dose of machine learning”.*

So as I mentioned in yesterday's post, this week I am sharing some tips and insights about ONNX.

If you're a Tensorflow developer or PyTorch developer then the first step for you to use ONNX in production would be to transform your model to ONNX format.

To transform your tensorflow models into onnx format, you can use a tool called tf2onnx. The image below shows how to use this tool.


https://preview.redd.it/2bpyfzdz2b481.png?width=1666&format=png&auto=webp&s=e0599d7664bfe05e9f95de5e617f6e974c860140

You can learn more about this tool on their github repo : [https://github.com/onnx/tensorflow-onnx](https://github.com/onnx/tensorflow-onnx)

If you’re a Pytorch user, then you can transform your model from torch to onnx format directly using Pytorch. This is done using the module torch.onnx.
An example of this: [https://pytorch.org/tutorials/advanced/super\_resolution\_with\_onnxruntime.html](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)

Connect with me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***",LearnMachineLearning
rbp1e6,1638961896.0,What are state of the art approaches for product matching with high accuracy?,"My goal is to compare invoice product prices against a price catalog to qualify if the price is reasonable. The information I do have is everything that is usually seen on an invoice (e.g.: product description, unit, quantity, price)
Here is an example of some product descriptions (german):

* Kelit Kelox Windox-U Press-T-Stk. aus Windox-PPSU d20mm xl/2\\\\'IG messing
* Messing Muffe reduziert Nr. 3240 IG/IG 3/4''x1/2''
* Geberit Silent-PP Rohr mit Muffe Länge 50cm d40

I have tried string similarity so far, however this approach seems to be too slow and easily fails if the string length differs too much. The main problem is that the available data which can be used for comparison become gradually more.
Further, I also tried to capture certain attributes of a product description (e.g.: “brand”, “actual product”, “material out of which product is made”, …) by looking for phrases that are present in the product description. Consequently, comparison is done over these attributes. This approach is also very cumbersome as these phrases need to be set up manually.

What other approach could I try?

Thank you a lot!",LearnMachineLearning
rboc5r,1638959023.0,Running the model with slightly different attributes,"My question is about how you organize your notebooks.

I am testing different document representations with tf-idf or doc2vec. Sometimes I make some small adjustments to the model, I want to keep the results so that I can compare later, but I am not sure whether I should open a new notebook.

I am asking this because I am self-taught and I feel like my projects seems a bit this organized. Is there some framework maybe, to organize machine learning code?",LearnMachineLearning
rbo6s4,1638958414.0,Comparing Bayesian Linear Regression model with a Decision Tree Regression model,How would I go about comparing a Bayesian linear regression model with a Decision tree model?,LearnMachineLearning
rbo3c7,1638958018.0,Recursive Feature Elimination with Cross-Validation for imbalanced dataset?,"I'm currently new in Machine Learning and I just learned about  feature selection. In my project, I have a dataset with 89% being a  majority class and 11% as the minority class. Also, I have 24 features. I  opted to use Recursive Feature Elimination with Cross-Validation (RFECV  in the scikit-learn package) to find the optimal number of features in  the dataset. I also set the 'scoring' parameter to 'f1' since I'm  dealing with an imbalanced dataset. Furthermore, the estimator I used is  the Random Forest classifier. After fitting the data, I had around 12  features with an f1 score of 0.94.

Is using RFECV appropriate for imbalanced datasets?",LearnMachineLearning
rbno04,1638956326.0,CNN trained on fashion MNIST,"hello,

how would a CNN trained on the fashion MNIST perform on categorizing MNIST?

thanks!",LearnMachineLearning
rbm3yp,1638950022.0,I’m interested in machine learning but were do I start?,I’m looking for advice or tips about how to start learning about it,LearnMachineLearning
rblylk,1638949422.0,Can I optimizie a keras model with two losses so one of the losses is below the other losses threshold,"I have a keras model for predicting a time-series which I want to optimize in respect to two losses. One loss I just want to get close to a certain threshold value and the other one I want to minimize.

The first loss is MAE and the second loss is the mean error times a time series.


I've read [this](https://stackoverflow.com/questions/45961428/make-a-custom-loss-function-in-keras) stackoverflow question and it's quite close to my question, except I'm not entirely sure where to use the time-series in the input parameter for the loss function. Help is much appreciated!",LearnMachineLearning
rbl2r4,1638946103.0,Seeking Guidance," Hi, I am Jay Aslaiya. I am in second year of Bachelor of Engineering in Computer Science branch. I want to pursue Machine Learning as my career and i dont know how to become good in it. So i am asking for guidance or road map how to pursue it. I know what all things are required to be known in this field but dont know the resources from where to learn. So please guide me and it would be great if i get resources suggestions too. I know python and know that have to do maths as next step. So any resource and guidance will be helpfull.

Thanks a lot",LearnMachineLearning
rbj8vg,1638939654.0,Interpreting clustering results?,"Hi all, beginner here, I did kmeans clustering for customer data and added the cluster labels back to the original data which will then be exported as a csv (cluster labels in an additional column)

Wondering how do we do the analysis now for the customer personna? Ideally we want to be able to say Eg Cluster 1 are the well to do men in their 40s, cluster 2 are the women in technology in their 20s, etc.

Do we do pivot tables and averages in the excel or do we intepret from the centroid table in Jupyter notebook? (below)

Here is an [example](https://imgur.com/a/kvn7AoZ) of the centroid table output from Jupyter notebook (different from actual), but how would one interpret this?

Would i say for example cluster 1 is the group of people with lowest cooling tower frequency, and relatively low humidity etc?

Cluster 4 contains people with the highest cooling tower frequency and highest relative humidity?

Thanks so much in advance!",LearnMachineLearning
rbgeq0,1638930977.0,Can someone help me derive the equation used in this paper about NMF?,"&#x200B;

https://preview.redd.it/e26ukpird8481.png?width=922&format=png&auto=webp&s=6538bdb9f631f990274710e7302076de903fadb1

So I am reading [this paper](https://pubs.acs.org/doi/abs/10.1021/acs.jproteome.0c00819) that used NMF for its data analysis. Of course, I have already read the paper and have already gotten some insights on what some of these concepts mean, but the majority of the concepts are still very vague to me because the terms are very unfamiliar to me as a biological sciences major. Can someone explain the following concepts to me like I'm five:

1. Which language was this? Is this python? If I want to learn how to utilize this tool, should I take a crash course on Python? Would I be able to understand what ""initialize"" and ""until convergence' mean if I do that?
2. What does it mean when you ""initialize""? How is it related to the return *H* below?
3. What does the ""until convergence"" mean?
4. What does the ""←"" mean?
5. Can someone derive Formula 1 and Formula 2 from Formulas A, B, and C?
6. In formula one, what does the V/WH mean? How can you divide a Matrix over W and H?
7. In formula one, Can the right side of the component-wise multiplication be written as:

* (Matrix W)\^T × Matrix V × (Matrix W)\^-1 × (Matrix H)\^- 1?
* If so, I am aware that in matrix cross product the order of multiplication matters... How will I know what is the order of the multiplication? Did I provide the correct order for the cross-product?

1. Lastly, in formula 2, what do the four vertical bars in ||Hg|| mean?",LearnMachineLearning
rbgcu0,1638930814.0,EfficientNet and MobileNet for object detection?,"Recently I've seen a paper for what would seem to be a detection problem (would require object localization -> classification) but the researchers say they use MobileNet/EfficientNet. How would this be possible? There was no mention of the localization step, and it would be in scenarios with mostly background (ie not zoomed in enough to skip the localization). Previously I've seen software engineers use Yolo for detection and then pass the cropped bbox to MobileNet, EffNet, but that's definitely not the case here. So how is it possible to use a classification network for a detection problem?",LearnMachineLearning
rbeq9o,1638926056.0,Projects for Beginners that have to do with games?,"Hi all!

A few months ago I graduated with my BA in Psych, I recently applied for some PhD programs in psych and other more interdisciplinary programs. My primary research has to do with researching social cognition through games. I took some calculus during my undergrad, I also have a couple years of doing data analysis in python. If there's things I need to learn in order to do the project (highly likely) I'm more than happy to learn.

I'd like to start doing ML projects for various reasons. But I'm interested in any beginner projects that let me mess with game data. There are a few datasets on Kaggle that I might mess with. So far I'm thinking

* Sentiment Analysis of chat logs from twitch
* Sentiment Analysis of Dota 2 chat data

I'm really interested in analyzing toxicity within specific games/communities. So I think this would be a good starter project? If anyone has any other ideas or resources I'd love to hear them.

&#x200B;

Thank you!",LearnMachineLearning
rbdq2o,1638923019.0,Neural network determines gender of severely blurred images with 88% accuracy,"First of all, let me say that I am a layman who is passionate about AI and ML as a hobby. I have no programming skills, so I am experimenting with an image classification program that is very intuitive (Lobe).

I got from the website [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/)

35 male and 35 female faces, blurred them quite a bit and labeled them M or F accordingly.

Lobe claims 88% accuracy after training, and when I tested it with 10 test images I can roughly confirm that.

How on earth is that possible? The website mentioned above creates the faces completely randomly, only the image section is always the same. Skin color, age, head position and hairstyle are very different, so how can the model manage to find generalized criteria to judge the pixel mud?

I attach 2 examples each for M and F.

I'm sorry if the layman speaks out of me and the answer is obvious, I still welcome explanations.

&#x200B;

The key question is how a computer can extract info about gender from a complete pixel mud

&#x200B;

&#x200B;

[Male 1](https://preview.redd.it/80armpqsq7481.png?width=1024&format=png&auto=webp&s=d3c6f49b04f47e577c11c54f566db44d8de60c82)

[Female 1](https://preview.redd.it/mys39zpsq7481.png?width=1024&format=png&auto=webp&s=317ad3a8e0c9820d93e2d44795642ff1e15a16ea)

[Male 2](https://preview.redd.it/o4usn0qsq7481.png?width=1024&format=png&auto=webp&s=8ebe86286197b3a3b0432fcb5e503f66caee060d)

[Female 2](https://preview.redd.it/u1xcg3qsq7481.png?width=1024&format=png&auto=webp&s=c27a4289528eb75216c257d50344dfbd5c7bfd97)",LearnMachineLearning
rbc9fd,1638918744.0,Does anyone have experience using Riot Games API to do machine learning tasks?,"I am doing a final project in which I would like to use Riot Games API to do some basic machine learning analysis from my league of legends profile.

&#x200B;

I have been following this [tutorial](https://towardsdatascience.com/riot-api-a-machine-learning-and-data-analysis-application-c8524b4160b4) but there are some variables in it that are undefined when I followed the tutorial.

&#x200B;

I am just wondering if anyone could help me understand what I am missing or share a way that you used Riot API to learn. I am not doing something insanely hard or anything just want to do basic analysis.",LearnMachineLearning
rbbl88,1638916881.0,Image emotion classification,"When i researched about the subject i found [this](https://arxiv.org/pdf/1611.07145.pdf) paper and i am very pleased with the results they got. But they dont give any code or model (far as i saw) so the equations and explanations are pretty much magic to my untrained eyes. I also found [this](https://github.com/acumos/image-mood-classifier) classifier but i really cant get it to work for me. I am sure they did everything to explain how to set it up but it isnt clear enogh for me so if anyone can explain to me or give me any lead, it will be appreciated.",LearnMachineLearning
rbaq7a,1638914533.0,When should we apply normalisation to our data?,"Let's assume I have data set with tweets and binary variables that represents  positive and negative tweet
Do I need to normalise my binary data if I want to apply naive bayes?
What about if I have 3 category positive 1 negative-1 and natural 0 will I need normalisation",LearnMachineLearning
rbabt8,1638913399.0,How do you deal with multiples dataframes of the same structure but with different values over the years ?,"I'm trying to learn more about data science from the (too basic) courses I took during my past bachelor's degree in computer science.

Until now I have mainly worked with already strucured datasets that I found in various places. I always cleaned these datasets with Pandas and then applied supervised or semi-supervised methods just by manipulating variables names. The structure of the dataframes has always been the same, for example :

&#x200B;

||id|gender|country|age|
|:-|:-|:-|:-|:-|
|0|242|M|Brazil|40|
|...|...|...|...|...|
|n|815|F|Canada|38|

&#x200B;

So I decided to challenge myself by building my own dataset with indicators over the years I found from different free databases. After a first batch of headaches with data processing and data cleaning I finally managed to obtain two DataFrames with the same structure (years as index and names as column headers) except for the values which are different (one indicator per DF), here is an example:

**First DataFrame with weight**

||Name #1|Name #2|Name #...|Name #n|
|:-|:-|:-|:-|:-|
|Year 1|138|129|185|130|
|Year ...|...|...|...|...|
|Year n|174|155|134|220|

**Second DataFrame with size**

||Name #1|Name #2|Name #...|Name #n|
|:-|:-|:-|:-|:-|
|Year 1|49|51|49|55|
|Year ...|...|...|...|...|
|Year n|62|61|59|64|

&#x200B;

I don't have the impression that a merge or a concat are possible since I have almost 10 000 individuals over about 50 years, with the names of the columns as labels of the individuals themselves. Where I block is how to obtain same kind of structure that in the first exemple in order to apply some ML methods as usual ?

I have turned the problem around in several directions and I must have made a mistake somewhere but without managing where exactly, yet I dare to assume that this problem must be common in data science, unless I am wrong ? How can you perform an analysis with python in this case ?",LearnMachineLearning
rb84aa,1638907438.0,Does anyone know how to increase weka's perfomance on linux?,"I use linux for most of my tasks, I edit the dataset and do all my readings, but I still have to use windows to run weka, because of permance issues on linux. In linux, it takes way longer and consumes way more resources, and the heap size is more than enough for the program, meaning it's heavy on the CPU. Does anyone know how to fix it? It can get realy annoying to reset the pc everytime I need to boot Windows just to run Weka.",LearnMachineLearning
rb7oa7,1638906224.0,Dissecting Lobe.ai models in TensorFlow. Where to start?,"I'm new to machine learning and trying my hand at CNNs at the moment. I have a Raspberry Pi set up at my window to take pictures of birds who visit a feeder. What I want to make is a model with TensorFlow to detect and save photos of birds. Sometimes a car or pedestrian passing by is captured and are of no interest to me.

&#x200B;

The models I made thus far are no good for two reasons. Its accuracy is basically a coin toss, and I don't have enough knowledge to improve it (yet!). I came across the [Lobe.ai](https://Lobe.ai) model builder and decided to reverse engineer its models. It has a desirable accuracy with the same dataset, and I want to see what makes it tick. It can export the model it makes in TensorFlow format, but I have no clue on how to compare it with mine. I'm interested in the preprocessing, data augmentation, used layers, number of epochs, etc. Is this possible at all once the model is made? What files do I look at?

&#x200B;

I could just use Lobe's model, but that is my last resort since I won't learn anything.",LearnMachineLearning
rb7cs3,1638905387.0,Can someone explain some code of VQGAN+CLIP for art generation,"Here's the [link](https://colab.research.google.com/drive/1L8oL-vLJXVcRzCFbPwOoMkPKJ8-aYdPN). I read a few articles that explained how VQGAN worked like a GAN and CLIP as a discriminator. I looked at the code but didn't understand too much.

I just want to know which code block is doing what. The fifth block for example defines functions like sinc, lanczos and I actually don't know what they're being used for

Thank you!",LearnMachineLearning
rb7c6j,1638905344.0,Using Bayesian Belief Networks with Numerical Data,Is there a way to do this? All the tutorials I've seen are with Categorical Variables (makes sense given the probabilities associated with each node). Is there a way to use BBNs with Numerical Data without transforming it to a categorical dataset?,LearnMachineLearning
rb62hd,1638902430.0,How do I code in Ubuntu?,"Hi everyone!

I generally use Windows and python via Anaconda jupyter notebook. Today I installed ubuntu in my laptop in order to learn that as well parallely but I've already spent so much time in understanding basics like how to install package and launch jupyter notebook from terminal, etc.
I want to know if anyone can recommend me a brief tutorial for Ubuntu terminal.. Just enough to get me started.",LearnMachineLearning
rb5r86,1638901830.0,Machine learning and AI,"Hey guys, im doing an assignment for machine learning and ai, what topics should i cover?",LearnMachineLearning
rb2fsv,1638893281.0,Managing ML Experiments as Code with Git and DVC,"Machine learning experiment tracking tools log ML experiments to a central database and show them in a dashboard. This makes it easy to share them with teammates and compare. However, in an active experimentation phase, you may create hundreds of experiments, so team members may be overwhelmed, and loose the ability to effectively share experiments between team members.

The following article shows how with DVC tool, you can push experiments just like Git branches, giving you flexibility to share experiment you choose: **[Don't Just Track Your Machine Learning Experiments, Version Them - DVC](https://dvc.org/blog/ml-experiment-versioning)**

All the experiments you run are stored in your local repo, and only the best experiments are promoted to the central repo (GitHub for example) to share with teammates - distributed experiments are shared with the same people as your code repo.

* Traditional **experiment tracking** tools log ML experiments to a central database and show them in a dashboard. This makes it easy to share them with teammates and compare.
However, in an active experimentation phase, you may create hundreds of experiments, so team members may be overwhelmed, and loose the ability to effectively share experiments between team members.

* With DVC, **experiment versioning** treats experiments as code. It saves all metrics, hyperparameters, and artifact information in text files that can be versioned by Git. You do not need a centralized database or online services. Git becomes a store for experiment meta-information and DVC data versioning backs up the artifacts themselves anywhere.",LearnMachineLearning
rb1urf,1638891453.0,PhD in Machine Learning,"If I want to do PhD in ml, is it going to give me more importance? And which are the best topics for PhD in ml.",LearnMachineLearning
razw0l,1638885847.0,Requesting help,"I am a beginner in ML. I am working on a Tuberculosis(TB)  prediction project. I am doing binary classification using Tensorflow. I am using chest radiography images as the dataset for CNN. There are 3500 normal Chest X-Ray(CXR) images and 700 TB infected images. Do I need to perform data sampling? Or Can I find better dataset? Which optimizers, activation function, how many convolutional layers and dense layers to use? Should I use Data Augmentation? I am looking for suggestions.",LearnMachineLearning
raznry,1638885142.0,💊Your daily dose of machine learning : ONNX framework,"> *This is a series of posts that I post almost daily. I call them “your daily dose of machine learning”.*

 Last week I posted about different approaches you can use to deploy your Tensorflow model in C++.

One of these approaches is ONNX Runtime. This week I want to share more tips and insights about it.

So ONNX (Open Neural Network Exchange) is a framework or a whole ecosystem that allows for the standardization of neural networks.

Here’s the problem that ONNX is trying to solve.

There are many machine learning frameworks and libraries. Pytorch, Tensorflow and scikit-learn are famous examples.

Which one should you use?

Well, the answer is : it depends on so many factors!

There is no free launch!

So what ONNX aims to do is to make your choice easier. Basically, you can choose whichever framework you want, train your model with it and then, once you’re happy with your model, transform it into ONNX format and keep the production code working in ONNX only.

So ONNX will help you achieve 2 things:

* Keep your options open when it comes to choosing a machine learning library for your training.
* Standardize your production code, since you’ll only need to maintain one main dependency which is for ONNX.

Moreover, you can use your ONNX models in Python, C++ or other languages. 

More on ONNX in the upcoming days.

 Connect with me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***

&#x200B;

https://preview.redd.it/bbimbzoel4481.png?width=1176&format=png&auto=webp&s=7f526dc296ca6fd7fc9e0e544d47cea22f20f192",LearnMachineLearning
raxyt7,1638879616.0,Do you guys know a course;/tutorial that's not MNIST?,"I need to learn Tensorflow 1.x for a project.

Basically I have 6 features and want 1 binary ouput.

I've tried adapting MNIST code to perhaps work with what I need but it's just not working. It doesn't seem to be training at all and just stays constant no matter the epochs

    # The basic MLP graph
    x = tf.placeholder(tf.float32, shape=[None, 6])
    W = tf.Variable(tf.zeros([6, 1]))
    b = tf.Variable(tf.zeros([1, 1]))
    y = tf.nn.softmax(tf.add(tf.matmul(x, W),b))

    ## My highly likely dumb reshaped stuff
    y_train_reshaped = tf.reshape(y_train, [800, 1])
    y_test_reshaped = tf.reshape(y_test, [200, 1])
    ###

    # The placeholder for the correct result
    real_y = tf.placeholder(tf.float32, [None, 1])

    # # Loss function
    # cross_entropy = tf.reduce_mean(-tf.reduce_sum(
    #     real_y * tf.log(y), axis=[1])
    # )

    # # Optimization
    # optimizer = tf.train.GradientDescentOptimizer(0.5)
    # train_step = optimizer.minimize(cross_entropy)

    ## Maybe new cost and optimizer?
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=real_y) )
    train_step = tf.train.AdamOptimizer().minimize(cost)
    ###

    # Initialization
    init = tf.global_variables_initializer()

    with tf.Session() as session:

        epochs = 1
        session.run(init)
        for _ in range(epochs):
            session.run(train_step, feed_dict={x: X_train, real_y: y_train_reshaped.eval()})
        correct_prediction = tf.equal(y, real_y)
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
        network_accuracy = session.run(accuracy, feed_dict={x: X_test, real_y: y_test_reshaped.eval()})
        print('The accuracy over the the data is {:.2f}%'.format(network_accuracy * 100))",LearnMachineLearning
rawaur,1638873437.0,Want to get the top k important words from the sentence using infersent model," I am exploring the infersent model (Developed by Facebook researchers). In that model, there are a lot of methods and functions available and I am more interested in the visualize method which is showing the importance of the words from the sentence with the help of a bar graph. Now I am not getting any clue how to extract those words from the visualise methods, I checked the official GitHub page of infersent model, and I did not find any clue to get it done, I am totally clueless in this case.

&#x200B;

[Infersent Model visualize method](https://preview.redd.it/8yz3w68fm3481.png?width=780&format=png&auto=webp&s=bf768635c157de4761b11124e7227c1d433ccc90)

 **and the output I need is: moving, canada, business, meet. If I define k=4 (k is top number of words)**",LearnMachineLearning
rav68s,1638868730.0,tqdm and printing info,"&#x200B;

https://preview.redd.it/fd4xeg0h83481.png?width=696&format=png&auto=webp&s=315e421b2210c4fbae731ff3131050212a1678e8

So everytime i print, new progressbar is created,i tried position=0, [leave=True.Is](https://leave=True.Is) there way to stay with one progresbar?",LearnMachineLearning
rav2l3,1638868302.0,Starting machine learning from a natural science standpoint,"Hi. I am currently working on my PhD in material science.

As for my research, I am trying to accomodate machine learning into my work which requires quite a deep understanding of it. I cannot just tensorflow my way out of this one. Most research papers actually involves creating a package from scratch as the ML concepts are being adapted to molecular/atomistic calculations.

I am trying to find a good book that can give me:

1. a good perspective of the things happening in the ML space (what models / techniques are available and what is its used for )
2. A good rigour but not overly abstract introductions to the statistics and linear algebras used.
3. Equip me with skills understand research papers's methodology and hopefully dissecting their source codes that were made from scratch (most oftenly coded in python, c or fortran). Language isn't really a barrier to understanding but the implementation of the ML concepts instead.

Some of the good book candidates that I have are:

a.  An Introduction to Statistical Learning

b. Pattern recognition and machine learning

c. Deep learning

d.  Understanding Machine Learning: From Theory to Algorithms

e. Machine Learning: a Probabilistic Perspective

f. The Elements of Statistical Learning

I know that reading everything should be beneficial, but thats a bit unrealistic especially for a PhD student haha!

&#x200B;

Any help and opinions is appreciated! Thanks!",LearnMachineLearning
rau451,1638864259.0,No-Code Machine Learning Platform from Amazon : AWS SageMaker Canvas,"The recently launched SageMaker Canvas is pretty awesome for Beginners in Machine Learning like me! I tried my hands on a few ML Projects like Customer Churn, Spam SMS Detection etc and it gave the predictions 'almost' accurate !

There is a Short Course on Udemy on the same that is currently FREE. So, do join : [https://www.udemy.com/course/no-code-machine-learning-using-amazon-aws-sagemaker-canvas/?couponCode=AWS\_CANVAS\_DEC1](https://www.udemy.com/course/no-code-machine-learning-using-amazon-aws-sagemaker-canvas/?couponCode=AWS_CANVAS_DEC1)",LearnMachineLearning
rasfxo,1638857653.0,What parts of khan academy statistics should i learn?,"Hey, i jumped directly to this book https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291  but it was a bit too hard so i thought i would learn some python ( i work as a c# dev, so i thought i can figure it out someway ) and some statistics from khan academy. But the thing is there is a lot of content over khan academy https://www.khanacademy.org/math/statistics-probability . So i wanted to ask, what units should i learn, what parts are unrelated or could be skipped ? ( I get that everything in statistics is probably worth learning but i mean like what parts are actually useful in practice )",LearnMachineLearning
raqkuf,1638851027.0,"I want to learn ML on a long offline plane flight, any suggestions?","Hi,

I have a long plane flight of about 12 hours coming up and I've been interested in Machine Learning for a while. I'm a web developer and also familiar with python quite a bit. Unfortunately the airline I'm flying with doesn't even offer internet on the plane. This makes it hard for me to do any relevant work.

I'm not very familiar with the ML environment. I know most people are going to use Google colab/Notebooks for running their ML code. I'm guessing that's not going to be an option without internet.

So I'm planning to just download an entire udemy course on PyTorch or TensorFlow or something and prepare everything so I can churn through the course offline. Is this even a reasonable thing to try or did anyone ever try doing that? How far can I get with ML code offline? Can someone suggest a good course where I can go through the materials offline? What would I have to prepare on my laptop other than Python and relevant libraries

I have a M1 mac and a power outlet so I should be good doing whatever I can without internet.

Appreciate any help.

TLDR: Can I download a full udemy course on PyTorch/TensorFlow and go through it all without any internet?",LearnMachineLearning
raq8ti,1638849941.0,Can anyone help me with this multi-part question?," **(Question 1)** Suppose that you are working for a phone company that wants to predict churn. Let actual be a binary random variable meaning that a customer actually churns, and let flag be a binary random variable meaning that a certain classifier predicts that a customer will churn. Note that the classifier makes yes/no binary predictions.

The base rate for churn is P(actual = 1) = 0.05. Similarly, P(flag = 1) is the fraction of customers that are predicted to churn. Define a = P(actual = 1|flag = 1) and b = P(actual = 0|flag = 0). Your expensive data mining consultant Dr. Zuckerberg claims that he can train a classifier that achieves a = b = 0.9.

(a) Explain in English the meaning of Dr. Zuckerberg’s claim.

(b) Would such a classifier be useful for making decisions, or not?

(c) Write down a confusion matrix and show how to obtain P(actual = 1) and P(flag = 1) in terms of the entries in the confusion matrix.

(d) Show how to obtain a and b in terms of the entries in the confusion matrix.

(e) Give a precise and clear argument that shows that Dr. Zuckerberg is wrong, because no such classifier can possibly exist.

(f) Is it possible that Dr. Zuckerberg has fooled himself by falling victim to leakage?",LearnMachineLearning
ran9w0,1638840662.0,https://youtu.be/kW5hTTuz6Cs,,LearnMachineLearning
ramg5l,1638838093.0,Gradient checking implementation not working (most likely because of algorithm),"I am trying to implement gradient checking in Java (currently used with linear regression), but it doesn't seem to work. I want to check if my algorithm logistically makes sense because after checking via debugging all other methods seem to work, but the actual result is wrong.

I ran a test case to find actual vs predicted values. When running gradient check using x values of 1, 2, 3 and y values of 2, 3, 4 I got -2.84 (calculated this by hand as well, which means the algorithm I am using is wrong) as the approximate gradient for the first weight (theta0 which would be b in ax + b). The result I got from linear regression was -9, which is significantly different. However I am more inclined to believe the linear regression is corrected as linear regression performs fine on the data, but using the approximated values that gradient checking comes up with gives an inaccurate result. While debugging my code I found no errors in the methods used, so I am inclined to believe that it is an error with my algorithm.

The algorithm I am using is essentially the following. I take my weights theta and iterate through them. I add a value epsilon to the ith weight while keeping the rest of them the same. Next I calculate the cost with this new slightly changed weight vector and do the same thing, except subtracting epsilon. I then take the the two values and subtract them and divide by 2 \* epsilon (these are my gradient approximations). I add this to an array and repeat the process for all weights in the weight vector. Afterwards, I calculate the euclidian distance between my gradient approximations and the gradients via gradient descent, and divide that by the sum of the euclidian lengths of the approximation gradients and the true gradients. This is the value I return.

I am posting a link to a gist which has my linear regression class that has the gradient check method (this is the part that isn't working). [https://gist.github.com/arnavkartikeya/fa0f7ea0f3e81eff04a39ca4d1d3919c](https://gist.github.com/arnavkartikeya/fa0f7ea0f3e81eff04a39ca4d1d3919c)

A few more notes:

dApprox is what I am calling my approximated gradient values and dTheta is the real gradient values. trainInputs is a matrix of the training data with a column of 1s attached before it.

trainOutputs
is simply the outputs as a m\*1 matrix where m is the number of data points (in this case it is simply \[\[2\], \[3\], \[4\]\]",LearnMachineLearning
raly1j,1638836504.0,"Can I use the ""Mathematics for machine learning"" book as a one-stop shop for covering my math essentials?","Question mostly is in the title. I am aware that the book omits proofs and isn't as rigorous as a math textbook, and that it doesn't hold **all** relevant material, this is why I am asking if you think it covers the essentials sufficiently to get one going.

I find myself not knowing what I'm looking for when I search for general information and courses on different domains of mathematics. It gets me confused, because I don't have a good enough map of the math. At that point I am trying to determine if there is a single pill I can take, which would suffice.

My full question is; Do you think the topics covered (plus the ""further reading"" portions of each chapter) are sufficient in terms of what they teach and for getting a good map of what math is needed and why? If not, what other sources oriented at ML would you recommend to address the deficiencies?

Thank you",LearnMachineLearning
rakmq4,1638832623.0,Learning how to learn,"So, Im trying to get into machine learning, and so Im trying to learn how to learn.  Ive been programming for 30 years now, so I have a fairly firm grasp of those portions of things.


But, I dont even know how to approach learning how to do things.


For example, I would like to do something like the following:

Identify bikes

Identify bike parts

Identify bike statistics, ie wheel size, height etc.

Identify specific bikes.


So, how do I approach something like that?  I presume thats 3 different tasks (maybe more?).


So, like how many pieces of data do I need to collect?  Is 1000 marked up images of bikes enough to at least start?  I mean, thats what Im going to start with.",LearnMachineLearning
rakeii,1638831986.0,I want to do product recommendations for an online shop,"No idea where to start.

Any model suggestions/what data I need to collect?

This is for my school coursework.",LearnMachineLearning
rahzmf,1638825444.0,How to optimize my gradient boosting model?,"Hi there ML noob here so I'm sorry if it a stupid question but, for a machine learning exercise I needed to train a few models to make a decision on the best model for predicting defaults of bank loans. Of the 5 models I have tested, the gradient boosting model came out on top with an accuracy of 0.66, sensitivity of 0.68 and specificity of 0.60.

No I need to optimize my model even further but am quite at a loss on how to exactly do this. Due to the inbalanced dataset I have decided that sensitivity be my metric of focus. What is the best way of doing hyperparameter tuning to get my model working even  better?

&#x200B;

Thanks in advance for the replies and help.",LearnMachineLearning
ragkis,1638821753.0,Grad probability/Bayesian book recommendations?,"### why do you need prob/Bayes?

I'm a bioinformatician and my field deals with discrete probability quite a bit: motifs/patterns, mutational frequencies, normalizations. We also deal with non-normal distributions often. My understanding is that DL and NN are often the wrong tool for the job and so a broader foundation is needed. I can't just PyTorch/fast.ai my way into success here.

### what are you looking for?

I'm enrolled in a MS in data science and I'm startled by how little probability is emphasized, especially Bayesian probability. **Any book recommendations for newcomers to Bayesian stats, or am I on the right track?** I'd prefer graduate and professional level texts.

### what have you considered?

I've added ""Statistical Rethinking"" (McElreath, CRC), ""[Machine Learning: a Probabilistic Perspective](https://probml.github.io/pml-book/)"" (PML book, Murphy), ""Applied Multivariate Stat. Analysis"" (Johnson & Wichern, Pearson), and of course ""Bayesian Data Analysis"" (Gelman, CRC) to my reading list, based [in part off of this post.](https://www.reddit.com/r/Bayes/comments/4ugjby/what_textbooks_to_read_besides_rubin_and_gelmans/)

### what level are you?

I've completed Sophomore/Junior level probability, stats/biostats, linear algebra, calculus, and multivariate analysis coursework. My uni (Delaware) essentially rehashes undergrad probability in their graduate course, with a little more calc and theory thrown in, without real world applications or calculations emphasized. It's total crap IMO.",LearnMachineLearning
ragd4u,1638821219.0,Where can I find good resources for using NLP techniques when it comes to processing commands and requests from a user?,,LearnMachineLearning
raen75,1638816798.0,Training to produce a text file. (dipping my toes into your world),"I  am looking for some guidance from this community. I have never used  machine learning for anything, but I think I have a use case for it and  am hoping someone could steer me in the right direction.

Background:

Programming  background is mostly in the C and Python languages, plus shell  scripting for glue. C for embedded programming, and python as a general  purpose programming language. I have experience with some of the most  popular dashboard libraries for python.

Problem:

I  have something like 100 examples of configuration files that are mostly  generated based off of the contents of 1 file. If i have 100 example  test cases, each will have a unique {config, source} file pairing.  Currently, all new configuration files are filled out by hand and while I  could partially automate it the way I have in the past (parsing through  the files), I'd like to see if I can learn something new and approach  the problem from a machine learning perspective.

Are there specific types of libraries for training a model to consume and produce simple text files?

Is this an appropriate use case for machine learning?

Is my training sample set too small?

Have I overthought the problem? Would this require more work than it's worth?

Any thoughts or criticism is appreciated!",LearnMachineLearning
radyhw,1638815026.0,Distributed training - Parameter Servers vs MapReduce,I was watching Andrew Ng's [video](https://www.youtube.com/watch?v=TCA2VuHTHcM) on data parallelism where he talks about using MapReduce to update the weights after distributing the training to several GPUs. Is this the same thing as TensorFlow's [parameter server](https://www.tensorflow.org/guide/distributed_training#parameterserverstrategy) approach? The way I understand parameter servers is that we have several GPUs sending their gradients to a server which aggregates them and sends them back for global weight updates. This seems to be the essence of Ng's video but he does not mention a separate parameter server but the GPUs somehow combine the results (see e.g timestamp 12:18).,LearnMachineLearning
radt2v,1638814646.0,How to pick a high level DL API in 2021?,"After years of resisting the push to move on from Keras, I decided it is time to find a new high level Deep Learning API for myself before its too late.

When looking up the best alternatives, Haiku and Flax caught my attention. I found it weird though that Google would have to libraries with such similar approaches. Does anyone has experience working with both? What should one take into consideration when picking one?

In addition to the aforementioned couple, Elegy also looks really interesting.

I must mention though, the reason why I stayed with Keras for so long is that there is just so much material out there for learning. So many good tutorials and examples, past projects... My initial impression is that newer promising frameworks are still lacking on that front.

Maybe I should change level of abstraction and develop everything on PyTorch.

What do you guys think? What frameworks do you currently use for DL?",LearnMachineLearning
radok8,1638814345.0,Conditional GAN for image generation w/ specified attributes?,"Hello!  Am very new to this world so please accept my apologies for misuse of terminology.  Right now I'm just looking for pointers in the general direction of where I'm going (a project to look at, a research paper with sample code, etc.)

I have a large dataset of labeled images of ""widgets"".  Which is to say:  Each image has a number of  known attributes (size, weight, color, model #) that I can specify in a metadata collection.

I would like to train a model on this dataset and then generate new images with different specific attributes which I can control / tune, along with some amount of randomness to make the new images novel.  To use an example w/ faces, I would like to be able to specify the attributes of the generated face (brown eyes, blue hair, male, etc.) but have it still be ""new"".

So far my reading & research has led me to things like StyleGAN2 & 3, which focus on totally-novel images (w/ random seeds), or things like DALL-E which focus on natural language processing or generation from unlabeled training data.  I don't need something that complex (yet).

Any ideas or pointers would be super helpful!  Thank you.",LearnMachineLearning
rade21,1638813624.0,Writing own inference engine with Quantization from float32 to int8?,"Hello, I have a question about quantization. I'm trying to write an inference thing manually. I've already written a working version using float32. It is an image processing network, so the size of intermediate layers is mostly effected by the resolution of the input. I wanted to try to use Int8 instead of Float32 to see how well it worked. In pytorch, when I convert to quantized int, it only converts the weights, not the bias. Intuitively, I think I would need the weight, bias, \_and\_ input data to be in Int8 to run the network with the smaller memory usage. Is this intuition true? If so, how should I convert the bias to int8?",LearnMachineLearning
rab8fe,1638808165.0,Discussion about applications of deeplearning,"I have to questions related to each other ....I have just completed my course in deeplearning without any background about AI or machine learning and I really understand it....so when I made my model is that has any affect ...like do I have not enough informations when I design my models? and the second thing as I told you I have just finished my course can anyone suggest to me a ink or page for a suggestions ideas or projects for the types of models (application idea for ANN,CNN,RNN,Som,autoEncoder & RBM) so I can expand my ideas about real life situation application?",LearnMachineLearning
ra9jmh,1638803781.0,how to apply feature selection if I have mixed inputs (numeric and nominal)?,how to apply feature selection if I have mixed inputs (numeric and nominal)?,LearnMachineLearning
ra89tu,1638800206.0,"I am just starting off my Masters in Data science, I’m trying to get ahead of the game and learn as much as I can. Any book, online instructional, YouTube, or other recommendations?",,LearnMachineLearning
ra70fe,1638796441.0,Bayesian Linear Regression,"I've been tasked with doing some regression on a specific dataset.

I've built a decision tree regressor model, and a random forest regression model.

I can compare the two of these using the R squared value on a test dataset.

I've also computed a third model using Bayesian linear regression?

However, I have no idea how to determine how good a model the Bayesian linear regression computes. As far as I'm aware there's no R squared metric for Bayesian linear regression.

I've sampled from my posterior distributions but am unsure what my next step is essentially.",LearnMachineLearning
ra5m26,1638791822.0,An idea for a project with CNN's,"Hello all,

I am currently looking for some cool project ideas to work on which have CNN's so that i can familiarize with them

The more interesting the project the more i can learn

so pls drop some cool project ideas in the comments below

thanks in advance",LearnMachineLearning
ra5420,1638789852.0,Mixture model of different distributions,"Hi,

If one trains a mixture model using EM, where we have two observed variables which depend on a hidden variable, does this mean that we can multiply the probabilities together while running the EM-algorithm? Im thinking that it should be possible, however I am not sure because looking at the graphical representation of the model, it seems that they are not independent because the path between them is tail to tail with respect to the hidden variable.",LearnMachineLearning
ra4er4,1638787114.0,How to capture words order in a sentence?,"Hi guys, i'm data science student and i'm trying to capture the words order in a sentence for checking if in n triples (subject, predicate and object) this order is respected.

For example, given the phrase ""Rougue is a comedy movie"" and given these 3 triples:

1. \[Rougue, is, movie\]
2. \[Rougue, movie, is\]
3. \[Movie, is, Rogue\]

In this example, only the first triple is correct (for my task).

I guess that in order to achieve my goal I have to vectorize the reference sentence, but I don't understand how to capture the correct order so that only the first triple turns out to be right.

How can i do this? Thanks all.

EDIT

My dataset, for more than 90%, is composed of simple sentences where the order S + V + O is respected, so even the triples that are extracted, as a second check, should follow this order. As a first check, I thought about using the Bag of Words or TF-IDF to check the presence of the words contained in the extracted triple within the reference sentence.

However, it is still not clear to me how to check if the word order within the triple is respected. I know it's a coarse job, however it serves as a basic control for skimming.",LearnMachineLearning
ra1vy3,1638776962.0,artist seeking to learn more about high-res image synthesis,"hey there /r/learnmachinelearning,

i'm a photographer and artist preparing for a small gallery show in february, and i've been experimenting with VQGAN + Clip and Taming Transformers, as well as image scraping tools like flickr-scraper and the likes, in the hopes to achieve generative images based on famous artworks. i'm hoping the kind folks on this sub can provide me with a little guidance.

here's what I want to do:
produce ai-generated, photo-realistic images from source images of well-known photographs

i've tried creating my own image segmentations to feed to Taming Transformers but it appears that it doesn't handle photographs involving people very well, only landscape images.

is there any way to get somewhat photo-realistic images generated by AI?",LearnMachineLearning
r9za5a,1638767375.0,'Scene detection using OpenCV' <-- How can I make this project?,"If for example the 2 scenes which my model can differentiate between are 1) an excercise scene 2) a cooking/kitchen scene.
Also where can I aquire the datasets for these?
Any help will be helpful",LearnMachineLearning
r9yjtq,1638764973.0,Minimum Support,"Hi all,

A basic question from me. When working on a data set, I've been told the minimum support value is 0.3. What would this mean?

Any help is greatly appreciated, thanks!",LearnMachineLearning
r9soc8,1638747239.0,Training accuracy vs test accuracy,"I’m a little confused as to whether or not the difference in the training dataset accuracy compared to the test dataset accuracy shows whether overfitting has occurred or not.

Say I’m training a MLP using cross-validation to stop the learning if the validation set accuracy does not increase by a tolerance value for a set number of epochs.

Intuitively, if I then look at the validation accuracy curve, I can see it increases and then remains constant.

However, when testing the model on training and test data, the training accuracy is a few percentage points higher.

So, my question is does this small difference indicate that some overfitting had occurred or not?

Does a better score in the training dataset compared to the test dataset generally indicate overfitting has occurred or not?",LearnMachineLearning
r9sm6d,1638747068.0,Ml Project Help,"Hi, I am currently taking a Theory of Computation course in my uni and had a question on ML on my assignment. I do not understand any part of it, so if any of you has little bit of time and could help me out would appreciate it.",LearnMachineLearning
r9rcka,1638743538.0,Made a visualization for cost complexity pruning of decision trees,"I made an animation to visualize cost complexity pruning of decision trees. https://et9797.github.io/cost-complexity-pruning/

Hope it can help someone. Appreciate all feedback on style, code, interface etc!",LearnMachineLearning
r9qmhc,1638741619.0,Machine Learning Equivalent of Deep Learning Book (Goodfellow),"I came across Goodfellow et al.'s Deep Learning book, and am wondering if there is a machine learning equivalent to this book. Any suggestions? Thanks in advance!",LearnMachineLearning
r9n6rs,1638732354.0,[BEGINNER] Can someone please give me a breakdown of the math I have to study? (For machine learning and related data science),"Hi, I'm 20 and from South Asia (SL) and I think I've studied up to pre-calculus (very basic graphs, statistics, and matrices). I studied biology, chemistry, and physics due to unavoidable reasons in High School, but my mind was always stuck with IT. I'll be starting my Information Systems BSc degree in a few months.

Now I'm working as a data science intern (some data automation, basic DS, and ML), with a very good mentor guiding me, so the lack of math knowledge isn't affecting me much. However, I do know that I have to understand how certain functions work, and matrices for DL, if I want to be good at what I'm doing. I really love the field and I want to enter the field of deep learning, ofc through DS and then ML.  I'm very unfamiliar with math since I've had virtually no interactions in the past 5 years, but I'm quite comfortable with computer science, especially Python.

**Now,** I know that for a beginner level, I have to study **linear algebra, statistics, probability, and differential calculus.** What I don't know is what sections of those aforementioned fields I have to study. I'm thinking about hiring a private tutor to gain at least **basic knowledge within the next 4 - 5 months.**

**The tutor asked what exactly I wanted to study and I didn't know, hence my request.**

* Could you please give me a structure of the topics I should study (beginner level)? e.g:

&#x200B;

    Linear algebra
        * topic 1
        * topic 2
    Statistics
        * topic 1
        * topic 2

* Is it possible to be familiar with the topics you suggested within the mentioned time frame?
* Could you please suggest some good books for self learning?
* **Your advice, suggested resources (books, sites) are greatly appreciated.**

Thanks a lot for your contribution. Feel free to ask me any questions.",LearnMachineLearning
r9lzpr,1638729184.0,[Project] Advanced Methodology of Robust Optimal Design on the Base of Agent-Oriented Approach by Means of Reinforced Learning,"

I am addressing to you with hope to set the business contacts in the areas of ""Machine Learning Methods in Multi-Objective Problems of Robust Optimal Design and Intellectual Diagnostics of Systems under Uncertainty "".

**The goals of robust optimal design (ROD)** are to achieve maximum efficiency of the system being developed and / or reduction of rejects in the manufacture of a batch of products.

**Algorithm for the numerical solution of the multicriteria (multidisciplinary) ROD problem on the base of agent-oriented approach by means of** **reinforced learning**

1. The values of the confidence intervals for the design parameters of the system (manufacturing accuracy based on the available technological equipment), the confidence intervals of the regime variables (measurement accuracy) for the system prototype are known.

2. A training sample of alternatives (analogs of the designed system) is generated on a set of design parameters and regime variables. Each alternative can include different subsets of values: design parameters, regime variables, objective functions. The values of the objective functions can be obtained by CFD-modeling of processes in analogs of the designed system. We supplement the training set with data on the prototype.

3. Searching of the robust metamodels of processes in the form of analytical dependences of objective functions on their variables using discrete data on analogs and prototype.

4. The choice of the target values of the average values of the objective functions and the corresponding of them confidence intervals for the designed system, assuming that a batch of N systems will be manufactured.

5. Reducing the dimension of the space of design parameters based on the analysis of the informativeness of the metamodel variables (Sensitivity Analyzes).

6. Searching of the solutions of multicriteria optimization problem in a deterministic formulation.

7. Determination of the values of the average values of the objective functions and the corresponding of them confidence intervals for the prototype and the optimal variant, according to paragraph 6, assuming that a batch of N systems will be manufactured (Monte-Carlo Analyzes).

8. Searching of the solutions of multicriteria optimization problem in a stochastic formulation. The results of solving this problem will be rational values of the average values of the design parameters, which need to be found, of the system, as well as the corresponding of them confidence intervals (the required manufacturing accuracy, ensuring the conditions, according to paragraph 4).

Thus, solutions of the direct problem of calculating dimensional design chains can be obtained - to determine the rational values of the average values of the design parameters and control variables of the system, as well as the corresponding confidence intervals of the quantities, which need to be found. According to the definitions accepted in mathematics, the elements of the Pareto set or normal solutions will be obtained for the given target values of the average values of the objective functions and the corresponding of them confidence intervals.

9. CFD modeling of processes in the optimal variant of system, the design parameters and control variables of which are obtained in paragraph 8. Calculation verification - determination of the accuracy of the numerical solutions obtained using the proposed numerical model (paragraphs 3-8) in comparison with the solutions using the original CFD -models.

10. Reinforcement learning. We supplement the training set (point 2) with  the new alternatives based on the results of calculations of processes in the optimal variant of system, the design parameters and control variables of which were obtained in paragraph 8.

11. We continue the process of finding solutions of the multicriteria (multidisciplinary) ROD problem, according paragraphs 2-9 (look Figure), based on agent-oriented approach by means of reinforced learning, until the specified solution accuracy is achieved.

The given statements are reliable, as they had confirmed by the experience of using “ROD & IDS” software in different fields of activity. Look, please, on articles and presentations of our results in the section ""achievements / publications"":

https:// [www.linkedin.com/in/mykhaylo-ugryumov-63148313b/](http://www.linkedin.com/in/mykhaylo-ugryumov-63148313b/)

[https://www.researchgate.net/profile/Mykhaylo-Ugryumov](https://www.researchgate.net/profile/Mykhaylo-Ugryumov)

[https://orcid.org/0000-0003-0902-2735](https://orcid.org/0000-0003-0902-2735)

We have the right to consider ourselves as specialists in the development of effective Machine Learning Methods (MLM’s) for solving “ROD&IDS” problems. We offer you our software in order to resolve specific problems or we can do calculations on our own for you. In addition, we are always open to have contacts and discuss how our methodology, computational methods and IT-realization theirs in form of Interactive Compute Decision Making Support Software System “ROD&IDS” can be helpful for your company. We hope that your use of our MLM’s will increase customer demand for new versions of your software.",LearnMachineLearning
r9kpez,1638725669.0,Can anyone explain me how do i make predictions on a test dataset?,"Hey everyone,

I am fairly new to machine learning so aplogiws for any incorrections.

For a school project I have to implement a logistic regression model to see the probability of a customer to enter a default state. The teacher provided us with a train and a test csv and i need to make a submission in a competion on kaggle with the customer id and with the predictions.

So i developed a notebook performed EDA, selected the features that I wanted ploted confusion matriz and checked performance metrics.

I know have the test data set but I am not sure how am I suppose to procced.

For instance on the train dataset i did drop some features dealt with outliers and missing values performed one hot encoding.

Should i do the same for the test set? I know that i have to use the predict() funtion in python but all i am getting is 1.

I have a list with the colimns that i want to use to make predictios but i am only geting 1 as a result and cant understand why.

Sorry for the long post but any help is really apreciated.",LearnMachineLearning
r9i0a8,1638718037.0,Categorical features in image classification,"I'm training a CNN with tensorflow on a large dataset of images in order to predict a number of classes. There are some obvious distinct groups of images in the training set, which - in a tabular data setting - I would have included in training in the form of a categorical variable/feature. Is there a way to do this with image data?

An obvious workaround would be to train two separate models - one per group (assuming 2 groups). But is that really the best way? I may be having more categorical features soon so having a separate model for all their combinations won't be practical.

Any suggestions?",LearnMachineLearning
r9hir9,1638716594.0,Latent Space Walk with custom StyleGAN2 model," Hi guys,

I   started working with AI a few weeks ago and it's going well so far. I   dont have any coding experience, so it's pretty hard sometimes to   understand what's going on in some GitHub posts or Colab notebooks. But I   managed to train a custom model with styleGAN2 in Colab+.

I   trained the AI with around 6k pictures for 3 days. Now I want to  create  an interpolation video with this custom model but I'm kinda  stuck at  this point. I couldn't find a proper tutorial or an  easy-to-use colab  notebook on how to generate a video. Can anyone  explain to me how I can  generate a video from my cutsom model or has a  functional colab  notebook/guidance ?

I found [this](https://colab.research.google.com/drive/15EGSIv_jZxDvYWODNwEG-I4pV9i3RwlN?usp=sharing#scrollTo=T5w1LOJSY2hB)  notebook which generates an interpolation video of anime faces from a   pretrained model. I believe this notebook could work with any other   custom model but I couldn't manage to edit the code properly.

I grant 1000years of thankfullness for every help <3",LearnMachineLearning
r9f3nh,1638708719.0,Need help with extracting number of objects of different classes using YOLO,"I was trying to extract number of objects that yolov4 detect in a video in a frame into a variable which I can further use for processing but I couldn't find any particular method to do so (I think I will have to save it in a variable before printing it but couldn't find a way to do this.

thanks",LearnMachineLearning
r9cz9r,1638700091.0,Reproducing WebNLG Challenge 2017 on OpenNMT-py,"Hi guys, I'm Data Science student and i'm learning to use OpenNMT-py for my master degree thesis.

I reproduced the challenge with the old deprecated repository, now I would like to replicate it with the updated repository (as I will need it for a similar task within my thesis).

I am now approaching the NLP field, but I am not very clear about some things:

* since it is not a translation task, is it necessary to build a vocabulary like in the machine translation OpenNMT-py tutorial?
* The epochs command I noticed has been deprecated, now it works with train\_steps, however I am not clear about the ""conversion"", so to speak. With the old repository the number of epochs to train the model with was 13. I tried this by looking at old problems from these repositories: default train\_steps (100000) / deault batch\_size (64) \* 13 (epochs number of the old repository) = 20313.

is this reasoning correct? Thanks everyone for your attention.",LearnMachineLearning
r9bwjd,1638695542.0,Getting Better at NN,"I'm a recent CS grad, and took several classes that discussed Neural Nets, and also an internship. I now got a job where one of my tasks is implementing and testing new NN architectures for a task that wasn't solved with ML before.

I know building an NN is a lof of trial and error but I was hoping for guidance on how to continue learning.

Are there any books or online courses that could help me become ""better"" at building new NNs? How to choose an architecture, hyperparameters, tricks like Batch Norm etc. Or just expose myself to new ideas and options to try out.",LearnMachineLearning
r9b2kp,1638692098.0,Public Streaming Datasets,"Hi!

For personal research purposes, I'm looking for a tabular streaming data public datasets, with two or more classes.

Something along these lines:

[https://www.kaggle.com/ealaxi/paysim1](https://www.kaggle.com/ealaxi/paysim1?fbclid=IwAR1zLybVujdRfNrm_AE4pAPtmaYKDcD961DXoxJzOa8MkhcPCRv7yikXWSE)

Would love to hear if you know of something similar

Thanks!",LearnMachineLearning
r97cfd,1638677992.0,Buddies to watch ML lectures with,I am planning to watch some Machine learning lectures beginning today. Would appreciate buddies to keep company as we learn something new together! My first plan is to watch lectures for PCA and SVM followed by others. Feel free to dm me or leave a comment below and We can connect on discord.,LearnMachineLearning
r928oo,1638661043.0,Books / Online Courses / Where to Start with ML,"

All - I'd like to study machine learning: where could I start?

Me: I have a degree in Physics, a bachelor's in Information Technology (both mid 1990s, so 25 years ago), I'm a high school mathematics teacher, and so do like things mathematical. I also enjoy coding - C++ sometimes, JavaScript sometimes, R sometimes, Visual Basic or whatever. They're all pretty much the same at some level.

I'd consider my math good, but currently only to high school level - think calculus, college algebra level etc. Beyond that, I've forgotten what I learned. Like I remember the name Fourier Series, but can't quite remember what they're doing. I remember doing well with Karnaugh maps, but can't really remember from 25 years ago ... and so on.

So: could any fine folks here recommend what to look for for machine-learning and the comp sci / mathematics courses I should investigate? I mean, should I look for a course on linear algebra or some sort of algorithm course... or what what what? I can then find appropriate books / online courses and the like.

Hard to describe more - hope I've provided enough of a push to get the ball rolling!

Thanks muchly.",LearnMachineLearning
r91vk3,1638659946.0,"Why can methods like ReSuMe, Chronotron and SPAN only train single-layer spiking neural networks?","Many papers which discuss them mention that they are primarily suitable for training single-layer spiking neural networks. However, it is not clear to me why this is the case. Is it because they use local learning rules?",LearnMachineLearning
r8zwc5,1638654178.0,What Deep Learning model would work for this kind of String Translation task?,"If I have a dataset which looks something like this.


NLVPMVATV,CASSPVTGGIYGYTF

GLCTLVAML,CSARDGTGNGYTF

NLVPMVATV,CASRPDGRETQYF

NLVPMVATV,CASSETGFGNQPQHF

NLVPMVATV,CASSLAPGATNEKLFF


Where the thing before the comma is the input and the thing after is output. What would be good DL model to learn this kind of structure?

The Data is always in capital letters and doesnt have any special characters I think a character level model would made sense here. But I really have no idea what to start with.

Does anyone have any experience with similar task of string translation? Anything would be helpful.

Thanks",LearnMachineLearning
r8w4s7,1638643491.0,Feeling weirdly motivated and demotivated at the same time.,"I am taking a class with all beginners in ML. We got a dataset ([https://raw.githubusercontent.com/jrnkng/datasetssss/main/dt\_car\_train\_2021.csv](https://raw.githubusercontent.com/jrnkng/datasetssss/main/dt_car_train_2021.csv)) and out of 200 students I am one of the lowest even though I tried pretty hard.

I have ran a full SearchGridCV for a Random Forest Tree and can't get it above 68% even though more than 60% of the students have an accuracy over 70%.  I have also tried the same for  XGBoosting, same story... stuck at 67%.

Due to COVID, the course started offline and I don't really know anyone so I can't ask them. It's the middle of the weekend and it's killing me. The dataset is pretty clean already but I removed the features of low importance, that gave a small boost but didn't help much.

Somehow it demotivates me but at the same time I feel obsessed trying to get to that 70% and learn more about the algorithms. Has anybody ever been in the same spot and has some tips for when you  feel like this?",LearnMachineLearning
r8w3zc,1638643431.0,How does a neural network pipeline look ?,"Hey, in my third year of CS ug and decided to learn ml and neural networks and make cool projects as it will also help with my cv. I started with classical ml algorithms like linear regression, logistic reg and other classifying algorithms. I learnt the math behind them which I enjoyed like how gradient descent works, why scale your features, etc. and can implement them using sklearn
, no problemo.

 but I absolutely hate the feature engineering part, I find it very mundane and boring maybe because I don't know where to start or becuz I suck with plotting graphs. I'm not sure, i tried to do a kaggle competition on titanic and got an accuracy of about 80%, mostly becuz i did not chose the features properly.

So given I don't have much time, I signed up for the Andrew ng's  deeplearning.ai first course and I quiet like it, I find it really cool how matrices that I learnt in highschool are put to use to do stuff efficiently, among other things. Also, I maybe wrong but I find it more straightforward than machine learning where you have all these models to chose from and here you have only one thing -NN.

So my question, is does working with NN need good feature extraction like ML ? For example, image detection or stock market prediction if im talking about harder projects. I feel like it is important since feeding my model with redundant info may slow the training process or even result  even worse accuracy. Am I correct ?

If there is no way out of the feature extraction hell how do I become decent at it ? Also it'll be helpful of you guys can give a top view of a NN pipe line

Edit - any constructive criticism on my naivete is appreciated as long as it steers me the right path",LearnMachineLearning
r8vicl,1638641749.0,PCA Explained from Basic Principles.,"In this Blog Post, I have arrived at the optimisation problem of PCA from using some basic principles of mathematics. Click the [link](https://therickysen.medium.com/understanding-principle-component-analysis-pca-from-scratch-db7ceda623eb) to read.",LearnMachineLearning
r8v6rk,1638640872.0,Career change into Programming-Data at 30 y/o – Options in Europe,"

Hi everyone,

before starting, just a little bit of background about myself:

* I am 30 years old, French and therefore a citizen of the EU.
* I already have a Master's degree in engineering - in chemistry/materials/polymer
* I have been working for 6 years as a process engineer in the automotive industry: 2 years in France, 2 years in Slovakia, 2 years in Germany.
* I have ended my job in Germany, but I am currently learning part-time German in a school there to pass B2 (= I have a lot of free time, 6-7 hours / day)
* I also speak English (if necessary I can consider passing my C1 certificate), and French.

So now, after many months of reflexion, I would like to change my career. The field in which I can work (mainly related to my studies), does not suit me.

I have a great attraction for programming and computer science, and even if I only have a very basic knowledge about it, I would like to change into this field.

I'm not totally sure yet, but I would like to do (and am therefore targeting) one of the following jobs: Data Analyst or Data Scientist and/or probably later on a job related to Machine Learning/AI (I'm just discovering it, but I am getting super excited about it).

For info, I have been learning Python/Data-science by myself for a couple of months now.

***So finally, here are my questions:***

\- What are the best learning options for me **to be employable**? Or to have **certified or company-recognised knowledge**? In order to find a job in Data science (Data Analyst for example to start)

\- And which ones would you recommend? (I am looking to study in **Europe**, **online** if possible (or in person in Germany), to find a job in Germany/Switzerland/Austria for example). It can be fast, or long, free or not. I have some time and money to invest now, but I don't want to if I realise it is not necessary)

Different options I found so far:

* Self-learning + a couple of Portfolio-projects: Free, but difficult to prove competences, no certifications and no supervision
* Bootcamps (online): Expensive, but quite fast and intense. However I'm not sure if this is recognized by companies, and if it will really help me find a job.
* Master Degree: Do you think I have a chance to find a master **Online** in **Europe**, starting 2022 ? Knowing that I don’t have any educational background in CS.
* Master conversion course: I have heard about it, maybe it is only in UK? I am not totally sure about how it works. But knowing that I have already a Master degree, I probably don’t have to start from the beginning?
* Bachelor: Or should I start first with a Bachelor? Is a Bachelor enough? Online would be better (any city or country in Europe)
* Or are there other schools, which are not Bootcamps, not Universities, but something between, that are recognized or certified and might help me a lot?

If you have any ideas or information that could help me, I would be very grateful. I am motivated and I can invest a lot in this project, but I am not totally sure what would be the best option at the moment, or what qualification is really necessary for this job.

Thanks in advance !",LearnMachineLearning
r8v65d,1638640830.0,[Career Advice] How can I best prepare myself to get a purely R&D Data Science position?,"I am currently in the last semester of my Master's, and I was hoping to get a taste of industry for a couple years before pursing a PhD (or perhaps to pursue it part time). Either way, my goal is to secure a Data Science position focused on performing research and with the potential to publish papers, not just deploy and maintain models/big data.

I have an offer to work as an Associate Data Scientist at a local startup, but they made it clear I would be spending half my time doing development and the other half R&D. The Data Science team is just two people and is just starting out, so I'm somewhat concerned that I won't have much opportunity to expand my ML skill set there.

However, I also have a good chance of securing a full time research intern position through a government organization that my research advisor is involved with, but the position is only for six months. Even so, another opportunity could arise after this, or I'll have an extra six months to hunt for jobs.

I'm wondering a couple things:

1. Is a research intern position through a government org considered 'industry experience' in regards to hiring managers? I'd like to not be restricted to applying to entry level positions.

2. Would it be better to obtain industry experience at the start up, even if there's a chance I mostly won't be doing R&D, or to get more experience doing full-time research, even if it's temporary?

3. Is it plausible to secure a position doing 100% R&D without a PhD? Which of these options would be a better stepping stone towards that?",LearnMachineLearning
r8uwp1,1638640101.0,NLP: Should you preprocess your text before running your deep learning model?,"Is it recommended to preprocess the text before training? by preprocessing I mean things like lemmatization, stemming etc.

Intuitively, I think the first answer is no as any preprocessing might remove some information from the original data or it can introduce its own noise.

Would the answer be the same for a Kaggle competition? (where the processing time and recourses are limited)

&#x200B;

Thanks",LearnMachineLearning
r8tpdc,1638636673.0,Best way to label video for object detection,"Hello, i want to label some videos that contains some small fish in order to detect and track them. I am looking for a way to turn these videos into a dataset so i can train yolo to detect them . Can you please suggest any free tool that i can use. ( besides label studio somehow it doesn't work for my videos or maybe i just don't know how to use it well)",LearnMachineLearning
r8tnzq,1638636567.0,Batches in Sequential Iteration of Sequential Data,"Hello all, I have been studying RNNs from the d2l.ai book. In the part where they describe how we are reading the data sequentially, there is a choice they have made and I have been wondering if there is any reason for this.


For example, let's assume our sequential data is as follows:



    my_seq = [1, 2, ..., 32, 33, 34]

When they iterate through the data, they get the following values for the first two batches:



    X:  tensor([[ 0,  1,  2,  3,  4], [17, 18, 19, 20, 21]]) # First Batch
    X:  tensor([[ 5,  6,  7,  8,  9], [22, 23, 24, 25, 26]]) # Second Batch

My question is, is there any reason that we are not iterating through the batches as:


    X:  tensor([[ 0,  1,  2,  3,  4], [ 5,  6,  7,  8,  9]])  # First Batch
    X:  tensor([[10, 11, 12, 13, 14], [15, 16, 17, 18, 19]])  # Second Batch
If you want to read more about the chapter I am talking about, you can check the [corresponding chapter](https://d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html). Thank you.",LearnMachineLearning
r8tdrm,1638635760.0,A logo detection NN outperforms by almost 2X a logo detection NN which is only one year older. What am I missing?,"We have these two NN architectures:

* Logo-Yolo (2020) - [paper](https://arxiv.org/pdf/2008.05359.pdf) \- [benchmarks](https://i.imgur.com/FcDVPwc.png)
* OSF-Logo (2021) - [paper](https://www.sciencedirect.com/science/article/abs/pii/S0141938221000962) \- [benchmarks](https://i.imgur.com/ZJJe31f.png)

Both NNs were tested on the LogoDet-3K-1000 dataset, but even though the papers are only one year apart, the mAP scores from the new paper are almost double the ones from the older paper.

Take for instance `Cascade R-CNN`. It has an mAP of **48.14** on the older paper and **89.1** on the newer paper. Yes, `Cascade R-CNN` seems to have different backbones on each paper, but can it make such a big difference? Also, ResNet-101 has 101 layers, while R-50-FPN (I assume) has 50 layers. Is it even realistic that in one year you get a backbone architecture with way fewer layers and it's way more performant than the previous one?

So which model is state-of-the-art in logo detection? Thank you! :)",LearnMachineLearning
r8r8qr,1638629568.0,"Is there anywhere a offline running text to speech, that works on linux and has a german male voice?","Every single text to speech I find is either googles shitty gtts which requires internet and only has 1 female voice for german, or pyttsx3. Which works fine on Windows, with a german male voice, but not on Linux. The only german voice is a horrible robotic sounding 1960 voice.

Unless there is a way to get the Windows voices to Linux to use with pyttsx3, is there ANYTHING else??",LearnMachineLearning
r8r78q,1638629441.0,Alternative for Isolation Forest,"There is requirement for finding the anomalies.. In detail, we have a two files one from one cloud platform and other from aso we need to find the anomalies for communication between the tenants access to vm.. ( sorry to give very vague information I was asked to just give names of algorithm) but the currently used algorithm is isolation forest.. Just wanted to know is there any alternative or such.. Or even any other similar algo for finding anomalies for a comparative study..",LearnMachineLearning
r8r78p,1638629441.0,Alternative for Isolation Forest,"There is requirement for finding the anomalies.. In detail, we have a two files one from one cloud platform and other from aso we need to find the anomalies for communication between the tenants access to vm.. ( sorry to give very vague information I was asked to just give names of algorithm) but the currently used algorithm is isolation forest.. Just wanted to know is there any alternative or such.. Or even any other similar algo for finding anomalies for a comparative study..",LearnMachineLearning
r8pr04,1638624756.0,picking a kernel for SVM?,"I am working with higher dimensional data and and only need to differentiate two classes with SVM

if I do PCA the training data for the two classes separate very cleanly, so cleanly that I can just ""manually"" draw a vertical line though the x axis of the PC plot and it does a great job of separating the classes.

It seem like each tutorial page I look at, people use slightly different parameter dictionaries for grid search (GridSearchCV from scikit learn in my case). I am kind of wondering how much of an ""art"" this step is? Like if I can intuitively see linear kernel is probably the right choice should I always still iterate through rbf, poly?",LearnMachineLearning
r8n8qx,1638615193.0,any suggestion for best use of datacamp as I have just started using datacamp,,LearnMachineLearning
r8n2dr,1638614403.0,How to go about exercises in ML textbooks?,"Hello, I am trying to self-study ML theory using the text ‘Understanding machine learning: from theory to algorithms.’

I can follow the content in the book pretty well but I’m having some difficulties solving the exercises. So I was hoping to get some advice on:

1. How long you should work on a problem before consulting the solution manual (if at all)

2. How to decide which questions to work on

3. What to do if you are stuck

My short-term goal is to get a deep enough understanding of ML to be able to contribute to research as an undergrad.

Any advice would be very helpful. Thanks!",LearnMachineLearning
r8ly8r,1638609587.0,taking Coursera NLP Specialization but i don't understand the lecture...,"i'm halfway through week 1's material but doesn't really understand those equations...and i don't think i'm able to write those python(they're quite long...)

i then realised the specialisation is intermediate level. i read reviews and many said the class is too easy and not deep enough but i think the opposite...

i don't know anything abt NLP. i only have taken python for everybody course(by dr. chuck) and learned some python. is there other NLP course that i should try instead? or should i try Andrew Ng's machine learning course first?? i'm planning to apply for NLP/language technology master next spring semester...

\*\*thanks so much for taking your time to answer!!! i'm going to do more pyhton and ML courses  first before diving into NLP!",LearnMachineLearning
r8lq9b,1638608610.0,Need to learn Image recognition for an interior design project,"Hi everyone, I'm a DevOps engineer who has studied CS in college, worked on Linux for more than 10 years and know Python. Obviously I'm not a developer but I know how to code and have the basis for any IT-related topic. For an idea, I need to learn image recognition. I need to be able to write a program that can identify the surfaces and (not now, but in close future) measure the dimensions of a room. The application should be able to identify the floor, walls, ceiling, big objects (door, windows, table, softa, etc.) and later on, be able to interact wtih the texture (change the color, the surface type, etc.) using a phone or a VR headset.

I've checked out youtube, did some research online, etc. Up to now I've found out OpenCV and Tensorflow have libraries that can get the job done, but I want to start smart. I want to follow a route and use tools that can certainly takes me to the right track, even if the project turns out into a big one, creating a full-feature app. What are your recommendations fellow redditors? Which technology to use and where to start and what to look for? And how much time and effort is needed for someone like me to build a simple app like that?",LearnMachineLearning
r8kxdc,1638605200.0,Is creating your own dataset for a popular ML problem is good choice?,"I'm want to build a project for MGR (Music Genre Recognition). I was studying some papers on the famous GTZAN dataset and it mentions there are some flaws in the data( I myself have listened to some music samples from the dataset and found some repetition in the samples/artists).

Here are my inferences:

1. Some samples are from the same song.
2. In some genres, data is disproportionately sampled (taking samples from single artist)
3. Only 100 songs are in a given Genre (total 10 genres)
4. Not all ""super genres"" are included. I'm planning to follow [MusicMap](https://musicmap.info) for major genre classification.
5. One song is labelled under single genre only. Micheal Jackson's songs can be labelled under both R&B and Hip-hop.



There is another dataset which is used as alternative for GTZAN -- FMA (Free Music Archive) -- haven't studied about that yet. Data size is certainly larger (~100000) than GTZAN(~1000). FMA also doesn't include all super genres.

As a given musical piece can come under many different genres, I'm thinking of training NN which will give percentage of super genres present in the song.

I think I can use the dataset from FMA and add sample from the genres absent in the data. Is this a good approach?",LearnMachineLearning
r8jea7,1638598965.0,"What specific ML technique should I use for predicting a country's religion through the attributes of its flag (colors, stripes, crosses, etc)?","It's my 2nd year in the Computer Science field and Machine Learning already broke my brain just trying to learn it. Don't get me wrong, the concept of Machine Learning is so cool and that's why I gambled myself into learning it and making something out of bit.

We are required to create any system that can predict something out of a dataset. My idea was to have a dataset of countries with its flag and its attributes (still trying find it somewhere online or I'll create it myself) and predict the country's religion (either nationally recognized or practice by the largest population).

However, I can't seem to think of a technique that would help this project. I'm thinking of Neural Networks but it might be a bit difficult technique to start but I won't mind doing it as long as it is the best option. That's why I'm here asking the humble but powerful Redditors to guide me :)

Peace!",LearnMachineLearning
r8geii,1638588362.0,[D] (Paper Overview) NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion,"**Video**

[**https://youtu.be/C9CTnZJ9ZE0**](https://youtu.be/C9CTnZJ9ZE0)

**Paper**

[**https://arxiv.org/abs/2111.12417**](https://arxiv.org/abs/2111.12417)

**Code**

[**https://github.com/microsoft/NUWA**](https://github.com/microsoft/NUWA)

**Abstract**

This paper presents a unified multimodal pre-trained model called NÜWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate NÜWA on 8 downstream tasks. Compared to several strong baselines, NÜWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks.",LearnMachineLearning
r8do0d,1638579499.0,CUDA cuDNN docker container from NVIDIA - license/distribution question,"I am being told that the license does not allow a person to use the cuda cudnn devel docker image as a base to then build OpenCV with cuDNN install a python program and distribute it to people to use?

The License reads as ->

1. LICENSE. Subject to the terms of this license, NVIDIA hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this license) to:
2. a. Install and use copies of the CONTAINER, and modify and create derivative works of samples or example source code delivered in the CONTAINER (if applicable), to develop and test services and applications,
3. b. Deploy the CONTAINER on infrastructure you own or lease to offer a service to third parties, without distributing the CONTAINER or exposing the NVIDIA APIs in the CONTAINER directly to such service users, and
4. c. Develop and extend the CONTAINER to create a Compatible (as defined below) derived CONTAINER that includes the entire CONTAINER plus other software with primary functionality, to develop and compile applications, and distribute such derived CONTAINER to run applications, subject to the distribution requirements indicated in this license. As used in this section, “Compatible” means that extensions to the CONTAINER must not adversely affect the functionality of the other components in the CONTAINER.

&#x200B;

Is the pertinent part the non-exclusive, non-transferable? Then what is the wording of extend and distribute? From searching it seems if I build my own image using debian with cuda and cuDNN and then distribute it, thats ok, but using the nvidia cuda cudnn container as a base isn't?

&#x200B;

Thanks for helping to clear up my confusion.",LearnMachineLearning
r8d36l,1638577659.0,Can i split a training set?,"Hi all!

I need a little help with one of my homeworks. So I have a training data set and a testing data set. Two separate data sets. Is it possible for me to split the training data set 70/30 and then use the testing datatset to actually test the model? Is there anything wrong with this? I believe the 30% of the data would be a validation data set, and im not sure if there is anything wrong with doing this.

thanks!",LearnMachineLearning
r8bbsb,1638572411.0,Requesting help for my situation,"Hello, I am an undergraduate researcher in my senior year of my computer science and engineering degree. The research is focused mainly on computer vision. I am currently working with pose estimation, image segmentation and object tracking. I have been working on this research for about 3 months. I have had enough machine learning background to complete my tasks so far, but the last 3 months have made me realize I have some gaps in my knowledge.

About 2 years ago, I completed a deep learning course on Udacity using Pytorch, it went through all of the basics for machine learning including a lot of the essential mathematics, however I didn't apply what I learned for awhile after the course was over and I'm a bit rusty on some of the details. Additionally, I took an autonomous vehicles course at my university which included some basic introductions to computer vision. I have also learned a lot over the past 3 months. Basically, I'm comfortable doing things like utilizing existing models to solve a specific problem and tuning models hyperparameters/structure to boost performance, however the mathematics and theory behind what I'm doing is fuzzy in my memory, and I would really like to rebuild my understanding from the ground up.

Next semester, I will be required to do some more advanced work (using computer vision techniques to either fine-tune an existing animal behavioral analysis network or finish building one that has already been started) that will most likely benefit from some additional learning on my part, and I would like to use my winter break (a little over 3 weeks) to do it.

My question is, how can I best utilize this time? I know 3 weeks isn't a ton of time, but I want to be as efficient as possible with the time I have. I know the Coursera Stanford Machine Learning course is highly recommended, however I am concerned it is too broad/introductory for my case as it covers a decent amount of topics I am already comfortable with, on the other hand, there is a good chunk of information in that course that will be great to fill in the gaps in my knowledge. Would it be better to do a more advanced course that focuses on computer vision specifically, even though some of my fundamentals are missing/rusty? Thanks!",LearnMachineLearning
r86c1r,1638557854.0,Resources to explain how to build a neural network from scratch,Need some resources to assist in building a neural network from scratch.,LearnMachineLearning
r85ozs,1638556109.0,Your recommended sub to learn ML?,"First time opining this sub, I see that it is 65% memes and 35% showing what can be achieved with ML. No question posts, no links to useful articles/books/courses, no showing off *your own* results.

With this in mind, what is your preferred sub to actually learn ML?",LearnMachineLearning
r84reh,1638553676.0,Common uses of ML?," Hello,

I'm trying to recap all the very common/main real-world usage of ML (things that a company would need or find helpful). Here is the list of what I have for the moment:

\- time-series prediction

\- classification (image, text)

\- pattern recognition

\- text summarization

\- anomaly detection

\- voice recognition

\- text/keyword extraction

\- chatbot

Do you have any other ideas of what can I add to that? I want to see other's perspectives + I'm sure I missed some

Thanks for consideration",LearnMachineLearning
r84o2r,1638553440.0,"Qualitatively speaking, training loss seems to decrease strongly during the beginning of training, but slows down over time. Is there a quantitative study (or if I'm lucky, proof) of this qualitative observation?",,LearnMachineLearning
r81wi5,1638546103.0,Where can I find a good overview on which AI techiques to use on which kind of data and how to combine different techniques into pipelines?,"I have taken some AI courses in my university and while I was able to grasp the concepts, I am struggeling to make choices on what to use when. Especially, when it comes to creating my own pipelines this lack of orientation really gets in the way, as I don't know what goes well together and what should rather be avoided.

I realize, there often is no easy solution to a specific use case and you always need to fiddle around, but some kind of overview on where to start and what to try first with certain kinds of data would be a great help. I am not asking for a blueprint, but merely for a rough orientation to tackle new projects.",LearnMachineLearning
r7x490,1638530934.0,Is it always necessary to retrain a model on an entire dataset?,"To further elaborate on this question I will give an example. Let's say we have a dataset consisting of 1000 datapoints. We have trained a model on this data. We deploy it and it starts making predictions. Every week however, we gather more and more data (lets say 250 new datapoints every week). To improve our model we want the model to learn from these new points.

Is it necessary to retrain the model on these 1250 datapoints or is there someway to 'keep' its original learning and only have it additionally learn from these 250 points?

I can imagine that for a smaller dataset this does not really matter but once you get to much bigger datasets this can be a very costly process to have to retrain a model on all the data.",LearnMachineLearning
r7wdou,1638528047.0,I need help understanding the meaning of the loss Values of WGAN with Gradient Penalty,"Hey Guys,

I am currently working on training a Auxiliary Classifier Wasserstein GAN with Gradient Penalty. I based my implementation off of https://keras.io/examples/generative/wgan_gp/ (to which I added the Auxiliary Classifier functionality).

I have a model trained off of quite a few epochs of emoji images, and I am now trying to filter out bad (or vice versa, good) samples from the set of generated examples. I understand that the best way to automatically do this is to utilize the trained discriminator and its loss values to evaluate the ""fakeness"" of the images in order to be able to select ones that are able to fool the discriminator.

Now, I understand, that the Discriminator, using the Wasserstein Distance attempts to separate the loss values of fake samples as much as possible from the ones of real samples. Given that my Discriminator output for a randomly chosen set of generated images looks something like the below table, I think that the images with maximum or minimum values should be the highest and lowest graded examples.

    -22.96732
    -12.37780
    -23.39248
    -44.45711
    -14.15668
    -11.19169
    -35.99777
    -9.65943
    -16.71531
    -9.35125
    -25.98240
    -4.36232
    -8.58446
    -24.78805
    -7.47653
    -19.14746
    -28.33695
    -30.18404
    2.67499
    -8.63077

This should mean that my most ""fake"" and most ""real"" generated image are the samples with -44.4 ... or 2.67... , but neither of these outliers are particularly more ""real"" looking than the other and both look much worse than randomly chosen examples (see here: https://imgur.com/a/GVQCJlL )

How should I interpret these loss values? Would it make sense to go for a median loss value to get more of the good looking ""average loss"" images? If so, why? Would it make sense to run a 2-means clustering algorithm on the losses to try and separate realistic and fake samples?

Anyways, thanks in advance for your help",LearnMachineLearning
r7vnzk,1638525167.0,Can I say that my model is doing nothing if RMSE ~ SD of the validation set?,"I am training a univariate regression model using neural network.

The training set has SD of 0.6.

When I train my model, the val\_rmse get around 0.6 also.

As I understand, both RMSE and SD have the same unit and similar formula.

So if SD \~ RMSE, can I say that my model is just predicting the mean value of the dataset (which is basically useless)

And if RMSE > SD, can I say that my model is a poor predictor?",LearnMachineLearning
r7uqrp,1638521391.0,Sentiment Analysis API vs Custom Text Classification: Which one to choose? - KDnuggets,[https://www.reddit.com/r/edenai/comments/r7ukit/sentiment\_analysis\_api\_vs\_custom\_text/](https://www.reddit.com/r/edenai/comments/r7ukit/sentiment_analysis_api_vs_custom_text/),LearnMachineLearning
r7seqs,1638512360.0,Python Interview Questions,"Greetings.
I made an Android app called ""Python Interview Questions"". It is intended for Python software developers over the world. It is helpful not only for job interview situations, but also for refreshing many aspects of Python programming language during normal working schedule.
It provides 140+  Python questions with answers and code examples.
The knowledge is divided by 8 categories, including Data types, Operators, Classes and OOP, NumPy, Pandas, and more.
There is also a ""Random questions"" game - try it to test your knowledge!
[https://play.google.com/store/apps/details?id=eu.ydns.chernish2.python\_free&referrer=utm\_source%3Dreddit%26utm\_medium%3Dlearnmachinelearning](https://play.google.com/store/apps/details?id=eu.ydns.chernish2.python_free&referrer=utm_source%3Dreddit%26utm_medium%3Dlearnmachinelearning&authuser=0)",LearnMachineLearning
r7o01i,1638498540.0,Swimming thoughts,"Wouldn't it be cool to evolve/train deep sea creatures such as

https://youtu.be/ojTAxYheSns?t=681

https://youtu.be/ojTAxYheSns?t=1619

https://youtu.be/ojTAxYheSns?t=2348

Literature search:

OpenAI Gym: Swimmer-v2

https://gym.openai.com/envs/Swimmer-v2/

""train the three-link fish to swim to a direction""

https://github.com/mjysh/RL3linkFish

Fish schooling RL in a Navier-Stokes environment:

https://www.pnas.org/content/115/23/5849

Swimming at microscale with RL, nice model details:

https://arxiv.org/pdf/1808.07639.pdf",LearnMachineLearning
r7n8k8,1638496263.0,Recommendations for creating a NLP text harmonization algorithm,"Hello, looking for recommendations & tips on how to solve this issue. Hopefully, it's allowed here, this same question was deleted in StackOverflow and I don't know where to ask for help.

&#x200B;

Let's say I have this set:

**Company:** KPMG USA, KPMG Europe, Microsoft Corporation, Microsoft, DHL GLOBAL FORWARDING, DHL EXPRESS

But I want to harmonize company names, to get a result like the following:

&#x200B;

|Company|Company|Harmonized Company|
|:-|:-|:-|
|KPMG USA|KPMG EUROPE|KPMG|
|MICROSOFT|Microsoft Corporation|Microsoft Corporation|
|DHL GLOBAL FORWARDING|DHL EXPRESS|DHL|

&#x200B;

This problem can be segmented into two core problems:

1. Matching (Fuzzy Matching)
2. Harmonization (Providing the cleanest and most representative name)

For the matching part, I know there are several pyhton packages to fuzzy match through tokenization algorithms, such as python record linkage, among others.

For the Harmonization part, I have no clue what is available. Seeking recommendation for this part.

Additionally, I am looking to implement a form of supervised NLP algorith, so that it can learn with time, how to output the best harmonized name and how to match correctly. Seeking recommendations for this part as well.

I already have a large dataset with around (1M rows and I can get more if needed), with examples like KPMG USA -> KPMG.",LearnMachineLearning
r7mekf,1638493714.0,"IN a class, lost and confused.","Greetings all!

&#x200B;

 Let’s get the disclaimers out of the way first.

Yes. This is a graded project that makes up a significant part of my class grade.

No. I am not looking for y’all to do my homework.

&#x200B;

Y’all I need help. I am so lost I do not even know the correct question to ask. I am in a machine learning college class. Pre-requisites for the class are Linear Algebra, Probabilities (Stats 2), and Calc 3. I’ve also got CompSci classes under my belt. I passed all of those, with good grades. For the past 12 weeks, this class has been theory lecture and homework to show you can calculate the algorithm, but there has been no application or functional use. There is a textbook that we have not cracked open (mine still has the unscratched access code inside the font cover.) We now have a project to do. It has been discussed for the semester, with the last few weeks for ‘research’ and only -yesterday- did we get the rubric for the presentation in under 10 days. I am not the only one in the class who is lost.

I said I would do a presentation on financial forecasting on a data set I pulled from a public source of historical data. For ease of discussion, I’ll call it a 50x50 matrix – 50 numeric observations (rows) with 50 categories (col); category averages wildly vary from each other. So the first thing I do is normalize them (1/|| col|| \* col) and drop into a new matrix. Ok. Great. I also know I need to break it up into training, verification, and active (use? testing? ) picked 70/15/15. So I lop off the first 35 normalized rows. I am doing this all in MatLab as prescribed by the course.

Now what? Instructor has said “Oh, that will be perfect for Support Vector Machine.” And then a a couple days ago gave me a stack of finance time series journal article.

&#x200B;

What the frack do I do? I don’t know where to go from here. The instructor I believe could help me if I had any idea what the question was to ask. I can at least get either the classification app or the ‘Econometric Modeler” from the computational finance app section open that the data loaded and I don’t understand anything of the outputs.  I just .. I am completely lost.",LearnMachineLearning
r7lq0u,1638491697.0,Resources for practicing advanced maths for data science/machine learning,"I am familiar with the basics. I can solve easy ones but when I encounter difficult questions (recently faced some in a job screening test), I get stumped. I don't want resources to learn the basics, I want resources where I can try to solve and see the solutions to difficult maths questions (preferably in the context of data science/machine learning). I tried googling but only found entry-level courses.",LearnMachineLearning
r7ki6k,1638488184.0,"How to fix ""Multioutput target data is not supported with label binarization"" error","Hello,

I'm currently trying to figure out how to use scikit-learn and when I try to use an MLPClassifier, it spits out this error: ""Multioutput target data is not supported with label binarization"". I'm not really sure what it means and the people who have asked about it on stackoverflow seem to get it for different reasons. My X\_train is an ndarray where all the columns are float values (except the id column, which are simply integers). My y\_train is also an ndarray with integers in both columns (id, class). The classes are from ranges 1 to 10. I would really appreciate any pointers about this.",LearnMachineLearning
r7hd01,1638479279.0,From Zero to Research on Deep Learning Vision: in-depth courses + google colab tutorials + Anki cards,"Hey, I'm Arthur a final year PhD student at Sorbonne in France.

I'm teaching for graduate students Computer Vision with Deep Learning, and I've made all my courses available for free on my website:

[https://arthurdouillard.com/deepcourse](https://arthurdouillard.com/deepcourse/)

&#x200B;

[Tree of the Deep Learning course, yellow rectangles are course, orange rectangles are colab, and circles are anki cards.](https://preview.redd.it/t7c4b2fr27381.jpg?width=944&format=pjpg&auto=webp&s=7bb914a55959f961309173333fb849ad82a1fe7a)

We start from the basics, what is a neuron, how to do a forward & backward pass, and gradually step up to cover the majority of computer vision done by deep learning.

In each course, you have extensive slides, a lot of resources to read, google colab tutorials (with answers hidden so you'll never be stuck!), and to finish [Anki cards](https://ankiweb.net/) to do spaced-repetition and not to forget what you've learned :)

The course is very up-to-date, you'll even learn about research papers published this November! But there also a lot of information about the good old models.

Tell me if you liked, and don't hesitate to give me feedback to improve it!

Happy learning,


EDIT: thanks kind strangers for the rewards, and all of you for your nice comments, it'll motivate me to record my lectures :)",LearnMachineLearning
r7f2wb,1638473068.0,I built a pill identifier using Edge Impulse,"I wanted a way to classify images on a phone with or without using the Internet. So I built a project in Edge Impulse and used a deep learning CNN image classification model that was deployed to a phone.

Simply stated, my mom mixes up her pills when she takes them out of the pill containers and she doesn't always have good internet connection.

The model is not perfect but I think it shows a useful ML project (versus just a cool, trendy one that no-one will actually use in the real world). I wrote-up the steps I took here: [https://www.hackster.io/sara-kubik/edge-impulse-pill-identifier-7c1219](https://www.hackster.io/sara-kubik/edge-impulse-pill-identifier-7c1219) and my Edge Impulse project is public and available here: [https://studio.edgeimpulse.com/studio/64036/](https://studio.edgeimpulse.com/studio/64036/) (fyi, Edge Impulse is free for devs).",LearnMachineLearning
r7ejz8,1638471680.0,Machine Learning maths,"Good evening!


I am very new to machine learning and I have a question, which for some reason is non-trivial to me.

I know that we are not likely not operate with the data, which consists of small amount of observarions and a great amount of variables. However, in such cases as recommendation systems, computer vision and e t c, we just cannot avoid it.

So, how do the ml models face with the case described above and why some models are better than others in these terms?

Thank you.",LearnMachineLearning
r7e49h,1638470513.0,(Recommendation) Just getting started in studying ML and general AI. Need podcasts for work,"Hello ether

I am starting my first college level courses in ML next semester and wanted to get a head start, so I started taking udamy courses when off work. What I want to do though while I’m working is listen to a podcasts so I can maximize all my time. So my question is:

1. Does anyone know a good podcast(s) that goes from A to Z in regards to learning ML/AI/ all other subjects related to the matter? Starting from scratch here but have a general understanding of technology

2. Is there a podcast(s) that I should follow to keep up to-date in the field or see what’s coming down the pipeline?

Thank you all for your help and be safe in your travels",LearnMachineLearning
r7dbuf,1638468386.0,Which laptop is better for machine learning?,"

[View Poll](https://www.reddit.com/poll/r7dbuf)",LearnMachineLearning
r7ch51,1638466051.0,"""while True: learn()"" game free on Epic Games","I haven't played it and I don't know how much you'll learn, but 'while True: learn()' is a machine learning game and it's the free game this week on epicgames.com.",LearnMachineLearning
r7bns7,1638463824.0,"Guys, I'm very keen to learn to create AI Art. How do I go about it as a beginner?","Guys, so as the question says, I'm very new to machine learning. I want to create my own AI to generate the art. I know it is a very long way to go and a vague question to ask. But, I'm really keen on creating it. I'm a programmer with about 1 year of experience. Any kind of help will be appreciated....",LearnMachineLearning
r76vis,1638449767.0,Help with first complex ML code,"Hi all,

I have limited experience with ML and would like to create somehting a little more complex, I am not expecting full solutions but if anyone could just tell me a few keywords I would need to google to get myself started with what kind of ML could solve this (if any) it would be very appreciated.

Baisically, I work for a contract manufacturing pharma company, when we get a new drug we need to make there is a list of around 20-30 different liquids that are involved in making the drug, itll come in the form like this:

Liquid             Volume needed (L)         Corrosiveness Factor      Needed in Step 1?        Needed in Step 2?

Water             10000                              1                                                       X                                        X

Acid                5000                                 50                                                       X

etc.

&#x200B;

What I want to automate is the long and tedious process of allocating which of our tanks 1) make and 2) store each liquid,

So I have a list of Tanks like this (1 for preparation tanks and 1 for storing):

Tank Name       Volume (L)       Material

P-001                     20000               Steel

P-002                     5000                  Reinforced Steel

etc

&#x200B;

And, to complicate this, I have another table like this:

Tank        P-001        P-002      etc

S-001        X                   X

S-002                             X

etc

&#x200B;

Basicially its a long table showing the 'connections' between the preparation and storage tanks, an 'X' means that there is piping available to pump the liquid from, for example there is piping between prep tank 001 and Storage tank 001, but not from P-001 to S-002, therefore you cannot prepare a liquid in P-001 then Store it in S-002.

There is also another table:

Tank                  Step 1                Step 2

S-001                       X                      X

S-002                     X

etc

This shows the 'connections' between the storage tanks and the acutal manufactuing floor, if a liquid is needed in step 1 and 2 it should be stored in S-001 to avoid needing new piping etc.

So there is a long list of rules:

&#x200B;

* You must strictly follow the preparation-storage connections as no new connections here can be made
* If a liquid has a 'corrosiveness factor' number above 30, it needs preped & stored in a reinforced steel vessel
* But less corrosive liquids can be stored in any tank
* Volume of the tanks should be sufficient to prep and store all of the liquid
* Its inevitable that some new connections between the storage tanks and the manufacturing floor might need to be made, but the allocation should be designed so as to keep this a minimum

Output will look something like:

Liquid               Prep Tank Allocation              Storage Tank Allocation

Water                 P-001                                            S-001

Acid                   P-002                                             S-002

etc

Along with the number of 'new' pipings that need to be made between storage and manufacturing. I understand this is complex but is there any method of ML that can take multiple tables like this, along with a strict set of 'rules' along with around 30 previous allocation projects and then make its own algorithm to determine allocations if you give it a list of liquids with their volumes, corrosiveness and where they are needed in the process?

Thanks!",LearnMachineLearning
r76otc,1638449130.0,SaneBox | Clean up your inbox in minutes & keep it that way forever,"

https://preview.redd.it/cu28tboxk4381.png?width=600&format=png&auto=webp&s=b3a43fead9f1609549394a1113f138551c1ddf52

Implement SaneBox into your workday, and never waste time on email again! [SaneBox](https://sanebox.grsm.io/zyqeprbwfssr) simplifies the email process by using powerful algorithms that learn your email behavior to organize your inbox. An average SaneBox customer saves 12+ hours/month. It works with any email provider, client or device. Keep your email organized with SaneBox. After experiencing a clean inbox, it will be hard to imagine life without it!

SaneBox is for the average or power email user! It works everywhere you check your email and fits into your existing workflow. Customers choose [SaneBox](https://sanebox.grsm.io/zyqeprbwfssr)because it saves time and boosts productivity!

SaneBox Pricing Overview

They have a 14-day free trial, don't miss the chance to start now

[SaneBox](https://sanebox.grsm.io/zyqeprbwfssr) pricing starts at $7.00 as a flat rate, per month. They do not have a free version.",LearnMachineLearning
r76jtx,1638448633.0,Bayes probability network with pomegranate package,"Does anybody familiar could explain me what model\*\*.\*\*predict\_proba() method actually doing. I suppose it calculates the conditional probabilities when we have particular input, based on input data but I am not sure? Here I made the I hope good learning example for my own learning and I also implemented predict method which gives integer outcome based on probability (i hope i understood this well)?

[https://github.com/Vitomir84/Statistics-and-probability/blob/master/Bayes%20probability%20network.ipynb](https://github.com/Vitomir84/Statistics-and-probability/blob/master/Bayes%20probability%20network.ipynb)

Star example if you like it! And if anybody has some idea how to improve it, I would be grateful!",LearnMachineLearning
r75y47,1638446459.0,💊Your daily dose of machine learning : ONNX Runtime for deep learning in C++,"> *This is a series of posts that I post almost daily. I call them “your daily dose of machine learning”.*

This week I shared with you my experience deploying Tensorflow models using C++. I shared with you how we used OpenCV DNN module and Tensorflow C++ api.

Today I want to share with you my experience deploying Tensorflow models in a C++ environment using ONNX Runtime.

ONNX stands for Open Neural Networks Exchange and it’s a whole ecosystem that aims to standardize the representation of machine learning models. It’s developed by Microsoft.

What ONNX aims to do is make it easier to deploy any kind of machine learning model, coming from any type of ML framework including Tensorflow.

To deploy Tensorflow models using ONNX in C++, you need to do 2 things:

\- Convert your Tensorflow model to ONNX format. There is an open source tool for this called tf2onnx.
\- Use ONNX Runtime to deploy your converted model.

I’ve personally tested this approach on so many deep learning models and it works great.

For example, I converted almost all of the models that are in the tensorflow object detection api into ONNX format and I was able to run inference with them with no problem.

I fell in love with this tool after it was suggested to me by a friend and after seeing all of its capabilities.

In future posts I might go into more details about the capabilities of this tool.

I also post almost daily on LinkedIn and Twitter, [***follow me there***](https://withkoji.com/@Nour_Islam)!",LearnMachineLearning
r73zke,1638438740.0,Metalearning conditional weights (?),"I've been reading [this](https://arxiv.org/pdf/2003.10780.pdf) paper on class weighting and domain adaptation with my reading club and there's an issue that's still a bit unclear to us (or at least to me).

It seems like the paper updates the model and sample-conditional weights alternatingly, but looking at algorithm 1 I'm not sure how the epsilon values are being updated (line 13). Are they being backpropagated through the updated model theta or what?",LearnMachineLearning
r73sq8,1638437952.0,Multi-task learning: How's that done?,"Hi guys. I've recently heard about the concept of multi-task learning(I'll call it MTL). I've read some articles online and have watched some youtube videos on the topic, however, there are some aspects that I don't understand.

Assume that we have a project that we want to do MTL on 3 seperate tasks. What I know is there is a base model to extract features and then there are three models on top of that to do the tasks.

What I don't understand is how do we train these models? Is it done simultanously? If yes, there will be three set of gradients backpropagated for each task to update the base model, then, ho do we handle that? If no, then how do we train the base model?",LearnMachineLearning
r718bt,1638427546.0,Build a Job Recommender from scratch using Networkx and Streamlit.,"Hey guys check out my newest project where I built a Job recommender from scratch using Scrapy, Networkx and Streamlit, this is how it looks like. I have public all the code and tutorial in my GitHub project page here:  [huynhnhathao/job\_recommender: A Job Recommender System using Graph-based data representation and Hybrid recommender system algorithm. (github.com)](https://github.com/huynhnhathao/job_recommender) , and this is demo link:  [The Ultimate Jobs Recommender · Streamlit](https://share.streamlit.io/huynhnhathao/job_recommender/main/recommender/core/my_web_app.py)

[Job recommender](https://preview.redd.it/nwhddh2ds2381.png?width=1920&format=png&auto=webp&s=ffdcf9057d57c9fb88dd3c959a9ae411ef0a1b65)",LearnMachineLearning
r6xzz7,1638416969.0,Machine Learning Methods for Classification With Categorical Variables,"To start, I'd like to say I have very little experience in machine learning, or statistics/computer science in general.

What I am interested in is a list of models I can use to classify a binary dependent (response/output/Y) variable with non-ordered categorical independent (explanatory/input/X) variables. I know the list at [https://topepo.github.io/caret/train-models-by-tag.html#Neural\_Network](https://topepo.github.io/caret/train-models-by-tag.html#Neural_Network) that has been super helpful, but I can't tell which models use quantitative or ordered variables, or a quantitative output variable.

I've used a randomforest and neural network model to some great success, but I'd like to find some other models that I can play/learn with.

&#x200B;

Edit just in case anyone is curious what models I've used so far:

Single Decision Tree model (rpart in R)

Random Forest classification

Neural Network classification using one-hot-encoding of predictors (done in nnet package)

Naive Bayes classification (done in e1071 package, the naivebayes package had some issues)",LearnMachineLearning
r6v5dx,1638408570.0,Logistic Regression log odds Linearity assumption.,"Hello, I'm trying to understand what this assumption means. I drew a picture of what I think it means, that is, I believe that the assumption is true if the data is separated so that we can split it correctly with a straight line (left image), and the assumption is violated if the boundary between the classes is highly non-linear or just mixed together (right image). Just looking for confirmation on this really, thanks.

Then as a follow-up question, if the above is true, is it okay to just fit the model and simply evaluate the validity of the assumption with the model accuracy? So if we get a low accuracy, it probably means the assumption is violated, and if we have a high accuracy, the assumption is probably valid?

https://preview.redd.it/ff830vs981381.jpg?width=2594&format=pjpg&auto=webp&s=b585647fe6141c10a11c79e7e18b3db9bfccbaac",LearnMachineLearning
r6trwq,1638404652.0,Which Python library is best for counting objects using edge detection?,"I'm wanting to create a program that will count a number of objects that are all touching, for example a number of boxes that are tightly packed together, and then counting each box.

Any suggestions on which library to use or how to break down this project  would be appreciated. I'm thinking TensorFlow or OpenCV. Cheers.",LearnMachineLearning
r6srub,1638402009.0,Where do I look for data," Hello, I'm a newbee trying to make an Elearning or Ehealth website and I want to some big data sets to implement machine learning ideas into the website. Where do i find such data sets and which is easier to find data for (elearning or ehealth). ( And I'd appreciate some machine learning suggestions if you guys have any!)
Thanks!",LearnMachineLearning
r6pedp,1638393285.0,Resource to learn time-series analysis to prepare for quantitative research take-home assessment?,"Hi, I've received an offer for a 24-hour coding assessment for the role of quantitative researcher for a trading company. The main component seems to be to ""statistically analyze large scale tick by tick financial data to extract alpha patterns"". I guess I'm supposed to analyze large Time Series data and detect alpha patterns?

I have Python, numpy, and pandas background, but have not dealt with any professional time-series analysis and presentations.

Any resources, eg tutorials, sample presentations, articles, books, youtube resources on how to prepare for this? Further clarification on what to expect would be great as well. Thanks!",LearnMachineLearning
r6op6g,1638391508.0,Inter-rater Reliability Metrics: Understanding Cohen's Kappa,"I often see subtle misuses of interrater reliability metrics.

For example, imagine you're running a Search Relevance task, where search raters label query/result pairs on a 5-point scale: Very Relevant (+2), Slightly Relevant (+1), Okay (0), Slightly Irrelevant (-1), Very Irrelevant (-2).

Marking ""Very Relevant"" vs. ""Slightly Relevant"" isn't a big difference, but ""Very Relevant"" vs. ""Very Irrelevant"" is. However, most IRR calculations don't take this kind of ordering into account, so it gets ignored.

I wrote an introduction to Cohen's kappa (a rather simplistic and flawed metric, but a good starting point to understanding IRR). Hope it helps + I welcome feedback!",LearnMachineLearning
r6ns57,1638389157.0,Colab Pro+: Only P100s,"I've recenetly upgraded to Colab Pro+ and as the title states above, I've only received P100s over the past week since I upgraded. I was wondering if this was common or is it just very busy?

If it is a common issue, I might as well have not upgraded - even though I knew getting a V100 or A100 was still not garunteed, just wish I recieved it once!

Or I'm the just the unluckiest Colab user.",LearnMachineLearning
r6ldxk,1638383028.0,AI Recruitment Survey,"Hey Everyone,

My name is Reeti, and I work for a consulting organization called Illinois Business Consulting. We’re working for a company in the AI sector, and are hoping to improve their recruitment. We made a survey about AI recruitment, so if you guys wouldn’t mind filling out the survey below, it would be awesome! Thank you again!!!

[https://qfreeaccountssjc1.az1.qualtrics.com/jfe/form/SV\_eX3xJevjYUULfKK](https://qfreeaccountssjc1.az1.qualtrics.com/jfe/form/SV_eX3xJevjYUULfKK)",LearnMachineLearning
r6lbdj,1638382849.0,Clustering data sets of mixed types,"Hi all

In your experience what's the best way to cluster data sets of mixed data types?

K-prototypes and/or agglomorative single, average, and complete linkage?

Thanks.",LearnMachineLearning
r6l8nh,1638382656.0,Algorithm Selection (basic music generation),"I'm looking to generate a very basic sound snippet (around 40-50 notes) based around a small training set for 5 different genres but I'm new to machine learning and am having trouble determining which algorithms I should pursue to implement this.

 I'm working in MATLAB and will use the frequencies of the first 40 chord of a song (about 5-10 songs per genre) to create a prediction of what the next frequency to play will be based on the previous frequency.

I.e. the genre is indie pop, putting each song in its most basic form (just chords), the first frequency of the song is a Bflat major chord, and the second chord is a g minor chord. These chords would be randomly generated. Knowing that it is part of the indie genre, I'd want to predict the next 40 chords of the song. No rhythms for now, just frequencies.

Any algorithm suggestions for this?",LearnMachineLearning
r6k36f,1638379754.0,Finding big data sets,"Hello, I'm a newbee trying to make an Elearning or Ehealth website and I want to some big data sets to implement machine learning ideas into the website. Where do i find such data sets and which is easier to find data for (elearning or ehealth). ( And I'd appreciate some machine learning suggestions if you guys have any!)",LearnMachineLearning
r6jee3,1638377936.0,How to load 85.6 GB of XML data into a dataframe,"

Hello everyone,

I am trying to load a large XML dataset (\~85.6GB) into a dataframe in jupyter notebook (python). I have tried:

1. pd.read\_xml() -> System crashes - High memory usage
2. ElementTree -> System crashes - High memory usage
3. vaex \[OpenSource library\] -> Does not handles XML data well. throws an error.

Anyone has encountered a similar challenge? Kindly share how did you go about it?

Thanks in advance.

Regards,
Mustafa (ML student)

\# Python
\# Machine Learning
\# DataScience",LearnMachineLearning
r6hwc4,1638374033.0,Prerequisites to cs229/elements of statistical learning,"Hi , I want to do the cs229 course and read the elements of statistical learning but I'm not sure if I have the prerequisites to them .
I have a quite advanced level in calculus and linear algebra . And I understand some probability theory .

Is that enough ?
Or should I learn something else ( statistics ... ) , if so what are the best books/ resources available ?",LearnMachineLearning
r6eple,1638365175.0,NOVEMBER UPDATES - The Data Science Interview book,"The month of November is up and Christmas month is here, feels like 2021 has almost slipped past us.

So what did we do in the month of November:

- NLP section updated
- Missing values section added
- Formatting changes in the Statistics section
- Took some break, was obsessively working on this 😌
- New section - Tree based approaches, Industry application added
- Launched our [LinkedIn page](https://www.linkedin.com/company/the-data-science-interview-book/?viewAsMember=true), have some interesting plans for it in near future
- Added support for dark theme, 🤯 had to remove it as it was breaking a lot of other stuff. Will wait for official support
- Added new problems in Probability, Python, Regression, SQL
- Added Temporary Datasets and Time page in SQL covering CTEs
- Regression section extensively updated

**Don't forget to show this project your ❤️ and support**",LearnMachineLearning
r6dcj3,1638360592.0,Exercises collection?,"I think the best way to learn is by doing, otherwise it doesn't stick.

So, is there anywhere online a list of machine learning exercises, ideally with example code solutions?

I've found so far the following:

* various textbooks - those rarely have code, and the data is hard to get sometimes
* coursera/MIT courses homework - usually just 1-2 easy problems per chapter, if that
* hackerrank - many problems, with community solutions, but not that well structured and often poorly curated",LearnMachineLearning
r6cobf,1638358110.0,💊Your daily dose of machine learning : Tensorflow C++ API,"> *This is a series of posts that I post almost daily. I call them “your daily dose of machine learning”.*

Last time, I shared with you how I used OpenCV to deploy Tensorflow models in a C++ environment.

Today I want to share with you my experience deploying deep learning models using Tensorflow C++ API.

Tensorflow is built using C++ and it offers an API to make it relatively easier to deploy models (and even train models if you wish to) in C++. This sounds great, until you start trying to implement a program that uses this API.

As a developer, you know that the documentation is very important, but Tensorflow’s C++ api is very very very limited. It’s very hard to find the information you’re looking for just by reading the documentation.

Secondly, there is no support for Windows 32bits. So if you have such a system then you’re facing a wall here and it’s better to try looking for other options.

But the great thing about this API is that you don’t have to worry about the compatibility of your model that you trained in Python with the C++ api, especially if you’re using the same version of Tensorflow in Python and C++.

I’ve personally deployed image classification models and object detection models using this API, and apart from the limiting factors that I mention above, the models worked exactly as expected.

The last option that I personally tried when it comes to deploying Tensorflow models in C++ is ONNX Runtime. More about this in an upcoming post.

 Follow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***",LearnMachineLearning
r6bttm,1638354644.0,"Back to Basic: Linear Regression (with Python code, in 2 minutes)","One of the biggest fields of application of Python these days is machine learning, and one of the first things anyone learns about machine learning is how to do linear regression. Linear regression is attractive because it is very simple to understand (at least compared to other learning models). We also know how to implement it efficiently, and more importantly, we can make sense of the results.

I made a 2 minutes video exactly about this topic.

[https://youtu.be/CtsRRUddV2s](https://youtu.be/CtsRRUddV2s)

By design, a 2 minutes is never exhaustive, but hopefully this will serve as a good intro to the topic and will give you the motivation to dive deeper if it sounds like something you are interested in. I know that when I started out I wished there were resources like that available, so I would love to hear what you think about this format.",LearnMachineLearning
r6b4uu,1638351802.0,Why do we need regularisation (L2 or L1 norm) in logistic regression?,"As I was revising through my logistic regression notes and came around the loss minimization interpretation of logistic regression which is:

argmin(w) log(1 + exp(-Zi)) + 1/2*lambda*||w||^2 where Zi = Yi.Wi.Xi summation i : 1->n

I know that, the L2 regularisation as used in the above optimization function is used to find a balance between a good seperating hyperplane (decision surface) and weight coefficients that are not too large (tending to infinity) to be overestimated. I can't seem to intuitively understand as to how regularisation is working to balance the weight coefficients to avoid overfitting/underfitting?
Also I might be having a misunderstanding here but in the loss function optimization part of the expression, if we consider that we are not using any regularisation, then ideally to minimise the loss function,
For points that are correctly seperated, the weights corresponding to features should tend to infinity such the value of Zi tends to infinity which results in log(1 + exp(-Zi)) tending to 0 so we are minimizing the sum over correctly classified points but for the same plane with infinitely big weights, if a point comes out to be incorrectly classified it's loss function value will tend to infinity which makes it working against the optimisation problem. So accordingly the weights should get readjusted to smaller values, such that the sum of loss is minimized, without the need of a regularisation term.
So I am really very confused as do we even need regularisation in logistic regression, if yes,  how regularisation term in the expression is working towards balancing the weights?",LearnMachineLearning
r6asy1,1638350407.0,Poll on why Data Quality remains the top level concern for most machine learning teams?,"A recent study by concluded that Data quality is increasingly becoming a top-level concern among companies implementing AI. Being in the labeling and annotation business ourselves [Cogito Tech LLC](https://www.cogitotech.com/?utm_source=reddit&utm_medium=social&utm_campaign=reddit) have noticed several cases where organization approached us for labeling work for various reasons. I wanted to know from a larger community that why do you think are the key reasons the demand for data quality is still increasing, despite so many datasets being available and so much work being done on them?

[View Poll](https://www.reddit.com/poll/r6asy1)",LearnMachineLearning
r6aja2,1638349221.0,Can someone explain to me in simpler terms what this paragraph means in simpler terms?,"I am not sure if NMF is directly under Machine Learning, but I will try.

Can someone explain to me in simpler terms what this passage means in this [paper](https://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf) that I am reading about NMF?

I did not get how the above formula got transformed into the 2nd formula below.

https://preview.redd.it/i0jwxkpkbw281.png?width=1023&format=png&auto=webp&s=6dea9818d386918d79767fb5be99f45a79409f88",LearnMachineLearning
r688iw,1638339750.0,Can somebody explain a classification reports meaning?,Can somebody explain a classification report done on an SGD classifier to me. I have 79% precision and 100% recall on 0s but I have 0% precision and 0% recall on 1s,LearnMachineLearning
r6789x,1638336017.0,How to autusave or resume training after Google Colab shutting down brutly?,"I am new to google colab and have bought a colab pro+ member. I found it can run around 24 ~ 36 hours, but brutly shut down each time you know.After that I have to retrain all my GAN. Sometimes I would download the pkl file manually and upload it to retrain, but it looks stupid.


Is there a way to autosave pkl files in google drive during each gan training snapcount? Thanks a lot",LearnMachineLearning
r66avr,1638332809.0,"Really noob question, is there a way to look through the CLIP database manually?",I just think it'd be neat to see all the images used to train CLIP,LearnMachineLearning
r65cbg,1638329642.0,MLB came out today to say that 2 different types of baseballs were used last season. What type of clustering algorithms would you use to determine which pitch sequence contained which type?,"Hi, just wanted to share a question I thought would be interesting and discuss with the subreddit how this might be accomplished. This is an unsupervised learning problem, but it's much tougher than most. You do have a definitive answer for number of clusters at least. But, you have a mostly small sized, varying length pitch sequence to make a determination with, and have a few variables to account for (such as pitchers and which stadium you're at).",LearnMachineLearning
r6314s,1638322314.0,How do you feel about job recruitment in AI? (Survey),"

Hey Everyone,

My name is Davis and I work for a consulting organization called Illinois Business Consulting. We’re working for a company in the AI sector and are hoping to improve their recruitment. We made a survey about AI recruitment, so if you guys wouldn’t mind filling out the survey below, it would be awesome!

Thank you again!!!

[https://qfreeaccountssjc1.az1.qualtrics.com/jfe/form/SV\_eX3xJevjYUULfKK](https://qfreeaccountssjc1.az1.qualtrics.com/jfe/form/SV_eX3xJevjYUULfKK)",LearnMachineLearning
r61irj,1638317815.0,Need help with theory questions asked for a research position,"I was asked these questions in an assessment for a research position in machine learning. Any intuition on how they can be solved/reference to solve would be really helpful:

1.  Non-Uniform Weights in Linear Regression: You are given a dataset in which the data points are denoted by (xn, tn), n = 1, · · · ,N. Each data point is associated with a non-negative weighting factor gn > 0. The error function is thus modified to:



https://preview.redd.it/v25kzxgept281.png?width=272&format=png&auto=webp&s=e2981a06d420e973eca295a25a97110bae9cfdde

where Φ(·) is any representation of the data.

a.  Find an expression for the solution w^(∗) that minimizes the above error function.

b.  Give two alternative interpretations of the above weighted sum-of-squares error function in
terms of: (i) data-dependent noise variance and (ii) replicated data points.

2.   Given D-dimensional data x = \[x1, x2, · · · , xD\], consider a linear model of the form:

&#x200B;

https://preview.redd.it/ijaupdpupt281.png?width=270&format=png&auto=webp&s=ec9f56fa45395e24534bd2a032dbe7989d856766



Now, for N such data samples with their corresponding labels (xi, ti), i = 1, 2, · · · ,N, the sum-of-squares error (or mean-squared-error) function is given by:

&#x200B;

https://preview.redd.it/7vbsqjmwpt281.png?width=347&format=png&auto=webp&s=190dd2676a842dd4ef9789def2256673a6cb7d5f

 Now, suppose that Gaussian noise ϵk ∼ N(0, σ\^2) (i.e. zero mean and variance σ\^2) is added independently to each of the input variables xk. Find a relation between: minimizing the above sum-of-squares error averaged over the noisy data, and minimizing the standard sum-of-squares error (averaged over noise-free input data) with a L2 weight-decay regularization term, in which the bias parameter w0 is omitted from the regularizer.

3.

Solve the problem given below by hand (no programming implementation required):

(a) Consider the training set and test set given in Tables 1 and 2.

&#x200B;

https://preview.redd.it/3dllbm5bqt281.png?width=573&format=png&auto=webp&s=320610544f7105d36c77d79d42d92c4bc190cf3e

 We use the linear model fθ(x1, x2) = θ0 + θ1x1 + θ2x2 and the logistic regression function σ(fθ(x1, x2)) =

https://preview.redd.it/wq7myzbdqt281.png?width=153&format=png&auto=webp&s=8307fbf23cb8dbb6791f8e6768389ff9542b6423

 Consider the initial weights as θ0 = −1, θ1 = 1.5, θ2 = 0.5, and learning rate as 0.1 (for gradient descent).
i. What is the logistic model P(ˆy = 1|x1, x2) and its cross-entropy error function?
ii. Use gradient descent to update θ0, θ1, θ2 for one iteration. Write down the updated logistic regression model. Calculate and report the accuracy, precision and recall to evaluate this model at this iteration.",LearnMachineLearning
r5zkt2,1638312232.0,Giving reviews a rating based on existing reviews.,"I have a big project I am working on, an issue I've run into is that some data on my review rating column is missing. I would like to make an algorithm that can take the provided reviews and there ratings in my CSV file and then make a predictions on those missing ratings. My question is, is there any simple tutorial about NLP that doesn't involve deep learning algorithms?",LearnMachineLearning
r5ynbw,1638309685.0,TSNE - Back to the original feature space,"Hello,

Assume that I have a dataset whose number of features is 256. I apply TSNE on this dataset for 2  components. Then, I have 2 dimensions right now. After I perform some operations on that 2-dimensional space, I come up with a data point. What I want to do is to expand this point to the original feature size, which is 256.

I meant the below flow:

256 -> TSNE -> 2 -> (SOME MAGIC) -> 256

I am looking for that magic actually. Is there any way to do so?",LearnMachineLearning
r5uu53,1638299500.0,Introductory Tensorflow course with a focus on computer vision,"Hello there!

You might have seen my ""daily doses of ML"" posts! Today I want to share with you a free introductory Tensorflow course that I put together.

It focuses on building convolutional neural networks for computer vision tasks.

Check the course curriculum below before you decide, maybe it's too basic for you!

You will also be able to ask me any question if you're stuck!

I hope to see you in class!

***Course link :*** [***https://aifee.teachable.com/p/introduction-to-tensorflow-2-for-computer-vision***](https://aifee.teachable.com/p/introduction-to-tensorflow-2-for-computer-vision)

&#x200B;

[Course curriculum](https://i.redd.it/w3e5mgiv7s281.gif)",LearnMachineLearning
r5tfm4,1638295830.0,Unbalanced predictions on MNIST,"So I've implemented a simple LeNet model to learn and predict the MNIST dataset; the purpose being to experiment with a new loss function (evidential deep learning) which makes the model aware of epistemic uncertainty (i.e. a cat/dog classifier can say ""I don't know"" to a picture of a whale)

The training set I've balanced the labels, so there are the same number of samples for each label, the weights are randomly (with no seed) initialised, the dataset shuffled, and yet the predictions are weighted to the 'lower' digits (see image), and rather substantially to the 0 digit. The implementation is in PyTorch.

Does anyone know of methods to mitigate this?

&#x200B;

https://preview.redd.it/jzan7oaywr281.png?width=389&format=png&auto=webp&s=5c7eaed8933e7797aa9f47e2eab99ca73163f116",LearnMachineLearning
r5s348,1638292443.0,"Given measurement values from 20 air pressure sensors collected over 3 months in a pipe system, how to detect the location of broken parts/ holes in that system?","First thing, that came to mind was outlier detection, but as the size of a broken part increases, the pressure in the whole system seems to drop and all sensors return more or less lower values. the one that breaks, drops first though. Therefore, detecting the moment when something started to break, worked fine with several outlier detection techniques, but the localization did not work at all.

Has anyone got some smart idea of an approach that could work better?",LearnMachineLearning
r5rkbv,1638291083.0,"Making a ""bonding"" discord group for everyone","Hey everyone,

Ive been wanting to make a ""bonding"" group of ppl in this discipline. The thing is, study groups often dont work as we sometimes work on very different stuff. Plus, there is no incentive for most of us, as we tend to treat it like a burden. Id just like to have a group where we can work mutually and independently, chill, and have someone to distract from the boredom of coding/studying/researching throughout the day.

The server will have a voice chat and a general text chat, nothing more. Thanks guys.

[https://discord.gg/acQsQhd2kH](https://discord.gg/acQsQhd2kH)",LearnMachineLearning
r5pjtp,1638285653.0,"Started learning Machine Learning today, Enrolled in Andrew Ng's Course. How to approach the course and get on with it?",How to get from theory to projects? Any particular sources etc for a beginner.,LearnMachineLearning
r5o8q0,1638281872.0,What is this parameter tuning method called? Does it work?,"Hello! I work for the Meteorological Service of Canada. Suppose I want to tune some parameters for a prediction system. However, this is for a heavy prediction system like weather forecasting, and the supercomputer takes an hour to finish one run, then we get a score result for how good the prediction was. There's ten parameters to tune.

Which parameters should we try next? How about training a NN, even just on a few examples, to model which parameters will output which scores. We then use the NN to try thousands of combinations of parameters. The next set of parameters we run for an hour for real are the best parameters, according to the NN guess.

After the hour run, we retrain the NN on the new example. Repeat.

Is there a name for this? Does it work well? Example articles or discussions? Is there some better way?

Thanks.",LearnMachineLearning
r5lf8h,1638272265.0,The Reward is Enough hypothesis,"Hi guys,

Here's a video I made explaining the ""Reward is enough"" paper from Deepmind's David Silver. In it, they posit that Reinforcement Learning is the right paradigm to further the development of General Artificial Intelligence. Hope you enjoy it:

[https://www.youtube.com/watch?v=tqNkUQMIAAs](https://www.youtube.com/watch?v=tqNkUQMIAAs)",LearnMachineLearning
r5la61,1638271707.0,"ValueError: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 2])) is deprecated. Please ensure they have the same size.","I am trying to build a binary classification model with CNN. However getting the above error. Here is my code

    class parallel_all_you_want(nn.Module):
        # Define all layers present in the network
        def __init__(self,num_emotions):
            super().__init__()

            ################ TRANSFORMER BLOCK #############################
            # maxpool the input feature map/tensor to the transformer
            # a rectangular kernel worked better here for the rectangular input spectrogram feature map/tensor
            self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])

            # define single transformer encoder layer
            # self-attention + feedforward network from ""Attention is All You Need"" paper
            # 4 multi-head self-attention layers each with 40-->512--->40 feedforward network
            transformer_layer = nn.TransformerEncoderLayer(
                d_model=40, # input feature (frequency) dim after maxpooling 40*282 -> 40*70 (MFC*time)
                nhead=4, # 4 self-attention layers in each multi-head self-attention layer in each encoder block
                dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 40-->512--->40
                dropout=0.4,
                activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time
            )

            # I'm using 4 instead of the 6 identical stacked encoder layrs used in Attention is All You Need paper
            # Complete transformer block contains 4 full transformer encoder layers (each w/ multihead self-attention+feedforward)
            self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)

            ############### 1ST PARALLEL 2D CONVOLUTION BLOCK ############
            # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)
            self.conv2Dblock1 = nn.Sequential(

                # 1st 2D convolution layer
                nn.Conv2d(
                    in_channels=1, # input volume depth == input channel dim == 1
                    out_channels=16, # expand output feature map volume's depth to 16
                    kernel_size=3, # typical 3*3 stride 1 kernel
                    stride=1,
                    padding=1
                          ),
                nn.BatchNorm2d(16), # batch normalize the output feature map before activation
                nn.ReLU(), # feature map --> activation map
                nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size
                nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training

                # 2nd 2D convolution layer identical to last except output dim, maxpool kernel
                nn.Conv2d(
                    in_channels=16,
                    out_channels=32, # expand output feature map volume's depth to 32
                    kernel_size=3,
                    stride=1,
                    padding=1
                          ),
                nn.BatchNorm2d(32),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters
                nn.Dropout(p=0.3),

                # 3rd 2D convolution layer identical to last except output dim
                nn.Conv2d(
                    in_channels=32,
                    out_channels=64, # expand output feature map volume's depth to 64
                    kernel_size=3,
                    stride=1,
                    padding=1
                          ),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=4, stride=4),
                nn.Dropout(p=0.3),
            )
            ############### 2ND PARALLEL 2D CONVOLUTION BLOCK ############
            # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)
            self.conv2Dblock2 = nn.Sequential(

                # 1st 2D convolution layer
                nn.Conv2d(
                    in_channels=1, # input volume depth == input channel dim == 1
                    out_channels=16, # expand output feature map volume's depth to 16
                    kernel_size=3, # typical 3*3 stride 1 kernel
                    stride=1,
                    padding=1
                          ),
                nn.BatchNorm2d(16), # batch normalize the output feature map before activation
                nn.ReLU(), # feature map --> activation map
                nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size
                nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training

                # 2nd 2D convolution layer identical to last except output dim, maxpool kernel
                nn.Conv2d(
                    in_channels=16,
                    out_channels=32, # expand output feature map volume's depth to 32
                    kernel_size=3,
                    stride=1,
                    padding=1
                         ),
                nn.BatchNorm2d(32),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters
                nn.Dropout(p=0.3),

               # 3rd 2D convolution layer identical to last except output dim
                nn.Conv2d(
                    in_channels=32,
                    out_channels=64, # expand output feature map volume's depth to 64
                    kernel_size=3,
                    stride=1,
                    padding=1
                          ),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=4, stride=4),
                nn.Dropout(p=0.3),
            )

            ################# FINAL LINEAR BLOCK ####################
            # Linear softmax layer to take final concatenated embedding tensor
            #    from parallel 2D convolutional and transformer blocks, output 8 logits
            # Each full convolution block outputs (64*1*8) embedding flattened to dim 512 1D array
            # Full transformer block outputs 40*70 feature map, which we time-avg to dim 40 1D array
            # 512*2+40 == 1064 input features --> 8 output emotions
            self.fc1_linear = nn.Linear(512*2+40,2)

            ### Softmax layer for the 8 output logits from final FC linear layer
            self.softmax_out = nn.Sigmoid() # dim==1 is the freq embedding

        # define one complete parallel fwd pass of input feature tensor thru 2*conv+1*transformer blocks
        def forward(self,x):

            ############ 1st parallel Conv2D block: 4 Convolutional layers ############################
            # create final feature embedding from 1st convolutional layer
            # input features pased through 4 sequential 2D convolutional layers
            conv2d_embedding1 = self.conv2Dblock1(x) # x == N/batch * channel * freq * time

            # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array
            # skip the 1st (N/batch) dimension when flattening
            conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim=1)

            ############ 2nd parallel Conv2D block: 4 Convolutional layers #############################
            # create final feature embedding from 2nd convolutional layer
            # input features pased through 4 sequential 2D convolutional layers
            conv2d_embedding2 = self.conv2Dblock2(x) # x == N/batch * channel * freq * time

            # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array
            # skip the 1st (N/batch) dimension when flattening
            conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim=1)


            ########## 4-encoder-layer Transformer block w/ 40-->512-->40 feedfwd network ##############
            # maxpool input feature map: 1*40*282 w/ 1*4 kernel --> 1*40*70
            x_maxpool = self.transformer_maxpool(x)

            # remove channel dim: 1*40*70 --> 40*70
            x_maxpool_reduced = torch.squeeze(x_maxpool,1)

            # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format
            # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)
            x = x_maxpool_reduced.permute(2,0,1)

            # finally, pass reduced input feature map x into transformer encoder layers
            transformer_output = self.transformer_encoder(x)

            # create final feature emedding from transformer layer by taking mean in the time dimension (now the 0th dim)
            # transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average
            transformer_embedding = torch.mean(transformer_output, dim=0) # dim 40x70 --> 40

            ############# concatenate freq embeddings from convolutional and transformer blocks ######
            # concatenate embedding tensors output by parallel 2*conv and 1*transformer blocks
            complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2,transformer_embedding], dim=1)

            ######### final FC linear layer, need logits for loss #########################
            output_logits = self.fc1_linear(complete_embedding)

            ######### Final Softmax layer: use logits from FC linear, get softmax for prediction ######
            output_softmax = self.softmax_out(output_logits)
            #output_softmax = self.sigmoid(output_logits)
            # need output logits to compute cross entropy loss, need softmax probabilities to predict class
            return output_logits, output_softmax


def criterion(predictions, targets):

 return nn.BCELoss()(input=predictions, target=targets)

    optimizer = torch.optim.SGD(model.parameters(),lr=0.001, weight_decay=1e-3, momentum=0.8)

    # define function to create a single step of the training phase
    def make_train_step(model, criterion, optimizer):

        # define the training step of the training phase
        def train_step(X,Y):

            # forward pass
            output_logits, output_softmax = model(X)
            predictions = torch.argmax(output_softmax,dim=1)
            accuracy = torch.sum(Y==predictions)/float(len(Y))

            # compute loss on logits because nn.CrossEntropyLoss implements log softmax
            loss = criterion(output_logits, Y)

            # compute gradients for the optimizer to use
            loss.backward()

            # update network parameters based on gradient stored (by calling loss.backward())
            optimizer.step()

            # zero out gradients for next pass
            # pytorch accumulates gradients from backwards passes (convenient for RNNs)
            optimizer.zero_grad()

            return loss.item(), accuracy*100
        return train_step",LearnMachineLearning
r5l11k,1638270689.0,Is there any AI/ML Model to use for predicting the side effects of previous taken decisions?,"Hello,

Just to let you know I am totally newbie in the AI/ML field. I am a Full Stack PHP developer with knowledge in JavaScript / HTML / CSS / etc.

I was wondering if there's any AI/ML model that is capable to predict what will be the side effect based on previous taken actions.

For example, let's say I have plant, that I used to water it every day. This is the normal process. But if I forget one day to water it, the flower leafs are turning down. If another day I water the plant twice, then the leafs are getting yellow. In another day I water them, but by mistake in the water there was petrol inside as well my wife water it with chlorine, and finally the plant is getting die due to the petrol.

As you see, according to some actions I take, the plant has some side effect, like, remains healthy, the leafs turning down, the leafs getting yellow, the plan is die.

So, I was wandering, if there's anything that can make link of the side effect and the actions taken previously, like, if you had watered the plant with water, petrol and chlorine the plant died. If you had water the plant too many times, the leafs became yellow.

If you don't have anything in mind, what it could be a possible scenario for creating such a model?

Thank you in advance ! :)",LearnMachineLearning
r5iyb6,1638261951.0,I need help with this tutorial,"Hi! I'm completely new to learning machine learning and I was watching this video on how to make your first AI with Python: [https://youtu.be/z1PGJ9quPV8](https://youtu.be/z1PGJ9quPV8)

It was going well until I ran the [model.fit](https://model.fit) cell and I got [this error](https://imgur.com/gallery/zQ2fIeD) and the only idea I have as to what could be the issue is that some external file is outdated, but otherwise I have no clue. You guys and girls have any idea as to what might be wrong?

Thanks alot for any answers and have a good one!",LearnMachineLearning
r5hymj,1638257966.0,Is Data structures and algorithms important to become a machine learning engineer?,"Hi, I have started to prepare for Machine Learning roles, and I am working on the maths and python required. I see a few people say that learning data structures and algorithms is important as well. Any suggestions would be helpful.",LearnMachineLearning
r5fyxe,1638250615.0,"Deep Learning Optimizers for beginners, in very simple language with very neat Python 3 codes","Learning optimizers may be the first step for beginners in Deep Learning, but most of the time in the tutorials, the codes are skipped and only mathematical notations are shown to them. And people watching them simply jump to DL libraries and compile with an optimizer of their choice. And sometimes, beginners have no idea what is happening. One example is, the beginners, wonder why their loss is increasing with training in the case of loss divergence problem. So, in these posts on medium, I have talked about widely used Optimizers with neat codes in very simple language.  I hope that these posts prove themselves useful to beginners.

SGD - [https://medium.com/@neuralthreads/gradient-descent-trick-developed-in-1847-is-now-the-first-step-in-the-field-of-neural-networks-12491818a1af](https://medium.com/@neuralthreads/gradient-descent-trick-developed-in-1847-is-now-the-first-step-in-the-field-of-neural-networks-12491818a1af)

SGD with Momentum - [https://medium.com/@neuralthreads/what-exactly-is-momentum-in-sgd-with-momentum-6db36acca965](https://medium.com/@neuralthreads/what-exactly-is-momentum-in-sgd-with-momentum-6db36acca965)

SGD with Nesterov acceleration - [https://medium.com/@neuralthreads/sgd-with-nesterov-acceleration-how-it-reduces-the-oscillation-in-sgd-with-momentum-b1b6f7309b81](https://medium.com/@neuralthreads/sgd-with-nesterov-acceleration-how-it-reduces-the-oscillation-in-sgd-with-momentum-b1b6f7309b81)

Adagrad - [https://medium.com/@neuralthreads/adagrad-storing-squares-of-gradients-d1461eae1be0](https://medium.com/@neuralthreads/adagrad-storing-squares-of-gradients-d1461eae1be0)

RMSprop - [https://medium.com/@neuralthreads/rmsprop-widely-used-optimizers-that-is-not-yet-published-6566ea1ede3](https://medium.com/@neuralthreads/rmsprop-widely-used-optimizers-that-is-not-yet-published-6566ea1ede3)

Adadelta - [https://medium.com/@neuralthreads/adadelta-optimizer-which-was-developed-to-eliminate-the-need-for-the-learning-rate-93e8f295abc7](https://medium.com/@neuralthreads/adadelta-optimizer-which-was-developed-to-eliminate-the-need-for-the-learning-rate-93e8f295abc7)

Adam - [https://medium.com/@neuralthreads/adam-update-to-rmsprop-storing-gradient-and-square-of-gradients-b2b54d59f741](https://medium.com/@neuralthreads/adam-update-to-rmsprop-storing-gradient-and-square-of-gradients-b2b54d59f741)

Amsgrad - [https://medium.com/@neuralthreads/amsgrad-a-variant-of-adam-using-the-maximum-of-past-square-gradients-544a9cc84619](https://medium.com/@neuralthreads/amsgrad-a-variant-of-adam-using-the-maximum-of-past-square-gradients-544a9cc84619)

Adamax - [https://medium.com/@neuralthreads/adamax-extension-to-adam-based-on-infinity-norm-ba9a766deafe](https://medium.com/@neuralthreads/adamax-extension-to-adam-based-on-infinity-norm-ba9a766deafe)

Optimizers racing to the Minima - [https://medium.com/@neuralthreads/optimizers-racing-to-the-minima-aedb070c9d29](https://medium.com/@neuralthreads/optimizers-racing-to-the-minima-aedb070c9d29)

&#x200B;

If you like these posts, then follow me on **medium** [https://medium.com/@neuralthreads](https://medium.com/@neuralthreads), **reddit** [https://www.reddit.com/r/neuralthreads/](https://www.reddit.com/r/neuralthreads/), and **youtube** [https://www.youtube.com/channel/UCorUqR9utU1SBdDeMLpGzKA/videos](https://www.youtube.com/channel/UCorUqR9utU1SBdDeMLpGzKA/videos) for future posts in which I will talk about Neural Networks in very simple language with very neat codes.",LearnMachineLearning
r5bqyg,1638237470.0,Do nodes lose activation when activated?,"I understand that nodes take in a sum from other nodes its connected to, checks that sum against an activation function, and then activates depending on that value +/- a bias, but when it activates, does it:

Retain that activated value and have it added to its activation every loop?

Retain that activated value and have it added to its activation post-activation (normalized)?

set its activation back to 0?

Hope my question makes sense, thanks!",LearnMachineLearning
r5a7i5,1638232806.0,How to separate overlapping clusters in kmeans?,"Hello, I am trying to understand how I might best approach a clustering data science problem that I have encountered, where clusters overlap. I will explain with a conceptual example.

I am conducting a study where I am trying to detect how much time is spent using different water-consuming devices (for lack of a better word) in households, such as a bath tub or a sink, etc. I have water-consumption readers on each water-using device reporting water usage levels (in this case gallons/minute) at chronological minute-intervals. And so, each row indicates a minute's worth of water use at each indicated gallon's/minute rate. I am doing this across 10 households, each with its own ID. A dataframe of this looks like so (GPM = gallons per minute):

           ID      GPM
    -------------------
      0     1       43
      1     1       23
      2     1       54
      3     1       33
        ...
    134     2       44
    135     2       52
    136     2       63
    137     2       33
        ...
    245     3       23
    246     3       12
    247     3       15
    248     3       51
        ...
    356     4       22
    357     4       61
    358     4       54
    359     4       84
    ...

As shown, each household ID has at each reading, an attributed water consumption value. Given how different water-using devices will have different typical water consumption levels, the water consumption values in the data will likely cluster, where each cluster represents a different water-using device. Each of these households has 5 of these studied water-using devices, and so I am using 5 clusters.

The goal is to create 5 unique ""ranges"", similar to bins, that I could simply drop the values of any new dataset into, so that I can find the number of ""minute"" observations that fall within each water-using device's range, to therefore say oh! this new household spent x minutes over the week using the sink and x minutes over the week using the shower, etc.

Now, while I could simply just run k-means on the entire dataset itself, ignoring ID, but this would not account for the consideration that each of these households could have different models and sizes of sinks, bath tubs, and showers, etc, meaning slightly different typical water consumption values, thus distorting the clusters. That is why I want to run clustering on each sub-dataset grouped by ID, so the clustering is true to that household.

And so, I would run kmeans on the ""GPM"" values attributed to ID 1, then run the next kmeans on the ""Value"" values attributed to ID 2, and then run the next kmeans on the ""Value"" values attributed to ID 3, and so on. In this example lets say I have 25 unique IDs with corresponding ""GPM"" values. What I want to do is create all-encompassing summary cluster ranges that capture all of these individual kmeans runs. Specifically by this I mean that for each ID's kmeans run I want to produce a min and max value for each of the 5 clusters. And then from that I want to produce a single range for each cluster that captures all of the values in the whole dataset. This means that I want to find the minimum ""Min"" value and the maximum ""Max"" value for each cluster across all of the IDs. From this I would make the simple summary dataframe:

         Min    Max
    ----------------
    0     ??     ??
    1     ??     ??
    2     ??     ??
    3     ??     ??
    4     ??     ??

And so this would create a cluster 1 range that would capture ALL of the cluster 1 values across all IDs, and create a cluster 2 range that would capture ALL of the cluster 2 values across all IDs, and create a cluster 3 range that would capture ALL of the cluster 3 values across all IDs, etc.

The big problem is that these ranges in my summary dataframe overlap! In theory it seemed like my idea would work, that these final summary ranges would capture all of the values in the dataset correctly, but it appears there is just to much shift between how the values cluster for each ID. I am having a hard time trying to find what to properly label this problem as, but would someone perhaps know of any ways I could address this issue, so that I can produce more accurate summary cluster ranges? Thanks! Please accept my apologies if my problem was not clear, I would be happy to go back and explain better.",LearnMachineLearning
r55v90,1638220509.0,"In a Statistical Learning Theory setting, why is the hypothesis class chosen before looking at the data ?","In a strict statistical learning theory setting:

A data scientist has to do supervised classification. They consult with the domain experts and choose some features which they believe to be predictive of the label. They also choose a hypothesis class [aka a Model] in which -they believe- contains a hypothesis having a low true error[aka population error]
>_

All the things mentioned above were done making use of some prior knowledge of the classification task in question.
>_

The data scientist *then* draws a big enough i.i.d sample [collects the data ] and does Empirical risk minimization on it [training]
>_

What would be different if we choose the hypothesis class *after* doing data exploration [or *after* a google search regarding which model to use]? I'm confused about what could be thought of as prior knowledge and what cannot?",LearnMachineLearning
r52qrb,1638212066.0,How to Transform Categorical Data to Numerical Data Using Pandas,"I am writing a python program that uses logistic regression to predict an outcome based on survey data from a csv. However, I'm running into the issue that some survey data is non-numerical. I need to:

* transform categorical data to numerical data, **without knowing which columns are categorical or how many categories per column there are ahead of time**
* be able to map the numerical data onto the category labels later

Any suggestions on how to approach this? I sincerely appreciate any thoughts!

Example data:

|weight|systolic blood pressure|has diabetes?|
|:-|:-|:-|
|155|119|no|
|210|131|yes|
|301|143|yes|

Example output:

|weight|systolic blood pressure|has diabetes?|
|:-|:-|:-|
|155|119|0|
|210|131|1|
|301|143|1|

    diabetes_dict = {
        0: ""no"",
        1: ""yes""
    }",LearnMachineLearning
r51wa6,1638209748.0,difference between deeplearning framework & Teachable Machine, I learned deeplearning but I was wondering why companies didnot use Teachable Machine as it is already developed ? what is the difference &why They are seeking deeplearning analyist ?why I just use the already developed app? I am just new to this track,LearnMachineLearning
r51g2v,1638208549.0,"GitHub - GoogleCloudPlatform/mlops-with-vertex-ai: An end-to-end example of MLOps on Google Cloud using TensorFlow, TFX, and Vertex AI","Check out this repository by Google cloud. I find it very useful.
An end-to-end example of 𝐌𝐋𝐎𝐩𝐬 𝐨𝐧 𝐆𝐨𝐨𝐠𝐥𝐞 𝐂𝐥𝐨𝐮𝐝 using 𝐓𝐞𝐧𝐬𝐨𝐫𝐅𝐥𝐨𝐰, 𝐓𝐅𝐗, 𝐚𝐧𝐝 𝐕𝐞𝐫𝐭𝐞𝐱 𝐀𝐈. 
1. Performing exploratory data analysis on the data in 𝐁𝐢𝐠𝐐𝐮𝐞𝐫𝐲..
2. Creating Vertex AI Dataset resource using the Python SDK.
3. Generating the schema for the raw data using 𝐓𝐞𝐧𝐬𝐨𝐫𝐅𝐥𝐨𝐰 𝐃𝐚𝐭𝐚 𝐕𝐚𝐥𝐢𝐝𝐚𝐭𝐢𝐨𝐧.
4. Preparing the data using 𝐃𝐚𝐭𝐚𝐟𝐥𝐨𝐰.
5. Implementing a 𝐊𝐞𝐫𝐚𝐬 𝐜𝐥𝐚𝐬𝐬𝐢𝐟𝐢𝐜𝐚𝐭𝐢𝐨𝐧 𝐦𝐨𝐝𝐞𝐥.
6. Training the Keras model with Vertex AI using a pre-built container.
7. Upload the exported model from 𝐂𝐥𝐨𝐮𝐝 𝐒𝐭𝐨𝐫𝐚𝐠𝐞 𝐭𝐨 𝐕𝐞𝐫𝐭𝐞𝐱 𝐀𝐈.
8. Extract and visualize experiment parameters from Vertex AI Metadata.
9. Use Vertex AI for 𝐡𝐲𝐩𝐞𝐫𝐩𝐚𝐫𝐚𝐦𝐞𝐭𝐞𝐫 𝐭𝐮𝐧𝐢𝐧𝐠.
10. Clone the repository to the built environment.
11. Run unit tests.
12. Run a local 𝐞𝟐𝐞 𝐭𝐞𝐬𝐭 𝐨𝐟 𝐭𝐡𝐞 𝐓𝐅𝐗 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞.
13. Build the 𝐌𝐋 𝐜𝐨𝐧𝐭𝐚𝐢𝐧𝐞𝐫 𝐢𝐦𝐚𝐠𝐞 for pipeline steps.
14.  𝐂𝐨𝐦𝐩𝐢𝐥𝐞 the pipeline.
15. Upload the pipeline to Cloud Storage
16. Creating a 𝐂𝐥𝐨𝐮𝐝 𝐏𝐮𝐛/𝐒𝐮𝐛 𝐭𝐨𝐩𝐢𝐜.
17. Deploying a 𝐂𝐥𝐨𝐮𝐝 𝐅𝐮𝐧𝐜𝐭𝐢𝐨𝐧.
18. Triggering the pipeline.
19. Receive hyper-parameters using 𝐡𝐲𝐩𝐞𝐫𝐩𝐚𝐫𝐚𝐦\_𝐠𝐞𝐧 custom python component.
20. Extract data from BigQuery using 𝐁𝐢𝐠𝐐𝐮𝐞𝐫𝐲𝐄𝐱𝐚𝐦𝐩𝐥𝐞𝐆𝐞𝐧  component.
21. Validate the raw data using 𝐒𝐭𝐚𝐭𝐢𝐬𝐭𝐢𝐜𝐬𝐆𝐞𝐧 𝐚𝐧𝐝 𝐄𝐱𝐚𝐦𝐩𝐥𝐞𝐕𝐚𝐥𝐢𝐝𝐚𝐭𝐨𝐫 𝐜𝐨𝐦𝐩𝐨𝐧𝐞𝐧𝐭..
22. Process the data using on 𝐃𝐚𝐭𝐚𝐟𝐥𝐨𝐰 𝐓𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦 𝐜𝐨𝐦𝐩𝐨𝐧𝐞𝐧𝐭.
23. Train a custom model with Vertex AI using the 𝐓𝐫𝐚𝐢𝐧𝐞𝐫 𝐜𝐨𝐦𝐩𝐨𝐧𝐞𝐧𝐭.
24. Evaluate and validate the custom model using  𝐌𝐨𝐝𝐞𝐥𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐨𝐫 𝐜𝐨𝐦𝐩𝐨𝐧𝐞𝐧𝐭.
25. Save the blessed to model registry location in Cloud Storage using 𝐏𝐮𝐬𝐡𝐞𝐫 𝐜𝐨𝐦𝐩𝐨𝐧𝐞𝐧𝐭.
26. Upload the model to Vertex AI using 𝐯𝐞𝐫𝐭𝐞𝐱\_𝐦𝐨𝐝𝐞𝐥\_𝐩𝐮𝐬𝐡𝐞𝐫 custom python component.
27. Model Deployment
28. Test model interface.
29. Create an 𝐞𝐧𝐝𝐩𝐨𝐢𝐧𝐭 𝐢𝐧 𝐕𝐞𝐫𝐭𝐞𝐱 𝐀𝐈.
30. Deploy the model to the endpoint.
31. Test the Vertex AI endpoint.
32. Use the Vertex AI endpoint for online prediction.
33. Use the Vertex AI uploaded model for batch prediction.
34. Run the batch prediction using 𝐕𝐞𝐫𝐭𝐞𝐱 𝐏𝐢𝐩𝐞𝐥𝐢𝐧𝐞𝐬.
35. Set 𝐬𝐤𝐞𝐰 𝐚𝐧𝐝 𝐝𝐫𝐢𝐟𝐭 𝐭𝐡𝐫𝐞𝐬𝐡𝐨𝐥𝐝.
36. Create a monitoring job for all the models under and endpoint.
37. List the monitoring jobs.
38. List artifacts produced by monitoring job.
39. Pause and delete the monitoring job.
40. 𝐌𝐞𝐭𝐚𝐝𝐚𝐭𝐚 𝐭𝐫𝐚𝐜𝐤𝐢𝐧𝐠 in GCP console.",LearnMachineLearning
r519uc,1638208065.0,What is most beginner friendly python machine learning library,I just wan to start with machine learning. I searched on internet for python library for ml. What is the best library for beginners?,LearnMachineLearning
r515bm,1638207724.0,iMerit ML DataOps Summit – TechCrunch,"Hear from top AI and ML leaders from Facebook AI, Microsoft, Cruise, GE Healthcare, and others revealing the latest trends in successfully deploying machine learning data operations at the iMerit ML DataOps Summit, co-hosted by TechCrunch.

𝐑𝐞𝐠𝐢𝐬𝐭𝐞𝐫 𝐟𝐨𝐫 𝐅𝐫𝐞𝐞: [https://tcrn.ch/3FK12bz](https://tcrn.ch/3FK12bz)
Join 1800+ attendees on Dec 2nd and gain insights on:

👉 Why human-in-the-loop data labeling is the critical path to achieving widespread production and adoption of AI applications

👉 Overcoming edge cases and leveraging high-quality proprietary data

👉 How leading AI companies are scaling their data pipeline

👉 The data labeling ecosystem and its future

𝐂𝐡𝐞𝐜𝐤 𝐨𝐮𝐭 𝐨𝐮𝐫 𝐬𝐩𝐞𝐚𝐤𝐞𝐫𝐬 𝐚𝐧𝐝 𝐫𝐞𝐠𝐢𝐬𝐭𝐞𝐫 𝐭𝐨𝐝𝐚𝐲: [https://tcrn.ch/3FK12bz](https://tcrn.ch/3FK12bz)
\#MLDataOpsSummit #machinelearning #robotics #ai #ml #artificialintelligence",LearnMachineLearning
r4yvl3,1638201566.0,Which is the best beginner book for machine learning?,Which is the best beginner book for machine learning with Python? Something that's not super rigorous but also doesn't gloss over details and covers almost everything for a beginner along with implementing machine learning algos.,LearnMachineLearning
r4yj1u,1638200581.0,Why is 1 the best value for Laplace Smoothing," Hello everyone,

I have applied Laplace Smoothing with k values lower and higher than 1 on my Naive Bayes classifier.

Comparing the accuracy and f1 scores, it is obvious that k = 1 is the best value for smoothing. I was wondering why? Would be grateful for any feedback.",LearnMachineLearning
r4wd3a,1638194166.0,How to illustrate in a fairly simple manner how RoBERTa gets fine tuned on a downstream task," My thesis defense is coming up next week, and I wanted to have your take on an issue I'm currently facing. One of my thesis contributions is ""Adapting RoBERTa to the task of rumor detection on Twitter""

**I want to explain to the jury how RoBERTa can adjust its weights based on the dataset that I fine-tune it on**.

In simple terms, I fed RoBERTa a variety of datasets describing the task of ""Rumor detection on Twitter"" while altering the class distribution in the datasets to see how it influences the embedding that RoBERTa produces. I evaluated the quality of the embeddings by feeding them to a set of classifiers (Random Forest, Decision Tree, SVM) to see how they perform. I used standard metrics (Precision-Recall and F1-score) focusing on the model performance in recognizing the class rumor. I was considering explaining it this way: RoBERTa takes in a tweet with a label (Rumor/non-rumor), then it weighs the words and their impact on the class in question. And words that occur often in a class are the ones that are potentially correlated to it. But I feel like that's too much watering down and even an insult to the intricacy involved in RoBERTa's inner workings. So for you out there with much more knowledge and expertise than me, will you please indulge my request and enlighten me on How one can explain the details of fine-tuning pre-trained language models on a downstream task.",LearnMachineLearning
r4um3h,1638188263.0,Can anyone recommend courses that will give the solid foundations of NLP?,"Guys, can you please recommend good NLP courses? any free?

 I did computational neuroscience/robotics masters and had a very solid computer vision course there,  that gave me all the basic knowledge ( like optics, matching, geometry, basic algorithms, etc ). Overall it provided a great overview of the field before we started on CNNs and I then moved to GANs myself.

Now I will need to use NLP techniques for the analysis and I am looking for similar courses that will give the basic info first.",LearnMachineLearning
r4txgv,1638185628.0,💊Your daily dose of machine learning : deploying Tensorflow models in C++,"> This is a series of posts that I post almost daily. I call them “your daily dose of machine learning”.

 In machine learning projects, there are several ways to deploy your final model. It could be cloud deployment, deployment on mobile devices, deployment on embedded systems, etc.


For this, you can leverage several programming languages depending on the tech stack you’re using in your day job or project.


In some companies that I worked for, we needed to deploy our deep learning models in a C++ environment. These models were mostly dealing with image classification and object detection.


I remember asking a question on stackoverflow almost 4 years ago on how I can deploy a tensorflow model in a C++ environment. I received only 2 answers in this 4 years period.


That’s when I first started looking into the different options of deploying tensorflow models in a C++ environment. I’ve mainly tried 3 options because they seemed the most promising:

1- Deploy the models using Tensorflow C++ api.
2- Deploy the models using OpenCV DNN module.
3- Deploy the models using ONNX Runtime.


Based on my experience, if you had to choose one of these 3 options, then go with ONNX Runtime. It’s a great tool and it offers a lot of flexibility when it comes to models deployment.


I’ll go a little bit into more details about this in upcoming posts.

 Follow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***",LearnMachineLearning
r4rxry,1638177262.0,Looking for teammates!,"Hi everyone! I am looking for teammates to participate in the kaggle competition: [https://www.kaggle.com/c/jigsaw-toxic-severity-rating/overview](https://www.kaggle.com/c/jigsaw-toxic-severity-rating/overview)


Please DM if Interested!",LearnMachineLearning
r4rd5h,1638174841.0,Object detection for handwritten signatures,"I'm trying to run object detection on pdf documents to recognize the signature position.

Do you know any pretrained model that can recognize signatures?",LearnMachineLearning
r4pjqu,1638167608.0,Tensorflow and --no-cache-dir in requirements.txt,I am trying to install tensorflow for my flask application. How do I write --no-cache-dir in requirements.txt file? Where should i write it?,LearnMachineLearning
r4oghg,1638163657.0,How to recap a whole TV series and movies using artificial intelligence?,"I am working on a college project where I decide I would do something like this but I have no vague idea how to do this, I have just started taking AI Ml Andrew Yang classes and I can't figure out how to do this. .can anyone help? Please",LearnMachineLearning
r4mlvk,1638157566.0,How do I get this basic LSTM example working?,"I've created a 'basic scenario' so I can see the effects further development has on my model. I've hit a snag when including an LSTM layer. As you can see with the code below, without the LSTM (and assuming the inputs are all 100% correct), the model produces 100% accurate forecasts. However when including the LSTM layer, the forecasts go way off.

Since the inputs all sum to the correct output, the model shouldn't ""learn"" anything and should output the correct answer all the time; so what have I done wrong here?

    import numpy as np
    import pandas as pd
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Input, LSTM
    from tensorflow.keras.preprocessing import timeseries_dataset_from_array

    x = np.arange(100)
    df = pd.DataFrame(index=x)
    df['input'] = x
    df['input_addition'] = 1
    df['target'] = x + 1

    X = df[['input', 'input_addition']]
    Y = df['target']

    max_bars_back = 1
    batch_size = 2

    train = timeseries_dataset_from_array(X.iloc[0:40].values, Y.iloc[0:40].values, max_bars_back, batch_size=batch_size)
    validation = timeseries_dataset_from_array(X.iloc[40:70].values, Y.iloc[40:70].values, max_bars_back, batch_size=batch_size)
    test = timeseries_dataset_from_array(X.iloc[70:].values, Y.iloc[70:].values, max_bars_back, batch_size=batch_size)

    model = Sequential()
    model.add(Input(batch_input_shape=(batch_size, max_bars_back, len(X.columns))))
    # model.add(LSTM(1, kernel_initializer='ones', return_sequences=True))
    model.add(Dense(1, kernel_initializer='ones'))
    model.compile(loss='mse', optimizer='adam', metrics=['mse'])
    model.fit(train, epochs=3, validation_data=validation, shuffle=False)

    test_index = Y.index[70:]
    outputs = pd.Series(index=test_index)
    predictions = model.predict(test)
    for x in range(len(predictions)):
        outputs.iloc[x] = predictions[x][0][0]

    test_df = df.loc[test_index, :]
    test_df['outputs'] = outputs
    test_df",LearnMachineLearning
r4it3w,1638145989.0,Are there any examples of semantic search being leveraged at scale?,"I've been doing some research and I haven't been able to find any resources outside of FAISS prototypes. It has me wondering if this is something that is used at scale?

Any examples or sources would be much appreciated.",LearnMachineLearning
r4hhs3,1638142171.0,Looking for study partners for Applied Predictive Modelling,"Hi aspiring data scientists,

Applied predictive modelling is a great book for learning the ML foundations that are used in industry. I've looked into other books such as Elements of Statistical Learning and they don't quite have the application focus as this one. It's the top rated book from this r/datascience  [book suggestion thread](https://www.reddit.com/r/datascience/comments/8jneyb/ds_book_suggestionsrecommendations_megathread/).

I'm here to help keep you accountable to your learning goals, discuss content and exercises, and understand the content together. We'll be meet up on a discord server and having discussion once a week. Let me know and I'll pm you a link.

If your goal is to find a data scientist role, you'll be in a great position to find work 3 months from now. There's 12 chapters with exercises and the aim is to complete 1 chapter per week. If you don't have the book, I'm happy to hook you up.

Cheers.",LearnMachineLearning
r4h993,1638141487.0,Sentiment Analysis Ideas,"Hi all,

I'm working on a final project for one of my classes involving sentiment analysis on a data set of IMDB movie reviews (data set courtesy of keras). It's a fairly straightforward binary classification (classify the review as positive or negative). The thing is, I'm a little short on ideas with regards to how to accomplish this. I've already utilized a few models but I feel like they're a little simple, and in the interest of getting into the spirit of things I was wondering if there were more advanced techniques that a novice like me could still use.

For what I've already done (all inputs are word embeddings of the dataset imported from keras):

1. CNN
2. LSTM
3. Transformer
4. CNN-LSTM
5. LSTM-SVM

A few things I've been looking at but not sure about implementing:

1. Generative-discriminative models (the generator would perform feature extraction on the data)
2. Data augmentation techniques to use on the data and then feed into the aforementioned models (thus far they've been fairly simple, like swapping in synonyms or antonyms, randomly deleting or adding words, etc.)

Any recommended course of action/source for the two ideas I've been looking at? And do you have any other suggestions that I could feasibly implement? I'm using Google Colab (got a premium membership) and the data set isn't very large (25,000 training samples) so computational expense should not be a concern. I'd appreciate any suggestions you guys have!",LearnMachineLearning
r4eumq,1638134836.0,unable to download CASIA-IrisV4 dataset,"Hello everyone,

I have a project to train my model on the Iris dataset but I have a problem downloading CASIA-Iris v4. to download it seem we must sign in, but I get the error.

[the link](https://hycasia.github.io/dataset/casia-irisv4/)

could someone guide me on how to download it?

Thanks",LearnMachineLearning
r4aio0,1638123302.0,Model to predict sales to different customers,"Hi,

I would like to build a model for predicting the sales of certain products to approx 100 different customers. I have to predict the montly sales to the more important customers and the total volume sold that month.  I have the time series of the  volumes sold each month to each group of customer in the past 10 years and they show a very seasonal pattern. Unfortunately many customers became such only in the past few years and they do not place orders each month so the timeseries have many zeros.

I initially thought about adding some autoregressive terms (maybe considering all of them with an exponential decay), a categorical for the seasonality  and I also tried to cluster the group of customer  according to the correlation of their timeseries and use the clusters as categorical variables, but the dendrogram shows that many timeseries are clustered only at a very low correlation, making the clustering almost useless. I don't really know what to do

How do you suggest to build the model?",LearnMachineLearning
r4aaj9,1638122710.0,Suggestions on applying machine learning in structural health monitoring or mechanical fault prediction,"I am a mechanical engineer working for a reputed organisation. In recent years, I am being asked to take up projects related to machine learning and apply the concepts in fault prediction and similar things. I have a very rudimentary idea of ML algorithms and had used sklearn some 4/5 years back. Can anyone suggest how to apply ML in fault prediction field? Any books, tutorials or suggestions will be really appreciated.",LearnMachineLearning
r489w8,1638117348.0,Resources on Sequential Pattern Mining in R,"I'm looking for good resources on how to apply the Sequential Pattern Mining on my App data, specifically in R. The most common package for the job is apparently arulesSequences and the cSpade algorithm. I found [this](https://www.r-bloggers.com/2019/02/tutorial-sequential-pattern-mining-in-r-for-business-recommendations/) blog post and am basically looking for something similar. All help is appreciated. Thanks in advance.",LearnMachineLearning
r47lvu,1638115577.0,Looking for beginners to try out machine learning online course,"Hello,

I am preparing a series of courses to train aspiring data scientists, either starting from scratch or wanting a career change (for example, from software engineering or physics).

I am looking for some students that would like to enroll early on (for free) and give me feedback on the courses.

The first course is on the foundations of machine learning, and will cover pretty much everything you need to know to pass an interview in the field. I've worked in data science for ten years and interviewed a lot of candidates, so my course is focused on what's important to know and avoiding typical red flags, without spending time on irrelevant things (outdated methods, lengthy math proofs, etc.)

Please, send me a private message if you would like to participate or comment below!",LearnMachineLearning
r46h3o,1638112504.0,A quick introduction to Pruning and how to apply it to your model,"Hey everyone,

I wrote [a short article](https://www.sicara.ai/blog/computer-vision-pruning) about Pruning and thought it might be useful to this community.

I hope you will find it helpful and am looking forward to hearing your feedback!",LearnMachineLearning
r4639a,1638111483.0,Checking understanding of neural networks,"Hi, I just wanted to check whether I have actually understood how a neural network works.

So you first initialise a network with an input layer and some hidden layers and an output layer each with some nodes (is that the right word?  should I use perceptron or neuron instead?). The input layer takes the weighted sum of the inputs and adds a bias. Weights and biases are usually random at first. This is then run through an activation function (usually sigmoid) which represents the probability that the neuron is active. The input is fed forward through the network in this way and an error is calculated at the end.

After that, backpropagation is used to adjust weights and biases and then the input is fed forward and another error is calculated. This way, error reduces over time.

This network can then be tested on test data.

How accurate is this description? Also, how is this different to a CNN or DNN?

Thank you.",LearnMachineLearning
r43jhv,1638103510.0,Deep Learning with Python: Good for absolute beginners in ML ?,"Hello everyone,

I will keep this post short. Basically, I want to get into deep learning for NLP, but I have absolutely no background in higher mathematics or ML, AI or DL whatsoever. I am an intermediate python programmer with basic skills in other programming languages.

I would like to now if this [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) would be a goood start for me or would it be better to go for the standard [Machine Learning with Andrew Ng](https://www.coursera.org/learn/machine-learning#syllabus)?

Thanks so much for your support and sorry for the newb question.",LearnMachineLearning
r40m4u,1638092103.0,I’m about to start producing CircuitMess Batmobile - a DIY robot car that teaches kids about STEM sciences,"Hi everyone,

As you may remember, my name is Albert, and I'm a 22-year-old DIY electronics enthusiast who also happens to be a huge Batman fan. 🦇

Earlier, I have posted here about CircuitMess Batmobile, a project I have developed after getting the license from Warner Bros and which is aimed to teach kids technologies of the future (such as AI, machine learning, computer vision, and more).

I have received really nice feedback from the people in this group, so I wanted to notify you all that my Batmobile is now funded and that I will officially start producing this kit soon. ✨

The crowdfunding campaign will be live for 5 more days, meaning that you can still pre-order your Batmobile or support the project if you find it interesting. Here’s the link to the listing, where you can also get more info about Batmobile’s software and hardware:

[https://www.kickstarter.com/projects/albertgajsak/circuitmess-batmobile?ref=535kx4](https://www.kickstarter.com/projects/albertgajsak/circuitmess-batmobile?ref=535kx4)

Thank you all for your support so far!

https://preview.redd.it/lpvtitdb3b281.jpg?width=8242&format=pjpg&auto=webp&s=d0f7f24672a3544aec128e7f2d9abd212c77955e",LearnMachineLearning
r3xwru,1638081291.0,K-means high cluster count,"If I use high cluster count than what suggested by elbow method in kmeans, it may lead to some extra data analysis..but will it lead to mixing of data points of one cluster into another to a high extent? (I am using 200 clusters instead 50 as suggested by elbow method as high % of data was left unclustered)",LearnMachineLearning
r3rzms,1638061800.0,Determining Loss Function For CNN Autoencoder,"I'm making an autoencoder to compress audio, and using  `librosa.stft`s to make them images. I chose adadelta optimizer and binary\_crossentropy loss based on [this](https://www.tensorflow.org/tutorials/generative/autoencoder) tutorial by tensorflow, but when I train, the loss becomes a pretty big negative number (somewhere in the -100ks). I know something is wrong, but I'm not sure how to fix it or even why this is happening. I included the relevant model code below for more info.

    class Autoencoder(Model):
      def __init__(self):
        super(Autoencoder, self).__init__()
        self.encoder = tf.keras.Sequential([
          layers.Input(shape=(1024, 1024, 1)),
          layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),
          layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2),
          layers.Conv2D(4, (3, 3), activation='relu', padding='same', strides=2)
        ])

        self.decoder = tf.keras.Sequential([
          layers.Conv2DTranspose(4, kernel_size=3, strides=2, activation='relu', padding='same'),
          layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),
          layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),
          layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')
        ])

      def call(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

    autoencoder = Autoencoder()

    autoencoder.compile(optimizer='adadelta', loss=""binary_crossentropy"")",LearnMachineLearning
r3odcu,1638051244.0,Mobile app for machine learning deployment?,"Is there a package like streamlit, which have pre-made templates for mobile application for convenient creation of machine learning apps? If any please suggest",LearnMachineLearning
r3ldxf,1638042734.0,Embedding a ml model into a microcontroller to enlarge the functionality of a thermal station.,"Hi guys, i recently built a sensor station that monitors thermal comfort in indoor spaces. The monitoring station has 3 sensors and 1 microcontroller of the type esp32. Now I would like to enlarge the functionality of this station with ml. But since I am a beginner i am kind of confused what to do here. Any suggestions how to go about it?
Thanks in advance.",LearnMachineLearning
r3j9ps,1638036830.0,Are there any Simple hosted MLOps or auto scaling solutions?,"

I’m looking to deploy VQGAN+CLIP models at a pretty large scale (30+ GPUs) and I want to explore options other than Kubernetes on AWS or GCP.

Is there a hosted MLOps platform where I can simply upload my ML Service (docker + flask) and the platform can completely take care of scaling and GPU provisioning by itself?

Essentially outsource scaling up/down and handling traffic to a product or service with me having to do minimal setup",LearnMachineLearning
r3j0d7,1638036117.0,Courses/Books/PDFs on how to take action from EDA?,"I haven't done any ML in a while and I've gotten pretty rusty because of it. I started a new project and I'm just totally blanking on what to do with my data in response to the EDA I've conducted.

Could you fine people recommend some resources that cover some actionable steps in response to exploratory data analysis?

edit: lol downvoted? is EDA not a crucial part of learning ML? just throw the data in and giddy up? alrighty!",LearnMachineLearning
r3hb3j,1638031366.0,Is there any overview of what specific layers/groups of layers do?,"I'm in a strange situation: I'm working on a program that should help people to learn about neural networks by showing pretty much everything I can that goes on inbetween like the outputs of layers (visualized if they look like images), the data that goes through the layers and so on, while I'm not very experienced with creating NNs myself.

I know this is a strange situation: writing a program to help people to understand NNs without understanding them myself. But this is just how it is now.

I do this with TensorFlow.js, so people can do it in the browser without having to write any code. You can think of my program as a GUI for TFJS in which you can drag&drop layers around and that automatically generates graphs like http://alexlenail.me/NN-SVG/AlexNet.html , the corresponding python-code and the tf-model in the browser and so on for example from the structure of the network in the GUI. This part works fine by now, so people who know about NNs can already use my program and train the network in the browser.

What I want to add is something like this (this is a real, albeit modified screenshot of my working program prototype):

https://imgur.com/Vgtbj2w

My question is: is there any overview of what ""groups"" of layers (like some Conv2d's followed by a MaxPooling or something) do, so I can add bracket-notations like that one that make it easier for people to understand what's going on?

Since data can be very diverse, I'm aware that no single way of describing thouse groups will be correct all the time. But I'd be fine when it's correct in *most* ""default"" use cases.

I'd really appreciate any links and as I've said, please assume I have no idea what NNs really are, because except for a very ""high level"" description of them, I don't.",LearnMachineLearning
r3h9rp,1638031260.0,On what filter does the CONV layers operate if the previous CONV output filter is less than the current?,"Hi guys,

I have a question about filter and COnv layer. For the sake of clarity, let's take a look at this simple model:

`input = layers.Input(shape=(224,224,3))`
`layer1 = layers.Conv2D(64, kernel_size=3)(input)`
`layer2 = layers.Conv2D(256, kernel_size=3)(layer1)`
`model = keras.Model(input, layer2)`
`model.summary()`

whose output is

`Layer (type)                 Output Shape              Param #`

`=================================================================`

`input_1 (InputLayer)         [(None, 224, 224, 3)]     0`

`_________________________________________________________________`

`conv2d (Conv2D)              (None, 222, 222, 64)      1792`

`_________________________________________________________________`

`conv2d_1 (Conv2D)            (None, 220, 220, 256)     147712`

`=================================================================`

`Total params: 149,504`

My question are:

1. The CONV layer 1 has as output 64 filter. The image that take as input has 3 channels. Every filter works on these 3 channel and than merge together? Or some filters work on the first channel, other in the second, and so on? If so, how the network choose how to assign filters to  channels?
2.  The layer 1 has, as output, 64 channels. The layer 2 has as output 256 channels and as input 64. Basically, the question is the same as before. An example: which channel take into account the first filter of the layer 2, i.e conv2d\_1?

Thanks for the answers",LearnMachineLearning
r3gz0z,1638030394.0,Making Synthetic Render of small Electronics more realistic using real pictures with DCGAN,"Hi,

This is my first big dive into Machine Learning, and the first real big project I am doing. I am stuck at this stage, however. I have leaded the synthetic dataset and real dataset and the GAN is taking in those inputs. However no matter how long I run the GAN for, the final image seems grainy and while the resolution is getting better it is far from a normal shape.

The synthetic images I have are renders of an object from very different angles, and the real images are basically pictures of the irl object from multiple positions (taken at fixed angles 30/60/90 etc).

Currently, I have just put all real images in a folder and all synthetic images in a folder without worrying about the position and am wondering if that is the problem.



There are a few things that I can do here but would love some more input from you guys.

1. Label the real dataset so, all the images taken from the same position will be together. This however confuses me as it would make my dataset smaller and also I do not exactly know how to use all these positions together since the synthetic dataset positions do not correspond to the real dataset.
2. I have been reading about CycleGans and other Reddit posts suggest moving to that.
3. Something other than GANs?

Please feel free to ask any questions if need be. I am properly stumped here and except Apple's SIMGan paper, I have no clue where to go. The common celeb face example used in the tutorials is good but all the headshots are from the same angle, unlike my electronics.",LearnMachineLearning
r3fd4m,1638025571.0,Specifics on Random Forest Decision Tree prediction step.,"

Greetings all, my first post on this subreddit. I'm a PhD that wants to use ML for various tasks for my future career, so I figured I would dig into various ML algorithms and code them up! That is to say, I might be posting more in the future so I'm grateful this page exists.

Currently I am trying to code the algorithm for Random Forests after completing the Decision Tree algorithm, and I am having trouble wrapping my head around the ensemble prediction step.

So as far as I understand, we take some data, create multiple subsets of that data and then we create decision trees from that data. simple enough. But then we are supposed to make predictions based on the decision tree model created. My question is this: How do we do the prediction step? What data are we supposed to test the individual decision trees?

My impression is that we make one prediction for each decision tree, but my naïve understanding is making me think this is not the proper way to build the ensemble model. Can anyone give me a thorough explanation of how we aggregate the decision tree outputs? Thanks in advance!",LearnMachineLearning
r3esql,1638023819.0,[D] (Paper Overview) Florence: A New Foundation Model for Computer Vision," **Video**

[**https://youtu.be/QNOTBMPecKM**](https://youtu.be/QNOTBMPecKM)

**Paper**

[**https://arxiv.org/abs/2111.11432**](https://arxiv.org/abs/2111.11432)

**Abstract**

Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.",LearnMachineLearning
r3elk4,1638023172.0,Tips for interesting school project ideas?,"Maybe you read an interesting paper or have done something before that you can recommend. I’m familiar with PyTorch and Tensorflow so it doesn’t have to be beginner level, but it shouldn’t be too advanced so that you can’t build it from scratch and finish it within a month.",LearnMachineLearning
r3cvu4,1638017192.0,"Do you have difficulty keeping up with new research and applications in AI / ML, and your ""to-read"" list just gets bigger? Easy fix!","Do you have difficulty keeping up with new research and applications in AI / ML, and your ""to-read"" list just gets bigger?

Easy fix: Follow my channel or blog where I explain new applications/papers in short videos and articles weekly!

I hope it can be useful to you and let me know your thoughts on the content so I can improve and better answer this need!

[https://youtube.com/c/WhatsAI](https://youtube.com/c/WhatsAI)",LearnMachineLearning
r3bs4u,1638012720.0,How video of taking down for shooting game can extract?,"I am making a system. It makes a montage of 'VALORANT'. 'montage' is videos of good playing in games. In other words, taking down to other players in a row.

At first, I attempted to use CNN. Cutting video of playing the game myself as training data, and cutting at each frame.

But, I think there's a way of something better. Cutting data of about 100GB is too much hassle.

I am a beginner in Machine Learning. Maybe, having way I don't know.",LearnMachineLearning
r3bi9l,1638011626.0,A problems with Gaussian processes: with or without quaternions?,"A problem involving Gaussian processes: should I use quaternions or not?

I am researching a solution to a problem that requires the use of Gaussian optimization. I am looking for the rotation of a 3D object that maximize an arbitrary score. The space of the problem is a unit quaternion, and the score is a real scalar for each possible rotation.

Do you think it would be better:

1. To use a Gaussian process H -> R

2. To use a Gaussian process R^3 -> R with some cyclic kernels such that there is no discontinuities and the space wraps around the unit sphere?

I saw a lot of papers using dual quaternions, but translations are not required in my case, I just need to find the way to rotate my object to maximize the score. I don’t mind if I were to get Euler angles instead of a quaternion, I am just uncertain about the best way to go. Quaternions feel more theoretically beautiful, but I am not confident in my abilities to implement and combine Gaussian processes with 4D complex numbers.

I hope I gave enough information in this post to give relevant answers, otherwise feel free to ask more questions and I will edit the post.",LearnMachineLearning
r3ab0x,1638006488.0,does anyone have the answer manual for THE PRACTICE OF COMPUTING USING PYTHON,"would be much appreciated


thanks in advance =)",LearnMachineLearning
r3a0dx,1638005209.0,Is AI&ML hard to learn?,"I applied for Artificial intelligence & machine learning course in my college nd im stressed coz of this.. everyone says machine learning is tough , difficult.

 I want to know basics about this course , so that I can motivate myself to learn more

Hope I can something from u guys !",LearnMachineLearning
r36vlk,1637992882.0,Could someone help me understand how parameter estimation and finding optimal predictors contributes to the big picture of a simple ML model like polynomial regression please?,"I’m wondering how all of these things fit together. I’m taking my first ML course in university right now and it’s basically a math course to provide the foundation of the calculus, probability/statistics and linear algebra involved in ML. I feel like I don’t have a good understanding of how some of the sections relate to each other yet though.

For example I know about parameter estimation and how we can use MLE, MAP, Bayesian etc. to estimate parameters for different distributions.

I know we can create an optimal predictor by minimizing the expected value of the cost function. For example the optimal squared error cost function being the conditional expectation E[ Y | X = x ].

But I’m not sure how this fits together with something like polynomial regression and how to use all of these things in the model, how it relates to fitting the data, how the coefficients for the polynomial are found, etc.

Could someone please explain how this all fits together. Starting from the initial data matrix, how do these things come together to create a final model that is able to make predictions?

Thank you in advance!",LearnMachineLearning
r31m4v,1637975554.0,"After generating a .weights file from training, how do I check it's accuracy?",I trained a model to recognize an object using yolo and the model generated a .weights file. How do I validate the accuracy of the model now with the .weights file?,LearnMachineLearning
r2x9eg,1637962144.0,Linear Regression question + code,"I am trying to implement simple linear regression but the code explodes when the independent variable(x) gets to high ranges. Here is the code:


https://preview.redd.it/epmccq97c0281.png?width=671&format=png&auto=webp&s=dcbf6ac29de530c99b5eef19e2c9443d52371bb2

It works for small data sets but when I try to scale it to anything moderately large like 1000  data points on the x axis the weight goes to nan.

Actually, this kind of seems like it makes sense based on the math tho - let's say we have two samples with equal error, one at x=1 and another at x=1000. If the linear model is off by 1 for both then the gradient of the loss is equal (2\*(y - y\_pred) \* -1 ---> 2\*(1)\*-1) but the gradient wrt to the weight is wayyy bigger on the large x. e.g. 1 for x=1 and 1000 for x=1000. What am I missing here?",LearnMachineLearning
r2trr7,1637951775.0,Best way to learn AI/ML? Help with my current path.,"Hey guys I'm a few months into my coding journey. I know Python rather well and am now midway through the first course on [fast.ai](https://fast.ai/).

My career goal is to be able to work with and develop artificial intelligence. However, I am in need of a job as soon as possible. And since I know that I will not be a master in artificial intelligence, I know that I will likely need to get into a similar field as I work on my skills.

My question is, what should I learn in order to get a job in as close of a field as possible to AI/ML? I'm pretty sure data science is not too far away from AI, however I'm sure the barrier to entry is close to as high as AI.

I was thinking about taking some courses on SQL and possibly software engineering to further my resume, but I am unsure if that is the correct route to take. I know that many here have much more experience than I do in the field of computer science as a whole and would thus love to hear from you guys!

Thanks in advance.",LearnMachineLearning
r2t541,1637949958.0,How would I train a bot to analyze movie scripts and then make its own?,"I’ve been researching these bots that analyze a series of movie scripts or text (i.e [College Humor](https://youtu.be/BjngNWP9C5s), [infomercials](https://youtu.be/9MFpEVrt2a8), [a syllabus](https://youtu.be/2HcServj7HI), etc) and then with the data collected they make their own. I was curious on what would it take to make one. I have a pretty good understanding of programming and I thought Python would be the best language to write something like this. I thought some of the results would make for a pretty funny project. However, I don’t know where to start. Any sources or tutorials on how to begin? Thanks!",LearnMachineLearning
r2rw37,1637946375.0,What should I visualize for humor detection model to gain some useful insight?,"I was going through bunch ([1][1],[2][2],[3][3]) of humor detection paper. But most papers don't include any visualizations, say some graph related to model being trained. I was thinking to train some language models like BERT, GPT, XLNet. But was guessing what kind of some interesting visualization should I aim for in order to gather the data during training and gain some sort of insight.

Or is it like that these fine-tuning or zero/one/few shot learning based models don't have to train for long and does not involve significant learning ""from scratch"" or they are somewhat black boxes, that's why there is nothing much to visualize?


  [1]: https://cs224d.stanford.edu/reports/OliveiraLuke.pdf
  [2]: https://arxiv.org/pdf/1909.00252.pdf
  [3]: https://arxiv.org/pdf/2004.12765v5.pdf",LearnMachineLearning
r2ruzj,1637946292.0,How to properly structure training data?,"Hello

I am trying to fit a neural network with training data, but keep getting errors like this when reaching the fit function:

    ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).

I don't understand why I get this error when fitting. Because as you can see [from the official Keras documentation here](https://keras.rstudio.com/reference/fit.html), the fit function accepts lists and arrays. I have tried to pass both but always get errors.

Passing lists:

    ... gathering all trainig data ...

    model = Sequential()
    model.add(Embedding(DICTIONNARY_SIZE, 120, input_length = NR_OF_TRAINING_VECTORS))
    model.add(SpatialDropout1D(0.4))
    model.add(LSTM(176, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dense(1,activation='sigmoid'))
    model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])

    print(""type: "" , type(trainingInput))
    print(""type[0]: "" , type(trainingInput[0]))
    print(""type[0][0]: "" , type(trainingInput[0][0]))

    print(""\n "")
    print(""type labels: "" , type(trainingLabels))
    print(""type labels[0]: "" , type(trainingLabels[0]))
    print(""type labels[0][0]: "" , type(trainingLabels[0][0]))


    model.fit(trainingInput, trainingLabels, epochs = 5, batch_size=NR_OF_TRAINIG_VECTORS, verbose = 'auto')

prints:

    type:  <class 'list'>
    type[0]:  <class 'list'>
    type[0][0]:  <class 'int'>


    type labels:  <class 'list'>
    type labels[0]:  <class 'list'>
    type labels[0][0]:  <class 'int'>
    ...
    ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).


I have tried converting the data like this as well:


    model = Sequential()
    model.add(Embedding(DICTIONNARY_SIZE, 120, input_length = NR_OF_TRAINING_VECTORS))
    model.add(SpatialDropout1D(0.4))
    model.add(LSTM(176, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dense(1,activation='sigmoid'))
    model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])

    trainingInput = tf.constant(trainingInput, dtype=tf.float32)
    trainingLabels = tf.constant(trainingLabels, dtype=tf.float32)

    print(""type: "" , type(trainingInput))
    print(""type[0]: "" , type(trainingInput[0]))
    print(""type[0][0]: "" , type(trainingInput[0][0]))

    print(""\n "")

    print(""type labels: "" , type(trainingLabels))
    print(""type labels[0]: "" , type(trainingLabels[0]))
    print(""type labels[0][0]: "" , type(trainingLabels[0][0]))

    model.fit(trainingInput, trainingLabels, epochs = 5, batch_size=NR_OF_TRAINIG_VECTORS, verbose = 'auto')

But this leads to the following error:

    ValueError: Can't convert non-rectangular Python sequence to Tensor.


Lastly I have tried giving all my input training vectors the same length. Because the vectors are a series of integers and have varying lengths:

    for i in range (NR_OF_TRAINIG_VECTORS):
        trainingInput[i] = trainingInput[i][:10]

    model = Sequential()
    model.add(Embedding(DICTIONNARY_SIZE, 120, input_length = NR_OF_TRAINING_VECTORS))
    model.add(SpatialDropout1D(0.4))
    model.add(LSTM(176, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dense(1,activation='sigmoid'))
    model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])

    print(""type: "" , type(trainingInput))
    print(""type[0]: "" , type(trainingInput[0]))
    print(""type[0][0]: "" , type(trainingInput[0][0]))

    print(""\n "")

    print(""type labels: "" , type(trainingLabels))
    print(""type labels[0]: "" , type(trainingLabels[0]))
    print(""type labels[0][0]: "" , type(trainingLabels[0][0]))

    model.fit(trainingInput, trainingLabels, epochs = 5, batch_size=NR_OF_TRAINIG_VECTORS, verbose = 'auto')

But this again leads to issues:

    type:  <class 'list'>
    type[0]:  <class 'list'>
    type[0][0]:  <class 'int'>

    type labels:  <class 'list'>
    type labels[0]:  <class 'list'>
    type labels[0][0]:  <class 'int'>

    ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).


At this point I am not quite sure anymore of how I have to structure my vectors in order to not have issues any more.

What do I have to do at this point?",LearnMachineLearning
r2rn3d,1637945679.0,Understand Tensorflow train output/understand batch size parameter,"Hello,

I noticed that when changing the batch size in train\_model(), the status number changes as follows for my dataset:

batch size 10:

Epoch 1/10

552/552 \[==============================\] - 9s 11ms/step - loss: 0.3074 - val\_loss: 0.0041

batch size 20:

Epoch 1/10

276/276 \[==============================\] - 6s 12ms/step - loss: 0.3802 - val\_loss: 0.0107

&#x200B;

So far I thought the train function will use 10/20 from the train sample and do that 10 times. But now seeing the numbers 552 and 276 being anti-proportional to batch\_size, I think I got that wrong :D

Is it in reality so that the train\_model() slices up the train set into batch\_size pieces and trains ALL of them per epoch? And the number gets smaller because now it can parallelize better and need less steps to go through them?",LearnMachineLearning
r2przt,1637940169.0,Where can I get data related to company's cost of equity capital,"I have a list of company identifiers (GVkeys, sic, permno, etc) and need the associated cost of equity capital. I don't mind which method is used to estimate the cost of equity capital. I do have access to standard databases like compustat, CRSP, IBES, AUditanalytics, eventus, etc",LearnMachineLearning
r2p2vo,1637938153.0,Are derived or computed inputs bad for CNNs?,"I am building a CNN and am wondering if inputting derived or computed inputs are generally bad for the effectiveness of CNNs? Or just NNs in general?

By derived or computed values I mean data which is not ""raw"", and instead is computed based on the raw data. For example, in a very simple form, using timeseries data as the ""raw"" data and computing a 30 day SMA as a ""derived/computed"" value, and as another input.

Is this bad practice at boosting the networks effectiveness? If it is not bad practice, are there any tips at what kind of computed values someone should consider when adding new inputs?

The goal of my NN is for building predictions in timeseries data. (Sorry if this is a newb question -- I am indeed new to ML)",LearnMachineLearning
r2ouvr,1637937487.0,How large should my dataset be ?,"Hey guys!

I'm an AI student and I'm new to deep learning. I want to work on Breast Cancer detection using cnn or other architecture. I found an ultrasound imaging dataset which contains 437 benign , 210 malignant and 133 normal images ( each image with its mask) . Do you think that this is a sufficient amount of datat to train , validate and test my model ?

Thanks in advance :)",LearnMachineLearning
r2o6jr,1637935478.0,Need Help with project,"So as a part of my final year project.I've decided to do a project of leaf disease detection using image of leaf.I've just started studying machine learning for few months and only know some basics.The problem is i've searched for papers and tutorials for this kind of project and they all use CNN but my teacher hasn't allowed me to use CNN since i've just started into machine learning . he says i need to use traditional methods like random forest,svm etc.
So can anyone help me which method should i use and provide a basic roadmap for my learning.",LearnMachineLearning
r2o0mu,1637934973.0,Help withML class project,"Hello,

I’m currently conducting a research study for a class project. I’m examining employees’ attitudes toward telecommuting (i.e., work from home), and how we can use machine learning to predict that. The survey will take approximately 5-10 minutes to complete, so if you have some time to spare, please help me fill out the google form survey below. No identifiable information will be collected. Also, if you know others who might be interested in taking this survey, please feel free to forward this survey link to them. Thank you!

[https://forms.gle/5mxGsWvzr4Hh6UPJ6](https://forms.gle/5mxGsWvzr4Hh6UPJ6)",LearnMachineLearning
r2njr1,1637933465.0,What model should I use for this simple animation generator?,"I've been getting into computer vision recently, and I got this idea to make the simplest possible video generator like [this one](https://youtu.be/4GRnqqaZOHw).

I'm still thinking if I should make it as like 1 input frame and output is 3-sec animation (predict next frame).

Also, I know that for example GANs look good but are complex and I may not need them.

I'm not really sure if I should use a GAN (StyleGAN, BigGAN, Pix2Pix,...)  or an autoencoder (VAE,...)or something else?",LearnMachineLearning
r2lfjf,1637927348.0,What exactly is a machine learning algorithm?,"Hey guys, I'm just beginning to learn about machine learning, and there is something I am confused about. I am confused about the usage of the word ""algorithm"" in the context of machine learning, because as with what I learned, an algorithm means a step-by-step instructions to perform a task that takes some input and produces an output. In machine learning, for instance, a linear regression algorithm (model?) is just some linear function that is fitted to some dataset in a graph, based from my understanding. Why is it called an ""algorithm""? Can you guys eli5 me with that? Thanks!

Update: Found an [article](https://medium.com/@raosrinivas2580/how-does-the-ml-algorithm-differ-from-the-traditional-algorithm-b7c3a2799e10) that helped me further clear up the confusion. You can check it out, it might help you too. :D",LearnMachineLearning
r2gm3v,1637907524.0,Can I separate out the steps of learn() in stable baselines3?,"I'm working on a project where two  agents train simultaneously, but each agent only sometimes needs to make  a decision. Is it possible to have code that follows roughly the  following structure:

    model = A2C(""MlpPolicy"", env, verbose=1, learning_rate=0.0005)
    obs = env.reset() for i in range(2000000):
         action, _states = model.predict(obs)
         obs, rewards, dones, info = env.step(action)
         model.update_from_experience(obs, action, reward) #Does this type of function exist?

I'm also not married to  stable baselines to if there's a way to do this in another library that  would also be greatly appreciated.

Thanks!",LearnMachineLearning
r2fjja,1637903646.0,cheese cake doge,"&#x200B;

[https:\/\/i.imgur.com\/mTQ3P7P.gif](https://i.redd.it/p2hqw7mbjv181.gif)

I just implement optimization based CLIP + StyleTransfer

Open it in

* [replicate](https://replicate.com/mistake0316/style-transfer-clip)
* [github](https://github.com/mistake0316/CLIPStyleTransfer)",LearnMachineLearning
r29dv7,1637882941.0,Analysis of micro-macro data,"I am trying to run some analyses on data with a somewhat peculiar structure and I am hoping that someone on this sub can help me identify relevant resources.

I have nested data where approximately 10,000 repeated measurements are nested within 400 individuals (some individuals have one observation whereas others have hundreds of repeated measurements). The ""weird"" part is that the outcome (a binary outcome variable) is at the level of the individual (i.e. I have 10000 rows of data but I am only trying to predict values that varies across 400 unique individuals).

My initial research into this suggested that I can just make sure that my test train split prevents any one individual from contributing observations to both the train and test data.  I tried this with random forest (using all 10000 rows) and obtain near perfect prediction for my training set (AUC near 1) but not for my training set (AUC of .60).  This remained true after iterating through varied hyperparameter values. Of note creating a test and train split while ignoring dependence between observations leads to great performance across both datasets but I don't trust it one bit.

Next I tried to average my features such that my data now only contained one row per person (it's heart breaking to see all of this data disappear). I now get random forest that performs about the same on test and training sets (AUC of .60).

I want to continue experimenting with other models or perhaps some feature engineering to see if I can improve prediction metrics. However, I am not sure which approach I should prioritize when experimenting.

Evaluating whether the model generalizes independent cases feels absolutely right (this is what I would want the model to be used for if it is ever rolled out in the real world). Getting similar performance estimates between my train and test data also make me think that anything I learn during tuning will generalize to my training set.  However I wonder if I can do more with my repeated data than compute means for each person/feature. Also, ICCs are all over the place for my features which would suggest that, for some features, there isn't much individual-level variability for me capture reliably.

I was able to find a paper that suggests a latent variable approach with this data structure (https://scholar.google.com/scholar?cluster=6527582051658515903&hl=en&as_sdt=40000005&sciodt=0,22#d=gs_qabs&u=%23p%3Dv0VSl0qhlloJ).

Is anyone on this sub aware of a similar ressource pertaining to applying machine learning to a comparable data structure.  Surely there must be something out there but I am failing to find it. Even just reading a paper that seems to successfully deal with this issue would be helpful. Thank in advance!

Oh and happy Thanksgiving!!  🦃",LearnMachineLearning
r27jbb,1637877462.0,Do you use data augmentation online or offline?,"Hi there!

Do you use data augmentation (in image based tasks) online or offline?

What I mean by this is, do you augment your images at the time of training or do you augment them before the training and then you use the final augmented dataset for training your model?

Why one and not the other?

I've been experimenting with both and it seems like both approaches have some advantages and disadvantages.

I would love to hear your take on this, especially if you're working on projects in the industry.",LearnMachineLearning
r277kg,1637876499.0,"Supervised, Semi-Supervised, Unsupervised, and Self-Supervised Learning","When I first began learning machine learning, I had difficulty understanding what exactly supervised and unsupervised learning are. I wrote an article describing my understanding of them, with the addition of semi-supervised and self-supervised learning. Hope you will like it!

[https://taying-cheng.medium.com/supervised-semi-supervised-unsupervised-and-self-supervised-learning-7fa79aa9247c](https://taying-cheng.medium.com/supervised-semi-supervised-unsupervised-and-self-supervised-learning-7fa79aa9247c)",LearnMachineLearning
r24q0v,1637869505.0,Creating numeric word representation of input sentences resulting in MemoryError,"I am trying to use [`CountVectorizer`](https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage) to obtain word numerical word representation of data which is essentialy list of 160000 English sentences:

    import pandas as pd
    import numpy as np
    from sklearn.feature_extraction.text import CountVectorizer

    df_train = pd.read_csv('data/train.csv')

    vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\b\w+\b', min_df=1)
    X = vectorizer.fit_transform(list(df_train.text))

Then printing `X`:

    >>> X
    <160000x693699 sparse matrix of type '<class 'numpy.int64'>'
    with 3721191 stored elements in Compressed Sparse Row format>

But converting the whole to array to get the numerical word representation of all data gives:

    >>> X.toarray()
    ---------------------------------------------------------------------------
    MemoryError                               Traceback (most recent call last)
    ~\AppData\Local\Temp/ipykernel_11636/854451212.py in <module>
    ----> 1 X.toarray()

    c:\users\crrma\.virtualenvs\humor-detection-2-8vpiokuk\lib\site-packages\scipy\sparse\compressed.py in toarray(self, order, out)
       1037         if out is None and order is None:
       1038             order = self._swap('cf')[0]
    -> 1039         out = self._process_toarray_args(order, out)
       1040         if not (out.flags.c_contiguous or out.flags.f_contiguous):
       1041             raise ValueError('Output array must be C or F contiguous')

    c:\users\crrma\.virtualenvs\humor-detection-2-8vpiokuk\lib\site-packages\scipy\sparse\base.py in _process_toarray_args(self, order, out)
       1200             return out
       1201         else:
    -> 1202             return np.zeros(self.shape, dtype=self.dtype, order=order)
       1203
       1204

    MemoryError: Unable to allocate 827. GiB for an array with shape (160000, 693699) and data type int64

For the example in the linked schikit learn [doc page](https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage), they have used only five sentences. Thus, for them `X.toarray()` seem to have returned the array of numerical word representation. But since my dataset contains 160000 sentences, (in error message) it seems that it is resulting in vocabulary of size 693699 (which contains both unique unigrams and bigrams, due to `ngram_range` parameter passed to `CountVectorizer`) and hence facing insufficient memory issue.

**Q1.** How can I fix this? I am thinking to simply reject `X` and separately transform in mini batches as shown below. Is this correct?

    >>> X_batch = list(df_train[:10].text)  # do this for 160000 / batch_size batches
    >>> X_batch_encoding = vectorizer.transform(X_batch).toarray()
    >>> X_batch_encoding
    array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)

    >>> X_batch_encoding[0].shape
    (693699,)

**Q2.** I am thinking to train neural network and decision tree on this encoding for humor detection. But I guess it wont be great idea to have 693699 length vector to represent single sentence. Right? If yes, what should I do instead? Should I opt to use only unigrams while fitting `CountVectorizer` (but it will not capture even minimal context of words, unlike bigrams) ?

PS: I am creating baseline for humor detection, I am required to use `CountVectorizer`.",LearnMachineLearning
r246im,1637868037.0,Leave columns out for training a model,"I have a basic classifier for a dataset which includes about 40 columns. The first two are an ID and the associated name.
I eliminated all the non-numeric columns so my classifier only uses the numeric values for its prediction, so I eliminated the ID and name column too.
Now I want to associate the predicted data and the test set with the IDs and names which I excluded before the classifier training. Is there a way to either leave the IDs and names in the data set and tell python Not to use them during Training or to associate them back After the prediction?
I use python with pandas, sklearn, etc. The classifier is a logistic regression with sklearn.",LearnMachineLearning
r23juf,1637866347.0,Matching Pursuit Explanation,Does anybody can explain me the matching pursuit algorithm with all its steps and their meanings?,LearnMachineLearning
r23jpj,1637866339.0,Generating music on AWS - questions,"I just want to upload mp3 files, and receive mp3 outputs.

I'm looking for something different from AWS's DeepComposer, which creates a song output based on your input of a melody.

The difference is that I want to upload a dataset of many songs, while DeepComposer seems to be a 'keyboard' where you design a melody of a single song.

Can you point me in the right direction of guides or tools? Or recommend AI/ML models?

I have 550 files, total size is 12.5 GB. AWS is a requirement (to use the credits.)",LearnMachineLearning
r22tyr,1637864378.0,Nothing shows up when I try to create a bounding box around my object,"So I trained a model on Jupyter Notebook and it worked with an error of <0.8. It generated a yolo weights file that I now am using to create a bounding box around an image. I have the following code to create a bounding box using Yolo and OCV but nothing shows in the image. No bounding box at all. The training worked for sure but I don't know whats wrong. Here is my code:

[https://paste.ofcode.org/AgGJwxnZjBWHRbS82FGY8n](https://paste.ofcode.org/AgGJwxnZjBWHRbS82FGY8n)

It doesn't throw me any errors but the dog image shows up with no box around it",LearnMachineLearning
r20dmh,1637857835.0,Predicting the number of citations on essays,"Hello!

For a machine learning project, I and some other students are working in a group to build a python based citations prediction algorithm. We have a dataset of 10.000 essays in a JSON file, each with a unique DOI number, citation count, publication venue, title, topics, authors and a small abstract of the paper. We have to use this to make a model which predicts the number of citations for each essay. We have tried a lot of models and did some feature engineering, but can't seem to get a much higher r2 score than 0.42. We've dived a lot into literature regarding the topic but none seem to specify which exact model works best, just what features work best. We've made log transformations on original and created features like 'references', 'years\_old', 'OA\_Yes', 'venue\_count', 'topic\_count', 'abstract\_avg\_nr\_chars', 'author\_count'.

Do any of you have experience with this, and/or, if so, can you give us any hints/tips regarding model usage or feature engineering?

Thank you in advance for any input! It is very much appreciated!",LearnMachineLearning
r1w2hr,1637845260.0,"45 worked examples in machine learning (energy, medicine, banking, retail, physics, finance...)", [https://www.neuraldesigner.com/learning/examples](https://www.neuraldesigner.com/learning/examples),LearnMachineLearning
r1v2z9,1637841690.0,Generating confidence scores for clustering algorithms .,"I have a dataset which requires confidence scores for each point in the dataset . Apart from the confidence score I require each point have a leader and a follower associated with it. For the case of a leader, I think assigning the point closest to the centroid for a particular cluster as the leader. Now I'm not sure related to the confidence score or the follower of the points.

Any advise is appreciated!",LearnMachineLearning
r1v2n3,1637841655.0,Help with AI assignment," Hi guys. I'm new to AI and ML and I need help with an assignment, could someone please walk me through what needs to be done on each question in the assignment because I don't understand a thing.

**Question**

In sports prediction, large numbers of factors including the historical performance of the teams, results of matches, and data on players have to be accounted for to help different stakeholders understand the odds of winning or losing. **Demonstrate the following:**

* **FIFA 20 Dataset:** FIFA 20 complete player data set is a collection of detailed attributes for every player registered in the latest edition of the FIFA 20 database. Get the data on Kaggle.

1. Demonstrate the data preparation & feature extraction process
2. Create feature subsets which show maximum correlation with the dependent variable.
3. Create and train a suitable regression machine learning model that predicts the overall rating score of a player based on his attributes.
4. Measure the performance of the model and fine-tune it as a process of optimisation.
5. Use the data from another season which was not used during the training to test how good the model is.
6. Deploy the model on a simple web page using either (Heroku, Streamlite or Flask) and upload a link to the video that shows how the application(on the website) works.",LearnMachineLearning
r1uz2q,1637841294.0,RTX 3080ti(12gb) or QUADRO M6000(24gb) for deep learning ?,,LearnMachineLearning
r1sqzw,1637832259.0,Where should i start with ML?,"Hi, everyone, I'm 22, and i have a beginner knowledge of python from past learning experiences. I'm really interested in the world of machine learning, neural networks and artificial intelligence, but i don't know what the ""roadmap"" is for learning about this. If anyone could point out the steps, and maybe recommend some good learning material or courses i'd be really grateful.",LearnMachineLearning
r1sj1e,1637831345.0,Which GAN make people(not real) smile best?,"I'd like to train a gan to get a people from sad to smile. But the people is not the real people, it should be like the people in famous painting like Mona Lisa. Which gan model or pretraned model can I use, and which dataset can I use please.

Thanks a lot.",LearnMachineLearning
r1rghr,1637826995.0,Programming the gradient descent from the scratch,"This is the most fundamental and essential part for understanding the machine learning.

Here I made, for the purposes of my learning, functions and explanations for programming the gradient descent from the scratch. Purposely, I made it for linear regression. When I understood it on linear regression it was no pain to fully implement it on any kind of Neural Networks. Give a star for my further motivation if you like it.

[https://github.com/Vitomir84/Statistics-and-probability/blob/master/Gradient%20and%20programming%20gradient%20descent.ipynb](https://github.com/Vitomir84/Statistics-and-probability/blob/master/Gradient%20and%20programming%20gradient%20descent.ipynb)",LearnMachineLearning
r1pf4c,1637819560.0,Image is tinted blue even when opened afresh,"&#x200B;

[Original image](https://preview.redd.it/57zr3pnnko181.png?width=640&format=png&auto=webp&s=771d27f0c6c031effb2311b34f4ae93cc40f7943)

[Displayed image](https://preview.redd.it/xj6v867pko181.png?width=640&format=png&auto=webp&s=27a877b308f6c93fd58d6e40c706fe6e2169daa4)

Hello!
So I have some code that pre-processes some images for me. However, the image is tinted blue by the end of the process for some reason I don't understand.

    og_image = imread('test_image1.PNG')
    cv2.imshow("""", og_image)

Even when I use imread and imshow to display the original image again towards the end of that file, it still looks blue for some reason. I understand if it turns blue somewhere along the way, but why does it do that even when I read and display the original image afresh? If anybody knows any common mistakes that lead to this, please do tell me. The libraries I've used are skimage, numpy and cv2.

Couldn't find anything useful on StackOverflow or anywhere else. Sorry I can't post my entire code here for obvious reasons, but I'll try my best to share as much as I can if it'll help understand the problem. I'm kinda new to this, so any help is greatly appreciated!",LearnMachineLearning
r1p73d,1637818806.0,Time series forecasting daily temperature,So I have this dataset where it contains monthly record of temperature. Can I still predict daily temperature even if I only have monthly record of temperature??,LearnMachineLearning
r1owph,1637817816.0,Is a powerful laptop really needed when one is doing MS in data science or will MacBook Air be enough?,,LearnMachineLearning
r1lspw,1637807752.0,Best way to plot soft-clustering models in Python?,"

Hi guys, I'm in a machine learning course, and we're doing an assignment on hard vs soft-clustering algorithms (in particular K-means vs. fuzzy C-means). I'm at the point where I want to plot my clusters. K-means is easy because it's either part of the cluster or its not, so I can set colors on the data points in Matplotlib accordingly.

However for the C-means it's possible for clusters to overlap. I could just assume it's part of the cluster with the largest weight, but I feel like that is removing possibly important information about how things are clustered, and would end up producing a similar plot to the K-means.

Are there any good ways for plotting soft-clustering algorithms in Python?",LearnMachineLearning
r1ledn,1637806461.0,Salary: SWE vs ML,"Oftentimes, I see posts from people in either here or r/machinelearning, and the OPs talk about having been in the field and are making less money than those who do ""vanilla"" or ""basic"" SWE work.",LearnMachineLearning
r1h9ld,1637793750.0,StackOverflow Analysis,"Hello everyone. I am a cloud and data science noob. As a personal project I want to do some analysis on stackoverflow posts (topic modeling and then hopefully identifying useful trends and how they have evolved since 2015).
I see something called SOTorrent hosted on BigQuery. I would want to query all the posts and store them somewhere (~100GB). I cannot store it locally as I don’t have enough storage on my laptop.
So my question is where could I store it such that it is accessible to my Colab notebook (where I will run some python code for analysis)?
Any ideas/pointers, over even suggestion for alternatives to the described flow would be super helpful.",LearnMachineLearning
r1du84,1637784143.0,Forecasting using LSTM in PyTorch," Hi,

I want to use an LSTM in a time forecasting problem. Actually, I want to replace the convolutional layers in the rainbow algorithm (RL) network with an LSTM to consider time series. I have seen many examples on the internet but none of them helped me with my confusion. My input is divided into an encoder part and decoder part which are both divided into continuous variables and categorical variables. What is the best way to build an architecture that takes into consideration all the types of variables? Can anyone share any helpful resources? Also, my confusion is with the loss function. Will I need to do anything like in the picture attached?
Thanks a lot

&#x200B;

https://preview.redd.it/59hrd8linl181.png?width=854&format=png&auto=webp&s=1c65f4944d9a992f2159d543aa51c78bceceff04",LearnMachineLearning
r1bd7l,1637777508.0,No data no problem... Unsupervised learning (TSDAE) and sentence transformers,"Hi all, I put together [an article and video covering TSDAE fine-tuning](https://www.pinecone.io/learn/unsupervised-training-sentence-transformers/) for sentence transformer models. Basically, how we can use plain unstructured text data to fine-tune a sentence transformer (not quite *no* data, but close!). From the TSDAE paper, you actually only need something like 10-100K sentences to fine-tune a pretrained transformer for producing pretty good sentence embeddings.

I was achieving same STSb evaluation with a TSDAE train BERT as I was getting with my own NLI (labeled) dataset trained BERT (using softmax loss). So pretty cool imo - although in reality supervised methods produce better performing models, if you have no labeled data, unsupervised is the way to go.

It was really cool learning about this, planning to do more on unsupervised sentence transformers in the future - let me know what you think!",LearnMachineLearning
r1bau8,1637777328.0,Model for credit payment probability?,"I was asked to work on a model to predit a credit payment probability. I know there are models to predict credit default which gives you true or false in the outcomes, but in this case I need a number as a probability. Which model could give me that?",LearnMachineLearning
r1667s,1637763370.0,Is it possible to gather user info just using Javascript ?,"I changed my major to Information Security this year and my teacher gave me an assignment that i don't really know how to do. We started learning python this year and my assignment is gathering user information from our website, like users interests. But i'm better with javascript, so is it possible to do such a task with javascript ?

Sorry if my english is bad, i live in europe :/",LearnMachineLearning
r15mte,1637761879.0,Has anyone used Algorithmia to deploy models? I'm unconvinced.,"[Algorithmia](https://algorithmia.com/) advertises themselves as an MLops platform for data scientists, and they provide an easy way to host models on a scalable REST API.

This sounds like a perfect solution for a data scientist or hobbyist who wants to host models for cheap and not worry about the devops. But as I've gotten more familiar with it, I have more questions...

For the base tier, Algorithmia requires you to host your model's request handling code on a [Github repository owned by them](https://algorithmia.com/developers/algorithm-development/your-first-algo). A separate repository for your request handling code seems like a strange pattern to develop in. They also encourage you to develop in their Web UI. Again, another pattern that feels forced.

They also have an ominous section in their [terms of service](https://algorithmia.com/api_dev_terms) that says: ""You do not transfer ownership of the Software to Algorithmia, but you do hereby grant Algorithmia ... [a] fully paid-up and royalty free license to use and permit others to use the Software"". Which feels overly aggressive for forcing you to use their source hosting.

Between an unnatural development environment and a sketchy ownership clause, I'm reluctant to continue using Algorithmia.

Has anyone had similar experiences with Algorithmia? Am I just being overly skittish and misinterpreting the ownership clause? Are their better repository patterns (git subtrees) that other people have used with them? Are there better companies to host models or should I have never even attempted leaving the AWS and GCP hosting land?",LearnMachineLearning
r13jq5,1637755030.0,Possibilities object recognition,"As a complete newbie in the AI and Machine Learning space, I was wondering how far the possibilities currently reach. I came up with an idea to recognize whether different types of glasses are filled or empty. The idea is extremely novel still and therefore I'm wondering if that's (already) possible with AI and Machine learning. At the same time am I wondering if something like this can be easily programmed (or learned how to program) by a beginner or that it requires a certain experience and can only be done by someone with experience in the field.",LearnMachineLearning
r13iiq,1637754906.0,Federated Learning implementation,"Hi,

I was wondering if anyone has come across an implementation of a federated machine learning system

I want to build one for the hospital system and hardware is not my forte.


Could I can spin VM on the cloud system of the respective hospitals and make sure they can communicate between each other?


Thanks!",LearnMachineLearning
r132f4,1637753254.0,"Good AI & MLcourses, YouTube channels and books for beginners","Hi guys which courses, YouTube channels and books do you recommend for absolute beginners learning AI & ML.",LearnMachineLearning
r130k1,1637753065.0,Beginners Helpful Guide to Training Deep Learning Models on the Cloud," A while ago, a friend of mine contacted me here on LinkedIn and asked me if I could give him some guiding into how he can train his deep learning models on the cloud.

I realized that he was facing the same issue that I faced when I tried to train my models on the cloud for the first time.

The issue is that there are lots of resources online and it might take you hours and days to find exactly what you need.

After all, you just want to take your training code and data and put them on the cloud.

In order to address this issue, I wrote a guide on how to do exactly this. It's titled : ""Beginners Helpful Guide to Training Deep Learning Models on the Cloud"".

Link to article : [https://pub.towardsai.net/beginners-helpful-guide-to-training-deep-learning-models-on-the-cloud-3202f29afd1b](https://pub.towardsai.net/beginners-helpful-guide-to-training-deep-learning-models-on-the-cloud-3202f29afd1b)

Follow me on Medium if you want more of this kind of articles!",LearnMachineLearning
r12jwv,1637751296.0,I want to say this about that,"These are non-ML-generated titles that I conceived of in the Bath for ML-based nonverbal commentaries on the nature of being and time.

> Pyroclastic renubilation on the boundaries of the apothecary era, reimagined for today's humans

> An apple in the eye of Orange

> First tries at replacing human genitals with extra hair

> Blood of thorns

> Globular ellipsoids of phat

> Tremor with beef

> Placement of intriguity

> Purple day-glo revival-spackle captions

> Particle diving boards

Earlier-mentioned:

> Just pursued, caught, and then retracted by our marketing department: Lark keepsakes that froze intact then woke up when injected into microdotted housefly larval brain incongruities

Where the sober rush to crazy titles started:

http://phobrain.com/pr/home/siagal.html

Let's remember gcc on Thanksgiving!

https://www.youtube.com/watch?v=ZEAlhVOZ8qQ

With a nod to history without statues:

https://www.capecodtimes.com/in-depth/news/2020/11/19/plymouth-400-mayflower-first-thanksgiving-pilgrims-wampanoag-massasoit/6283891002/",LearnMachineLearning
r11355,1637745266.0,How to evaluate your model's performance,"At [LabelFlow](https://labelflow.ai/website), we know firsthand how important it is to choose the right metrics to evaluate your AI model's performance.
Check out our latest [blog article](https://labelflow.ai/posts/ai-metrics-image-classification) where we walk you through how to evaluate your model and what metrics to use, as we take the example of an image classification model!

[#machinelearning](https://www.linkedin.com/feed/hashtag/?keywords=machinelearning&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6868929020562571264) [#AI](https://www.linkedin.com/feed/hashtag/?keywords=ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6868929020562571264) [#evaluation](https://www.linkedin.com/feed/hashtag/?keywords=evaluation&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6868929020562571264)",LearnMachineLearning
r110aq,1637744938.0,Machine learning for profit not classification.,"

Hi guys I am learning about Machine Learning and have a question...

It  seems that most machine learning is used for classification  purposes.... for example if we are looking at horse racing predictions  some models are simply interested in classifying the winner..

But  what if in our example of horse racing, we are interested not in  classifying the winner but in producing a model which maximizes profit  out of the learning data set ?

It  would or course be much to find a model that can predict a steady load  of high odds winners rather than lots of smaller priced winners...

What models can do this and are built for this purpose?",LearnMachineLearning
r10z1o,1637744785.0,"Finding optimal sell limits in a stock trade, based on historical data and buy signals.","This is for crypto (I know, I know). I have the following data inputs available:


- historical data of the [open, low, high, close] prices of 4hr intervals of ETH/USD.


- buy signals on [close] of some intervals - ie. ""*Buy <here>*"". These appear to be pretty reliable ~60-70% of the time.


**Q - If I buy on the buy signals, how could I start training a system to detect the *optimal sell limits* for my trade?**


ie. ""Does it rise +0.7% before I sell? Does it fall -2% before I sell at loss?"" etc.


I've coded quite a bit but am new to ML - hope to use tensorflow.js but open to ideas. I guess any conceptual starting points or useful resources would be helpful.",LearnMachineLearning
r10kwg,1637743175.0,Questions regarding Machine learning/ AI calculation power locally/cloud,"Hello everyone,

For my university I'm doing an internship where I have to do research to formulate a solution to the current problems around  machine learning/artificial intelligence calculation power.

Now on the internet I can find plenty of resources regarding AWS/Azure to make this but one of the main things I have to pay attention too is the costs of setting a system up and finding out which one suits my university the best. For this I was considering a local solution and a cloud solution as viable options. but I have found trouble finding the correct resources that can point me into the right direction regarding local solutions. Most treads/posts/blogs I have found so far are 4-5 years old and while the info could be helpful I'd much rather have the chance to see a more updated opinion or information.


So my main question would be to hear everyone's opinion about different solutions regarding Machine learning calculation power that is cloud based or locally.


Thanks in advance for the replies.",LearnMachineLearning
r0zvrb,1637740538.0,Monte Carlo simulation for approximation of pi number,"The number π is a mathematical constant, approximately equal to 3.14159. It is defined in Euclidean geometry as the ratio of a circle's circumference to its diameter, and also has various equivalent definitions. The number appears in many formulas in all areas of mathematics and physics. The earliest known use of the Greek letter π to represent the ratio of a circle's circumference to its diameter was by Welsh mathematician William Jones in 1706. It is also referred to as Archimedes's constant.

The simulation works on the principle of generating random points in the square of the page r = 1. Based on the distance of the point of the center of the coordinate system, it is possible to determine whether the point is inside or outside imaginary quarters of a circle. How is a quarter of the surface of a circle$\\frac{r\^2\\pi}{4}$, and area of square $r\^2$, number $\\pi$ can be approximated as $\\frac{k}{m} \* 4 $ where k is the number of points within a quarter of a circle and m is the total number of simulated random points. As the number of randomly generated points increases, it is possible to calculate the number $\\pi$ with greater precision at the expense of time efficiency.

[https://github.com/Vitomir84/Mathematical\_research/blob/master/Approximation\_of\_pi\_number.ipynb](https://github.com/Vitomir84/Mathematical_research/blob/master/Approximation_of_pi_number.ipynb)",LearnMachineLearning
r0um7e,1637722724.0,Opinions on books for core concepts of machine learning.,"Hi,

I want to buy some books for christmas for myself and I'm currently trying to develop my machine learning skills. I'm just wondering though, because of how much things tend to change with jobs like machine learning and data science, is it actually worth buying books, or are they going to become outdated relatively quickly? Am I better off saving my money and learning with google? I really want to buy core concept books that are going to last a long time and act as a foundational reference.


If these types of book existl, does anyone have any recommendation's please?",LearnMachineLearning
